id,quality_attribute,keyword,matched_word,match_idx,sentence,source,author,repo,version,wiki,url
https://github.com/google/deepvariant/issues/812:2319,availability,echo,echo,2319,"9.3"". ```. ## Install Singularity. I don't have Singularity on the machine yet, so:. https://docs.sylabs.io/guides/4.1/user-guide/quick_start.html#quick-installation-steps. ```bash. sudo yum update -y && \. sudo yum groupinstall -y 'Development Tools' && \. sudo yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools. ```. ```. sudo yum groupinstall -y 'Development Tools'. # Install RPM packages for dependencies. sudo yum install -y \. autoconf \. automake \. cryptsetup \. fuse3-devel \. git \. glib2-devel \. libseccomp-devel \. libtool \. runc \. squashfs-tools \. wget \. zlib-devel. ```. ```bash. sudo dnf install dnf-plugins-core. sudo dnf copr enable dctrud/squashfs-tools-ng. sudo dnf install squashfs-tools-ng. ```. ```bash. export VERSION=1.21.0 OS=linux ARCH=amd64 && \. wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz && \. sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz && \. rm go$VERSION.$OS-$ARCH.tar.gz. ```. ```bash. echo 'export PATH=/usr/local/go/bin:$PATH' >> ~/.bashrc && \. source ~/.bashrc. ```. ```bash. export VERSION=4.1.0 && \. wget https://github.com/sylabs/singularity/releases/download/v${VERSION}/singularity-ce-${VERSION}.tar.gz && \. tar -xzf singularity-ce-${VERSION}.tar.gz && \. cd singularity-ce-${VERSION}. ```. ```bash. ./mconfig && \. make -C builddir && \. sudo make -C builddir install. ```. At this point, I have singularity installed. ```bash. $ singularity --version. singularity-ce version 4.1.0. ```. ## Get data and run DeepVariant. Now, let me try to follow similar steps:. ```bash. singularity build DeepVariant_1.6.1.sif docker://google/deepvariant:1.6.1. ```. From here, I used https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-pacbio-model-case-study.md to test. Download data:. ```bash. mkdir -p reference. FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_G",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:2492,availability,down,download,2492,"`bash. sudo yum update -y && \. sudo yum groupinstall -y 'Development Tools' && \. sudo yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools. ```. ```. sudo yum groupinstall -y 'Development Tools'. # Install RPM packages for dependencies. sudo yum install -y \. autoconf \. automake \. cryptsetup \. fuse3-devel \. git \. glib2-devel \. libseccomp-devel \. libtool \. runc \. squashfs-tools \. wget \. zlib-devel. ```. ```bash. sudo dnf install dnf-plugins-core. sudo dnf copr enable dctrud/squashfs-tools-ng. sudo dnf install squashfs-tools-ng. ```. ```bash. export VERSION=1.21.0 OS=linux ARCH=amd64 && \. wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz && \. sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz && \. rm go$VERSION.$OS-$ARCH.tar.gz. ```. ```bash. echo 'export PATH=/usr/local/go/bin:$PATH' >> ~/.bashrc && \. source ~/.bashrc. ```. ```bash. export VERSION=4.1.0 && \. wget https://github.com/sylabs/singularity/releases/download/v${VERSION}/singularity-ce-${VERSION}.tar.gz && \. tar -xzf singularity-ce-${VERSION}.tar.gz && \. cd singularity-ce-${VERSION}. ```. ```bash. ./mconfig && \. make -C builddir && \. sudo make -C builddir install. ```. At this point, I have singularity installed. ```bash. $ singularity --version. singularity-ce version 4.1.0. ```. ## Get data and run DeepVariant. Now, let me try to follow similar steps:. ```bash. singularity build DeepVariant_1.6.1.sif docker://google/deepvariant:1.6.1. ```. From here, I used https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-pacbio-model-case-study.md to test. Download data:. ```bash. mkdir -p reference. FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_n",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:3118,availability,Down,Download,3118,"=linux ARCH=amd64 && \. wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz && \. sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz && \. rm go$VERSION.$OS-$ARCH.tar.gz. ```. ```bash. echo 'export PATH=/usr/local/go/bin:$PATH' >> ~/.bashrc && \. source ~/.bashrc. ```. ```bash. export VERSION=4.1.0 && \. wget https://github.com/sylabs/singularity/releases/download/v${VERSION}/singularity-ce-${VERSION}.tar.gz && \. tar -xzf singularity-ce-${VERSION}.tar.gz && \. cd singularity-ce-${VERSION}. ```. ```bash. ./mconfig && \. make -C builddir && \. sudo make -C builddir install. ```. At this point, I have singularity installed. ```bash. $ singularity --version. singularity-ce version 4.1.0. ```. ## Get data and run DeepVariant. Now, let me try to follow similar steps:. ```bash. singularity build DeepVariant_1.6.1.sif docker://google/deepvariant:1.6.1. ```. From here, I used https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-pacbio-model-case-study.md to test. Download data:. ```bash. mkdir -p reference. FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai. ```. ```bash. mkdir -p input. HTTPDIR=https://downloads.pacbcloud.com/public/dataset/HG003/deepvariant-case-study. curl ${HTTPDIR}/HG003.GRCh38.chr20.pFDA_truthv2.bam > input/HG003.GRCh38.chr20.pFDA_truthv2.bam. curl ${HTTPDIR}/HG003.GRCh38.chr20.pFDA_truthv2.bam.bai > input/HG003.GRCh38.chr20.pFDA_truthv2.bam.bai. ```. ```bash. ulimit -u 10000. BIN_VERSION=""1.6.1"". mkdir -p deepvariant_output. ```. @Carl-labhub mentioned ""When I run it, I’m doing so from an interactive session with singularity exec"". I'm a bit confused by this. Maybe you mean `singularity shell`? So I tried:. ```bash. sin",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:3572,availability,down,downloads,3572,"{VERSION}.tar.gz && \. cd singularity-ce-${VERSION}. ```. ```bash. ./mconfig && \. make -C builddir && \. sudo make -C builddir install. ```. At this point, I have singularity installed. ```bash. $ singularity --version. singularity-ce version 4.1.0. ```. ## Get data and run DeepVariant. Now, let me try to follow similar steps:. ```bash. singularity build DeepVariant_1.6.1.sif docker://google/deepvariant:1.6.1. ```. From here, I used https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-pacbio-model-case-study.md to test. Download data:. ```bash. mkdir -p reference. FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai. ```. ```bash. mkdir -p input. HTTPDIR=https://downloads.pacbcloud.com/public/dataset/HG003/deepvariant-case-study. curl ${HTTPDIR}/HG003.GRCh38.chr20.pFDA_truthv2.bam > input/HG003.GRCh38.chr20.pFDA_truthv2.bam. curl ${HTTPDIR}/HG003.GRCh38.chr20.pFDA_truthv2.bam.bai > input/HG003.GRCh38.chr20.pFDA_truthv2.bam.bai. ```. ```bash. ulimit -u 10000. BIN_VERSION=""1.6.1"". mkdir -p deepvariant_output. ```. @Carl-labhub mentioned ""When I run it, I’m doing so from an interactive session with singularity exec"". I'm a bit confused by this. Maybe you mean `singularity shell`? So I tried:. ```bash. singularity shell --bind /usr/lib/locale/ DeepVariant_1.6.1.sif. ```. This gets into a shell mode, then I ran:. ```. Singularity> /opt/deepvariant/bin/run_deepvariant \. > --model_type PACBIO \. > --ref reference/GRCh38_no_alt_analysis_set.fasta \. > --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. > --output_vcf deepvariant_output/output.vcf.gz \. > --num_shards $(nproc) \. > --regions chr20. ```. Directly `singularity exec` with the command (just l",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:4906,availability,error,errors,4906,"i.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai. ```. ```bash. mkdir -p input. HTTPDIR=https://downloads.pacbcloud.com/public/dataset/HG003/deepvariant-case-study. curl ${HTTPDIR}/HG003.GRCh38.chr20.pFDA_truthv2.bam > input/HG003.GRCh38.chr20.pFDA_truthv2.bam. curl ${HTTPDIR}/HG003.GRCh38.chr20.pFDA_truthv2.bam.bai > input/HG003.GRCh38.chr20.pFDA_truthv2.bam.bai. ```. ```bash. ulimit -u 10000. BIN_VERSION=""1.6.1"". mkdir -p deepvariant_output. ```. @Carl-labhub mentioned ""When I run it, I’m doing so from an interactive session with singularity exec"". I'm a bit confused by this. Maybe you mean `singularity shell`? So I tried:. ```bash. singularity shell --bind /usr/lib/locale/ DeepVariant_1.6.1.sif. ```. This gets into a shell mode, then I ran:. ```. Singularity> /opt/deepvariant/bin/run_deepvariant \. > --model_type PACBIO \. > --ref reference/GRCh38_no_alt_analysis_set.fasta \. > --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. > --output_vcf deepvariant_output/output.vcf.gz \. > --num_shards $(nproc) \. > --regions chr20. ```. Directly `singularity exec` with the command (just like https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-pacbio-model-case-study.md) should be fine too. My `make_examples` step completed without any issues. It took:. ```. real 9m7.540s. user 215m38.303s. sys 6m41.297s. ```. I let the whole run finish just to be sure. --> The full run also completed without any errors. @Carl-labhub, can you check what I did above, and see what might be different on your side? And, there were previous GitHub issues that might be worth reading through to see if there are any relevant clues. For example: https://github.com/google/deepvariant/issues/677.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:71,deployability,version,version,71,"# Attempt to reproduce the issue. ## Get a machine with the same Linux version. I used GCP to get a machine to test. Hopefully this provides a similar environment:. `gcloud compute instances create ""${USER}-test"" --scopes ""compute-rw,storage-full,cloud-platform"" --image-family almalinux-9 --image-project almalinux-cloud --machine-type ""n1-standard-64"" --zone ""us-west1-b""`. I ssh'ed into the machine:. ```bash. gcloud compute ssh pichuan-test --zone ""us-west1-b"". ```. Check the Linux version:. ```. $ uname -a. Linux pichuan-test.us-west1-b.c.brain-genomics.google.com.internal 5.14.0-362.24.2.el9_3.x86_64 #1 SMP PREEMPT_DYNAMIC Sat Mar 30 14:11:54 EDT 2024 x86_64 x86_64 x86_64 GNU/Linux. ```. And I ran this too:. ```. $ cat /etc/os-release. NAME=""AlmaLinux"". VERSION=""9.3 (Shamrock Pampas Cat)"". ID=""almalinux"". ID_LIKE=""rhel centos fedora"". VERSION_ID=""9.3"". PLATFORM_ID=""platform:el9"". PRETTY_NAME=""AlmaLinux 9.3 (Shamrock Pampas Cat)"". ANSI_COLOR=""0;34"". LOGO=""fedora-logo-icon"". CPE_NAME=""cpe:/o:almalinux:almalinux:9::baseos"". HOME_URL=""https://almalinux.org/"". DOCUMENTATION_URL=""https://wiki.almalinux.org/"". BUG_REPORT_URL=""https://bugs.almalinux.org/"". ALMALINUX_MANTISBT_PROJECT=""AlmaLinux-9"". ALMALINUX_MANTISBT_PROJECT_VERSION=""9.3"". REDHAT_SUPPORT_PRODUCT=""AlmaLinux"". REDHAT_SUPPORT_PRODUCT_VERSION=""9.3"". ```. ## Install Singularity. I don't have Singularity on the machine yet, so:. https://docs.sylabs.io/guides/4.1/user-guide/quick_start.html#quick-installation-steps. ```bash. sudo yum update -y && \. sudo yum groupinstall -y 'Development Tools' && \. sudo yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools. ```. ```. sudo yum groupinstall -y 'Development Tools'. # Install RPM packages for dependencies. sudo yum install -y \. autoconf \. automake \. cryptsetup \. fuse3-devel \. git \. glib2-devel \. libseccomp-devel \. libtool \. runc \. squashfs-tools \. wget \. zlib-devel. ```. ```bash. sudo dnf install dnf-plugins-core.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:487,deployability,version,version,487,"# Attempt to reproduce the issue. ## Get a machine with the same Linux version. I used GCP to get a machine to test. Hopefully this provides a similar environment:. `gcloud compute instances create ""${USER}-test"" --scopes ""compute-rw,storage-full,cloud-platform"" --image-family almalinux-9 --image-project almalinux-cloud --machine-type ""n1-standard-64"" --zone ""us-west1-b""`. I ssh'ed into the machine:. ```bash. gcloud compute ssh pichuan-test --zone ""us-west1-b"". ```. Check the Linux version:. ```. $ uname -a. Linux pichuan-test.us-west1-b.c.brain-genomics.google.com.internal 5.14.0-362.24.2.el9_3.x86_64 #1 SMP PREEMPT_DYNAMIC Sat Mar 30 14:11:54 EDT 2024 x86_64 x86_64 x86_64 GNU/Linux. ```. And I ran this too:. ```. $ cat /etc/os-release. NAME=""AlmaLinux"". VERSION=""9.3 (Shamrock Pampas Cat)"". ID=""almalinux"". ID_LIKE=""rhel centos fedora"". VERSION_ID=""9.3"". PLATFORM_ID=""platform:el9"". PRETTY_NAME=""AlmaLinux 9.3 (Shamrock Pampas Cat)"". ANSI_COLOR=""0;34"". LOGO=""fedora-logo-icon"". CPE_NAME=""cpe:/o:almalinux:almalinux:9::baseos"". HOME_URL=""https://almalinux.org/"". DOCUMENTATION_URL=""https://wiki.almalinux.org/"". BUG_REPORT_URL=""https://bugs.almalinux.org/"". ALMALINUX_MANTISBT_PROJECT=""AlmaLinux-9"". ALMALINUX_MANTISBT_PROJECT_VERSION=""9.3"". REDHAT_SUPPORT_PRODUCT=""AlmaLinux"". REDHAT_SUPPORT_PRODUCT_VERSION=""9.3"". ```. ## Install Singularity. I don't have Singularity on the machine yet, so:. https://docs.sylabs.io/guides/4.1/user-guide/quick_start.html#quick-installation-steps. ```bash. sudo yum update -y && \. sudo yum groupinstall -y 'Development Tools' && \. sudo yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools. ```. ```. sudo yum groupinstall -y 'Development Tools'. # Install RPM packages for dependencies. sudo yum install -y \. autoconf \. automake \. cryptsetup \. fuse3-devel \. git \. glib2-devel \. libseccomp-devel \. libtool \. runc \. squashfs-tools \. wget \. zlib-devel. ```. ```bash. sudo dnf install dnf-plugins-core.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:739,deployability,releas,release,739,"# Attempt to reproduce the issue. ## Get a machine with the same Linux version. I used GCP to get a machine to test. Hopefully this provides a similar environment:. `gcloud compute instances create ""${USER}-test"" --scopes ""compute-rw,storage-full,cloud-platform"" --image-family almalinux-9 --image-project almalinux-cloud --machine-type ""n1-standard-64"" --zone ""us-west1-b""`. I ssh'ed into the machine:. ```bash. gcloud compute ssh pichuan-test --zone ""us-west1-b"". ```. Check the Linux version:. ```. $ uname -a. Linux pichuan-test.us-west1-b.c.brain-genomics.google.com.internal 5.14.0-362.24.2.el9_3.x86_64 #1 SMP PREEMPT_DYNAMIC Sat Mar 30 14:11:54 EDT 2024 x86_64 x86_64 x86_64 GNU/Linux. ```. And I ran this too:. ```. $ cat /etc/os-release. NAME=""AlmaLinux"". VERSION=""9.3 (Shamrock Pampas Cat)"". ID=""almalinux"". ID_LIKE=""rhel centos fedora"". VERSION_ID=""9.3"". PLATFORM_ID=""platform:el9"". PRETTY_NAME=""AlmaLinux 9.3 (Shamrock Pampas Cat)"". ANSI_COLOR=""0;34"". LOGO=""fedora-logo-icon"". CPE_NAME=""cpe:/o:almalinux:almalinux:9::baseos"". HOME_URL=""https://almalinux.org/"". DOCUMENTATION_URL=""https://wiki.almalinux.org/"". BUG_REPORT_URL=""https://bugs.almalinux.org/"". ALMALINUX_MANTISBT_PROJECT=""AlmaLinux-9"". ALMALINUX_MANTISBT_PROJECT_VERSION=""9.3"". REDHAT_SUPPORT_PRODUCT=""AlmaLinux"". REDHAT_SUPPORT_PRODUCT_VERSION=""9.3"". ```. ## Install Singularity. I don't have Singularity on the machine yet, so:. https://docs.sylabs.io/guides/4.1/user-guide/quick_start.html#quick-installation-steps. ```bash. sudo yum update -y && \. sudo yum groupinstall -y 'Development Tools' && \. sudo yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools. ```. ```. sudo yum groupinstall -y 'Development Tools'. # Install RPM packages for dependencies. sudo yum install -y \. autoconf \. automake \. cryptsetup \. fuse3-devel \. git \. glib2-devel \. libseccomp-devel \. libtool \. runc \. squashfs-tools \. wget \. zlib-devel. ```. ```bash. sudo dnf install dnf-plugins-core.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:766,deployability,VERSION,VERSION,766,"# Attempt to reproduce the issue. ## Get a machine with the same Linux version. I used GCP to get a machine to test. Hopefully this provides a similar environment:. `gcloud compute instances create ""${USER}-test"" --scopes ""compute-rw,storage-full,cloud-platform"" --image-family almalinux-9 --image-project almalinux-cloud --machine-type ""n1-standard-64"" --zone ""us-west1-b""`. I ssh'ed into the machine:. ```bash. gcloud compute ssh pichuan-test --zone ""us-west1-b"". ```. Check the Linux version:. ```. $ uname -a. Linux pichuan-test.us-west1-b.c.brain-genomics.google.com.internal 5.14.0-362.24.2.el9_3.x86_64 #1 SMP PREEMPT_DYNAMIC Sat Mar 30 14:11:54 EDT 2024 x86_64 x86_64 x86_64 GNU/Linux. ```. And I ran this too:. ```. $ cat /etc/os-release. NAME=""AlmaLinux"". VERSION=""9.3 (Shamrock Pampas Cat)"". ID=""almalinux"". ID_LIKE=""rhel centos fedora"". VERSION_ID=""9.3"". PLATFORM_ID=""platform:el9"". PRETTY_NAME=""AlmaLinux 9.3 (Shamrock Pampas Cat)"". ANSI_COLOR=""0;34"". LOGO=""fedora-logo-icon"". CPE_NAME=""cpe:/o:almalinux:almalinux:9::baseos"". HOME_URL=""https://almalinux.org/"". DOCUMENTATION_URL=""https://wiki.almalinux.org/"". BUG_REPORT_URL=""https://bugs.almalinux.org/"". ALMALINUX_MANTISBT_PROJECT=""AlmaLinux-9"". ALMALINUX_MANTISBT_PROJECT_VERSION=""9.3"". REDHAT_SUPPORT_PRODUCT=""AlmaLinux"". REDHAT_SUPPORT_PRODUCT_VERSION=""9.3"". ```. ## Install Singularity. I don't have Singularity on the machine yet, so:. https://docs.sylabs.io/guides/4.1/user-guide/quick_start.html#quick-installation-steps. ```bash. sudo yum update -y && \. sudo yum groupinstall -y 'Development Tools' && \. sudo yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools. ```. ```. sudo yum groupinstall -y 'Development Tools'. # Install RPM packages for dependencies. sudo yum install -y \. autoconf \. automake \. cryptsetup \. fuse3-devel \. git \. glib2-devel \. libseccomp-devel \. libtool \. runc \. squashfs-tools \. wget \. zlib-devel. ```. ```bash. sudo dnf install dnf-plugins-core.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:965,deployability,LOG,LOGO,965,"# Attempt to reproduce the issue. ## Get a machine with the same Linux version. I used GCP to get a machine to test. Hopefully this provides a similar environment:. `gcloud compute instances create ""${USER}-test"" --scopes ""compute-rw,storage-full,cloud-platform"" --image-family almalinux-9 --image-project almalinux-cloud --machine-type ""n1-standard-64"" --zone ""us-west1-b""`. I ssh'ed into the machine:. ```bash. gcloud compute ssh pichuan-test --zone ""us-west1-b"". ```. Check the Linux version:. ```. $ uname -a. Linux pichuan-test.us-west1-b.c.brain-genomics.google.com.internal 5.14.0-362.24.2.el9_3.x86_64 #1 SMP PREEMPT_DYNAMIC Sat Mar 30 14:11:54 EDT 2024 x86_64 x86_64 x86_64 GNU/Linux. ```. And I ran this too:. ```. $ cat /etc/os-release. NAME=""AlmaLinux"". VERSION=""9.3 (Shamrock Pampas Cat)"". ID=""almalinux"". ID_LIKE=""rhel centos fedora"". VERSION_ID=""9.3"". PLATFORM_ID=""platform:el9"". PRETTY_NAME=""AlmaLinux 9.3 (Shamrock Pampas Cat)"". ANSI_COLOR=""0;34"". LOGO=""fedora-logo-icon"". CPE_NAME=""cpe:/o:almalinux:almalinux:9::baseos"". HOME_URL=""https://almalinux.org/"". DOCUMENTATION_URL=""https://wiki.almalinux.org/"". BUG_REPORT_URL=""https://bugs.almalinux.org/"". ALMALINUX_MANTISBT_PROJECT=""AlmaLinux-9"". ALMALINUX_MANTISBT_PROJECT_VERSION=""9.3"". REDHAT_SUPPORT_PRODUCT=""AlmaLinux"". REDHAT_SUPPORT_PRODUCT_VERSION=""9.3"". ```. ## Install Singularity. I don't have Singularity on the machine yet, so:. https://docs.sylabs.io/guides/4.1/user-guide/quick_start.html#quick-installation-steps. ```bash. sudo yum update -y && \. sudo yum groupinstall -y 'Development Tools' && \. sudo yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools. ```. ```. sudo yum groupinstall -y 'Development Tools'. # Install RPM packages for dependencies. sudo yum install -y \. autoconf \. automake \. cryptsetup \. fuse3-devel \. git \. glib2-devel \. libseccomp-devel \. libtool \. runc \. squashfs-tools \. wget \. zlib-devel. ```. ```bash. sudo dnf install dnf-plugins-core.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:978,deployability,log,logo-icon,978,"# Attempt to reproduce the issue. ## Get a machine with the same Linux version. I used GCP to get a machine to test. Hopefully this provides a similar environment:. `gcloud compute instances create ""${USER}-test"" --scopes ""compute-rw,storage-full,cloud-platform"" --image-family almalinux-9 --image-project almalinux-cloud --machine-type ""n1-standard-64"" --zone ""us-west1-b""`. I ssh'ed into the machine:. ```bash. gcloud compute ssh pichuan-test --zone ""us-west1-b"". ```. Check the Linux version:. ```. $ uname -a. Linux pichuan-test.us-west1-b.c.brain-genomics.google.com.internal 5.14.0-362.24.2.el9_3.x86_64 #1 SMP PREEMPT_DYNAMIC Sat Mar 30 14:11:54 EDT 2024 x86_64 x86_64 x86_64 GNU/Linux. ```. And I ran this too:. ```. $ cat /etc/os-release. NAME=""AlmaLinux"". VERSION=""9.3 (Shamrock Pampas Cat)"". ID=""almalinux"". ID_LIKE=""rhel centos fedora"". VERSION_ID=""9.3"". PLATFORM_ID=""platform:el9"". PRETTY_NAME=""AlmaLinux 9.3 (Shamrock Pampas Cat)"". ANSI_COLOR=""0;34"". LOGO=""fedora-logo-icon"". CPE_NAME=""cpe:/o:almalinux:almalinux:9::baseos"". HOME_URL=""https://almalinux.org/"". DOCUMENTATION_URL=""https://wiki.almalinux.org/"". BUG_REPORT_URL=""https://bugs.almalinux.org/"". ALMALINUX_MANTISBT_PROJECT=""AlmaLinux-9"". ALMALINUX_MANTISBT_PROJECT_VERSION=""9.3"". REDHAT_SUPPORT_PRODUCT=""AlmaLinux"". REDHAT_SUPPORT_PRODUCT_VERSION=""9.3"". ```. ## Install Singularity. I don't have Singularity on the machine yet, so:. https://docs.sylabs.io/guides/4.1/user-guide/quick_start.html#quick-installation-steps. ```bash. sudo yum update -y && \. sudo yum groupinstall -y 'Development Tools' && \. sudo yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools. ```. ```. sudo yum groupinstall -y 'Development Tools'. # Install RPM packages for dependencies. sudo yum install -y \. autoconf \. automake \. cryptsetup \. fuse3-devel \. git \. glib2-devel \. libseccomp-devel \. libtool \. runc \. squashfs-tools \. wget \. zlib-devel. ```. ```bash. sudo dnf install dnf-plugins-core.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:1335,deployability,Instal,Install,1335,"1-standard-64"" --zone ""us-west1-b""`. I ssh'ed into the machine:. ```bash. gcloud compute ssh pichuan-test --zone ""us-west1-b"". ```. Check the Linux version:. ```. $ uname -a. Linux pichuan-test.us-west1-b.c.brain-genomics.google.com.internal 5.14.0-362.24.2.el9_3.x86_64 #1 SMP PREEMPT_DYNAMIC Sat Mar 30 14:11:54 EDT 2024 x86_64 x86_64 x86_64 GNU/Linux. ```. And I ran this too:. ```. $ cat /etc/os-release. NAME=""AlmaLinux"". VERSION=""9.3 (Shamrock Pampas Cat)"". ID=""almalinux"". ID_LIKE=""rhel centos fedora"". VERSION_ID=""9.3"". PLATFORM_ID=""platform:el9"". PRETTY_NAME=""AlmaLinux 9.3 (Shamrock Pampas Cat)"". ANSI_COLOR=""0;34"". LOGO=""fedora-logo-icon"". CPE_NAME=""cpe:/o:almalinux:almalinux:9::baseos"". HOME_URL=""https://almalinux.org/"". DOCUMENTATION_URL=""https://wiki.almalinux.org/"". BUG_REPORT_URL=""https://bugs.almalinux.org/"". ALMALINUX_MANTISBT_PROJECT=""AlmaLinux-9"". ALMALINUX_MANTISBT_PROJECT_VERSION=""9.3"". REDHAT_SUPPORT_PRODUCT=""AlmaLinux"". REDHAT_SUPPORT_PRODUCT_VERSION=""9.3"". ```. ## Install Singularity. I don't have Singularity on the machine yet, so:. https://docs.sylabs.io/guides/4.1/user-guide/quick_start.html#quick-installation-steps. ```bash. sudo yum update -y && \. sudo yum groupinstall -y 'Development Tools' && \. sudo yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools. ```. ```. sudo yum groupinstall -y 'Development Tools'. # Install RPM packages for dependencies. sudo yum install -y \. autoconf \. automake \. cryptsetup \. fuse3-devel \. git \. glib2-devel \. libseccomp-devel \. libtool \. runc \. squashfs-tools \. wget \. zlib-devel. ```. ```bash. sudo dnf install dnf-plugins-core. sudo dnf copr enable dctrud/squashfs-tools-ng. sudo dnf install squashfs-tools-ng. ```. ```bash. export VERSION=1.21.0 OS=linux ARCH=amd64 && \. wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz && \. sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz && \. rm go$VERSION.$OS-$ARCH.tar.gz. ```. ```bash. echo 'export PATH=/u",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:1474,deployability,instal,installation-steps,1474,"nux version:. ```. $ uname -a. Linux pichuan-test.us-west1-b.c.brain-genomics.google.com.internal 5.14.0-362.24.2.el9_3.x86_64 #1 SMP PREEMPT_DYNAMIC Sat Mar 30 14:11:54 EDT 2024 x86_64 x86_64 x86_64 GNU/Linux. ```. And I ran this too:. ```. $ cat /etc/os-release. NAME=""AlmaLinux"". VERSION=""9.3 (Shamrock Pampas Cat)"". ID=""almalinux"". ID_LIKE=""rhel centos fedora"". VERSION_ID=""9.3"". PLATFORM_ID=""platform:el9"". PRETTY_NAME=""AlmaLinux 9.3 (Shamrock Pampas Cat)"". ANSI_COLOR=""0;34"". LOGO=""fedora-logo-icon"". CPE_NAME=""cpe:/o:almalinux:almalinux:9::baseos"". HOME_URL=""https://almalinux.org/"". DOCUMENTATION_URL=""https://wiki.almalinux.org/"". BUG_REPORT_URL=""https://bugs.almalinux.org/"". ALMALINUX_MANTISBT_PROJECT=""AlmaLinux-9"". ALMALINUX_MANTISBT_PROJECT_VERSION=""9.3"". REDHAT_SUPPORT_PRODUCT=""AlmaLinux"". REDHAT_SUPPORT_PRODUCT_VERSION=""9.3"". ```. ## Install Singularity. I don't have Singularity on the machine yet, so:. https://docs.sylabs.io/guides/4.1/user-guide/quick_start.html#quick-installation-steps. ```bash. sudo yum update -y && \. sudo yum groupinstall -y 'Development Tools' && \. sudo yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools. ```. ```. sudo yum groupinstall -y 'Development Tools'. # Install RPM packages for dependencies. sudo yum install -y \. autoconf \. automake \. cryptsetup \. fuse3-devel \. git \. glib2-devel \. libseccomp-devel \. libtool \. runc \. squashfs-tools \. wget \. zlib-devel. ```. ```bash. sudo dnf install dnf-plugins-core. sudo dnf copr enable dctrud/squashfs-tools-ng. sudo dnf install squashfs-tools-ng. ```. ```bash. export VERSION=1.21.0 OS=linux ARCH=amd64 && \. wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz && \. sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz && \. rm go$VERSION.$OS-$ARCH.tar.gz. ```. ```bash. echo 'export PATH=/usr/local/go/bin:$PATH' >> ~/.bashrc && \. source ~/.bashrc. ```. ```bash. export VERSION=4.1.0 && \. wget https://github.com/sylabs/singularity/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:1512,deployability,updat,update,1512,"inux pichuan-test.us-west1-b.c.brain-genomics.google.com.internal 5.14.0-362.24.2.el9_3.x86_64 #1 SMP PREEMPT_DYNAMIC Sat Mar 30 14:11:54 EDT 2024 x86_64 x86_64 x86_64 GNU/Linux. ```. And I ran this too:. ```. $ cat /etc/os-release. NAME=""AlmaLinux"". VERSION=""9.3 (Shamrock Pampas Cat)"". ID=""almalinux"". ID_LIKE=""rhel centos fedora"". VERSION_ID=""9.3"". PLATFORM_ID=""platform:el9"". PRETTY_NAME=""AlmaLinux 9.3 (Shamrock Pampas Cat)"". ANSI_COLOR=""0;34"". LOGO=""fedora-logo-icon"". CPE_NAME=""cpe:/o:almalinux:almalinux:9::baseos"". HOME_URL=""https://almalinux.org/"". DOCUMENTATION_URL=""https://wiki.almalinux.org/"". BUG_REPORT_URL=""https://bugs.almalinux.org/"". ALMALINUX_MANTISBT_PROJECT=""AlmaLinux-9"". ALMALINUX_MANTISBT_PROJECT_VERSION=""9.3"". REDHAT_SUPPORT_PRODUCT=""AlmaLinux"". REDHAT_SUPPORT_PRODUCT_VERSION=""9.3"". ```. ## Install Singularity. I don't have Singularity on the machine yet, so:. https://docs.sylabs.io/guides/4.1/user-guide/quick_start.html#quick-installation-steps. ```bash. sudo yum update -y && \. sudo yum groupinstall -y 'Development Tools' && \. sudo yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools. ```. ```. sudo yum groupinstall -y 'Development Tools'. # Install RPM packages for dependencies. sudo yum install -y \. autoconf \. automake \. cryptsetup \. fuse3-devel \. git \. glib2-devel \. libseccomp-devel \. libtool \. runc \. squashfs-tools \. wget \. zlib-devel. ```. ```bash. sudo dnf install dnf-plugins-core. sudo dnf copr enable dctrud/squashfs-tools-ng. sudo dnf install squashfs-tools-ng. ```. ```bash. export VERSION=1.21.0 OS=linux ARCH=amd64 && \. wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz && \. sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz && \. rm go$VERSION.$OS-$ARCH.tar.gz. ```. ```bash. echo 'export PATH=/usr/local/go/bin:$PATH' >> ~/.bashrc && \. source ~/.bashrc. ```. ```bash. export VERSION=4.1.0 && \. wget https://github.com/sylabs/singularity/releases/download/v${VERSION}/si",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:1588,deployability,instal,install,1588,"24.2.el9_3.x86_64 #1 SMP PREEMPT_DYNAMIC Sat Mar 30 14:11:54 EDT 2024 x86_64 x86_64 x86_64 GNU/Linux. ```. And I ran this too:. ```. $ cat /etc/os-release. NAME=""AlmaLinux"". VERSION=""9.3 (Shamrock Pampas Cat)"". ID=""almalinux"". ID_LIKE=""rhel centos fedora"". VERSION_ID=""9.3"". PLATFORM_ID=""platform:el9"". PRETTY_NAME=""AlmaLinux 9.3 (Shamrock Pampas Cat)"". ANSI_COLOR=""0;34"". LOGO=""fedora-logo-icon"". CPE_NAME=""cpe:/o:almalinux:almalinux:9::baseos"". HOME_URL=""https://almalinux.org/"". DOCUMENTATION_URL=""https://wiki.almalinux.org/"". BUG_REPORT_URL=""https://bugs.almalinux.org/"". ALMALINUX_MANTISBT_PROJECT=""AlmaLinux-9"". ALMALINUX_MANTISBT_PROJECT_VERSION=""9.3"". REDHAT_SUPPORT_PRODUCT=""AlmaLinux"". REDHAT_SUPPORT_PRODUCT_VERSION=""9.3"". ```. ## Install Singularity. I don't have Singularity on the machine yet, so:. https://docs.sylabs.io/guides/4.1/user-guide/quick_start.html#quick-installation-steps. ```bash. sudo yum update -y && \. sudo yum groupinstall -y 'Development Tools' && \. sudo yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools. ```. ```. sudo yum groupinstall -y 'Development Tools'. # Install RPM packages for dependencies. sudo yum install -y \. autoconf \. automake \. cryptsetup \. fuse3-devel \. git \. glib2-devel \. libseccomp-devel \. libtool \. runc \. squashfs-tools \. wget \. zlib-devel. ```. ```bash. sudo dnf install dnf-plugins-core. sudo dnf copr enable dctrud/squashfs-tools-ng. sudo dnf install squashfs-tools-ng. ```. ```bash. export VERSION=1.21.0 OS=linux ARCH=amd64 && \. wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz && \. sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz && \. rm go$VERSION.$OS-$ARCH.tar.gz. ```. ```bash. echo 'export PATH=/usr/local/go/bin:$PATH' >> ~/.bashrc && \. source ~/.bashrc. ```. ```bash. export VERSION=4.1.0 && \. wget https://github.com/sylabs/singularity/releases/download/v${VERSION}/singularity-ce-${VERSION}.tar.gz && \. tar -xzf singularity-ce-${VERSION}.tar.g",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:1738,deployability,Instal,Install,1738,"ease. NAME=""AlmaLinux"". VERSION=""9.3 (Shamrock Pampas Cat)"". ID=""almalinux"". ID_LIKE=""rhel centos fedora"". VERSION_ID=""9.3"". PLATFORM_ID=""platform:el9"". PRETTY_NAME=""AlmaLinux 9.3 (Shamrock Pampas Cat)"". ANSI_COLOR=""0;34"". LOGO=""fedora-logo-icon"". CPE_NAME=""cpe:/o:almalinux:almalinux:9::baseos"". HOME_URL=""https://almalinux.org/"". DOCUMENTATION_URL=""https://wiki.almalinux.org/"". BUG_REPORT_URL=""https://bugs.almalinux.org/"". ALMALINUX_MANTISBT_PROJECT=""AlmaLinux-9"". ALMALINUX_MANTISBT_PROJECT_VERSION=""9.3"". REDHAT_SUPPORT_PRODUCT=""AlmaLinux"". REDHAT_SUPPORT_PRODUCT_VERSION=""9.3"". ```. ## Install Singularity. I don't have Singularity on the machine yet, so:. https://docs.sylabs.io/guides/4.1/user-guide/quick_start.html#quick-installation-steps. ```bash. sudo yum update -y && \. sudo yum groupinstall -y 'Development Tools' && \. sudo yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools. ```. ```. sudo yum groupinstall -y 'Development Tools'. # Install RPM packages for dependencies. sudo yum install -y \. autoconf \. automake \. cryptsetup \. fuse3-devel \. git \. glib2-devel \. libseccomp-devel \. libtool \. runc \. squashfs-tools \. wget \. zlib-devel. ```. ```bash. sudo dnf install dnf-plugins-core. sudo dnf copr enable dctrud/squashfs-tools-ng. sudo dnf install squashfs-tools-ng. ```. ```bash. export VERSION=1.21.0 OS=linux ARCH=amd64 && \. wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz && \. sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz && \. rm go$VERSION.$OS-$ARCH.tar.gz. ```. ```bash. echo 'export PATH=/usr/local/go/bin:$PATH' >> ~/.bashrc && \. source ~/.bashrc. ```. ```bash. export VERSION=4.1.0 && \. wget https://github.com/sylabs/singularity/releases/download/v${VERSION}/singularity-ce-${VERSION}.tar.gz && \. tar -xzf singularity-ce-${VERSION}.tar.gz && \. cd singularity-ce-${VERSION}. ```. ```bash. ./mconfig && \. make -C builddir && \. sudo make -C builddir install. ```. At this point, I have s",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:1763,deployability,depend,dependencies,1763,"SION=""9.3 (Shamrock Pampas Cat)"". ID=""almalinux"". ID_LIKE=""rhel centos fedora"". VERSION_ID=""9.3"". PLATFORM_ID=""platform:el9"". PRETTY_NAME=""AlmaLinux 9.3 (Shamrock Pampas Cat)"". ANSI_COLOR=""0;34"". LOGO=""fedora-logo-icon"". CPE_NAME=""cpe:/o:almalinux:almalinux:9::baseos"". HOME_URL=""https://almalinux.org/"". DOCUMENTATION_URL=""https://wiki.almalinux.org/"". BUG_REPORT_URL=""https://bugs.almalinux.org/"". ALMALINUX_MANTISBT_PROJECT=""AlmaLinux-9"". ALMALINUX_MANTISBT_PROJECT_VERSION=""9.3"". REDHAT_SUPPORT_PRODUCT=""AlmaLinux"". REDHAT_SUPPORT_PRODUCT_VERSION=""9.3"". ```. ## Install Singularity. I don't have Singularity on the machine yet, so:. https://docs.sylabs.io/guides/4.1/user-guide/quick_start.html#quick-installation-steps. ```bash. sudo yum update -y && \. sudo yum groupinstall -y 'Development Tools' && \. sudo yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools. ```. ```. sudo yum groupinstall -y 'Development Tools'. # Install RPM packages for dependencies. sudo yum install -y \. autoconf \. automake \. cryptsetup \. fuse3-devel \. git \. glib2-devel \. libseccomp-devel \. libtool \. runc \. squashfs-tools \. wget \. zlib-devel. ```. ```bash. sudo dnf install dnf-plugins-core. sudo dnf copr enable dctrud/squashfs-tools-ng. sudo dnf install squashfs-tools-ng. ```. ```bash. export VERSION=1.21.0 OS=linux ARCH=amd64 && \. wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz && \. sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz && \. rm go$VERSION.$OS-$ARCH.tar.gz. ```. ```bash. echo 'export PATH=/usr/local/go/bin:$PATH' >> ~/.bashrc && \. source ~/.bashrc. ```. ```bash. export VERSION=4.1.0 && \. wget https://github.com/sylabs/singularity/releases/download/v${VERSION}/singularity-ce-${VERSION}.tar.gz && \. tar -xzf singularity-ce-${VERSION}.tar.gz && \. cd singularity-ce-${VERSION}. ```. ```bash. ./mconfig && \. make -C builddir && \. sudo make -C builddir install. ```. At this point, I have singularity installed. ```ba",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:1786,deployability,instal,install,1786,"ampas Cat)"". ID=""almalinux"". ID_LIKE=""rhel centos fedora"". VERSION_ID=""9.3"". PLATFORM_ID=""platform:el9"". PRETTY_NAME=""AlmaLinux 9.3 (Shamrock Pampas Cat)"". ANSI_COLOR=""0;34"". LOGO=""fedora-logo-icon"". CPE_NAME=""cpe:/o:almalinux:almalinux:9::baseos"". HOME_URL=""https://almalinux.org/"". DOCUMENTATION_URL=""https://wiki.almalinux.org/"". BUG_REPORT_URL=""https://bugs.almalinux.org/"". ALMALINUX_MANTISBT_PROJECT=""AlmaLinux-9"". ALMALINUX_MANTISBT_PROJECT_VERSION=""9.3"". REDHAT_SUPPORT_PRODUCT=""AlmaLinux"". REDHAT_SUPPORT_PRODUCT_VERSION=""9.3"". ```. ## Install Singularity. I don't have Singularity on the machine yet, so:. https://docs.sylabs.io/guides/4.1/user-guide/quick_start.html#quick-installation-steps. ```bash. sudo yum update -y && \. sudo yum groupinstall -y 'Development Tools' && \. sudo yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools. ```. ```. sudo yum groupinstall -y 'Development Tools'. # Install RPM packages for dependencies. sudo yum install -y \. autoconf \. automake \. cryptsetup \. fuse3-devel \. git \. glib2-devel \. libseccomp-devel \. libtool \. runc \. squashfs-tools \. wget \. zlib-devel. ```. ```bash. sudo dnf install dnf-plugins-core. sudo dnf copr enable dctrud/squashfs-tools-ng. sudo dnf install squashfs-tools-ng. ```. ```bash. export VERSION=1.21.0 OS=linux ARCH=amd64 && \. wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz && \. sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz && \. rm go$VERSION.$OS-$ARCH.tar.gz. ```. ```bash. echo 'export PATH=/usr/local/go/bin:$PATH' >> ~/.bashrc && \. source ~/.bashrc. ```. ```bash. export VERSION=4.1.0 && \. wget https://github.com/sylabs/singularity/releases/download/v${VERSION}/singularity-ce-${VERSION}.tar.gz && \. tar -xzf singularity-ce-${VERSION}.tar.gz && \. cd singularity-ce-${VERSION}. ```. ```bash. ./mconfig && \. make -C builddir && \. sudo make -C builddir install. ```. At this point, I have singularity installed. ```bash. $ singularity --v",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:1975,deployability,instal,install,1975,"ogo-icon"". CPE_NAME=""cpe:/o:almalinux:almalinux:9::baseos"". HOME_URL=""https://almalinux.org/"". DOCUMENTATION_URL=""https://wiki.almalinux.org/"". BUG_REPORT_URL=""https://bugs.almalinux.org/"". ALMALINUX_MANTISBT_PROJECT=""AlmaLinux-9"". ALMALINUX_MANTISBT_PROJECT_VERSION=""9.3"". REDHAT_SUPPORT_PRODUCT=""AlmaLinux"". REDHAT_SUPPORT_PRODUCT_VERSION=""9.3"". ```. ## Install Singularity. I don't have Singularity on the machine yet, so:. https://docs.sylabs.io/guides/4.1/user-guide/quick_start.html#quick-installation-steps. ```bash. sudo yum update -y && \. sudo yum groupinstall -y 'Development Tools' && \. sudo yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools. ```. ```. sudo yum groupinstall -y 'Development Tools'. # Install RPM packages for dependencies. sudo yum install -y \. autoconf \. automake \. cryptsetup \. fuse3-devel \. git \. glib2-devel \. libseccomp-devel \. libtool \. runc \. squashfs-tools \. wget \. zlib-devel. ```. ```bash. sudo dnf install dnf-plugins-core. sudo dnf copr enable dctrud/squashfs-tools-ng. sudo dnf install squashfs-tools-ng. ```. ```bash. export VERSION=1.21.0 OS=linux ARCH=amd64 && \. wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz && \. sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz && \. rm go$VERSION.$OS-$ARCH.tar.gz. ```. ```bash. echo 'export PATH=/usr/local/go/bin:$PATH' >> ~/.bashrc && \. source ~/.bashrc. ```. ```bash. export VERSION=4.1.0 && \. wget https://github.com/sylabs/singularity/releases/download/v${VERSION}/singularity-ce-${VERSION}.tar.gz && \. tar -xzf singularity-ce-${VERSION}.tar.gz && \. cd singularity-ce-${VERSION}. ```. ```bash. ./mconfig && \. make -C builddir && \. sudo make -C builddir install. ```. At this point, I have singularity installed. ```bash. $ singularity --version. singularity-ce version 4.1.0. ```. ## Get data and run DeepVariant. Now, let me try to follow similar steps:. ```bash. singularity build DeepVariant_1.6.1.sif docker://google/deepva",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:2057,deployability,instal,install,2057,"linux.org/"". DOCUMENTATION_URL=""https://wiki.almalinux.org/"". BUG_REPORT_URL=""https://bugs.almalinux.org/"". ALMALINUX_MANTISBT_PROJECT=""AlmaLinux-9"". ALMALINUX_MANTISBT_PROJECT_VERSION=""9.3"". REDHAT_SUPPORT_PRODUCT=""AlmaLinux"". REDHAT_SUPPORT_PRODUCT_VERSION=""9.3"". ```. ## Install Singularity. I don't have Singularity on the machine yet, so:. https://docs.sylabs.io/guides/4.1/user-guide/quick_start.html#quick-installation-steps. ```bash. sudo yum update -y && \. sudo yum groupinstall -y 'Development Tools' && \. sudo yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools. ```. ```. sudo yum groupinstall -y 'Development Tools'. # Install RPM packages for dependencies. sudo yum install -y \. autoconf \. automake \. cryptsetup \. fuse3-devel \. git \. glib2-devel \. libseccomp-devel \. libtool \. runc \. squashfs-tools \. wget \. zlib-devel. ```. ```bash. sudo dnf install dnf-plugins-core. sudo dnf copr enable dctrud/squashfs-tools-ng. sudo dnf install squashfs-tools-ng. ```. ```bash. export VERSION=1.21.0 OS=linux ARCH=amd64 && \. wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz && \. sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz && \. rm go$VERSION.$OS-$ARCH.tar.gz. ```. ```bash. echo 'export PATH=/usr/local/go/bin:$PATH' >> ~/.bashrc && \. source ~/.bashrc. ```. ```bash. export VERSION=4.1.0 && \. wget https://github.com/sylabs/singularity/releases/download/v${VERSION}/singularity-ce-${VERSION}.tar.gz && \. tar -xzf singularity-ce-${VERSION}.tar.gz && \. cd singularity-ce-${VERSION}. ```. ```bash. ./mconfig && \. make -C builddir && \. sudo make -C builddir install. ```. At this point, I have singularity installed. ```bash. $ singularity --version. singularity-ce version 4.1.0. ```. ## Get data and run DeepVariant. Now, let me try to follow similar steps:. ```bash. singularity build DeepVariant_1.6.1.sif docker://google/deepvariant:1.6.1. ```. From here, I used https://github.com/google/deepvariant/blob/r1.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:2105,deployability,VERSION,VERSION,2105,"alinux.org/"". BUG_REPORT_URL=""https://bugs.almalinux.org/"". ALMALINUX_MANTISBT_PROJECT=""AlmaLinux-9"". ALMALINUX_MANTISBT_PROJECT_VERSION=""9.3"". REDHAT_SUPPORT_PRODUCT=""AlmaLinux"". REDHAT_SUPPORT_PRODUCT_VERSION=""9.3"". ```. ## Install Singularity. I don't have Singularity on the machine yet, so:. https://docs.sylabs.io/guides/4.1/user-guide/quick_start.html#quick-installation-steps. ```bash. sudo yum update -y && \. sudo yum groupinstall -y 'Development Tools' && \. sudo yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools. ```. ```. sudo yum groupinstall -y 'Development Tools'. # Install RPM packages for dependencies. sudo yum install -y \. autoconf \. automake \. cryptsetup \. fuse3-devel \. git \. glib2-devel \. libseccomp-devel \. libtool \. runc \. squashfs-tools \. wget \. zlib-devel. ```. ```bash. sudo dnf install dnf-plugins-core. sudo dnf copr enable dctrud/squashfs-tools-ng. sudo dnf install squashfs-tools-ng. ```. ```bash. export VERSION=1.21.0 OS=linux ARCH=amd64 && \. wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz && \. sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz && \. rm go$VERSION.$OS-$ARCH.tar.gz. ```. ```bash. echo 'export PATH=/usr/local/go/bin:$PATH' >> ~/.bashrc && \. source ~/.bashrc. ```. ```bash. export VERSION=4.1.0 && \. wget https://github.com/sylabs/singularity/releases/download/v${VERSION}/singularity-ce-${VERSION}.tar.gz && \. tar -xzf singularity-ce-${VERSION}.tar.gz && \. cd singularity-ce-${VERSION}. ```. ```bash. ./mconfig && \. make -C builddir && \. sudo make -C builddir install. ```. At this point, I have singularity installed. ```bash. $ singularity --version. singularity-ce version 4.1.0. ```. ## Get data and run DeepVariant. Now, let me try to follow similar steps:. ```bash. singularity build DeepVariant_1.6.1.sif docker://google/deepvariant:1.6.1. ```. From here, I used https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-pacbio-model-case-study.md ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:2179,deployability,VERSION,VERSION,2179,"ISBT_PROJECT=""AlmaLinux-9"". ALMALINUX_MANTISBT_PROJECT_VERSION=""9.3"". REDHAT_SUPPORT_PRODUCT=""AlmaLinux"". REDHAT_SUPPORT_PRODUCT_VERSION=""9.3"". ```. ## Install Singularity. I don't have Singularity on the machine yet, so:. https://docs.sylabs.io/guides/4.1/user-guide/quick_start.html#quick-installation-steps. ```bash. sudo yum update -y && \. sudo yum groupinstall -y 'Development Tools' && \. sudo yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools. ```. ```. sudo yum groupinstall -y 'Development Tools'. # Install RPM packages for dependencies. sudo yum install -y \. autoconf \. automake \. cryptsetup \. fuse3-devel \. git \. glib2-devel \. libseccomp-devel \. libtool \. runc \. squashfs-tools \. wget \. zlib-devel. ```. ```bash. sudo dnf install dnf-plugins-core. sudo dnf copr enable dctrud/squashfs-tools-ng. sudo dnf install squashfs-tools-ng. ```. ```bash. export VERSION=1.21.0 OS=linux ARCH=amd64 && \. wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz && \. sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz && \. rm go$VERSION.$OS-$ARCH.tar.gz. ```. ```bash. echo 'export PATH=/usr/local/go/bin:$PATH' >> ~/.bashrc && \. source ~/.bashrc. ```. ```bash. export VERSION=4.1.0 && \. wget https://github.com/sylabs/singularity/releases/download/v${VERSION}/singularity-ce-${VERSION}.tar.gz && \. tar -xzf singularity-ce-${VERSION}.tar.gz && \. cd singularity-ce-${VERSION}. ```. ```bash. ./mconfig && \. make -C builddir && \. sudo make -C builddir install. ```. At this point, I have singularity installed. ```bash. $ singularity --version. singularity-ce version 4.1.0. ```. ## Get data and run DeepVariant. Now, let me try to follow similar steps:. ```bash. singularity build DeepVariant_1.6.1.sif docker://google/deepvariant:1.6.1. ```. From here, I used https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-pacbio-model-case-study.md to test. Download data:. ```bash. mkdir -p reference. FTPDIR=ftp://ftp.ncb",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:2242,deployability,VERSION,VERSION,2242,"""9.3"". REDHAT_SUPPORT_PRODUCT=""AlmaLinux"". REDHAT_SUPPORT_PRODUCT_VERSION=""9.3"". ```. ## Install Singularity. I don't have Singularity on the machine yet, so:. https://docs.sylabs.io/guides/4.1/user-guide/quick_start.html#quick-installation-steps. ```bash. sudo yum update -y && \. sudo yum groupinstall -y 'Development Tools' && \. sudo yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools. ```. ```. sudo yum groupinstall -y 'Development Tools'. # Install RPM packages for dependencies. sudo yum install -y \. autoconf \. automake \. cryptsetup \. fuse3-devel \. git \. glib2-devel \. libseccomp-devel \. libtool \. runc \. squashfs-tools \. wget \. zlib-devel. ```. ```bash. sudo dnf install dnf-plugins-core. sudo dnf copr enable dctrud/squashfs-tools-ng. sudo dnf install squashfs-tools-ng. ```. ```bash. export VERSION=1.21.0 OS=linux ARCH=amd64 && \. wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz && \. sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz && \. rm go$VERSION.$OS-$ARCH.tar.gz. ```. ```bash. echo 'export PATH=/usr/local/go/bin:$PATH' >> ~/.bashrc && \. source ~/.bashrc. ```. ```bash. export VERSION=4.1.0 && \. wget https://github.com/sylabs/singularity/releases/download/v${VERSION}/singularity-ce-${VERSION}.tar.gz && \. tar -xzf singularity-ce-${VERSION}.tar.gz && \. cd singularity-ce-${VERSION}. ```. ```bash. ./mconfig && \. make -C builddir && \. sudo make -C builddir install. ```. At this point, I have singularity installed. ```bash. $ singularity --version. singularity-ce version 4.1.0. ```. ## Get data and run DeepVariant. Now, let me try to follow similar steps:. ```bash. singularity build DeepVariant_1.6.1.sif docker://google/deepvariant:1.6.1. ```. From here, I used https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-pacbio-model-case-study.md to test. Download data:. ```bash. mkdir -p reference. FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:2279,deployability,VERSION,VERSION,2279,"nux"". REDHAT_SUPPORT_PRODUCT_VERSION=""9.3"". ```. ## Install Singularity. I don't have Singularity on the machine yet, so:. https://docs.sylabs.io/guides/4.1/user-guide/quick_start.html#quick-installation-steps. ```bash. sudo yum update -y && \. sudo yum groupinstall -y 'Development Tools' && \. sudo yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools. ```. ```. sudo yum groupinstall -y 'Development Tools'. # Install RPM packages for dependencies. sudo yum install -y \. autoconf \. automake \. cryptsetup \. fuse3-devel \. git \. glib2-devel \. libseccomp-devel \. libtool \. runc \. squashfs-tools \. wget \. zlib-devel. ```. ```bash. sudo dnf install dnf-plugins-core. sudo dnf copr enable dctrud/squashfs-tools-ng. sudo dnf install squashfs-tools-ng. ```. ```bash. export VERSION=1.21.0 OS=linux ARCH=amd64 && \. wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz && \. sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz && \. rm go$VERSION.$OS-$ARCH.tar.gz. ```. ```bash. echo 'export PATH=/usr/local/go/bin:$PATH' >> ~/.bashrc && \. source ~/.bashrc. ```. ```bash. export VERSION=4.1.0 && \. wget https://github.com/sylabs/singularity/releases/download/v${VERSION}/singularity-ce-${VERSION}.tar.gz && \. tar -xzf singularity-ce-${VERSION}.tar.gz && \. cd singularity-ce-${VERSION}. ```. ```bash. ./mconfig && \. make -C builddir && \. sudo make -C builddir install. ```. At this point, I have singularity installed. ```bash. $ singularity --version. singularity-ce version 4.1.0. ```. ## Get data and run DeepVariant. Now, let me try to follow similar steps:. ```bash. singularity build DeepVariant_1.6.1.sif docker://google/deepvariant:1.6.1. ```. From here, I used https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-pacbio-model-case-study.md to test. Download data:. ```bash. mkdir -p reference. FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:2420,deployability,VERSION,VERSION,2420,"s.io/guides/4.1/user-guide/quick_start.html#quick-installation-steps. ```bash. sudo yum update -y && \. sudo yum groupinstall -y 'Development Tools' && \. sudo yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools. ```. ```. sudo yum groupinstall -y 'Development Tools'. # Install RPM packages for dependencies. sudo yum install -y \. autoconf \. automake \. cryptsetup \. fuse3-devel \. git \. glib2-devel \. libseccomp-devel \. libtool \. runc \. squashfs-tools \. wget \. zlib-devel. ```. ```bash. sudo dnf install dnf-plugins-core. sudo dnf copr enable dctrud/squashfs-tools-ng. sudo dnf install squashfs-tools-ng. ```. ```bash. export VERSION=1.21.0 OS=linux ARCH=amd64 && \. wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz && \. sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz && \. rm go$VERSION.$OS-$ARCH.tar.gz. ```. ```bash. echo 'export PATH=/usr/local/go/bin:$PATH' >> ~/.bashrc && \. source ~/.bashrc. ```. ```bash. export VERSION=4.1.0 && \. wget https://github.com/sylabs/singularity/releases/download/v${VERSION}/singularity-ce-${VERSION}.tar.gz && \. tar -xzf singularity-ce-${VERSION}.tar.gz && \. cd singularity-ce-${VERSION}. ```. ```bash. ./mconfig && \. make -C builddir && \. sudo make -C builddir install. ```. At this point, I have singularity installed. ```bash. $ singularity --version. singularity-ce version 4.1.0. ```. ## Get data and run DeepVariant. Now, let me try to follow similar steps:. ```bash. singularity build DeepVariant_1.6.1.sif docker://google/deepvariant:1.6.1. ```. From here, I used https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-pacbio-model-case-study.md to test. Download data:. ```bash. mkdir -p reference. FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta. curl ${FTPDIR}/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:2483,deployability,releas,releases,2483,"steps. ```bash. sudo yum update -y && \. sudo yum groupinstall -y 'Development Tools' && \. sudo yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools. ```. ```. sudo yum groupinstall -y 'Development Tools'. # Install RPM packages for dependencies. sudo yum install -y \. autoconf \. automake \. cryptsetup \. fuse3-devel \. git \. glib2-devel \. libseccomp-devel \. libtool \. runc \. squashfs-tools \. wget \. zlib-devel. ```. ```bash. sudo dnf install dnf-plugins-core. sudo dnf copr enable dctrud/squashfs-tools-ng. sudo dnf install squashfs-tools-ng. ```. ```bash. export VERSION=1.21.0 OS=linux ARCH=amd64 && \. wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz && \. sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz && \. rm go$VERSION.$OS-$ARCH.tar.gz. ```. ```bash. echo 'export PATH=/usr/local/go/bin:$PATH' >> ~/.bashrc && \. source ~/.bashrc. ```. ```bash. export VERSION=4.1.0 && \. wget https://github.com/sylabs/singularity/releases/download/v${VERSION}/singularity-ce-${VERSION}.tar.gz && \. tar -xzf singularity-ce-${VERSION}.tar.gz && \. cd singularity-ce-${VERSION}. ```. ```bash. ./mconfig && \. make -C builddir && \. sudo make -C builddir install. ```. At this point, I have singularity installed. ```bash. $ singularity --version. singularity-ce version 4.1.0. ```. ## Get data and run DeepVariant. Now, let me try to follow similar steps:. ```bash. singularity build DeepVariant_1.6.1.sif docker://google/deepvariant:1.6.1. ```. From here, I used https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-pacbio-model-case-study.md to test. Download data:. ```bash. mkdir -p reference. FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:2504,deployability,VERSION,VERSION,2504,"yum update -y && \. sudo yum groupinstall -y 'Development Tools' && \. sudo yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools. ```. ```. sudo yum groupinstall -y 'Development Tools'. # Install RPM packages for dependencies. sudo yum install -y \. autoconf \. automake \. cryptsetup \. fuse3-devel \. git \. glib2-devel \. libseccomp-devel \. libtool \. runc \. squashfs-tools \. wget \. zlib-devel. ```. ```bash. sudo dnf install dnf-plugins-core. sudo dnf copr enable dctrud/squashfs-tools-ng. sudo dnf install squashfs-tools-ng. ```. ```bash. export VERSION=1.21.0 OS=linux ARCH=amd64 && \. wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz && \. sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz && \. rm go$VERSION.$OS-$ARCH.tar.gz. ```. ```bash. echo 'export PATH=/usr/local/go/bin:$PATH' >> ~/.bashrc && \. source ~/.bashrc. ```. ```bash. export VERSION=4.1.0 && \. wget https://github.com/sylabs/singularity/releases/download/v${VERSION}/singularity-ce-${VERSION}.tar.gz && \. tar -xzf singularity-ce-${VERSION}.tar.gz && \. cd singularity-ce-${VERSION}. ```. ```bash. ./mconfig && \. make -C builddir && \. sudo make -C builddir install. ```. At this point, I have singularity installed. ```bash. $ singularity --version. singularity-ce version 4.1.0. ```. ## Get data and run DeepVariant. Now, let me try to follow similar steps:. ```bash. singularity build DeepVariant_1.6.1.sif docker://google/deepvariant:1.6.1. ```. From here, I used https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-pacbio-model-case-study.md to test. Download data:. ```bash. mkdir -p reference. FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analys",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:2530,deployability,VERSION,VERSION,2530,"um groupinstall -y 'Development Tools' && \. sudo yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools. ```. ```. sudo yum groupinstall -y 'Development Tools'. # Install RPM packages for dependencies. sudo yum install -y \. autoconf \. automake \. cryptsetup \. fuse3-devel \. git \. glib2-devel \. libseccomp-devel \. libtool \. runc \. squashfs-tools \. wget \. zlib-devel. ```. ```bash. sudo dnf install dnf-plugins-core. sudo dnf copr enable dctrud/squashfs-tools-ng. sudo dnf install squashfs-tools-ng. ```. ```bash. export VERSION=1.21.0 OS=linux ARCH=amd64 && \. wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz && \. sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz && \. rm go$VERSION.$OS-$ARCH.tar.gz. ```. ```bash. echo 'export PATH=/usr/local/go/bin:$PATH' >> ~/.bashrc && \. source ~/.bashrc. ```. ```bash. export VERSION=4.1.0 && \. wget https://github.com/sylabs/singularity/releases/download/v${VERSION}/singularity-ce-${VERSION}.tar.gz && \. tar -xzf singularity-ce-${VERSION}.tar.gz && \. cd singularity-ce-${VERSION}. ```. ```bash. ./mconfig && \. make -C builddir && \. sudo make -C builddir install. ```. At this point, I have singularity installed. ```bash. $ singularity --version. singularity-ce version 4.1.0. ```. ## Get data and run DeepVariant. Now, let me try to follow similar steps:. ```bash. singularity build DeepVariant_1.6.1.sif docker://google/deepvariant:1.6.1. ```. From here, I used https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-pacbio-model-case-study.md to test. Download data:. ```bash. mkdir -p reference. FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai. ```. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:2578,deployability,VERSION,VERSION,2578,"o yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools. ```. ```. sudo yum groupinstall -y 'Development Tools'. # Install RPM packages for dependencies. sudo yum install -y \. autoconf \. automake \. cryptsetup \. fuse3-devel \. git \. glib2-devel \. libseccomp-devel \. libtool \. runc \. squashfs-tools \. wget \. zlib-devel. ```. ```bash. sudo dnf install dnf-plugins-core. sudo dnf copr enable dctrud/squashfs-tools-ng. sudo dnf install squashfs-tools-ng. ```. ```bash. export VERSION=1.21.0 OS=linux ARCH=amd64 && \. wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz && \. sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz && \. rm go$VERSION.$OS-$ARCH.tar.gz. ```. ```bash. echo 'export PATH=/usr/local/go/bin:$PATH' >> ~/.bashrc && \. source ~/.bashrc. ```. ```bash. export VERSION=4.1.0 && \. wget https://github.com/sylabs/singularity/releases/download/v${VERSION}/singularity-ce-${VERSION}.tar.gz && \. tar -xzf singularity-ce-${VERSION}.tar.gz && \. cd singularity-ce-${VERSION}. ```. ```bash. ./mconfig && \. make -C builddir && \. sudo make -C builddir install. ```. At this point, I have singularity installed. ```bash. $ singularity --version. singularity-ce version 4.1.0. ```. ## Get data and run DeepVariant. Now, let me try to follow similar steps:. ```bash. singularity build DeepVariant_1.6.1.sif docker://google/deepvariant:1.6.1. ```. From here, I used https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-pacbio-model-case-study.md to test. Download data:. ```bash. mkdir -p reference. FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai. ```. ```bash. mkdir -p input. HTTPDIR=https://downloads.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:2620,deployability,VERSION,VERSION,2620,"id-devel \. libseccomp-devel \. wget \. squashfs-tools. ```. ```. sudo yum groupinstall -y 'Development Tools'. # Install RPM packages for dependencies. sudo yum install -y \. autoconf \. automake \. cryptsetup \. fuse3-devel \. git \. glib2-devel \. libseccomp-devel \. libtool \. runc \. squashfs-tools \. wget \. zlib-devel. ```. ```bash. sudo dnf install dnf-plugins-core. sudo dnf copr enable dctrud/squashfs-tools-ng. sudo dnf install squashfs-tools-ng. ```. ```bash. export VERSION=1.21.0 OS=linux ARCH=amd64 && \. wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz && \. sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz && \. rm go$VERSION.$OS-$ARCH.tar.gz. ```. ```bash. echo 'export PATH=/usr/local/go/bin:$PATH' >> ~/.bashrc && \. source ~/.bashrc. ```. ```bash. export VERSION=4.1.0 && \. wget https://github.com/sylabs/singularity/releases/download/v${VERSION}/singularity-ce-${VERSION}.tar.gz && \. tar -xzf singularity-ce-${VERSION}.tar.gz && \. cd singularity-ce-${VERSION}. ```. ```bash. ./mconfig && \. make -C builddir && \. sudo make -C builddir install. ```. At this point, I have singularity installed. ```bash. $ singularity --version. singularity-ce version 4.1.0. ```. ## Get data and run DeepVariant. Now, let me try to follow similar steps:. ```bash. singularity build DeepVariant_1.6.1.sif docker://google/deepvariant:1.6.1. ```. From here, I used https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-pacbio-model-case-study.md to test. Download data:. ```bash. mkdir -p reference. FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai. ```. ```bash. mkdir -p input. HTTPDIR=https://downloads.pacbcloud.com/public/dataset/HG003/deepvar",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:2668,deployability,build,builddir,2668,"-tools. ```. ```. sudo yum groupinstall -y 'Development Tools'. # Install RPM packages for dependencies. sudo yum install -y \. autoconf \. automake \. cryptsetup \. fuse3-devel \. git \. glib2-devel \. libseccomp-devel \. libtool \. runc \. squashfs-tools \. wget \. zlib-devel. ```. ```bash. sudo dnf install dnf-plugins-core. sudo dnf copr enable dctrud/squashfs-tools-ng. sudo dnf install squashfs-tools-ng. ```. ```bash. export VERSION=1.21.0 OS=linux ARCH=amd64 && \. wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz && \. sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz && \. rm go$VERSION.$OS-$ARCH.tar.gz. ```. ```bash. echo 'export PATH=/usr/local/go/bin:$PATH' >> ~/.bashrc && \. source ~/.bashrc. ```. ```bash. export VERSION=4.1.0 && \. wget https://github.com/sylabs/singularity/releases/download/v${VERSION}/singularity-ce-${VERSION}.tar.gz && \. tar -xzf singularity-ce-${VERSION}.tar.gz && \. cd singularity-ce-${VERSION}. ```. ```bash. ./mconfig && \. make -C builddir && \. sudo make -C builddir install. ```. At this point, I have singularity installed. ```bash. $ singularity --version. singularity-ce version 4.1.0. ```. ## Get data and run DeepVariant. Now, let me try to follow similar steps:. ```bash. singularity build DeepVariant_1.6.1.sif docker://google/deepvariant:1.6.1. ```. From here, I used https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-pacbio-model-case-study.md to test. Download data:. ```bash. mkdir -p reference. FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai. ```. ```bash. mkdir -p input. HTTPDIR=https://downloads.pacbcloud.com/public/dataset/HG003/deepvariant-case-study. curl ${HTTPDIR}/HG003.GRCh38.ch",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:2696,deployability,build,builddir,2696,"roupinstall -y 'Development Tools'. # Install RPM packages for dependencies. sudo yum install -y \. autoconf \. automake \. cryptsetup \. fuse3-devel \. git \. glib2-devel \. libseccomp-devel \. libtool \. runc \. squashfs-tools \. wget \. zlib-devel. ```. ```bash. sudo dnf install dnf-plugins-core. sudo dnf copr enable dctrud/squashfs-tools-ng. sudo dnf install squashfs-tools-ng. ```. ```bash. export VERSION=1.21.0 OS=linux ARCH=amd64 && \. wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz && \. sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz && \. rm go$VERSION.$OS-$ARCH.tar.gz. ```. ```bash. echo 'export PATH=/usr/local/go/bin:$PATH' >> ~/.bashrc && \. source ~/.bashrc. ```. ```bash. export VERSION=4.1.0 && \. wget https://github.com/sylabs/singularity/releases/download/v${VERSION}/singularity-ce-${VERSION}.tar.gz && \. tar -xzf singularity-ce-${VERSION}.tar.gz && \. cd singularity-ce-${VERSION}. ```. ```bash. ./mconfig && \. make -C builddir && \. sudo make -C builddir install. ```. At this point, I have singularity installed. ```bash. $ singularity --version. singularity-ce version 4.1.0. ```. ## Get data and run DeepVariant. Now, let me try to follow similar steps:. ```bash. singularity build DeepVariant_1.6.1.sif docker://google/deepvariant:1.6.1. ```. From here, I used https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-pacbio-model-case-study.md to test. Download data:. ```bash. mkdir -p reference. FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai. ```. ```bash. mkdir -p input. HTTPDIR=https://downloads.pacbcloud.com/public/dataset/HG003/deepvariant-case-study. curl ${HTTPDIR}/HG003.GRCh38.chr20.pFDA_truthv2.bam > input",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:2705,deployability,instal,install,2705,"ll -y 'Development Tools'. # Install RPM packages for dependencies. sudo yum install -y \. autoconf \. automake \. cryptsetup \. fuse3-devel \. git \. glib2-devel \. libseccomp-devel \. libtool \. runc \. squashfs-tools \. wget \. zlib-devel. ```. ```bash. sudo dnf install dnf-plugins-core. sudo dnf copr enable dctrud/squashfs-tools-ng. sudo dnf install squashfs-tools-ng. ```. ```bash. export VERSION=1.21.0 OS=linux ARCH=amd64 && \. wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz && \. sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz && \. rm go$VERSION.$OS-$ARCH.tar.gz. ```. ```bash. echo 'export PATH=/usr/local/go/bin:$PATH' >> ~/.bashrc && \. source ~/.bashrc. ```. ```bash. export VERSION=4.1.0 && \. wget https://github.com/sylabs/singularity/releases/download/v${VERSION}/singularity-ce-${VERSION}.tar.gz && \. tar -xzf singularity-ce-${VERSION}.tar.gz && \. cd singularity-ce-${VERSION}. ```. ```bash. ./mconfig && \. make -C builddir && \. sudo make -C builddir install. ```. At this point, I have singularity installed. ```bash. $ singularity --version. singularity-ce version 4.1.0. ```. ## Get data and run DeepVariant. Now, let me try to follow similar steps:. ```bash. singularity build DeepVariant_1.6.1.sif docker://google/deepvariant:1.6.1. ```. From here, I used https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-pacbio-model-case-study.md to test. Download data:. ```bash. mkdir -p reference. FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai. ```. ```bash. mkdir -p input. HTTPDIR=https://downloads.pacbcloud.com/public/dataset/HG003/deepvariant-case-study. curl ${HTTPDIR}/HG003.GRCh38.chr20.pFDA_truthv2.bam > input/HG003.GR",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:2753,deployability,instal,installed,2753," for dependencies. sudo yum install -y \. autoconf \. automake \. cryptsetup \. fuse3-devel \. git \. glib2-devel \. libseccomp-devel \. libtool \. runc \. squashfs-tools \. wget \. zlib-devel. ```. ```bash. sudo dnf install dnf-plugins-core. sudo dnf copr enable dctrud/squashfs-tools-ng. sudo dnf install squashfs-tools-ng. ```. ```bash. export VERSION=1.21.0 OS=linux ARCH=amd64 && \. wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz && \. sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz && \. rm go$VERSION.$OS-$ARCH.tar.gz. ```. ```bash. echo 'export PATH=/usr/local/go/bin:$PATH' >> ~/.bashrc && \. source ~/.bashrc. ```. ```bash. export VERSION=4.1.0 && \. wget https://github.com/sylabs/singularity/releases/download/v${VERSION}/singularity-ce-${VERSION}.tar.gz && \. tar -xzf singularity-ce-${VERSION}.tar.gz && \. cd singularity-ce-${VERSION}. ```. ```bash. ./mconfig && \. make -C builddir && \. sudo make -C builddir install. ```. At this point, I have singularity installed. ```bash. $ singularity --version. singularity-ce version 4.1.0. ```. ## Get data and run DeepVariant. Now, let me try to follow similar steps:. ```bash. singularity build DeepVariant_1.6.1.sif docker://google/deepvariant:1.6.1. ```. From here, I used https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-pacbio-model-case-study.md to test. Download data:. ```bash. mkdir -p reference. FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai. ```. ```bash. mkdir -p input. HTTPDIR=https://downloads.pacbcloud.com/public/dataset/HG003/deepvariant-case-study. curl ${HTTPDIR}/HG003.GRCh38.chr20.pFDA_truthv2.bam > input/HG003.GRCh38.chr20.pFDA_truthv2.bam. curl ${HTTPDIR}/HG00",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:2789,deployability,version,version,2789," -y \. autoconf \. automake \. cryptsetup \. fuse3-devel \. git \. glib2-devel \. libseccomp-devel \. libtool \. runc \. squashfs-tools \. wget \. zlib-devel. ```. ```bash. sudo dnf install dnf-plugins-core. sudo dnf copr enable dctrud/squashfs-tools-ng. sudo dnf install squashfs-tools-ng. ```. ```bash. export VERSION=1.21.0 OS=linux ARCH=amd64 && \. wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz && \. sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz && \. rm go$VERSION.$OS-$ARCH.tar.gz. ```. ```bash. echo 'export PATH=/usr/local/go/bin:$PATH' >> ~/.bashrc && \. source ~/.bashrc. ```. ```bash. export VERSION=4.1.0 && \. wget https://github.com/sylabs/singularity/releases/download/v${VERSION}/singularity-ce-${VERSION}.tar.gz && \. tar -xzf singularity-ce-${VERSION}.tar.gz && \. cd singularity-ce-${VERSION}. ```. ```bash. ./mconfig && \. make -C builddir && \. sudo make -C builddir install. ```. At this point, I have singularity installed. ```bash. $ singularity --version. singularity-ce version 4.1.0. ```. ## Get data and run DeepVariant. Now, let me try to follow similar steps:. ```bash. singularity build DeepVariant_1.6.1.sif docker://google/deepvariant:1.6.1. ```. From here, I used https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-pacbio-model-case-study.md to test. Download data:. ```bash. mkdir -p reference. FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai. ```. ```bash. mkdir -p input. HTTPDIR=https://downloads.pacbcloud.com/public/dataset/HG003/deepvariant-case-study. curl ${HTTPDIR}/HG003.GRCh38.chr20.pFDA_truthv2.bam > input/HG003.GRCh38.chr20.pFDA_truthv2.bam. curl ${HTTPDIR}/HG003.GRCh38.chr20.pFDA_truthv2.bam.bai",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:2813,deployability,version,version,2813,"ake \. cryptsetup \. fuse3-devel \. git \. glib2-devel \. libseccomp-devel \. libtool \. runc \. squashfs-tools \. wget \. zlib-devel. ```. ```bash. sudo dnf install dnf-plugins-core. sudo dnf copr enable dctrud/squashfs-tools-ng. sudo dnf install squashfs-tools-ng. ```. ```bash. export VERSION=1.21.0 OS=linux ARCH=amd64 && \. wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz && \. sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz && \. rm go$VERSION.$OS-$ARCH.tar.gz. ```. ```bash. echo 'export PATH=/usr/local/go/bin:$PATH' >> ~/.bashrc && \. source ~/.bashrc. ```. ```bash. export VERSION=4.1.0 && \. wget https://github.com/sylabs/singularity/releases/download/v${VERSION}/singularity-ce-${VERSION}.tar.gz && \. tar -xzf singularity-ce-${VERSION}.tar.gz && \. cd singularity-ce-${VERSION}. ```. ```bash. ./mconfig && \. make -C builddir && \. sudo make -C builddir install. ```. At this point, I have singularity installed. ```bash. $ singularity --version. singularity-ce version 4.1.0. ```. ## Get data and run DeepVariant. Now, let me try to follow similar steps:. ```bash. singularity build DeepVariant_1.6.1.sif docker://google/deepvariant:1.6.1. ```. From here, I used https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-pacbio-model-case-study.md to test. Download data:. ```bash. mkdir -p reference. FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai. ```. ```bash. mkdir -p input. HTTPDIR=https://downloads.pacbcloud.com/public/dataset/HG003/deepvariant-case-study. curl ${HTTPDIR}/HG003.GRCh38.chr20.pFDA_truthv2.bam > input/HG003.GRCh38.chr20.pFDA_truthv2.bam. curl ${HTTPDIR}/HG003.GRCh38.chr20.pFDA_truthv2.bam.bai > input/HG003.GRCh38.ch",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:2929,deployability,build,build,2929,"wget \. zlib-devel. ```. ```bash. sudo dnf install dnf-plugins-core. sudo dnf copr enable dctrud/squashfs-tools-ng. sudo dnf install squashfs-tools-ng. ```. ```bash. export VERSION=1.21.0 OS=linux ARCH=amd64 && \. wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz && \. sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz && \. rm go$VERSION.$OS-$ARCH.tar.gz. ```. ```bash. echo 'export PATH=/usr/local/go/bin:$PATH' >> ~/.bashrc && \. source ~/.bashrc. ```. ```bash. export VERSION=4.1.0 && \. wget https://github.com/sylabs/singularity/releases/download/v${VERSION}/singularity-ce-${VERSION}.tar.gz && \. tar -xzf singularity-ce-${VERSION}.tar.gz && \. cd singularity-ce-${VERSION}. ```. ```bash. ./mconfig && \. make -C builddir && \. sudo make -C builddir install. ```. At this point, I have singularity installed. ```bash. $ singularity --version. singularity-ce version 4.1.0. ```. ## Get data and run DeepVariant. Now, let me try to follow similar steps:. ```bash. singularity build DeepVariant_1.6.1.sif docker://google/deepvariant:1.6.1. ```. From here, I used https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-pacbio-model-case-study.md to test. Download data:. ```bash. mkdir -p reference. FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai. ```. ```bash. mkdir -p input. HTTPDIR=https://downloads.pacbcloud.com/public/dataset/HG003/deepvariant-case-study. curl ${HTTPDIR}/HG003.GRCh38.chr20.pFDA_truthv2.bam > input/HG003.GRCh38.chr20.pFDA_truthv2.bam. curl ${HTTPDIR}/HG003.GRCh38.chr20.pFDA_truthv2.bam.bai > input/HG003.GRCh38.chr20.pFDA_truthv2.bam.bai. ```. ```bash. ulimit -u 10000. BIN_VERSION=""1.6.1"". mkdir -p deepvariant_output. ```. @Ca",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:247,energy efficiency,cloud,cloud-platform,247,"# Attempt to reproduce the issue. ## Get a machine with the same Linux version. I used GCP to get a machine to test. Hopefully this provides a similar environment:. `gcloud compute instances create ""${USER}-test"" --scopes ""compute-rw,storage-full,cloud-platform"" --image-family almalinux-9 --image-project almalinux-cloud --machine-type ""n1-standard-64"" --zone ""us-west1-b""`. I ssh'ed into the machine:. ```bash. gcloud compute ssh pichuan-test --zone ""us-west1-b"". ```. Check the Linux version:. ```. $ uname -a. Linux pichuan-test.us-west1-b.c.brain-genomics.google.com.internal 5.14.0-362.24.2.el9_3.x86_64 #1 SMP PREEMPT_DYNAMIC Sat Mar 30 14:11:54 EDT 2024 x86_64 x86_64 x86_64 GNU/Linux. ```. And I ran this too:. ```. $ cat /etc/os-release. NAME=""AlmaLinux"". VERSION=""9.3 (Shamrock Pampas Cat)"". ID=""almalinux"". ID_LIKE=""rhel centos fedora"". VERSION_ID=""9.3"". PLATFORM_ID=""platform:el9"". PRETTY_NAME=""AlmaLinux 9.3 (Shamrock Pampas Cat)"". ANSI_COLOR=""0;34"". LOGO=""fedora-logo-icon"". CPE_NAME=""cpe:/o:almalinux:almalinux:9::baseos"". HOME_URL=""https://almalinux.org/"". DOCUMENTATION_URL=""https://wiki.almalinux.org/"". BUG_REPORT_URL=""https://bugs.almalinux.org/"". ALMALINUX_MANTISBT_PROJECT=""AlmaLinux-9"". ALMALINUX_MANTISBT_PROJECT_VERSION=""9.3"". REDHAT_SUPPORT_PRODUCT=""AlmaLinux"". REDHAT_SUPPORT_PRODUCT_VERSION=""9.3"". ```. ## Install Singularity. I don't have Singularity on the machine yet, so:. https://docs.sylabs.io/guides/4.1/user-guide/quick_start.html#quick-installation-steps. ```bash. sudo yum update -y && \. sudo yum groupinstall -y 'Development Tools' && \. sudo yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools. ```. ```. sudo yum groupinstall -y 'Development Tools'. # Install RPM packages for dependencies. sudo yum install -y \. autoconf \. automake \. cryptsetup \. fuse3-devel \. git \. glib2-devel \. libseccomp-devel \. libtool \. runc \. squashfs-tools \. wget \. zlib-devel. ```. ```bash. sudo dnf install dnf-plugins-core.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:316,energy efficiency,cloud,cloud,316,"# Attempt to reproduce the issue. ## Get a machine with the same Linux version. I used GCP to get a machine to test. Hopefully this provides a similar environment:. `gcloud compute instances create ""${USER}-test"" --scopes ""compute-rw,storage-full,cloud-platform"" --image-family almalinux-9 --image-project almalinux-cloud --machine-type ""n1-standard-64"" --zone ""us-west1-b""`. I ssh'ed into the machine:. ```bash. gcloud compute ssh pichuan-test --zone ""us-west1-b"". ```. Check the Linux version:. ```. $ uname -a. Linux pichuan-test.us-west1-b.c.brain-genomics.google.com.internal 5.14.0-362.24.2.el9_3.x86_64 #1 SMP PREEMPT_DYNAMIC Sat Mar 30 14:11:54 EDT 2024 x86_64 x86_64 x86_64 GNU/Linux. ```. And I ran this too:. ```. $ cat /etc/os-release. NAME=""AlmaLinux"". VERSION=""9.3 (Shamrock Pampas Cat)"". ID=""almalinux"". ID_LIKE=""rhel centos fedora"". VERSION_ID=""9.3"". PLATFORM_ID=""platform:el9"". PRETTY_NAME=""AlmaLinux 9.3 (Shamrock Pampas Cat)"". ANSI_COLOR=""0;34"". LOGO=""fedora-logo-icon"". CPE_NAME=""cpe:/o:almalinux:almalinux:9::baseos"". HOME_URL=""https://almalinux.org/"". DOCUMENTATION_URL=""https://wiki.almalinux.org/"". BUG_REPORT_URL=""https://bugs.almalinux.org/"". ALMALINUX_MANTISBT_PROJECT=""AlmaLinux-9"". ALMALINUX_MANTISBT_PROJECT_VERSION=""9.3"". REDHAT_SUPPORT_PRODUCT=""AlmaLinux"". REDHAT_SUPPORT_PRODUCT_VERSION=""9.3"". ```. ## Install Singularity. I don't have Singularity on the machine yet, so:. https://docs.sylabs.io/guides/4.1/user-guide/quick_start.html#quick-installation-steps. ```bash. sudo yum update -y && \. sudo yum groupinstall -y 'Development Tools' && \. sudo yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools. ```. ```. sudo yum groupinstall -y 'Development Tools'. # Install RPM packages for dependencies. sudo yum install -y \. autoconf \. automake \. cryptsetup \. fuse3-devel \. git \. glib2-devel \. libseccomp-devel \. libtool \. runc \. squashfs-tools \. wget \. zlib-devel. ```. ```bash. sudo dnf install dnf-plugins-core.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:1995,energy efficiency,core,core,1995,"E=""cpe:/o:almalinux:almalinux:9::baseos"". HOME_URL=""https://almalinux.org/"". DOCUMENTATION_URL=""https://wiki.almalinux.org/"". BUG_REPORT_URL=""https://bugs.almalinux.org/"". ALMALINUX_MANTISBT_PROJECT=""AlmaLinux-9"". ALMALINUX_MANTISBT_PROJECT_VERSION=""9.3"". REDHAT_SUPPORT_PRODUCT=""AlmaLinux"". REDHAT_SUPPORT_PRODUCT_VERSION=""9.3"". ```. ## Install Singularity. I don't have Singularity on the machine yet, so:. https://docs.sylabs.io/guides/4.1/user-guide/quick_start.html#quick-installation-steps. ```bash. sudo yum update -y && \. sudo yum groupinstall -y 'Development Tools' && \. sudo yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools. ```. ```. sudo yum groupinstall -y 'Development Tools'. # Install RPM packages for dependencies. sudo yum install -y \. autoconf \. automake \. cryptsetup \. fuse3-devel \. git \. glib2-devel \. libseccomp-devel \. libtool \. runc \. squashfs-tools \. wget \. zlib-devel. ```. ```bash. sudo dnf install dnf-plugins-core. sudo dnf copr enable dctrud/squashfs-tools-ng. sudo dnf install squashfs-tools-ng. ```. ```bash. export VERSION=1.21.0 OS=linux ARCH=amd64 && \. wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz && \. sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz && \. rm go$VERSION.$OS-$ARCH.tar.gz. ```. ```bash. echo 'export PATH=/usr/local/go/bin:$PATH' >> ~/.bashrc && \. source ~/.bashrc. ```. ```bash. export VERSION=4.1.0 && \. wget https://github.com/sylabs/singularity/releases/download/v${VERSION}/singularity-ce-${VERSION}.tar.gz && \. tar -xzf singularity-ce-${VERSION}.tar.gz && \. cd singularity-ce-${VERSION}. ```. ```bash. ./mconfig && \. make -C builddir && \. sudo make -C builddir install. ```. At this point, I have singularity installed. ```bash. $ singularity --version. singularity-ce version 4.1.0. ```. ## Get data and run DeepVariant. Now, let me try to follow similar steps:. ```bash. singularity build DeepVariant_1.6.1.sif docker://google/deepvariant:1.6.1. ```. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:3089,energy efficiency,model,model-case-study,3089," export VERSION=1.21.0 OS=linux ARCH=amd64 && \. wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz && \. sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz && \. rm go$VERSION.$OS-$ARCH.tar.gz. ```. ```bash. echo 'export PATH=/usr/local/go/bin:$PATH' >> ~/.bashrc && \. source ~/.bashrc. ```. ```bash. export VERSION=4.1.0 && \. wget https://github.com/sylabs/singularity/releases/download/v${VERSION}/singularity-ce-${VERSION}.tar.gz && \. tar -xzf singularity-ce-${VERSION}.tar.gz && \. cd singularity-ce-${VERSION}. ```. ```bash. ./mconfig && \. make -C builddir && \. sudo make -C builddir install. ```. At this point, I have singularity installed. ```bash. $ singularity --version. singularity-ce version 4.1.0. ```. ## Get data and run DeepVariant. Now, let me try to follow similar steps:. ```bash. singularity build DeepVariant_1.6.1.sif docker://google/deepvariant:1.6.1. ```. From here, I used https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-pacbio-model-case-study.md to test. Download data:. ```bash. mkdir -p reference. FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai. ```. ```bash. mkdir -p input. HTTPDIR=https://downloads.pacbcloud.com/public/dataset/HG003/deepvariant-case-study. curl ${HTTPDIR}/HG003.GRCh38.chr20.pFDA_truthv2.bam > input/HG003.GRCh38.chr20.pFDA_truthv2.bam. curl ${HTTPDIR}/HG003.GRCh38.chr20.pFDA_truthv2.bam.bai > input/HG003.GRCh38.chr20.pFDA_truthv2.bam.bai. ```. ```bash. ulimit -u 10000. BIN_VERSION=""1.6.1"". mkdir -p deepvariant_output. ```. @Carl-labhub mentioned ""When I run it, I’m doing so from an interactive session with singularity exec"". I'm a bit confused by this. Maybe you mean `singularity shell`? ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:4655,energy efficiency,model,model-case-study,4655,"i.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai. ```. ```bash. mkdir -p input. HTTPDIR=https://downloads.pacbcloud.com/public/dataset/HG003/deepvariant-case-study. curl ${HTTPDIR}/HG003.GRCh38.chr20.pFDA_truthv2.bam > input/HG003.GRCh38.chr20.pFDA_truthv2.bam. curl ${HTTPDIR}/HG003.GRCh38.chr20.pFDA_truthv2.bam.bai > input/HG003.GRCh38.chr20.pFDA_truthv2.bam.bai. ```. ```bash. ulimit -u 10000. BIN_VERSION=""1.6.1"". mkdir -p deepvariant_output. ```. @Carl-labhub mentioned ""When I run it, I’m doing so from an interactive session with singularity exec"". I'm a bit confused by this. Maybe you mean `singularity shell`? So I tried:. ```bash. singularity shell --bind /usr/lib/locale/ DeepVariant_1.6.1.sif. ```. This gets into a shell mode, then I ran:. ```. Singularity> /opt/deepvariant/bin/run_deepvariant \. > --model_type PACBIO \. > --ref reference/GRCh38_no_alt_analysis_set.fasta \. > --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. > --output_vcf deepvariant_output/output.vcf.gz \. > --num_shards $(nproc) \. > --regions chr20. ```. Directly `singularity exec` with the command (just like https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-pacbio-model-case-study.md) should be fine too. My `make_examples` step completed without any issues. It took:. ```. real 9m7.540s. user 215m38.303s. sys 6m41.297s. ```. I let the whole run finish just to be sure. --> The full run also completed without any errors. @Carl-labhub, can you check what I did above, and see what might be different on your side? And, there were previous GitHub issues that might be worth reading through to see if there are any relevant clues. For example: https://github.com/google/deepvariant/issues/677.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:71,integrability,version,version,71,"# Attempt to reproduce the issue. ## Get a machine with the same Linux version. I used GCP to get a machine to test. Hopefully this provides a similar environment:. `gcloud compute instances create ""${USER}-test"" --scopes ""compute-rw,storage-full,cloud-platform"" --image-family almalinux-9 --image-project almalinux-cloud --machine-type ""n1-standard-64"" --zone ""us-west1-b""`. I ssh'ed into the machine:. ```bash. gcloud compute ssh pichuan-test --zone ""us-west1-b"". ```. Check the Linux version:. ```. $ uname -a. Linux pichuan-test.us-west1-b.c.brain-genomics.google.com.internal 5.14.0-362.24.2.el9_3.x86_64 #1 SMP PREEMPT_DYNAMIC Sat Mar 30 14:11:54 EDT 2024 x86_64 x86_64 x86_64 GNU/Linux. ```. And I ran this too:. ```. $ cat /etc/os-release. NAME=""AlmaLinux"". VERSION=""9.3 (Shamrock Pampas Cat)"". ID=""almalinux"". ID_LIKE=""rhel centos fedora"". VERSION_ID=""9.3"". PLATFORM_ID=""platform:el9"". PRETTY_NAME=""AlmaLinux 9.3 (Shamrock Pampas Cat)"". ANSI_COLOR=""0;34"". LOGO=""fedora-logo-icon"". CPE_NAME=""cpe:/o:almalinux:almalinux:9::baseos"". HOME_URL=""https://almalinux.org/"". DOCUMENTATION_URL=""https://wiki.almalinux.org/"". BUG_REPORT_URL=""https://bugs.almalinux.org/"". ALMALINUX_MANTISBT_PROJECT=""AlmaLinux-9"". ALMALINUX_MANTISBT_PROJECT_VERSION=""9.3"". REDHAT_SUPPORT_PRODUCT=""AlmaLinux"". REDHAT_SUPPORT_PRODUCT_VERSION=""9.3"". ```. ## Install Singularity. I don't have Singularity on the machine yet, so:. https://docs.sylabs.io/guides/4.1/user-guide/quick_start.html#quick-installation-steps. ```bash. sudo yum update -y && \. sudo yum groupinstall -y 'Development Tools' && \. sudo yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools. ```. ```. sudo yum groupinstall -y 'Development Tools'. # Install RPM packages for dependencies. sudo yum install -y \. autoconf \. automake \. cryptsetup \. fuse3-devel \. git \. glib2-devel \. libseccomp-devel \. libtool \. runc \. squashfs-tools \. wget \. zlib-devel. ```. ```bash. sudo dnf install dnf-plugins-core.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:487,integrability,version,version,487,"# Attempt to reproduce the issue. ## Get a machine with the same Linux version. I used GCP to get a machine to test. Hopefully this provides a similar environment:. `gcloud compute instances create ""${USER}-test"" --scopes ""compute-rw,storage-full,cloud-platform"" --image-family almalinux-9 --image-project almalinux-cloud --machine-type ""n1-standard-64"" --zone ""us-west1-b""`. I ssh'ed into the machine:. ```bash. gcloud compute ssh pichuan-test --zone ""us-west1-b"". ```. Check the Linux version:. ```. $ uname -a. Linux pichuan-test.us-west1-b.c.brain-genomics.google.com.internal 5.14.0-362.24.2.el9_3.x86_64 #1 SMP PREEMPT_DYNAMIC Sat Mar 30 14:11:54 EDT 2024 x86_64 x86_64 x86_64 GNU/Linux. ```. And I ran this too:. ```. $ cat /etc/os-release. NAME=""AlmaLinux"". VERSION=""9.3 (Shamrock Pampas Cat)"". ID=""almalinux"". ID_LIKE=""rhel centos fedora"". VERSION_ID=""9.3"". PLATFORM_ID=""platform:el9"". PRETTY_NAME=""AlmaLinux 9.3 (Shamrock Pampas Cat)"". ANSI_COLOR=""0;34"". LOGO=""fedora-logo-icon"". CPE_NAME=""cpe:/o:almalinux:almalinux:9::baseos"". HOME_URL=""https://almalinux.org/"". DOCUMENTATION_URL=""https://wiki.almalinux.org/"". BUG_REPORT_URL=""https://bugs.almalinux.org/"". ALMALINUX_MANTISBT_PROJECT=""AlmaLinux-9"". ALMALINUX_MANTISBT_PROJECT_VERSION=""9.3"". REDHAT_SUPPORT_PRODUCT=""AlmaLinux"". REDHAT_SUPPORT_PRODUCT_VERSION=""9.3"". ```. ## Install Singularity. I don't have Singularity on the machine yet, so:. https://docs.sylabs.io/guides/4.1/user-guide/quick_start.html#quick-installation-steps. ```bash. sudo yum update -y && \. sudo yum groupinstall -y 'Development Tools' && \. sudo yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools. ```. ```. sudo yum groupinstall -y 'Development Tools'. # Install RPM packages for dependencies. sudo yum install -y \. autoconf \. automake \. cryptsetup \. fuse3-devel \. git \. glib2-devel \. libseccomp-devel \. libtool \. runc \. squashfs-tools \. wget \. zlib-devel. ```. ```bash. sudo dnf install dnf-plugins-core.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:766,integrability,VERSION,VERSION,766,"# Attempt to reproduce the issue. ## Get a machine with the same Linux version. I used GCP to get a machine to test. Hopefully this provides a similar environment:. `gcloud compute instances create ""${USER}-test"" --scopes ""compute-rw,storage-full,cloud-platform"" --image-family almalinux-9 --image-project almalinux-cloud --machine-type ""n1-standard-64"" --zone ""us-west1-b""`. I ssh'ed into the machine:. ```bash. gcloud compute ssh pichuan-test --zone ""us-west1-b"". ```. Check the Linux version:. ```. $ uname -a. Linux pichuan-test.us-west1-b.c.brain-genomics.google.com.internal 5.14.0-362.24.2.el9_3.x86_64 #1 SMP PREEMPT_DYNAMIC Sat Mar 30 14:11:54 EDT 2024 x86_64 x86_64 x86_64 GNU/Linux. ```. And I ran this too:. ```. $ cat /etc/os-release. NAME=""AlmaLinux"". VERSION=""9.3 (Shamrock Pampas Cat)"". ID=""almalinux"". ID_LIKE=""rhel centos fedora"". VERSION_ID=""9.3"". PLATFORM_ID=""platform:el9"". PRETTY_NAME=""AlmaLinux 9.3 (Shamrock Pampas Cat)"". ANSI_COLOR=""0;34"". LOGO=""fedora-logo-icon"". CPE_NAME=""cpe:/o:almalinux:almalinux:9::baseos"". HOME_URL=""https://almalinux.org/"". DOCUMENTATION_URL=""https://wiki.almalinux.org/"". BUG_REPORT_URL=""https://bugs.almalinux.org/"". ALMALINUX_MANTISBT_PROJECT=""AlmaLinux-9"". ALMALINUX_MANTISBT_PROJECT_VERSION=""9.3"". REDHAT_SUPPORT_PRODUCT=""AlmaLinux"". REDHAT_SUPPORT_PRODUCT_VERSION=""9.3"". ```. ## Install Singularity. I don't have Singularity on the machine yet, so:. https://docs.sylabs.io/guides/4.1/user-guide/quick_start.html#quick-installation-steps. ```bash. sudo yum update -y && \. sudo yum groupinstall -y 'Development Tools' && \. sudo yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools. ```. ```. sudo yum groupinstall -y 'Development Tools'. # Install RPM packages for dependencies. sudo yum install -y \. autoconf \. automake \. cryptsetup \. fuse3-devel \. git \. glib2-devel \. libseccomp-devel \. libtool \. runc \. squashfs-tools \. wget \. zlib-devel. ```. ```bash. sudo dnf install dnf-plugins-core.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:1763,integrability,depend,dependencies,1763,"SION=""9.3 (Shamrock Pampas Cat)"". ID=""almalinux"". ID_LIKE=""rhel centos fedora"". VERSION_ID=""9.3"". PLATFORM_ID=""platform:el9"". PRETTY_NAME=""AlmaLinux 9.3 (Shamrock Pampas Cat)"". ANSI_COLOR=""0;34"". LOGO=""fedora-logo-icon"". CPE_NAME=""cpe:/o:almalinux:almalinux:9::baseos"". HOME_URL=""https://almalinux.org/"". DOCUMENTATION_URL=""https://wiki.almalinux.org/"". BUG_REPORT_URL=""https://bugs.almalinux.org/"". ALMALINUX_MANTISBT_PROJECT=""AlmaLinux-9"". ALMALINUX_MANTISBT_PROJECT_VERSION=""9.3"". REDHAT_SUPPORT_PRODUCT=""AlmaLinux"". REDHAT_SUPPORT_PRODUCT_VERSION=""9.3"". ```. ## Install Singularity. I don't have Singularity on the machine yet, so:. https://docs.sylabs.io/guides/4.1/user-guide/quick_start.html#quick-installation-steps. ```bash. sudo yum update -y && \. sudo yum groupinstall -y 'Development Tools' && \. sudo yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools. ```. ```. sudo yum groupinstall -y 'Development Tools'. # Install RPM packages for dependencies. sudo yum install -y \. autoconf \. automake \. cryptsetup \. fuse3-devel \. git \. glib2-devel \. libseccomp-devel \. libtool \. runc \. squashfs-tools \. wget \. zlib-devel. ```. ```bash. sudo dnf install dnf-plugins-core. sudo dnf copr enable dctrud/squashfs-tools-ng. sudo dnf install squashfs-tools-ng. ```. ```bash. export VERSION=1.21.0 OS=linux ARCH=amd64 && \. wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz && \. sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz && \. rm go$VERSION.$OS-$ARCH.tar.gz. ```. ```bash. echo 'export PATH=/usr/local/go/bin:$PATH' >> ~/.bashrc && \. source ~/.bashrc. ```. ```bash. export VERSION=4.1.0 && \. wget https://github.com/sylabs/singularity/releases/download/v${VERSION}/singularity-ce-${VERSION}.tar.gz && \. tar -xzf singularity-ce-${VERSION}.tar.gz && \. cd singularity-ce-${VERSION}. ```. ```bash. ./mconfig && \. make -C builddir && \. sudo make -C builddir install. ```. At this point, I have singularity installed. ```ba",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:2105,integrability,VERSION,VERSION,2105,"alinux.org/"". BUG_REPORT_URL=""https://bugs.almalinux.org/"". ALMALINUX_MANTISBT_PROJECT=""AlmaLinux-9"". ALMALINUX_MANTISBT_PROJECT_VERSION=""9.3"". REDHAT_SUPPORT_PRODUCT=""AlmaLinux"". REDHAT_SUPPORT_PRODUCT_VERSION=""9.3"". ```. ## Install Singularity. I don't have Singularity on the machine yet, so:. https://docs.sylabs.io/guides/4.1/user-guide/quick_start.html#quick-installation-steps. ```bash. sudo yum update -y && \. sudo yum groupinstall -y 'Development Tools' && \. sudo yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools. ```. ```. sudo yum groupinstall -y 'Development Tools'. # Install RPM packages for dependencies. sudo yum install -y \. autoconf \. automake \. cryptsetup \. fuse3-devel \. git \. glib2-devel \. libseccomp-devel \. libtool \. runc \. squashfs-tools \. wget \. zlib-devel. ```. ```bash. sudo dnf install dnf-plugins-core. sudo dnf copr enable dctrud/squashfs-tools-ng. sudo dnf install squashfs-tools-ng. ```. ```bash. export VERSION=1.21.0 OS=linux ARCH=amd64 && \. wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz && \. sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz && \. rm go$VERSION.$OS-$ARCH.tar.gz. ```. ```bash. echo 'export PATH=/usr/local/go/bin:$PATH' >> ~/.bashrc && \. source ~/.bashrc. ```. ```bash. export VERSION=4.1.0 && \. wget https://github.com/sylabs/singularity/releases/download/v${VERSION}/singularity-ce-${VERSION}.tar.gz && \. tar -xzf singularity-ce-${VERSION}.tar.gz && \. cd singularity-ce-${VERSION}. ```. ```bash. ./mconfig && \. make -C builddir && \. sudo make -C builddir install. ```. At this point, I have singularity installed. ```bash. $ singularity --version. singularity-ce version 4.1.0. ```. ## Get data and run DeepVariant. Now, let me try to follow similar steps:. ```bash. singularity build DeepVariant_1.6.1.sif docker://google/deepvariant:1.6.1. ```. From here, I used https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-pacbio-model-case-study.md ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:2179,integrability,VERSION,VERSION,2179,"ISBT_PROJECT=""AlmaLinux-9"". ALMALINUX_MANTISBT_PROJECT_VERSION=""9.3"". REDHAT_SUPPORT_PRODUCT=""AlmaLinux"". REDHAT_SUPPORT_PRODUCT_VERSION=""9.3"". ```. ## Install Singularity. I don't have Singularity on the machine yet, so:. https://docs.sylabs.io/guides/4.1/user-guide/quick_start.html#quick-installation-steps. ```bash. sudo yum update -y && \. sudo yum groupinstall -y 'Development Tools' && \. sudo yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools. ```. ```. sudo yum groupinstall -y 'Development Tools'. # Install RPM packages for dependencies. sudo yum install -y \. autoconf \. automake \. cryptsetup \. fuse3-devel \. git \. glib2-devel \. libseccomp-devel \. libtool \. runc \. squashfs-tools \. wget \. zlib-devel. ```. ```bash. sudo dnf install dnf-plugins-core. sudo dnf copr enable dctrud/squashfs-tools-ng. sudo dnf install squashfs-tools-ng. ```. ```bash. export VERSION=1.21.0 OS=linux ARCH=amd64 && \. wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz && \. sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz && \. rm go$VERSION.$OS-$ARCH.tar.gz. ```. ```bash. echo 'export PATH=/usr/local/go/bin:$PATH' >> ~/.bashrc && \. source ~/.bashrc. ```. ```bash. export VERSION=4.1.0 && \. wget https://github.com/sylabs/singularity/releases/download/v${VERSION}/singularity-ce-${VERSION}.tar.gz && \. tar -xzf singularity-ce-${VERSION}.tar.gz && \. cd singularity-ce-${VERSION}. ```. ```bash. ./mconfig && \. make -C builddir && \. sudo make -C builddir install. ```. At this point, I have singularity installed. ```bash. $ singularity --version. singularity-ce version 4.1.0. ```. ## Get data and run DeepVariant. Now, let me try to follow similar steps:. ```bash. singularity build DeepVariant_1.6.1.sif docker://google/deepvariant:1.6.1. ```. From here, I used https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-pacbio-model-case-study.md to test. Download data:. ```bash. mkdir -p reference. FTPDIR=ftp://ftp.ncb",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:2242,integrability,VERSION,VERSION,2242,"""9.3"". REDHAT_SUPPORT_PRODUCT=""AlmaLinux"". REDHAT_SUPPORT_PRODUCT_VERSION=""9.3"". ```. ## Install Singularity. I don't have Singularity on the machine yet, so:. https://docs.sylabs.io/guides/4.1/user-guide/quick_start.html#quick-installation-steps. ```bash. sudo yum update -y && \. sudo yum groupinstall -y 'Development Tools' && \. sudo yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools. ```. ```. sudo yum groupinstall -y 'Development Tools'. # Install RPM packages for dependencies. sudo yum install -y \. autoconf \. automake \. cryptsetup \. fuse3-devel \. git \. glib2-devel \. libseccomp-devel \. libtool \. runc \. squashfs-tools \. wget \. zlib-devel. ```. ```bash. sudo dnf install dnf-plugins-core. sudo dnf copr enable dctrud/squashfs-tools-ng. sudo dnf install squashfs-tools-ng. ```. ```bash. export VERSION=1.21.0 OS=linux ARCH=amd64 && \. wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz && \. sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz && \. rm go$VERSION.$OS-$ARCH.tar.gz. ```. ```bash. echo 'export PATH=/usr/local/go/bin:$PATH' >> ~/.bashrc && \. source ~/.bashrc. ```. ```bash. export VERSION=4.1.0 && \. wget https://github.com/sylabs/singularity/releases/download/v${VERSION}/singularity-ce-${VERSION}.tar.gz && \. tar -xzf singularity-ce-${VERSION}.tar.gz && \. cd singularity-ce-${VERSION}. ```. ```bash. ./mconfig && \. make -C builddir && \. sudo make -C builddir install. ```. At this point, I have singularity installed. ```bash. $ singularity --version. singularity-ce version 4.1.0. ```. ## Get data and run DeepVariant. Now, let me try to follow similar steps:. ```bash. singularity build DeepVariant_1.6.1.sif docker://google/deepvariant:1.6.1. ```. From here, I used https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-pacbio-model-case-study.md to test. Download data:. ```bash. mkdir -p reference. FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:2279,integrability,VERSION,VERSION,2279,"nux"". REDHAT_SUPPORT_PRODUCT_VERSION=""9.3"". ```. ## Install Singularity. I don't have Singularity on the machine yet, so:. https://docs.sylabs.io/guides/4.1/user-guide/quick_start.html#quick-installation-steps. ```bash. sudo yum update -y && \. sudo yum groupinstall -y 'Development Tools' && \. sudo yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools. ```. ```. sudo yum groupinstall -y 'Development Tools'. # Install RPM packages for dependencies. sudo yum install -y \. autoconf \. automake \. cryptsetup \. fuse3-devel \. git \. glib2-devel \. libseccomp-devel \. libtool \. runc \. squashfs-tools \. wget \. zlib-devel. ```. ```bash. sudo dnf install dnf-plugins-core. sudo dnf copr enable dctrud/squashfs-tools-ng. sudo dnf install squashfs-tools-ng. ```. ```bash. export VERSION=1.21.0 OS=linux ARCH=amd64 && \. wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz && \. sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz && \. rm go$VERSION.$OS-$ARCH.tar.gz. ```. ```bash. echo 'export PATH=/usr/local/go/bin:$PATH' >> ~/.bashrc && \. source ~/.bashrc. ```. ```bash. export VERSION=4.1.0 && \. wget https://github.com/sylabs/singularity/releases/download/v${VERSION}/singularity-ce-${VERSION}.tar.gz && \. tar -xzf singularity-ce-${VERSION}.tar.gz && \. cd singularity-ce-${VERSION}. ```. ```bash. ./mconfig && \. make -C builddir && \. sudo make -C builddir install. ```. At this point, I have singularity installed. ```bash. $ singularity --version. singularity-ce version 4.1.0. ```. ## Get data and run DeepVariant. Now, let me try to follow similar steps:. ```bash. singularity build DeepVariant_1.6.1.sif docker://google/deepvariant:1.6.1. ```. From here, I used https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-pacbio-model-case-study.md to test. Download data:. ```bash. mkdir -p reference. FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:2420,integrability,VERSION,VERSION,2420,"s.io/guides/4.1/user-guide/quick_start.html#quick-installation-steps. ```bash. sudo yum update -y && \. sudo yum groupinstall -y 'Development Tools' && \. sudo yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools. ```. ```. sudo yum groupinstall -y 'Development Tools'. # Install RPM packages for dependencies. sudo yum install -y \. autoconf \. automake \. cryptsetup \. fuse3-devel \. git \. glib2-devel \. libseccomp-devel \. libtool \. runc \. squashfs-tools \. wget \. zlib-devel. ```. ```bash. sudo dnf install dnf-plugins-core. sudo dnf copr enable dctrud/squashfs-tools-ng. sudo dnf install squashfs-tools-ng. ```. ```bash. export VERSION=1.21.0 OS=linux ARCH=amd64 && \. wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz && \. sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz && \. rm go$VERSION.$OS-$ARCH.tar.gz. ```. ```bash. echo 'export PATH=/usr/local/go/bin:$PATH' >> ~/.bashrc && \. source ~/.bashrc. ```. ```bash. export VERSION=4.1.0 && \. wget https://github.com/sylabs/singularity/releases/download/v${VERSION}/singularity-ce-${VERSION}.tar.gz && \. tar -xzf singularity-ce-${VERSION}.tar.gz && \. cd singularity-ce-${VERSION}. ```. ```bash. ./mconfig && \. make -C builddir && \. sudo make -C builddir install. ```. At this point, I have singularity installed. ```bash. $ singularity --version. singularity-ce version 4.1.0. ```. ## Get data and run DeepVariant. Now, let me try to follow similar steps:. ```bash. singularity build DeepVariant_1.6.1.sif docker://google/deepvariant:1.6.1. ```. From here, I used https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-pacbio-model-case-study.md to test. Download data:. ```bash. mkdir -p reference. FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta. curl ${FTPDIR}/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:2504,integrability,VERSION,VERSION,2504,"yum update -y && \. sudo yum groupinstall -y 'Development Tools' && \. sudo yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools. ```. ```. sudo yum groupinstall -y 'Development Tools'. # Install RPM packages for dependencies. sudo yum install -y \. autoconf \. automake \. cryptsetup \. fuse3-devel \. git \. glib2-devel \. libseccomp-devel \. libtool \. runc \. squashfs-tools \. wget \. zlib-devel. ```. ```bash. sudo dnf install dnf-plugins-core. sudo dnf copr enable dctrud/squashfs-tools-ng. sudo dnf install squashfs-tools-ng. ```. ```bash. export VERSION=1.21.0 OS=linux ARCH=amd64 && \. wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz && \. sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz && \. rm go$VERSION.$OS-$ARCH.tar.gz. ```. ```bash. echo 'export PATH=/usr/local/go/bin:$PATH' >> ~/.bashrc && \. source ~/.bashrc. ```. ```bash. export VERSION=4.1.0 && \. wget https://github.com/sylabs/singularity/releases/download/v${VERSION}/singularity-ce-${VERSION}.tar.gz && \. tar -xzf singularity-ce-${VERSION}.tar.gz && \. cd singularity-ce-${VERSION}. ```. ```bash. ./mconfig && \. make -C builddir && \. sudo make -C builddir install. ```. At this point, I have singularity installed. ```bash. $ singularity --version. singularity-ce version 4.1.0. ```. ## Get data and run DeepVariant. Now, let me try to follow similar steps:. ```bash. singularity build DeepVariant_1.6.1.sif docker://google/deepvariant:1.6.1. ```. From here, I used https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-pacbio-model-case-study.md to test. Download data:. ```bash. mkdir -p reference. FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analys",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:2530,integrability,VERSION,VERSION,2530,"um groupinstall -y 'Development Tools' && \. sudo yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools. ```. ```. sudo yum groupinstall -y 'Development Tools'. # Install RPM packages for dependencies. sudo yum install -y \. autoconf \. automake \. cryptsetup \. fuse3-devel \. git \. glib2-devel \. libseccomp-devel \. libtool \. runc \. squashfs-tools \. wget \. zlib-devel. ```. ```bash. sudo dnf install dnf-plugins-core. sudo dnf copr enable dctrud/squashfs-tools-ng. sudo dnf install squashfs-tools-ng. ```. ```bash. export VERSION=1.21.0 OS=linux ARCH=amd64 && \. wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz && \. sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz && \. rm go$VERSION.$OS-$ARCH.tar.gz. ```. ```bash. echo 'export PATH=/usr/local/go/bin:$PATH' >> ~/.bashrc && \. source ~/.bashrc. ```. ```bash. export VERSION=4.1.0 && \. wget https://github.com/sylabs/singularity/releases/download/v${VERSION}/singularity-ce-${VERSION}.tar.gz && \. tar -xzf singularity-ce-${VERSION}.tar.gz && \. cd singularity-ce-${VERSION}. ```. ```bash. ./mconfig && \. make -C builddir && \. sudo make -C builddir install. ```. At this point, I have singularity installed. ```bash. $ singularity --version. singularity-ce version 4.1.0. ```. ## Get data and run DeepVariant. Now, let me try to follow similar steps:. ```bash. singularity build DeepVariant_1.6.1.sif docker://google/deepvariant:1.6.1. ```. From here, I used https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-pacbio-model-case-study.md to test. Download data:. ```bash. mkdir -p reference. FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai. ```. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:2578,integrability,VERSION,VERSION,2578,"o yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools. ```. ```. sudo yum groupinstall -y 'Development Tools'. # Install RPM packages for dependencies. sudo yum install -y \. autoconf \. automake \. cryptsetup \. fuse3-devel \. git \. glib2-devel \. libseccomp-devel \. libtool \. runc \. squashfs-tools \. wget \. zlib-devel. ```. ```bash. sudo dnf install dnf-plugins-core. sudo dnf copr enable dctrud/squashfs-tools-ng. sudo dnf install squashfs-tools-ng. ```. ```bash. export VERSION=1.21.0 OS=linux ARCH=amd64 && \. wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz && \. sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz && \. rm go$VERSION.$OS-$ARCH.tar.gz. ```. ```bash. echo 'export PATH=/usr/local/go/bin:$PATH' >> ~/.bashrc && \. source ~/.bashrc. ```. ```bash. export VERSION=4.1.0 && \. wget https://github.com/sylabs/singularity/releases/download/v${VERSION}/singularity-ce-${VERSION}.tar.gz && \. tar -xzf singularity-ce-${VERSION}.tar.gz && \. cd singularity-ce-${VERSION}. ```. ```bash. ./mconfig && \. make -C builddir && \. sudo make -C builddir install. ```. At this point, I have singularity installed. ```bash. $ singularity --version. singularity-ce version 4.1.0. ```. ## Get data and run DeepVariant. Now, let me try to follow similar steps:. ```bash. singularity build DeepVariant_1.6.1.sif docker://google/deepvariant:1.6.1. ```. From here, I used https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-pacbio-model-case-study.md to test. Download data:. ```bash. mkdir -p reference. FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai. ```. ```bash. mkdir -p input. HTTPDIR=https://downloads.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:2620,integrability,VERSION,VERSION,2620,"id-devel \. libseccomp-devel \. wget \. squashfs-tools. ```. ```. sudo yum groupinstall -y 'Development Tools'. # Install RPM packages for dependencies. sudo yum install -y \. autoconf \. automake \. cryptsetup \. fuse3-devel \. git \. glib2-devel \. libseccomp-devel \. libtool \. runc \. squashfs-tools \. wget \. zlib-devel. ```. ```bash. sudo dnf install dnf-plugins-core. sudo dnf copr enable dctrud/squashfs-tools-ng. sudo dnf install squashfs-tools-ng. ```. ```bash. export VERSION=1.21.0 OS=linux ARCH=amd64 && \. wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz && \. sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz && \. rm go$VERSION.$OS-$ARCH.tar.gz. ```. ```bash. echo 'export PATH=/usr/local/go/bin:$PATH' >> ~/.bashrc && \. source ~/.bashrc. ```. ```bash. export VERSION=4.1.0 && \. wget https://github.com/sylabs/singularity/releases/download/v${VERSION}/singularity-ce-${VERSION}.tar.gz && \. tar -xzf singularity-ce-${VERSION}.tar.gz && \. cd singularity-ce-${VERSION}. ```. ```bash. ./mconfig && \. make -C builddir && \. sudo make -C builddir install. ```. At this point, I have singularity installed. ```bash. $ singularity --version. singularity-ce version 4.1.0. ```. ## Get data and run DeepVariant. Now, let me try to follow similar steps:. ```bash. singularity build DeepVariant_1.6.1.sif docker://google/deepvariant:1.6.1. ```. From here, I used https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-pacbio-model-case-study.md to test. Download data:. ```bash. mkdir -p reference. FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai. ```. ```bash. mkdir -p input. HTTPDIR=https://downloads.pacbcloud.com/public/dataset/HG003/deepvar",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:2789,integrability,version,version,2789," -y \. autoconf \. automake \. cryptsetup \. fuse3-devel \. git \. glib2-devel \. libseccomp-devel \. libtool \. runc \. squashfs-tools \. wget \. zlib-devel. ```. ```bash. sudo dnf install dnf-plugins-core. sudo dnf copr enable dctrud/squashfs-tools-ng. sudo dnf install squashfs-tools-ng. ```. ```bash. export VERSION=1.21.0 OS=linux ARCH=amd64 && \. wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz && \. sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz && \. rm go$VERSION.$OS-$ARCH.tar.gz. ```. ```bash. echo 'export PATH=/usr/local/go/bin:$PATH' >> ~/.bashrc && \. source ~/.bashrc. ```. ```bash. export VERSION=4.1.0 && \. wget https://github.com/sylabs/singularity/releases/download/v${VERSION}/singularity-ce-${VERSION}.tar.gz && \. tar -xzf singularity-ce-${VERSION}.tar.gz && \. cd singularity-ce-${VERSION}. ```. ```bash. ./mconfig && \. make -C builddir && \. sudo make -C builddir install. ```. At this point, I have singularity installed. ```bash. $ singularity --version. singularity-ce version 4.1.0. ```. ## Get data and run DeepVariant. Now, let me try to follow similar steps:. ```bash. singularity build DeepVariant_1.6.1.sif docker://google/deepvariant:1.6.1. ```. From here, I used https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-pacbio-model-case-study.md to test. Download data:. ```bash. mkdir -p reference. FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai. ```. ```bash. mkdir -p input. HTTPDIR=https://downloads.pacbcloud.com/public/dataset/HG003/deepvariant-case-study. curl ${HTTPDIR}/HG003.GRCh38.chr20.pFDA_truthv2.bam > input/HG003.GRCh38.chr20.pFDA_truthv2.bam. curl ${HTTPDIR}/HG003.GRCh38.chr20.pFDA_truthv2.bam.bai",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:2813,integrability,version,version,2813,"ake \. cryptsetup \. fuse3-devel \. git \. glib2-devel \. libseccomp-devel \. libtool \. runc \. squashfs-tools \. wget \. zlib-devel. ```. ```bash. sudo dnf install dnf-plugins-core. sudo dnf copr enable dctrud/squashfs-tools-ng. sudo dnf install squashfs-tools-ng. ```. ```bash. export VERSION=1.21.0 OS=linux ARCH=amd64 && \. wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz && \. sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz && \. rm go$VERSION.$OS-$ARCH.tar.gz. ```. ```bash. echo 'export PATH=/usr/local/go/bin:$PATH' >> ~/.bashrc && \. source ~/.bashrc. ```. ```bash. export VERSION=4.1.0 && \. wget https://github.com/sylabs/singularity/releases/download/v${VERSION}/singularity-ce-${VERSION}.tar.gz && \. tar -xzf singularity-ce-${VERSION}.tar.gz && \. cd singularity-ce-${VERSION}. ```. ```bash. ./mconfig && \. make -C builddir && \. sudo make -C builddir install. ```. At this point, I have singularity installed. ```bash. $ singularity --version. singularity-ce version 4.1.0. ```. ## Get data and run DeepVariant. Now, let me try to follow similar steps:. ```bash. singularity build DeepVariant_1.6.1.sif docker://google/deepvariant:1.6.1. ```. From here, I used https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-pacbio-model-case-study.md to test. Download data:. ```bash. mkdir -p reference. FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai. ```. ```bash. mkdir -p input. HTTPDIR=https://downloads.pacbcloud.com/public/dataset/HG003/deepvariant-case-study. curl ${HTTPDIR}/HG003.GRCh38.chr20.pFDA_truthv2.bam > input/HG003.GRCh38.chr20.pFDA_truthv2.bam. curl ${HTTPDIR}/HG003.GRCh38.chr20.pFDA_truthv2.bam.bai > input/HG003.GRCh38.ch",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:3596,integrability,pub,public,3596," cd singularity-ce-${VERSION}. ```. ```bash. ./mconfig && \. make -C builddir && \. sudo make -C builddir install. ```. At this point, I have singularity installed. ```bash. $ singularity --version. singularity-ce version 4.1.0. ```. ## Get data and run DeepVariant. Now, let me try to follow similar steps:. ```bash. singularity build DeepVariant_1.6.1.sif docker://google/deepvariant:1.6.1. ```. From here, I used https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-pacbio-model-case-study.md to test. Download data:. ```bash. mkdir -p reference. FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai. ```. ```bash. mkdir -p input. HTTPDIR=https://downloads.pacbcloud.com/public/dataset/HG003/deepvariant-case-study. curl ${HTTPDIR}/HG003.GRCh38.chr20.pFDA_truthv2.bam > input/HG003.GRCh38.chr20.pFDA_truthv2.bam. curl ${HTTPDIR}/HG003.GRCh38.chr20.pFDA_truthv2.bam.bai > input/HG003.GRCh38.chr20.pFDA_truthv2.bam.bai. ```. ```bash. ulimit -u 10000. BIN_VERSION=""1.6.1"". mkdir -p deepvariant_output. ```. @Carl-labhub mentioned ""When I run it, I’m doing so from an interactive session with singularity exec"". I'm a bit confused by this. Maybe you mean `singularity shell`? So I tried:. ```bash. singularity shell --bind /usr/lib/locale/ DeepVariant_1.6.1.sif. ```. This gets into a shell mode, then I ran:. ```. Singularity> /opt/deepvariant/bin/run_deepvariant \. > --model_type PACBIO \. > --ref reference/GRCh38_no_alt_analysis_set.fasta \. > --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. > --output_vcf deepvariant_output/output.vcf.gz \. > --num_shards $(nproc) \. > --regions chr20. ```. Directly `singularity exec` with the command (just like https://github.com",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:253,interoperability,platform,platform,253,"# Attempt to reproduce the issue. ## Get a machine with the same Linux version. I used GCP to get a machine to test. Hopefully this provides a similar environment:. `gcloud compute instances create ""${USER}-test"" --scopes ""compute-rw,storage-full,cloud-platform"" --image-family almalinux-9 --image-project almalinux-cloud --machine-type ""n1-standard-64"" --zone ""us-west1-b""`. I ssh'ed into the machine:. ```bash. gcloud compute ssh pichuan-test --zone ""us-west1-b"". ```. Check the Linux version:. ```. $ uname -a. Linux pichuan-test.us-west1-b.c.brain-genomics.google.com.internal 5.14.0-362.24.2.el9_3.x86_64 #1 SMP PREEMPT_DYNAMIC Sat Mar 30 14:11:54 EDT 2024 x86_64 x86_64 x86_64 GNU/Linux. ```. And I ran this too:. ```. $ cat /etc/os-release. NAME=""AlmaLinux"". VERSION=""9.3 (Shamrock Pampas Cat)"". ID=""almalinux"". ID_LIKE=""rhel centos fedora"". VERSION_ID=""9.3"". PLATFORM_ID=""platform:el9"". PRETTY_NAME=""AlmaLinux 9.3 (Shamrock Pampas Cat)"". ANSI_COLOR=""0;34"". LOGO=""fedora-logo-icon"". CPE_NAME=""cpe:/o:almalinux:almalinux:9::baseos"". HOME_URL=""https://almalinux.org/"". DOCUMENTATION_URL=""https://wiki.almalinux.org/"". BUG_REPORT_URL=""https://bugs.almalinux.org/"". ALMALINUX_MANTISBT_PROJECT=""AlmaLinux-9"". ALMALINUX_MANTISBT_PROJECT_VERSION=""9.3"". REDHAT_SUPPORT_PRODUCT=""AlmaLinux"". REDHAT_SUPPORT_PRODUCT_VERSION=""9.3"". ```. ## Install Singularity. I don't have Singularity on the machine yet, so:. https://docs.sylabs.io/guides/4.1/user-guide/quick_start.html#quick-installation-steps. ```bash. sudo yum update -y && \. sudo yum groupinstall -y 'Development Tools' && \. sudo yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools. ```. ```. sudo yum groupinstall -y 'Development Tools'. # Install RPM packages for dependencies. sudo yum install -y \. autoconf \. automake \. cryptsetup \. fuse3-devel \. git \. glib2-devel \. libseccomp-devel \. libtool \. runc \. squashfs-tools \. wget \. zlib-devel. ```. ```bash. sudo dnf install dnf-plugins-core.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:341,interoperability,standard,standard-,341,"# Attempt to reproduce the issue. ## Get a machine with the same Linux version. I used GCP to get a machine to test. Hopefully this provides a similar environment:. `gcloud compute instances create ""${USER}-test"" --scopes ""compute-rw,storage-full,cloud-platform"" --image-family almalinux-9 --image-project almalinux-cloud --machine-type ""n1-standard-64"" --zone ""us-west1-b""`. I ssh'ed into the machine:. ```bash. gcloud compute ssh pichuan-test --zone ""us-west1-b"". ```. Check the Linux version:. ```. $ uname -a. Linux pichuan-test.us-west1-b.c.brain-genomics.google.com.internal 5.14.0-362.24.2.el9_3.x86_64 #1 SMP PREEMPT_DYNAMIC Sat Mar 30 14:11:54 EDT 2024 x86_64 x86_64 x86_64 GNU/Linux. ```. And I ran this too:. ```. $ cat /etc/os-release. NAME=""AlmaLinux"". VERSION=""9.3 (Shamrock Pampas Cat)"". ID=""almalinux"". ID_LIKE=""rhel centos fedora"". VERSION_ID=""9.3"". PLATFORM_ID=""platform:el9"". PRETTY_NAME=""AlmaLinux 9.3 (Shamrock Pampas Cat)"". ANSI_COLOR=""0;34"". LOGO=""fedora-logo-icon"". CPE_NAME=""cpe:/o:almalinux:almalinux:9::baseos"". HOME_URL=""https://almalinux.org/"". DOCUMENTATION_URL=""https://wiki.almalinux.org/"". BUG_REPORT_URL=""https://bugs.almalinux.org/"". ALMALINUX_MANTISBT_PROJECT=""AlmaLinux-9"". ALMALINUX_MANTISBT_PROJECT_VERSION=""9.3"". REDHAT_SUPPORT_PRODUCT=""AlmaLinux"". REDHAT_SUPPORT_PRODUCT_VERSION=""9.3"". ```. ## Install Singularity. I don't have Singularity on the machine yet, so:. https://docs.sylabs.io/guides/4.1/user-guide/quick_start.html#quick-installation-steps. ```bash. sudo yum update -y && \. sudo yum groupinstall -y 'Development Tools' && \. sudo yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools. ```. ```. sudo yum groupinstall -y 'Development Tools'. # Install RPM packages for dependencies. sudo yum install -y \. autoconf \. automake \. cryptsetup \. fuse3-devel \. git \. glib2-devel \. libseccomp-devel \. libtool \. runc \. squashfs-tools \. wget \. zlib-devel. ```. ```bash. sudo dnf install dnf-plugins-core.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:880,interoperability,platform,platform,880,"# Attempt to reproduce the issue. ## Get a machine with the same Linux version. I used GCP to get a machine to test. Hopefully this provides a similar environment:. `gcloud compute instances create ""${USER}-test"" --scopes ""compute-rw,storage-full,cloud-platform"" --image-family almalinux-9 --image-project almalinux-cloud --machine-type ""n1-standard-64"" --zone ""us-west1-b""`. I ssh'ed into the machine:. ```bash. gcloud compute ssh pichuan-test --zone ""us-west1-b"". ```. Check the Linux version:. ```. $ uname -a. Linux pichuan-test.us-west1-b.c.brain-genomics.google.com.internal 5.14.0-362.24.2.el9_3.x86_64 #1 SMP PREEMPT_DYNAMIC Sat Mar 30 14:11:54 EDT 2024 x86_64 x86_64 x86_64 GNU/Linux. ```. And I ran this too:. ```. $ cat /etc/os-release. NAME=""AlmaLinux"". VERSION=""9.3 (Shamrock Pampas Cat)"". ID=""almalinux"". ID_LIKE=""rhel centos fedora"". VERSION_ID=""9.3"". PLATFORM_ID=""platform:el9"". PRETTY_NAME=""AlmaLinux 9.3 (Shamrock Pampas Cat)"". ANSI_COLOR=""0;34"". LOGO=""fedora-logo-icon"". CPE_NAME=""cpe:/o:almalinux:almalinux:9::baseos"". HOME_URL=""https://almalinux.org/"". DOCUMENTATION_URL=""https://wiki.almalinux.org/"". BUG_REPORT_URL=""https://bugs.almalinux.org/"". ALMALINUX_MANTISBT_PROJECT=""AlmaLinux-9"". ALMALINUX_MANTISBT_PROJECT_VERSION=""9.3"". REDHAT_SUPPORT_PRODUCT=""AlmaLinux"". REDHAT_SUPPORT_PRODUCT_VERSION=""9.3"". ```. ## Install Singularity. I don't have Singularity on the machine yet, so:. https://docs.sylabs.io/guides/4.1/user-guide/quick_start.html#quick-installation-steps. ```bash. sudo yum update -y && \. sudo yum groupinstall -y 'Development Tools' && \. sudo yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools. ```. ```. sudo yum groupinstall -y 'Development Tools'. # Install RPM packages for dependencies. sudo yum install -y \. autoconf \. automake \. cryptsetup \. fuse3-devel \. git \. glib2-devel \. libseccomp-devel \. libtool \. runc \. squashfs-tools \. wget \. zlib-devel. ```. ```bash. sudo dnf install dnf-plugins-core.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:1987,interoperability,plug,plugins-core,1987,"_NAME=""cpe:/o:almalinux:almalinux:9::baseos"". HOME_URL=""https://almalinux.org/"". DOCUMENTATION_URL=""https://wiki.almalinux.org/"". BUG_REPORT_URL=""https://bugs.almalinux.org/"". ALMALINUX_MANTISBT_PROJECT=""AlmaLinux-9"". ALMALINUX_MANTISBT_PROJECT_VERSION=""9.3"". REDHAT_SUPPORT_PRODUCT=""AlmaLinux"". REDHAT_SUPPORT_PRODUCT_VERSION=""9.3"". ```. ## Install Singularity. I don't have Singularity on the machine yet, so:. https://docs.sylabs.io/guides/4.1/user-guide/quick_start.html#quick-installation-steps. ```bash. sudo yum update -y && \. sudo yum groupinstall -y 'Development Tools' && \. sudo yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools. ```. ```. sudo yum groupinstall -y 'Development Tools'. # Install RPM packages for dependencies. sudo yum install -y \. autoconf \. automake \. cryptsetup \. fuse3-devel \. git \. glib2-devel \. libseccomp-devel \. libtool \. runc \. squashfs-tools \. wget \. zlib-devel. ```. ```bash. sudo dnf install dnf-plugins-core. sudo dnf copr enable dctrud/squashfs-tools-ng. sudo dnf install squashfs-tools-ng. ```. ```bash. export VERSION=1.21.0 OS=linux ARCH=amd64 && \. wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz && \. sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz && \. rm go$VERSION.$OS-$ARCH.tar.gz. ```. ```bash. echo 'export PATH=/usr/local/go/bin:$PATH' >> ~/.bashrc && \. source ~/.bashrc. ```. ```bash. export VERSION=4.1.0 && \. wget https://github.com/sylabs/singularity/releases/download/v${VERSION}/singularity-ce-${VERSION}.tar.gz && \. tar -xzf singularity-ce-${VERSION}.tar.gz && \. cd singularity-ce-${VERSION}. ```. ```bash. ./mconfig && \. make -C builddir && \. sudo make -C builddir install. ```. At this point, I have singularity installed. ```bash. $ singularity --version. singularity-ce version 4.1.0. ```. ## Get data and run DeepVariant. Now, let me try to follow similar steps:. ```bash. singularity build DeepVariant_1.6.1.sif docker://google/deepvariant:1.6.1. `",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:4139,interoperability,bind,bind,4139,". mkdir -p reference. FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai. ```. ```bash. mkdir -p input. HTTPDIR=https://downloads.pacbcloud.com/public/dataset/HG003/deepvariant-case-study. curl ${HTTPDIR}/HG003.GRCh38.chr20.pFDA_truthv2.bam > input/HG003.GRCh38.chr20.pFDA_truthv2.bam. curl ${HTTPDIR}/HG003.GRCh38.chr20.pFDA_truthv2.bam.bai > input/HG003.GRCh38.chr20.pFDA_truthv2.bam.bai. ```. ```bash. ulimit -u 10000. BIN_VERSION=""1.6.1"". mkdir -p deepvariant_output. ```. @Carl-labhub mentioned ""When I run it, I’m doing so from an interactive session with singularity exec"". I'm a bit confused by this. Maybe you mean `singularity shell`? So I tried:. ```bash. singularity shell --bind /usr/lib/locale/ DeepVariant_1.6.1.sif. ```. This gets into a shell mode, then I ran:. ```. Singularity> /opt/deepvariant/bin/run_deepvariant \. > --model_type PACBIO \. > --ref reference/GRCh38_no_alt_analysis_set.fasta \. > --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. > --output_vcf deepvariant_output/output.vcf.gz \. > --num_shards $(nproc) \. > --regions chr20. ```. Directly `singularity exec` with the command (just like https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-pacbio-model-case-study.md) should be fine too. My `make_examples` step completed without any issues. It took:. ```. real 9m7.540s. user 215m38.303s. sys 6m41.297s. ```. I let the whole run finish just to be sure. --> The full run also completed without any errors. @Carl-labhub, can you check what I did above, and see what might be different on your side? And, there were previous GitHub issues that might be worth reading through to see if there are any relevant clues. For example: https:/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:71,modifiability,version,version,71,"# Attempt to reproduce the issue. ## Get a machine with the same Linux version. I used GCP to get a machine to test. Hopefully this provides a similar environment:. `gcloud compute instances create ""${USER}-test"" --scopes ""compute-rw,storage-full,cloud-platform"" --image-family almalinux-9 --image-project almalinux-cloud --machine-type ""n1-standard-64"" --zone ""us-west1-b""`. I ssh'ed into the machine:. ```bash. gcloud compute ssh pichuan-test --zone ""us-west1-b"". ```. Check the Linux version:. ```. $ uname -a. Linux pichuan-test.us-west1-b.c.brain-genomics.google.com.internal 5.14.0-362.24.2.el9_3.x86_64 #1 SMP PREEMPT_DYNAMIC Sat Mar 30 14:11:54 EDT 2024 x86_64 x86_64 x86_64 GNU/Linux. ```. And I ran this too:. ```. $ cat /etc/os-release. NAME=""AlmaLinux"". VERSION=""9.3 (Shamrock Pampas Cat)"". ID=""almalinux"". ID_LIKE=""rhel centos fedora"". VERSION_ID=""9.3"". PLATFORM_ID=""platform:el9"". PRETTY_NAME=""AlmaLinux 9.3 (Shamrock Pampas Cat)"". ANSI_COLOR=""0;34"". LOGO=""fedora-logo-icon"". CPE_NAME=""cpe:/o:almalinux:almalinux:9::baseos"". HOME_URL=""https://almalinux.org/"". DOCUMENTATION_URL=""https://wiki.almalinux.org/"". BUG_REPORT_URL=""https://bugs.almalinux.org/"". ALMALINUX_MANTISBT_PROJECT=""AlmaLinux-9"". ALMALINUX_MANTISBT_PROJECT_VERSION=""9.3"". REDHAT_SUPPORT_PRODUCT=""AlmaLinux"". REDHAT_SUPPORT_PRODUCT_VERSION=""9.3"". ```. ## Install Singularity. I don't have Singularity on the machine yet, so:. https://docs.sylabs.io/guides/4.1/user-guide/quick_start.html#quick-installation-steps. ```bash. sudo yum update -y && \. sudo yum groupinstall -y 'Development Tools' && \. sudo yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools. ```. ```. sudo yum groupinstall -y 'Development Tools'. # Install RPM packages for dependencies. sudo yum install -y \. autoconf \. automake \. cryptsetup \. fuse3-devel \. git \. glib2-devel \. libseccomp-devel \. libtool \. runc \. squashfs-tools \. wget \. zlib-devel. ```. ```bash. sudo dnf install dnf-plugins-core.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:487,modifiability,version,version,487,"# Attempt to reproduce the issue. ## Get a machine with the same Linux version. I used GCP to get a machine to test. Hopefully this provides a similar environment:. `gcloud compute instances create ""${USER}-test"" --scopes ""compute-rw,storage-full,cloud-platform"" --image-family almalinux-9 --image-project almalinux-cloud --machine-type ""n1-standard-64"" --zone ""us-west1-b""`. I ssh'ed into the machine:. ```bash. gcloud compute ssh pichuan-test --zone ""us-west1-b"". ```. Check the Linux version:. ```. $ uname -a. Linux pichuan-test.us-west1-b.c.brain-genomics.google.com.internal 5.14.0-362.24.2.el9_3.x86_64 #1 SMP PREEMPT_DYNAMIC Sat Mar 30 14:11:54 EDT 2024 x86_64 x86_64 x86_64 GNU/Linux. ```. And I ran this too:. ```. $ cat /etc/os-release. NAME=""AlmaLinux"". VERSION=""9.3 (Shamrock Pampas Cat)"". ID=""almalinux"". ID_LIKE=""rhel centos fedora"". VERSION_ID=""9.3"". PLATFORM_ID=""platform:el9"". PRETTY_NAME=""AlmaLinux 9.3 (Shamrock Pampas Cat)"". ANSI_COLOR=""0;34"". LOGO=""fedora-logo-icon"". CPE_NAME=""cpe:/o:almalinux:almalinux:9::baseos"". HOME_URL=""https://almalinux.org/"". DOCUMENTATION_URL=""https://wiki.almalinux.org/"". BUG_REPORT_URL=""https://bugs.almalinux.org/"". ALMALINUX_MANTISBT_PROJECT=""AlmaLinux-9"". ALMALINUX_MANTISBT_PROJECT_VERSION=""9.3"". REDHAT_SUPPORT_PRODUCT=""AlmaLinux"". REDHAT_SUPPORT_PRODUCT_VERSION=""9.3"". ```. ## Install Singularity. I don't have Singularity on the machine yet, so:. https://docs.sylabs.io/guides/4.1/user-guide/quick_start.html#quick-installation-steps. ```bash. sudo yum update -y && \. sudo yum groupinstall -y 'Development Tools' && \. sudo yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools. ```. ```. sudo yum groupinstall -y 'Development Tools'. # Install RPM packages for dependencies. sudo yum install -y \. autoconf \. automake \. cryptsetup \. fuse3-devel \. git \. glib2-devel \. libseccomp-devel \. libtool \. runc \. squashfs-tools \. wget \. zlib-devel. ```. ```bash. sudo dnf install dnf-plugins-core.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:766,modifiability,VERSION,VERSION,766,"# Attempt to reproduce the issue. ## Get a machine with the same Linux version. I used GCP to get a machine to test. Hopefully this provides a similar environment:. `gcloud compute instances create ""${USER}-test"" --scopes ""compute-rw,storage-full,cloud-platform"" --image-family almalinux-9 --image-project almalinux-cloud --machine-type ""n1-standard-64"" --zone ""us-west1-b""`. I ssh'ed into the machine:. ```bash. gcloud compute ssh pichuan-test --zone ""us-west1-b"". ```. Check the Linux version:. ```. $ uname -a. Linux pichuan-test.us-west1-b.c.brain-genomics.google.com.internal 5.14.0-362.24.2.el9_3.x86_64 #1 SMP PREEMPT_DYNAMIC Sat Mar 30 14:11:54 EDT 2024 x86_64 x86_64 x86_64 GNU/Linux. ```. And I ran this too:. ```. $ cat /etc/os-release. NAME=""AlmaLinux"". VERSION=""9.3 (Shamrock Pampas Cat)"". ID=""almalinux"". ID_LIKE=""rhel centos fedora"". VERSION_ID=""9.3"". PLATFORM_ID=""platform:el9"". PRETTY_NAME=""AlmaLinux 9.3 (Shamrock Pampas Cat)"". ANSI_COLOR=""0;34"". LOGO=""fedora-logo-icon"". CPE_NAME=""cpe:/o:almalinux:almalinux:9::baseos"". HOME_URL=""https://almalinux.org/"". DOCUMENTATION_URL=""https://wiki.almalinux.org/"". BUG_REPORT_URL=""https://bugs.almalinux.org/"". ALMALINUX_MANTISBT_PROJECT=""AlmaLinux-9"". ALMALINUX_MANTISBT_PROJECT_VERSION=""9.3"". REDHAT_SUPPORT_PRODUCT=""AlmaLinux"". REDHAT_SUPPORT_PRODUCT_VERSION=""9.3"". ```. ## Install Singularity. I don't have Singularity on the machine yet, so:. https://docs.sylabs.io/guides/4.1/user-guide/quick_start.html#quick-installation-steps. ```bash. sudo yum update -y && \. sudo yum groupinstall -y 'Development Tools' && \. sudo yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools. ```. ```. sudo yum groupinstall -y 'Development Tools'. # Install RPM packages for dependencies. sudo yum install -y \. autoconf \. automake \. cryptsetup \. fuse3-devel \. git \. glib2-devel \. libseccomp-devel \. libtool \. runc \. squashfs-tools \. wget \. zlib-devel. ```. ```bash. sudo dnf install dnf-plugins-core.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:1750,modifiability,pac,packages,1750,"AlmaLinux"". VERSION=""9.3 (Shamrock Pampas Cat)"". ID=""almalinux"". ID_LIKE=""rhel centos fedora"". VERSION_ID=""9.3"". PLATFORM_ID=""platform:el9"". PRETTY_NAME=""AlmaLinux 9.3 (Shamrock Pampas Cat)"". ANSI_COLOR=""0;34"". LOGO=""fedora-logo-icon"". CPE_NAME=""cpe:/o:almalinux:almalinux:9::baseos"". HOME_URL=""https://almalinux.org/"". DOCUMENTATION_URL=""https://wiki.almalinux.org/"". BUG_REPORT_URL=""https://bugs.almalinux.org/"". ALMALINUX_MANTISBT_PROJECT=""AlmaLinux-9"". ALMALINUX_MANTISBT_PROJECT_VERSION=""9.3"". REDHAT_SUPPORT_PRODUCT=""AlmaLinux"". REDHAT_SUPPORT_PRODUCT_VERSION=""9.3"". ```. ## Install Singularity. I don't have Singularity on the machine yet, so:. https://docs.sylabs.io/guides/4.1/user-guide/quick_start.html#quick-installation-steps. ```bash. sudo yum update -y && \. sudo yum groupinstall -y 'Development Tools' && \. sudo yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools. ```. ```. sudo yum groupinstall -y 'Development Tools'. # Install RPM packages for dependencies. sudo yum install -y \. autoconf \. automake \. cryptsetup \. fuse3-devel \. git \. glib2-devel \. libseccomp-devel \. libtool \. runc \. squashfs-tools \. wget \. zlib-devel. ```. ```bash. sudo dnf install dnf-plugins-core. sudo dnf copr enable dctrud/squashfs-tools-ng. sudo dnf install squashfs-tools-ng. ```. ```bash. export VERSION=1.21.0 OS=linux ARCH=amd64 && \. wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz && \. sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz && \. rm go$VERSION.$OS-$ARCH.tar.gz. ```. ```bash. echo 'export PATH=/usr/local/go/bin:$PATH' >> ~/.bashrc && \. source ~/.bashrc. ```. ```bash. export VERSION=4.1.0 && \. wget https://github.com/sylabs/singularity/releases/download/v${VERSION}/singularity-ce-${VERSION}.tar.gz && \. tar -xzf singularity-ce-${VERSION}.tar.gz && \. cd singularity-ce-${VERSION}. ```. ```bash. ./mconfig && \. make -C builddir && \. sudo make -C builddir install. ```. At this point, I have singularity i",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:1763,modifiability,depend,dependencies,1763,"SION=""9.3 (Shamrock Pampas Cat)"". ID=""almalinux"". ID_LIKE=""rhel centos fedora"". VERSION_ID=""9.3"". PLATFORM_ID=""platform:el9"". PRETTY_NAME=""AlmaLinux 9.3 (Shamrock Pampas Cat)"". ANSI_COLOR=""0;34"". LOGO=""fedora-logo-icon"". CPE_NAME=""cpe:/o:almalinux:almalinux:9::baseos"". HOME_URL=""https://almalinux.org/"". DOCUMENTATION_URL=""https://wiki.almalinux.org/"". BUG_REPORT_URL=""https://bugs.almalinux.org/"". ALMALINUX_MANTISBT_PROJECT=""AlmaLinux-9"". ALMALINUX_MANTISBT_PROJECT_VERSION=""9.3"". REDHAT_SUPPORT_PRODUCT=""AlmaLinux"". REDHAT_SUPPORT_PRODUCT_VERSION=""9.3"". ```. ## Install Singularity. I don't have Singularity on the machine yet, so:. https://docs.sylabs.io/guides/4.1/user-guide/quick_start.html#quick-installation-steps. ```bash. sudo yum update -y && \. sudo yum groupinstall -y 'Development Tools' && \. sudo yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools. ```. ```. sudo yum groupinstall -y 'Development Tools'. # Install RPM packages for dependencies. sudo yum install -y \. autoconf \. automake \. cryptsetup \. fuse3-devel \. git \. glib2-devel \. libseccomp-devel \. libtool \. runc \. squashfs-tools \. wget \. zlib-devel. ```. ```bash. sudo dnf install dnf-plugins-core. sudo dnf copr enable dctrud/squashfs-tools-ng. sudo dnf install squashfs-tools-ng. ```. ```bash. export VERSION=1.21.0 OS=linux ARCH=amd64 && \. wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz && \. sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz && \. rm go$VERSION.$OS-$ARCH.tar.gz. ```. ```bash. echo 'export PATH=/usr/local/go/bin:$PATH' >> ~/.bashrc && \. source ~/.bashrc. ```. ```bash. export VERSION=4.1.0 && \. wget https://github.com/sylabs/singularity/releases/download/v${VERSION}/singularity-ce-${VERSION}.tar.gz && \. tar -xzf singularity-ce-${VERSION}.tar.gz && \. cd singularity-ce-${VERSION}. ```. ```bash. ./mconfig && \. make -C builddir && \. sudo make -C builddir install. ```. At this point, I have singularity installed. ```ba",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:2105,modifiability,VERSION,VERSION,2105,"alinux.org/"". BUG_REPORT_URL=""https://bugs.almalinux.org/"". ALMALINUX_MANTISBT_PROJECT=""AlmaLinux-9"". ALMALINUX_MANTISBT_PROJECT_VERSION=""9.3"". REDHAT_SUPPORT_PRODUCT=""AlmaLinux"". REDHAT_SUPPORT_PRODUCT_VERSION=""9.3"". ```. ## Install Singularity. I don't have Singularity on the machine yet, so:. https://docs.sylabs.io/guides/4.1/user-guide/quick_start.html#quick-installation-steps. ```bash. sudo yum update -y && \. sudo yum groupinstall -y 'Development Tools' && \. sudo yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools. ```. ```. sudo yum groupinstall -y 'Development Tools'. # Install RPM packages for dependencies. sudo yum install -y \. autoconf \. automake \. cryptsetup \. fuse3-devel \. git \. glib2-devel \. libseccomp-devel \. libtool \. runc \. squashfs-tools \. wget \. zlib-devel. ```. ```bash. sudo dnf install dnf-plugins-core. sudo dnf copr enable dctrud/squashfs-tools-ng. sudo dnf install squashfs-tools-ng. ```. ```bash. export VERSION=1.21.0 OS=linux ARCH=amd64 && \. wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz && \. sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz && \. rm go$VERSION.$OS-$ARCH.tar.gz. ```. ```bash. echo 'export PATH=/usr/local/go/bin:$PATH' >> ~/.bashrc && \. source ~/.bashrc. ```. ```bash. export VERSION=4.1.0 && \. wget https://github.com/sylabs/singularity/releases/download/v${VERSION}/singularity-ce-${VERSION}.tar.gz && \. tar -xzf singularity-ce-${VERSION}.tar.gz && \. cd singularity-ce-${VERSION}. ```. ```bash. ./mconfig && \. make -C builddir && \. sudo make -C builddir install. ```. At this point, I have singularity installed. ```bash. $ singularity --version. singularity-ce version 4.1.0. ```. ## Get data and run DeepVariant. Now, let me try to follow similar steps:. ```bash. singularity build DeepVariant_1.6.1.sif docker://google/deepvariant:1.6.1. ```. From here, I used https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-pacbio-model-case-study.md ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:2179,modifiability,VERSION,VERSION,2179,"ISBT_PROJECT=""AlmaLinux-9"". ALMALINUX_MANTISBT_PROJECT_VERSION=""9.3"". REDHAT_SUPPORT_PRODUCT=""AlmaLinux"". REDHAT_SUPPORT_PRODUCT_VERSION=""9.3"". ```. ## Install Singularity. I don't have Singularity on the machine yet, so:. https://docs.sylabs.io/guides/4.1/user-guide/quick_start.html#quick-installation-steps. ```bash. sudo yum update -y && \. sudo yum groupinstall -y 'Development Tools' && \. sudo yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools. ```. ```. sudo yum groupinstall -y 'Development Tools'. # Install RPM packages for dependencies. sudo yum install -y \. autoconf \. automake \. cryptsetup \. fuse3-devel \. git \. glib2-devel \. libseccomp-devel \. libtool \. runc \. squashfs-tools \. wget \. zlib-devel. ```. ```bash. sudo dnf install dnf-plugins-core. sudo dnf copr enable dctrud/squashfs-tools-ng. sudo dnf install squashfs-tools-ng. ```. ```bash. export VERSION=1.21.0 OS=linux ARCH=amd64 && \. wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz && \. sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz && \. rm go$VERSION.$OS-$ARCH.tar.gz. ```. ```bash. echo 'export PATH=/usr/local/go/bin:$PATH' >> ~/.bashrc && \. source ~/.bashrc. ```. ```bash. export VERSION=4.1.0 && \. wget https://github.com/sylabs/singularity/releases/download/v${VERSION}/singularity-ce-${VERSION}.tar.gz && \. tar -xzf singularity-ce-${VERSION}.tar.gz && \. cd singularity-ce-${VERSION}. ```. ```bash. ./mconfig && \. make -C builddir && \. sudo make -C builddir install. ```. At this point, I have singularity installed. ```bash. $ singularity --version. singularity-ce version 4.1.0. ```. ## Get data and run DeepVariant. Now, let me try to follow similar steps:. ```bash. singularity build DeepVariant_1.6.1.sif docker://google/deepvariant:1.6.1. ```. From here, I used https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-pacbio-model-case-study.md to test. Download data:. ```bash. mkdir -p reference. FTPDIR=ftp://ftp.ncb",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:2242,modifiability,VERSION,VERSION,2242,"""9.3"". REDHAT_SUPPORT_PRODUCT=""AlmaLinux"". REDHAT_SUPPORT_PRODUCT_VERSION=""9.3"". ```. ## Install Singularity. I don't have Singularity on the machine yet, so:. https://docs.sylabs.io/guides/4.1/user-guide/quick_start.html#quick-installation-steps. ```bash. sudo yum update -y && \. sudo yum groupinstall -y 'Development Tools' && \. sudo yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools. ```. ```. sudo yum groupinstall -y 'Development Tools'. # Install RPM packages for dependencies. sudo yum install -y \. autoconf \. automake \. cryptsetup \. fuse3-devel \. git \. glib2-devel \. libseccomp-devel \. libtool \. runc \. squashfs-tools \. wget \. zlib-devel. ```. ```bash. sudo dnf install dnf-plugins-core. sudo dnf copr enable dctrud/squashfs-tools-ng. sudo dnf install squashfs-tools-ng. ```. ```bash. export VERSION=1.21.0 OS=linux ARCH=amd64 && \. wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz && \. sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz && \. rm go$VERSION.$OS-$ARCH.tar.gz. ```. ```bash. echo 'export PATH=/usr/local/go/bin:$PATH' >> ~/.bashrc && \. source ~/.bashrc. ```. ```bash. export VERSION=4.1.0 && \. wget https://github.com/sylabs/singularity/releases/download/v${VERSION}/singularity-ce-${VERSION}.tar.gz && \. tar -xzf singularity-ce-${VERSION}.tar.gz && \. cd singularity-ce-${VERSION}. ```. ```bash. ./mconfig && \. make -C builddir && \. sudo make -C builddir install. ```. At this point, I have singularity installed. ```bash. $ singularity --version. singularity-ce version 4.1.0. ```. ## Get data and run DeepVariant. Now, let me try to follow similar steps:. ```bash. singularity build DeepVariant_1.6.1.sif docker://google/deepvariant:1.6.1. ```. From here, I used https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-pacbio-model-case-study.md to test. Download data:. ```bash. mkdir -p reference. FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:2279,modifiability,VERSION,VERSION,2279,"nux"". REDHAT_SUPPORT_PRODUCT_VERSION=""9.3"". ```. ## Install Singularity. I don't have Singularity on the machine yet, so:. https://docs.sylabs.io/guides/4.1/user-guide/quick_start.html#quick-installation-steps. ```bash. sudo yum update -y && \. sudo yum groupinstall -y 'Development Tools' && \. sudo yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools. ```. ```. sudo yum groupinstall -y 'Development Tools'. # Install RPM packages for dependencies. sudo yum install -y \. autoconf \. automake \. cryptsetup \. fuse3-devel \. git \. glib2-devel \. libseccomp-devel \. libtool \. runc \. squashfs-tools \. wget \. zlib-devel. ```. ```bash. sudo dnf install dnf-plugins-core. sudo dnf copr enable dctrud/squashfs-tools-ng. sudo dnf install squashfs-tools-ng. ```. ```bash. export VERSION=1.21.0 OS=linux ARCH=amd64 && \. wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz && \. sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz && \. rm go$VERSION.$OS-$ARCH.tar.gz. ```. ```bash. echo 'export PATH=/usr/local/go/bin:$PATH' >> ~/.bashrc && \. source ~/.bashrc. ```. ```bash. export VERSION=4.1.0 && \. wget https://github.com/sylabs/singularity/releases/download/v${VERSION}/singularity-ce-${VERSION}.tar.gz && \. tar -xzf singularity-ce-${VERSION}.tar.gz && \. cd singularity-ce-${VERSION}. ```. ```bash. ./mconfig && \. make -C builddir && \. sudo make -C builddir install. ```. At this point, I have singularity installed. ```bash. $ singularity --version. singularity-ce version 4.1.0. ```. ## Get data and run DeepVariant. Now, let me try to follow similar steps:. ```bash. singularity build DeepVariant_1.6.1.sif docker://google/deepvariant:1.6.1. ```. From here, I used https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-pacbio-model-case-study.md to test. Download data:. ```bash. mkdir -p reference. FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:2420,modifiability,VERSION,VERSION,2420,"s.io/guides/4.1/user-guide/quick_start.html#quick-installation-steps. ```bash. sudo yum update -y && \. sudo yum groupinstall -y 'Development Tools' && \. sudo yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools. ```. ```. sudo yum groupinstall -y 'Development Tools'. # Install RPM packages for dependencies. sudo yum install -y \. autoconf \. automake \. cryptsetup \. fuse3-devel \. git \. glib2-devel \. libseccomp-devel \. libtool \. runc \. squashfs-tools \. wget \. zlib-devel. ```. ```bash. sudo dnf install dnf-plugins-core. sudo dnf copr enable dctrud/squashfs-tools-ng. sudo dnf install squashfs-tools-ng. ```. ```bash. export VERSION=1.21.0 OS=linux ARCH=amd64 && \. wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz && \. sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz && \. rm go$VERSION.$OS-$ARCH.tar.gz. ```. ```bash. echo 'export PATH=/usr/local/go/bin:$PATH' >> ~/.bashrc && \. source ~/.bashrc. ```. ```bash. export VERSION=4.1.0 && \. wget https://github.com/sylabs/singularity/releases/download/v${VERSION}/singularity-ce-${VERSION}.tar.gz && \. tar -xzf singularity-ce-${VERSION}.tar.gz && \. cd singularity-ce-${VERSION}. ```. ```bash. ./mconfig && \. make -C builddir && \. sudo make -C builddir install. ```. At this point, I have singularity installed. ```bash. $ singularity --version. singularity-ce version 4.1.0. ```. ## Get data and run DeepVariant. Now, let me try to follow similar steps:. ```bash. singularity build DeepVariant_1.6.1.sif docker://google/deepvariant:1.6.1. ```. From here, I used https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-pacbio-model-case-study.md to test. Download data:. ```bash. mkdir -p reference. FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta. curl ${FTPDIR}/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:2504,modifiability,VERSION,VERSION,2504,"yum update -y && \. sudo yum groupinstall -y 'Development Tools' && \. sudo yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools. ```. ```. sudo yum groupinstall -y 'Development Tools'. # Install RPM packages for dependencies. sudo yum install -y \. autoconf \. automake \. cryptsetup \. fuse3-devel \. git \. glib2-devel \. libseccomp-devel \. libtool \. runc \. squashfs-tools \. wget \. zlib-devel. ```. ```bash. sudo dnf install dnf-plugins-core. sudo dnf copr enable dctrud/squashfs-tools-ng. sudo dnf install squashfs-tools-ng. ```. ```bash. export VERSION=1.21.0 OS=linux ARCH=amd64 && \. wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz && \. sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz && \. rm go$VERSION.$OS-$ARCH.tar.gz. ```. ```bash. echo 'export PATH=/usr/local/go/bin:$PATH' >> ~/.bashrc && \. source ~/.bashrc. ```. ```bash. export VERSION=4.1.0 && \. wget https://github.com/sylabs/singularity/releases/download/v${VERSION}/singularity-ce-${VERSION}.tar.gz && \. tar -xzf singularity-ce-${VERSION}.tar.gz && \. cd singularity-ce-${VERSION}. ```. ```bash. ./mconfig && \. make -C builddir && \. sudo make -C builddir install. ```. At this point, I have singularity installed. ```bash. $ singularity --version. singularity-ce version 4.1.0. ```. ## Get data and run DeepVariant. Now, let me try to follow similar steps:. ```bash. singularity build DeepVariant_1.6.1.sif docker://google/deepvariant:1.6.1. ```. From here, I used https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-pacbio-model-case-study.md to test. Download data:. ```bash. mkdir -p reference. FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analys",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:2530,modifiability,VERSION,VERSION,2530,"um groupinstall -y 'Development Tools' && \. sudo yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools. ```. ```. sudo yum groupinstall -y 'Development Tools'. # Install RPM packages for dependencies. sudo yum install -y \. autoconf \. automake \. cryptsetup \. fuse3-devel \. git \. glib2-devel \. libseccomp-devel \. libtool \. runc \. squashfs-tools \. wget \. zlib-devel. ```. ```bash. sudo dnf install dnf-plugins-core. sudo dnf copr enable dctrud/squashfs-tools-ng. sudo dnf install squashfs-tools-ng. ```. ```bash. export VERSION=1.21.0 OS=linux ARCH=amd64 && \. wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz && \. sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz && \. rm go$VERSION.$OS-$ARCH.tar.gz. ```. ```bash. echo 'export PATH=/usr/local/go/bin:$PATH' >> ~/.bashrc && \. source ~/.bashrc. ```. ```bash. export VERSION=4.1.0 && \. wget https://github.com/sylabs/singularity/releases/download/v${VERSION}/singularity-ce-${VERSION}.tar.gz && \. tar -xzf singularity-ce-${VERSION}.tar.gz && \. cd singularity-ce-${VERSION}. ```. ```bash. ./mconfig && \. make -C builddir && \. sudo make -C builddir install. ```. At this point, I have singularity installed. ```bash. $ singularity --version. singularity-ce version 4.1.0. ```. ## Get data and run DeepVariant. Now, let me try to follow similar steps:. ```bash. singularity build DeepVariant_1.6.1.sif docker://google/deepvariant:1.6.1. ```. From here, I used https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-pacbio-model-case-study.md to test. Download data:. ```bash. mkdir -p reference. FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai. ```. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:2578,modifiability,VERSION,VERSION,2578,"o yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools. ```. ```. sudo yum groupinstall -y 'Development Tools'. # Install RPM packages for dependencies. sudo yum install -y \. autoconf \. automake \. cryptsetup \. fuse3-devel \. git \. glib2-devel \. libseccomp-devel \. libtool \. runc \. squashfs-tools \. wget \. zlib-devel. ```. ```bash. sudo dnf install dnf-plugins-core. sudo dnf copr enable dctrud/squashfs-tools-ng. sudo dnf install squashfs-tools-ng. ```. ```bash. export VERSION=1.21.0 OS=linux ARCH=amd64 && \. wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz && \. sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz && \. rm go$VERSION.$OS-$ARCH.tar.gz. ```. ```bash. echo 'export PATH=/usr/local/go/bin:$PATH' >> ~/.bashrc && \. source ~/.bashrc. ```. ```bash. export VERSION=4.1.0 && \. wget https://github.com/sylabs/singularity/releases/download/v${VERSION}/singularity-ce-${VERSION}.tar.gz && \. tar -xzf singularity-ce-${VERSION}.tar.gz && \. cd singularity-ce-${VERSION}. ```. ```bash. ./mconfig && \. make -C builddir && \. sudo make -C builddir install. ```. At this point, I have singularity installed. ```bash. $ singularity --version. singularity-ce version 4.1.0. ```. ## Get data and run DeepVariant. Now, let me try to follow similar steps:. ```bash. singularity build DeepVariant_1.6.1.sif docker://google/deepvariant:1.6.1. ```. From here, I used https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-pacbio-model-case-study.md to test. Download data:. ```bash. mkdir -p reference. FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai. ```. ```bash. mkdir -p input. HTTPDIR=https://downloads.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:2620,modifiability,VERSION,VERSION,2620,"id-devel \. libseccomp-devel \. wget \. squashfs-tools. ```. ```. sudo yum groupinstall -y 'Development Tools'. # Install RPM packages for dependencies. sudo yum install -y \. autoconf \. automake \. cryptsetup \. fuse3-devel \. git \. glib2-devel \. libseccomp-devel \. libtool \. runc \. squashfs-tools \. wget \. zlib-devel. ```. ```bash. sudo dnf install dnf-plugins-core. sudo dnf copr enable dctrud/squashfs-tools-ng. sudo dnf install squashfs-tools-ng. ```. ```bash. export VERSION=1.21.0 OS=linux ARCH=amd64 && \. wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz && \. sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz && \. rm go$VERSION.$OS-$ARCH.tar.gz. ```. ```bash. echo 'export PATH=/usr/local/go/bin:$PATH' >> ~/.bashrc && \. source ~/.bashrc. ```. ```bash. export VERSION=4.1.0 && \. wget https://github.com/sylabs/singularity/releases/download/v${VERSION}/singularity-ce-${VERSION}.tar.gz && \. tar -xzf singularity-ce-${VERSION}.tar.gz && \. cd singularity-ce-${VERSION}. ```. ```bash. ./mconfig && \. make -C builddir && \. sudo make -C builddir install. ```. At this point, I have singularity installed. ```bash. $ singularity --version. singularity-ce version 4.1.0. ```. ## Get data and run DeepVariant. Now, let me try to follow similar steps:. ```bash. singularity build DeepVariant_1.6.1.sif docker://google/deepvariant:1.6.1. ```. From here, I used https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-pacbio-model-case-study.md to test. Download data:. ```bash. mkdir -p reference. FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai. ```. ```bash. mkdir -p input. HTTPDIR=https://downloads.pacbcloud.com/public/dataset/HG003/deepvar",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:2789,modifiability,version,version,2789," -y \. autoconf \. automake \. cryptsetup \. fuse3-devel \. git \. glib2-devel \. libseccomp-devel \. libtool \. runc \. squashfs-tools \. wget \. zlib-devel. ```. ```bash. sudo dnf install dnf-plugins-core. sudo dnf copr enable dctrud/squashfs-tools-ng. sudo dnf install squashfs-tools-ng. ```. ```bash. export VERSION=1.21.0 OS=linux ARCH=amd64 && \. wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz && \. sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz && \. rm go$VERSION.$OS-$ARCH.tar.gz. ```. ```bash. echo 'export PATH=/usr/local/go/bin:$PATH' >> ~/.bashrc && \. source ~/.bashrc. ```. ```bash. export VERSION=4.1.0 && \. wget https://github.com/sylabs/singularity/releases/download/v${VERSION}/singularity-ce-${VERSION}.tar.gz && \. tar -xzf singularity-ce-${VERSION}.tar.gz && \. cd singularity-ce-${VERSION}. ```. ```bash. ./mconfig && \. make -C builddir && \. sudo make -C builddir install. ```. At this point, I have singularity installed. ```bash. $ singularity --version. singularity-ce version 4.1.0. ```. ## Get data and run DeepVariant. Now, let me try to follow similar steps:. ```bash. singularity build DeepVariant_1.6.1.sif docker://google/deepvariant:1.6.1. ```. From here, I used https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-pacbio-model-case-study.md to test. Download data:. ```bash. mkdir -p reference. FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai. ```. ```bash. mkdir -p input. HTTPDIR=https://downloads.pacbcloud.com/public/dataset/HG003/deepvariant-case-study. curl ${HTTPDIR}/HG003.GRCh38.chr20.pFDA_truthv2.bam > input/HG003.GRCh38.chr20.pFDA_truthv2.bam. curl ${HTTPDIR}/HG003.GRCh38.chr20.pFDA_truthv2.bam.bai",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:2813,modifiability,version,version,2813,"ake \. cryptsetup \. fuse3-devel \. git \. glib2-devel \. libseccomp-devel \. libtool \. runc \. squashfs-tools \. wget \. zlib-devel. ```. ```bash. sudo dnf install dnf-plugins-core. sudo dnf copr enable dctrud/squashfs-tools-ng. sudo dnf install squashfs-tools-ng. ```. ```bash. export VERSION=1.21.0 OS=linux ARCH=amd64 && \. wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz && \. sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz && \. rm go$VERSION.$OS-$ARCH.tar.gz. ```. ```bash. echo 'export PATH=/usr/local/go/bin:$PATH' >> ~/.bashrc && \. source ~/.bashrc. ```. ```bash. export VERSION=4.1.0 && \. wget https://github.com/sylabs/singularity/releases/download/v${VERSION}/singularity-ce-${VERSION}.tar.gz && \. tar -xzf singularity-ce-${VERSION}.tar.gz && \. cd singularity-ce-${VERSION}. ```. ```bash. ./mconfig && \. make -C builddir && \. sudo make -C builddir install. ```. At this point, I have singularity installed. ```bash. $ singularity --version. singularity-ce version 4.1.0. ```. ## Get data and run DeepVariant. Now, let me try to follow similar steps:. ```bash. singularity build DeepVariant_1.6.1.sif docker://google/deepvariant:1.6.1. ```. From here, I used https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-pacbio-model-case-study.md to test. Download data:. ```bash. mkdir -p reference. FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai. ```. ```bash. mkdir -p input. HTTPDIR=https://downloads.pacbcloud.com/public/dataset/HG003/deepvariant-case-study. curl ${HTTPDIR}/HG003.GRCh38.chr20.pFDA_truthv2.bam > input/HG003.GRCh38.chr20.pFDA_truthv2.bam. curl ${HTTPDIR}/HG003.GRCh38.chr20.pFDA_truthv2.bam.bai > input/HG003.GRCh38.ch",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:3082,modifiability,pac,pacbio-model-case-study,3082,"sh. export VERSION=1.21.0 OS=linux ARCH=amd64 && \. wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz && \. sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz && \. rm go$VERSION.$OS-$ARCH.tar.gz. ```. ```bash. echo 'export PATH=/usr/local/go/bin:$PATH' >> ~/.bashrc && \. source ~/.bashrc. ```. ```bash. export VERSION=4.1.0 && \. wget https://github.com/sylabs/singularity/releases/download/v${VERSION}/singularity-ce-${VERSION}.tar.gz && \. tar -xzf singularity-ce-${VERSION}.tar.gz && \. cd singularity-ce-${VERSION}. ```. ```bash. ./mconfig && \. make -C builddir && \. sudo make -C builddir install. ```. At this point, I have singularity installed. ```bash. $ singularity --version. singularity-ce version 4.1.0. ```. ## Get data and run DeepVariant. Now, let me try to follow similar steps:. ```bash. singularity build DeepVariant_1.6.1.sif docker://google/deepvariant:1.6.1. ```. From here, I used https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-pacbio-model-case-study.md to test. Download data:. ```bash. mkdir -p reference. FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai. ```. ```bash. mkdir -p input. HTTPDIR=https://downloads.pacbcloud.com/public/dataset/HG003/deepvariant-case-study. curl ${HTTPDIR}/HG003.GRCh38.chr20.pFDA_truthv2.bam > input/HG003.GRCh38.chr20.pFDA_truthv2.bam. curl ${HTTPDIR}/HG003.GRCh38.chr20.pFDA_truthv2.bam.bai > input/HG003.GRCh38.chr20.pFDA_truthv2.bam.bai. ```. ```bash. ulimit -u 10000. BIN_VERSION=""1.6.1"". mkdir -p deepvariant_output. ```. @Carl-labhub mentioned ""When I run it, I’m doing so from an interactive session with singularity exec"". I'm a bit confused by this. Maybe you mean `singularity shell",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:3582,modifiability,pac,pacbcloud,3582,"tar.gz && \. cd singularity-ce-${VERSION}. ```. ```bash. ./mconfig && \. make -C builddir && \. sudo make -C builddir install. ```. At this point, I have singularity installed. ```bash. $ singularity --version. singularity-ce version 4.1.0. ```. ## Get data and run DeepVariant. Now, let me try to follow similar steps:. ```bash. singularity build DeepVariant_1.6.1.sif docker://google/deepvariant:1.6.1. ```. From here, I used https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-pacbio-model-case-study.md to test. Download data:. ```bash. mkdir -p reference. FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai. ```. ```bash. mkdir -p input. HTTPDIR=https://downloads.pacbcloud.com/public/dataset/HG003/deepvariant-case-study. curl ${HTTPDIR}/HG003.GRCh38.chr20.pFDA_truthv2.bam > input/HG003.GRCh38.chr20.pFDA_truthv2.bam. curl ${HTTPDIR}/HG003.GRCh38.chr20.pFDA_truthv2.bam.bai > input/HG003.GRCh38.chr20.pFDA_truthv2.bam.bai. ```. ```bash. ulimit -u 10000. BIN_VERSION=""1.6.1"". mkdir -p deepvariant_output. ```. @Carl-labhub mentioned ""When I run it, I’m doing so from an interactive session with singularity exec"". I'm a bit confused by this. Maybe you mean `singularity shell`? So I tried:. ```bash. singularity shell --bind /usr/lib/locale/ DeepVariant_1.6.1.sif. ```. This gets into a shell mode, then I ran:. ```. Singularity> /opt/deepvariant/bin/run_deepvariant \. > --model_type PACBIO \. > --ref reference/GRCh38_no_alt_analysis_set.fasta \. > --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. > --output_vcf deepvariant_output/output.vcf.gz \. > --num_shards $(nproc) \. > --regions chr20. ```. Directly `singularity exec` with the command (just like https:",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:4139,modifiability,bind,bind,4139,". mkdir -p reference. FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai. ```. ```bash. mkdir -p input. HTTPDIR=https://downloads.pacbcloud.com/public/dataset/HG003/deepvariant-case-study. curl ${HTTPDIR}/HG003.GRCh38.chr20.pFDA_truthv2.bam > input/HG003.GRCh38.chr20.pFDA_truthv2.bam. curl ${HTTPDIR}/HG003.GRCh38.chr20.pFDA_truthv2.bam.bai > input/HG003.GRCh38.chr20.pFDA_truthv2.bam.bai. ```. ```bash. ulimit -u 10000. BIN_VERSION=""1.6.1"". mkdir -p deepvariant_output. ```. @Carl-labhub mentioned ""When I run it, I’m doing so from an interactive session with singularity exec"". I'm a bit confused by this. Maybe you mean `singularity shell`? So I tried:. ```bash. singularity shell --bind /usr/lib/locale/ DeepVariant_1.6.1.sif. ```. This gets into a shell mode, then I ran:. ```. Singularity> /opt/deepvariant/bin/run_deepvariant \. > --model_type PACBIO \. > --ref reference/GRCh38_no_alt_analysis_set.fasta \. > --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. > --output_vcf deepvariant_output/output.vcf.gz \. > --num_shards $(nproc) \. > --regions chr20. ```. Directly `singularity exec` with the command (just like https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-pacbio-model-case-study.md) should be fine too. My `make_examples` step completed without any issues. It took:. ```. real 9m7.540s. user 215m38.303s. sys 6m41.297s. ```. I let the whole run finish just to be sure. --> The full run also completed without any errors. @Carl-labhub, can you check what I did above, and see what might be different on your side? And, there were previous GitHub issues that might be worth reading through to see if there are any relevant clues. For example: https:/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:4304,modifiability,PAC,PACBIO,4304,"i.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai. ```. ```bash. mkdir -p input. HTTPDIR=https://downloads.pacbcloud.com/public/dataset/HG003/deepvariant-case-study. curl ${HTTPDIR}/HG003.GRCh38.chr20.pFDA_truthv2.bam > input/HG003.GRCh38.chr20.pFDA_truthv2.bam. curl ${HTTPDIR}/HG003.GRCh38.chr20.pFDA_truthv2.bam.bai > input/HG003.GRCh38.chr20.pFDA_truthv2.bam.bai. ```. ```bash. ulimit -u 10000. BIN_VERSION=""1.6.1"". mkdir -p deepvariant_output. ```. @Carl-labhub mentioned ""When I run it, I’m doing so from an interactive session with singularity exec"". I'm a bit confused by this. Maybe you mean `singularity shell`? So I tried:. ```bash. singularity shell --bind /usr/lib/locale/ DeepVariant_1.6.1.sif. ```. This gets into a shell mode, then I ran:. ```. Singularity> /opt/deepvariant/bin/run_deepvariant \. > --model_type PACBIO \. > --ref reference/GRCh38_no_alt_analysis_set.fasta \. > --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. > --output_vcf deepvariant_output/output.vcf.gz \. > --num_shards $(nproc) \. > --regions chr20. ```. Directly `singularity exec` with the command (just like https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-pacbio-model-case-study.md) should be fine too. My `make_examples` step completed without any issues. It took:. ```. real 9m7.540s. user 215m38.303s. sys 6m41.297s. ```. I let the whole run finish just to be sure. --> The full run also completed without any errors. @Carl-labhub, can you check what I did above, and see what might be different on your side? And, there were previous GitHub issues that might be worth reading through to see if there are any relevant clues. For example: https://github.com/google/deepvariant/issues/677.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:4648,modifiability,pac,pacbio-model-case-study,4648,"i.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai. ```. ```bash. mkdir -p input. HTTPDIR=https://downloads.pacbcloud.com/public/dataset/HG003/deepvariant-case-study. curl ${HTTPDIR}/HG003.GRCh38.chr20.pFDA_truthv2.bam > input/HG003.GRCh38.chr20.pFDA_truthv2.bam. curl ${HTTPDIR}/HG003.GRCh38.chr20.pFDA_truthv2.bam.bai > input/HG003.GRCh38.chr20.pFDA_truthv2.bam.bai. ```. ```bash. ulimit -u 10000. BIN_VERSION=""1.6.1"". mkdir -p deepvariant_output. ```. @Carl-labhub mentioned ""When I run it, I’m doing so from an interactive session with singularity exec"". I'm a bit confused by this. Maybe you mean `singularity shell`? So I tried:. ```bash. singularity shell --bind /usr/lib/locale/ DeepVariant_1.6.1.sif. ```. This gets into a shell mode, then I ran:. ```. Singularity> /opt/deepvariant/bin/run_deepvariant \. > --model_type PACBIO \. > --ref reference/GRCh38_no_alt_analysis_set.fasta \. > --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. > --output_vcf deepvariant_output/output.vcf.gz \. > --num_shards $(nproc) \. > --regions chr20. ```. Directly `singularity exec` with the command (just like https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-pacbio-model-case-study.md) should be fine too. My `make_examples` step completed without any issues. It took:. ```. real 9m7.540s. user 215m38.303s. sys 6m41.297s. ```. I let the whole run finish just to be sure. --> The full run also completed without any errors. @Carl-labhub, can you check what I did above, and see what might be different on your side? And, there were previous GitHub issues that might be worth reading through to see if there are any relevant clues. For example: https://github.com/google/deepvariant/issues/677.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:4906,performance,error,errors,4906,"i.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai. ```. ```bash. mkdir -p input. HTTPDIR=https://downloads.pacbcloud.com/public/dataset/HG003/deepvariant-case-study. curl ${HTTPDIR}/HG003.GRCh38.chr20.pFDA_truthv2.bam > input/HG003.GRCh38.chr20.pFDA_truthv2.bam. curl ${HTTPDIR}/HG003.GRCh38.chr20.pFDA_truthv2.bam.bai > input/HG003.GRCh38.chr20.pFDA_truthv2.bam.bai. ```. ```bash. ulimit -u 10000. BIN_VERSION=""1.6.1"". mkdir -p deepvariant_output. ```. @Carl-labhub mentioned ""When I run it, I’m doing so from an interactive session with singularity exec"". I'm a bit confused by this. Maybe you mean `singularity shell`? So I tried:. ```bash. singularity shell --bind /usr/lib/locale/ DeepVariant_1.6.1.sif. ```. This gets into a shell mode, then I ran:. ```. Singularity> /opt/deepvariant/bin/run_deepvariant \. > --model_type PACBIO \. > --ref reference/GRCh38_no_alt_analysis_set.fasta \. > --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. > --output_vcf deepvariant_output/output.vcf.gz \. > --num_shards $(nproc) \. > --regions chr20. ```. Directly `singularity exec` with the command (just like https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-pacbio-model-case-study.md) should be fine too. My `make_examples` step completed without any issues. It took:. ```. real 9m7.540s. user 215m38.303s. sys 6m41.297s. ```. I let the whole run finish just to be sure. --> The full run also completed without any errors. @Carl-labhub, can you check what I did above, and see what might be different on your side? And, there were previous GitHub issues that might be worth reading through to see if there are any relevant clues. For example: https://github.com/google/deepvariant/issues/677.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:111,safety,test,test,111,"# Attempt to reproduce the issue. ## Get a machine with the same Linux version. I used GCP to get a machine to test. Hopefully this provides a similar environment:. `gcloud compute instances create ""${USER}-test"" --scopes ""compute-rw,storage-full,cloud-platform"" --image-family almalinux-9 --image-project almalinux-cloud --machine-type ""n1-standard-64"" --zone ""us-west1-b""`. I ssh'ed into the machine:. ```bash. gcloud compute ssh pichuan-test --zone ""us-west1-b"". ```. Check the Linux version:. ```. $ uname -a. Linux pichuan-test.us-west1-b.c.brain-genomics.google.com.internal 5.14.0-362.24.2.el9_3.x86_64 #1 SMP PREEMPT_DYNAMIC Sat Mar 30 14:11:54 EDT 2024 x86_64 x86_64 x86_64 GNU/Linux. ```. And I ran this too:. ```. $ cat /etc/os-release. NAME=""AlmaLinux"". VERSION=""9.3 (Shamrock Pampas Cat)"". ID=""almalinux"". ID_LIKE=""rhel centos fedora"". VERSION_ID=""9.3"". PLATFORM_ID=""platform:el9"". PRETTY_NAME=""AlmaLinux 9.3 (Shamrock Pampas Cat)"". ANSI_COLOR=""0;34"". LOGO=""fedora-logo-icon"". CPE_NAME=""cpe:/o:almalinux:almalinux:9::baseos"". HOME_URL=""https://almalinux.org/"". DOCUMENTATION_URL=""https://wiki.almalinux.org/"". BUG_REPORT_URL=""https://bugs.almalinux.org/"". ALMALINUX_MANTISBT_PROJECT=""AlmaLinux-9"". ALMALINUX_MANTISBT_PROJECT_VERSION=""9.3"". REDHAT_SUPPORT_PRODUCT=""AlmaLinux"". REDHAT_SUPPORT_PRODUCT_VERSION=""9.3"". ```. ## Install Singularity. I don't have Singularity on the machine yet, so:. https://docs.sylabs.io/guides/4.1/user-guide/quick_start.html#quick-installation-steps. ```bash. sudo yum update -y && \. sudo yum groupinstall -y 'Development Tools' && \. sudo yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools. ```. ```. sudo yum groupinstall -y 'Development Tools'. # Install RPM packages for dependencies. sudo yum install -y \. autoconf \. automake \. cryptsetup \. fuse3-devel \. git \. glib2-devel \. libseccomp-devel \. libtool \. runc \. squashfs-tools \. wget \. zlib-devel. ```. ```bash. sudo dnf install dnf-plugins-core.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:207,safety,test,test,207,"# Attempt to reproduce the issue. ## Get a machine with the same Linux version. I used GCP to get a machine to test. Hopefully this provides a similar environment:. `gcloud compute instances create ""${USER}-test"" --scopes ""compute-rw,storage-full,cloud-platform"" --image-family almalinux-9 --image-project almalinux-cloud --machine-type ""n1-standard-64"" --zone ""us-west1-b""`. I ssh'ed into the machine:. ```bash. gcloud compute ssh pichuan-test --zone ""us-west1-b"". ```. Check the Linux version:. ```. $ uname -a. Linux pichuan-test.us-west1-b.c.brain-genomics.google.com.internal 5.14.0-362.24.2.el9_3.x86_64 #1 SMP PREEMPT_DYNAMIC Sat Mar 30 14:11:54 EDT 2024 x86_64 x86_64 x86_64 GNU/Linux. ```. And I ran this too:. ```. $ cat /etc/os-release. NAME=""AlmaLinux"". VERSION=""9.3 (Shamrock Pampas Cat)"". ID=""almalinux"". ID_LIKE=""rhel centos fedora"". VERSION_ID=""9.3"". PLATFORM_ID=""platform:el9"". PRETTY_NAME=""AlmaLinux 9.3 (Shamrock Pampas Cat)"". ANSI_COLOR=""0;34"". LOGO=""fedora-logo-icon"". CPE_NAME=""cpe:/o:almalinux:almalinux:9::baseos"". HOME_URL=""https://almalinux.org/"". DOCUMENTATION_URL=""https://wiki.almalinux.org/"". BUG_REPORT_URL=""https://bugs.almalinux.org/"". ALMALINUX_MANTISBT_PROJECT=""AlmaLinux-9"". ALMALINUX_MANTISBT_PROJECT_VERSION=""9.3"". REDHAT_SUPPORT_PRODUCT=""AlmaLinux"". REDHAT_SUPPORT_PRODUCT_VERSION=""9.3"". ```. ## Install Singularity. I don't have Singularity on the machine yet, so:. https://docs.sylabs.io/guides/4.1/user-guide/quick_start.html#quick-installation-steps. ```bash. sudo yum update -y && \. sudo yum groupinstall -y 'Development Tools' && \. sudo yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools. ```. ```. sudo yum groupinstall -y 'Development Tools'. # Install RPM packages for dependencies. sudo yum install -y \. autoconf \. automake \. cryptsetup \. fuse3-devel \. git \. glib2-devel \. libseccomp-devel \. libtool \. runc \. squashfs-tools \. wget \. zlib-devel. ```. ```bash. sudo dnf install dnf-plugins-core.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:440,safety,test,test,440,"# Attempt to reproduce the issue. ## Get a machine with the same Linux version. I used GCP to get a machine to test. Hopefully this provides a similar environment:. `gcloud compute instances create ""${USER}-test"" --scopes ""compute-rw,storage-full,cloud-platform"" --image-family almalinux-9 --image-project almalinux-cloud --machine-type ""n1-standard-64"" --zone ""us-west1-b""`. I ssh'ed into the machine:. ```bash. gcloud compute ssh pichuan-test --zone ""us-west1-b"". ```. Check the Linux version:. ```. $ uname -a. Linux pichuan-test.us-west1-b.c.brain-genomics.google.com.internal 5.14.0-362.24.2.el9_3.x86_64 #1 SMP PREEMPT_DYNAMIC Sat Mar 30 14:11:54 EDT 2024 x86_64 x86_64 x86_64 GNU/Linux. ```. And I ran this too:. ```. $ cat /etc/os-release. NAME=""AlmaLinux"". VERSION=""9.3 (Shamrock Pampas Cat)"". ID=""almalinux"". ID_LIKE=""rhel centos fedora"". VERSION_ID=""9.3"". PLATFORM_ID=""platform:el9"". PRETTY_NAME=""AlmaLinux 9.3 (Shamrock Pampas Cat)"". ANSI_COLOR=""0;34"". LOGO=""fedora-logo-icon"". CPE_NAME=""cpe:/o:almalinux:almalinux:9::baseos"". HOME_URL=""https://almalinux.org/"". DOCUMENTATION_URL=""https://wiki.almalinux.org/"". BUG_REPORT_URL=""https://bugs.almalinux.org/"". ALMALINUX_MANTISBT_PROJECT=""AlmaLinux-9"". ALMALINUX_MANTISBT_PROJECT_VERSION=""9.3"". REDHAT_SUPPORT_PRODUCT=""AlmaLinux"". REDHAT_SUPPORT_PRODUCT_VERSION=""9.3"". ```. ## Install Singularity. I don't have Singularity on the machine yet, so:. https://docs.sylabs.io/guides/4.1/user-guide/quick_start.html#quick-installation-steps. ```bash. sudo yum update -y && \. sudo yum groupinstall -y 'Development Tools' && \. sudo yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools. ```. ```. sudo yum groupinstall -y 'Development Tools'. # Install RPM packages for dependencies. sudo yum install -y \. autoconf \. automake \. cryptsetup \. fuse3-devel \. git \. glib2-devel \. libseccomp-devel \. libtool \. runc \. squashfs-tools \. wget \. zlib-devel. ```. ```bash. sudo dnf install dnf-plugins-core.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:528,safety,test,test,528,"# Attempt to reproduce the issue. ## Get a machine with the same Linux version. I used GCP to get a machine to test. Hopefully this provides a similar environment:. `gcloud compute instances create ""${USER}-test"" --scopes ""compute-rw,storage-full,cloud-platform"" --image-family almalinux-9 --image-project almalinux-cloud --machine-type ""n1-standard-64"" --zone ""us-west1-b""`. I ssh'ed into the machine:. ```bash. gcloud compute ssh pichuan-test --zone ""us-west1-b"". ```. Check the Linux version:. ```. $ uname -a. Linux pichuan-test.us-west1-b.c.brain-genomics.google.com.internal 5.14.0-362.24.2.el9_3.x86_64 #1 SMP PREEMPT_DYNAMIC Sat Mar 30 14:11:54 EDT 2024 x86_64 x86_64 x86_64 GNU/Linux. ```. And I ran this too:. ```. $ cat /etc/os-release. NAME=""AlmaLinux"". VERSION=""9.3 (Shamrock Pampas Cat)"". ID=""almalinux"". ID_LIKE=""rhel centos fedora"". VERSION_ID=""9.3"". PLATFORM_ID=""platform:el9"". PRETTY_NAME=""AlmaLinux 9.3 (Shamrock Pampas Cat)"". ANSI_COLOR=""0;34"". LOGO=""fedora-logo-icon"". CPE_NAME=""cpe:/o:almalinux:almalinux:9::baseos"". HOME_URL=""https://almalinux.org/"". DOCUMENTATION_URL=""https://wiki.almalinux.org/"". BUG_REPORT_URL=""https://bugs.almalinux.org/"". ALMALINUX_MANTISBT_PROJECT=""AlmaLinux-9"". ALMALINUX_MANTISBT_PROJECT_VERSION=""9.3"". REDHAT_SUPPORT_PRODUCT=""AlmaLinux"". REDHAT_SUPPORT_PRODUCT_VERSION=""9.3"". ```. ## Install Singularity. I don't have Singularity on the machine yet, so:. https://docs.sylabs.io/guides/4.1/user-guide/quick_start.html#quick-installation-steps. ```bash. sudo yum update -y && \. sudo yum groupinstall -y 'Development Tools' && \. sudo yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools. ```. ```. sudo yum groupinstall -y 'Development Tools'. # Install RPM packages for dependencies. sudo yum install -y \. autoconf \. automake \. cryptsetup \. fuse3-devel \. git \. glib2-devel \. libseccomp-devel \. libtool \. runc \. squashfs-tools \. wget \. zlib-devel. ```. ```bash. sudo dnf install dnf-plugins-core.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:965,safety,LOG,LOGO,965,"# Attempt to reproduce the issue. ## Get a machine with the same Linux version. I used GCP to get a machine to test. Hopefully this provides a similar environment:. `gcloud compute instances create ""${USER}-test"" --scopes ""compute-rw,storage-full,cloud-platform"" --image-family almalinux-9 --image-project almalinux-cloud --machine-type ""n1-standard-64"" --zone ""us-west1-b""`. I ssh'ed into the machine:. ```bash. gcloud compute ssh pichuan-test --zone ""us-west1-b"". ```. Check the Linux version:. ```. $ uname -a. Linux pichuan-test.us-west1-b.c.brain-genomics.google.com.internal 5.14.0-362.24.2.el9_3.x86_64 #1 SMP PREEMPT_DYNAMIC Sat Mar 30 14:11:54 EDT 2024 x86_64 x86_64 x86_64 GNU/Linux. ```. And I ran this too:. ```. $ cat /etc/os-release. NAME=""AlmaLinux"". VERSION=""9.3 (Shamrock Pampas Cat)"". ID=""almalinux"". ID_LIKE=""rhel centos fedora"". VERSION_ID=""9.3"". PLATFORM_ID=""platform:el9"". PRETTY_NAME=""AlmaLinux 9.3 (Shamrock Pampas Cat)"". ANSI_COLOR=""0;34"". LOGO=""fedora-logo-icon"". CPE_NAME=""cpe:/o:almalinux:almalinux:9::baseos"". HOME_URL=""https://almalinux.org/"". DOCUMENTATION_URL=""https://wiki.almalinux.org/"". BUG_REPORT_URL=""https://bugs.almalinux.org/"". ALMALINUX_MANTISBT_PROJECT=""AlmaLinux-9"". ALMALINUX_MANTISBT_PROJECT_VERSION=""9.3"". REDHAT_SUPPORT_PRODUCT=""AlmaLinux"". REDHAT_SUPPORT_PRODUCT_VERSION=""9.3"". ```. ## Install Singularity. I don't have Singularity on the machine yet, so:. https://docs.sylabs.io/guides/4.1/user-guide/quick_start.html#quick-installation-steps. ```bash. sudo yum update -y && \. sudo yum groupinstall -y 'Development Tools' && \. sudo yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools. ```. ```. sudo yum groupinstall -y 'Development Tools'. # Install RPM packages for dependencies. sudo yum install -y \. autoconf \. automake \. cryptsetup \. fuse3-devel \. git \. glib2-devel \. libseccomp-devel \. libtool \. runc \. squashfs-tools \. wget \. zlib-devel. ```. ```bash. sudo dnf install dnf-plugins-core.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:978,safety,log,logo-icon,978,"# Attempt to reproduce the issue. ## Get a machine with the same Linux version. I used GCP to get a machine to test. Hopefully this provides a similar environment:. `gcloud compute instances create ""${USER}-test"" --scopes ""compute-rw,storage-full,cloud-platform"" --image-family almalinux-9 --image-project almalinux-cloud --machine-type ""n1-standard-64"" --zone ""us-west1-b""`. I ssh'ed into the machine:. ```bash. gcloud compute ssh pichuan-test --zone ""us-west1-b"". ```. Check the Linux version:. ```. $ uname -a. Linux pichuan-test.us-west1-b.c.brain-genomics.google.com.internal 5.14.0-362.24.2.el9_3.x86_64 #1 SMP PREEMPT_DYNAMIC Sat Mar 30 14:11:54 EDT 2024 x86_64 x86_64 x86_64 GNU/Linux. ```. And I ran this too:. ```. $ cat /etc/os-release. NAME=""AlmaLinux"". VERSION=""9.3 (Shamrock Pampas Cat)"". ID=""almalinux"". ID_LIKE=""rhel centos fedora"". VERSION_ID=""9.3"". PLATFORM_ID=""platform:el9"". PRETTY_NAME=""AlmaLinux 9.3 (Shamrock Pampas Cat)"". ANSI_COLOR=""0;34"". LOGO=""fedora-logo-icon"". CPE_NAME=""cpe:/o:almalinux:almalinux:9::baseos"". HOME_URL=""https://almalinux.org/"". DOCUMENTATION_URL=""https://wiki.almalinux.org/"". BUG_REPORT_URL=""https://bugs.almalinux.org/"". ALMALINUX_MANTISBT_PROJECT=""AlmaLinux-9"". ALMALINUX_MANTISBT_PROJECT_VERSION=""9.3"". REDHAT_SUPPORT_PRODUCT=""AlmaLinux"". REDHAT_SUPPORT_PRODUCT_VERSION=""9.3"". ```. ## Install Singularity. I don't have Singularity on the machine yet, so:. https://docs.sylabs.io/guides/4.1/user-guide/quick_start.html#quick-installation-steps. ```bash. sudo yum update -y && \. sudo yum groupinstall -y 'Development Tools' && \. sudo yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools. ```. ```. sudo yum groupinstall -y 'Development Tools'. # Install RPM packages for dependencies. sudo yum install -y \. autoconf \. automake \. cryptsetup \. fuse3-devel \. git \. glib2-devel \. libseccomp-devel \. libtool \. runc \. squashfs-tools \. wget \. zlib-devel. ```. ```bash. sudo dnf install dnf-plugins-core.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:1512,safety,updat,update,1512,"inux pichuan-test.us-west1-b.c.brain-genomics.google.com.internal 5.14.0-362.24.2.el9_3.x86_64 #1 SMP PREEMPT_DYNAMIC Sat Mar 30 14:11:54 EDT 2024 x86_64 x86_64 x86_64 GNU/Linux. ```. And I ran this too:. ```. $ cat /etc/os-release. NAME=""AlmaLinux"". VERSION=""9.3 (Shamrock Pampas Cat)"". ID=""almalinux"". ID_LIKE=""rhel centos fedora"". VERSION_ID=""9.3"". PLATFORM_ID=""platform:el9"". PRETTY_NAME=""AlmaLinux 9.3 (Shamrock Pampas Cat)"". ANSI_COLOR=""0;34"". LOGO=""fedora-logo-icon"". CPE_NAME=""cpe:/o:almalinux:almalinux:9::baseos"". HOME_URL=""https://almalinux.org/"". DOCUMENTATION_URL=""https://wiki.almalinux.org/"". BUG_REPORT_URL=""https://bugs.almalinux.org/"". ALMALINUX_MANTISBT_PROJECT=""AlmaLinux-9"". ALMALINUX_MANTISBT_PROJECT_VERSION=""9.3"". REDHAT_SUPPORT_PRODUCT=""AlmaLinux"". REDHAT_SUPPORT_PRODUCT_VERSION=""9.3"". ```. ## Install Singularity. I don't have Singularity on the machine yet, so:. https://docs.sylabs.io/guides/4.1/user-guide/quick_start.html#quick-installation-steps. ```bash. sudo yum update -y && \. sudo yum groupinstall -y 'Development Tools' && \. sudo yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools. ```. ```. sudo yum groupinstall -y 'Development Tools'. # Install RPM packages for dependencies. sudo yum install -y \. autoconf \. automake \. cryptsetup \. fuse3-devel \. git \. glib2-devel \. libseccomp-devel \. libtool \. runc \. squashfs-tools \. wget \. zlib-devel. ```. ```bash. sudo dnf install dnf-plugins-core. sudo dnf copr enable dctrud/squashfs-tools-ng. sudo dnf install squashfs-tools-ng. ```. ```bash. export VERSION=1.21.0 OS=linux ARCH=amd64 && \. wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz && \. sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz && \. rm go$VERSION.$OS-$ARCH.tar.gz. ```. ```bash. echo 'export PATH=/usr/local/go/bin:$PATH' >> ~/.bashrc && \. source ~/.bashrc. ```. ```bash. export VERSION=4.1.0 && \. wget https://github.com/sylabs/singularity/releases/download/v${VERSION}/si",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:1763,safety,depend,dependencies,1763,"SION=""9.3 (Shamrock Pampas Cat)"". ID=""almalinux"". ID_LIKE=""rhel centos fedora"". VERSION_ID=""9.3"". PLATFORM_ID=""platform:el9"". PRETTY_NAME=""AlmaLinux 9.3 (Shamrock Pampas Cat)"". ANSI_COLOR=""0;34"". LOGO=""fedora-logo-icon"". CPE_NAME=""cpe:/o:almalinux:almalinux:9::baseos"". HOME_URL=""https://almalinux.org/"". DOCUMENTATION_URL=""https://wiki.almalinux.org/"". BUG_REPORT_URL=""https://bugs.almalinux.org/"". ALMALINUX_MANTISBT_PROJECT=""AlmaLinux-9"". ALMALINUX_MANTISBT_PROJECT_VERSION=""9.3"". REDHAT_SUPPORT_PRODUCT=""AlmaLinux"". REDHAT_SUPPORT_PRODUCT_VERSION=""9.3"". ```. ## Install Singularity. I don't have Singularity on the machine yet, so:. https://docs.sylabs.io/guides/4.1/user-guide/quick_start.html#quick-installation-steps. ```bash. sudo yum update -y && \. sudo yum groupinstall -y 'Development Tools' && \. sudo yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools. ```. ```. sudo yum groupinstall -y 'Development Tools'. # Install RPM packages for dependencies. sudo yum install -y \. autoconf \. automake \. cryptsetup \. fuse3-devel \. git \. glib2-devel \. libseccomp-devel \. libtool \. runc \. squashfs-tools \. wget \. zlib-devel. ```. ```bash. sudo dnf install dnf-plugins-core. sudo dnf copr enable dctrud/squashfs-tools-ng. sudo dnf install squashfs-tools-ng. ```. ```bash. export VERSION=1.21.0 OS=linux ARCH=amd64 && \. wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz && \. sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz && \. rm go$VERSION.$OS-$ARCH.tar.gz. ```. ```bash. echo 'export PATH=/usr/local/go/bin:$PATH' >> ~/.bashrc && \. source ~/.bashrc. ```. ```bash. export VERSION=4.1.0 && \. wget https://github.com/sylabs/singularity/releases/download/v${VERSION}/singularity-ce-${VERSION}.tar.gz && \. tar -xzf singularity-ce-${VERSION}.tar.gz && \. cd singularity-ce-${VERSION}. ```. ```bash. ./mconfig && \. make -C builddir && \. sudo make -C builddir install. ```. At this point, I have singularity installed. ```ba",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:3112,safety,test,test,3112,".21.0 OS=linux ARCH=amd64 && \. wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz && \. sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz && \. rm go$VERSION.$OS-$ARCH.tar.gz. ```. ```bash. echo 'export PATH=/usr/local/go/bin:$PATH' >> ~/.bashrc && \. source ~/.bashrc. ```. ```bash. export VERSION=4.1.0 && \. wget https://github.com/sylabs/singularity/releases/download/v${VERSION}/singularity-ce-${VERSION}.tar.gz && \. tar -xzf singularity-ce-${VERSION}.tar.gz && \. cd singularity-ce-${VERSION}. ```. ```bash. ./mconfig && \. make -C builddir && \. sudo make -C builddir install. ```. At this point, I have singularity installed. ```bash. $ singularity --version. singularity-ce version 4.1.0. ```. ## Get data and run DeepVariant. Now, let me try to follow similar steps:. ```bash. singularity build DeepVariant_1.6.1.sif docker://google/deepvariant:1.6.1. ```. From here, I used https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-pacbio-model-case-study.md to test. Download data:. ```bash. mkdir -p reference. FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai. ```. ```bash. mkdir -p input. HTTPDIR=https://downloads.pacbcloud.com/public/dataset/HG003/deepvariant-case-study. curl ${HTTPDIR}/HG003.GRCh38.chr20.pFDA_truthv2.bam > input/HG003.GRCh38.chr20.pFDA_truthv2.bam. curl ${HTTPDIR}/HG003.GRCh38.chr20.pFDA_truthv2.bam.bai > input/HG003.GRCh38.chr20.pFDA_truthv2.bam.bai. ```. ```bash. ulimit -u 10000. BIN_VERSION=""1.6.1"". mkdir -p deepvariant_output. ```. @Carl-labhub mentioned ""When I run it, I’m doing so from an interactive session with singularity exec"". I'm a bit confused by this. Maybe you mean `singularity shell`? So I tried:. ```b",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:3549,safety,input,input,3549,"tar -xzf singularity-ce-${VERSION}.tar.gz && \. cd singularity-ce-${VERSION}. ```. ```bash. ./mconfig && \. make -C builddir && \. sudo make -C builddir install. ```. At this point, I have singularity installed. ```bash. $ singularity --version. singularity-ce version 4.1.0. ```. ## Get data and run DeepVariant. Now, let me try to follow similar steps:. ```bash. singularity build DeepVariant_1.6.1.sif docker://google/deepvariant:1.6.1. ```. From here, I used https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-pacbio-model-case-study.md to test. Download data:. ```bash. mkdir -p reference. FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai. ```. ```bash. mkdir -p input. HTTPDIR=https://downloads.pacbcloud.com/public/dataset/HG003/deepvariant-case-study. curl ${HTTPDIR}/HG003.GRCh38.chr20.pFDA_truthv2.bam > input/HG003.GRCh38.chr20.pFDA_truthv2.bam. curl ${HTTPDIR}/HG003.GRCh38.chr20.pFDA_truthv2.bam.bai > input/HG003.GRCh38.chr20.pFDA_truthv2.bam.bai. ```. ```bash. ulimit -u 10000. BIN_VERSION=""1.6.1"". mkdir -p deepvariant_output. ```. @Carl-labhub mentioned ""When I run it, I’m doing so from an interactive session with singularity exec"". I'm a bit confused by this. Maybe you mean `singularity shell`? So I tried:. ```bash. singularity shell --bind /usr/lib/locale/ DeepVariant_1.6.1.sif. ```. This gets into a shell mode, then I ran:. ```. Singularity> /opt/deepvariant/bin/run_deepvariant \. > --model_type PACBIO \. > --ref reference/GRCh38_no_alt_analysis_set.fasta \. > --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. > --output_vcf deepvariant_output/output.vcf.gz \. > --num_shards $(nproc) \. > --regions chr20. ```. Directly `singularity exec`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:3695,safety,input,input,3695,"ilddir install. ```. At this point, I have singularity installed. ```bash. $ singularity --version. singularity-ce version 4.1.0. ```. ## Get data and run DeepVariant. Now, let me try to follow similar steps:. ```bash. singularity build DeepVariant_1.6.1.sif docker://google/deepvariant:1.6.1. ```. From here, I used https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-pacbio-model-case-study.md to test. Download data:. ```bash. mkdir -p reference. FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai. ```. ```bash. mkdir -p input. HTTPDIR=https://downloads.pacbcloud.com/public/dataset/HG003/deepvariant-case-study. curl ${HTTPDIR}/HG003.GRCh38.chr20.pFDA_truthv2.bam > input/HG003.GRCh38.chr20.pFDA_truthv2.bam. curl ${HTTPDIR}/HG003.GRCh38.chr20.pFDA_truthv2.bam.bai > input/HG003.GRCh38.chr20.pFDA_truthv2.bam.bai. ```. ```bash. ulimit -u 10000. BIN_VERSION=""1.6.1"". mkdir -p deepvariant_output. ```. @Carl-labhub mentioned ""When I run it, I’m doing so from an interactive session with singularity exec"". I'm a bit confused by this. Maybe you mean `singularity shell`? So I tried:. ```bash. singularity shell --bind /usr/lib/locale/ DeepVariant_1.6.1.sif. ```. This gets into a shell mode, then I ran:. ```. Singularity> /opt/deepvariant/bin/run_deepvariant \. > --model_type PACBIO \. > --ref reference/GRCh38_no_alt_analysis_set.fasta \. > --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. > --output_vcf deepvariant_output/output.vcf.gz \. > --num_shards $(nproc) \. > --regions chr20. ```. Directly `singularity exec` with the command (just like https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-pacbio-model-case-study.md) should be fine too. My",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:3796,safety,input,input,3796,"ingularity-ce version 4.1.0. ```. ## Get data and run DeepVariant. Now, let me try to follow similar steps:. ```bash. singularity build DeepVariant_1.6.1.sif docker://google/deepvariant:1.6.1. ```. From here, I used https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-pacbio-model-case-study.md to test. Download data:. ```bash. mkdir -p reference. FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai. ```. ```bash. mkdir -p input. HTTPDIR=https://downloads.pacbcloud.com/public/dataset/HG003/deepvariant-case-study. curl ${HTTPDIR}/HG003.GRCh38.chr20.pFDA_truthv2.bam > input/HG003.GRCh38.chr20.pFDA_truthv2.bam. curl ${HTTPDIR}/HG003.GRCh38.chr20.pFDA_truthv2.bam.bai > input/HG003.GRCh38.chr20.pFDA_truthv2.bam.bai. ```. ```bash. ulimit -u 10000. BIN_VERSION=""1.6.1"". mkdir -p deepvariant_output. ```. @Carl-labhub mentioned ""When I run it, I’m doing so from an interactive session with singularity exec"". I'm a bit confused by this. Maybe you mean `singularity shell`? So I tried:. ```bash. singularity shell --bind /usr/lib/locale/ DeepVariant_1.6.1.sif. ```. This gets into a shell mode, then I ran:. ```. Singularity> /opt/deepvariant/bin/run_deepvariant \. > --model_type PACBIO \. > --ref reference/GRCh38_no_alt_analysis_set.fasta \. > --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. > --output_vcf deepvariant_output/output.vcf.gz \. > --num_shards $(nproc) \. > --regions chr20. ```. Directly `singularity exec` with the command (just like https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-pacbio-model-case-study.md) should be fine too. My `make_examples` step completed without any issues. It took:. ```. real 9m7.540s. user 215m38.303s. s",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:4378,safety,input,input,4378,"i.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai. ```. ```bash. mkdir -p input. HTTPDIR=https://downloads.pacbcloud.com/public/dataset/HG003/deepvariant-case-study. curl ${HTTPDIR}/HG003.GRCh38.chr20.pFDA_truthv2.bam > input/HG003.GRCh38.chr20.pFDA_truthv2.bam. curl ${HTTPDIR}/HG003.GRCh38.chr20.pFDA_truthv2.bam.bai > input/HG003.GRCh38.chr20.pFDA_truthv2.bam.bai. ```. ```bash. ulimit -u 10000. BIN_VERSION=""1.6.1"". mkdir -p deepvariant_output. ```. @Carl-labhub mentioned ""When I run it, I’m doing so from an interactive session with singularity exec"". I'm a bit confused by this. Maybe you mean `singularity shell`? So I tried:. ```bash. singularity shell --bind /usr/lib/locale/ DeepVariant_1.6.1.sif. ```. This gets into a shell mode, then I ran:. ```. Singularity> /opt/deepvariant/bin/run_deepvariant \. > --model_type PACBIO \. > --ref reference/GRCh38_no_alt_analysis_set.fasta \. > --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. > --output_vcf deepvariant_output/output.vcf.gz \. > --num_shards $(nproc) \. > --regions chr20. ```. Directly `singularity exec` with the command (just like https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-pacbio-model-case-study.md) should be fine too. My `make_examples` step completed without any issues. It took:. ```. real 9m7.540s. user 215m38.303s. sys 6m41.297s. ```. I let the whole run finish just to be sure. --> The full run also completed without any errors. @Carl-labhub, can you check what I did above, and see what might be different on your side? And, there were previous GitHub issues that might be worth reading through to see if there are any relevant clues. For example: https://github.com/google/deepvariant/issues/677.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:4720,safety,compl,completed,4720,"i.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai. ```. ```bash. mkdir -p input. HTTPDIR=https://downloads.pacbcloud.com/public/dataset/HG003/deepvariant-case-study. curl ${HTTPDIR}/HG003.GRCh38.chr20.pFDA_truthv2.bam > input/HG003.GRCh38.chr20.pFDA_truthv2.bam. curl ${HTTPDIR}/HG003.GRCh38.chr20.pFDA_truthv2.bam.bai > input/HG003.GRCh38.chr20.pFDA_truthv2.bam.bai. ```. ```bash. ulimit -u 10000. BIN_VERSION=""1.6.1"". mkdir -p deepvariant_output. ```. @Carl-labhub mentioned ""When I run it, I’m doing so from an interactive session with singularity exec"". I'm a bit confused by this. Maybe you mean `singularity shell`? So I tried:. ```bash. singularity shell --bind /usr/lib/locale/ DeepVariant_1.6.1.sif. ```. This gets into a shell mode, then I ran:. ```. Singularity> /opt/deepvariant/bin/run_deepvariant \. > --model_type PACBIO \. > --ref reference/GRCh38_no_alt_analysis_set.fasta \. > --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. > --output_vcf deepvariant_output/output.vcf.gz \. > --num_shards $(nproc) \. > --regions chr20. ```. Directly `singularity exec` with the command (just like https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-pacbio-model-case-study.md) should be fine too. My `make_examples` step completed without any issues. It took:. ```. real 9m7.540s. user 215m38.303s. sys 6m41.297s. ```. I let the whole run finish just to be sure. --> The full run also completed without any errors. @Carl-labhub, can you check what I did above, and see what might be different on your side? And, there were previous GitHub issues that might be worth reading through to see if there are any relevant clues. For example: https://github.com/google/deepvariant/issues/677.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:4884,safety,compl,completed,4884,"i.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai. ```. ```bash. mkdir -p input. HTTPDIR=https://downloads.pacbcloud.com/public/dataset/HG003/deepvariant-case-study. curl ${HTTPDIR}/HG003.GRCh38.chr20.pFDA_truthv2.bam > input/HG003.GRCh38.chr20.pFDA_truthv2.bam. curl ${HTTPDIR}/HG003.GRCh38.chr20.pFDA_truthv2.bam.bai > input/HG003.GRCh38.chr20.pFDA_truthv2.bam.bai. ```. ```bash. ulimit -u 10000. BIN_VERSION=""1.6.1"". mkdir -p deepvariant_output. ```. @Carl-labhub mentioned ""When I run it, I’m doing so from an interactive session with singularity exec"". I'm a bit confused by this. Maybe you mean `singularity shell`? So I tried:. ```bash. singularity shell --bind /usr/lib/locale/ DeepVariant_1.6.1.sif. ```. This gets into a shell mode, then I ran:. ```. Singularity> /opt/deepvariant/bin/run_deepvariant \. > --model_type PACBIO \. > --ref reference/GRCh38_no_alt_analysis_set.fasta \. > --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. > --output_vcf deepvariant_output/output.vcf.gz \. > --num_shards $(nproc) \. > --regions chr20. ```. Directly `singularity exec` with the command (just like https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-pacbio-model-case-study.md) should be fine too. My `make_examples` step completed without any issues. It took:. ```. real 9m7.540s. user 215m38.303s. sys 6m41.297s. ```. I let the whole run finish just to be sure. --> The full run also completed without any errors. @Carl-labhub, can you check what I did above, and see what might be different on your side? And, there were previous GitHub issues that might be worth reading through to see if there are any relevant clues. For example: https://github.com/google/deepvariant/issues/677.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:4906,safety,error,errors,4906,"i.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai. ```. ```bash. mkdir -p input. HTTPDIR=https://downloads.pacbcloud.com/public/dataset/HG003/deepvariant-case-study. curl ${HTTPDIR}/HG003.GRCh38.chr20.pFDA_truthv2.bam > input/HG003.GRCh38.chr20.pFDA_truthv2.bam. curl ${HTTPDIR}/HG003.GRCh38.chr20.pFDA_truthv2.bam.bai > input/HG003.GRCh38.chr20.pFDA_truthv2.bam.bai. ```. ```bash. ulimit -u 10000. BIN_VERSION=""1.6.1"". mkdir -p deepvariant_output. ```. @Carl-labhub mentioned ""When I run it, I’m doing so from an interactive session with singularity exec"". I'm a bit confused by this. Maybe you mean `singularity shell`? So I tried:. ```bash. singularity shell --bind /usr/lib/locale/ DeepVariant_1.6.1.sif. ```. This gets into a shell mode, then I ran:. ```. Singularity> /opt/deepvariant/bin/run_deepvariant \. > --model_type PACBIO \. > --ref reference/GRCh38_no_alt_analysis_set.fasta \. > --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. > --output_vcf deepvariant_output/output.vcf.gz \. > --num_shards $(nproc) \. > --regions chr20. ```. Directly `singularity exec` with the command (just like https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-pacbio-model-case-study.md) should be fine too. My `make_examples` step completed without any issues. It took:. ```. real 9m7.540s. user 215m38.303s. sys 6m41.297s. ```. I let the whole run finish just to be sure. --> The full run also completed without any errors. @Carl-labhub, can you check what I did above, and see what might be different on your side? And, there were previous GitHub issues that might be worth reading through to see if there are any relevant clues. For example: https://github.com/google/deepvariant/issues/677.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:378,security,ssh,ssh,378,"# Attempt to reproduce the issue. ## Get a machine with the same Linux version. I used GCP to get a machine to test. Hopefully this provides a similar environment:. `gcloud compute instances create ""${USER}-test"" --scopes ""compute-rw,storage-full,cloud-platform"" --image-family almalinux-9 --image-project almalinux-cloud --machine-type ""n1-standard-64"" --zone ""us-west1-b""`. I ssh'ed into the machine:. ```bash. gcloud compute ssh pichuan-test --zone ""us-west1-b"". ```. Check the Linux version:. ```. $ uname -a. Linux pichuan-test.us-west1-b.c.brain-genomics.google.com.internal 5.14.0-362.24.2.el9_3.x86_64 #1 SMP PREEMPT_DYNAMIC Sat Mar 30 14:11:54 EDT 2024 x86_64 x86_64 x86_64 GNU/Linux. ```. And I ran this too:. ```. $ cat /etc/os-release. NAME=""AlmaLinux"". VERSION=""9.3 (Shamrock Pampas Cat)"". ID=""almalinux"". ID_LIKE=""rhel centos fedora"". VERSION_ID=""9.3"". PLATFORM_ID=""platform:el9"". PRETTY_NAME=""AlmaLinux 9.3 (Shamrock Pampas Cat)"". ANSI_COLOR=""0;34"". LOGO=""fedora-logo-icon"". CPE_NAME=""cpe:/o:almalinux:almalinux:9::baseos"". HOME_URL=""https://almalinux.org/"". DOCUMENTATION_URL=""https://wiki.almalinux.org/"". BUG_REPORT_URL=""https://bugs.almalinux.org/"". ALMALINUX_MANTISBT_PROJECT=""AlmaLinux-9"". ALMALINUX_MANTISBT_PROJECT_VERSION=""9.3"". REDHAT_SUPPORT_PRODUCT=""AlmaLinux"". REDHAT_SUPPORT_PRODUCT_VERSION=""9.3"". ```. ## Install Singularity. I don't have Singularity on the machine yet, so:. https://docs.sylabs.io/guides/4.1/user-guide/quick_start.html#quick-installation-steps. ```bash. sudo yum update -y && \. sudo yum groupinstall -y 'Development Tools' && \. sudo yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools. ```. ```. sudo yum groupinstall -y 'Development Tools'. # Install RPM packages for dependencies. sudo yum install -y \. autoconf \. automake \. cryptsetup \. fuse3-devel \. git \. glib2-devel \. libseccomp-devel \. libtool \. runc \. squashfs-tools \. wget \. zlib-devel. ```. ```bash. sudo dnf install dnf-plugins-core.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:428,security,ssh,ssh,428,"# Attempt to reproduce the issue. ## Get a machine with the same Linux version. I used GCP to get a machine to test. Hopefully this provides a similar environment:. `gcloud compute instances create ""${USER}-test"" --scopes ""compute-rw,storage-full,cloud-platform"" --image-family almalinux-9 --image-project almalinux-cloud --machine-type ""n1-standard-64"" --zone ""us-west1-b""`. I ssh'ed into the machine:. ```bash. gcloud compute ssh pichuan-test --zone ""us-west1-b"". ```. Check the Linux version:. ```. $ uname -a. Linux pichuan-test.us-west1-b.c.brain-genomics.google.com.internal 5.14.0-362.24.2.el9_3.x86_64 #1 SMP PREEMPT_DYNAMIC Sat Mar 30 14:11:54 EDT 2024 x86_64 x86_64 x86_64 GNU/Linux. ```. And I ran this too:. ```. $ cat /etc/os-release. NAME=""AlmaLinux"". VERSION=""9.3 (Shamrock Pampas Cat)"". ID=""almalinux"". ID_LIKE=""rhel centos fedora"". VERSION_ID=""9.3"". PLATFORM_ID=""platform:el9"". PRETTY_NAME=""AlmaLinux 9.3 (Shamrock Pampas Cat)"". ANSI_COLOR=""0;34"". LOGO=""fedora-logo-icon"". CPE_NAME=""cpe:/o:almalinux:almalinux:9::baseos"". HOME_URL=""https://almalinux.org/"". DOCUMENTATION_URL=""https://wiki.almalinux.org/"". BUG_REPORT_URL=""https://bugs.almalinux.org/"". ALMALINUX_MANTISBT_PROJECT=""AlmaLinux-9"". ALMALINUX_MANTISBT_PROJECT_VERSION=""9.3"". REDHAT_SUPPORT_PRODUCT=""AlmaLinux"". REDHAT_SUPPORT_PRODUCT_VERSION=""9.3"". ```. ## Install Singularity. I don't have Singularity on the machine yet, so:. https://docs.sylabs.io/guides/4.1/user-guide/quick_start.html#quick-installation-steps. ```bash. sudo yum update -y && \. sudo yum groupinstall -y 'Development Tools' && \. sudo yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools. ```. ```. sudo yum groupinstall -y 'Development Tools'. # Install RPM packages for dependencies. sudo yum install -y \. autoconf \. automake \. cryptsetup \. fuse3-devel \. git \. glib2-devel \. libseccomp-devel \. libtool \. runc \. squashfs-tools \. wget \. zlib-devel. ```. ```bash. sudo dnf install dnf-plugins-core.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:965,security,LOG,LOGO,965,"# Attempt to reproduce the issue. ## Get a machine with the same Linux version. I used GCP to get a machine to test. Hopefully this provides a similar environment:. `gcloud compute instances create ""${USER}-test"" --scopes ""compute-rw,storage-full,cloud-platform"" --image-family almalinux-9 --image-project almalinux-cloud --machine-type ""n1-standard-64"" --zone ""us-west1-b""`. I ssh'ed into the machine:. ```bash. gcloud compute ssh pichuan-test --zone ""us-west1-b"". ```. Check the Linux version:. ```. $ uname -a. Linux pichuan-test.us-west1-b.c.brain-genomics.google.com.internal 5.14.0-362.24.2.el9_3.x86_64 #1 SMP PREEMPT_DYNAMIC Sat Mar 30 14:11:54 EDT 2024 x86_64 x86_64 x86_64 GNU/Linux. ```. And I ran this too:. ```. $ cat /etc/os-release. NAME=""AlmaLinux"". VERSION=""9.3 (Shamrock Pampas Cat)"". ID=""almalinux"". ID_LIKE=""rhel centos fedora"". VERSION_ID=""9.3"". PLATFORM_ID=""platform:el9"". PRETTY_NAME=""AlmaLinux 9.3 (Shamrock Pampas Cat)"". ANSI_COLOR=""0;34"". LOGO=""fedora-logo-icon"". CPE_NAME=""cpe:/o:almalinux:almalinux:9::baseos"". HOME_URL=""https://almalinux.org/"". DOCUMENTATION_URL=""https://wiki.almalinux.org/"". BUG_REPORT_URL=""https://bugs.almalinux.org/"". ALMALINUX_MANTISBT_PROJECT=""AlmaLinux-9"". ALMALINUX_MANTISBT_PROJECT_VERSION=""9.3"". REDHAT_SUPPORT_PRODUCT=""AlmaLinux"". REDHAT_SUPPORT_PRODUCT_VERSION=""9.3"". ```. ## Install Singularity. I don't have Singularity on the machine yet, so:. https://docs.sylabs.io/guides/4.1/user-guide/quick_start.html#quick-installation-steps. ```bash. sudo yum update -y && \. sudo yum groupinstall -y 'Development Tools' && \. sudo yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools. ```. ```. sudo yum groupinstall -y 'Development Tools'. # Install RPM packages for dependencies. sudo yum install -y \. autoconf \. automake \. cryptsetup \. fuse3-devel \. git \. glib2-devel \. libseccomp-devel \. libtool \. runc \. squashfs-tools \. wget \. zlib-devel. ```. ```bash. sudo dnf install dnf-plugins-core.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:978,security,log,logo-icon,978,"# Attempt to reproduce the issue. ## Get a machine with the same Linux version. I used GCP to get a machine to test. Hopefully this provides a similar environment:. `gcloud compute instances create ""${USER}-test"" --scopes ""compute-rw,storage-full,cloud-platform"" --image-family almalinux-9 --image-project almalinux-cloud --machine-type ""n1-standard-64"" --zone ""us-west1-b""`. I ssh'ed into the machine:. ```bash. gcloud compute ssh pichuan-test --zone ""us-west1-b"". ```. Check the Linux version:. ```. $ uname -a. Linux pichuan-test.us-west1-b.c.brain-genomics.google.com.internal 5.14.0-362.24.2.el9_3.x86_64 #1 SMP PREEMPT_DYNAMIC Sat Mar 30 14:11:54 EDT 2024 x86_64 x86_64 x86_64 GNU/Linux. ```. And I ran this too:. ```. $ cat /etc/os-release. NAME=""AlmaLinux"". VERSION=""9.3 (Shamrock Pampas Cat)"". ID=""almalinux"". ID_LIKE=""rhel centos fedora"". VERSION_ID=""9.3"". PLATFORM_ID=""platform:el9"". PRETTY_NAME=""AlmaLinux 9.3 (Shamrock Pampas Cat)"". ANSI_COLOR=""0;34"". LOGO=""fedora-logo-icon"". CPE_NAME=""cpe:/o:almalinux:almalinux:9::baseos"". HOME_URL=""https://almalinux.org/"". DOCUMENTATION_URL=""https://wiki.almalinux.org/"". BUG_REPORT_URL=""https://bugs.almalinux.org/"". ALMALINUX_MANTISBT_PROJECT=""AlmaLinux-9"". ALMALINUX_MANTISBT_PROJECT_VERSION=""9.3"". REDHAT_SUPPORT_PRODUCT=""AlmaLinux"". REDHAT_SUPPORT_PRODUCT_VERSION=""9.3"". ```. ## Install Singularity. I don't have Singularity on the machine yet, so:. https://docs.sylabs.io/guides/4.1/user-guide/quick_start.html#quick-installation-steps. ```bash. sudo yum update -y && \. sudo yum groupinstall -y 'Development Tools' && \. sudo yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools. ```. ```. sudo yum groupinstall -y 'Development Tools'. # Install RPM packages for dependencies. sudo yum install -y \. autoconf \. automake \. cryptsetup \. fuse3-devel \. git \. glib2-devel \. libseccomp-devel \. libtool \. runc \. squashfs-tools \. wget \. zlib-devel. ```. ```bash. sudo dnf install dnf-plugins-core.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:1512,security,updat,update,1512,"inux pichuan-test.us-west1-b.c.brain-genomics.google.com.internal 5.14.0-362.24.2.el9_3.x86_64 #1 SMP PREEMPT_DYNAMIC Sat Mar 30 14:11:54 EDT 2024 x86_64 x86_64 x86_64 GNU/Linux. ```. And I ran this too:. ```. $ cat /etc/os-release. NAME=""AlmaLinux"". VERSION=""9.3 (Shamrock Pampas Cat)"". ID=""almalinux"". ID_LIKE=""rhel centos fedora"". VERSION_ID=""9.3"". PLATFORM_ID=""platform:el9"". PRETTY_NAME=""AlmaLinux 9.3 (Shamrock Pampas Cat)"". ANSI_COLOR=""0;34"". LOGO=""fedora-logo-icon"". CPE_NAME=""cpe:/o:almalinux:almalinux:9::baseos"". HOME_URL=""https://almalinux.org/"". DOCUMENTATION_URL=""https://wiki.almalinux.org/"". BUG_REPORT_URL=""https://bugs.almalinux.org/"". ALMALINUX_MANTISBT_PROJECT=""AlmaLinux-9"". ALMALINUX_MANTISBT_PROJECT_VERSION=""9.3"". REDHAT_SUPPORT_PRODUCT=""AlmaLinux"". REDHAT_SUPPORT_PRODUCT_VERSION=""9.3"". ```. ## Install Singularity. I don't have Singularity on the machine yet, so:. https://docs.sylabs.io/guides/4.1/user-guide/quick_start.html#quick-installation-steps. ```bash. sudo yum update -y && \. sudo yum groupinstall -y 'Development Tools' && \. sudo yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools. ```. ```. sudo yum groupinstall -y 'Development Tools'. # Install RPM packages for dependencies. sudo yum install -y \. autoconf \. automake \. cryptsetup \. fuse3-devel \. git \. glib2-devel \. libseccomp-devel \. libtool \. runc \. squashfs-tools \. wget \. zlib-devel. ```. ```bash. sudo dnf install dnf-plugins-core. sudo dnf copr enable dctrud/squashfs-tools-ng. sudo dnf install squashfs-tools-ng. ```. ```bash. export VERSION=1.21.0 OS=linux ARCH=amd64 && \. wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz && \. sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz && \. rm go$VERSION.$OS-$ARCH.tar.gz. ```. ```bash. echo 'export PATH=/usr/local/go/bin:$PATH' >> ~/.bashrc && \. source ~/.bashrc. ```. ```bash. export VERSION=4.1.0 && \. wget https://github.com/sylabs/singularity/releases/download/v${VERSION}/si",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:3089,security,model,model-case-study,3089," export VERSION=1.21.0 OS=linux ARCH=amd64 && \. wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz && \. sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz && \. rm go$VERSION.$OS-$ARCH.tar.gz. ```. ```bash. echo 'export PATH=/usr/local/go/bin:$PATH' >> ~/.bashrc && \. source ~/.bashrc. ```. ```bash. export VERSION=4.1.0 && \. wget https://github.com/sylabs/singularity/releases/download/v${VERSION}/singularity-ce-${VERSION}.tar.gz && \. tar -xzf singularity-ce-${VERSION}.tar.gz && \. cd singularity-ce-${VERSION}. ```. ```bash. ./mconfig && \. make -C builddir && \. sudo make -C builddir install. ```. At this point, I have singularity installed. ```bash. $ singularity --version. singularity-ce version 4.1.0. ```. ## Get data and run DeepVariant. Now, let me try to follow similar steps:. ```bash. singularity build DeepVariant_1.6.1.sif docker://google/deepvariant:1.6.1. ```. From here, I used https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-pacbio-model-case-study.md to test. Download data:. ```bash. mkdir -p reference. FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai. ```. ```bash. mkdir -p input. HTTPDIR=https://downloads.pacbcloud.com/public/dataset/HG003/deepvariant-case-study. curl ${HTTPDIR}/HG003.GRCh38.chr20.pFDA_truthv2.bam > input/HG003.GRCh38.chr20.pFDA_truthv2.bam. curl ${HTTPDIR}/HG003.GRCh38.chr20.pFDA_truthv2.bam.bai > input/HG003.GRCh38.chr20.pFDA_truthv2.bam.bai. ```. ```bash. ulimit -u 10000. BIN_VERSION=""1.6.1"". mkdir -p deepvariant_output. ```. @Carl-labhub mentioned ""When I run it, I’m doing so from an interactive session with singularity exec"". I'm a bit confused by this. Maybe you mean `singularity shell`? ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:4001,security,session,session,4001,"e, I used https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-pacbio-model-case-study.md to test. Download data:. ```bash. mkdir -p reference. FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai. ```. ```bash. mkdir -p input. HTTPDIR=https://downloads.pacbcloud.com/public/dataset/HG003/deepvariant-case-study. curl ${HTTPDIR}/HG003.GRCh38.chr20.pFDA_truthv2.bam > input/HG003.GRCh38.chr20.pFDA_truthv2.bam. curl ${HTTPDIR}/HG003.GRCh38.chr20.pFDA_truthv2.bam.bai > input/HG003.GRCh38.chr20.pFDA_truthv2.bam.bai. ```. ```bash. ulimit -u 10000. BIN_VERSION=""1.6.1"". mkdir -p deepvariant_output. ```. @Carl-labhub mentioned ""When I run it, I’m doing so from an interactive session with singularity exec"". I'm a bit confused by this. Maybe you mean `singularity shell`? So I tried:. ```bash. singularity shell --bind /usr/lib/locale/ DeepVariant_1.6.1.sif. ```. This gets into a shell mode, then I ran:. ```. Singularity> /opt/deepvariant/bin/run_deepvariant \. > --model_type PACBIO \. > --ref reference/GRCh38_no_alt_analysis_set.fasta \. > --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. > --output_vcf deepvariant_output/output.vcf.gz \. > --num_shards $(nproc) \. > --regions chr20. ```. Directly `singularity exec` with the command (just like https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-pacbio-model-case-study.md) should be fine too. My `make_examples` step completed without any issues. It took:. ```. real 9m7.540s. user 215m38.303s. sys 6m41.297s. ```. I let the whole run finish just to be sure. --> The full run also completed without any errors. @Carl-labhub, can you check what I did above, and see what might be different on your side?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:4655,security,model,model-case-study,4655,"i.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai. ```. ```bash. mkdir -p input. HTTPDIR=https://downloads.pacbcloud.com/public/dataset/HG003/deepvariant-case-study. curl ${HTTPDIR}/HG003.GRCh38.chr20.pFDA_truthv2.bam > input/HG003.GRCh38.chr20.pFDA_truthv2.bam. curl ${HTTPDIR}/HG003.GRCh38.chr20.pFDA_truthv2.bam.bai > input/HG003.GRCh38.chr20.pFDA_truthv2.bam.bai. ```. ```bash. ulimit -u 10000. BIN_VERSION=""1.6.1"". mkdir -p deepvariant_output. ```. @Carl-labhub mentioned ""When I run it, I’m doing so from an interactive session with singularity exec"". I'm a bit confused by this. Maybe you mean `singularity shell`? So I tried:. ```bash. singularity shell --bind /usr/lib/locale/ DeepVariant_1.6.1.sif. ```. This gets into a shell mode, then I ran:. ```. Singularity> /opt/deepvariant/bin/run_deepvariant \. > --model_type PACBIO \. > --ref reference/GRCh38_no_alt_analysis_set.fasta \. > --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. > --output_vcf deepvariant_output/output.vcf.gz \. > --num_shards $(nproc) \. > --regions chr20. ```. Directly `singularity exec` with the command (just like https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-pacbio-model-case-study.md) should be fine too. My `make_examples` step completed without any issues. It took:. ```. real 9m7.540s. user 215m38.303s. sys 6m41.297s. ```. I let the whole run finish just to be sure. --> The full run also completed without any errors. @Carl-labhub, can you check what I did above, and see what might be different on your side? And, there were previous GitHub issues that might be worth reading through to see if there are any relevant clues. For example: https://github.com/google/deepvariant/issues/677.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:4720,security,compl,completed,4720,"i.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai. ```. ```bash. mkdir -p input. HTTPDIR=https://downloads.pacbcloud.com/public/dataset/HG003/deepvariant-case-study. curl ${HTTPDIR}/HG003.GRCh38.chr20.pFDA_truthv2.bam > input/HG003.GRCh38.chr20.pFDA_truthv2.bam. curl ${HTTPDIR}/HG003.GRCh38.chr20.pFDA_truthv2.bam.bai > input/HG003.GRCh38.chr20.pFDA_truthv2.bam.bai. ```. ```bash. ulimit -u 10000. BIN_VERSION=""1.6.1"". mkdir -p deepvariant_output. ```. @Carl-labhub mentioned ""When I run it, I’m doing so from an interactive session with singularity exec"". I'm a bit confused by this. Maybe you mean `singularity shell`? So I tried:. ```bash. singularity shell --bind /usr/lib/locale/ DeepVariant_1.6.1.sif. ```. This gets into a shell mode, then I ran:. ```. Singularity> /opt/deepvariant/bin/run_deepvariant \. > --model_type PACBIO \. > --ref reference/GRCh38_no_alt_analysis_set.fasta \. > --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. > --output_vcf deepvariant_output/output.vcf.gz \. > --num_shards $(nproc) \. > --regions chr20. ```. Directly `singularity exec` with the command (just like https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-pacbio-model-case-study.md) should be fine too. My `make_examples` step completed without any issues. It took:. ```. real 9m7.540s. user 215m38.303s. sys 6m41.297s. ```. I let the whole run finish just to be sure. --> The full run also completed without any errors. @Carl-labhub, can you check what I did above, and see what might be different on your side? And, there were previous GitHub issues that might be worth reading through to see if there are any relevant clues. For example: https://github.com/google/deepvariant/issues/677.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:4884,security,compl,completed,4884,"i.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai. ```. ```bash. mkdir -p input. HTTPDIR=https://downloads.pacbcloud.com/public/dataset/HG003/deepvariant-case-study. curl ${HTTPDIR}/HG003.GRCh38.chr20.pFDA_truthv2.bam > input/HG003.GRCh38.chr20.pFDA_truthv2.bam. curl ${HTTPDIR}/HG003.GRCh38.chr20.pFDA_truthv2.bam.bai > input/HG003.GRCh38.chr20.pFDA_truthv2.bam.bai. ```. ```bash. ulimit -u 10000. BIN_VERSION=""1.6.1"". mkdir -p deepvariant_output. ```. @Carl-labhub mentioned ""When I run it, I’m doing so from an interactive session with singularity exec"". I'm a bit confused by this. Maybe you mean `singularity shell`? So I tried:. ```bash. singularity shell --bind /usr/lib/locale/ DeepVariant_1.6.1.sif. ```. This gets into a shell mode, then I ran:. ```. Singularity> /opt/deepvariant/bin/run_deepvariant \. > --model_type PACBIO \. > --ref reference/GRCh38_no_alt_analysis_set.fasta \. > --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. > --output_vcf deepvariant_output/output.vcf.gz \. > --num_shards $(nproc) \. > --regions chr20. ```. Directly `singularity exec` with the command (just like https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-pacbio-model-case-study.md) should be fine too. My `make_examples` step completed without any issues. It took:. ```. real 9m7.540s. user 215m38.303s. sys 6m41.297s. ```. I let the whole run finish just to be sure. --> The full run also completed without any errors. @Carl-labhub, can you check what I did above, and see what might be different on your side? And, there were previous GitHub issues that might be worth reading through to see if there are any relevant clues. For example: https://github.com/google/deepvariant/issues/677.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:111,testability,test,test,111,"# Attempt to reproduce the issue. ## Get a machine with the same Linux version. I used GCP to get a machine to test. Hopefully this provides a similar environment:. `gcloud compute instances create ""${USER}-test"" --scopes ""compute-rw,storage-full,cloud-platform"" --image-family almalinux-9 --image-project almalinux-cloud --machine-type ""n1-standard-64"" --zone ""us-west1-b""`. I ssh'ed into the machine:. ```bash. gcloud compute ssh pichuan-test --zone ""us-west1-b"". ```. Check the Linux version:. ```. $ uname -a. Linux pichuan-test.us-west1-b.c.brain-genomics.google.com.internal 5.14.0-362.24.2.el9_3.x86_64 #1 SMP PREEMPT_DYNAMIC Sat Mar 30 14:11:54 EDT 2024 x86_64 x86_64 x86_64 GNU/Linux. ```. And I ran this too:. ```. $ cat /etc/os-release. NAME=""AlmaLinux"". VERSION=""9.3 (Shamrock Pampas Cat)"". ID=""almalinux"". ID_LIKE=""rhel centos fedora"". VERSION_ID=""9.3"". PLATFORM_ID=""platform:el9"". PRETTY_NAME=""AlmaLinux 9.3 (Shamrock Pampas Cat)"". ANSI_COLOR=""0;34"". LOGO=""fedora-logo-icon"". CPE_NAME=""cpe:/o:almalinux:almalinux:9::baseos"". HOME_URL=""https://almalinux.org/"". DOCUMENTATION_URL=""https://wiki.almalinux.org/"". BUG_REPORT_URL=""https://bugs.almalinux.org/"". ALMALINUX_MANTISBT_PROJECT=""AlmaLinux-9"". ALMALINUX_MANTISBT_PROJECT_VERSION=""9.3"". REDHAT_SUPPORT_PRODUCT=""AlmaLinux"". REDHAT_SUPPORT_PRODUCT_VERSION=""9.3"". ```. ## Install Singularity. I don't have Singularity on the machine yet, so:. https://docs.sylabs.io/guides/4.1/user-guide/quick_start.html#quick-installation-steps. ```bash. sudo yum update -y && \. sudo yum groupinstall -y 'Development Tools' && \. sudo yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools. ```. ```. sudo yum groupinstall -y 'Development Tools'. # Install RPM packages for dependencies. sudo yum install -y \. autoconf \. automake \. cryptsetup \. fuse3-devel \. git \. glib2-devel \. libseccomp-devel \. libtool \. runc \. squashfs-tools \. wget \. zlib-devel. ```. ```bash. sudo dnf install dnf-plugins-core.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:207,testability,test,test,207,"# Attempt to reproduce the issue. ## Get a machine with the same Linux version. I used GCP to get a machine to test. Hopefully this provides a similar environment:. `gcloud compute instances create ""${USER}-test"" --scopes ""compute-rw,storage-full,cloud-platform"" --image-family almalinux-9 --image-project almalinux-cloud --machine-type ""n1-standard-64"" --zone ""us-west1-b""`. I ssh'ed into the machine:. ```bash. gcloud compute ssh pichuan-test --zone ""us-west1-b"". ```. Check the Linux version:. ```. $ uname -a. Linux pichuan-test.us-west1-b.c.brain-genomics.google.com.internal 5.14.0-362.24.2.el9_3.x86_64 #1 SMP PREEMPT_DYNAMIC Sat Mar 30 14:11:54 EDT 2024 x86_64 x86_64 x86_64 GNU/Linux. ```. And I ran this too:. ```. $ cat /etc/os-release. NAME=""AlmaLinux"". VERSION=""9.3 (Shamrock Pampas Cat)"". ID=""almalinux"". ID_LIKE=""rhel centos fedora"". VERSION_ID=""9.3"". PLATFORM_ID=""platform:el9"". PRETTY_NAME=""AlmaLinux 9.3 (Shamrock Pampas Cat)"". ANSI_COLOR=""0;34"". LOGO=""fedora-logo-icon"". CPE_NAME=""cpe:/o:almalinux:almalinux:9::baseos"". HOME_URL=""https://almalinux.org/"". DOCUMENTATION_URL=""https://wiki.almalinux.org/"". BUG_REPORT_URL=""https://bugs.almalinux.org/"". ALMALINUX_MANTISBT_PROJECT=""AlmaLinux-9"". ALMALINUX_MANTISBT_PROJECT_VERSION=""9.3"". REDHAT_SUPPORT_PRODUCT=""AlmaLinux"". REDHAT_SUPPORT_PRODUCT_VERSION=""9.3"". ```. ## Install Singularity. I don't have Singularity on the machine yet, so:. https://docs.sylabs.io/guides/4.1/user-guide/quick_start.html#quick-installation-steps. ```bash. sudo yum update -y && \. sudo yum groupinstall -y 'Development Tools' && \. sudo yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools. ```. ```. sudo yum groupinstall -y 'Development Tools'. # Install RPM packages for dependencies. sudo yum install -y \. autoconf \. automake \. cryptsetup \. fuse3-devel \. git \. glib2-devel \. libseccomp-devel \. libtool \. runc \. squashfs-tools \. wget \. zlib-devel. ```. ```bash. sudo dnf install dnf-plugins-core.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:440,testability,test,test,440,"# Attempt to reproduce the issue. ## Get a machine with the same Linux version. I used GCP to get a machine to test. Hopefully this provides a similar environment:. `gcloud compute instances create ""${USER}-test"" --scopes ""compute-rw,storage-full,cloud-platform"" --image-family almalinux-9 --image-project almalinux-cloud --machine-type ""n1-standard-64"" --zone ""us-west1-b""`. I ssh'ed into the machine:. ```bash. gcloud compute ssh pichuan-test --zone ""us-west1-b"". ```. Check the Linux version:. ```. $ uname -a. Linux pichuan-test.us-west1-b.c.brain-genomics.google.com.internal 5.14.0-362.24.2.el9_3.x86_64 #1 SMP PREEMPT_DYNAMIC Sat Mar 30 14:11:54 EDT 2024 x86_64 x86_64 x86_64 GNU/Linux. ```. And I ran this too:. ```. $ cat /etc/os-release. NAME=""AlmaLinux"". VERSION=""9.3 (Shamrock Pampas Cat)"". ID=""almalinux"". ID_LIKE=""rhel centos fedora"". VERSION_ID=""9.3"". PLATFORM_ID=""platform:el9"". PRETTY_NAME=""AlmaLinux 9.3 (Shamrock Pampas Cat)"". ANSI_COLOR=""0;34"". LOGO=""fedora-logo-icon"". CPE_NAME=""cpe:/o:almalinux:almalinux:9::baseos"". HOME_URL=""https://almalinux.org/"". DOCUMENTATION_URL=""https://wiki.almalinux.org/"". BUG_REPORT_URL=""https://bugs.almalinux.org/"". ALMALINUX_MANTISBT_PROJECT=""AlmaLinux-9"". ALMALINUX_MANTISBT_PROJECT_VERSION=""9.3"". REDHAT_SUPPORT_PRODUCT=""AlmaLinux"". REDHAT_SUPPORT_PRODUCT_VERSION=""9.3"". ```. ## Install Singularity. I don't have Singularity on the machine yet, so:. https://docs.sylabs.io/guides/4.1/user-guide/quick_start.html#quick-installation-steps. ```bash. sudo yum update -y && \. sudo yum groupinstall -y 'Development Tools' && \. sudo yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools. ```. ```. sudo yum groupinstall -y 'Development Tools'. # Install RPM packages for dependencies. sudo yum install -y \. autoconf \. automake \. cryptsetup \. fuse3-devel \. git \. glib2-devel \. libseccomp-devel \. libtool \. runc \. squashfs-tools \. wget \. zlib-devel. ```. ```bash. sudo dnf install dnf-plugins-core.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:528,testability,test,test,528,"# Attempt to reproduce the issue. ## Get a machine with the same Linux version. I used GCP to get a machine to test. Hopefully this provides a similar environment:. `gcloud compute instances create ""${USER}-test"" --scopes ""compute-rw,storage-full,cloud-platform"" --image-family almalinux-9 --image-project almalinux-cloud --machine-type ""n1-standard-64"" --zone ""us-west1-b""`. I ssh'ed into the machine:. ```bash. gcloud compute ssh pichuan-test --zone ""us-west1-b"". ```. Check the Linux version:. ```. $ uname -a. Linux pichuan-test.us-west1-b.c.brain-genomics.google.com.internal 5.14.0-362.24.2.el9_3.x86_64 #1 SMP PREEMPT_DYNAMIC Sat Mar 30 14:11:54 EDT 2024 x86_64 x86_64 x86_64 GNU/Linux. ```. And I ran this too:. ```. $ cat /etc/os-release. NAME=""AlmaLinux"". VERSION=""9.3 (Shamrock Pampas Cat)"". ID=""almalinux"". ID_LIKE=""rhel centos fedora"". VERSION_ID=""9.3"". PLATFORM_ID=""platform:el9"". PRETTY_NAME=""AlmaLinux 9.3 (Shamrock Pampas Cat)"". ANSI_COLOR=""0;34"". LOGO=""fedora-logo-icon"". CPE_NAME=""cpe:/o:almalinux:almalinux:9::baseos"". HOME_URL=""https://almalinux.org/"". DOCUMENTATION_URL=""https://wiki.almalinux.org/"". BUG_REPORT_URL=""https://bugs.almalinux.org/"". ALMALINUX_MANTISBT_PROJECT=""AlmaLinux-9"". ALMALINUX_MANTISBT_PROJECT_VERSION=""9.3"". REDHAT_SUPPORT_PRODUCT=""AlmaLinux"". REDHAT_SUPPORT_PRODUCT_VERSION=""9.3"". ```. ## Install Singularity. I don't have Singularity on the machine yet, so:. https://docs.sylabs.io/guides/4.1/user-guide/quick_start.html#quick-installation-steps. ```bash. sudo yum update -y && \. sudo yum groupinstall -y 'Development Tools' && \. sudo yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools. ```. ```. sudo yum groupinstall -y 'Development Tools'. # Install RPM packages for dependencies. sudo yum install -y \. autoconf \. automake \. cryptsetup \. fuse3-devel \. git \. glib2-devel \. libseccomp-devel \. libtool \. runc \. squashfs-tools \. wget \. zlib-devel. ```. ```bash. sudo dnf install dnf-plugins-core.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:965,testability,LOG,LOGO,965,"# Attempt to reproduce the issue. ## Get a machine with the same Linux version. I used GCP to get a machine to test. Hopefully this provides a similar environment:. `gcloud compute instances create ""${USER}-test"" --scopes ""compute-rw,storage-full,cloud-platform"" --image-family almalinux-9 --image-project almalinux-cloud --machine-type ""n1-standard-64"" --zone ""us-west1-b""`. I ssh'ed into the machine:. ```bash. gcloud compute ssh pichuan-test --zone ""us-west1-b"". ```. Check the Linux version:. ```. $ uname -a. Linux pichuan-test.us-west1-b.c.brain-genomics.google.com.internal 5.14.0-362.24.2.el9_3.x86_64 #1 SMP PREEMPT_DYNAMIC Sat Mar 30 14:11:54 EDT 2024 x86_64 x86_64 x86_64 GNU/Linux. ```. And I ran this too:. ```. $ cat /etc/os-release. NAME=""AlmaLinux"". VERSION=""9.3 (Shamrock Pampas Cat)"". ID=""almalinux"". ID_LIKE=""rhel centos fedora"". VERSION_ID=""9.3"". PLATFORM_ID=""platform:el9"". PRETTY_NAME=""AlmaLinux 9.3 (Shamrock Pampas Cat)"". ANSI_COLOR=""0;34"". LOGO=""fedora-logo-icon"". CPE_NAME=""cpe:/o:almalinux:almalinux:9::baseos"". HOME_URL=""https://almalinux.org/"". DOCUMENTATION_URL=""https://wiki.almalinux.org/"". BUG_REPORT_URL=""https://bugs.almalinux.org/"". ALMALINUX_MANTISBT_PROJECT=""AlmaLinux-9"". ALMALINUX_MANTISBT_PROJECT_VERSION=""9.3"". REDHAT_SUPPORT_PRODUCT=""AlmaLinux"". REDHAT_SUPPORT_PRODUCT_VERSION=""9.3"". ```. ## Install Singularity. I don't have Singularity on the machine yet, so:. https://docs.sylabs.io/guides/4.1/user-guide/quick_start.html#quick-installation-steps. ```bash. sudo yum update -y && \. sudo yum groupinstall -y 'Development Tools' && \. sudo yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools. ```. ```. sudo yum groupinstall -y 'Development Tools'. # Install RPM packages for dependencies. sudo yum install -y \. autoconf \. automake \. cryptsetup \. fuse3-devel \. git \. glib2-devel \. libseccomp-devel \. libtool \. runc \. squashfs-tools \. wget \. zlib-devel. ```. ```bash. sudo dnf install dnf-plugins-core.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:978,testability,log,logo-icon,978,"# Attempt to reproduce the issue. ## Get a machine with the same Linux version. I used GCP to get a machine to test. Hopefully this provides a similar environment:. `gcloud compute instances create ""${USER}-test"" --scopes ""compute-rw,storage-full,cloud-platform"" --image-family almalinux-9 --image-project almalinux-cloud --machine-type ""n1-standard-64"" --zone ""us-west1-b""`. I ssh'ed into the machine:. ```bash. gcloud compute ssh pichuan-test --zone ""us-west1-b"". ```. Check the Linux version:. ```. $ uname -a. Linux pichuan-test.us-west1-b.c.brain-genomics.google.com.internal 5.14.0-362.24.2.el9_3.x86_64 #1 SMP PREEMPT_DYNAMIC Sat Mar 30 14:11:54 EDT 2024 x86_64 x86_64 x86_64 GNU/Linux. ```. And I ran this too:. ```. $ cat /etc/os-release. NAME=""AlmaLinux"". VERSION=""9.3 (Shamrock Pampas Cat)"". ID=""almalinux"". ID_LIKE=""rhel centos fedora"". VERSION_ID=""9.3"". PLATFORM_ID=""platform:el9"". PRETTY_NAME=""AlmaLinux 9.3 (Shamrock Pampas Cat)"". ANSI_COLOR=""0;34"". LOGO=""fedora-logo-icon"". CPE_NAME=""cpe:/o:almalinux:almalinux:9::baseos"". HOME_URL=""https://almalinux.org/"". DOCUMENTATION_URL=""https://wiki.almalinux.org/"". BUG_REPORT_URL=""https://bugs.almalinux.org/"". ALMALINUX_MANTISBT_PROJECT=""AlmaLinux-9"". ALMALINUX_MANTISBT_PROJECT_VERSION=""9.3"". REDHAT_SUPPORT_PRODUCT=""AlmaLinux"". REDHAT_SUPPORT_PRODUCT_VERSION=""9.3"". ```. ## Install Singularity. I don't have Singularity on the machine yet, so:. https://docs.sylabs.io/guides/4.1/user-guide/quick_start.html#quick-installation-steps. ```bash. sudo yum update -y && \. sudo yum groupinstall -y 'Development Tools' && \. sudo yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools. ```. ```. sudo yum groupinstall -y 'Development Tools'. # Install RPM packages for dependencies. sudo yum install -y \. autoconf \. automake \. cryptsetup \. fuse3-devel \. git \. glib2-devel \. libseccomp-devel \. libtool \. runc \. squashfs-tools \. wget \. zlib-devel. ```. ```bash. sudo dnf install dnf-plugins-core.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:1763,testability,depend,dependencies,1763,"SION=""9.3 (Shamrock Pampas Cat)"". ID=""almalinux"". ID_LIKE=""rhel centos fedora"". VERSION_ID=""9.3"". PLATFORM_ID=""platform:el9"". PRETTY_NAME=""AlmaLinux 9.3 (Shamrock Pampas Cat)"". ANSI_COLOR=""0;34"". LOGO=""fedora-logo-icon"". CPE_NAME=""cpe:/o:almalinux:almalinux:9::baseos"". HOME_URL=""https://almalinux.org/"". DOCUMENTATION_URL=""https://wiki.almalinux.org/"". BUG_REPORT_URL=""https://bugs.almalinux.org/"". ALMALINUX_MANTISBT_PROJECT=""AlmaLinux-9"". ALMALINUX_MANTISBT_PROJECT_VERSION=""9.3"". REDHAT_SUPPORT_PRODUCT=""AlmaLinux"". REDHAT_SUPPORT_PRODUCT_VERSION=""9.3"". ```. ## Install Singularity. I don't have Singularity on the machine yet, so:. https://docs.sylabs.io/guides/4.1/user-guide/quick_start.html#quick-installation-steps. ```bash. sudo yum update -y && \. sudo yum groupinstall -y 'Development Tools' && \. sudo yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools. ```. ```. sudo yum groupinstall -y 'Development Tools'. # Install RPM packages for dependencies. sudo yum install -y \. autoconf \. automake \. cryptsetup \. fuse3-devel \. git \. glib2-devel \. libseccomp-devel \. libtool \. runc \. squashfs-tools \. wget \. zlib-devel. ```. ```bash. sudo dnf install dnf-plugins-core. sudo dnf copr enable dctrud/squashfs-tools-ng. sudo dnf install squashfs-tools-ng. ```. ```bash. export VERSION=1.21.0 OS=linux ARCH=amd64 && \. wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz && \. sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz && \. rm go$VERSION.$OS-$ARCH.tar.gz. ```. ```bash. echo 'export PATH=/usr/local/go/bin:$PATH' >> ~/.bashrc && \. source ~/.bashrc. ```. ```bash. export VERSION=4.1.0 && \. wget https://github.com/sylabs/singularity/releases/download/v${VERSION}/singularity-ce-${VERSION}.tar.gz && \. tar -xzf singularity-ce-${VERSION}.tar.gz && \. cd singularity-ce-${VERSION}. ```. ```bash. ./mconfig && \. make -C builddir && \. sudo make -C builddir install. ```. At this point, I have singularity installed. ```ba",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:3112,testability,test,test,3112,".21.0 OS=linux ARCH=amd64 && \. wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz && \. sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz && \. rm go$VERSION.$OS-$ARCH.tar.gz. ```. ```bash. echo 'export PATH=/usr/local/go/bin:$PATH' >> ~/.bashrc && \. source ~/.bashrc. ```. ```bash. export VERSION=4.1.0 && \. wget https://github.com/sylabs/singularity/releases/download/v${VERSION}/singularity-ce-${VERSION}.tar.gz && \. tar -xzf singularity-ce-${VERSION}.tar.gz && \. cd singularity-ce-${VERSION}. ```. ```bash. ./mconfig && \. make -C builddir && \. sudo make -C builddir install. ```. At this point, I have singularity installed. ```bash. $ singularity --version. singularity-ce version 4.1.0. ```. ## Get data and run DeepVariant. Now, let me try to follow similar steps:. ```bash. singularity build DeepVariant_1.6.1.sif docker://google/deepvariant:1.6.1. ```. From here, I used https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-pacbio-model-case-study.md to test. Download data:. ```bash. mkdir -p reference. FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai. ```. ```bash. mkdir -p input. HTTPDIR=https://downloads.pacbcloud.com/public/dataset/HG003/deepvariant-case-study. curl ${HTTPDIR}/HG003.GRCh38.chr20.pFDA_truthv2.bam > input/HG003.GRCh38.chr20.pFDA_truthv2.bam. curl ${HTTPDIR}/HG003.GRCh38.chr20.pFDA_truthv2.bam.bai > input/HG003.GRCh38.chr20.pFDA_truthv2.bam.bai. ```. ```bash. ulimit -u 10000. BIN_VERSION=""1.6.1"". mkdir -p deepvariant_output. ```. @Carl-labhub mentioned ""When I run it, I’m doing so from an interactive session with singularity exec"". I'm a bit confused by this. Maybe you mean `singularity shell`? So I tried:. ```b",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:201,usability,USER,USER,201,"# Attempt to reproduce the issue. ## Get a machine with the same Linux version. I used GCP to get a machine to test. Hopefully this provides a similar environment:. `gcloud compute instances create ""${USER}-test"" --scopes ""compute-rw,storage-full,cloud-platform"" --image-family almalinux-9 --image-project almalinux-cloud --machine-type ""n1-standard-64"" --zone ""us-west1-b""`. I ssh'ed into the machine:. ```bash. gcloud compute ssh pichuan-test --zone ""us-west1-b"". ```. Check the Linux version:. ```. $ uname -a. Linux pichuan-test.us-west1-b.c.brain-genomics.google.com.internal 5.14.0-362.24.2.el9_3.x86_64 #1 SMP PREEMPT_DYNAMIC Sat Mar 30 14:11:54 EDT 2024 x86_64 x86_64 x86_64 GNU/Linux. ```. And I ran this too:. ```. $ cat /etc/os-release. NAME=""AlmaLinux"". VERSION=""9.3 (Shamrock Pampas Cat)"". ID=""almalinux"". ID_LIKE=""rhel centos fedora"". VERSION_ID=""9.3"". PLATFORM_ID=""platform:el9"". PRETTY_NAME=""AlmaLinux 9.3 (Shamrock Pampas Cat)"". ANSI_COLOR=""0;34"". LOGO=""fedora-logo-icon"". CPE_NAME=""cpe:/o:almalinux:almalinux:9::baseos"". HOME_URL=""https://almalinux.org/"". DOCUMENTATION_URL=""https://wiki.almalinux.org/"". BUG_REPORT_URL=""https://bugs.almalinux.org/"". ALMALINUX_MANTISBT_PROJECT=""AlmaLinux-9"". ALMALINUX_MANTISBT_PROJECT_VERSION=""9.3"". REDHAT_SUPPORT_PRODUCT=""AlmaLinux"". REDHAT_SUPPORT_PRODUCT_VERSION=""9.3"". ```. ## Install Singularity. I don't have Singularity on the machine yet, so:. https://docs.sylabs.io/guides/4.1/user-guide/quick_start.html#quick-installation-steps. ```bash. sudo yum update -y && \. sudo yum groupinstall -y 'Development Tools' && \. sudo yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools. ```. ```. sudo yum groupinstall -y 'Development Tools'. # Install RPM packages for dependencies. sudo yum install -y \. autoconf \. automake \. cryptsetup \. fuse3-devel \. git \. glib2-devel \. libseccomp-devel \. libtool \. runc \. squashfs-tools \. wget \. zlib-devel. ```. ```bash. sudo dnf install dnf-plugins-core.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:1429,usability,guid,guides,1429,"pichuan-test --zone ""us-west1-b"". ```. Check the Linux version:. ```. $ uname -a. Linux pichuan-test.us-west1-b.c.brain-genomics.google.com.internal 5.14.0-362.24.2.el9_3.x86_64 #1 SMP PREEMPT_DYNAMIC Sat Mar 30 14:11:54 EDT 2024 x86_64 x86_64 x86_64 GNU/Linux. ```. And I ran this too:. ```. $ cat /etc/os-release. NAME=""AlmaLinux"". VERSION=""9.3 (Shamrock Pampas Cat)"". ID=""almalinux"". ID_LIKE=""rhel centos fedora"". VERSION_ID=""9.3"". PLATFORM_ID=""platform:el9"". PRETTY_NAME=""AlmaLinux 9.3 (Shamrock Pampas Cat)"". ANSI_COLOR=""0;34"". LOGO=""fedora-logo-icon"". CPE_NAME=""cpe:/o:almalinux:almalinux:9::baseos"". HOME_URL=""https://almalinux.org/"". DOCUMENTATION_URL=""https://wiki.almalinux.org/"". BUG_REPORT_URL=""https://bugs.almalinux.org/"". ALMALINUX_MANTISBT_PROJECT=""AlmaLinux-9"". ALMALINUX_MANTISBT_PROJECT_VERSION=""9.3"". REDHAT_SUPPORT_PRODUCT=""AlmaLinux"". REDHAT_SUPPORT_PRODUCT_VERSION=""9.3"". ```. ## Install Singularity. I don't have Singularity on the machine yet, so:. https://docs.sylabs.io/guides/4.1/user-guide/quick_start.html#quick-installation-steps. ```bash. sudo yum update -y && \. sudo yum groupinstall -y 'Development Tools' && \. sudo yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools. ```. ```. sudo yum groupinstall -y 'Development Tools'. # Install RPM packages for dependencies. sudo yum install -y \. autoconf \. automake \. cryptsetup \. fuse3-devel \. git \. glib2-devel \. libseccomp-devel \. libtool \. runc \. squashfs-tools \. wget \. zlib-devel. ```. ```bash. sudo dnf install dnf-plugins-core. sudo dnf copr enable dctrud/squashfs-tools-ng. sudo dnf install squashfs-tools-ng. ```. ```bash. export VERSION=1.21.0 OS=linux ARCH=amd64 && \. wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz && \. sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz && \. rm go$VERSION.$OS-$ARCH.tar.gz. ```. ```bash. echo 'export PATH=/usr/local/go/bin:$PATH' >> ~/.bashrc && \. source ~/.bashrc. ```. ```bash. export VERSION=4.1.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:1440,usability,user,user-guide,1440,"--zone ""us-west1-b"". ```. Check the Linux version:. ```. $ uname -a. Linux pichuan-test.us-west1-b.c.brain-genomics.google.com.internal 5.14.0-362.24.2.el9_3.x86_64 #1 SMP PREEMPT_DYNAMIC Sat Mar 30 14:11:54 EDT 2024 x86_64 x86_64 x86_64 GNU/Linux. ```. And I ran this too:. ```. $ cat /etc/os-release. NAME=""AlmaLinux"". VERSION=""9.3 (Shamrock Pampas Cat)"". ID=""almalinux"". ID_LIKE=""rhel centos fedora"". VERSION_ID=""9.3"". PLATFORM_ID=""platform:el9"". PRETTY_NAME=""AlmaLinux 9.3 (Shamrock Pampas Cat)"". ANSI_COLOR=""0;34"". LOGO=""fedora-logo-icon"". CPE_NAME=""cpe:/o:almalinux:almalinux:9::baseos"". HOME_URL=""https://almalinux.org/"". DOCUMENTATION_URL=""https://wiki.almalinux.org/"". BUG_REPORT_URL=""https://bugs.almalinux.org/"". ALMALINUX_MANTISBT_PROJECT=""AlmaLinux-9"". ALMALINUX_MANTISBT_PROJECT_VERSION=""9.3"". REDHAT_SUPPORT_PRODUCT=""AlmaLinux"". REDHAT_SUPPORT_PRODUCT_VERSION=""9.3"". ```. ## Install Singularity. I don't have Singularity on the machine yet, so:. https://docs.sylabs.io/guides/4.1/user-guide/quick_start.html#quick-installation-steps. ```bash. sudo yum update -y && \. sudo yum groupinstall -y 'Development Tools' && \. sudo yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools. ```. ```. sudo yum groupinstall -y 'Development Tools'. # Install RPM packages for dependencies. sudo yum install -y \. autoconf \. automake \. cryptsetup \. fuse3-devel \. git \. glib2-devel \. libseccomp-devel \. libtool \. runc \. squashfs-tools \. wget \. zlib-devel. ```. ```bash. sudo dnf install dnf-plugins-core. sudo dnf copr enable dctrud/squashfs-tools-ng. sudo dnf install squashfs-tools-ng. ```. ```bash. export VERSION=1.21.0 OS=linux ARCH=amd64 && \. wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz && \. sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz && \. rm go$VERSION.$OS-$ARCH.tar.gz. ```. ```bash. echo 'export PATH=/usr/local/go/bin:$PATH' >> ~/.bashrc && \. source ~/.bashrc. ```. ```bash. export VERSION=4.1.0 && \. wget ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:1566,usability,Tool,Tools,1566,"om.internal 5.14.0-362.24.2.el9_3.x86_64 #1 SMP PREEMPT_DYNAMIC Sat Mar 30 14:11:54 EDT 2024 x86_64 x86_64 x86_64 GNU/Linux. ```. And I ran this too:. ```. $ cat /etc/os-release. NAME=""AlmaLinux"". VERSION=""9.3 (Shamrock Pampas Cat)"". ID=""almalinux"". ID_LIKE=""rhel centos fedora"". VERSION_ID=""9.3"". PLATFORM_ID=""platform:el9"". PRETTY_NAME=""AlmaLinux 9.3 (Shamrock Pampas Cat)"". ANSI_COLOR=""0;34"". LOGO=""fedora-logo-icon"". CPE_NAME=""cpe:/o:almalinux:almalinux:9::baseos"". HOME_URL=""https://almalinux.org/"". DOCUMENTATION_URL=""https://wiki.almalinux.org/"". BUG_REPORT_URL=""https://bugs.almalinux.org/"". ALMALINUX_MANTISBT_PROJECT=""AlmaLinux-9"". ALMALINUX_MANTISBT_PROJECT_VERSION=""9.3"". REDHAT_SUPPORT_PRODUCT=""AlmaLinux"". REDHAT_SUPPORT_PRODUCT_VERSION=""9.3"". ```. ## Install Singularity. I don't have Singularity on the machine yet, so:. https://docs.sylabs.io/guides/4.1/user-guide/quick_start.html#quick-installation-steps. ```bash. sudo yum update -y && \. sudo yum groupinstall -y 'Development Tools' && \. sudo yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools. ```. ```. sudo yum groupinstall -y 'Development Tools'. # Install RPM packages for dependencies. sudo yum install -y \. autoconf \. automake \. cryptsetup \. fuse3-devel \. git \. glib2-devel \. libseccomp-devel \. libtool \. runc \. squashfs-tools \. wget \. zlib-devel. ```. ```bash. sudo dnf install dnf-plugins-core. sudo dnf copr enable dctrud/squashfs-tools-ng. sudo dnf install squashfs-tools-ng. ```. ```bash. export VERSION=1.21.0 OS=linux ARCH=amd64 && \. wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz && \. sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz && \. rm go$VERSION.$OS-$ARCH.tar.gz. ```. ```bash. echo 'export PATH=/usr/local/go/bin:$PATH' >> ~/.bashrc && \. source ~/.bashrc. ```. ```bash. export VERSION=4.1.0 && \. wget https://github.com/sylabs/singularity/releases/download/v${VERSION}/singularity-ce-${VERSION}.tar.gz && \. tar -xzf singular",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:1673,usability,tool,tools,1673,"x86_64 GNU/Linux. ```. And I ran this too:. ```. $ cat /etc/os-release. NAME=""AlmaLinux"". VERSION=""9.3 (Shamrock Pampas Cat)"". ID=""almalinux"". ID_LIKE=""rhel centos fedora"". VERSION_ID=""9.3"". PLATFORM_ID=""platform:el9"". PRETTY_NAME=""AlmaLinux 9.3 (Shamrock Pampas Cat)"". ANSI_COLOR=""0;34"". LOGO=""fedora-logo-icon"". CPE_NAME=""cpe:/o:almalinux:almalinux:9::baseos"". HOME_URL=""https://almalinux.org/"". DOCUMENTATION_URL=""https://wiki.almalinux.org/"". BUG_REPORT_URL=""https://bugs.almalinux.org/"". ALMALINUX_MANTISBT_PROJECT=""AlmaLinux-9"". ALMALINUX_MANTISBT_PROJECT_VERSION=""9.3"". REDHAT_SUPPORT_PRODUCT=""AlmaLinux"". REDHAT_SUPPORT_PRODUCT_VERSION=""9.3"". ```. ## Install Singularity. I don't have Singularity on the machine yet, so:. https://docs.sylabs.io/guides/4.1/user-guide/quick_start.html#quick-installation-steps. ```bash. sudo yum update -y && \. sudo yum groupinstall -y 'Development Tools' && \. sudo yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools. ```. ```. sudo yum groupinstall -y 'Development Tools'. # Install RPM packages for dependencies. sudo yum install -y \. autoconf \. automake \. cryptsetup \. fuse3-devel \. git \. glib2-devel \. libseccomp-devel \. libtool \. runc \. squashfs-tools \. wget \. zlib-devel. ```. ```bash. sudo dnf install dnf-plugins-core. sudo dnf copr enable dctrud/squashfs-tools-ng. sudo dnf install squashfs-tools-ng. ```. ```bash. export VERSION=1.21.0 OS=linux ARCH=amd64 && \. wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz && \. sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz && \. rm go$VERSION.$OS-$ARCH.tar.gz. ```. ```bash. echo 'export PATH=/usr/local/go/bin:$PATH' >> ~/.bashrc && \. source ~/.bashrc. ```. ```bash. export VERSION=4.1.0 && \. wget https://github.com/sylabs/singularity/releases/download/v${VERSION}/singularity-ce-${VERSION}.tar.gz && \. tar -xzf singularity-ce-${VERSION}.tar.gz && \. cd singularity-ce-${VERSION}. ```. ```bash. ./mconfig && \. make -C builddir",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:1728,usability,Tool,Tools,1728,"/etc/os-release. NAME=""AlmaLinux"". VERSION=""9.3 (Shamrock Pampas Cat)"". ID=""almalinux"". ID_LIKE=""rhel centos fedora"". VERSION_ID=""9.3"". PLATFORM_ID=""platform:el9"". PRETTY_NAME=""AlmaLinux 9.3 (Shamrock Pampas Cat)"". ANSI_COLOR=""0;34"". LOGO=""fedora-logo-icon"". CPE_NAME=""cpe:/o:almalinux:almalinux:9::baseos"". HOME_URL=""https://almalinux.org/"". DOCUMENTATION_URL=""https://wiki.almalinux.org/"". BUG_REPORT_URL=""https://bugs.almalinux.org/"". ALMALINUX_MANTISBT_PROJECT=""AlmaLinux-9"". ALMALINUX_MANTISBT_PROJECT_VERSION=""9.3"". REDHAT_SUPPORT_PRODUCT=""AlmaLinux"". REDHAT_SUPPORT_PRODUCT_VERSION=""9.3"". ```. ## Install Singularity. I don't have Singularity on the machine yet, so:. https://docs.sylabs.io/guides/4.1/user-guide/quick_start.html#quick-installation-steps. ```bash. sudo yum update -y && \. sudo yum groupinstall -y 'Development Tools' && \. sudo yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools. ```. ```. sudo yum groupinstall -y 'Development Tools'. # Install RPM packages for dependencies. sudo yum install -y \. autoconf \. automake \. cryptsetup \. fuse3-devel \. git \. glib2-devel \. libseccomp-devel \. libtool \. runc \. squashfs-tools \. wget \. zlib-devel. ```. ```bash. sudo dnf install dnf-plugins-core. sudo dnf copr enable dctrud/squashfs-tools-ng. sudo dnf install squashfs-tools-ng. ```. ```bash. export VERSION=1.21.0 OS=linux ARCH=amd64 && \. wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz && \. sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz && \. rm go$VERSION.$OS-$ARCH.tar.gz. ```. ```bash. echo 'export PATH=/usr/local/go/bin:$PATH' >> ~/.bashrc && \. source ~/.bashrc. ```. ```bash. export VERSION=4.1.0 && \. wget https://github.com/sylabs/singularity/releases/download/v${VERSION}/singularity-ce-${VERSION}.tar.gz && \. tar -xzf singularity-ce-${VERSION}.tar.gz && \. cd singularity-ce-${VERSION}. ```. ```bash. ./mconfig && \. make -C builddir && \. sudo make -C builddir install. ```. At this poin",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:1923,usability,tool,tools,1923,"mrock Pampas Cat)"". ANSI_COLOR=""0;34"". LOGO=""fedora-logo-icon"". CPE_NAME=""cpe:/o:almalinux:almalinux:9::baseos"". HOME_URL=""https://almalinux.org/"". DOCUMENTATION_URL=""https://wiki.almalinux.org/"". BUG_REPORT_URL=""https://bugs.almalinux.org/"". ALMALINUX_MANTISBT_PROJECT=""AlmaLinux-9"". ALMALINUX_MANTISBT_PROJECT_VERSION=""9.3"". REDHAT_SUPPORT_PRODUCT=""AlmaLinux"". REDHAT_SUPPORT_PRODUCT_VERSION=""9.3"". ```. ## Install Singularity. I don't have Singularity on the machine yet, so:. https://docs.sylabs.io/guides/4.1/user-guide/quick_start.html#quick-installation-steps. ```bash. sudo yum update -y && \. sudo yum groupinstall -y 'Development Tools' && \. sudo yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools. ```. ```. sudo yum groupinstall -y 'Development Tools'. # Install RPM packages for dependencies. sudo yum install -y \. autoconf \. automake \. cryptsetup \. fuse3-devel \. git \. glib2-devel \. libseccomp-devel \. libtool \. runc \. squashfs-tools \. wget \. zlib-devel. ```. ```bash. sudo dnf install dnf-plugins-core. sudo dnf copr enable dctrud/squashfs-tools-ng. sudo dnf install squashfs-tools-ng. ```. ```bash. export VERSION=1.21.0 OS=linux ARCH=amd64 && \. wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz && \. sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz && \. rm go$VERSION.$OS-$ARCH.tar.gz. ```. ```bash. echo 'export PATH=/usr/local/go/bin:$PATH' >> ~/.bashrc && \. source ~/.bashrc. ```. ```bash. export VERSION=4.1.0 && \. wget https://github.com/sylabs/singularity/releases/download/v${VERSION}/singularity-ce-${VERSION}.tar.gz && \. tar -xzf singularity-ce-${VERSION}.tar.gz && \. cd singularity-ce-${VERSION}. ```. ```bash. ./mconfig && \. make -C builddir && \. sudo make -C builddir install. ```. At this point, I have singularity installed. ```bash. $ singularity --version. singularity-ce version 4.1.0. ```. ## Get data and run DeepVariant. Now, let me try to follow similar steps:. ```bash. singulari",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:2038,usability,tool,tools-ng,2038,"E_URL=""https://almalinux.org/"". DOCUMENTATION_URL=""https://wiki.almalinux.org/"". BUG_REPORT_URL=""https://bugs.almalinux.org/"". ALMALINUX_MANTISBT_PROJECT=""AlmaLinux-9"". ALMALINUX_MANTISBT_PROJECT_VERSION=""9.3"". REDHAT_SUPPORT_PRODUCT=""AlmaLinux"". REDHAT_SUPPORT_PRODUCT_VERSION=""9.3"". ```. ## Install Singularity. I don't have Singularity on the machine yet, so:. https://docs.sylabs.io/guides/4.1/user-guide/quick_start.html#quick-installation-steps. ```bash. sudo yum update -y && \. sudo yum groupinstall -y 'Development Tools' && \. sudo yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools. ```. ```. sudo yum groupinstall -y 'Development Tools'. # Install RPM packages for dependencies. sudo yum install -y \. autoconf \. automake \. cryptsetup \. fuse3-devel \. git \. glib2-devel \. libseccomp-devel \. libtool \. runc \. squashfs-tools \. wget \. zlib-devel. ```. ```bash. sudo dnf install dnf-plugins-core. sudo dnf copr enable dctrud/squashfs-tools-ng. sudo dnf install squashfs-tools-ng. ```. ```bash. export VERSION=1.21.0 OS=linux ARCH=amd64 && \. wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz && \. sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz && \. rm go$VERSION.$OS-$ARCH.tar.gz. ```. ```bash. echo 'export PATH=/usr/local/go/bin:$PATH' >> ~/.bashrc && \. source ~/.bashrc. ```. ```bash. export VERSION=4.1.0 && \. wget https://github.com/sylabs/singularity/releases/download/v${VERSION}/singularity-ce-${VERSION}.tar.gz && \. tar -xzf singularity-ce-${VERSION}.tar.gz && \. cd singularity-ce-${VERSION}. ```. ```bash. ./mconfig && \. make -C builddir && \. sudo make -C builddir install. ```. At this point, I have singularity installed. ```bash. $ singularity --version. singularity-ce version 4.1.0. ```. ## Get data and run DeepVariant. Now, let me try to follow similar steps:. ```bash. singularity build DeepVariant_1.6.1.sif docker://google/deepvariant:1.6.1. ```. From here, I used https://github.com/google/d",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:2074,usability,tool,tools-ng,2074,"MENTATION_URL=""https://wiki.almalinux.org/"". BUG_REPORT_URL=""https://bugs.almalinux.org/"". ALMALINUX_MANTISBT_PROJECT=""AlmaLinux-9"". ALMALINUX_MANTISBT_PROJECT_VERSION=""9.3"". REDHAT_SUPPORT_PRODUCT=""AlmaLinux"". REDHAT_SUPPORT_PRODUCT_VERSION=""9.3"". ```. ## Install Singularity. I don't have Singularity on the machine yet, so:. https://docs.sylabs.io/guides/4.1/user-guide/quick_start.html#quick-installation-steps. ```bash. sudo yum update -y && \. sudo yum groupinstall -y 'Development Tools' && \. sudo yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools. ```. ```. sudo yum groupinstall -y 'Development Tools'. # Install RPM packages for dependencies. sudo yum install -y \. autoconf \. automake \. cryptsetup \. fuse3-devel \. git \. glib2-devel \. libseccomp-devel \. libtool \. runc \. squashfs-tools \. wget \. zlib-devel. ```. ```bash. sudo dnf install dnf-plugins-core. sudo dnf copr enable dctrud/squashfs-tools-ng. sudo dnf install squashfs-tools-ng. ```. ```bash. export VERSION=1.21.0 OS=linux ARCH=amd64 && \. wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz && \. sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz && \. rm go$VERSION.$OS-$ARCH.tar.gz. ```. ```bash. echo 'export PATH=/usr/local/go/bin:$PATH' >> ~/.bashrc && \. source ~/.bashrc. ```. ```bash. export VERSION=4.1.0 && \. wget https://github.com/sylabs/singularity/releases/download/v${VERSION}/singularity-ce-${VERSION}.tar.gz && \. tar -xzf singularity-ce-${VERSION}.tar.gz && \. cd singularity-ce-${VERSION}. ```. ```bash. ./mconfig && \. make -C builddir && \. sudo make -C builddir install. ```. At this point, I have singularity installed. ```bash. $ singularity --version. singularity-ce version 4.1.0. ```. ## Get data and run DeepVariant. Now, let me try to follow similar steps:. ```bash. singularity build DeepVariant_1.6.1.sif docker://google/deepvariant:1.6.1. ```. From here, I used https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvari",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:3549,usability,input,input,3549,"tar -xzf singularity-ce-${VERSION}.tar.gz && \. cd singularity-ce-${VERSION}. ```. ```bash. ./mconfig && \. make -C builddir && \. sudo make -C builddir install. ```. At this point, I have singularity installed. ```bash. $ singularity --version. singularity-ce version 4.1.0. ```. ## Get data and run DeepVariant. Now, let me try to follow similar steps:. ```bash. singularity build DeepVariant_1.6.1.sif docker://google/deepvariant:1.6.1. ```. From here, I used https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-pacbio-model-case-study.md to test. Download data:. ```bash. mkdir -p reference. FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai. ```. ```bash. mkdir -p input. HTTPDIR=https://downloads.pacbcloud.com/public/dataset/HG003/deepvariant-case-study. curl ${HTTPDIR}/HG003.GRCh38.chr20.pFDA_truthv2.bam > input/HG003.GRCh38.chr20.pFDA_truthv2.bam. curl ${HTTPDIR}/HG003.GRCh38.chr20.pFDA_truthv2.bam.bai > input/HG003.GRCh38.chr20.pFDA_truthv2.bam.bai. ```. ```bash. ulimit -u 10000. BIN_VERSION=""1.6.1"". mkdir -p deepvariant_output. ```. @Carl-labhub mentioned ""When I run it, I’m doing so from an interactive session with singularity exec"". I'm a bit confused by this. Maybe you mean `singularity shell`? So I tried:. ```bash. singularity shell --bind /usr/lib/locale/ DeepVariant_1.6.1.sif. ```. This gets into a shell mode, then I ran:. ```. Singularity> /opt/deepvariant/bin/run_deepvariant \. > --model_type PACBIO \. > --ref reference/GRCh38_no_alt_analysis_set.fasta \. > --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. > --output_vcf deepvariant_output/output.vcf.gz \. > --num_shards $(nproc) \. > --regions chr20. ```. Directly `singularity exec`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:3695,usability,input,input,3695,"ilddir install. ```. At this point, I have singularity installed. ```bash. $ singularity --version. singularity-ce version 4.1.0. ```. ## Get data and run DeepVariant. Now, let me try to follow similar steps:. ```bash. singularity build DeepVariant_1.6.1.sif docker://google/deepvariant:1.6.1. ```. From here, I used https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-pacbio-model-case-study.md to test. Download data:. ```bash. mkdir -p reference. FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai. ```. ```bash. mkdir -p input. HTTPDIR=https://downloads.pacbcloud.com/public/dataset/HG003/deepvariant-case-study. curl ${HTTPDIR}/HG003.GRCh38.chr20.pFDA_truthv2.bam > input/HG003.GRCh38.chr20.pFDA_truthv2.bam. curl ${HTTPDIR}/HG003.GRCh38.chr20.pFDA_truthv2.bam.bai > input/HG003.GRCh38.chr20.pFDA_truthv2.bam.bai. ```. ```bash. ulimit -u 10000. BIN_VERSION=""1.6.1"". mkdir -p deepvariant_output. ```. @Carl-labhub mentioned ""When I run it, I’m doing so from an interactive session with singularity exec"". I'm a bit confused by this. Maybe you mean `singularity shell`? So I tried:. ```bash. singularity shell --bind /usr/lib/locale/ DeepVariant_1.6.1.sif. ```. This gets into a shell mode, then I ran:. ```. Singularity> /opt/deepvariant/bin/run_deepvariant \. > --model_type PACBIO \. > --ref reference/GRCh38_no_alt_analysis_set.fasta \. > --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. > --output_vcf deepvariant_output/output.vcf.gz \. > --num_shards $(nproc) \. > --regions chr20. ```. Directly `singularity exec` with the command (just like https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-pacbio-model-case-study.md) should be fine too. My",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:3796,usability,input,input,3796,"ingularity-ce version 4.1.0. ```. ## Get data and run DeepVariant. Now, let me try to follow similar steps:. ```bash. singularity build DeepVariant_1.6.1.sif docker://google/deepvariant:1.6.1. ```. From here, I used https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-pacbio-model-case-study.md to test. Download data:. ```bash. mkdir -p reference. FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai. ```. ```bash. mkdir -p input. HTTPDIR=https://downloads.pacbcloud.com/public/dataset/HG003/deepvariant-case-study. curl ${HTTPDIR}/HG003.GRCh38.chr20.pFDA_truthv2.bam > input/HG003.GRCh38.chr20.pFDA_truthv2.bam. curl ${HTTPDIR}/HG003.GRCh38.chr20.pFDA_truthv2.bam.bai > input/HG003.GRCh38.chr20.pFDA_truthv2.bam.bai. ```. ```bash. ulimit -u 10000. BIN_VERSION=""1.6.1"". mkdir -p deepvariant_output. ```. @Carl-labhub mentioned ""When I run it, I’m doing so from an interactive session with singularity exec"". I'm a bit confused by this. Maybe you mean `singularity shell`? So I tried:. ```bash. singularity shell --bind /usr/lib/locale/ DeepVariant_1.6.1.sif. ```. This gets into a shell mode, then I ran:. ```. Singularity> /opt/deepvariant/bin/run_deepvariant \. > --model_type PACBIO \. > --ref reference/GRCh38_no_alt_analysis_set.fasta \. > --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. > --output_vcf deepvariant_output/output.vcf.gz \. > --num_shards $(nproc) \. > --regions chr20. ```. Directly `singularity exec` with the command (just like https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-pacbio-model-case-study.md) should be fine too. My `make_examples` step completed without any issues. It took:. ```. real 9m7.540s. user 215m38.303s. s",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:3989,usability,interact,interactive,3989,". From here, I used https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-pacbio-model-case-study.md to test. Download data:. ```bash. mkdir -p reference. FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai. ```. ```bash. mkdir -p input. HTTPDIR=https://downloads.pacbcloud.com/public/dataset/HG003/deepvariant-case-study. curl ${HTTPDIR}/HG003.GRCh38.chr20.pFDA_truthv2.bam > input/HG003.GRCh38.chr20.pFDA_truthv2.bam. curl ${HTTPDIR}/HG003.GRCh38.chr20.pFDA_truthv2.bam.bai > input/HG003.GRCh38.chr20.pFDA_truthv2.bam.bai. ```. ```bash. ulimit -u 10000. BIN_VERSION=""1.6.1"". mkdir -p deepvariant_output. ```. @Carl-labhub mentioned ""When I run it, I’m doing so from an interactive session with singularity exec"". I'm a bit confused by this. Maybe you mean `singularity shell`? So I tried:. ```bash. singularity shell --bind /usr/lib/locale/ DeepVariant_1.6.1.sif. ```. This gets into a shell mode, then I ran:. ```. Singularity> /opt/deepvariant/bin/run_deepvariant \. > --model_type PACBIO \. > --ref reference/GRCh38_no_alt_analysis_set.fasta \. > --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. > --output_vcf deepvariant_output/output.vcf.gz \. > --num_shards $(nproc) \. > --regions chr20. ```. Directly `singularity exec` with the command (just like https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-pacbio-model-case-study.md) should be fine too. My `make_examples` step completed without any issues. It took:. ```. real 9m7.540s. user 215m38.303s. sys 6m41.297s. ```. I let the whole run finish just to be sure. --> The full run also completed without any errors. @Carl-labhub, can you check what I did above, and see what might be different on ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:4378,usability,input,input,4378,"i.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai. ```. ```bash. mkdir -p input. HTTPDIR=https://downloads.pacbcloud.com/public/dataset/HG003/deepvariant-case-study. curl ${HTTPDIR}/HG003.GRCh38.chr20.pFDA_truthv2.bam > input/HG003.GRCh38.chr20.pFDA_truthv2.bam. curl ${HTTPDIR}/HG003.GRCh38.chr20.pFDA_truthv2.bam.bai > input/HG003.GRCh38.chr20.pFDA_truthv2.bam.bai. ```. ```bash. ulimit -u 10000. BIN_VERSION=""1.6.1"". mkdir -p deepvariant_output. ```. @Carl-labhub mentioned ""When I run it, I’m doing so from an interactive session with singularity exec"". I'm a bit confused by this. Maybe you mean `singularity shell`? So I tried:. ```bash. singularity shell --bind /usr/lib/locale/ DeepVariant_1.6.1.sif. ```. This gets into a shell mode, then I ran:. ```. Singularity> /opt/deepvariant/bin/run_deepvariant \. > --model_type PACBIO \. > --ref reference/GRCh38_no_alt_analysis_set.fasta \. > --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. > --output_vcf deepvariant_output/output.vcf.gz \. > --num_shards $(nproc) \. > --regions chr20. ```. Directly `singularity exec` with the command (just like https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-pacbio-model-case-study.md) should be fine too. My `make_examples` step completed without any issues. It took:. ```. real 9m7.540s. user 215m38.303s. sys 6m41.297s. ```. I let the whole run finish just to be sure. --> The full run also completed without any errors. @Carl-labhub, can you check what I did above, and see what might be different on your side? And, there were previous GitHub issues that might be worth reading through to see if there are any relevant clues. For example: https://github.com/google/deepvariant/issues/677.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:4562,usability,command,command,4562,"i.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai. ```. ```bash. mkdir -p input. HTTPDIR=https://downloads.pacbcloud.com/public/dataset/HG003/deepvariant-case-study. curl ${HTTPDIR}/HG003.GRCh38.chr20.pFDA_truthv2.bam > input/HG003.GRCh38.chr20.pFDA_truthv2.bam. curl ${HTTPDIR}/HG003.GRCh38.chr20.pFDA_truthv2.bam.bai > input/HG003.GRCh38.chr20.pFDA_truthv2.bam.bai. ```. ```bash. ulimit -u 10000. BIN_VERSION=""1.6.1"". mkdir -p deepvariant_output. ```. @Carl-labhub mentioned ""When I run it, I’m doing so from an interactive session with singularity exec"". I'm a bit confused by this. Maybe you mean `singularity shell`? So I tried:. ```bash. singularity shell --bind /usr/lib/locale/ DeepVariant_1.6.1.sif. ```. This gets into a shell mode, then I ran:. ```. Singularity> /opt/deepvariant/bin/run_deepvariant \. > --model_type PACBIO \. > --ref reference/GRCh38_no_alt_analysis_set.fasta \. > --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. > --output_vcf deepvariant_output/output.vcf.gz \. > --num_shards $(nproc) \. > --regions chr20. ```. Directly `singularity exec` with the command (just like https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-pacbio-model-case-study.md) should be fine too. My `make_examples` step completed without any issues. It took:. ```. real 9m7.540s. user 215m38.303s. sys 6m41.297s. ```. I let the whole run finish just to be sure. --> The full run also completed without any errors. @Carl-labhub, can you check what I did above, and see what might be different on your side? And, there were previous GitHub issues that might be worth reading through to see if there are any relevant clues. For example: https://github.com/google/deepvariant/issues/677.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:4780,usability,user,user,4780,"i.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai. ```. ```bash. mkdir -p input. HTTPDIR=https://downloads.pacbcloud.com/public/dataset/HG003/deepvariant-case-study. curl ${HTTPDIR}/HG003.GRCh38.chr20.pFDA_truthv2.bam > input/HG003.GRCh38.chr20.pFDA_truthv2.bam. curl ${HTTPDIR}/HG003.GRCh38.chr20.pFDA_truthv2.bam.bai > input/HG003.GRCh38.chr20.pFDA_truthv2.bam.bai. ```. ```bash. ulimit -u 10000. BIN_VERSION=""1.6.1"". mkdir -p deepvariant_output. ```. @Carl-labhub mentioned ""When I run it, I’m doing so from an interactive session with singularity exec"". I'm a bit confused by this. Maybe you mean `singularity shell`? So I tried:. ```bash. singularity shell --bind /usr/lib/locale/ DeepVariant_1.6.1.sif. ```. This gets into a shell mode, then I ran:. ```. Singularity> /opt/deepvariant/bin/run_deepvariant \. > --model_type PACBIO \. > --ref reference/GRCh38_no_alt_analysis_set.fasta \. > --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. > --output_vcf deepvariant_output/output.vcf.gz \. > --num_shards $(nproc) \. > --regions chr20. ```. Directly `singularity exec` with the command (just like https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-pacbio-model-case-study.md) should be fine too. My `make_examples` step completed without any issues. It took:. ```. real 9m7.540s. user 215m38.303s. sys 6m41.297s. ```. I let the whole run finish just to be sure. --> The full run also completed without any errors. @Carl-labhub, can you check what I did above, and see what might be different on your side? And, there were previous GitHub issues that might be worth reading through to see if there are any relevant clues. For example: https://github.com/google/deepvariant/issues/677.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:4906,usability,error,errors,4906,"i.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai. ```. ```bash. mkdir -p input. HTTPDIR=https://downloads.pacbcloud.com/public/dataset/HG003/deepvariant-case-study. curl ${HTTPDIR}/HG003.GRCh38.chr20.pFDA_truthv2.bam > input/HG003.GRCh38.chr20.pFDA_truthv2.bam. curl ${HTTPDIR}/HG003.GRCh38.chr20.pFDA_truthv2.bam.bai > input/HG003.GRCh38.chr20.pFDA_truthv2.bam.bai. ```. ```bash. ulimit -u 10000. BIN_VERSION=""1.6.1"". mkdir -p deepvariant_output. ```. @Carl-labhub mentioned ""When I run it, I’m doing so from an interactive session with singularity exec"". I'm a bit confused by this. Maybe you mean `singularity shell`? So I tried:. ```bash. singularity shell --bind /usr/lib/locale/ DeepVariant_1.6.1.sif. ```. This gets into a shell mode, then I ran:. ```. Singularity> /opt/deepvariant/bin/run_deepvariant \. > --model_type PACBIO \. > --ref reference/GRCh38_no_alt_analysis_set.fasta \. > --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. > --output_vcf deepvariant_output/output.vcf.gz \. > --num_shards $(nproc) \. > --regions chr20. ```. Directly `singularity exec` with the command (just like https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-pacbio-model-case-study.md) should be fine too. My `make_examples` step completed without any issues. It took:. ```. real 9m7.540s. user 215m38.303s. sys 6m41.297s. ```. I let the whole run finish just to be sure. --> The full run also completed without any errors. @Carl-labhub, can you check what I did above, and see what might be different on your side? And, there were previous GitHub issues that might be worth reading through to see if there are any relevant clues. For example: https://github.com/google/deepvariant/issues/677.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:106,deployability,updat,update,106,"Hi @Carl-labhub ,. given that there is no activity on this issue for a while, I'll close it. Feel free to update if you have more comments or questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:106,safety,updat,update,106,"Hi @Carl-labhub ,. given that there is no activity on this issue for a while, I'll close it. Feel free to update if you have more comments or questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:106,security,updat,update,106,"Hi @Carl-labhub ,. given that there is no activity on this issue for a while, I'll close it. Feel free to update if you have more comments or questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:83,usability,close,close,83,"Hi @Carl-labhub ,. given that there is no activity on this issue for a while, I'll close it. Feel free to update if you have more comments or questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/813:141,usability,command,command,141,"When running `make_examples`, you can pass the the following flag: `--select_variant_types='indels'`. If you are using the `run_deepvariant` command, you can pass `--make_examples_extra_args-""select_variant_types=indels""`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/813
https://github.com/google/deepvariant/issues/813:103,modifiability,paramet,parameter,103,"thank you @danielecook ,. How is `--select_variant_types='indels'` different from `types_to_alt_align` parameter? . I want to train DeepVariant on INDEL variants with particular length, is there a command in `make_examples` to generate labeled example for these particular variants? . Thank you",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/813
https://github.com/google/deepvariant/issues/813:197,usability,command,command,197,"thank you @danielecook ,. How is `--select_variant_types='indels'` different from `types_to_alt_align` parameter? . I want to train DeepVariant on INDEL variants with particular length, is there a command in `make_examples` to generate labeled example for these particular variants? . Thank you",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/813
https://github.com/google/deepvariant/issues/813:462,energy efficiency,model,model,462,"`types_to_alt_align` refers to the type of variants in which we perform alignments against the alternative variant, when you have also set the `alt_aligned_pileup` flag. . You might be able to accomplish something like this by making use of the vcf candidate importer. See `--truth_variants` + `--variant_caller=vcf_candidate_importer`. I would expect that if you perform filtering similarly on your training and test data, that this could be a way to develop a model specific to certain size INDEL variants, but we have never tried to do something like this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/813
https://github.com/google/deepvariant/issues/813:372,integrability,filter,filtering,372,"`types_to_alt_align` refers to the type of variants in which we perform alignments against the alternative variant, when you have also set the `alt_aligned_pileup` flag. . You might be able to accomplish something like this by making use of the vcf candidate importer. See `--truth_variants` + `--variant_caller=vcf_candidate_importer`. I would expect that if you perform filtering similarly on your training and test data, that this could be a way to develop a model specific to certain size INDEL variants, but we have never tried to do something like this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/813
https://github.com/google/deepvariant/issues/813:468,interoperability,specif,specific,468,"`types_to_alt_align` refers to the type of variants in which we perform alignments against the alternative variant, when you have also set the `alt_aligned_pileup` flag. . You might be able to accomplish something like this by making use of the vcf candidate importer. See `--truth_variants` + `--variant_caller=vcf_candidate_importer`. I would expect that if you perform filtering similarly on your training and test data, that this could be a way to develop a model specific to certain size INDEL variants, but we have never tried to do something like this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/813
https://github.com/google/deepvariant/issues/813:64,performance,perform,perform,64,"`types_to_alt_align` refers to the type of variants in which we perform alignments against the alternative variant, when you have also set the `alt_aligned_pileup` flag. . You might be able to accomplish something like this by making use of the vcf candidate importer. See `--truth_variants` + `--variant_caller=vcf_candidate_importer`. I would expect that if you perform filtering similarly on your training and test data, that this could be a way to develop a model specific to certain size INDEL variants, but we have never tried to do something like this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/813
https://github.com/google/deepvariant/issues/813:364,performance,perform,perform,364,"`types_to_alt_align` refers to the type of variants in which we perform alignments against the alternative variant, when you have also set the `alt_aligned_pileup` flag. . You might be able to accomplish something like this by making use of the vcf candidate importer. See `--truth_variants` + `--variant_caller=vcf_candidate_importer`. I would expect that if you perform filtering similarly on your training and test data, that this could be a way to develop a model specific to certain size INDEL variants, but we have never tried to do something like this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/813
https://github.com/google/deepvariant/issues/813:413,safety,test,test,413,"`types_to_alt_align` refers to the type of variants in which we perform alignments against the alternative variant, when you have also set the `alt_aligned_pileup` flag. . You might be able to accomplish something like this by making use of the vcf candidate importer. See `--truth_variants` + `--variant_caller=vcf_candidate_importer`. I would expect that if you perform filtering similarly on your training and test data, that this could be a way to develop a model specific to certain size INDEL variants, but we have never tried to do something like this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/813
https://github.com/google/deepvariant/issues/813:462,security,model,model,462,"`types_to_alt_align` refers to the type of variants in which we perform alignments against the alternative variant, when you have also set the `alt_aligned_pileup` flag. . You might be able to accomplish something like this by making use of the vcf candidate importer. See `--truth_variants` + `--variant_caller=vcf_candidate_importer`. I would expect that if you perform filtering similarly on your training and test data, that this could be a way to develop a model specific to certain size INDEL variants, but we have never tried to do something like this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/813
https://github.com/google/deepvariant/issues/813:413,testability,test,test,413,"`types_to_alt_align` refers to the type of variants in which we perform alignments against the alternative variant, when you have also set the `alt_aligned_pileup` flag. . You might be able to accomplish something like this by making use of the vcf candidate importer. See `--truth_variants` + `--variant_caller=vcf_candidate_importer`. I would expect that if you perform filtering similarly on your training and test data, that this could be a way to develop a model specific to certain size INDEL variants, but we have never tried to do something like this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/813
https://github.com/google/deepvariant/issues/813:64,usability,perform,perform,64,"`types_to_alt_align` refers to the type of variants in which we perform alignments against the alternative variant, when you have also set the `alt_aligned_pileup` flag. . You might be able to accomplish something like this by making use of the vcf candidate importer. See `--truth_variants` + `--variant_caller=vcf_candidate_importer`. I would expect that if you perform filtering similarly on your training and test data, that this could be a way to develop a model specific to certain size INDEL variants, but we have never tried to do something like this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/813
https://github.com/google/deepvariant/issues/813:364,usability,perform,perform,364,"`types_to_alt_align` refers to the type of variants in which we perform alignments against the alternative variant, when you have also set the `alt_aligned_pileup` flag. . You might be able to accomplish something like this by making use of the vcf candidate importer. See `--truth_variants` + `--variant_caller=vcf_candidate_importer`. I would expect that if you perform filtering similarly on your training and test data, that this could be a way to develop a model specific to certain size INDEL variants, but we have never tried to do something like this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/813
https://github.com/google/deepvariant/issues/813:119,deployability,observ,observed,119,"Hi @danielecook , I run the `run_deepvariant` command with and without `--select_variant_types='indels'` parameter and observed different number of INDELS between two outputs. My understanding is that this command will remove all the SNP candidates but surprisingly, it also lowers the number of INDELS variants. I attached the visual report of deepvariant result with and without this parameter. When all type of variants are considered:. ![HG003_all visual_report](https://github.com/google/deepvariant/assets/135994439/d8281b58-1d81-4cee-bf76-8e06288d702f). Only INDELS variants are reported:. ![HG003_indels visual_report](https://github.com/google/deepvariant/assets/135994439/ebbe5a77-2a60-4bbc-8aeb-dd2dbc9381ca).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/813
https://github.com/google/deepvariant/issues/813:105,modifiability,paramet,parameter,105,"Hi @danielecook , I run the `run_deepvariant` command with and without `--select_variant_types='indels'` parameter and observed different number of INDELS between two outputs. My understanding is that this command will remove all the SNP candidates but surprisingly, it also lowers the number of INDELS variants. I attached the visual report of deepvariant result with and without this parameter. When all type of variants are considered:. ![HG003_all visual_report](https://github.com/google/deepvariant/assets/135994439/d8281b58-1d81-4cee-bf76-8e06288d702f). Only INDELS variants are reported:. ![HG003_indels visual_report](https://github.com/google/deepvariant/assets/135994439/ebbe5a77-2a60-4bbc-8aeb-dd2dbc9381ca).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/813
https://github.com/google/deepvariant/issues/813:386,modifiability,paramet,parameter,386,"Hi @danielecook , I run the `run_deepvariant` command with and without `--select_variant_types='indels'` parameter and observed different number of INDELS between two outputs. My understanding is that this command will remove all the SNP candidates but surprisingly, it also lowers the number of INDELS variants. I attached the visual report of deepvariant result with and without this parameter. When all type of variants are considered:. ![HG003_all visual_report](https://github.com/google/deepvariant/assets/135994439/d8281b58-1d81-4cee-bf76-8e06288d702f). Only INDELS variants are reported:. ![HG003_indels visual_report](https://github.com/google/deepvariant/assets/135994439/ebbe5a77-2a60-4bbc-8aeb-dd2dbc9381ca).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/813
https://github.com/google/deepvariant/issues/813:119,testability,observ,observed,119,"Hi @danielecook , I run the `run_deepvariant` command with and without `--select_variant_types='indels'` parameter and observed different number of INDELS between two outputs. My understanding is that this command will remove all the SNP candidates but surprisingly, it also lowers the number of INDELS variants. I attached the visual report of deepvariant result with and without this parameter. When all type of variants are considered:. ![HG003_all visual_report](https://github.com/google/deepvariant/assets/135994439/d8281b58-1d81-4cee-bf76-8e06288d702f). Only INDELS variants are reported:. ![HG003_indels visual_report](https://github.com/google/deepvariant/assets/135994439/ebbe5a77-2a60-4bbc-8aeb-dd2dbc9381ca).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/813
https://github.com/google/deepvariant/issues/813:179,testability,understand,understanding,179,"Hi @danielecook , I run the `run_deepvariant` command with and without `--select_variant_types='indels'` parameter and observed different number of INDELS between two outputs. My understanding is that this command will remove all the SNP candidates but surprisingly, it also lowers the number of INDELS variants. I attached the visual report of deepvariant result with and without this parameter. When all type of variants are considered:. ![HG003_all visual_report](https://github.com/google/deepvariant/assets/135994439/d8281b58-1d81-4cee-bf76-8e06288d702f). Only INDELS variants are reported:. ![HG003_indels visual_report](https://github.com/google/deepvariant/assets/135994439/ebbe5a77-2a60-4bbc-8aeb-dd2dbc9381ca).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/813
https://github.com/google/deepvariant/issues/813:46,usability,command,command,46,"Hi @danielecook , I run the `run_deepvariant` command with and without `--select_variant_types='indels'` parameter and observed different number of INDELS between two outputs. My understanding is that this command will remove all the SNP candidates but surprisingly, it also lowers the number of INDELS variants. I attached the visual report of deepvariant result with and without this parameter. When all type of variants are considered:. ![HG003_all visual_report](https://github.com/google/deepvariant/assets/135994439/d8281b58-1d81-4cee-bf76-8e06288d702f). Only INDELS variants are reported:. ![HG003_indels visual_report](https://github.com/google/deepvariant/assets/135994439/ebbe5a77-2a60-4bbc-8aeb-dd2dbc9381ca).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/813
https://github.com/google/deepvariant/issues/813:206,usability,command,command,206,"Hi @danielecook , I run the `run_deepvariant` command with and without `--select_variant_types='indels'` parameter and observed different number of INDELS between two outputs. My understanding is that this command will remove all the SNP candidates but surprisingly, it also lowers the number of INDELS variants. I attached the visual report of deepvariant result with and without this parameter. When all type of variants are considered:. ![HG003_all visual_report](https://github.com/google/deepvariant/assets/135994439/d8281b58-1d81-4cee-bf76-8e06288d702f). Only INDELS variants are reported:. ![HG003_indels visual_report](https://github.com/google/deepvariant/assets/135994439/ebbe5a77-2a60-4bbc-8aeb-dd2dbc9381ca).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/813
https://github.com/google/deepvariant/issues/813:328,usability,visual,visual,328,"Hi @danielecook , I run the `run_deepvariant` command with and without `--select_variant_types='indels'` parameter and observed different number of INDELS between two outputs. My understanding is that this command will remove all the SNP candidates but surprisingly, it also lowers the number of INDELS variants. I attached the visual report of deepvariant result with and without this parameter. When all type of variants are considered:. ![HG003_all visual_report](https://github.com/google/deepvariant/assets/135994439/d8281b58-1d81-4cee-bf76-8e06288d702f). Only INDELS variants are reported:. ![HG003_indels visual_report](https://github.com/google/deepvariant/assets/135994439/ebbe5a77-2a60-4bbc-8aeb-dd2dbc9381ca).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/813
https://github.com/google/deepvariant/issues/813:36,interoperability,share,share,36,"Hi @sophienguyen01,. Can you please share the VCF files so I can look at it? I think the issue would be multi-allelic sites, but I want to confirm that.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/813
https://github.com/google/deepvariant/issues/813:139,usability,confirm,confirm,139,"Hi @sophienguyen01,. Can you please share the VCF files so I can look at it? I think the issue would be multi-allelic sites, but I want to confirm that.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/813
https://github.com/google/deepvariant/issues/813:2310,availability,avail,available,2310," subtract:. ```bash. bedtools subtract \. -a HG003_043024.indels_only.bcftools_filter.vcf.gz \. -b HG003_indels_043024.vcf.gz | wc -l. 180869. ```. So roughly it matches. Now look at some variants:. ```bash. bedtools subtract \. -a HG003_043024.indels_only.bcftools_filter.vcf.gz \. -b HG003_indels_043024.vcf.gz | head. chr1	10247	.	TAAACCCTA	T	0.5	RefCall	.	GT:GQ:DP:AD:VAF:PL	./.:9:41:28,4:0.097561:0,14,10. chr1	98999	.	TTTTATTTA	T,TTTTATTTATTTA	20	PASS	.	GT:GQ:DP:AD:VAF:PL	1/2:10:31:20,9,2:0.290323,0.0645161:19,16,12,16,0,21. chr1	99092	.	C	CT	2.7	RefCall	.	GT:GQ:DP:AD:VAF:PL	./.:3:50:19,7:0.14:0,1,8. chr1	101674	.	C	CAAA	0.6	RefCall	.	GT:GQ:DP:AD:VAF:PL	./.:9:29:23,2:0.0689655:0,8,17. chr1	104160	.	A	AACAC,AACACACAC	15.1	PASS	.	GT:GQ:DP:AD:VAF:PL	1/2:5:79:1,37,21:0.468354,0.265823:13,14,6,14,0,9. chr1	108545	.	C	CA	2.7	RefCall	.	GT:GQ:DP:AD:VAF:PL	./.:3:44:12,21:0.477273:0,1,6. chr1	109575	.	CGT	C,CGTGTGT	13	PASS	.	GT:GQ:DP:AD:VAF:PL	1/2:4:22:0,8,10:0.363636,0.454545:11,13,15,13,0,4. chr1	111513	.	C	CTA	19.3	PASS	.	GT:GQ:DP:AD:VAF:PL	1/1:18:33:0,30:0.909091:19,22,0. chr1	180150	.	AC	A,GC	15	PASS	.	GT:GQ:DP:AD:VAF:PL	1/2:2:19:2,6,9:0.315789,0.473684:11,13,2,13,0,1. chr1	180174	.	TAA	T	3.5	PASS	.	GT:GQ:DP:AD:VAF:PL	1/1:3:14:7,4:0.285714:0,9,0. ```. So there are few variants that we are not picking up. Next, I picked the region where variant ""chr1 10247"" is and ran make_examples with a debug command:. Without filter command:. ```bash. chr1 10240 T ['TA']. chr1 10246 TA ['T']. chr1 10249 A ['C']. chr1 10253 TA ['T']. chr1 10256 A ['C']. ```. I see these five variants. With filtering I see:. ```bash. FILTERING CANDIDATES. chr1 10240 T ['TA']. chr1 10246 TA ['T']. chr1 10253 TA ['T']. ```. I am unsure how to reproduce this. Are you using a publicly available bam file? I can also run DV with and without this command and generate results to investigate further. It would be faster and helpful if you can point me to the bam you are using so it's more specific to your issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/813
https://github.com/google/deepvariant/issues/813:536,integrability,sub,subtract,536,"@sophienguyen01 ,. I think I was able to get to the problem but unfortunately, I am unable to reproduce the issue. . Here's what I did:. Extract indels from the full file:. ```bash. bcftools view -v indels HG003_043024.vcf.gz > HG003_043024.indels_only.bcftools_filter.vcf. ```. Run stats:. ```. bcftools stats HG003_043024.indels_only.bcftools_filter.vcf.gz | grep 'indels:'. SN	0	number of indels:	1240956. ```. Compared to parameter-based:. ```bash. SN	0	number of indels:	1056774. ```. So, we are looking for 184182 variants. So do subtract:. ```bash. bedtools subtract \. -a HG003_043024.indels_only.bcftools_filter.vcf.gz \. -b HG003_indels_043024.vcf.gz | wc -l. 180869. ```. So roughly it matches. Now look at some variants:. ```bash. bedtools subtract \. -a HG003_043024.indels_only.bcftools_filter.vcf.gz \. -b HG003_indels_043024.vcf.gz | head. chr1	10247	.	TAAACCCTA	T	0.5	RefCall	.	GT:GQ:DP:AD:VAF:PL	./.:9:41:28,4:0.097561:0,14,10. chr1	98999	.	TTTTATTTA	T,TTTTATTTATTTA	20	PASS	.	GT:GQ:DP:AD:VAF:PL	1/2:10:31:20,9,2:0.290323,0.0645161:19,16,12,16,0,21. chr1	99092	.	C	CT	2.7	RefCall	.	GT:GQ:DP:AD:VAF:PL	./.:3:50:19,7:0.14:0,1,8. chr1	101674	.	C	CAAA	0.6	RefCall	.	GT:GQ:DP:AD:VAF:PL	./.:9:29:23,2:0.0689655:0,8,17. chr1	104160	.	A	AACAC,AACACACAC	15.1	PASS	.	GT:GQ:DP:AD:VAF:PL	1/2:5:79:1,37,21:0.468354,0.265823:13,14,6,14,0,9. chr1	108545	.	C	CA	2.7	RefCall	.	GT:GQ:DP:AD:VAF:PL	./.:3:44:12,21:0.477273:0,1,6. chr1	109575	.	CGT	C,CGTGTGT	13	PASS	.	GT:GQ:DP:AD:VAF:PL	1/2:4:22:0,8,10:0.363636,0.454545:11,13,15,13,0,4. chr1	111513	.	C	CTA	19.3	PASS	.	GT:GQ:DP:AD:VAF:PL	1/1:18:33:0,30:0.909091:19,22,0. chr1	180150	.	AC	A,GC	15	PASS	.	GT:GQ:DP:AD:VAF:PL	1/2:2:19:2,6,9:0.315789,0.473684:11,13,2,13,0,1. chr1	180174	.	TAA	T	3.5	PASS	.	GT:GQ:DP:AD:VAF:PL	1/1:3:14:7,4:0.285714:0,9,0. ```. So there are few variants that we are not picking up. Next, I picked the region where variant ""chr1 10247"" is and ran make_examples with a debug command:. Without filter command:. ```bash. chr1 10",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/813
https://github.com/google/deepvariant/issues/813:565,integrability,sub,subtract,565,"@sophienguyen01 ,. I think I was able to get to the problem but unfortunately, I am unable to reproduce the issue. . Here's what I did:. Extract indels from the full file:. ```bash. bcftools view -v indels HG003_043024.vcf.gz > HG003_043024.indels_only.bcftools_filter.vcf. ```. Run stats:. ```. bcftools stats HG003_043024.indels_only.bcftools_filter.vcf.gz | grep 'indels:'. SN	0	number of indels:	1240956. ```. Compared to parameter-based:. ```bash. SN	0	number of indels:	1056774. ```. So, we are looking for 184182 variants. So do subtract:. ```bash. bedtools subtract \. -a HG003_043024.indels_only.bcftools_filter.vcf.gz \. -b HG003_indels_043024.vcf.gz | wc -l. 180869. ```. So roughly it matches. Now look at some variants:. ```bash. bedtools subtract \. -a HG003_043024.indels_only.bcftools_filter.vcf.gz \. -b HG003_indels_043024.vcf.gz | head. chr1	10247	.	TAAACCCTA	T	0.5	RefCall	.	GT:GQ:DP:AD:VAF:PL	./.:9:41:28,4:0.097561:0,14,10. chr1	98999	.	TTTTATTTA	T,TTTTATTTATTTA	20	PASS	.	GT:GQ:DP:AD:VAF:PL	1/2:10:31:20,9,2:0.290323,0.0645161:19,16,12,16,0,21. chr1	99092	.	C	CT	2.7	RefCall	.	GT:GQ:DP:AD:VAF:PL	./.:3:50:19,7:0.14:0,1,8. chr1	101674	.	C	CAAA	0.6	RefCall	.	GT:GQ:DP:AD:VAF:PL	./.:9:29:23,2:0.0689655:0,8,17. chr1	104160	.	A	AACAC,AACACACAC	15.1	PASS	.	GT:GQ:DP:AD:VAF:PL	1/2:5:79:1,37,21:0.468354,0.265823:13,14,6,14,0,9. chr1	108545	.	C	CA	2.7	RefCall	.	GT:GQ:DP:AD:VAF:PL	./.:3:44:12,21:0.477273:0,1,6. chr1	109575	.	CGT	C,CGTGTGT	13	PASS	.	GT:GQ:DP:AD:VAF:PL	1/2:4:22:0,8,10:0.363636,0.454545:11,13,15,13,0,4. chr1	111513	.	C	CTA	19.3	PASS	.	GT:GQ:DP:AD:VAF:PL	1/1:18:33:0,30:0.909091:19,22,0. chr1	180150	.	AC	A,GC	15	PASS	.	GT:GQ:DP:AD:VAF:PL	1/2:2:19:2,6,9:0.315789,0.473684:11,13,2,13,0,1. chr1	180174	.	TAA	T	3.5	PASS	.	GT:GQ:DP:AD:VAF:PL	1/1:3:14:7,4:0.285714:0,9,0. ```. So there are few variants that we are not picking up. Next, I picked the region where variant ""chr1 10247"" is and ran make_examples with a debug command:. Without filter command:. ```bash. chr1 10",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/813
https://github.com/google/deepvariant/issues/813:752,integrability,sub,subtract,752,"@sophienguyen01 ,. I think I was able to get to the problem but unfortunately, I am unable to reproduce the issue. . Here's what I did:. Extract indels from the full file:. ```bash. bcftools view -v indels HG003_043024.vcf.gz > HG003_043024.indels_only.bcftools_filter.vcf. ```. Run stats:. ```. bcftools stats HG003_043024.indels_only.bcftools_filter.vcf.gz | grep 'indels:'. SN	0	number of indels:	1240956. ```. Compared to parameter-based:. ```bash. SN	0	number of indels:	1056774. ```. So, we are looking for 184182 variants. So do subtract:. ```bash. bedtools subtract \. -a HG003_043024.indels_only.bcftools_filter.vcf.gz \. -b HG003_indels_043024.vcf.gz | wc -l. 180869. ```. So roughly it matches. Now look at some variants:. ```bash. bedtools subtract \. -a HG003_043024.indels_only.bcftools_filter.vcf.gz \. -b HG003_indels_043024.vcf.gz | head. chr1	10247	.	TAAACCCTA	T	0.5	RefCall	.	GT:GQ:DP:AD:VAF:PL	./.:9:41:28,4:0.097561:0,14,10. chr1	98999	.	TTTTATTTA	T,TTTTATTTATTTA	20	PASS	.	GT:GQ:DP:AD:VAF:PL	1/2:10:31:20,9,2:0.290323,0.0645161:19,16,12,16,0,21. chr1	99092	.	C	CT	2.7	RefCall	.	GT:GQ:DP:AD:VAF:PL	./.:3:50:19,7:0.14:0,1,8. chr1	101674	.	C	CAAA	0.6	RefCall	.	GT:GQ:DP:AD:VAF:PL	./.:9:29:23,2:0.0689655:0,8,17. chr1	104160	.	A	AACAC,AACACACAC	15.1	PASS	.	GT:GQ:DP:AD:VAF:PL	1/2:5:79:1,37,21:0.468354,0.265823:13,14,6,14,0,9. chr1	108545	.	C	CA	2.7	RefCall	.	GT:GQ:DP:AD:VAF:PL	./.:3:44:12,21:0.477273:0,1,6. chr1	109575	.	CGT	C,CGTGTGT	13	PASS	.	GT:GQ:DP:AD:VAF:PL	1/2:4:22:0,8,10:0.363636,0.454545:11,13,15,13,0,4. chr1	111513	.	C	CTA	19.3	PASS	.	GT:GQ:DP:AD:VAF:PL	1/1:18:33:0,30:0.909091:19,22,0. chr1	180150	.	AC	A,GC	15	PASS	.	GT:GQ:DP:AD:VAF:PL	1/2:2:19:2,6,9:0.315789,0.473684:11,13,2,13,0,1. chr1	180174	.	TAA	T	3.5	PASS	.	GT:GQ:DP:AD:VAF:PL	1/1:3:14:7,4:0.285714:0,9,0. ```. So there are few variants that we are not picking up. Next, I picked the region where variant ""chr1 10247"" is and ran make_examples with a debug command:. Without filter command:. ```bash. chr1 10",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/813
https://github.com/google/deepvariant/issues/813:1967,integrability,filter,filter,1967," subtract:. ```bash. bedtools subtract \. -a HG003_043024.indels_only.bcftools_filter.vcf.gz \. -b HG003_indels_043024.vcf.gz | wc -l. 180869. ```. So roughly it matches. Now look at some variants:. ```bash. bedtools subtract \. -a HG003_043024.indels_only.bcftools_filter.vcf.gz \. -b HG003_indels_043024.vcf.gz | head. chr1	10247	.	TAAACCCTA	T	0.5	RefCall	.	GT:GQ:DP:AD:VAF:PL	./.:9:41:28,4:0.097561:0,14,10. chr1	98999	.	TTTTATTTA	T,TTTTATTTATTTA	20	PASS	.	GT:GQ:DP:AD:VAF:PL	1/2:10:31:20,9,2:0.290323,0.0645161:19,16,12,16,0,21. chr1	99092	.	C	CT	2.7	RefCall	.	GT:GQ:DP:AD:VAF:PL	./.:3:50:19,7:0.14:0,1,8. chr1	101674	.	C	CAAA	0.6	RefCall	.	GT:GQ:DP:AD:VAF:PL	./.:9:29:23,2:0.0689655:0,8,17. chr1	104160	.	A	AACAC,AACACACAC	15.1	PASS	.	GT:GQ:DP:AD:VAF:PL	1/2:5:79:1,37,21:0.468354,0.265823:13,14,6,14,0,9. chr1	108545	.	C	CA	2.7	RefCall	.	GT:GQ:DP:AD:VAF:PL	./.:3:44:12,21:0.477273:0,1,6. chr1	109575	.	CGT	C,CGTGTGT	13	PASS	.	GT:GQ:DP:AD:VAF:PL	1/2:4:22:0,8,10:0.363636,0.454545:11,13,15,13,0,4. chr1	111513	.	C	CTA	19.3	PASS	.	GT:GQ:DP:AD:VAF:PL	1/1:18:33:0,30:0.909091:19,22,0. chr1	180150	.	AC	A,GC	15	PASS	.	GT:GQ:DP:AD:VAF:PL	1/2:2:19:2,6,9:0.315789,0.473684:11,13,2,13,0,1. chr1	180174	.	TAA	T	3.5	PASS	.	GT:GQ:DP:AD:VAF:PL	1/1:3:14:7,4:0.285714:0,9,0. ```. So there are few variants that we are not picking up. Next, I picked the region where variant ""chr1 10247"" is and ran make_examples with a debug command:. Without filter command:. ```bash. chr1 10240 T ['TA']. chr1 10246 TA ['T']. chr1 10249 A ['C']. chr1 10253 TA ['T']. chr1 10256 A ['C']. ```. I see these five variants. With filtering I see:. ```bash. FILTERING CANDIDATES. chr1 10240 T ['TA']. chr1 10246 TA ['T']. chr1 10253 TA ['T']. ```. I am unsure how to reproduce this. Are you using a publicly available bam file? I can also run DV with and without this command and generate results to investigate further. It would be faster and helpful if you can point me to the bam you are using so it's more specific to your issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/813
https://github.com/google/deepvariant/issues/813:2133,integrability,filter,filtering,2133," subtract:. ```bash. bedtools subtract \. -a HG003_043024.indels_only.bcftools_filter.vcf.gz \. -b HG003_indels_043024.vcf.gz | wc -l. 180869. ```. So roughly it matches. Now look at some variants:. ```bash. bedtools subtract \. -a HG003_043024.indels_only.bcftools_filter.vcf.gz \. -b HG003_indels_043024.vcf.gz | head. chr1	10247	.	TAAACCCTA	T	0.5	RefCall	.	GT:GQ:DP:AD:VAF:PL	./.:9:41:28,4:0.097561:0,14,10. chr1	98999	.	TTTTATTTA	T,TTTTATTTATTTA	20	PASS	.	GT:GQ:DP:AD:VAF:PL	1/2:10:31:20,9,2:0.290323,0.0645161:19,16,12,16,0,21. chr1	99092	.	C	CT	2.7	RefCall	.	GT:GQ:DP:AD:VAF:PL	./.:3:50:19,7:0.14:0,1,8. chr1	101674	.	C	CAAA	0.6	RefCall	.	GT:GQ:DP:AD:VAF:PL	./.:9:29:23,2:0.0689655:0,8,17. chr1	104160	.	A	AACAC,AACACACAC	15.1	PASS	.	GT:GQ:DP:AD:VAF:PL	1/2:5:79:1,37,21:0.468354,0.265823:13,14,6,14,0,9. chr1	108545	.	C	CA	2.7	RefCall	.	GT:GQ:DP:AD:VAF:PL	./.:3:44:12,21:0.477273:0,1,6. chr1	109575	.	CGT	C,CGTGTGT	13	PASS	.	GT:GQ:DP:AD:VAF:PL	1/2:4:22:0,8,10:0.363636,0.454545:11,13,15,13,0,4. chr1	111513	.	C	CTA	19.3	PASS	.	GT:GQ:DP:AD:VAF:PL	1/1:18:33:0,30:0.909091:19,22,0. chr1	180150	.	AC	A,GC	15	PASS	.	GT:GQ:DP:AD:VAF:PL	1/2:2:19:2,6,9:0.315789,0.473684:11,13,2,13,0,1. chr1	180174	.	TAA	T	3.5	PASS	.	GT:GQ:DP:AD:VAF:PL	1/1:3:14:7,4:0.285714:0,9,0. ```. So there are few variants that we are not picking up. Next, I picked the region where variant ""chr1 10247"" is and ran make_examples with a debug command:. Without filter command:. ```bash. chr1 10240 T ['TA']. chr1 10246 TA ['T']. chr1 10249 A ['C']. chr1 10253 TA ['T']. chr1 10256 A ['C']. ```. I see these five variants. With filtering I see:. ```bash. FILTERING CANDIDATES. chr1 10240 T ['TA']. chr1 10246 TA ['T']. chr1 10253 TA ['T']. ```. I am unsure how to reproduce this. Are you using a publicly available bam file? I can also run DV with and without this command and generate results to investigate further. It would be faster and helpful if you can point me to the bam you are using so it's more specific to your issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/813
https://github.com/google/deepvariant/issues/813:2160,integrability,FILTER,FILTERING,2160," subtract:. ```bash. bedtools subtract \. -a HG003_043024.indels_only.bcftools_filter.vcf.gz \. -b HG003_indels_043024.vcf.gz | wc -l. 180869. ```. So roughly it matches. Now look at some variants:. ```bash. bedtools subtract \. -a HG003_043024.indels_only.bcftools_filter.vcf.gz \. -b HG003_indels_043024.vcf.gz | head. chr1	10247	.	TAAACCCTA	T	0.5	RefCall	.	GT:GQ:DP:AD:VAF:PL	./.:9:41:28,4:0.097561:0,14,10. chr1	98999	.	TTTTATTTA	T,TTTTATTTATTTA	20	PASS	.	GT:GQ:DP:AD:VAF:PL	1/2:10:31:20,9,2:0.290323,0.0645161:19,16,12,16,0,21. chr1	99092	.	C	CT	2.7	RefCall	.	GT:GQ:DP:AD:VAF:PL	./.:3:50:19,7:0.14:0,1,8. chr1	101674	.	C	CAAA	0.6	RefCall	.	GT:GQ:DP:AD:VAF:PL	./.:9:29:23,2:0.0689655:0,8,17. chr1	104160	.	A	AACAC,AACACACAC	15.1	PASS	.	GT:GQ:DP:AD:VAF:PL	1/2:5:79:1,37,21:0.468354,0.265823:13,14,6,14,0,9. chr1	108545	.	C	CA	2.7	RefCall	.	GT:GQ:DP:AD:VAF:PL	./.:3:44:12,21:0.477273:0,1,6. chr1	109575	.	CGT	C,CGTGTGT	13	PASS	.	GT:GQ:DP:AD:VAF:PL	1/2:4:22:0,8,10:0.363636,0.454545:11,13,15,13,0,4. chr1	111513	.	C	CTA	19.3	PASS	.	GT:GQ:DP:AD:VAF:PL	1/1:18:33:0,30:0.909091:19,22,0. chr1	180150	.	AC	A,GC	15	PASS	.	GT:GQ:DP:AD:VAF:PL	1/2:2:19:2,6,9:0.315789,0.473684:11,13,2,13,0,1. chr1	180174	.	TAA	T	3.5	PASS	.	GT:GQ:DP:AD:VAF:PL	1/1:3:14:7,4:0.285714:0,9,0. ```. So there are few variants that we are not picking up. Next, I picked the region where variant ""chr1 10247"" is and ran make_examples with a debug command:. Without filter command:. ```bash. chr1 10240 T ['TA']. chr1 10246 TA ['T']. chr1 10249 A ['C']. chr1 10253 TA ['T']. chr1 10256 A ['C']. ```. I see these five variants. With filtering I see:. ```bash. FILTERING CANDIDATES. chr1 10240 T ['TA']. chr1 10246 TA ['T']. chr1 10253 TA ['T']. ```. I am unsure how to reproduce this. Are you using a publicly available bam file? I can also run DV with and without this command and generate results to investigate further. It would be faster and helpful if you can point me to the bam you are using so it's more specific to your issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/813
https://github.com/google/deepvariant/issues/813:2301,integrability,pub,publicly,2301," subtract:. ```bash. bedtools subtract \. -a HG003_043024.indels_only.bcftools_filter.vcf.gz \. -b HG003_indels_043024.vcf.gz | wc -l. 180869. ```. So roughly it matches. Now look at some variants:. ```bash. bedtools subtract \. -a HG003_043024.indels_only.bcftools_filter.vcf.gz \. -b HG003_indels_043024.vcf.gz | head. chr1	10247	.	TAAACCCTA	T	0.5	RefCall	.	GT:GQ:DP:AD:VAF:PL	./.:9:41:28,4:0.097561:0,14,10. chr1	98999	.	TTTTATTTA	T,TTTTATTTATTTA	20	PASS	.	GT:GQ:DP:AD:VAF:PL	1/2:10:31:20,9,2:0.290323,0.0645161:19,16,12,16,0,21. chr1	99092	.	C	CT	2.7	RefCall	.	GT:GQ:DP:AD:VAF:PL	./.:3:50:19,7:0.14:0,1,8. chr1	101674	.	C	CAAA	0.6	RefCall	.	GT:GQ:DP:AD:VAF:PL	./.:9:29:23,2:0.0689655:0,8,17. chr1	104160	.	A	AACAC,AACACACAC	15.1	PASS	.	GT:GQ:DP:AD:VAF:PL	1/2:5:79:1,37,21:0.468354,0.265823:13,14,6,14,0,9. chr1	108545	.	C	CA	2.7	RefCall	.	GT:GQ:DP:AD:VAF:PL	./.:3:44:12,21:0.477273:0,1,6. chr1	109575	.	CGT	C,CGTGTGT	13	PASS	.	GT:GQ:DP:AD:VAF:PL	1/2:4:22:0,8,10:0.363636,0.454545:11,13,15,13,0,4. chr1	111513	.	C	CTA	19.3	PASS	.	GT:GQ:DP:AD:VAF:PL	1/1:18:33:0,30:0.909091:19,22,0. chr1	180150	.	AC	A,GC	15	PASS	.	GT:GQ:DP:AD:VAF:PL	1/2:2:19:2,6,9:0.315789,0.473684:11,13,2,13,0,1. chr1	180174	.	TAA	T	3.5	PASS	.	GT:GQ:DP:AD:VAF:PL	1/1:3:14:7,4:0.285714:0,9,0. ```. So there are few variants that we are not picking up. Next, I picked the region where variant ""chr1 10247"" is and ran make_examples with a debug command:. Without filter command:. ```bash. chr1 10240 T ['TA']. chr1 10246 TA ['T']. chr1 10249 A ['C']. chr1 10253 TA ['T']. chr1 10256 A ['C']. ```. I see these five variants. With filtering I see:. ```bash. FILTERING CANDIDATES. chr1 10240 T ['TA']. chr1 10246 TA ['T']. chr1 10253 TA ['T']. ```. I am unsure how to reproduce this. Are you using a publicly available bam file? I can also run DV with and without this command and generate results to investigate further. It would be faster and helpful if you can point me to the bam you are using so it's more specific to your issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/813
https://github.com/google/deepvariant/issues/813:2512,interoperability,specif,specific,2512," subtract:. ```bash. bedtools subtract \. -a HG003_043024.indels_only.bcftools_filter.vcf.gz \. -b HG003_indels_043024.vcf.gz | wc -l. 180869. ```. So roughly it matches. Now look at some variants:. ```bash. bedtools subtract \. -a HG003_043024.indels_only.bcftools_filter.vcf.gz \. -b HG003_indels_043024.vcf.gz | head. chr1	10247	.	TAAACCCTA	T	0.5	RefCall	.	GT:GQ:DP:AD:VAF:PL	./.:9:41:28,4:0.097561:0,14,10. chr1	98999	.	TTTTATTTA	T,TTTTATTTATTTA	20	PASS	.	GT:GQ:DP:AD:VAF:PL	1/2:10:31:20,9,2:0.290323,0.0645161:19,16,12,16,0,21. chr1	99092	.	C	CT	2.7	RefCall	.	GT:GQ:DP:AD:VAF:PL	./.:3:50:19,7:0.14:0,1,8. chr1	101674	.	C	CAAA	0.6	RefCall	.	GT:GQ:DP:AD:VAF:PL	./.:9:29:23,2:0.0689655:0,8,17. chr1	104160	.	A	AACAC,AACACACAC	15.1	PASS	.	GT:GQ:DP:AD:VAF:PL	1/2:5:79:1,37,21:0.468354,0.265823:13,14,6,14,0,9. chr1	108545	.	C	CA	2.7	RefCall	.	GT:GQ:DP:AD:VAF:PL	./.:3:44:12,21:0.477273:0,1,6. chr1	109575	.	CGT	C,CGTGTGT	13	PASS	.	GT:GQ:DP:AD:VAF:PL	1/2:4:22:0,8,10:0.363636,0.454545:11,13,15,13,0,4. chr1	111513	.	C	CTA	19.3	PASS	.	GT:GQ:DP:AD:VAF:PL	1/1:18:33:0,30:0.909091:19,22,0. chr1	180150	.	AC	A,GC	15	PASS	.	GT:GQ:DP:AD:VAF:PL	1/2:2:19:2,6,9:0.315789,0.473684:11,13,2,13,0,1. chr1	180174	.	TAA	T	3.5	PASS	.	GT:GQ:DP:AD:VAF:PL	1/1:3:14:7,4:0.285714:0,9,0. ```. So there are few variants that we are not picking up. Next, I picked the region where variant ""chr1 10247"" is and ran make_examples with a debug command:. Without filter command:. ```bash. chr1 10240 T ['TA']. chr1 10246 TA ['T']. chr1 10249 A ['C']. chr1 10253 TA ['T']. chr1 10256 A ['C']. ```. I see these five variants. With filtering I see:. ```bash. FILTERING CANDIDATES. chr1 10240 T ['TA']. chr1 10246 TA ['T']. chr1 10253 TA ['T']. ```. I am unsure how to reproduce this. Are you using a publicly available bam file? I can also run DV with and without this command and generate results to investigate further. It would be faster and helpful if you can point me to the bam you are using so it's more specific to your issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/813
https://github.com/google/deepvariant/issues/813:426,modifiability,paramet,parameter-based,426,"@sophienguyen01 ,. I think I was able to get to the problem but unfortunately, I am unable to reproduce the issue. . Here's what I did:. Extract indels from the full file:. ```bash. bcftools view -v indels HG003_043024.vcf.gz > HG003_043024.indels_only.bcftools_filter.vcf. ```. Run stats:. ```. bcftools stats HG003_043024.indels_only.bcftools_filter.vcf.gz | grep 'indels:'. SN	0	number of indels:	1240956. ```. Compared to parameter-based:. ```bash. SN	0	number of indels:	1056774. ```. So, we are looking for 184182 variants. So do subtract:. ```bash. bedtools subtract \. -a HG003_043024.indels_only.bcftools_filter.vcf.gz \. -b HG003_indels_043024.vcf.gz | wc -l. 180869. ```. So roughly it matches. Now look at some variants:. ```bash. bedtools subtract \. -a HG003_043024.indels_only.bcftools_filter.vcf.gz \. -b HG003_indels_043024.vcf.gz | head. chr1	10247	.	TAAACCCTA	T	0.5	RefCall	.	GT:GQ:DP:AD:VAF:PL	./.:9:41:28,4:0.097561:0,14,10. chr1	98999	.	TTTTATTTA	T,TTTTATTTATTTA	20	PASS	.	GT:GQ:DP:AD:VAF:PL	1/2:10:31:20,9,2:0.290323,0.0645161:19,16,12,16,0,21. chr1	99092	.	C	CT	2.7	RefCall	.	GT:GQ:DP:AD:VAF:PL	./.:3:50:19,7:0.14:0,1,8. chr1	101674	.	C	CAAA	0.6	RefCall	.	GT:GQ:DP:AD:VAF:PL	./.:9:29:23,2:0.0689655:0,8,17. chr1	104160	.	A	AACAC,AACACACAC	15.1	PASS	.	GT:GQ:DP:AD:VAF:PL	1/2:5:79:1,37,21:0.468354,0.265823:13,14,6,14,0,9. chr1	108545	.	C	CA	2.7	RefCall	.	GT:GQ:DP:AD:VAF:PL	./.:3:44:12,21:0.477273:0,1,6. chr1	109575	.	CGT	C,CGTGTGT	13	PASS	.	GT:GQ:DP:AD:VAF:PL	1/2:4:22:0,8,10:0.363636,0.454545:11,13,15,13,0,4. chr1	111513	.	C	CTA	19.3	PASS	.	GT:GQ:DP:AD:VAF:PL	1/1:18:33:0,30:0.909091:19,22,0. chr1	180150	.	AC	A,GC	15	PASS	.	GT:GQ:DP:AD:VAF:PL	1/2:2:19:2,6,9:0.315789,0.473684:11,13,2,13,0,1. chr1	180174	.	TAA	T	3.5	PASS	.	GT:GQ:DP:AD:VAF:PL	1/1:3:14:7,4:0.285714:0,9,0. ```. So there are few variants that we are not picking up. Next, I picked the region where variant ""chr1 10247"" is and ran make_examples with a debug command:. Without filter command:. ```bash. chr1 10",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/813
https://github.com/google/deepvariant/issues/813:2310,reliability,availab,available,2310," subtract:. ```bash. bedtools subtract \. -a HG003_043024.indels_only.bcftools_filter.vcf.gz \. -b HG003_indels_043024.vcf.gz | wc -l. 180869. ```. So roughly it matches. Now look at some variants:. ```bash. bedtools subtract \. -a HG003_043024.indels_only.bcftools_filter.vcf.gz \. -b HG003_indels_043024.vcf.gz | head. chr1	10247	.	TAAACCCTA	T	0.5	RefCall	.	GT:GQ:DP:AD:VAF:PL	./.:9:41:28,4:0.097561:0,14,10. chr1	98999	.	TTTTATTTA	T,TTTTATTTATTTA	20	PASS	.	GT:GQ:DP:AD:VAF:PL	1/2:10:31:20,9,2:0.290323,0.0645161:19,16,12,16,0,21. chr1	99092	.	C	CT	2.7	RefCall	.	GT:GQ:DP:AD:VAF:PL	./.:3:50:19,7:0.14:0,1,8. chr1	101674	.	C	CAAA	0.6	RefCall	.	GT:GQ:DP:AD:VAF:PL	./.:9:29:23,2:0.0689655:0,8,17. chr1	104160	.	A	AACAC,AACACACAC	15.1	PASS	.	GT:GQ:DP:AD:VAF:PL	1/2:5:79:1,37,21:0.468354,0.265823:13,14,6,14,0,9. chr1	108545	.	C	CA	2.7	RefCall	.	GT:GQ:DP:AD:VAF:PL	./.:3:44:12,21:0.477273:0,1,6. chr1	109575	.	CGT	C,CGTGTGT	13	PASS	.	GT:GQ:DP:AD:VAF:PL	1/2:4:22:0,8,10:0.363636,0.454545:11,13,15,13,0,4. chr1	111513	.	C	CTA	19.3	PASS	.	GT:GQ:DP:AD:VAF:PL	1/1:18:33:0,30:0.909091:19,22,0. chr1	180150	.	AC	A,GC	15	PASS	.	GT:GQ:DP:AD:VAF:PL	1/2:2:19:2,6,9:0.315789,0.473684:11,13,2,13,0,1. chr1	180174	.	TAA	T	3.5	PASS	.	GT:GQ:DP:AD:VAF:PL	1/1:3:14:7,4:0.285714:0,9,0. ```. So there are few variants that we are not picking up. Next, I picked the region where variant ""chr1 10247"" is and ran make_examples with a debug command:. Without filter command:. ```bash. chr1 10240 T ['TA']. chr1 10246 TA ['T']. chr1 10249 A ['C']. chr1 10253 TA ['T']. chr1 10256 A ['C']. ```. I see these five variants. With filtering I see:. ```bash. FILTERING CANDIDATES. chr1 10240 T ['TA']. chr1 10246 TA ['T']. chr1 10253 TA ['T']. ```. I am unsure how to reproduce this. Are you using a publicly available bam file? I can also run DV with and without this command and generate results to investigate further. It would be faster and helpful if you can point me to the bam you are using so it's more specific to your issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/813
https://github.com/google/deepvariant/issues/813:2310,safety,avail,available,2310," subtract:. ```bash. bedtools subtract \. -a HG003_043024.indels_only.bcftools_filter.vcf.gz \. -b HG003_indels_043024.vcf.gz | wc -l. 180869. ```. So roughly it matches. Now look at some variants:. ```bash. bedtools subtract \. -a HG003_043024.indels_only.bcftools_filter.vcf.gz \. -b HG003_indels_043024.vcf.gz | head. chr1	10247	.	TAAACCCTA	T	0.5	RefCall	.	GT:GQ:DP:AD:VAF:PL	./.:9:41:28,4:0.097561:0,14,10. chr1	98999	.	TTTTATTTA	T,TTTTATTTATTTA	20	PASS	.	GT:GQ:DP:AD:VAF:PL	1/2:10:31:20,9,2:0.290323,0.0645161:19,16,12,16,0,21. chr1	99092	.	C	CT	2.7	RefCall	.	GT:GQ:DP:AD:VAF:PL	./.:3:50:19,7:0.14:0,1,8. chr1	101674	.	C	CAAA	0.6	RefCall	.	GT:GQ:DP:AD:VAF:PL	./.:9:29:23,2:0.0689655:0,8,17. chr1	104160	.	A	AACAC,AACACACAC	15.1	PASS	.	GT:GQ:DP:AD:VAF:PL	1/2:5:79:1,37,21:0.468354,0.265823:13,14,6,14,0,9. chr1	108545	.	C	CA	2.7	RefCall	.	GT:GQ:DP:AD:VAF:PL	./.:3:44:12,21:0.477273:0,1,6. chr1	109575	.	CGT	C,CGTGTGT	13	PASS	.	GT:GQ:DP:AD:VAF:PL	1/2:4:22:0,8,10:0.363636,0.454545:11,13,15,13,0,4. chr1	111513	.	C	CTA	19.3	PASS	.	GT:GQ:DP:AD:VAF:PL	1/1:18:33:0,30:0.909091:19,22,0. chr1	180150	.	AC	A,GC	15	PASS	.	GT:GQ:DP:AD:VAF:PL	1/2:2:19:2,6,9:0.315789,0.473684:11,13,2,13,0,1. chr1	180174	.	TAA	T	3.5	PASS	.	GT:GQ:DP:AD:VAF:PL	1/1:3:14:7,4:0.285714:0,9,0. ```. So there are few variants that we are not picking up. Next, I picked the region where variant ""chr1 10247"" is and ran make_examples with a debug command:. Without filter command:. ```bash. chr1 10240 T ['TA']. chr1 10246 TA ['T']. chr1 10249 A ['C']. chr1 10253 TA ['T']. chr1 10256 A ['C']. ```. I see these five variants. With filtering I see:. ```bash. FILTERING CANDIDATES. chr1 10240 T ['TA']. chr1 10246 TA ['T']. chr1 10253 TA ['T']. ```. I am unsure how to reproduce this. Are you using a publicly available bam file? I can also run DV with and without this command and generate results to investigate further. It would be faster and helpful if you can point me to the bam you are using so it's more specific to your issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/813
https://github.com/google/deepvariant/issues/813:2310,security,availab,available,2310," subtract:. ```bash. bedtools subtract \. -a HG003_043024.indels_only.bcftools_filter.vcf.gz \. -b HG003_indels_043024.vcf.gz | wc -l. 180869. ```. So roughly it matches. Now look at some variants:. ```bash. bedtools subtract \. -a HG003_043024.indels_only.bcftools_filter.vcf.gz \. -b HG003_indels_043024.vcf.gz | head. chr1	10247	.	TAAACCCTA	T	0.5	RefCall	.	GT:GQ:DP:AD:VAF:PL	./.:9:41:28,4:0.097561:0,14,10. chr1	98999	.	TTTTATTTA	T,TTTTATTTATTTA	20	PASS	.	GT:GQ:DP:AD:VAF:PL	1/2:10:31:20,9,2:0.290323,0.0645161:19,16,12,16,0,21. chr1	99092	.	C	CT	2.7	RefCall	.	GT:GQ:DP:AD:VAF:PL	./.:3:50:19,7:0.14:0,1,8. chr1	101674	.	C	CAAA	0.6	RefCall	.	GT:GQ:DP:AD:VAF:PL	./.:9:29:23,2:0.0689655:0,8,17. chr1	104160	.	A	AACAC,AACACACAC	15.1	PASS	.	GT:GQ:DP:AD:VAF:PL	1/2:5:79:1,37,21:0.468354,0.265823:13,14,6,14,0,9. chr1	108545	.	C	CA	2.7	RefCall	.	GT:GQ:DP:AD:VAF:PL	./.:3:44:12,21:0.477273:0,1,6. chr1	109575	.	CGT	C,CGTGTGT	13	PASS	.	GT:GQ:DP:AD:VAF:PL	1/2:4:22:0,8,10:0.363636,0.454545:11,13,15,13,0,4. chr1	111513	.	C	CTA	19.3	PASS	.	GT:GQ:DP:AD:VAF:PL	1/1:18:33:0,30:0.909091:19,22,0. chr1	180150	.	AC	A,GC	15	PASS	.	GT:GQ:DP:AD:VAF:PL	1/2:2:19:2,6,9:0.315789,0.473684:11,13,2,13,0,1. chr1	180174	.	TAA	T	3.5	PASS	.	GT:GQ:DP:AD:VAF:PL	1/1:3:14:7,4:0.285714:0,9,0. ```. So there are few variants that we are not picking up. Next, I picked the region where variant ""chr1 10247"" is and ran make_examples with a debug command:. Without filter command:. ```bash. chr1 10240 T ['TA']. chr1 10246 TA ['T']. chr1 10249 A ['C']. chr1 10253 TA ['T']. chr1 10256 A ['C']. ```. I see these five variants. With filtering I see:. ```bash. FILTERING CANDIDATES. chr1 10240 T ['TA']. chr1 10246 TA ['T']. chr1 10253 TA ['T']. ```. I am unsure how to reproduce this. Are you using a publicly available bam file? I can also run DV with and without this command and generate results to investigate further. It would be faster and helpful if you can point me to the bam you are using so it's more specific to your issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/813
https://github.com/google/deepvariant/issues/813:1949,usability,command,command,1949," subtract:. ```bash. bedtools subtract \. -a HG003_043024.indels_only.bcftools_filter.vcf.gz \. -b HG003_indels_043024.vcf.gz | wc -l. 180869. ```. So roughly it matches. Now look at some variants:. ```bash. bedtools subtract \. -a HG003_043024.indels_only.bcftools_filter.vcf.gz \. -b HG003_indels_043024.vcf.gz | head. chr1	10247	.	TAAACCCTA	T	0.5	RefCall	.	GT:GQ:DP:AD:VAF:PL	./.:9:41:28,4:0.097561:0,14,10. chr1	98999	.	TTTTATTTA	T,TTTTATTTATTTA	20	PASS	.	GT:GQ:DP:AD:VAF:PL	1/2:10:31:20,9,2:0.290323,0.0645161:19,16,12,16,0,21. chr1	99092	.	C	CT	2.7	RefCall	.	GT:GQ:DP:AD:VAF:PL	./.:3:50:19,7:0.14:0,1,8. chr1	101674	.	C	CAAA	0.6	RefCall	.	GT:GQ:DP:AD:VAF:PL	./.:9:29:23,2:0.0689655:0,8,17. chr1	104160	.	A	AACAC,AACACACAC	15.1	PASS	.	GT:GQ:DP:AD:VAF:PL	1/2:5:79:1,37,21:0.468354,0.265823:13,14,6,14,0,9. chr1	108545	.	C	CA	2.7	RefCall	.	GT:GQ:DP:AD:VAF:PL	./.:3:44:12,21:0.477273:0,1,6. chr1	109575	.	CGT	C,CGTGTGT	13	PASS	.	GT:GQ:DP:AD:VAF:PL	1/2:4:22:0,8,10:0.363636,0.454545:11,13,15,13,0,4. chr1	111513	.	C	CTA	19.3	PASS	.	GT:GQ:DP:AD:VAF:PL	1/1:18:33:0,30:0.909091:19,22,0. chr1	180150	.	AC	A,GC	15	PASS	.	GT:GQ:DP:AD:VAF:PL	1/2:2:19:2,6,9:0.315789,0.473684:11,13,2,13,0,1. chr1	180174	.	TAA	T	3.5	PASS	.	GT:GQ:DP:AD:VAF:PL	1/1:3:14:7,4:0.285714:0,9,0. ```. So there are few variants that we are not picking up. Next, I picked the region where variant ""chr1 10247"" is and ran make_examples with a debug command:. Without filter command:. ```bash. chr1 10240 T ['TA']. chr1 10246 TA ['T']. chr1 10249 A ['C']. chr1 10253 TA ['T']. chr1 10256 A ['C']. ```. I see these five variants. With filtering I see:. ```bash. FILTERING CANDIDATES. chr1 10240 T ['TA']. chr1 10246 TA ['T']. chr1 10253 TA ['T']. ```. I am unsure how to reproduce this. Are you using a publicly available bam file? I can also run DV with and without this command and generate results to investigate further. It would be faster and helpful if you can point me to the bam you are using so it's more specific to your issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/813
https://github.com/google/deepvariant/issues/813:1974,usability,command,command,1974," subtract:. ```bash. bedtools subtract \. -a HG003_043024.indels_only.bcftools_filter.vcf.gz \. -b HG003_indels_043024.vcf.gz | wc -l. 180869. ```. So roughly it matches. Now look at some variants:. ```bash. bedtools subtract \. -a HG003_043024.indels_only.bcftools_filter.vcf.gz \. -b HG003_indels_043024.vcf.gz | head. chr1	10247	.	TAAACCCTA	T	0.5	RefCall	.	GT:GQ:DP:AD:VAF:PL	./.:9:41:28,4:0.097561:0,14,10. chr1	98999	.	TTTTATTTA	T,TTTTATTTATTTA	20	PASS	.	GT:GQ:DP:AD:VAF:PL	1/2:10:31:20,9,2:0.290323,0.0645161:19,16,12,16,0,21. chr1	99092	.	C	CT	2.7	RefCall	.	GT:GQ:DP:AD:VAF:PL	./.:3:50:19,7:0.14:0,1,8. chr1	101674	.	C	CAAA	0.6	RefCall	.	GT:GQ:DP:AD:VAF:PL	./.:9:29:23,2:0.0689655:0,8,17. chr1	104160	.	A	AACAC,AACACACAC	15.1	PASS	.	GT:GQ:DP:AD:VAF:PL	1/2:5:79:1,37,21:0.468354,0.265823:13,14,6,14,0,9. chr1	108545	.	C	CA	2.7	RefCall	.	GT:GQ:DP:AD:VAF:PL	./.:3:44:12,21:0.477273:0,1,6. chr1	109575	.	CGT	C,CGTGTGT	13	PASS	.	GT:GQ:DP:AD:VAF:PL	1/2:4:22:0,8,10:0.363636,0.454545:11,13,15,13,0,4. chr1	111513	.	C	CTA	19.3	PASS	.	GT:GQ:DP:AD:VAF:PL	1/1:18:33:0,30:0.909091:19,22,0. chr1	180150	.	AC	A,GC	15	PASS	.	GT:GQ:DP:AD:VAF:PL	1/2:2:19:2,6,9:0.315789,0.473684:11,13,2,13,0,1. chr1	180174	.	TAA	T	3.5	PASS	.	GT:GQ:DP:AD:VAF:PL	1/1:3:14:7,4:0.285714:0,9,0. ```. So there are few variants that we are not picking up. Next, I picked the region where variant ""chr1 10247"" is and ran make_examples with a debug command:. Without filter command:. ```bash. chr1 10240 T ['TA']. chr1 10246 TA ['T']. chr1 10249 A ['C']. chr1 10253 TA ['T']. chr1 10256 A ['C']. ```. I see these five variants. With filtering I see:. ```bash. FILTERING CANDIDATES. chr1 10240 T ['TA']. chr1 10246 TA ['T']. chr1 10253 TA ['T']. ```. I am unsure how to reproduce this. Are you using a publicly available bam file? I can also run DV with and without this command and generate results to investigate further. It would be faster and helpful if you can point me to the bam you are using so it's more specific to your issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/813
https://github.com/google/deepvariant/issues/813:2370,usability,command,command,2370," subtract:. ```bash. bedtools subtract \. -a HG003_043024.indels_only.bcftools_filter.vcf.gz \. -b HG003_indels_043024.vcf.gz | wc -l. 180869. ```. So roughly it matches. Now look at some variants:. ```bash. bedtools subtract \. -a HG003_043024.indels_only.bcftools_filter.vcf.gz \. -b HG003_indels_043024.vcf.gz | head. chr1	10247	.	TAAACCCTA	T	0.5	RefCall	.	GT:GQ:DP:AD:VAF:PL	./.:9:41:28,4:0.097561:0,14,10. chr1	98999	.	TTTTATTTA	T,TTTTATTTATTTA	20	PASS	.	GT:GQ:DP:AD:VAF:PL	1/2:10:31:20,9,2:0.290323,0.0645161:19,16,12,16,0,21. chr1	99092	.	C	CT	2.7	RefCall	.	GT:GQ:DP:AD:VAF:PL	./.:3:50:19,7:0.14:0,1,8. chr1	101674	.	C	CAAA	0.6	RefCall	.	GT:GQ:DP:AD:VAF:PL	./.:9:29:23,2:0.0689655:0,8,17. chr1	104160	.	A	AACAC,AACACACAC	15.1	PASS	.	GT:GQ:DP:AD:VAF:PL	1/2:5:79:1,37,21:0.468354,0.265823:13,14,6,14,0,9. chr1	108545	.	C	CA	2.7	RefCall	.	GT:GQ:DP:AD:VAF:PL	./.:3:44:12,21:0.477273:0,1,6. chr1	109575	.	CGT	C,CGTGTGT	13	PASS	.	GT:GQ:DP:AD:VAF:PL	1/2:4:22:0,8,10:0.363636,0.454545:11,13,15,13,0,4. chr1	111513	.	C	CTA	19.3	PASS	.	GT:GQ:DP:AD:VAF:PL	1/1:18:33:0,30:0.909091:19,22,0. chr1	180150	.	AC	A,GC	15	PASS	.	GT:GQ:DP:AD:VAF:PL	1/2:2:19:2,6,9:0.315789,0.473684:11,13,2,13,0,1. chr1	180174	.	TAA	T	3.5	PASS	.	GT:GQ:DP:AD:VAF:PL	1/1:3:14:7,4:0.285714:0,9,0. ```. So there are few variants that we are not picking up. Next, I picked the region where variant ""chr1 10247"" is and ran make_examples with a debug command:. Without filter command:. ```bash. chr1 10240 T ['TA']. chr1 10246 TA ['T']. chr1 10249 A ['C']. chr1 10253 TA ['T']. chr1 10256 A ['C']. ```. I see these five variants. With filtering I see:. ```bash. FILTERING CANDIDATES. chr1 10240 T ['TA']. chr1 10246 TA ['T']. chr1 10253 TA ['T']. ```. I am unsure how to reproduce this. Are you using a publicly available bam file? I can also run DV with and without this command and generate results to investigate further. It would be faster and helpful if you can point me to the bam you are using so it's more specific to your issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/813
https://github.com/google/deepvariant/issues/813:2446,usability,help,helpful,2446," subtract:. ```bash. bedtools subtract \. -a HG003_043024.indels_only.bcftools_filter.vcf.gz \. -b HG003_indels_043024.vcf.gz | wc -l. 180869. ```. So roughly it matches. Now look at some variants:. ```bash. bedtools subtract \. -a HG003_043024.indels_only.bcftools_filter.vcf.gz \. -b HG003_indels_043024.vcf.gz | head. chr1	10247	.	TAAACCCTA	T	0.5	RefCall	.	GT:GQ:DP:AD:VAF:PL	./.:9:41:28,4:0.097561:0,14,10. chr1	98999	.	TTTTATTTA	T,TTTTATTTATTTA	20	PASS	.	GT:GQ:DP:AD:VAF:PL	1/2:10:31:20,9,2:0.290323,0.0645161:19,16,12,16,0,21. chr1	99092	.	C	CT	2.7	RefCall	.	GT:GQ:DP:AD:VAF:PL	./.:3:50:19,7:0.14:0,1,8. chr1	101674	.	C	CAAA	0.6	RefCall	.	GT:GQ:DP:AD:VAF:PL	./.:9:29:23,2:0.0689655:0,8,17. chr1	104160	.	A	AACAC,AACACACAC	15.1	PASS	.	GT:GQ:DP:AD:VAF:PL	1/2:5:79:1,37,21:0.468354,0.265823:13,14,6,14,0,9. chr1	108545	.	C	CA	2.7	RefCall	.	GT:GQ:DP:AD:VAF:PL	./.:3:44:12,21:0.477273:0,1,6. chr1	109575	.	CGT	C,CGTGTGT	13	PASS	.	GT:GQ:DP:AD:VAF:PL	1/2:4:22:0,8,10:0.363636,0.454545:11,13,15,13,0,4. chr1	111513	.	C	CTA	19.3	PASS	.	GT:GQ:DP:AD:VAF:PL	1/1:18:33:0,30:0.909091:19,22,0. chr1	180150	.	AC	A,GC	15	PASS	.	GT:GQ:DP:AD:VAF:PL	1/2:2:19:2,6,9:0.315789,0.473684:11,13,2,13,0,1. chr1	180174	.	TAA	T	3.5	PASS	.	GT:GQ:DP:AD:VAF:PL	1/1:3:14:7,4:0.285714:0,9,0. ```. So there are few variants that we are not picking up. Next, I picked the region where variant ""chr1 10247"" is and ran make_examples with a debug command:. Without filter command:. ```bash. chr1 10240 T ['TA']. chr1 10246 TA ['T']. chr1 10249 A ['C']. chr1 10253 TA ['T']. chr1 10256 A ['C']. ```. I see these five variants. With filtering I see:. ```bash. FILTERING CANDIDATES. chr1 10240 T ['TA']. chr1 10246 TA ['T']. chr1 10253 TA ['T']. ```. I am unsure how to reproduce this. Are you using a publicly available bam file? I can also run DV with and without this command and generate results to investigate further. It would be faster and helpful if you can point me to the bam you are using so it's more specific to your issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/813
https://github.com/google/deepvariant/issues/813:170,integrability,pub,public,170,"Hi @kishwarshafin,. It's a HG003 sample, I believe you also use this sample for training DeepVariant. I used our internal HG003 cram file. If you can send me a link to a public HG003 bam/cram file, I can rerun DV and see if the issue still persists. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/813
https://github.com/google/deepvariant/issues/813:57,modifiability,paramet,parameter,57,"Hello, . I found out the reason of the difference. Using parameter `--select_variant_types='indels'` removes multi-allelic site of the INDELS. You can see it using the command:. `bcftools view -i 'N_ALT>1' HG003_043024.vcf.gz` and no multiallelic site shows up. My question is if i use `--select_variant_types='indels multi-allelics'` , DeepVariant output will include INDELS and multiallelic variants of SNPS and INDELS type right?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/813
https://github.com/google/deepvariant/issues/813:168,usability,command,command,168,"Hello, . I found out the reason of the difference. Using parameter `--select_variant_types='indels'` removes multi-allelic site of the INDELS. You can see it using the command:. `bcftools view -i 'N_ALT>1' HG003_043024.vcf.gz` and no multiallelic site shows up. My question is if i use `--select_variant_types='indels multi-allelics'` , DeepVariant output will include INDELS and multiallelic variants of SNPS and INDELS type right?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/813
https://github.com/google/deepvariant/issues/813:59,availability,replic,replicate,59,"@sophienguyen01 , I can run a chr20 test on our end try to replicate. Looking at the code, the filtering logics are implemented [here](https://github.com/google/deepvariant/blob/r1.6.1/deepvariant/make_examples_core.py#L123). ```python. def _select_biallelic_snps(v):. return variant_utils.is_snp(v) and variant_utils.is_biallelic(v). def _select_biallelic_indels(v):. return variant_utils.is_indel(v) and variant_utils.is_biallelic(v). def _select_biallelic_insertions(v):. return variant_utils.has_insertion(v) and variant_utils.is_biallelic(v). def _select_biallelic_deletions(v):. return variant_utils.has_deletion(v) and variant_utils.is_biallelic(v). VARIANT_TYPE_SELECTORS = {. 'snps': _select_biallelic_snps,. 'indels': _select_biallelic_indels,. 'insertions': _select_biallelic_insertions,. 'deletions': _select_biallelic_deletions,. 'multi-allelics': variant_utils.is_multiallelic,. 'all': lambda v: True,. }. ```. And the filtering logic is implemented [here](https://github.com/google/deepvariant/blob/r1.6.1/deepvariant/make_examples_core.py#L905). . It looks like `--select_variant_types='indels multi-allelics'` will give you all multi-allelic indels too. I am unsure if it will solve the issue because the VCF you provided before also misses bi-allelic indels. I will need to debug it further to see if there's something missing. Meanwhile, you can use `--select_variant_types='indels multi-allelics'` to see if it fixes your issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/813
https://github.com/google/deepvariant/issues/813:105,deployability,log,logics,105,"@sophienguyen01 , I can run a chr20 test on our end try to replicate. Looking at the code, the filtering logics are implemented [here](https://github.com/google/deepvariant/blob/r1.6.1/deepvariant/make_examples_core.py#L123). ```python. def _select_biallelic_snps(v):. return variant_utils.is_snp(v) and variant_utils.is_biallelic(v). def _select_biallelic_indels(v):. return variant_utils.is_indel(v) and variant_utils.is_biallelic(v). def _select_biallelic_insertions(v):. return variant_utils.has_insertion(v) and variant_utils.is_biallelic(v). def _select_biallelic_deletions(v):. return variant_utils.has_deletion(v) and variant_utils.is_biallelic(v). VARIANT_TYPE_SELECTORS = {. 'snps': _select_biallelic_snps,. 'indels': _select_biallelic_indels,. 'insertions': _select_biallelic_insertions,. 'deletions': _select_biallelic_deletions,. 'multi-allelics': variant_utils.is_multiallelic,. 'all': lambda v: True,. }. ```. And the filtering logic is implemented [here](https://github.com/google/deepvariant/blob/r1.6.1/deepvariant/make_examples_core.py#L905). . It looks like `--select_variant_types='indels multi-allelics'` will give you all multi-allelic indels too. I am unsure if it will solve the issue because the VCF you provided before also misses bi-allelic indels. I will need to debug it further to see if there's something missing. Meanwhile, you can use `--select_variant_types='indels multi-allelics'` to see if it fixes your issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/813
https://github.com/google/deepvariant/issues/813:943,deployability,log,logic,943,"@sophienguyen01 , I can run a chr20 test on our end try to replicate. Looking at the code, the filtering logics are implemented [here](https://github.com/google/deepvariant/blob/r1.6.1/deepvariant/make_examples_core.py#L123). ```python. def _select_biallelic_snps(v):. return variant_utils.is_snp(v) and variant_utils.is_biallelic(v). def _select_biallelic_indels(v):. return variant_utils.is_indel(v) and variant_utils.is_biallelic(v). def _select_biallelic_insertions(v):. return variant_utils.has_insertion(v) and variant_utils.is_biallelic(v). def _select_biallelic_deletions(v):. return variant_utils.has_deletion(v) and variant_utils.is_biallelic(v). VARIANT_TYPE_SELECTORS = {. 'snps': _select_biallelic_snps,. 'indels': _select_biallelic_indels,. 'insertions': _select_biallelic_insertions,. 'deletions': _select_biallelic_deletions,. 'multi-allelics': variant_utils.is_multiallelic,. 'all': lambda v: True,. }. ```. And the filtering logic is implemented [here](https://github.com/google/deepvariant/blob/r1.6.1/deepvariant/make_examples_core.py#L905). . It looks like `--select_variant_types='indels multi-allelics'` will give you all multi-allelic indels too. I am unsure if it will solve the issue because the VCF you provided before also misses bi-allelic indels. I will need to debug it further to see if there's something missing. Meanwhile, you can use `--select_variant_types='indels multi-allelics'` to see if it fixes your issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/813
https://github.com/google/deepvariant/issues/813:95,integrability,filter,filtering,95,"@sophienguyen01 , I can run a chr20 test on our end try to replicate. Looking at the code, the filtering logics are implemented [here](https://github.com/google/deepvariant/blob/r1.6.1/deepvariant/make_examples_core.py#L123). ```python. def _select_biallelic_snps(v):. return variant_utils.is_snp(v) and variant_utils.is_biallelic(v). def _select_biallelic_indels(v):. return variant_utils.is_indel(v) and variant_utils.is_biallelic(v). def _select_biallelic_insertions(v):. return variant_utils.has_insertion(v) and variant_utils.is_biallelic(v). def _select_biallelic_deletions(v):. return variant_utils.has_deletion(v) and variant_utils.is_biallelic(v). VARIANT_TYPE_SELECTORS = {. 'snps': _select_biallelic_snps,. 'indels': _select_biallelic_indels,. 'insertions': _select_biallelic_insertions,. 'deletions': _select_biallelic_deletions,. 'multi-allelics': variant_utils.is_multiallelic,. 'all': lambda v: True,. }. ```. And the filtering logic is implemented [here](https://github.com/google/deepvariant/blob/r1.6.1/deepvariant/make_examples_core.py#L905). . It looks like `--select_variant_types='indels multi-allelics'` will give you all multi-allelic indels too. I am unsure if it will solve the issue because the VCF you provided before also misses bi-allelic indels. I will need to debug it further to see if there's something missing. Meanwhile, you can use `--select_variant_types='indels multi-allelics'` to see if it fixes your issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/813
https://github.com/google/deepvariant/issues/813:933,integrability,filter,filtering,933,"@sophienguyen01 , I can run a chr20 test on our end try to replicate. Looking at the code, the filtering logics are implemented [here](https://github.com/google/deepvariant/blob/r1.6.1/deepvariant/make_examples_core.py#L123). ```python. def _select_biallelic_snps(v):. return variant_utils.is_snp(v) and variant_utils.is_biallelic(v). def _select_biallelic_indels(v):. return variant_utils.is_indel(v) and variant_utils.is_biallelic(v). def _select_biallelic_insertions(v):. return variant_utils.has_insertion(v) and variant_utils.is_biallelic(v). def _select_biallelic_deletions(v):. return variant_utils.has_deletion(v) and variant_utils.is_biallelic(v). VARIANT_TYPE_SELECTORS = {. 'snps': _select_biallelic_snps,. 'indels': _select_biallelic_indels,. 'insertions': _select_biallelic_insertions,. 'deletions': _select_biallelic_deletions,. 'multi-allelics': variant_utils.is_multiallelic,. 'all': lambda v: True,. }. ```. And the filtering logic is implemented [here](https://github.com/google/deepvariant/blob/r1.6.1/deepvariant/make_examples_core.py#L905). . It looks like `--select_variant_types='indels multi-allelics'` will give you all multi-allelic indels too. I am unsure if it will solve the issue because the VCF you provided before also misses bi-allelic indels. I will need to debug it further to see if there's something missing. Meanwhile, you can use `--select_variant_types='indels multi-allelics'` to see if it fixes your issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/813
https://github.com/google/deepvariant/issues/813:36,safety,test,test,36,"@sophienguyen01 , I can run a chr20 test on our end try to replicate. Looking at the code, the filtering logics are implemented [here](https://github.com/google/deepvariant/blob/r1.6.1/deepvariant/make_examples_core.py#L123). ```python. def _select_biallelic_snps(v):. return variant_utils.is_snp(v) and variant_utils.is_biallelic(v). def _select_biallelic_indels(v):. return variant_utils.is_indel(v) and variant_utils.is_biallelic(v). def _select_biallelic_insertions(v):. return variant_utils.has_insertion(v) and variant_utils.is_biallelic(v). def _select_biallelic_deletions(v):. return variant_utils.has_deletion(v) and variant_utils.is_biallelic(v). VARIANT_TYPE_SELECTORS = {. 'snps': _select_biallelic_snps,. 'indels': _select_biallelic_indels,. 'insertions': _select_biallelic_insertions,. 'deletions': _select_biallelic_deletions,. 'multi-allelics': variant_utils.is_multiallelic,. 'all': lambda v: True,. }. ```. And the filtering logic is implemented [here](https://github.com/google/deepvariant/blob/r1.6.1/deepvariant/make_examples_core.py#L905). . It looks like `--select_variant_types='indels multi-allelics'` will give you all multi-allelic indels too. I am unsure if it will solve the issue because the VCF you provided before also misses bi-allelic indels. I will need to debug it further to see if there's something missing. Meanwhile, you can use `--select_variant_types='indels multi-allelics'` to see if it fixes your issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/813
https://github.com/google/deepvariant/issues/813:105,safety,log,logics,105,"@sophienguyen01 , I can run a chr20 test on our end try to replicate. Looking at the code, the filtering logics are implemented [here](https://github.com/google/deepvariant/blob/r1.6.1/deepvariant/make_examples_core.py#L123). ```python. def _select_biallelic_snps(v):. return variant_utils.is_snp(v) and variant_utils.is_biallelic(v). def _select_biallelic_indels(v):. return variant_utils.is_indel(v) and variant_utils.is_biallelic(v). def _select_biallelic_insertions(v):. return variant_utils.has_insertion(v) and variant_utils.is_biallelic(v). def _select_biallelic_deletions(v):. return variant_utils.has_deletion(v) and variant_utils.is_biallelic(v). VARIANT_TYPE_SELECTORS = {. 'snps': _select_biallelic_snps,. 'indels': _select_biallelic_indels,. 'insertions': _select_biallelic_insertions,. 'deletions': _select_biallelic_deletions,. 'multi-allelics': variant_utils.is_multiallelic,. 'all': lambda v: True,. }. ```. And the filtering logic is implemented [here](https://github.com/google/deepvariant/blob/r1.6.1/deepvariant/make_examples_core.py#L905). . It looks like `--select_variant_types='indels multi-allelics'` will give you all multi-allelic indels too. I am unsure if it will solve the issue because the VCF you provided before also misses bi-allelic indels. I will need to debug it further to see if there's something missing. Meanwhile, you can use `--select_variant_types='indels multi-allelics'` to see if it fixes your issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/813
https://github.com/google/deepvariant/issues/813:943,safety,log,logic,943,"@sophienguyen01 , I can run a chr20 test on our end try to replicate. Looking at the code, the filtering logics are implemented [here](https://github.com/google/deepvariant/blob/r1.6.1/deepvariant/make_examples_core.py#L123). ```python. def _select_biallelic_snps(v):. return variant_utils.is_snp(v) and variant_utils.is_biallelic(v). def _select_biallelic_indels(v):. return variant_utils.is_indel(v) and variant_utils.is_biallelic(v). def _select_biallelic_insertions(v):. return variant_utils.has_insertion(v) and variant_utils.is_biallelic(v). def _select_biallelic_deletions(v):. return variant_utils.has_deletion(v) and variant_utils.is_biallelic(v). VARIANT_TYPE_SELECTORS = {. 'snps': _select_biallelic_snps,. 'indels': _select_biallelic_indels,. 'insertions': _select_biallelic_insertions,. 'deletions': _select_biallelic_deletions,. 'multi-allelics': variant_utils.is_multiallelic,. 'all': lambda v: True,. }. ```. And the filtering logic is implemented [here](https://github.com/google/deepvariant/blob/r1.6.1/deepvariant/make_examples_core.py#L905). . It looks like `--select_variant_types='indels multi-allelics'` will give you all multi-allelic indels too. I am unsure if it will solve the issue because the VCF you provided before also misses bi-allelic indels. I will need to debug it further to see if there's something missing. Meanwhile, you can use `--select_variant_types='indels multi-allelics'` to see if it fixes your issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/813
https://github.com/google/deepvariant/issues/813:105,security,log,logics,105,"@sophienguyen01 , I can run a chr20 test on our end try to replicate. Looking at the code, the filtering logics are implemented [here](https://github.com/google/deepvariant/blob/r1.6.1/deepvariant/make_examples_core.py#L123). ```python. def _select_biallelic_snps(v):. return variant_utils.is_snp(v) and variant_utils.is_biallelic(v). def _select_biallelic_indels(v):. return variant_utils.is_indel(v) and variant_utils.is_biallelic(v). def _select_biallelic_insertions(v):. return variant_utils.has_insertion(v) and variant_utils.is_biallelic(v). def _select_biallelic_deletions(v):. return variant_utils.has_deletion(v) and variant_utils.is_biallelic(v). VARIANT_TYPE_SELECTORS = {. 'snps': _select_biallelic_snps,. 'indels': _select_biallelic_indels,. 'insertions': _select_biallelic_insertions,. 'deletions': _select_biallelic_deletions,. 'multi-allelics': variant_utils.is_multiallelic,. 'all': lambda v: True,. }. ```. And the filtering logic is implemented [here](https://github.com/google/deepvariant/blob/r1.6.1/deepvariant/make_examples_core.py#L905). . It looks like `--select_variant_types='indels multi-allelics'` will give you all multi-allelic indels too. I am unsure if it will solve the issue because the VCF you provided before also misses bi-allelic indels. I will need to debug it further to see if there's something missing. Meanwhile, you can use `--select_variant_types='indels multi-allelics'` to see if it fixes your issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/813
https://github.com/google/deepvariant/issues/813:943,security,log,logic,943,"@sophienguyen01 , I can run a chr20 test on our end try to replicate. Looking at the code, the filtering logics are implemented [here](https://github.com/google/deepvariant/blob/r1.6.1/deepvariant/make_examples_core.py#L123). ```python. def _select_biallelic_snps(v):. return variant_utils.is_snp(v) and variant_utils.is_biallelic(v). def _select_biallelic_indels(v):. return variant_utils.is_indel(v) and variant_utils.is_biallelic(v). def _select_biallelic_insertions(v):. return variant_utils.has_insertion(v) and variant_utils.is_biallelic(v). def _select_biallelic_deletions(v):. return variant_utils.has_deletion(v) and variant_utils.is_biallelic(v). VARIANT_TYPE_SELECTORS = {. 'snps': _select_biallelic_snps,. 'indels': _select_biallelic_indels,. 'insertions': _select_biallelic_insertions,. 'deletions': _select_biallelic_deletions,. 'multi-allelics': variant_utils.is_multiallelic,. 'all': lambda v: True,. }. ```. And the filtering logic is implemented [here](https://github.com/google/deepvariant/blob/r1.6.1/deepvariant/make_examples_core.py#L905). . It looks like `--select_variant_types='indels multi-allelics'` will give you all multi-allelic indels too. I am unsure if it will solve the issue because the VCF you provided before also misses bi-allelic indels. I will need to debug it further to see if there's something missing. Meanwhile, you can use `--select_variant_types='indels multi-allelics'` to see if it fixes your issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/813
https://github.com/google/deepvariant/issues/813:36,testability,test,test,36,"@sophienguyen01 , I can run a chr20 test on our end try to replicate. Looking at the code, the filtering logics are implemented [here](https://github.com/google/deepvariant/blob/r1.6.1/deepvariant/make_examples_core.py#L123). ```python. def _select_biallelic_snps(v):. return variant_utils.is_snp(v) and variant_utils.is_biallelic(v). def _select_biallelic_indels(v):. return variant_utils.is_indel(v) and variant_utils.is_biallelic(v). def _select_biallelic_insertions(v):. return variant_utils.has_insertion(v) and variant_utils.is_biallelic(v). def _select_biallelic_deletions(v):. return variant_utils.has_deletion(v) and variant_utils.is_biallelic(v). VARIANT_TYPE_SELECTORS = {. 'snps': _select_biallelic_snps,. 'indels': _select_biallelic_indels,. 'insertions': _select_biallelic_insertions,. 'deletions': _select_biallelic_deletions,. 'multi-allelics': variant_utils.is_multiallelic,. 'all': lambda v: True,. }. ```. And the filtering logic is implemented [here](https://github.com/google/deepvariant/blob/r1.6.1/deepvariant/make_examples_core.py#L905). . It looks like `--select_variant_types='indels multi-allelics'` will give you all multi-allelic indels too. I am unsure if it will solve the issue because the VCF you provided before also misses bi-allelic indels. I will need to debug it further to see if there's something missing. Meanwhile, you can use `--select_variant_types='indels multi-allelics'` to see if it fixes your issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/813
https://github.com/google/deepvariant/issues/813:105,testability,log,logics,105,"@sophienguyen01 , I can run a chr20 test on our end try to replicate. Looking at the code, the filtering logics are implemented [here](https://github.com/google/deepvariant/blob/r1.6.1/deepvariant/make_examples_core.py#L123). ```python. def _select_biallelic_snps(v):. return variant_utils.is_snp(v) and variant_utils.is_biallelic(v). def _select_biallelic_indels(v):. return variant_utils.is_indel(v) and variant_utils.is_biallelic(v). def _select_biallelic_insertions(v):. return variant_utils.has_insertion(v) and variant_utils.is_biallelic(v). def _select_biallelic_deletions(v):. return variant_utils.has_deletion(v) and variant_utils.is_biallelic(v). VARIANT_TYPE_SELECTORS = {. 'snps': _select_biallelic_snps,. 'indels': _select_biallelic_indels,. 'insertions': _select_biallelic_insertions,. 'deletions': _select_biallelic_deletions,. 'multi-allelics': variant_utils.is_multiallelic,. 'all': lambda v: True,. }. ```. And the filtering logic is implemented [here](https://github.com/google/deepvariant/blob/r1.6.1/deepvariant/make_examples_core.py#L905). . It looks like `--select_variant_types='indels multi-allelics'` will give you all multi-allelic indels too. I am unsure if it will solve the issue because the VCF you provided before also misses bi-allelic indels. I will need to debug it further to see if there's something missing. Meanwhile, you can use `--select_variant_types='indels multi-allelics'` to see if it fixes your issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/813
https://github.com/google/deepvariant/issues/813:943,testability,log,logic,943,"@sophienguyen01 , I can run a chr20 test on our end try to replicate. Looking at the code, the filtering logics are implemented [here](https://github.com/google/deepvariant/blob/r1.6.1/deepvariant/make_examples_core.py#L123). ```python. def _select_biallelic_snps(v):. return variant_utils.is_snp(v) and variant_utils.is_biallelic(v). def _select_biallelic_indels(v):. return variant_utils.is_indel(v) and variant_utils.is_biallelic(v). def _select_biallelic_insertions(v):. return variant_utils.has_insertion(v) and variant_utils.is_biallelic(v). def _select_biallelic_deletions(v):. return variant_utils.has_deletion(v) and variant_utils.is_biallelic(v). VARIANT_TYPE_SELECTORS = {. 'snps': _select_biallelic_snps,. 'indels': _select_biallelic_indels,. 'insertions': _select_biallelic_insertions,. 'deletions': _select_biallelic_deletions,. 'multi-allelics': variant_utils.is_multiallelic,. 'all': lambda v: True,. }. ```. And the filtering logic is implemented [here](https://github.com/google/deepvariant/blob/r1.6.1/deepvariant/make_examples_core.py#L905). . It looks like `--select_variant_types='indels multi-allelics'` will give you all multi-allelic indels too. I am unsure if it will solve the issue because the VCF you provided before also misses bi-allelic indels. I will need to debug it further to see if there's something missing. Meanwhile, you can use `--select_variant_types='indels multi-allelics'` to see if it fixes your issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/813
https://github.com/google/deepvariant/issues/813:386,deployability,contain,contains,386,"Hi,. I tried `--select_variant_types='indels multi-allelics'`, the vcf includes all the INDELS from vcf output that is not included with `--select_variant_types`. However, the output vcf using `--select_variant_types='indels multi-allelics'` also includes SNPs variants. . My purpose is to create INDELs training examples only and `--select_variant_types='indels multi-allelics'` still contains SNPs examples. Is there a way to filter examples (from tfrecord???.gz files) after `make_examples` step but before `shuffling` step. I also tried `--truth_variants` (using the truth vcf that only contains INDEL variants) and `-variant_caller=vcf_candidate_importer` parameter, but the number of examples are reduced significantly.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/813
https://github.com/google/deepvariant/issues/813:591,deployability,contain,contains,591,"Hi,. I tried `--select_variant_types='indels multi-allelics'`, the vcf includes all the INDELS from vcf output that is not included with `--select_variant_types`. However, the output vcf using `--select_variant_types='indels multi-allelics'` also includes SNPs variants. . My purpose is to create INDELs training examples only and `--select_variant_types='indels multi-allelics'` still contains SNPs examples. Is there a way to filter examples (from tfrecord???.gz files) after `make_examples` step but before `shuffling` step. I also tried `--truth_variants` (using the truth vcf that only contains INDEL variants) and `-variant_caller=vcf_candidate_importer` parameter, but the number of examples are reduced significantly.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/813
https://github.com/google/deepvariant/issues/813:703,energy efficiency,reduc,reduced,703,"Hi,. I tried `--select_variant_types='indels multi-allelics'`, the vcf includes all the INDELS from vcf output that is not included with `--select_variant_types`. However, the output vcf using `--select_variant_types='indels multi-allelics'` also includes SNPs variants. . My purpose is to create INDELs training examples only and `--select_variant_types='indels multi-allelics'` still contains SNPs examples. Is there a way to filter examples (from tfrecord???.gz files) after `make_examples` step but before `shuffling` step. I also tried `--truth_variants` (using the truth vcf that only contains INDEL variants) and `-variant_caller=vcf_candidate_importer` parameter, but the number of examples are reduced significantly.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/813
https://github.com/google/deepvariant/issues/813:428,integrability,filter,filter,428,"Hi,. I tried `--select_variant_types='indels multi-allelics'`, the vcf includes all the INDELS from vcf output that is not included with `--select_variant_types`. However, the output vcf using `--select_variant_types='indels multi-allelics'` also includes SNPs variants. . My purpose is to create INDELs training examples only and `--select_variant_types='indels multi-allelics'` still contains SNPs examples. Is there a way to filter examples (from tfrecord???.gz files) after `make_examples` step but before `shuffling` step. I also tried `--truth_variants` (using the truth vcf that only contains INDEL variants) and `-variant_caller=vcf_candidate_importer` parameter, but the number of examples are reduced significantly.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/813
https://github.com/google/deepvariant/issues/813:661,modifiability,paramet,parameter,661,"Hi,. I tried `--select_variant_types='indels multi-allelics'`, the vcf includes all the INDELS from vcf output that is not included with `--select_variant_types`. However, the output vcf using `--select_variant_types='indels multi-allelics'` also includes SNPs variants. . My purpose is to create INDELs training examples only and `--select_variant_types='indels multi-allelics'` still contains SNPs examples. Is there a way to filter examples (from tfrecord???.gz files) after `make_examples` step but before `shuffling` step. I also tried `--truth_variants` (using the truth vcf that only contains INDEL variants) and `-variant_caller=vcf_candidate_importer` parameter, but the number of examples are reduced significantly.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/813
https://github.com/google/deepvariant/issues/813:711,security,sign,significantly,711,"Hi,. I tried `--select_variant_types='indels multi-allelics'`, the vcf includes all the INDELS from vcf output that is not included with `--select_variant_types`. However, the output vcf using `--select_variant_types='indels multi-allelics'` also includes SNPs variants. . My purpose is to create INDELs training examples only and `--select_variant_types='indels multi-allelics'` still contains SNPs examples. Is there a way to filter examples (from tfrecord???.gz files) after `make_examples` step but before `shuffling` step. I also tried `--truth_variants` (using the truth vcf that only contains INDEL variants) and `-variant_caller=vcf_candidate_importer` parameter, but the number of examples are reduced significantly.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/813
https://github.com/google/deepvariant/issues/813:271,availability,down,downsampling,271,"@sophienguyen01 , . If you are looking for INDEL purity then `--select_variant_types='indels'` should work well I believe? You are losing some INDELs but ultimately you are creating a training set that contains only INDELs. You can also generate more training samples by downsampling the bam and re-running `make_examples`. On the other hand, if you think about inference pipeline, when you are in prediction mode, you can't switch your `--select_variant_types` to something else than your training data as the model will now have to deal with data it has never seen before. There can be two scenarios:. 1) You train a model with `--select_variant_types='indels'` and after the model is trained, you are running inference with `--select_variant_types='indels multi-allelics'`. Then the model will have to deal with mult-allelic variants it has never seen before so the prediction on those would expected to be poor. 2) You train a model with `--select_variant_types='indels multi-allelics'` and allow a few snps to be present in the training. In which case during your inference or prediction, the model will be confident on all indel cases. It depends on what exactly you are trying to do, but, my suggestion would be to use `--select_variant_types='indels multi-allelics'` for your purpose.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/813
https://github.com/google/deepvariant/issues/813:202,deployability,contain,contains,202,"@sophienguyen01 , . If you are looking for INDEL purity then `--select_variant_types='indels'` should work well I believe? You are losing some INDELs but ultimately you are creating a training set that contains only INDELs. You can also generate more training samples by downsampling the bam and re-running `make_examples`. On the other hand, if you think about inference pipeline, when you are in prediction mode, you can't switch your `--select_variant_types` to something else than your training data as the model will now have to deal with data it has never seen before. There can be two scenarios:. 1) You train a model with `--select_variant_types='indels'` and after the model is trained, you are running inference with `--select_variant_types='indels multi-allelics'`. Then the model will have to deal with mult-allelic variants it has never seen before so the prediction on those would expected to be poor. 2) You train a model with `--select_variant_types='indels multi-allelics'` and allow a few snps to be present in the training. In which case during your inference or prediction, the model will be confident on all indel cases. It depends on what exactly you are trying to do, but, my suggestion would be to use `--select_variant_types='indels multi-allelics'` for your purpose.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/813
https://github.com/google/deepvariant/issues/813:372,deployability,pipelin,pipeline,372,"@sophienguyen01 , . If you are looking for INDEL purity then `--select_variant_types='indels'` should work well I believe? You are losing some INDELs but ultimately you are creating a training set that contains only INDELs. You can also generate more training samples by downsampling the bam and re-running `make_examples`. On the other hand, if you think about inference pipeline, when you are in prediction mode, you can't switch your `--select_variant_types` to something else than your training data as the model will now have to deal with data it has never seen before. There can be two scenarios:. 1) You train a model with `--select_variant_types='indels'` and after the model is trained, you are running inference with `--select_variant_types='indels multi-allelics'`. Then the model will have to deal with mult-allelic variants it has never seen before so the prediction on those would expected to be poor. 2) You train a model with `--select_variant_types='indels multi-allelics'` and allow a few snps to be present in the training. In which case during your inference or prediction, the model will be confident on all indel cases. It depends on what exactly you are trying to do, but, my suggestion would be to use `--select_variant_types='indels multi-allelics'` for your purpose.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/813
https://github.com/google/deepvariant/issues/813:1145,deployability,depend,depends,1145,"@sophienguyen01 , . If you are looking for INDEL purity then `--select_variant_types='indels'` should work well I believe? You are losing some INDELs but ultimately you are creating a training set that contains only INDELs. You can also generate more training samples by downsampling the bam and re-running `make_examples`. On the other hand, if you think about inference pipeline, when you are in prediction mode, you can't switch your `--select_variant_types` to something else than your training data as the model will now have to deal with data it has never seen before. There can be two scenarios:. 1) You train a model with `--select_variant_types='indels'` and after the model is trained, you are running inference with `--select_variant_types='indels multi-allelics'`. Then the model will have to deal with mult-allelic variants it has never seen before so the prediction on those would expected to be poor. 2) You train a model with `--select_variant_types='indels multi-allelics'` and allow a few snps to be present in the training. In which case during your inference or prediction, the model will be confident on all indel cases. It depends on what exactly you are trying to do, but, my suggestion would be to use `--select_variant_types='indels multi-allelics'` for your purpose.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/813
https://github.com/google/deepvariant/issues/813:398,energy efficiency,predict,prediction,398,"@sophienguyen01 , . If you are looking for INDEL purity then `--select_variant_types='indels'` should work well I believe? You are losing some INDELs but ultimately you are creating a training set that contains only INDELs. You can also generate more training samples by downsampling the bam and re-running `make_examples`. On the other hand, if you think about inference pipeline, when you are in prediction mode, you can't switch your `--select_variant_types` to something else than your training data as the model will now have to deal with data it has never seen before. There can be two scenarios:. 1) You train a model with `--select_variant_types='indels'` and after the model is trained, you are running inference with `--select_variant_types='indels multi-allelics'`. Then the model will have to deal with mult-allelic variants it has never seen before so the prediction on those would expected to be poor. 2) You train a model with `--select_variant_types='indels multi-allelics'` and allow a few snps to be present in the training. In which case during your inference or prediction, the model will be confident on all indel cases. It depends on what exactly you are trying to do, but, my suggestion would be to use `--select_variant_types='indels multi-allelics'` for your purpose.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/813
https://github.com/google/deepvariant/issues/813:511,energy efficiency,model,model,511,"@sophienguyen01 , . If you are looking for INDEL purity then `--select_variant_types='indels'` should work well I believe? You are losing some INDELs but ultimately you are creating a training set that contains only INDELs. You can also generate more training samples by downsampling the bam and re-running `make_examples`. On the other hand, if you think about inference pipeline, when you are in prediction mode, you can't switch your `--select_variant_types` to something else than your training data as the model will now have to deal with data it has never seen before. There can be two scenarios:. 1) You train a model with `--select_variant_types='indels'` and after the model is trained, you are running inference with `--select_variant_types='indels multi-allelics'`. Then the model will have to deal with mult-allelic variants it has never seen before so the prediction on those would expected to be poor. 2) You train a model with `--select_variant_types='indels multi-allelics'` and allow a few snps to be present in the training. In which case during your inference or prediction, the model will be confident on all indel cases. It depends on what exactly you are trying to do, but, my suggestion would be to use `--select_variant_types='indels multi-allelics'` for your purpose.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/813
https://github.com/google/deepvariant/issues/813:619,energy efficiency,model,model,619,"@sophienguyen01 , . If you are looking for INDEL purity then `--select_variant_types='indels'` should work well I believe? You are losing some INDELs but ultimately you are creating a training set that contains only INDELs. You can also generate more training samples by downsampling the bam and re-running `make_examples`. On the other hand, if you think about inference pipeline, when you are in prediction mode, you can't switch your `--select_variant_types` to something else than your training data as the model will now have to deal with data it has never seen before. There can be two scenarios:. 1) You train a model with `--select_variant_types='indels'` and after the model is trained, you are running inference with `--select_variant_types='indels multi-allelics'`. Then the model will have to deal with mult-allelic variants it has never seen before so the prediction on those would expected to be poor. 2) You train a model with `--select_variant_types='indels multi-allelics'` and allow a few snps to be present in the training. In which case during your inference or prediction, the model will be confident on all indel cases. It depends on what exactly you are trying to do, but, my suggestion would be to use `--select_variant_types='indels multi-allelics'` for your purpose.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/813
https://github.com/google/deepvariant/issues/813:678,energy efficiency,model,model,678,"@sophienguyen01 , . If you are looking for INDEL purity then `--select_variant_types='indels'` should work well I believe? You are losing some INDELs but ultimately you are creating a training set that contains only INDELs. You can also generate more training samples by downsampling the bam and re-running `make_examples`. On the other hand, if you think about inference pipeline, when you are in prediction mode, you can't switch your `--select_variant_types` to something else than your training data as the model will now have to deal with data it has never seen before. There can be two scenarios:. 1) You train a model with `--select_variant_types='indels'` and after the model is trained, you are running inference with `--select_variant_types='indels multi-allelics'`. Then the model will have to deal with mult-allelic variants it has never seen before so the prediction on those would expected to be poor. 2) You train a model with `--select_variant_types='indels multi-allelics'` and allow a few snps to be present in the training. In which case during your inference or prediction, the model will be confident on all indel cases. It depends on what exactly you are trying to do, but, my suggestion would be to use `--select_variant_types='indels multi-allelics'` for your purpose.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/813
https://github.com/google/deepvariant/issues/813:786,energy efficiency,model,model,786,"@sophienguyen01 , . If you are looking for INDEL purity then `--select_variant_types='indels'` should work well I believe? You are losing some INDELs but ultimately you are creating a training set that contains only INDELs. You can also generate more training samples by downsampling the bam and re-running `make_examples`. On the other hand, if you think about inference pipeline, when you are in prediction mode, you can't switch your `--select_variant_types` to something else than your training data as the model will now have to deal with data it has never seen before. There can be two scenarios:. 1) You train a model with `--select_variant_types='indels'` and after the model is trained, you are running inference with `--select_variant_types='indels multi-allelics'`. Then the model will have to deal with mult-allelic variants it has never seen before so the prediction on those would expected to be poor. 2) You train a model with `--select_variant_types='indels multi-allelics'` and allow a few snps to be present in the training. In which case during your inference or prediction, the model will be confident on all indel cases. It depends on what exactly you are trying to do, but, my suggestion would be to use `--select_variant_types='indels multi-allelics'` for your purpose.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/813
https://github.com/google/deepvariant/issues/813:869,energy efficiency,predict,prediction,869,"@sophienguyen01 , . If you are looking for INDEL purity then `--select_variant_types='indels'` should work well I believe? You are losing some INDELs but ultimately you are creating a training set that contains only INDELs. You can also generate more training samples by downsampling the bam and re-running `make_examples`. On the other hand, if you think about inference pipeline, when you are in prediction mode, you can't switch your `--select_variant_types` to something else than your training data as the model will now have to deal with data it has never seen before. There can be two scenarios:. 1) You train a model with `--select_variant_types='indels'` and after the model is trained, you are running inference with `--select_variant_types='indels multi-allelics'`. Then the model will have to deal with mult-allelic variants it has never seen before so the prediction on those would expected to be poor. 2) You train a model with `--select_variant_types='indels multi-allelics'` and allow a few snps to be present in the training. In which case during your inference or prediction, the model will be confident on all indel cases. It depends on what exactly you are trying to do, but, my suggestion would be to use `--select_variant_types='indels multi-allelics'` for your purpose.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/813
https://github.com/google/deepvariant/issues/813:931,energy efficiency,model,model,931,"@sophienguyen01 , . If you are looking for INDEL purity then `--select_variant_types='indels'` should work well I believe? You are losing some INDELs but ultimately you are creating a training set that contains only INDELs. You can also generate more training samples by downsampling the bam and re-running `make_examples`. On the other hand, if you think about inference pipeline, when you are in prediction mode, you can't switch your `--select_variant_types` to something else than your training data as the model will now have to deal with data it has never seen before. There can be two scenarios:. 1) You train a model with `--select_variant_types='indels'` and after the model is trained, you are running inference with `--select_variant_types='indels multi-allelics'`. Then the model will have to deal with mult-allelic variants it has never seen before so the prediction on those would expected to be poor. 2) You train a model with `--select_variant_types='indels multi-allelics'` and allow a few snps to be present in the training. In which case during your inference or prediction, the model will be confident on all indel cases. It depends on what exactly you are trying to do, but, my suggestion would be to use `--select_variant_types='indels multi-allelics'` for your purpose.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/813
https://github.com/google/deepvariant/issues/813:1082,energy efficiency,predict,prediction,1082,"@sophienguyen01 , . If you are looking for INDEL purity then `--select_variant_types='indels'` should work well I believe? You are losing some INDELs but ultimately you are creating a training set that contains only INDELs. You can also generate more training samples by downsampling the bam and re-running `make_examples`. On the other hand, if you think about inference pipeline, when you are in prediction mode, you can't switch your `--select_variant_types` to something else than your training data as the model will now have to deal with data it has never seen before. There can be two scenarios:. 1) You train a model with `--select_variant_types='indels'` and after the model is trained, you are running inference with `--select_variant_types='indels multi-allelics'`. Then the model will have to deal with mult-allelic variants it has never seen before so the prediction on those would expected to be poor. 2) You train a model with `--select_variant_types='indels multi-allelics'` and allow a few snps to be present in the training. In which case during your inference or prediction, the model will be confident on all indel cases. It depends on what exactly you are trying to do, but, my suggestion would be to use `--select_variant_types='indels multi-allelics'` for your purpose.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/813
https://github.com/google/deepvariant/issues/813:1098,energy efficiency,model,model,1098,"@sophienguyen01 , . If you are looking for INDEL purity then `--select_variant_types='indels'` should work well I believe? You are losing some INDELs but ultimately you are creating a training set that contains only INDELs. You can also generate more training samples by downsampling the bam and re-running `make_examples`. On the other hand, if you think about inference pipeline, when you are in prediction mode, you can't switch your `--select_variant_types` to something else than your training data as the model will now have to deal with data it has never seen before. There can be two scenarios:. 1) You train a model with `--select_variant_types='indels'` and after the model is trained, you are running inference with `--select_variant_types='indels multi-allelics'`. Then the model will have to deal with mult-allelic variants it has never seen before so the prediction on those would expected to be poor. 2) You train a model with `--select_variant_types='indels multi-allelics'` and allow a few snps to be present in the training. In which case during your inference or prediction, the model will be confident on all indel cases. It depends on what exactly you are trying to do, but, my suggestion would be to use `--select_variant_types='indels multi-allelics'` for your purpose.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/813
https://github.com/google/deepvariant/issues/813:372,integrability,pipelin,pipeline,372,"@sophienguyen01 , . If you are looking for INDEL purity then `--select_variant_types='indels'` should work well I believe? You are losing some INDELs but ultimately you are creating a training set that contains only INDELs. You can also generate more training samples by downsampling the bam and re-running `make_examples`. On the other hand, if you think about inference pipeline, when you are in prediction mode, you can't switch your `--select_variant_types` to something else than your training data as the model will now have to deal with data it has never seen before. There can be two scenarios:. 1) You train a model with `--select_variant_types='indels'` and after the model is trained, you are running inference with `--select_variant_types='indels multi-allelics'`. Then the model will have to deal with mult-allelic variants it has never seen before so the prediction on those would expected to be poor. 2) You train a model with `--select_variant_types='indels multi-allelics'` and allow a few snps to be present in the training. In which case during your inference or prediction, the model will be confident on all indel cases. It depends on what exactly you are trying to do, but, my suggestion would be to use `--select_variant_types='indels multi-allelics'` for your purpose.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/813
https://github.com/google/deepvariant/issues/813:1145,integrability,depend,depends,1145,"@sophienguyen01 , . If you are looking for INDEL purity then `--select_variant_types='indels'` should work well I believe? You are losing some INDELs but ultimately you are creating a training set that contains only INDELs. You can also generate more training samples by downsampling the bam and re-running `make_examples`. On the other hand, if you think about inference pipeline, when you are in prediction mode, you can't switch your `--select_variant_types` to something else than your training data as the model will now have to deal with data it has never seen before. There can be two scenarios:. 1) You train a model with `--select_variant_types='indels'` and after the model is trained, you are running inference with `--select_variant_types='indels multi-allelics'`. Then the model will have to deal with mult-allelic variants it has never seen before so the prediction on those would expected to be poor. 2) You train a model with `--select_variant_types='indels multi-allelics'` and allow a few snps to be present in the training. In which case during your inference or prediction, the model will be confident on all indel cases. It depends on what exactly you are trying to do, but, my suggestion would be to use `--select_variant_types='indels multi-allelics'` for your purpose.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/813
https://github.com/google/deepvariant/issues/813:592,modifiability,scenario,scenarios,592,"@sophienguyen01 , . If you are looking for INDEL purity then `--select_variant_types='indels'` should work well I believe? You are losing some INDELs but ultimately you are creating a training set that contains only INDELs. You can also generate more training samples by downsampling the bam and re-running `make_examples`. On the other hand, if you think about inference pipeline, when you are in prediction mode, you can't switch your `--select_variant_types` to something else than your training data as the model will now have to deal with data it has never seen before. There can be two scenarios:. 1) You train a model with `--select_variant_types='indels'` and after the model is trained, you are running inference with `--select_variant_types='indels multi-allelics'`. Then the model will have to deal with mult-allelic variants it has never seen before so the prediction on those would expected to be poor. 2) You train a model with `--select_variant_types='indels multi-allelics'` and allow a few snps to be present in the training. In which case during your inference or prediction, the model will be confident on all indel cases. It depends on what exactly you are trying to do, but, my suggestion would be to use `--select_variant_types='indels multi-allelics'` for your purpose.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/813
https://github.com/google/deepvariant/issues/813:1145,modifiability,depend,depends,1145,"@sophienguyen01 , . If you are looking for INDEL purity then `--select_variant_types='indels'` should work well I believe? You are losing some INDELs but ultimately you are creating a training set that contains only INDELs. You can also generate more training samples by downsampling the bam and re-running `make_examples`. On the other hand, if you think about inference pipeline, when you are in prediction mode, you can't switch your `--select_variant_types` to something else than your training data as the model will now have to deal with data it has never seen before. There can be two scenarios:. 1) You train a model with `--select_variant_types='indels'` and after the model is trained, you are running inference with `--select_variant_types='indels multi-allelics'`. Then the model will have to deal with mult-allelic variants it has never seen before so the prediction on those would expected to be poor. 2) You train a model with `--select_variant_types='indels multi-allelics'` and allow a few snps to be present in the training. In which case during your inference or prediction, the model will be confident on all indel cases. It depends on what exactly you are trying to do, but, my suggestion would be to use `--select_variant_types='indels multi-allelics'` for your purpose.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/813
https://github.com/google/deepvariant/issues/813:398,safety,predict,prediction,398,"@sophienguyen01 , . If you are looking for INDEL purity then `--select_variant_types='indels'` should work well I believe? You are losing some INDELs but ultimately you are creating a training set that contains only INDELs. You can also generate more training samples by downsampling the bam and re-running `make_examples`. On the other hand, if you think about inference pipeline, when you are in prediction mode, you can't switch your `--select_variant_types` to something else than your training data as the model will now have to deal with data it has never seen before. There can be two scenarios:. 1) You train a model with `--select_variant_types='indels'` and after the model is trained, you are running inference with `--select_variant_types='indels multi-allelics'`. Then the model will have to deal with mult-allelic variants it has never seen before so the prediction on those would expected to be poor. 2) You train a model with `--select_variant_types='indels multi-allelics'` and allow a few snps to be present in the training. In which case during your inference or prediction, the model will be confident on all indel cases. It depends on what exactly you are trying to do, but, my suggestion would be to use `--select_variant_types='indels multi-allelics'` for your purpose.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/813
https://github.com/google/deepvariant/issues/813:869,safety,predict,prediction,869,"@sophienguyen01 , . If you are looking for INDEL purity then `--select_variant_types='indels'` should work well I believe? You are losing some INDELs but ultimately you are creating a training set that contains only INDELs. You can also generate more training samples by downsampling the bam and re-running `make_examples`. On the other hand, if you think about inference pipeline, when you are in prediction mode, you can't switch your `--select_variant_types` to something else than your training data as the model will now have to deal with data it has never seen before. There can be two scenarios:. 1) You train a model with `--select_variant_types='indels'` and after the model is trained, you are running inference with `--select_variant_types='indels multi-allelics'`. Then the model will have to deal with mult-allelic variants it has never seen before so the prediction on those would expected to be poor. 2) You train a model with `--select_variant_types='indels multi-allelics'` and allow a few snps to be present in the training. In which case during your inference or prediction, the model will be confident on all indel cases. It depends on what exactly you are trying to do, but, my suggestion would be to use `--select_variant_types='indels multi-allelics'` for your purpose.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/813
https://github.com/google/deepvariant/issues/813:1082,safety,predict,prediction,1082,"@sophienguyen01 , . If you are looking for INDEL purity then `--select_variant_types='indels'` should work well I believe? You are losing some INDELs but ultimately you are creating a training set that contains only INDELs. You can also generate more training samples by downsampling the bam and re-running `make_examples`. On the other hand, if you think about inference pipeline, when you are in prediction mode, you can't switch your `--select_variant_types` to something else than your training data as the model will now have to deal with data it has never seen before. There can be two scenarios:. 1) You train a model with `--select_variant_types='indels'` and after the model is trained, you are running inference with `--select_variant_types='indels multi-allelics'`. Then the model will have to deal with mult-allelic variants it has never seen before so the prediction on those would expected to be poor. 2) You train a model with `--select_variant_types='indels multi-allelics'` and allow a few snps to be present in the training. In which case during your inference or prediction, the model will be confident on all indel cases. It depends on what exactly you are trying to do, but, my suggestion would be to use `--select_variant_types='indels multi-allelics'` for your purpose.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/813
https://github.com/google/deepvariant/issues/813:1145,safety,depend,depends,1145,"@sophienguyen01 , . If you are looking for INDEL purity then `--select_variant_types='indels'` should work well I believe? You are losing some INDELs but ultimately you are creating a training set that contains only INDELs. You can also generate more training samples by downsampling the bam and re-running `make_examples`. On the other hand, if you think about inference pipeline, when you are in prediction mode, you can't switch your `--select_variant_types` to something else than your training data as the model will now have to deal with data it has never seen before. There can be two scenarios:. 1) You train a model with `--select_variant_types='indels'` and after the model is trained, you are running inference with `--select_variant_types='indels multi-allelics'`. Then the model will have to deal with mult-allelic variants it has never seen before so the prediction on those would expected to be poor. 2) You train a model with `--select_variant_types='indels multi-allelics'` and allow a few snps to be present in the training. In which case during your inference or prediction, the model will be confident on all indel cases. It depends on what exactly you are trying to do, but, my suggestion would be to use `--select_variant_types='indels multi-allelics'` for your purpose.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/813
https://github.com/google/deepvariant/issues/813:511,security,model,model,511,"@sophienguyen01 , . If you are looking for INDEL purity then `--select_variant_types='indels'` should work well I believe? You are losing some INDELs but ultimately you are creating a training set that contains only INDELs. You can also generate more training samples by downsampling the bam and re-running `make_examples`. On the other hand, if you think about inference pipeline, when you are in prediction mode, you can't switch your `--select_variant_types` to something else than your training data as the model will now have to deal with data it has never seen before. There can be two scenarios:. 1) You train a model with `--select_variant_types='indels'` and after the model is trained, you are running inference with `--select_variant_types='indels multi-allelics'`. Then the model will have to deal with mult-allelic variants it has never seen before so the prediction on those would expected to be poor. 2) You train a model with `--select_variant_types='indels multi-allelics'` and allow a few snps to be present in the training. In which case during your inference or prediction, the model will be confident on all indel cases. It depends on what exactly you are trying to do, but, my suggestion would be to use `--select_variant_types='indels multi-allelics'` for your purpose.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/813
https://github.com/google/deepvariant/issues/813:619,security,model,model,619,"@sophienguyen01 , . If you are looking for INDEL purity then `--select_variant_types='indels'` should work well I believe? You are losing some INDELs but ultimately you are creating a training set that contains only INDELs. You can also generate more training samples by downsampling the bam and re-running `make_examples`. On the other hand, if you think about inference pipeline, when you are in prediction mode, you can't switch your `--select_variant_types` to something else than your training data as the model will now have to deal with data it has never seen before. There can be two scenarios:. 1) You train a model with `--select_variant_types='indels'` and after the model is trained, you are running inference with `--select_variant_types='indels multi-allelics'`. Then the model will have to deal with mult-allelic variants it has never seen before so the prediction on those would expected to be poor. 2) You train a model with `--select_variant_types='indels multi-allelics'` and allow a few snps to be present in the training. In which case during your inference or prediction, the model will be confident on all indel cases. It depends on what exactly you are trying to do, but, my suggestion would be to use `--select_variant_types='indels multi-allelics'` for your purpose.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/813
https://github.com/google/deepvariant/issues/813:678,security,model,model,678,"@sophienguyen01 , . If you are looking for INDEL purity then `--select_variant_types='indels'` should work well I believe? You are losing some INDELs but ultimately you are creating a training set that contains only INDELs. You can also generate more training samples by downsampling the bam and re-running `make_examples`. On the other hand, if you think about inference pipeline, when you are in prediction mode, you can't switch your `--select_variant_types` to something else than your training data as the model will now have to deal with data it has never seen before. There can be two scenarios:. 1) You train a model with `--select_variant_types='indels'` and after the model is trained, you are running inference with `--select_variant_types='indels multi-allelics'`. Then the model will have to deal with mult-allelic variants it has never seen before so the prediction on those would expected to be poor. 2) You train a model with `--select_variant_types='indels multi-allelics'` and allow a few snps to be present in the training. In which case during your inference or prediction, the model will be confident on all indel cases. It depends on what exactly you are trying to do, but, my suggestion would be to use `--select_variant_types='indels multi-allelics'` for your purpose.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/813
https://github.com/google/deepvariant/issues/813:786,security,model,model,786,"@sophienguyen01 , . If you are looking for INDEL purity then `--select_variant_types='indels'` should work well I believe? You are losing some INDELs but ultimately you are creating a training set that contains only INDELs. You can also generate more training samples by downsampling the bam and re-running `make_examples`. On the other hand, if you think about inference pipeline, when you are in prediction mode, you can't switch your `--select_variant_types` to something else than your training data as the model will now have to deal with data it has never seen before. There can be two scenarios:. 1) You train a model with `--select_variant_types='indels'` and after the model is trained, you are running inference with `--select_variant_types='indels multi-allelics'`. Then the model will have to deal with mult-allelic variants it has never seen before so the prediction on those would expected to be poor. 2) You train a model with `--select_variant_types='indels multi-allelics'` and allow a few snps to be present in the training. In which case during your inference or prediction, the model will be confident on all indel cases. It depends on what exactly you are trying to do, but, my suggestion would be to use `--select_variant_types='indels multi-allelics'` for your purpose.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/813
https://github.com/google/deepvariant/issues/813:931,security,model,model,931,"@sophienguyen01 , . If you are looking for INDEL purity then `--select_variant_types='indels'` should work well I believe? You are losing some INDELs but ultimately you are creating a training set that contains only INDELs. You can also generate more training samples by downsampling the bam and re-running `make_examples`. On the other hand, if you think about inference pipeline, when you are in prediction mode, you can't switch your `--select_variant_types` to something else than your training data as the model will now have to deal with data it has never seen before. There can be two scenarios:. 1) You train a model with `--select_variant_types='indels'` and after the model is trained, you are running inference with `--select_variant_types='indels multi-allelics'`. Then the model will have to deal with mult-allelic variants it has never seen before so the prediction on those would expected to be poor. 2) You train a model with `--select_variant_types='indels multi-allelics'` and allow a few snps to be present in the training. In which case during your inference or prediction, the model will be confident on all indel cases. It depends on what exactly you are trying to do, but, my suggestion would be to use `--select_variant_types='indels multi-allelics'` for your purpose.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/813
https://github.com/google/deepvariant/issues/813:1098,security,model,model,1098,"@sophienguyen01 , . If you are looking for INDEL purity then `--select_variant_types='indels'` should work well I believe? You are losing some INDELs but ultimately you are creating a training set that contains only INDELs. You can also generate more training samples by downsampling the bam and re-running `make_examples`. On the other hand, if you think about inference pipeline, when you are in prediction mode, you can't switch your `--select_variant_types` to something else than your training data as the model will now have to deal with data it has never seen before. There can be two scenarios:. 1) You train a model with `--select_variant_types='indels'` and after the model is trained, you are running inference with `--select_variant_types='indels multi-allelics'`. Then the model will have to deal with mult-allelic variants it has never seen before so the prediction on those would expected to be poor. 2) You train a model with `--select_variant_types='indels multi-allelics'` and allow a few snps to be present in the training. In which case during your inference or prediction, the model will be confident on all indel cases. It depends on what exactly you are trying to do, but, my suggestion would be to use `--select_variant_types='indels multi-allelics'` for your purpose.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/813
https://github.com/google/deepvariant/issues/813:1145,testability,depend,depends,1145,"@sophienguyen01 , . If you are looking for INDEL purity then `--select_variant_types='indels'` should work well I believe? You are losing some INDELs but ultimately you are creating a training set that contains only INDELs. You can also generate more training samples by downsampling the bam and re-running `make_examples`. On the other hand, if you think about inference pipeline, when you are in prediction mode, you can't switch your `--select_variant_types` to something else than your training data as the model will now have to deal with data it has never seen before. There can be two scenarios:. 1) You train a model with `--select_variant_types='indels'` and after the model is trained, you are running inference with `--select_variant_types='indels multi-allelics'`. Then the model will have to deal with mult-allelic variants it has never seen before so the prediction on those would expected to be poor. 2) You train a model with `--select_variant_types='indels multi-allelics'` and allow a few snps to be present in the training. In which case during your inference or prediction, the model will be confident on all indel cases. It depends on what exactly you are trying to do, but, my suggestion would be to use `--select_variant_types='indels multi-allelics'` for your purpose.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/813
https://github.com/google/deepvariant/issues/813:90,usability,close,close,90,"@sophienguyen01, given you have found a solution and we have discussed the to-dos, I will close this bug. Please feel free to reopen if you want to discuss further.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/813
https://github.com/google/deepvariant/issues/814:73,energy efficiency,current,current,73,"@bkurtoglu , can you please let us know what datatype you are using? The current model only works with ONT R10.4 simplex and duplex data. Can you provide some more details about your data and run:. 1) What chemistry is your ONT data? 2) What basecaller did you use? 3) How many cpus are you using to run make_examples?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/814
https://github.com/google/deepvariant/issues/814:81,energy efficiency,model,model,81,"@bkurtoglu , can you please let us know what datatype you are using? The current model only works with ONT R10.4 simplex and duplex data. Can you provide some more details about your data and run:. 1) What chemistry is your ONT data? 2) What basecaller did you use? 3) How many cpus are you using to run make_examples?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/814
https://github.com/google/deepvariant/issues/814:278,energy efficiency,cpu,cpus,278,"@bkurtoglu , can you please let us know what datatype you are using? The current model only works with ONT R10.4 simplex and duplex data. Can you provide some more details about your data and run:. 1) What chemistry is your ONT data? 2) What basecaller did you use? 3) How many cpus are you using to run make_examples?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/814
https://github.com/google/deepvariant/issues/814:278,performance,cpu,cpus,278,"@bkurtoglu , can you please let us know what datatype you are using? The current model only works with ONT R10.4 simplex and duplex data. Can you provide some more details about your data and run:. 1) What chemistry is your ONT data? 2) What basecaller did you use? 3) How many cpus are you using to run make_examples?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/814
https://github.com/google/deepvariant/issues/814:81,security,model,model,81,"@bkurtoglu , can you please let us know what datatype you are using? The current model only works with ONT R10.4 simplex and duplex data. Can you provide some more details about your data and run:. 1) What chemistry is your ONT data? 2) What basecaller did you use? 3) How many cpus are you using to run make_examples?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/814
https://github.com/google/deepvariant/issues/814:113,testability,simpl,simplex,113,"@bkurtoglu , can you please let us know what datatype you are using? The current model only works with ONT R10.4 simplex and duplex data. Can you provide some more details about your data and run:. 1) What chemistry is your ONT data? 2) What basecaller did you use? 3) How many cpus are you using to run make_examples?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/814
https://github.com/google/deepvariant/issues/814:113,usability,simpl,simplex,113,"@bkurtoglu , can you please let us know what datatype you are using? The current model only works with ONT R10.4 simplex and duplex data. Can you provide some more details about your data and run:. 1) What chemistry is your ONT data? 2) What basecaller did you use? 3) How many cpus are you using to run make_examples?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/814
https://github.com/google/deepvariant/issues/814:194,deployability,updat,update,194,"1. The type of the data is ONT_R9 simplex. 2. The basecaller is guppy. . 3. Threads are set to 48. I also provide the code as below. Thank you for your help. . `BIN_VERSION=""1.6.1"". sudo apt -y update. sudo apt-get -y install docker.io. sudo docker pull google/deepvariant:""${BIN_VERSION}"". INPUT_DIR=""/home/user/Masaüstü/BEYZA/input"" OUTPUT_DIR=""/home/user/Masaüstü/BEYZA/output"" THREADS=48 MODEL_NAME=""ONT_R104"". sudo docker run -v ""/home/massive/Desktop/beyza/input"":""/input"" -v ""/home/massive/Desktop/beyza/OUTPUTDEEP"":""/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=ONT_R104 --ref=""/input/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna"" --reads=""/input/NA12878-minion-ul_GRCh38.bam"" –output_vcf=""/output/deep.vcf"" --threads=48. `",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/814
https://github.com/google/deepvariant/issues/814:218,deployability,instal,install,218,"1. The type of the data is ONT_R9 simplex. 2. The basecaller is guppy. . 3. Threads are set to 48. I also provide the code as below. Thank you for your help. . `BIN_VERSION=""1.6.1"". sudo apt -y update. sudo apt-get -y install docker.io. sudo docker pull google/deepvariant:""${BIN_VERSION}"". INPUT_DIR=""/home/user/Masaüstü/BEYZA/input"" OUTPUT_DIR=""/home/user/Masaüstü/BEYZA/output"" THREADS=48 MODEL_NAME=""ONT_R104"". sudo docker run -v ""/home/massive/Desktop/beyza/input"":""/input"" -v ""/home/massive/Desktop/beyza/OUTPUTDEEP"":""/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=ONT_R104 --ref=""/input/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna"" --reads=""/input/NA12878-minion-ul_GRCh38.bam"" –output_vcf=""/output/deep.vcf"" --threads=48. `",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/814
https://github.com/google/deepvariant/issues/814:194,safety,updat,update,194,"1. The type of the data is ONT_R9 simplex. 2. The basecaller is guppy. . 3. Threads are set to 48. I also provide the code as below. Thank you for your help. . `BIN_VERSION=""1.6.1"". sudo apt -y update. sudo apt-get -y install docker.io. sudo docker pull google/deepvariant:""${BIN_VERSION}"". INPUT_DIR=""/home/user/Masaüstü/BEYZA/input"" OUTPUT_DIR=""/home/user/Masaüstü/BEYZA/output"" THREADS=48 MODEL_NAME=""ONT_R104"". sudo docker run -v ""/home/massive/Desktop/beyza/input"":""/input"" -v ""/home/massive/Desktop/beyza/OUTPUTDEEP"":""/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=ONT_R104 --ref=""/input/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna"" --reads=""/input/NA12878-minion-ul_GRCh38.bam"" –output_vcf=""/output/deep.vcf"" --threads=48. `",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/814
https://github.com/google/deepvariant/issues/814:328,safety,input,input,328,"1. The type of the data is ONT_R9 simplex. 2. The basecaller is guppy. . 3. Threads are set to 48. I also provide the code as below. Thank you for your help. . `BIN_VERSION=""1.6.1"". sudo apt -y update. sudo apt-get -y install docker.io. sudo docker pull google/deepvariant:""${BIN_VERSION}"". INPUT_DIR=""/home/user/Masaüstü/BEYZA/input"" OUTPUT_DIR=""/home/user/Masaüstü/BEYZA/output"" THREADS=48 MODEL_NAME=""ONT_R104"". sudo docker run -v ""/home/massive/Desktop/beyza/input"":""/input"" -v ""/home/massive/Desktop/beyza/OUTPUTDEEP"":""/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=ONT_R104 --ref=""/input/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna"" --reads=""/input/NA12878-minion-ul_GRCh38.bam"" –output_vcf=""/output/deep.vcf"" --threads=48. `",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/814
https://github.com/google/deepvariant/issues/814:463,safety,input,input,463,"1. The type of the data is ONT_R9 simplex. 2. The basecaller is guppy. . 3. Threads are set to 48. I also provide the code as below. Thank you for your help. . `BIN_VERSION=""1.6.1"". sudo apt -y update. sudo apt-get -y install docker.io. sudo docker pull google/deepvariant:""${BIN_VERSION}"". INPUT_DIR=""/home/user/Masaüstü/BEYZA/input"" OUTPUT_DIR=""/home/user/Masaüstü/BEYZA/output"" THREADS=48 MODEL_NAME=""ONT_R104"". sudo docker run -v ""/home/massive/Desktop/beyza/input"":""/input"" -v ""/home/massive/Desktop/beyza/OUTPUTDEEP"":""/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=ONT_R104 --ref=""/input/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna"" --reads=""/input/NA12878-minion-ul_GRCh38.bam"" –output_vcf=""/output/deep.vcf"" --threads=48. `",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/814
https://github.com/google/deepvariant/issues/814:472,safety,input,input,472,"1. The type of the data is ONT_R9 simplex. 2. The basecaller is guppy. . 3. Threads are set to 48. I also provide the code as below. Thank you for your help. . `BIN_VERSION=""1.6.1"". sudo apt -y update. sudo apt-get -y install docker.io. sudo docker pull google/deepvariant:""${BIN_VERSION}"". INPUT_DIR=""/home/user/Masaüstü/BEYZA/input"" OUTPUT_DIR=""/home/user/Masaüstü/BEYZA/output"" THREADS=48 MODEL_NAME=""ONT_R104"". sudo docker run -v ""/home/massive/Desktop/beyza/input"":""/input"" -v ""/home/massive/Desktop/beyza/OUTPUTDEEP"":""/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=ONT_R104 --ref=""/input/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna"" --reads=""/input/NA12878-minion-ul_GRCh38.bam"" –output_vcf=""/output/deep.vcf"" --threads=48. `",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/814
https://github.com/google/deepvariant/issues/814:636,safety,input,input,636,"1. The type of the data is ONT_R9 simplex. 2. The basecaller is guppy. . 3. Threads are set to 48. I also provide the code as below. Thank you for your help. . `BIN_VERSION=""1.6.1"". sudo apt -y update. sudo apt-get -y install docker.io. sudo docker pull google/deepvariant:""${BIN_VERSION}"". INPUT_DIR=""/home/user/Masaüstü/BEYZA/input"" OUTPUT_DIR=""/home/user/Masaüstü/BEYZA/output"" THREADS=48 MODEL_NAME=""ONT_R104"". sudo docker run -v ""/home/massive/Desktop/beyza/input"":""/input"" -v ""/home/massive/Desktop/beyza/OUTPUTDEEP"":""/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=ONT_R104 --ref=""/input/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna"" --reads=""/input/NA12878-minion-ul_GRCh38.bam"" –output_vcf=""/output/deep.vcf"" --threads=48. `",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/814
https://github.com/google/deepvariant/issues/814:701,safety,input,input,701,"1. The type of the data is ONT_R9 simplex. 2. The basecaller is guppy. . 3. Threads are set to 48. I also provide the code as below. Thank you for your help. . `BIN_VERSION=""1.6.1"". sudo apt -y update. sudo apt-get -y install docker.io. sudo docker pull google/deepvariant:""${BIN_VERSION}"". INPUT_DIR=""/home/user/Masaüstü/BEYZA/input"" OUTPUT_DIR=""/home/user/Masaüstü/BEYZA/output"" THREADS=48 MODEL_NAME=""ONT_R104"". sudo docker run -v ""/home/massive/Desktop/beyza/input"":""/input"" -v ""/home/massive/Desktop/beyza/OUTPUTDEEP"":""/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=ONT_R104 --ref=""/input/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna"" --reads=""/input/NA12878-minion-ul_GRCh38.bam"" –output_vcf=""/output/deep.vcf"" --threads=48. `",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/814
https://github.com/google/deepvariant/issues/814:187,security,apt,apt,187,"1. The type of the data is ONT_R9 simplex. 2. The basecaller is guppy. . 3. Threads are set to 48. I also provide the code as below. Thank you for your help. . `BIN_VERSION=""1.6.1"". sudo apt -y update. sudo apt-get -y install docker.io. sudo docker pull google/deepvariant:""${BIN_VERSION}"". INPUT_DIR=""/home/user/Masaüstü/BEYZA/input"" OUTPUT_DIR=""/home/user/Masaüstü/BEYZA/output"" THREADS=48 MODEL_NAME=""ONT_R104"". sudo docker run -v ""/home/massive/Desktop/beyza/input"":""/input"" -v ""/home/massive/Desktop/beyza/OUTPUTDEEP"":""/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=ONT_R104 --ref=""/input/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna"" --reads=""/input/NA12878-minion-ul_GRCh38.bam"" –output_vcf=""/output/deep.vcf"" --threads=48. `",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/814
https://github.com/google/deepvariant/issues/814:194,security,updat,update,194,"1. The type of the data is ONT_R9 simplex. 2. The basecaller is guppy. . 3. Threads are set to 48. I also provide the code as below. Thank you for your help. . `BIN_VERSION=""1.6.1"". sudo apt -y update. sudo apt-get -y install docker.io. sudo docker pull google/deepvariant:""${BIN_VERSION}"". INPUT_DIR=""/home/user/Masaüstü/BEYZA/input"" OUTPUT_DIR=""/home/user/Masaüstü/BEYZA/output"" THREADS=48 MODEL_NAME=""ONT_R104"". sudo docker run -v ""/home/massive/Desktop/beyza/input"":""/input"" -v ""/home/massive/Desktop/beyza/OUTPUTDEEP"":""/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=ONT_R104 --ref=""/input/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna"" --reads=""/input/NA12878-minion-ul_GRCh38.bam"" –output_vcf=""/output/deep.vcf"" --threads=48. `",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/814
https://github.com/google/deepvariant/issues/814:207,security,apt,apt-get,207,"1. The type of the data is ONT_R9 simplex. 2. The basecaller is guppy. . 3. Threads are set to 48. I also provide the code as below. Thank you for your help. . `BIN_VERSION=""1.6.1"". sudo apt -y update. sudo apt-get -y install docker.io. sudo docker pull google/deepvariant:""${BIN_VERSION}"". INPUT_DIR=""/home/user/Masaüstü/BEYZA/input"" OUTPUT_DIR=""/home/user/Masaüstü/BEYZA/output"" THREADS=48 MODEL_NAME=""ONT_R104"". sudo docker run -v ""/home/massive/Desktop/beyza/input"":""/input"" -v ""/home/massive/Desktop/beyza/OUTPUTDEEP"":""/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=ONT_R104 --ref=""/input/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna"" --reads=""/input/NA12878-minion-ul_GRCh38.bam"" –output_vcf=""/output/deep.vcf"" --threads=48. `",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/814
https://github.com/google/deepvariant/issues/814:34,testability,simpl,simplex,34,"1. The type of the data is ONT_R9 simplex. 2. The basecaller is guppy. . 3. Threads are set to 48. I also provide the code as below. Thank you for your help. . `BIN_VERSION=""1.6.1"". sudo apt -y update. sudo apt-get -y install docker.io. sudo docker pull google/deepvariant:""${BIN_VERSION}"". INPUT_DIR=""/home/user/Masaüstü/BEYZA/input"" OUTPUT_DIR=""/home/user/Masaüstü/BEYZA/output"" THREADS=48 MODEL_NAME=""ONT_R104"". sudo docker run -v ""/home/massive/Desktop/beyza/input"":""/input"" -v ""/home/massive/Desktop/beyza/OUTPUTDEEP"":""/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=ONT_R104 --ref=""/input/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna"" --reads=""/input/NA12878-minion-ul_GRCh38.bam"" –output_vcf=""/output/deep.vcf"" --threads=48. `",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/814
https://github.com/google/deepvariant/issues/814:34,usability,simpl,simplex,34,"1. The type of the data is ONT_R9 simplex. 2. The basecaller is guppy. . 3. Threads are set to 48. I also provide the code as below. Thank you for your help. . `BIN_VERSION=""1.6.1"". sudo apt -y update. sudo apt-get -y install docker.io. sudo docker pull google/deepvariant:""${BIN_VERSION}"". INPUT_DIR=""/home/user/Masaüstü/BEYZA/input"" OUTPUT_DIR=""/home/user/Masaüstü/BEYZA/output"" THREADS=48 MODEL_NAME=""ONT_R104"". sudo docker run -v ""/home/massive/Desktop/beyza/input"":""/input"" -v ""/home/massive/Desktop/beyza/OUTPUTDEEP"":""/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=ONT_R104 --ref=""/input/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna"" --reads=""/input/NA12878-minion-ul_GRCh38.bam"" –output_vcf=""/output/deep.vcf"" --threads=48. `",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/814
https://github.com/google/deepvariant/issues/814:152,usability,help,help,152,"1. The type of the data is ONT_R9 simplex. 2. The basecaller is guppy. . 3. Threads are set to 48. I also provide the code as below. Thank you for your help. . `BIN_VERSION=""1.6.1"". sudo apt -y update. sudo apt-get -y install docker.io. sudo docker pull google/deepvariant:""${BIN_VERSION}"". INPUT_DIR=""/home/user/Masaüstü/BEYZA/input"" OUTPUT_DIR=""/home/user/Masaüstü/BEYZA/output"" THREADS=48 MODEL_NAME=""ONT_R104"". sudo docker run -v ""/home/massive/Desktop/beyza/input"":""/input"" -v ""/home/massive/Desktop/beyza/OUTPUTDEEP"":""/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=ONT_R104 --ref=""/input/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna"" --reads=""/input/NA12878-minion-ul_GRCh38.bam"" –output_vcf=""/output/deep.vcf"" --threads=48. `",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/814
https://github.com/google/deepvariant/issues/814:308,usability,user,user,308,"1. The type of the data is ONT_R9 simplex. 2. The basecaller is guppy. . 3. Threads are set to 48. I also provide the code as below. Thank you for your help. . `BIN_VERSION=""1.6.1"". sudo apt -y update. sudo apt-get -y install docker.io. sudo docker pull google/deepvariant:""${BIN_VERSION}"". INPUT_DIR=""/home/user/Masaüstü/BEYZA/input"" OUTPUT_DIR=""/home/user/Masaüstü/BEYZA/output"" THREADS=48 MODEL_NAME=""ONT_R104"". sudo docker run -v ""/home/massive/Desktop/beyza/input"":""/input"" -v ""/home/massive/Desktop/beyza/OUTPUTDEEP"":""/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=ONT_R104 --ref=""/input/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna"" --reads=""/input/NA12878-minion-ul_GRCh38.bam"" –output_vcf=""/output/deep.vcf"" --threads=48. `",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/814
https://github.com/google/deepvariant/issues/814:328,usability,input,input,328,"1. The type of the data is ONT_R9 simplex. 2. The basecaller is guppy. . 3. Threads are set to 48. I also provide the code as below. Thank you for your help. . `BIN_VERSION=""1.6.1"". sudo apt -y update. sudo apt-get -y install docker.io. sudo docker pull google/deepvariant:""${BIN_VERSION}"". INPUT_DIR=""/home/user/Masaüstü/BEYZA/input"" OUTPUT_DIR=""/home/user/Masaüstü/BEYZA/output"" THREADS=48 MODEL_NAME=""ONT_R104"". sudo docker run -v ""/home/massive/Desktop/beyza/input"":""/input"" -v ""/home/massive/Desktop/beyza/OUTPUTDEEP"":""/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=ONT_R104 --ref=""/input/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna"" --reads=""/input/NA12878-minion-ul_GRCh38.bam"" –output_vcf=""/output/deep.vcf"" --threads=48. `",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/814
https://github.com/google/deepvariant/issues/814:353,usability,user,user,353,"1. The type of the data is ONT_R9 simplex. 2. The basecaller is guppy. . 3. Threads are set to 48. I also provide the code as below. Thank you for your help. . `BIN_VERSION=""1.6.1"". sudo apt -y update. sudo apt-get -y install docker.io. sudo docker pull google/deepvariant:""${BIN_VERSION}"". INPUT_DIR=""/home/user/Masaüstü/BEYZA/input"" OUTPUT_DIR=""/home/user/Masaüstü/BEYZA/output"" THREADS=48 MODEL_NAME=""ONT_R104"". sudo docker run -v ""/home/massive/Desktop/beyza/input"":""/input"" -v ""/home/massive/Desktop/beyza/OUTPUTDEEP"":""/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=ONT_R104 --ref=""/input/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna"" --reads=""/input/NA12878-minion-ul_GRCh38.bam"" –output_vcf=""/output/deep.vcf"" --threads=48. `",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/814
https://github.com/google/deepvariant/issues/814:463,usability,input,input,463,"1. The type of the data is ONT_R9 simplex. 2. The basecaller is guppy. . 3. Threads are set to 48. I also provide the code as below. Thank you for your help. . `BIN_VERSION=""1.6.1"". sudo apt -y update. sudo apt-get -y install docker.io. sudo docker pull google/deepvariant:""${BIN_VERSION}"". INPUT_DIR=""/home/user/Masaüstü/BEYZA/input"" OUTPUT_DIR=""/home/user/Masaüstü/BEYZA/output"" THREADS=48 MODEL_NAME=""ONT_R104"". sudo docker run -v ""/home/massive/Desktop/beyza/input"":""/input"" -v ""/home/massive/Desktop/beyza/OUTPUTDEEP"":""/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=ONT_R104 --ref=""/input/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna"" --reads=""/input/NA12878-minion-ul_GRCh38.bam"" –output_vcf=""/output/deep.vcf"" --threads=48. `",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/814
https://github.com/google/deepvariant/issues/814:472,usability,input,input,472,"1. The type of the data is ONT_R9 simplex. 2. The basecaller is guppy. . 3. Threads are set to 48. I also provide the code as below. Thank you for your help. . `BIN_VERSION=""1.6.1"". sudo apt -y update. sudo apt-get -y install docker.io. sudo docker pull google/deepvariant:""${BIN_VERSION}"". INPUT_DIR=""/home/user/Masaüstü/BEYZA/input"" OUTPUT_DIR=""/home/user/Masaüstü/BEYZA/output"" THREADS=48 MODEL_NAME=""ONT_R104"". sudo docker run -v ""/home/massive/Desktop/beyza/input"":""/input"" -v ""/home/massive/Desktop/beyza/OUTPUTDEEP"":""/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=ONT_R104 --ref=""/input/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna"" --reads=""/input/NA12878-minion-ul_GRCh38.bam"" –output_vcf=""/output/deep.vcf"" --threads=48. `",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/814
https://github.com/google/deepvariant/issues/814:636,usability,input,input,636,"1. The type of the data is ONT_R9 simplex. 2. The basecaller is guppy. . 3. Threads are set to 48. I also provide the code as below. Thank you for your help. . `BIN_VERSION=""1.6.1"". sudo apt -y update. sudo apt-get -y install docker.io. sudo docker pull google/deepvariant:""${BIN_VERSION}"". INPUT_DIR=""/home/user/Masaüstü/BEYZA/input"" OUTPUT_DIR=""/home/user/Masaüstü/BEYZA/output"" THREADS=48 MODEL_NAME=""ONT_R104"". sudo docker run -v ""/home/massive/Desktop/beyza/input"":""/input"" -v ""/home/massive/Desktop/beyza/OUTPUTDEEP"":""/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=ONT_R104 --ref=""/input/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna"" --reads=""/input/NA12878-minion-ul_GRCh38.bam"" –output_vcf=""/output/deep.vcf"" --threads=48. `",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/814
https://github.com/google/deepvariant/issues/814:701,usability,input,input,701,"1. The type of the data is ONT_R9 simplex. 2. The basecaller is guppy. . 3. Threads are set to 48. I also provide the code as below. Thank you for your help. . `BIN_VERSION=""1.6.1"". sudo apt -y update. sudo apt-get -y install docker.io. sudo docker pull google/deepvariant:""${BIN_VERSION}"". INPUT_DIR=""/home/user/Masaüstü/BEYZA/input"" OUTPUT_DIR=""/home/user/Masaüstü/BEYZA/output"" THREADS=48 MODEL_NAME=""ONT_R104"". sudo docker run -v ""/home/massive/Desktop/beyza/input"":""/input"" -v ""/home/massive/Desktop/beyza/OUTPUTDEEP"":""/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=ONT_R104 --ref=""/input/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna"" --reads=""/input/NA12878-minion-ul_GRCh38.bam"" –output_vcf=""/output/deep.vcf"" --threads=48. `",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/814
https://github.com/google/deepvariant/issues/814:37,availability,error,error,37,"@bkurtoglu ,. ONT_R9 has a very high error rate, DeepVariant isn't able to work on it directly. You can try our previous solution [PEPPER](https://github.com/kishwarshafin/pepper) for R9 data. It's also mentioned on our README page: . * Oxford Nanopore R9.4.1 data by using [PEPPER-DeepVariant](https://github.com/kishwarshafin/pepper). So it's unlikely you will be successful to run DeepVariant R10.4 model on R9 data.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/814
https://github.com/google/deepvariant/issues/814:402,energy efficiency,model,model,402,"@bkurtoglu ,. ONT_R9 has a very high error rate, DeepVariant isn't able to work on it directly. You can try our previous solution [PEPPER](https://github.com/kishwarshafin/pepper) for R9 data. It's also mentioned on our README page: . * Oxford Nanopore R9.4.1 data by using [PEPPER-DeepVariant](https://github.com/kishwarshafin/pepper). So it's unlikely you will be successful to run DeepVariant R10.4 model on R9 data.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/814
https://github.com/google/deepvariant/issues/814:37,performance,error,error,37,"@bkurtoglu ,. ONT_R9 has a very high error rate, DeepVariant isn't able to work on it directly. You can try our previous solution [PEPPER](https://github.com/kishwarshafin/pepper) for R9 data. It's also mentioned on our README page: . * Oxford Nanopore R9.4.1 data by using [PEPPER-DeepVariant](https://github.com/kishwarshafin/pepper). So it's unlikely you will be successful to run DeepVariant R10.4 model on R9 data.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/814
https://github.com/google/deepvariant/issues/814:37,safety,error,error,37,"@bkurtoglu ,. ONT_R9 has a very high error rate, DeepVariant isn't able to work on it directly. You can try our previous solution [PEPPER](https://github.com/kishwarshafin/pepper) for R9 data. It's also mentioned on our README page: . * Oxford Nanopore R9.4.1 data by using [PEPPER-DeepVariant](https://github.com/kishwarshafin/pepper). So it's unlikely you will be successful to run DeepVariant R10.4 model on R9 data.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/814
https://github.com/google/deepvariant/issues/814:402,security,model,model,402,"@bkurtoglu ,. ONT_R9 has a very high error rate, DeepVariant isn't able to work on it directly. You can try our previous solution [PEPPER](https://github.com/kishwarshafin/pepper) for R9 data. It's also mentioned on our README page: . * Oxford Nanopore R9.4.1 data by using [PEPPER-DeepVariant](https://github.com/kishwarshafin/pepper). So it's unlikely you will be successful to run DeepVariant R10.4 model on R9 data.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/814
https://github.com/google/deepvariant/issues/814:37,usability,error,error,37,"@bkurtoglu ,. ONT_R9 has a very high error rate, DeepVariant isn't able to work on it directly. You can try our previous solution [PEPPER](https://github.com/kishwarshafin/pepper) for R9 data. It's also mentioned on our README page: . * Oxford Nanopore R9.4.1 data by using [PEPPER-DeepVariant](https://github.com/kishwarshafin/pepper). So it's unlikely you will be successful to run DeepVariant R10.4 model on R9 data.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/814
https://github.com/google/deepvariant/issues/814:119,deployability,pipelin,pipeline,119,"@kishwarshafin . Thanks a lot for clarifying that, we were making a benchmark study on variant callers to optimize our pipeline. We also used PEPPER-DeepVariant and it worked with high precision. Again, thanks a lot! It was a helpful communication.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/814
https://github.com/google/deepvariant/issues/814:106,energy efficiency,optim,optimize,106,"@kishwarshafin . Thanks a lot for clarifying that, we were making a benchmark study on variant callers to optimize our pipeline. We also used PEPPER-DeepVariant and it worked with high precision. Again, thanks a lot! It was a helpful communication.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/814
https://github.com/google/deepvariant/issues/814:119,integrability,pipelin,pipeline,119,"@kishwarshafin . Thanks a lot for clarifying that, we were making a benchmark study on variant callers to optimize our pipeline. We also used PEPPER-DeepVariant and it worked with high precision. Again, thanks a lot! It was a helpful communication.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/814
https://github.com/google/deepvariant/issues/814:106,performance,optimiz,optimize,106,"@kishwarshafin . Thanks a lot for clarifying that, we were making a benchmark study on variant callers to optimize our pipeline. We also used PEPPER-DeepVariant and it worked with high precision. Again, thanks a lot! It was a helpful communication.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/814
https://github.com/google/deepvariant/issues/814:226,usability,help,helpful,226,"@kishwarshafin . Thanks a lot for clarifying that, we were making a benchmark study on variant callers to optimize our pipeline. We also used PEPPER-DeepVariant and it worked with high precision. Again, thanks a lot! It was a helpful communication.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/814
https://github.com/google/deepvariant/issues/814:22,usability,close,close,22,"Great, thanks, I will close the issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/814
https://github.com/google/deepvariant/issues/815:152,deployability,log,log,152,"Looks like this is the GLnexus question. Could you please post the question at GLNexus [page ](https://github.com/dnanexus-rnd/GLnexus). Also, from the log output it looks like GLnexus was completed successfully.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/815
https://github.com/google/deepvariant/issues/815:152,safety,log,log,152,"Looks like this is the GLnexus question. Could you please post the question at GLNexus [page ](https://github.com/dnanexus-rnd/GLnexus). Also, from the log output it looks like GLnexus was completed successfully.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/815
https://github.com/google/deepvariant/issues/815:189,safety,compl,completed,189,"Looks like this is the GLnexus question. Could you please post the question at GLNexus [page ](https://github.com/dnanexus-rnd/GLnexus). Also, from the log output it looks like GLnexus was completed successfully.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/815
https://github.com/google/deepvariant/issues/815:152,security,log,log,152,"Looks like this is the GLnexus question. Could you please post the question at GLNexus [page ](https://github.com/dnanexus-rnd/GLnexus). Also, from the log output it looks like GLnexus was completed successfully.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/815
https://github.com/google/deepvariant/issues/815:189,security,compl,completed,189,"Looks like this is the GLnexus question. Could you please post the question at GLNexus [page ](https://github.com/dnanexus-rnd/GLnexus). Also, from the log output it looks like GLnexus was completed successfully.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/815
https://github.com/google/deepvariant/issues/815:152,testability,log,log,152,"Looks like this is the GLnexus question. Could you please post the question at GLNexus [page ](https://github.com/dnanexus-rnd/GLnexus). Also, from the log output it looks like GLnexus was completed successfully.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/815
https://github.com/google/deepvariant/issues/815:54,deployability,log,log,54,"Hi @poddarharsh15 . Actually, can you go back in your log and confirm that DeepTrio runs actually finish correctly? If I remember correctly, our run_deeptrio one-step script might continue to run the following steps even when previous steps failed.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/815
https://github.com/google/deepvariant/issues/815:180,deployability,continu,continue,180,"Hi @poddarharsh15 . Actually, can you go back in your log and confirm that DeepTrio runs actually finish correctly? If I remember correctly, our run_deeptrio one-step script might continue to run the following steps even when previous steps failed.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/815
https://github.com/google/deepvariant/issues/815:241,deployability,fail,failed,241,"Hi @poddarharsh15 . Actually, can you go back in your log and confirm that DeepTrio runs actually finish correctly? If I remember correctly, our run_deeptrio one-step script might continue to run the following steps even when previous steps failed.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/815
https://github.com/google/deepvariant/issues/815:241,reliability,fail,failed,241,"Hi @poddarharsh15 . Actually, can you go back in your log and confirm that DeepTrio runs actually finish correctly? If I remember correctly, our run_deeptrio one-step script might continue to run the following steps even when previous steps failed.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/815
https://github.com/google/deepvariant/issues/815:54,safety,log,log,54,"Hi @poddarharsh15 . Actually, can you go back in your log and confirm that DeepTrio runs actually finish correctly? If I remember correctly, our run_deeptrio one-step script might continue to run the following steps even when previous steps failed.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/815
https://github.com/google/deepvariant/issues/815:121,safety,reme,remember,121,"Hi @poddarharsh15 . Actually, can you go back in your log and confirm that DeepTrio runs actually finish correctly? If I remember correctly, our run_deeptrio one-step script might continue to run the following steps even when previous steps failed.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/815
https://github.com/google/deepvariant/issues/815:54,security,log,log,54,"Hi @poddarharsh15 . Actually, can you go back in your log and confirm that DeepTrio runs actually finish correctly? If I remember correctly, our run_deeptrio one-step script might continue to run the following steps even when previous steps failed.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/815
https://github.com/google/deepvariant/issues/815:54,testability,log,log,54,"Hi @poddarharsh15 . Actually, can you go back in your log and confirm that DeepTrio runs actually finish correctly? If I remember correctly, our run_deeptrio one-step script might continue to run the following steps even when previous steps failed.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/815
https://github.com/google/deepvariant/issues/815:62,usability,confirm,confirm,62,"Hi @poddarharsh15 . Actually, can you go back in your log and confirm that DeepTrio runs actually finish correctly? If I remember correctly, our run_deeptrio one-step script might continue to run the following steps even when previous steps failed.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/815
https://github.com/google/deepvariant/issues/815:294,deployability,log,log,294,"And, follow up on @akolesnikov 's point, if you have gotten to this point, it would seem like these files should be complete? ```. /output/HG002.g.vcf.gz \. /output/HG003.g.vcf.gz \. /output/HG004.g.vcf.gz \. ```. If you can examine those files and confirm, that will be great. (Or look at the log like I mentioned before. But given you have the files, checking the files directly might be easier :))",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/815
https://github.com/google/deepvariant/issues/815:116,safety,compl,complete,116,"And, follow up on @akolesnikov 's point, if you have gotten to this point, it would seem like these files should be complete? ```. /output/HG002.g.vcf.gz \. /output/HG003.g.vcf.gz \. /output/HG004.g.vcf.gz \. ```. If you can examine those files and confirm, that will be great. (Or look at the log like I mentioned before. But given you have the files, checking the files directly might be easier :))",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/815
https://github.com/google/deepvariant/issues/815:294,safety,log,log,294,"And, follow up on @akolesnikov 's point, if you have gotten to this point, it would seem like these files should be complete? ```. /output/HG002.g.vcf.gz \. /output/HG003.g.vcf.gz \. /output/HG004.g.vcf.gz \. ```. If you can examine those files and confirm, that will be great. (Or look at the log like I mentioned before. But given you have the files, checking the files directly might be easier :))",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/815
https://github.com/google/deepvariant/issues/815:116,security,compl,complete,116,"And, follow up on @akolesnikov 's point, if you have gotten to this point, it would seem like these files should be complete? ```. /output/HG002.g.vcf.gz \. /output/HG003.g.vcf.gz \. /output/HG004.g.vcf.gz \. ```. If you can examine those files and confirm, that will be great. (Or look at the log like I mentioned before. But given you have the files, checking the files directly might be easier :))",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/815
https://github.com/google/deepvariant/issues/815:294,security,log,log,294,"And, follow up on @akolesnikov 's point, if you have gotten to this point, it would seem like these files should be complete? ```. /output/HG002.g.vcf.gz \. /output/HG003.g.vcf.gz \. /output/HG004.g.vcf.gz \. ```. If you can examine those files and confirm, that will be great. (Or look at the log like I mentioned before. But given you have the files, checking the files directly might be easier :))",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/815
https://github.com/google/deepvariant/issues/815:294,testability,log,log,294,"And, follow up on @akolesnikov 's point, if you have gotten to this point, it would seem like these files should be complete? ```. /output/HG002.g.vcf.gz \. /output/HG003.g.vcf.gz \. /output/HG004.g.vcf.gz \. ```. If you can examine those files and confirm, that will be great. (Or look at the log like I mentioned before. But given you have the files, checking the files directly might be easier :))",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/815
https://github.com/google/deepvariant/issues/815:249,usability,confirm,confirm,249,"And, follow up on @akolesnikov 's point, if you have gotten to this point, it would seem like these files should be complete? ```. /output/HG002.g.vcf.gz \. /output/HG003.g.vcf.gz \. /output/HG004.g.vcf.gz \. ```. If you can examine those files and confirm, that will be great. (Or look at the log like I mentioned before. But given you have the files, checking the files directly might be easier :))",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/815
https://github.com/google/deepvariant/issues/815:162,availability,error,errors,162,"Hi @pichuan, @akolesnikov,. I'm new to DeepTrio and couldn't locate the log files, but I have intermediate results showing that DeepTrio ran successfully without errors. Additionally, I successfully benchmarked the .vcf files generated by DeepTrio. I've attached screenshots for reference. Your assistance is greatly appreciated. Thank you. finished log. ![Screenshot from 2024-05-07 09-52-02](https://github.com/google/deepvariant/assets/45700858/1f9f1b6d-bdd5-4d5d-87fd-a1416e8b4f22). ![Screenshot from 2024-05-07 09-52-32](https://github.com/google/deepvariant/assets/45700858/1c40eb86-018e-4df7-a290-963dccb767b8). Benchmark. ![Screenshot from 2024-05-07 09-52-59](https://github.com/google/deepvariant/assets/45700858/e98c8307-12ba-4c15-a26a-2323948e8f05).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/815
https://github.com/google/deepvariant/issues/815:72,deployability,log,log,72,"Hi @pichuan, @akolesnikov,. I'm new to DeepTrio and couldn't locate the log files, but I have intermediate results showing that DeepTrio ran successfully without errors. Additionally, I successfully benchmarked the .vcf files generated by DeepTrio. I've attached screenshots for reference. Your assistance is greatly appreciated. Thank you. finished log. ![Screenshot from 2024-05-07 09-52-02](https://github.com/google/deepvariant/assets/45700858/1f9f1b6d-bdd5-4d5d-87fd-a1416e8b4f22). ![Screenshot from 2024-05-07 09-52-32](https://github.com/google/deepvariant/assets/45700858/1c40eb86-018e-4df7-a290-963dccb767b8). Benchmark. ![Screenshot from 2024-05-07 09-52-59](https://github.com/google/deepvariant/assets/45700858/e98c8307-12ba-4c15-a26a-2323948e8f05).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/815
https://github.com/google/deepvariant/issues/815:350,deployability,log,log,350,"Hi @pichuan, @akolesnikov,. I'm new to DeepTrio and couldn't locate the log files, but I have intermediate results showing that DeepTrio ran successfully without errors. Additionally, I successfully benchmarked the .vcf files generated by DeepTrio. I've attached screenshots for reference. Your assistance is greatly appreciated. Thank you. finished log. ![Screenshot from 2024-05-07 09-52-02](https://github.com/google/deepvariant/assets/45700858/1f9f1b6d-bdd5-4d5d-87fd-a1416e8b4f22). ![Screenshot from 2024-05-07 09-52-32](https://github.com/google/deepvariant/assets/45700858/1c40eb86-018e-4df7-a290-963dccb767b8). Benchmark. ![Screenshot from 2024-05-07 09-52-59](https://github.com/google/deepvariant/assets/45700858/e98c8307-12ba-4c15-a26a-2323948e8f05).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/815
https://github.com/google/deepvariant/issues/815:94,modifiability,interm,intermediate,94,"Hi @pichuan, @akolesnikov,. I'm new to DeepTrio and couldn't locate the log files, but I have intermediate results showing that DeepTrio ran successfully without errors. Additionally, I successfully benchmarked the .vcf files generated by DeepTrio. I've attached screenshots for reference. Your assistance is greatly appreciated. Thank you. finished log. ![Screenshot from 2024-05-07 09-52-02](https://github.com/google/deepvariant/assets/45700858/1f9f1b6d-bdd5-4d5d-87fd-a1416e8b4f22). ![Screenshot from 2024-05-07 09-52-32](https://github.com/google/deepvariant/assets/45700858/1c40eb86-018e-4df7-a290-963dccb767b8). Benchmark. ![Screenshot from 2024-05-07 09-52-59](https://github.com/google/deepvariant/assets/45700858/e98c8307-12ba-4c15-a26a-2323948e8f05).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/815
https://github.com/google/deepvariant/issues/815:162,performance,error,errors,162,"Hi @pichuan, @akolesnikov,. I'm new to DeepTrio and couldn't locate the log files, but I have intermediate results showing that DeepTrio ran successfully without errors. Additionally, I successfully benchmarked the .vcf files generated by DeepTrio. I've attached screenshots for reference. Your assistance is greatly appreciated. Thank you. finished log. ![Screenshot from 2024-05-07 09-52-02](https://github.com/google/deepvariant/assets/45700858/1f9f1b6d-bdd5-4d5d-87fd-a1416e8b4f22). ![Screenshot from 2024-05-07 09-52-32](https://github.com/google/deepvariant/assets/45700858/1c40eb86-018e-4df7-a290-963dccb767b8). Benchmark. ![Screenshot from 2024-05-07 09-52-59](https://github.com/google/deepvariant/assets/45700858/e98c8307-12ba-4c15-a26a-2323948e8f05).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/815
https://github.com/google/deepvariant/issues/815:72,safety,log,log,72,"Hi @pichuan, @akolesnikov,. I'm new to DeepTrio and couldn't locate the log files, but I have intermediate results showing that DeepTrio ran successfully without errors. Additionally, I successfully benchmarked the .vcf files generated by DeepTrio. I've attached screenshots for reference. Your assistance is greatly appreciated. Thank you. finished log. ![Screenshot from 2024-05-07 09-52-02](https://github.com/google/deepvariant/assets/45700858/1f9f1b6d-bdd5-4d5d-87fd-a1416e8b4f22). ![Screenshot from 2024-05-07 09-52-32](https://github.com/google/deepvariant/assets/45700858/1c40eb86-018e-4df7-a290-963dccb767b8). Benchmark. ![Screenshot from 2024-05-07 09-52-59](https://github.com/google/deepvariant/assets/45700858/e98c8307-12ba-4c15-a26a-2323948e8f05).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/815
https://github.com/google/deepvariant/issues/815:162,safety,error,errors,162,"Hi @pichuan, @akolesnikov,. I'm new to DeepTrio and couldn't locate the log files, but I have intermediate results showing that DeepTrio ran successfully without errors. Additionally, I successfully benchmarked the .vcf files generated by DeepTrio. I've attached screenshots for reference. Your assistance is greatly appreciated. Thank you. finished log. ![Screenshot from 2024-05-07 09-52-02](https://github.com/google/deepvariant/assets/45700858/1f9f1b6d-bdd5-4d5d-87fd-a1416e8b4f22). ![Screenshot from 2024-05-07 09-52-32](https://github.com/google/deepvariant/assets/45700858/1c40eb86-018e-4df7-a290-963dccb767b8). Benchmark. ![Screenshot from 2024-05-07 09-52-59](https://github.com/google/deepvariant/assets/45700858/e98c8307-12ba-4c15-a26a-2323948e8f05).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/815
https://github.com/google/deepvariant/issues/815:350,safety,log,log,350,"Hi @pichuan, @akolesnikov,. I'm new to DeepTrio and couldn't locate the log files, but I have intermediate results showing that DeepTrio ran successfully without errors. Additionally, I successfully benchmarked the .vcf files generated by DeepTrio. I've attached screenshots for reference. Your assistance is greatly appreciated. Thank you. finished log. ![Screenshot from 2024-05-07 09-52-02](https://github.com/google/deepvariant/assets/45700858/1f9f1b6d-bdd5-4d5d-87fd-a1416e8b4f22). ![Screenshot from 2024-05-07 09-52-32](https://github.com/google/deepvariant/assets/45700858/1c40eb86-018e-4df7-a290-963dccb767b8). Benchmark. ![Screenshot from 2024-05-07 09-52-59](https://github.com/google/deepvariant/assets/45700858/e98c8307-12ba-4c15-a26a-2323948e8f05).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/815
https://github.com/google/deepvariant/issues/815:72,security,log,log,72,"Hi @pichuan, @akolesnikov,. I'm new to DeepTrio and couldn't locate the log files, but I have intermediate results showing that DeepTrio ran successfully without errors. Additionally, I successfully benchmarked the .vcf files generated by DeepTrio. I've attached screenshots for reference. Your assistance is greatly appreciated. Thank you. finished log. ![Screenshot from 2024-05-07 09-52-02](https://github.com/google/deepvariant/assets/45700858/1f9f1b6d-bdd5-4d5d-87fd-a1416e8b4f22). ![Screenshot from 2024-05-07 09-52-32](https://github.com/google/deepvariant/assets/45700858/1c40eb86-018e-4df7-a290-963dccb767b8). Benchmark. ![Screenshot from 2024-05-07 09-52-59](https://github.com/google/deepvariant/assets/45700858/e98c8307-12ba-4c15-a26a-2323948e8f05).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/815
https://github.com/google/deepvariant/issues/815:350,security,log,log,350,"Hi @pichuan, @akolesnikov,. I'm new to DeepTrio and couldn't locate the log files, but I have intermediate results showing that DeepTrio ran successfully without errors. Additionally, I successfully benchmarked the .vcf files generated by DeepTrio. I've attached screenshots for reference. Your assistance is greatly appreciated. Thank you. finished log. ![Screenshot from 2024-05-07 09-52-02](https://github.com/google/deepvariant/assets/45700858/1f9f1b6d-bdd5-4d5d-87fd-a1416e8b4f22). ![Screenshot from 2024-05-07 09-52-32](https://github.com/google/deepvariant/assets/45700858/1c40eb86-018e-4df7-a290-963dccb767b8). Benchmark. ![Screenshot from 2024-05-07 09-52-59](https://github.com/google/deepvariant/assets/45700858/e98c8307-12ba-4c15-a26a-2323948e8f05).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/815
https://github.com/google/deepvariant/issues/815:72,testability,log,log,72,"Hi @pichuan, @akolesnikov,. I'm new to DeepTrio and couldn't locate the log files, but I have intermediate results showing that DeepTrio ran successfully without errors. Additionally, I successfully benchmarked the .vcf files generated by DeepTrio. I've attached screenshots for reference. Your assistance is greatly appreciated. Thank you. finished log. ![Screenshot from 2024-05-07 09-52-02](https://github.com/google/deepvariant/assets/45700858/1f9f1b6d-bdd5-4d5d-87fd-a1416e8b4f22). ![Screenshot from 2024-05-07 09-52-32](https://github.com/google/deepvariant/assets/45700858/1c40eb86-018e-4df7-a290-963dccb767b8). Benchmark. ![Screenshot from 2024-05-07 09-52-59](https://github.com/google/deepvariant/assets/45700858/e98c8307-12ba-4c15-a26a-2323948e8f05).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/815
https://github.com/google/deepvariant/issues/815:350,testability,log,log,350,"Hi @pichuan, @akolesnikov,. I'm new to DeepTrio and couldn't locate the log files, but I have intermediate results showing that DeepTrio ran successfully without errors. Additionally, I successfully benchmarked the .vcf files generated by DeepTrio. I've attached screenshots for reference. Your assistance is greatly appreciated. Thank you. finished log. ![Screenshot from 2024-05-07 09-52-02](https://github.com/google/deepvariant/assets/45700858/1f9f1b6d-bdd5-4d5d-87fd-a1416e8b4f22). ![Screenshot from 2024-05-07 09-52-32](https://github.com/google/deepvariant/assets/45700858/1c40eb86-018e-4df7-a290-963dccb767b8). Benchmark. ![Screenshot from 2024-05-07 09-52-59](https://github.com/google/deepvariant/assets/45700858/e98c8307-12ba-4c15-a26a-2323948e8f05).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/815
https://github.com/google/deepvariant/issues/815:162,usability,error,errors,162,"Hi @pichuan, @akolesnikov,. I'm new to DeepTrio and couldn't locate the log files, but I have intermediate results showing that DeepTrio ran successfully without errors. Additionally, I successfully benchmarked the .vcf files generated by DeepTrio. I've attached screenshots for reference. Your assistance is greatly appreciated. Thank you. finished log. ![Screenshot from 2024-05-07 09-52-02](https://github.com/google/deepvariant/assets/45700858/1f9f1b6d-bdd5-4d5d-87fd-a1416e8b4f22). ![Screenshot from 2024-05-07 09-52-32](https://github.com/google/deepvariant/assets/45700858/1c40eb86-018e-4df7-a290-963dccb767b8). Benchmark. ![Screenshot from 2024-05-07 09-52-59](https://github.com/google/deepvariant/assets/45700858/e98c8307-12ba-4c15-a26a-2323948e8f05).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/815
https://github.com/google/deepvariant/issues/815:188,availability,down,downstream,188,"Hi @poddarharsh15 , it seems like you're certain that the DeepTrio run finished correctly. In that case, I agree with @akolesnikov 's original assessment that this can be an issue for the downstream glnexus step, which we can't directly support. One suggestion to try:. If you need to check your run a bit more closely, maybe breaking it down to just running this part first:. ```bash. udocker run \. -v ""${PWD}/output"":""/output"" \. quay.io/mlin/glnexus:v1.2.7 \. /usr/local/bin/glnexus_cli \. --config DeepVariant_unfiltered \. /output/HG002.g.vcf.gz \. /output/HG003.g.vcf.gz \. /output/HG004.g.vcf.gz. ```. before piping to the next step. Maybe that could help you identify what the errors are coming out from that step?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/815
https://github.com/google/deepvariant/issues/815:338,availability,down,down,338,"Hi @poddarharsh15 , it seems like you're certain that the DeepTrio run finished correctly. In that case, I agree with @akolesnikov 's original assessment that this can be an issue for the downstream glnexus step, which we can't directly support. One suggestion to try:. If you need to check your run a bit more closely, maybe breaking it down to just running this part first:. ```bash. udocker run \. -v ""${PWD}/output"":""/output"" \. quay.io/mlin/glnexus:v1.2.7 \. /usr/local/bin/glnexus_cli \. --config DeepVariant_unfiltered \. /output/HG002.g.vcf.gz \. /output/HG003.g.vcf.gz \. /output/HG004.g.vcf.gz. ```. before piping to the next step. Maybe that could help you identify what the errors are coming out from that step?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/815
https://github.com/google/deepvariant/issues/815:686,availability,error,errors,686,"Hi @poddarharsh15 , it seems like you're certain that the DeepTrio run finished correctly. In that case, I agree with @akolesnikov 's original assessment that this can be an issue for the downstream glnexus step, which we can't directly support. One suggestion to try:. If you need to check your run a bit more closely, maybe breaking it down to just running this part first:. ```bash. udocker run \. -v ""${PWD}/output"":""/output"" \. quay.io/mlin/glnexus:v1.2.7 \. /usr/local/bin/glnexus_cli \. --config DeepVariant_unfiltered \. /output/HG002.g.vcf.gz \. /output/HG003.g.vcf.gz \. /output/HG004.g.vcf.gz. ```. before piping to the next step. Maybe that could help you identify what the errors are coming out from that step?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/815
https://github.com/google/deepvariant/issues/815:686,performance,error,errors,686,"Hi @poddarharsh15 , it seems like you're certain that the DeepTrio run finished correctly. In that case, I agree with @akolesnikov 's original assessment that this can be an issue for the downstream glnexus step, which we can't directly support. One suggestion to try:. If you need to check your run a bit more closely, maybe breaking it down to just running this part first:. ```bash. udocker run \. -v ""${PWD}/output"":""/output"" \. quay.io/mlin/glnexus:v1.2.7 \. /usr/local/bin/glnexus_cli \. --config DeepVariant_unfiltered \. /output/HG002.g.vcf.gz \. /output/HG003.g.vcf.gz \. /output/HG004.g.vcf.gz. ```. before piping to the next step. Maybe that could help you identify what the errors are coming out from that step?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/815
https://github.com/google/deepvariant/issues/815:686,safety,error,errors,686,"Hi @poddarharsh15 , it seems like you're certain that the DeepTrio run finished correctly. In that case, I agree with @akolesnikov 's original assessment that this can be an issue for the downstream glnexus step, which we can't directly support. One suggestion to try:. If you need to check your run a bit more closely, maybe breaking it down to just running this part first:. ```bash. udocker run \. -v ""${PWD}/output"":""/output"" \. quay.io/mlin/glnexus:v1.2.7 \. /usr/local/bin/glnexus_cli \. --config DeepVariant_unfiltered \. /output/HG002.g.vcf.gz \. /output/HG003.g.vcf.gz \. /output/HG004.g.vcf.gz. ```. before piping to the next step. Maybe that could help you identify what the errors are coming out from that step?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/815
https://github.com/google/deepvariant/issues/815:143,security,assess,assessment,143,"Hi @poddarharsh15 , it seems like you're certain that the DeepTrio run finished correctly. In that case, I agree with @akolesnikov 's original assessment that this can be an issue for the downstream glnexus step, which we can't directly support. One suggestion to try:. If you need to check your run a bit more closely, maybe breaking it down to just running this part first:. ```bash. udocker run \. -v ""${PWD}/output"":""/output"" \. quay.io/mlin/glnexus:v1.2.7 \. /usr/local/bin/glnexus_cli \. --config DeepVariant_unfiltered \. /output/HG002.g.vcf.gz \. /output/HG003.g.vcf.gz \. /output/HG004.g.vcf.gz. ```. before piping to the next step. Maybe that could help you identify what the errors are coming out from that step?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/815
https://github.com/google/deepvariant/issues/815:668,security,ident,identify,668,"Hi @poddarharsh15 , it seems like you're certain that the DeepTrio run finished correctly. In that case, I agree with @akolesnikov 's original assessment that this can be an issue for the downstream glnexus step, which we can't directly support. One suggestion to try:. If you need to check your run a bit more closely, maybe breaking it down to just running this part first:. ```bash. udocker run \. -v ""${PWD}/output"":""/output"" \. quay.io/mlin/glnexus:v1.2.7 \. /usr/local/bin/glnexus_cli \. --config DeepVariant_unfiltered \. /output/HG002.g.vcf.gz \. /output/HG003.g.vcf.gz \. /output/HG004.g.vcf.gz. ```. before piping to the next step. Maybe that could help you identify what the errors are coming out from that step?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/815
https://github.com/google/deepvariant/issues/815:237,usability,support,support,237,"Hi @poddarharsh15 , it seems like you're certain that the DeepTrio run finished correctly. In that case, I agree with @akolesnikov 's original assessment that this can be an issue for the downstream glnexus step, which we can't directly support. One suggestion to try:. If you need to check your run a bit more closely, maybe breaking it down to just running this part first:. ```bash. udocker run \. -v ""${PWD}/output"":""/output"" \. quay.io/mlin/glnexus:v1.2.7 \. /usr/local/bin/glnexus_cli \. --config DeepVariant_unfiltered \. /output/HG002.g.vcf.gz \. /output/HG003.g.vcf.gz \. /output/HG004.g.vcf.gz. ```. before piping to the next step. Maybe that could help you identify what the errors are coming out from that step?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/815
https://github.com/google/deepvariant/issues/815:311,usability,close,closely,311,"Hi @poddarharsh15 , it seems like you're certain that the DeepTrio run finished correctly. In that case, I agree with @akolesnikov 's original assessment that this can be an issue for the downstream glnexus step, which we can't directly support. One suggestion to try:. If you need to check your run a bit more closely, maybe breaking it down to just running this part first:. ```bash. udocker run \. -v ""${PWD}/output"":""/output"" \. quay.io/mlin/glnexus:v1.2.7 \. /usr/local/bin/glnexus_cli \. --config DeepVariant_unfiltered \. /output/HG002.g.vcf.gz \. /output/HG003.g.vcf.gz \. /output/HG004.g.vcf.gz. ```. before piping to the next step. Maybe that could help you identify what the errors are coming out from that step?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/815
https://github.com/google/deepvariant/issues/815:659,usability,help,help,659,"Hi @poddarharsh15 , it seems like you're certain that the DeepTrio run finished correctly. In that case, I agree with @akolesnikov 's original assessment that this can be an issue for the downstream glnexus step, which we can't directly support. One suggestion to try:. If you need to check your run a bit more closely, maybe breaking it down to just running this part first:. ```bash. udocker run \. -v ""${PWD}/output"":""/output"" \. quay.io/mlin/glnexus:v1.2.7 \. /usr/local/bin/glnexus_cli \. --config DeepVariant_unfiltered \. /output/HG002.g.vcf.gz \. /output/HG003.g.vcf.gz \. /output/HG004.g.vcf.gz. ```. before piping to the next step. Maybe that could help you identify what the errors are coming out from that step?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/815
https://github.com/google/deepvariant/issues/815:686,usability,error,errors,686,"Hi @poddarharsh15 , it seems like you're certain that the DeepTrio run finished correctly. In that case, I agree with @akolesnikov 's original assessment that this can be an issue for the downstream glnexus step, which we can't directly support. One suggestion to try:. If you need to check your run a bit more closely, maybe breaking it down to just running this part first:. ```bash. udocker run \. -v ""${PWD}/output"":""/output"" \. quay.io/mlin/glnexus:v1.2.7 \. /usr/local/bin/glnexus_cli \. --config DeepVariant_unfiltered \. /output/HG002.g.vcf.gz \. /output/HG003.g.vcf.gz \. /output/HG004.g.vcf.gz. ```. before piping to the next step. Maybe that could help you identify what the errors are coming out from that step?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/815
https://github.com/google/deepvariant/issues/816:349,security,access,access,349,"Hi @prasundutta87 , given that `--haploid_contigs` and `--par_regions_bed` are arguments for postprocess_variants, you can try using:. `--postprocess_variants_extra_args=""--haploid_contigs=yourValue,--par_regions_bed=yourValue"""". If you're using Docker, you likely need to make sure the par_regions_bed path are something that the run in Docker can access.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/816
https://github.com/google/deepvariant/issues/816:259,modifiability,pac,pacbio-case-study,259,"Hi @pichuan , thanks a lot for the quick reply. I am running the ""run_deeptrio"" script from DeepTrio docker (v1.6.1). As I have trios, I am running it all together in one command as described in https://github.com/google/deepvariant/blob/r1.6.1/docs/deeptrio-pacbio-case-study.md. I will then be merging the single sample gvcfs using GLnexus as described there. . Since, I am using trios, will just using `--postprocess_variants_extra_args=""--haploid_contigs=yourValue,--par_regions_bed=yourValue"""" help or I need to use the second suggestion? I have attached the example command I am running for your reference:. `apptainer run \. SNV_analysis_DeepTrio/images/deepvariant_deeptrio-1.6.1.sif \. run_deeptrio \. --model_type=PACBIO \. --ref=hg38_reference/GCA_000001405.15_GRCh38_no_alt_plus_hs38d1_analysis_set.fna \. --reads_child=BAMS/Proband_sorted.bam \. --reads_parent1=BAMS/Parent-1_sorted.bam \. --reads_parent2=BAMS/Parent-2_sorted.bam \. --output_vcf_child output/Proband.output.vcf.gz \. --output_vcf_parent1 output/Parent-1.output.vcf.gz \. --output_vcf_parent2 output/Parent-2.output.vcf.gz \. --sample_name_child 'Proband' \. --sample_name_parent1 'Parent-1' \. --sample_name_parent2 'Parent-2' \. --intermediate_results_dir output/intermediate_results_dir \. --output_gvcf_child output/Proband.g.vcf.gz \. --output_gvcf_parent1 output/Parent-1.g.vcf.gz \. --output_gvcf_parent2 output/Parent-2.g.vcf.gz \. --regions chr21 \. --num_shards=5. `",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/816
https://github.com/google/deepvariant/issues/816:724,modifiability,PAC,PACBIO,724,"Hi @pichuan , thanks a lot for the quick reply. I am running the ""run_deeptrio"" script from DeepTrio docker (v1.6.1). As I have trios, I am running it all together in one command as described in https://github.com/google/deepvariant/blob/r1.6.1/docs/deeptrio-pacbio-case-study.md. I will then be merging the single sample gvcfs using GLnexus as described there. . Since, I am using trios, will just using `--postprocess_variants_extra_args=""--haploid_contigs=yourValue,--par_regions_bed=yourValue"""" help or I need to use the second suggestion? I have attached the example command I am running for your reference:. `apptainer run \. SNV_analysis_DeepTrio/images/deepvariant_deeptrio-1.6.1.sif \. run_deeptrio \. --model_type=PACBIO \. --ref=hg38_reference/GCA_000001405.15_GRCh38_no_alt_plus_hs38d1_analysis_set.fna \. --reads_child=BAMS/Proband_sorted.bam \. --reads_parent1=BAMS/Parent-1_sorted.bam \. --reads_parent2=BAMS/Parent-2_sorted.bam \. --output_vcf_child output/Proband.output.vcf.gz \. --output_vcf_parent1 output/Parent-1.output.vcf.gz \. --output_vcf_parent2 output/Parent-2.output.vcf.gz \. --sample_name_child 'Proband' \. --sample_name_parent1 'Parent-1' \. --sample_name_parent2 'Parent-2' \. --intermediate_results_dir output/intermediate_results_dir \. --output_gvcf_child output/Proband.g.vcf.gz \. --output_gvcf_parent1 output/Parent-1.g.vcf.gz \. --output_gvcf_parent2 output/Parent-2.g.vcf.gz \. --regions chr21 \. --num_shards=5. `",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/816
https://github.com/google/deepvariant/issues/816:171,usability,command,command,171,"Hi @pichuan , thanks a lot for the quick reply. I am running the ""run_deeptrio"" script from DeepTrio docker (v1.6.1). As I have trios, I am running it all together in one command as described in https://github.com/google/deepvariant/blob/r1.6.1/docs/deeptrio-pacbio-case-study.md. I will then be merging the single sample gvcfs using GLnexus as described there. . Since, I am using trios, will just using `--postprocess_variants_extra_args=""--haploid_contigs=yourValue,--par_regions_bed=yourValue"""" help or I need to use the second suggestion? I have attached the example command I am running for your reference:. `apptainer run \. SNV_analysis_DeepTrio/images/deepvariant_deeptrio-1.6.1.sif \. run_deeptrio \. --model_type=PACBIO \. --ref=hg38_reference/GCA_000001405.15_GRCh38_no_alt_plus_hs38d1_analysis_set.fna \. --reads_child=BAMS/Proband_sorted.bam \. --reads_parent1=BAMS/Parent-1_sorted.bam \. --reads_parent2=BAMS/Parent-2_sorted.bam \. --output_vcf_child output/Proband.output.vcf.gz \. --output_vcf_parent1 output/Parent-1.output.vcf.gz \. --output_vcf_parent2 output/Parent-2.output.vcf.gz \. --sample_name_child 'Proband' \. --sample_name_parent1 'Parent-1' \. --sample_name_parent2 'Parent-2' \. --intermediate_results_dir output/intermediate_results_dir \. --output_gvcf_child output/Proband.g.vcf.gz \. --output_gvcf_parent1 output/Parent-1.g.vcf.gz \. --output_gvcf_parent2 output/Parent-2.g.vcf.gz \. --regions chr21 \. --num_shards=5. `",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/816
https://github.com/google/deepvariant/issues/816:499,usability,help,help,499,"Hi @pichuan , thanks a lot for the quick reply. I am running the ""run_deeptrio"" script from DeepTrio docker (v1.6.1). As I have trios, I am running it all together in one command as described in https://github.com/google/deepvariant/blob/r1.6.1/docs/deeptrio-pacbio-case-study.md. I will then be merging the single sample gvcfs using GLnexus as described there. . Since, I am using trios, will just using `--postprocess_variants_extra_args=""--haploid_contigs=yourValue,--par_regions_bed=yourValue"""" help or I need to use the second suggestion? I have attached the example command I am running for your reference:. `apptainer run \. SNV_analysis_DeepTrio/images/deepvariant_deeptrio-1.6.1.sif \. run_deeptrio \. --model_type=PACBIO \. --ref=hg38_reference/GCA_000001405.15_GRCh38_no_alt_plus_hs38d1_analysis_set.fna \. --reads_child=BAMS/Proband_sorted.bam \. --reads_parent1=BAMS/Parent-1_sorted.bam \. --reads_parent2=BAMS/Parent-2_sorted.bam \. --output_vcf_child output/Proband.output.vcf.gz \. --output_vcf_parent1 output/Parent-1.output.vcf.gz \. --output_vcf_parent2 output/Parent-2.output.vcf.gz \. --sample_name_child 'Proband' \. --sample_name_parent1 'Parent-1' \. --sample_name_parent2 'Parent-2' \. --intermediate_results_dir output/intermediate_results_dir \. --output_gvcf_child output/Proband.g.vcf.gz \. --output_gvcf_parent1 output/Parent-1.g.vcf.gz \. --output_gvcf_parent2 output/Parent-2.g.vcf.gz \. --regions chr21 \. --num_shards=5. `",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/816
https://github.com/google/deepvariant/issues/816:572,usability,command,command,572,"Hi @pichuan , thanks a lot for the quick reply. I am running the ""run_deeptrio"" script from DeepTrio docker (v1.6.1). As I have trios, I am running it all together in one command as described in https://github.com/google/deepvariant/blob/r1.6.1/docs/deeptrio-pacbio-case-study.md. I will then be merging the single sample gvcfs using GLnexus as described there. . Since, I am using trios, will just using `--postprocess_variants_extra_args=""--haploid_contigs=yourValue,--par_regions_bed=yourValue"""" help or I need to use the second suggestion? I have attached the example command I am running for your reference:. `apptainer run \. SNV_analysis_DeepTrio/images/deepvariant_deeptrio-1.6.1.sif \. run_deeptrio \. --model_type=PACBIO \. --ref=hg38_reference/GCA_000001405.15_GRCh38_no_alt_plus_hs38d1_analysis_set.fna \. --reads_child=BAMS/Proband_sorted.bam \. --reads_parent1=BAMS/Parent-1_sorted.bam \. --reads_parent2=BAMS/Parent-2_sorted.bam \. --output_vcf_child output/Proband.output.vcf.gz \. --output_vcf_parent1 output/Parent-1.output.vcf.gz \. --output_vcf_parent2 output/Parent-2.output.vcf.gz \. --sample_name_child 'Proband' \. --sample_name_parent1 'Parent-1' \. --sample_name_parent2 'Parent-2' \. --intermediate_results_dir output/intermediate_results_dir \. --output_gvcf_child output/Proband.g.vcf.gz \. --output_gvcf_parent1 output/Parent-1.g.vcf.gz \. --output_gvcf_parent2 output/Parent-2.g.vcf.gz \. --regions chr21 \. --num_shards=5. `",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/816
https://github.com/google/deepvariant/issues/816:87,interoperability,specif,specify,87,"Hi @prasundutta87 ,. Right, given that you're running on trio, you'll for sure want to specify different things for the parent1 and parent2: only the dad would be using `haploid_contigs` `par_regions_bed` flags. The mom would have two copies of chrX, so you should not specify chrX to be haploid. And, for the child: you should only use those flags if the child is male.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/816
https://github.com/google/deepvariant/issues/816:269,interoperability,specif,specify,269,"Hi @prasundutta87 ,. Right, given that you're running on trio, you'll for sure want to specify different things for the parent1 and parent2: only the dad would be using `haploid_contigs` `par_regions_bed` flags. The mom would have two copies of chrX, so you should not specify chrX to be haploid. And, for the child: you should only use those flags if the child is male.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/816
https://github.com/google/deepvariant/issues/816:133,deployability,pipelin,pipeline,133,"Sounds good, thanks a lot @pichuan . Just an additional query. If I want to joint call SNVs/indels in a set of multiple trios, which pipeline is recommended? Deepvariant or DeepTrio. My understanding is that if we joint call multiple samples together, the cohort allele frequency could be used to filter rare variants. If we use DeepTrio, AF calculation will be restricted to only the trio or family. What is the best practice here in terms of Deepvariant?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/816
https://github.com/google/deepvariant/issues/816:270,energy efficiency,frequenc,frequency,270,"Sounds good, thanks a lot @pichuan . Just an additional query. If I want to joint call SNVs/indels in a set of multiple trios, which pipeline is recommended? Deepvariant or DeepTrio. My understanding is that if we joint call multiple samples together, the cohort allele frequency could be used to filter rare variants. If we use DeepTrio, AF calculation will be restricted to only the trio or family. What is the best practice here in terms of Deepvariant?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/816
https://github.com/google/deepvariant/issues/816:133,integrability,pipelin,pipeline,133,"Sounds good, thanks a lot @pichuan . Just an additional query. If I want to joint call SNVs/indels in a set of multiple trios, which pipeline is recommended? Deepvariant or DeepTrio. My understanding is that if we joint call multiple samples together, the cohort allele frequency could be used to filter rare variants. If we use DeepTrio, AF calculation will be restricted to only the trio or family. What is the best practice here in terms of Deepvariant?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/816
https://github.com/google/deepvariant/issues/816:297,integrability,filter,filter,297,"Sounds good, thanks a lot @pichuan . Just an additional query. If I want to joint call SNVs/indels in a set of multiple trios, which pipeline is recommended? Deepvariant or DeepTrio. My understanding is that if we joint call multiple samples together, the cohort allele frequency could be used to filter rare variants. If we use DeepTrio, AF calculation will be restricted to only the trio or family. What is the best practice here in terms of Deepvariant?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/816
https://github.com/google/deepvariant/issues/816:418,reliability,pra,practice,418,"Sounds good, thanks a lot @pichuan . Just an additional query. If I want to joint call SNVs/indels in a set of multiple trios, which pipeline is recommended? Deepvariant or DeepTrio. My understanding is that if we joint call multiple samples together, the cohort allele frequency could be used to filter rare variants. If we use DeepTrio, AF calculation will be restricted to only the trio or family. What is the best practice here in terms of Deepvariant?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/816
https://github.com/google/deepvariant/issues/816:186,testability,understand,understanding,186,"Sounds good, thanks a lot @pichuan . Just an additional query. If I want to joint call SNVs/indels in a set of multiple trios, which pipeline is recommended? Deepvariant or DeepTrio. My understanding is that if we joint call multiple samples together, the cohort allele frequency could be used to filter rare variants. If we use DeepTrio, AF calculation will be restricted to only the trio or family. What is the best practice here in terms of Deepvariant?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/816
https://github.com/google/deepvariant/issues/816:527,energy efficiency,frequenc,frequency,527,"Hi @prasundutta87 . Both DeepVariant and DeepTrio can produce gVCFs, so you can joint call in a similar manner. In both cases you should use the DeepVariant_unfiltered preset for GLnexus, because there is family structure present which the other filtering presets wouldn't know about. Because you can joint genotype multiple trio gVCFs from either DeepVariant or DeepTrio in the same way, I would use DeepTrio to produce the gVCFs, take all of the gVCFs and run them together through glnexus, and then you can still use allele frequency information. To be clear, the unfiltered preset looks like this:. ```. sudo docker run \. -v ""${PWD}/output"":""/output"" \. quay.io/mlin/glnexus:v1.2.7 \. /usr/local/bin/glnexus_cli \. --config DeepVariant_unfiltered \. /output/child_trio_1.g.vcf.gz\. /output/parent1_trio_1.g.vcf.gz \. /output/parent2_trio_1.g.vcf.gz \. /output/child_trio_2g.vcf.gz\. /output/parent1_trio_2.g.vcf.gz \. /output/parent2_trio_2.g.vcf.gz \. | sudo docker run -i google/deepvariant:deeptrio-""${BIN_VERSION}"" \. bcftools view - \. | sudo docker run -i google/deepvariant:deeptrio-""${BIN_VERSION}"" \. bgzip -c > output/trio_cohort_merged.vcf.gz. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/816
https://github.com/google/deepvariant/issues/816:246,integrability,filter,filtering,246,"Hi @prasundutta87 . Both DeepVariant and DeepTrio can produce gVCFs, so you can joint call in a similar manner. In both cases you should use the DeepVariant_unfiltered preset for GLnexus, because there is family structure present which the other filtering presets wouldn't know about. Because you can joint genotype multiple trio gVCFs from either DeepVariant or DeepTrio in the same way, I would use DeepTrio to produce the gVCFs, take all of the gVCFs and run them together through glnexus, and then you can still use allele frequency information. To be clear, the unfiltered preset looks like this:. ```. sudo docker run \. -v ""${PWD}/output"":""/output"" \. quay.io/mlin/glnexus:v1.2.7 \. /usr/local/bin/glnexus_cli \. --config DeepVariant_unfiltered \. /output/child_trio_1.g.vcf.gz\. /output/parent1_trio_1.g.vcf.gz \. /output/parent2_trio_1.g.vcf.gz \. /output/child_trio_2g.vcf.gz\. /output/parent1_trio_2.g.vcf.gz \. /output/parent2_trio_2.g.vcf.gz \. | sudo docker run -i google/deepvariant:deeptrio-""${BIN_VERSION}"" \. bcftools view - \. | sudo docker run -i google/deepvariant:deeptrio-""${BIN_VERSION}"" \. bgzip -c > output/trio_cohort_merged.vcf.gz. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/816
https://github.com/google/deepvariant/issues/816:556,usability,clear,clear,556,"Hi @prasundutta87 . Both DeepVariant and DeepTrio can produce gVCFs, so you can joint call in a similar manner. In both cases you should use the DeepVariant_unfiltered preset for GLnexus, because there is family structure present which the other filtering presets wouldn't know about. Because you can joint genotype multiple trio gVCFs from either DeepVariant or DeepTrio in the same way, I would use DeepTrio to produce the gVCFs, take all of the gVCFs and run them together through glnexus, and then you can still use allele frequency information. To be clear, the unfiltered preset looks like this:. ```. sudo docker run \. -v ""${PWD}/output"":""/output"" \. quay.io/mlin/glnexus:v1.2.7 \. /usr/local/bin/glnexus_cli \. --config DeepVariant_unfiltered \. /output/child_trio_1.g.vcf.gz\. /output/parent1_trio_1.g.vcf.gz \. /output/parent2_trio_1.g.vcf.gz \. /output/child_trio_2g.vcf.gz\. /output/parent1_trio_2.g.vcf.gz \. /output/parent2_trio_2.g.vcf.gz \. | sudo docker run -i google/deepvariant:deeptrio-""${BIN_VERSION}"" \. bcftools view - \. | sudo docker run -i google/deepvariant:deeptrio-""${BIN_VERSION}"" \. bgzip -c > output/trio_cohort_merged.vcf.gz. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/816
https://github.com/google/deepvariant/issues/816:71,energy efficiency,model,models,71,"Hi @AndrewCarroll ,..sorry for the late reply, but aren't the inherent models different for DeepTrio and DeepVariant?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/816
https://github.com/google/deepvariant/issues/816:71,security,model,models,71,"Hi @AndrewCarroll ,..sorry for the late reply, but aren't the inherent models different for DeepTrio and DeepVariant?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/816
https://github.com/google/deepvariant/issues/816:44,energy efficiency,model,models,44,"Hi @prasundutta87 . Yes, they are different models. You would presumably want to use either one or the other (all DeepVariant or all DeepTrio). You can merge either with glnexus though.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/816
https://github.com/google/deepvariant/issues/816:44,security,model,models,44,"Hi @prasundutta87 . Yes, they are different models. You would presumably want to use either one or the other (all DeepVariant or all DeepTrio). You can merge either with glnexus though.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/816
https://github.com/google/deepvariant/issues/816:1239,availability,error,error,1239,"Hi @pichuan,. I tried to use `postprocess_variants_child_extra_args, postprocess_variants_parent1_extra_args, postprocess_variants_parent2_extra_args.` with deeptrio by running the docker: docker://google/deepvariant:deeptrio-1.6.1. here's my example code where parent1 is the father in the trio. ```. /opt/deepvariant/bin/deeptrio/run_deeptrio \. --model_type ""WGS"" \. --ref {params.ref_genome} \. --reads_child {input.child_cram} \. --reads_parent1 {input.dad_cram} \. --reads_parent2 {input.mom_cram} \. --output_vcf_child {output}/{params.child_name}.vcf.gz \. --output_vcf_parent1 {output}/{params.dad_name}.vcf.gz \. --output_vcf_parent2 {output}/{params.mom_name}.vcf.gz \. --sample_name_child {params.child_name} \. --sample_name_parent1 {params.dad_name} \. --sample_name_parent2 {params.mom_name} \. --num_shards {threads} \. --intermediate_results_dir deeptrio_tmp/{wildcards.family} \. --postprocess_variants_parent1_extra_args=""--haploid_contigs=""chrX,chrY"",--par_regions_bed={input.PAR}"" \. --output_gvcf_child {output}/{params.child_name}.g.vcf.gz \. --output_gvcf_parent1 {output}/{params.dad_name}.g.vcf.gz \. --output_gvcf_parent2 {output}/{params.mom_name}.g.vcf.gz \. --novcf_stats_report. ```. but I got the following error:. ```. FATAL Flags parsing error: Unknown command line flag 'postprocess_variants_parent1_extra_args'. Did you mean: postprocess_variants_extra_args ? Pass --helpshort or --helpfull to see help on flags. ```. checking --helpfull, it seems only --postprocess_variants_extra_args is available. Did I misunderstand the use of postprocess_variants_parent1_extra_args? Thank you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/816
https://github.com/google/deepvariant/issues/816:1272,availability,error,error,1272,"Hi @pichuan,. I tried to use `postprocess_variants_child_extra_args, postprocess_variants_parent1_extra_args, postprocess_variants_parent2_extra_args.` with deeptrio by running the docker: docker://google/deepvariant:deeptrio-1.6.1. here's my example code where parent1 is the father in the trio. ```. /opt/deepvariant/bin/deeptrio/run_deeptrio \. --model_type ""WGS"" \. --ref {params.ref_genome} \. --reads_child {input.child_cram} \. --reads_parent1 {input.dad_cram} \. --reads_parent2 {input.mom_cram} \. --output_vcf_child {output}/{params.child_name}.vcf.gz \. --output_vcf_parent1 {output}/{params.dad_name}.vcf.gz \. --output_vcf_parent2 {output}/{params.mom_name}.vcf.gz \. --sample_name_child {params.child_name} \. --sample_name_parent1 {params.dad_name} \. --sample_name_parent2 {params.mom_name} \. --num_shards {threads} \. --intermediate_results_dir deeptrio_tmp/{wildcards.family} \. --postprocess_variants_parent1_extra_args=""--haploid_contigs=""chrX,chrY"",--par_regions_bed={input.PAR}"" \. --output_gvcf_child {output}/{params.child_name}.g.vcf.gz \. --output_gvcf_parent1 {output}/{params.dad_name}.g.vcf.gz \. --output_gvcf_parent2 {output}/{params.mom_name}.g.vcf.gz \. --novcf_stats_report. ```. but I got the following error:. ```. FATAL Flags parsing error: Unknown command line flag 'postprocess_variants_parent1_extra_args'. Did you mean: postprocess_variants_extra_args ? Pass --helpshort or --helpfull to see help on flags. ```. checking --helpfull, it seems only --postprocess_variants_extra_args is available. Did I misunderstand the use of postprocess_variants_parent1_extra_args? Thank you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/816
https://github.com/google/deepvariant/issues/816:1526,availability,avail,available,1526,"Hi @pichuan,. I tried to use `postprocess_variants_child_extra_args, postprocess_variants_parent1_extra_args, postprocess_variants_parent2_extra_args.` with deeptrio by running the docker: docker://google/deepvariant:deeptrio-1.6.1. here's my example code where parent1 is the father in the trio. ```. /opt/deepvariant/bin/deeptrio/run_deeptrio \. --model_type ""WGS"" \. --ref {params.ref_genome} \. --reads_child {input.child_cram} \. --reads_parent1 {input.dad_cram} \. --reads_parent2 {input.mom_cram} \. --output_vcf_child {output}/{params.child_name}.vcf.gz \. --output_vcf_parent1 {output}/{params.dad_name}.vcf.gz \. --output_vcf_parent2 {output}/{params.mom_name}.vcf.gz \. --sample_name_child {params.child_name} \. --sample_name_parent1 {params.dad_name} \. --sample_name_parent2 {params.mom_name} \. --num_shards {threads} \. --intermediate_results_dir deeptrio_tmp/{wildcards.family} \. --postprocess_variants_parent1_extra_args=""--haploid_contigs=""chrX,chrY"",--par_regions_bed={input.PAR}"" \. --output_gvcf_child {output}/{params.child_name}.g.vcf.gz \. --output_gvcf_parent1 {output}/{params.dad_name}.g.vcf.gz \. --output_gvcf_parent2 {output}/{params.mom_name}.g.vcf.gz \. --novcf_stats_report. ```. but I got the following error:. ```. FATAL Flags parsing error: Unknown command line flag 'postprocess_variants_parent1_extra_args'. Did you mean: postprocess_variants_extra_args ? Pass --helpshort or --helpfull to see help on flags. ```. checking --helpfull, it seems only --postprocess_variants_extra_args is available. Did I misunderstand the use of postprocess_variants_parent1_extra_args? Thank you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/816
https://github.com/google/deepvariant/issues/816:1239,performance,error,error,1239,"Hi @pichuan,. I tried to use `postprocess_variants_child_extra_args, postprocess_variants_parent1_extra_args, postprocess_variants_parent2_extra_args.` with deeptrio by running the docker: docker://google/deepvariant:deeptrio-1.6.1. here's my example code where parent1 is the father in the trio. ```. /opt/deepvariant/bin/deeptrio/run_deeptrio \. --model_type ""WGS"" \. --ref {params.ref_genome} \. --reads_child {input.child_cram} \. --reads_parent1 {input.dad_cram} \. --reads_parent2 {input.mom_cram} \. --output_vcf_child {output}/{params.child_name}.vcf.gz \. --output_vcf_parent1 {output}/{params.dad_name}.vcf.gz \. --output_vcf_parent2 {output}/{params.mom_name}.vcf.gz \. --sample_name_child {params.child_name} \. --sample_name_parent1 {params.dad_name} \. --sample_name_parent2 {params.mom_name} \. --num_shards {threads} \. --intermediate_results_dir deeptrio_tmp/{wildcards.family} \. --postprocess_variants_parent1_extra_args=""--haploid_contigs=""chrX,chrY"",--par_regions_bed={input.PAR}"" \. --output_gvcf_child {output}/{params.child_name}.g.vcf.gz \. --output_gvcf_parent1 {output}/{params.dad_name}.g.vcf.gz \. --output_gvcf_parent2 {output}/{params.mom_name}.g.vcf.gz \. --novcf_stats_report. ```. but I got the following error:. ```. FATAL Flags parsing error: Unknown command line flag 'postprocess_variants_parent1_extra_args'. Did you mean: postprocess_variants_extra_args ? Pass --helpshort or --helpfull to see help on flags. ```. checking --helpfull, it seems only --postprocess_variants_extra_args is available. Did I misunderstand the use of postprocess_variants_parent1_extra_args? Thank you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/816
https://github.com/google/deepvariant/issues/816:1272,performance,error,error,1272,"Hi @pichuan,. I tried to use `postprocess_variants_child_extra_args, postprocess_variants_parent1_extra_args, postprocess_variants_parent2_extra_args.` with deeptrio by running the docker: docker://google/deepvariant:deeptrio-1.6.1. here's my example code where parent1 is the father in the trio. ```. /opt/deepvariant/bin/deeptrio/run_deeptrio \. --model_type ""WGS"" \. --ref {params.ref_genome} \. --reads_child {input.child_cram} \. --reads_parent1 {input.dad_cram} \. --reads_parent2 {input.mom_cram} \. --output_vcf_child {output}/{params.child_name}.vcf.gz \. --output_vcf_parent1 {output}/{params.dad_name}.vcf.gz \. --output_vcf_parent2 {output}/{params.mom_name}.vcf.gz \. --sample_name_child {params.child_name} \. --sample_name_parent1 {params.dad_name} \. --sample_name_parent2 {params.mom_name} \. --num_shards {threads} \. --intermediate_results_dir deeptrio_tmp/{wildcards.family} \. --postprocess_variants_parent1_extra_args=""--haploid_contigs=""chrX,chrY"",--par_regions_bed={input.PAR}"" \. --output_gvcf_child {output}/{params.child_name}.g.vcf.gz \. --output_gvcf_parent1 {output}/{params.dad_name}.g.vcf.gz \. --output_gvcf_parent2 {output}/{params.mom_name}.g.vcf.gz \. --novcf_stats_report. ```. but I got the following error:. ```. FATAL Flags parsing error: Unknown command line flag 'postprocess_variants_parent1_extra_args'. Did you mean: postprocess_variants_extra_args ? Pass --helpshort or --helpfull to see help on flags. ```. checking --helpfull, it seems only --postprocess_variants_extra_args is available. Did I misunderstand the use of postprocess_variants_parent1_extra_args? Thank you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/816
https://github.com/google/deepvariant/issues/816:1526,reliability,availab,available,1526,"Hi @pichuan,. I tried to use `postprocess_variants_child_extra_args, postprocess_variants_parent1_extra_args, postprocess_variants_parent2_extra_args.` with deeptrio by running the docker: docker://google/deepvariant:deeptrio-1.6.1. here's my example code where parent1 is the father in the trio. ```. /opt/deepvariant/bin/deeptrio/run_deeptrio \. --model_type ""WGS"" \. --ref {params.ref_genome} \. --reads_child {input.child_cram} \. --reads_parent1 {input.dad_cram} \. --reads_parent2 {input.mom_cram} \. --output_vcf_child {output}/{params.child_name}.vcf.gz \. --output_vcf_parent1 {output}/{params.dad_name}.vcf.gz \. --output_vcf_parent2 {output}/{params.mom_name}.vcf.gz \. --sample_name_child {params.child_name} \. --sample_name_parent1 {params.dad_name} \. --sample_name_parent2 {params.mom_name} \. --num_shards {threads} \. --intermediate_results_dir deeptrio_tmp/{wildcards.family} \. --postprocess_variants_parent1_extra_args=""--haploid_contigs=""chrX,chrY"",--par_regions_bed={input.PAR}"" \. --output_gvcf_child {output}/{params.child_name}.g.vcf.gz \. --output_gvcf_parent1 {output}/{params.dad_name}.g.vcf.gz \. --output_gvcf_parent2 {output}/{params.mom_name}.g.vcf.gz \. --novcf_stats_report. ```. but I got the following error:. ```. FATAL Flags parsing error: Unknown command line flag 'postprocess_variants_parent1_extra_args'. Did you mean: postprocess_variants_extra_args ? Pass --helpshort or --helpfull to see help on flags. ```. checking --helpfull, it seems only --postprocess_variants_extra_args is available. Did I misunderstand the use of postprocess_variants_parent1_extra_args? Thank you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/816
https://github.com/google/deepvariant/issues/816:414,safety,input,input,414,"Hi @pichuan,. I tried to use `postprocess_variants_child_extra_args, postprocess_variants_parent1_extra_args, postprocess_variants_parent2_extra_args.` with deeptrio by running the docker: docker://google/deepvariant:deeptrio-1.6.1. here's my example code where parent1 is the father in the trio. ```. /opt/deepvariant/bin/deeptrio/run_deeptrio \. --model_type ""WGS"" \. --ref {params.ref_genome} \. --reads_child {input.child_cram} \. --reads_parent1 {input.dad_cram} \. --reads_parent2 {input.mom_cram} \. --output_vcf_child {output}/{params.child_name}.vcf.gz \. --output_vcf_parent1 {output}/{params.dad_name}.vcf.gz \. --output_vcf_parent2 {output}/{params.mom_name}.vcf.gz \. --sample_name_child {params.child_name} \. --sample_name_parent1 {params.dad_name} \. --sample_name_parent2 {params.mom_name} \. --num_shards {threads} \. --intermediate_results_dir deeptrio_tmp/{wildcards.family} \. --postprocess_variants_parent1_extra_args=""--haploid_contigs=""chrX,chrY"",--par_regions_bed={input.PAR}"" \. --output_gvcf_child {output}/{params.child_name}.g.vcf.gz \. --output_gvcf_parent1 {output}/{params.dad_name}.g.vcf.gz \. --output_gvcf_parent2 {output}/{params.mom_name}.g.vcf.gz \. --novcf_stats_report. ```. but I got the following error:. ```. FATAL Flags parsing error: Unknown command line flag 'postprocess_variants_parent1_extra_args'. Did you mean: postprocess_variants_extra_args ? Pass --helpshort or --helpfull to see help on flags. ```. checking --helpfull, it seems only --postprocess_variants_extra_args is available. Did I misunderstand the use of postprocess_variants_parent1_extra_args? Thank you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/816
https://github.com/google/deepvariant/issues/816:452,safety,input,input,452,"Hi @pichuan,. I tried to use `postprocess_variants_child_extra_args, postprocess_variants_parent1_extra_args, postprocess_variants_parent2_extra_args.` with deeptrio by running the docker: docker://google/deepvariant:deeptrio-1.6.1. here's my example code where parent1 is the father in the trio. ```. /opt/deepvariant/bin/deeptrio/run_deeptrio \. --model_type ""WGS"" \. --ref {params.ref_genome} \. --reads_child {input.child_cram} \. --reads_parent1 {input.dad_cram} \. --reads_parent2 {input.mom_cram} \. --output_vcf_child {output}/{params.child_name}.vcf.gz \. --output_vcf_parent1 {output}/{params.dad_name}.vcf.gz \. --output_vcf_parent2 {output}/{params.mom_name}.vcf.gz \. --sample_name_child {params.child_name} \. --sample_name_parent1 {params.dad_name} \. --sample_name_parent2 {params.mom_name} \. --num_shards {threads} \. --intermediate_results_dir deeptrio_tmp/{wildcards.family} \. --postprocess_variants_parent1_extra_args=""--haploid_contigs=""chrX,chrY"",--par_regions_bed={input.PAR}"" \. --output_gvcf_child {output}/{params.child_name}.g.vcf.gz \. --output_gvcf_parent1 {output}/{params.dad_name}.g.vcf.gz \. --output_gvcf_parent2 {output}/{params.mom_name}.g.vcf.gz \. --novcf_stats_report. ```. but I got the following error:. ```. FATAL Flags parsing error: Unknown command line flag 'postprocess_variants_parent1_extra_args'. Did you mean: postprocess_variants_extra_args ? Pass --helpshort or --helpfull to see help on flags. ```. checking --helpfull, it seems only --postprocess_variants_extra_args is available. Did I misunderstand the use of postprocess_variants_parent1_extra_args? Thank you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/816
https://github.com/google/deepvariant/issues/816:488,safety,input,input,488,"Hi @pichuan,. I tried to use `postprocess_variants_child_extra_args, postprocess_variants_parent1_extra_args, postprocess_variants_parent2_extra_args.` with deeptrio by running the docker: docker://google/deepvariant:deeptrio-1.6.1. here's my example code where parent1 is the father in the trio. ```. /opt/deepvariant/bin/deeptrio/run_deeptrio \. --model_type ""WGS"" \. --ref {params.ref_genome} \. --reads_child {input.child_cram} \. --reads_parent1 {input.dad_cram} \. --reads_parent2 {input.mom_cram} \. --output_vcf_child {output}/{params.child_name}.vcf.gz \. --output_vcf_parent1 {output}/{params.dad_name}.vcf.gz \. --output_vcf_parent2 {output}/{params.mom_name}.vcf.gz \. --sample_name_child {params.child_name} \. --sample_name_parent1 {params.dad_name} \. --sample_name_parent2 {params.mom_name} \. --num_shards {threads} \. --intermediate_results_dir deeptrio_tmp/{wildcards.family} \. --postprocess_variants_parent1_extra_args=""--haploid_contigs=""chrX,chrY"",--par_regions_bed={input.PAR}"" \. --output_gvcf_child {output}/{params.child_name}.g.vcf.gz \. --output_gvcf_parent1 {output}/{params.dad_name}.g.vcf.gz \. --output_gvcf_parent2 {output}/{params.mom_name}.g.vcf.gz \. --novcf_stats_report. ```. but I got the following error:. ```. FATAL Flags parsing error: Unknown command line flag 'postprocess_variants_parent1_extra_args'. Did you mean: postprocess_variants_extra_args ? Pass --helpshort or --helpfull to see help on flags. ```. checking --helpfull, it seems only --postprocess_variants_extra_args is available. Did I misunderstand the use of postprocess_variants_parent1_extra_args? Thank you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/816
https://github.com/google/deepvariant/issues/816:990,safety,input,input,990,"Hi @pichuan,. I tried to use `postprocess_variants_child_extra_args, postprocess_variants_parent1_extra_args, postprocess_variants_parent2_extra_args.` with deeptrio by running the docker: docker://google/deepvariant:deeptrio-1.6.1. here's my example code where parent1 is the father in the trio. ```. /opt/deepvariant/bin/deeptrio/run_deeptrio \. --model_type ""WGS"" \. --ref {params.ref_genome} \. --reads_child {input.child_cram} \. --reads_parent1 {input.dad_cram} \. --reads_parent2 {input.mom_cram} \. --output_vcf_child {output}/{params.child_name}.vcf.gz \. --output_vcf_parent1 {output}/{params.dad_name}.vcf.gz \. --output_vcf_parent2 {output}/{params.mom_name}.vcf.gz \. --sample_name_child {params.child_name} \. --sample_name_parent1 {params.dad_name} \. --sample_name_parent2 {params.mom_name} \. --num_shards {threads} \. --intermediate_results_dir deeptrio_tmp/{wildcards.family} \. --postprocess_variants_parent1_extra_args=""--haploid_contigs=""chrX,chrY"",--par_regions_bed={input.PAR}"" \. --output_gvcf_child {output}/{params.child_name}.g.vcf.gz \. --output_gvcf_parent1 {output}/{params.dad_name}.g.vcf.gz \. --output_gvcf_parent2 {output}/{params.mom_name}.g.vcf.gz \. --novcf_stats_report. ```. but I got the following error:. ```. FATAL Flags parsing error: Unknown command line flag 'postprocess_variants_parent1_extra_args'. Did you mean: postprocess_variants_extra_args ? Pass --helpshort or --helpfull to see help on flags. ```. checking --helpfull, it seems only --postprocess_variants_extra_args is available. Did I misunderstand the use of postprocess_variants_parent1_extra_args? Thank you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/816
https://github.com/google/deepvariant/issues/816:1239,safety,error,error,1239,"Hi @pichuan,. I tried to use `postprocess_variants_child_extra_args, postprocess_variants_parent1_extra_args, postprocess_variants_parent2_extra_args.` with deeptrio by running the docker: docker://google/deepvariant:deeptrio-1.6.1. here's my example code where parent1 is the father in the trio. ```. /opt/deepvariant/bin/deeptrio/run_deeptrio \. --model_type ""WGS"" \. --ref {params.ref_genome} \. --reads_child {input.child_cram} \. --reads_parent1 {input.dad_cram} \. --reads_parent2 {input.mom_cram} \. --output_vcf_child {output}/{params.child_name}.vcf.gz \. --output_vcf_parent1 {output}/{params.dad_name}.vcf.gz \. --output_vcf_parent2 {output}/{params.mom_name}.vcf.gz \. --sample_name_child {params.child_name} \. --sample_name_parent1 {params.dad_name} \. --sample_name_parent2 {params.mom_name} \. --num_shards {threads} \. --intermediate_results_dir deeptrio_tmp/{wildcards.family} \. --postprocess_variants_parent1_extra_args=""--haploid_contigs=""chrX,chrY"",--par_regions_bed={input.PAR}"" \. --output_gvcf_child {output}/{params.child_name}.g.vcf.gz \. --output_gvcf_parent1 {output}/{params.dad_name}.g.vcf.gz \. --output_gvcf_parent2 {output}/{params.mom_name}.g.vcf.gz \. --novcf_stats_report. ```. but I got the following error:. ```. FATAL Flags parsing error: Unknown command line flag 'postprocess_variants_parent1_extra_args'. Did you mean: postprocess_variants_extra_args ? Pass --helpshort or --helpfull to see help on flags. ```. checking --helpfull, it seems only --postprocess_variants_extra_args is available. Did I misunderstand the use of postprocess_variants_parent1_extra_args? Thank you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/816
https://github.com/google/deepvariant/issues/816:1272,safety,error,error,1272,"Hi @pichuan,. I tried to use `postprocess_variants_child_extra_args, postprocess_variants_parent1_extra_args, postprocess_variants_parent2_extra_args.` with deeptrio by running the docker: docker://google/deepvariant:deeptrio-1.6.1. here's my example code where parent1 is the father in the trio. ```. /opt/deepvariant/bin/deeptrio/run_deeptrio \. --model_type ""WGS"" \. --ref {params.ref_genome} \. --reads_child {input.child_cram} \. --reads_parent1 {input.dad_cram} \. --reads_parent2 {input.mom_cram} \. --output_vcf_child {output}/{params.child_name}.vcf.gz \. --output_vcf_parent1 {output}/{params.dad_name}.vcf.gz \. --output_vcf_parent2 {output}/{params.mom_name}.vcf.gz \. --sample_name_child {params.child_name} \. --sample_name_parent1 {params.dad_name} \. --sample_name_parent2 {params.mom_name} \. --num_shards {threads} \. --intermediate_results_dir deeptrio_tmp/{wildcards.family} \. --postprocess_variants_parent1_extra_args=""--haploid_contigs=""chrX,chrY"",--par_regions_bed={input.PAR}"" \. --output_gvcf_child {output}/{params.child_name}.g.vcf.gz \. --output_gvcf_parent1 {output}/{params.dad_name}.g.vcf.gz \. --output_gvcf_parent2 {output}/{params.mom_name}.g.vcf.gz \. --novcf_stats_report. ```. but I got the following error:. ```. FATAL Flags parsing error: Unknown command line flag 'postprocess_variants_parent1_extra_args'. Did you mean: postprocess_variants_extra_args ? Pass --helpshort or --helpfull to see help on flags. ```. checking --helpfull, it seems only --postprocess_variants_extra_args is available. Did I misunderstand the use of postprocess_variants_parent1_extra_args? Thank you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/816
https://github.com/google/deepvariant/issues/816:1526,safety,avail,available,1526,"Hi @pichuan,. I tried to use `postprocess_variants_child_extra_args, postprocess_variants_parent1_extra_args, postprocess_variants_parent2_extra_args.` with deeptrio by running the docker: docker://google/deepvariant:deeptrio-1.6.1. here's my example code where parent1 is the father in the trio. ```. /opt/deepvariant/bin/deeptrio/run_deeptrio \. --model_type ""WGS"" \. --ref {params.ref_genome} \. --reads_child {input.child_cram} \. --reads_parent1 {input.dad_cram} \. --reads_parent2 {input.mom_cram} \. --output_vcf_child {output}/{params.child_name}.vcf.gz \. --output_vcf_parent1 {output}/{params.dad_name}.vcf.gz \. --output_vcf_parent2 {output}/{params.mom_name}.vcf.gz \. --sample_name_child {params.child_name} \. --sample_name_parent1 {params.dad_name} \. --sample_name_parent2 {params.mom_name} \. --num_shards {threads} \. --intermediate_results_dir deeptrio_tmp/{wildcards.family} \. --postprocess_variants_parent1_extra_args=""--haploid_contigs=""chrX,chrY"",--par_regions_bed={input.PAR}"" \. --output_gvcf_child {output}/{params.child_name}.g.vcf.gz \. --output_gvcf_parent1 {output}/{params.dad_name}.g.vcf.gz \. --output_gvcf_parent2 {output}/{params.mom_name}.g.vcf.gz \. --novcf_stats_report. ```. but I got the following error:. ```. FATAL Flags parsing error: Unknown command line flag 'postprocess_variants_parent1_extra_args'. Did you mean: postprocess_variants_extra_args ? Pass --helpshort or --helpfull to see help on flags. ```. checking --helpfull, it seems only --postprocess_variants_extra_args is available. Did I misunderstand the use of postprocess_variants_parent1_extra_args? Thank you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/816
https://github.com/google/deepvariant/issues/816:1526,security,availab,available,1526,"Hi @pichuan,. I tried to use `postprocess_variants_child_extra_args, postprocess_variants_parent1_extra_args, postprocess_variants_parent2_extra_args.` with deeptrio by running the docker: docker://google/deepvariant:deeptrio-1.6.1. here's my example code where parent1 is the father in the trio. ```. /opt/deepvariant/bin/deeptrio/run_deeptrio \. --model_type ""WGS"" \. --ref {params.ref_genome} \. --reads_child {input.child_cram} \. --reads_parent1 {input.dad_cram} \. --reads_parent2 {input.mom_cram} \. --output_vcf_child {output}/{params.child_name}.vcf.gz \. --output_vcf_parent1 {output}/{params.dad_name}.vcf.gz \. --output_vcf_parent2 {output}/{params.mom_name}.vcf.gz \. --sample_name_child {params.child_name} \. --sample_name_parent1 {params.dad_name} \. --sample_name_parent2 {params.mom_name} \. --num_shards {threads} \. --intermediate_results_dir deeptrio_tmp/{wildcards.family} \. --postprocess_variants_parent1_extra_args=""--haploid_contigs=""chrX,chrY"",--par_regions_bed={input.PAR}"" \. --output_gvcf_child {output}/{params.child_name}.g.vcf.gz \. --output_gvcf_parent1 {output}/{params.dad_name}.g.vcf.gz \. --output_gvcf_parent2 {output}/{params.mom_name}.g.vcf.gz \. --novcf_stats_report. ```. but I got the following error:. ```. FATAL Flags parsing error: Unknown command line flag 'postprocess_variants_parent1_extra_args'. Did you mean: postprocess_variants_extra_args ? Pass --helpshort or --helpfull to see help on flags. ```. checking --helpfull, it seems only --postprocess_variants_extra_args is available. Did I misunderstand the use of postprocess_variants_parent1_extra_args? Thank you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/816
https://github.com/google/deepvariant/issues/816:414,usability,input,input,414,"Hi @pichuan,. I tried to use `postprocess_variants_child_extra_args, postprocess_variants_parent1_extra_args, postprocess_variants_parent2_extra_args.` with deeptrio by running the docker: docker://google/deepvariant:deeptrio-1.6.1. here's my example code where parent1 is the father in the trio. ```. /opt/deepvariant/bin/deeptrio/run_deeptrio \. --model_type ""WGS"" \. --ref {params.ref_genome} \. --reads_child {input.child_cram} \. --reads_parent1 {input.dad_cram} \. --reads_parent2 {input.mom_cram} \. --output_vcf_child {output}/{params.child_name}.vcf.gz \. --output_vcf_parent1 {output}/{params.dad_name}.vcf.gz \. --output_vcf_parent2 {output}/{params.mom_name}.vcf.gz \. --sample_name_child {params.child_name} \. --sample_name_parent1 {params.dad_name} \. --sample_name_parent2 {params.mom_name} \. --num_shards {threads} \. --intermediate_results_dir deeptrio_tmp/{wildcards.family} \. --postprocess_variants_parent1_extra_args=""--haploid_contigs=""chrX,chrY"",--par_regions_bed={input.PAR}"" \. --output_gvcf_child {output}/{params.child_name}.g.vcf.gz \. --output_gvcf_parent1 {output}/{params.dad_name}.g.vcf.gz \. --output_gvcf_parent2 {output}/{params.mom_name}.g.vcf.gz \. --novcf_stats_report. ```. but I got the following error:. ```. FATAL Flags parsing error: Unknown command line flag 'postprocess_variants_parent1_extra_args'. Did you mean: postprocess_variants_extra_args ? Pass --helpshort or --helpfull to see help on flags. ```. checking --helpfull, it seems only --postprocess_variants_extra_args is available. Did I misunderstand the use of postprocess_variants_parent1_extra_args? Thank you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/816
https://github.com/google/deepvariant/issues/816:452,usability,input,input,452,"Hi @pichuan,. I tried to use `postprocess_variants_child_extra_args, postprocess_variants_parent1_extra_args, postprocess_variants_parent2_extra_args.` with deeptrio by running the docker: docker://google/deepvariant:deeptrio-1.6.1. here's my example code where parent1 is the father in the trio. ```. /opt/deepvariant/bin/deeptrio/run_deeptrio \. --model_type ""WGS"" \. --ref {params.ref_genome} \. --reads_child {input.child_cram} \. --reads_parent1 {input.dad_cram} \. --reads_parent2 {input.mom_cram} \. --output_vcf_child {output}/{params.child_name}.vcf.gz \. --output_vcf_parent1 {output}/{params.dad_name}.vcf.gz \. --output_vcf_parent2 {output}/{params.mom_name}.vcf.gz \. --sample_name_child {params.child_name} \. --sample_name_parent1 {params.dad_name} \. --sample_name_parent2 {params.mom_name} \. --num_shards {threads} \. --intermediate_results_dir deeptrio_tmp/{wildcards.family} \. --postprocess_variants_parent1_extra_args=""--haploid_contigs=""chrX,chrY"",--par_regions_bed={input.PAR}"" \. --output_gvcf_child {output}/{params.child_name}.g.vcf.gz \. --output_gvcf_parent1 {output}/{params.dad_name}.g.vcf.gz \. --output_gvcf_parent2 {output}/{params.mom_name}.g.vcf.gz \. --novcf_stats_report. ```. but I got the following error:. ```. FATAL Flags parsing error: Unknown command line flag 'postprocess_variants_parent1_extra_args'. Did you mean: postprocess_variants_extra_args ? Pass --helpshort or --helpfull to see help on flags. ```. checking --helpfull, it seems only --postprocess_variants_extra_args is available. Did I misunderstand the use of postprocess_variants_parent1_extra_args? Thank you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/816
https://github.com/google/deepvariant/issues/816:488,usability,input,input,488,"Hi @pichuan,. I tried to use `postprocess_variants_child_extra_args, postprocess_variants_parent1_extra_args, postprocess_variants_parent2_extra_args.` with deeptrio by running the docker: docker://google/deepvariant:deeptrio-1.6.1. here's my example code where parent1 is the father in the trio. ```. /opt/deepvariant/bin/deeptrio/run_deeptrio \. --model_type ""WGS"" \. --ref {params.ref_genome} \. --reads_child {input.child_cram} \. --reads_parent1 {input.dad_cram} \. --reads_parent2 {input.mom_cram} \. --output_vcf_child {output}/{params.child_name}.vcf.gz \. --output_vcf_parent1 {output}/{params.dad_name}.vcf.gz \. --output_vcf_parent2 {output}/{params.mom_name}.vcf.gz \. --sample_name_child {params.child_name} \. --sample_name_parent1 {params.dad_name} \. --sample_name_parent2 {params.mom_name} \. --num_shards {threads} \. --intermediate_results_dir deeptrio_tmp/{wildcards.family} \. --postprocess_variants_parent1_extra_args=""--haploid_contigs=""chrX,chrY"",--par_regions_bed={input.PAR}"" \. --output_gvcf_child {output}/{params.child_name}.g.vcf.gz \. --output_gvcf_parent1 {output}/{params.dad_name}.g.vcf.gz \. --output_gvcf_parent2 {output}/{params.mom_name}.g.vcf.gz \. --novcf_stats_report. ```. but I got the following error:. ```. FATAL Flags parsing error: Unknown command line flag 'postprocess_variants_parent1_extra_args'. Did you mean: postprocess_variants_extra_args ? Pass --helpshort or --helpfull to see help on flags. ```. checking --helpfull, it seems only --postprocess_variants_extra_args is available. Did I misunderstand the use of postprocess_variants_parent1_extra_args? Thank you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/816
https://github.com/google/deepvariant/issues/816:990,usability,input,input,990,"Hi @pichuan,. I tried to use `postprocess_variants_child_extra_args, postprocess_variants_parent1_extra_args, postprocess_variants_parent2_extra_args.` with deeptrio by running the docker: docker://google/deepvariant:deeptrio-1.6.1. here's my example code where parent1 is the father in the trio. ```. /opt/deepvariant/bin/deeptrio/run_deeptrio \. --model_type ""WGS"" \. --ref {params.ref_genome} \. --reads_child {input.child_cram} \. --reads_parent1 {input.dad_cram} \. --reads_parent2 {input.mom_cram} \. --output_vcf_child {output}/{params.child_name}.vcf.gz \. --output_vcf_parent1 {output}/{params.dad_name}.vcf.gz \. --output_vcf_parent2 {output}/{params.mom_name}.vcf.gz \. --sample_name_child {params.child_name} \. --sample_name_parent1 {params.dad_name} \. --sample_name_parent2 {params.mom_name} \. --num_shards {threads} \. --intermediate_results_dir deeptrio_tmp/{wildcards.family} \. --postprocess_variants_parent1_extra_args=""--haploid_contigs=""chrX,chrY"",--par_regions_bed={input.PAR}"" \. --output_gvcf_child {output}/{params.child_name}.g.vcf.gz \. --output_gvcf_parent1 {output}/{params.dad_name}.g.vcf.gz \. --output_gvcf_parent2 {output}/{params.mom_name}.g.vcf.gz \. --novcf_stats_report. ```. but I got the following error:. ```. FATAL Flags parsing error: Unknown command line flag 'postprocess_variants_parent1_extra_args'. Did you mean: postprocess_variants_extra_args ? Pass --helpshort or --helpfull to see help on flags. ```. checking --helpfull, it seems only --postprocess_variants_extra_args is available. Did I misunderstand the use of postprocess_variants_parent1_extra_args? Thank you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/816
https://github.com/google/deepvariant/issues/816:1239,usability,error,error,1239,"Hi @pichuan,. I tried to use `postprocess_variants_child_extra_args, postprocess_variants_parent1_extra_args, postprocess_variants_parent2_extra_args.` with deeptrio by running the docker: docker://google/deepvariant:deeptrio-1.6.1. here's my example code where parent1 is the father in the trio. ```. /opt/deepvariant/bin/deeptrio/run_deeptrio \. --model_type ""WGS"" \. --ref {params.ref_genome} \. --reads_child {input.child_cram} \. --reads_parent1 {input.dad_cram} \. --reads_parent2 {input.mom_cram} \. --output_vcf_child {output}/{params.child_name}.vcf.gz \. --output_vcf_parent1 {output}/{params.dad_name}.vcf.gz \. --output_vcf_parent2 {output}/{params.mom_name}.vcf.gz \. --sample_name_child {params.child_name} \. --sample_name_parent1 {params.dad_name} \. --sample_name_parent2 {params.mom_name} \. --num_shards {threads} \. --intermediate_results_dir deeptrio_tmp/{wildcards.family} \. --postprocess_variants_parent1_extra_args=""--haploid_contigs=""chrX,chrY"",--par_regions_bed={input.PAR}"" \. --output_gvcf_child {output}/{params.child_name}.g.vcf.gz \. --output_gvcf_parent1 {output}/{params.dad_name}.g.vcf.gz \. --output_gvcf_parent2 {output}/{params.mom_name}.g.vcf.gz \. --novcf_stats_report. ```. but I got the following error:. ```. FATAL Flags parsing error: Unknown command line flag 'postprocess_variants_parent1_extra_args'. Did you mean: postprocess_variants_extra_args ? Pass --helpshort or --helpfull to see help on flags. ```. checking --helpfull, it seems only --postprocess_variants_extra_args is available. Did I misunderstand the use of postprocess_variants_parent1_extra_args? Thank you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/816
https://github.com/google/deepvariant/issues/816:1272,usability,error,error,1272,"Hi @pichuan,. I tried to use `postprocess_variants_child_extra_args, postprocess_variants_parent1_extra_args, postprocess_variants_parent2_extra_args.` with deeptrio by running the docker: docker://google/deepvariant:deeptrio-1.6.1. here's my example code where parent1 is the father in the trio. ```. /opt/deepvariant/bin/deeptrio/run_deeptrio \. --model_type ""WGS"" \. --ref {params.ref_genome} \. --reads_child {input.child_cram} \. --reads_parent1 {input.dad_cram} \. --reads_parent2 {input.mom_cram} \. --output_vcf_child {output}/{params.child_name}.vcf.gz \. --output_vcf_parent1 {output}/{params.dad_name}.vcf.gz \. --output_vcf_parent2 {output}/{params.mom_name}.vcf.gz \. --sample_name_child {params.child_name} \. --sample_name_parent1 {params.dad_name} \. --sample_name_parent2 {params.mom_name} \. --num_shards {threads} \. --intermediate_results_dir deeptrio_tmp/{wildcards.family} \. --postprocess_variants_parent1_extra_args=""--haploid_contigs=""chrX,chrY"",--par_regions_bed={input.PAR}"" \. --output_gvcf_child {output}/{params.child_name}.g.vcf.gz \. --output_gvcf_parent1 {output}/{params.dad_name}.g.vcf.gz \. --output_gvcf_parent2 {output}/{params.mom_name}.g.vcf.gz \. --novcf_stats_report. ```. but I got the following error:. ```. FATAL Flags parsing error: Unknown command line flag 'postprocess_variants_parent1_extra_args'. Did you mean: postprocess_variants_extra_args ? Pass --helpshort or --helpfull to see help on flags. ```. checking --helpfull, it seems only --postprocess_variants_extra_args is available. Did I misunderstand the use of postprocess_variants_parent1_extra_args? Thank you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/816
https://github.com/google/deepvariant/issues/816:1287,usability,command,command,1287,"Hi @pichuan,. I tried to use `postprocess_variants_child_extra_args, postprocess_variants_parent1_extra_args, postprocess_variants_parent2_extra_args.` with deeptrio by running the docker: docker://google/deepvariant:deeptrio-1.6.1. here's my example code where parent1 is the father in the trio. ```. /opt/deepvariant/bin/deeptrio/run_deeptrio \. --model_type ""WGS"" \. --ref {params.ref_genome} \. --reads_child {input.child_cram} \. --reads_parent1 {input.dad_cram} \. --reads_parent2 {input.mom_cram} \. --output_vcf_child {output}/{params.child_name}.vcf.gz \. --output_vcf_parent1 {output}/{params.dad_name}.vcf.gz \. --output_vcf_parent2 {output}/{params.mom_name}.vcf.gz \. --sample_name_child {params.child_name} \. --sample_name_parent1 {params.dad_name} \. --sample_name_parent2 {params.mom_name} \. --num_shards {threads} \. --intermediate_results_dir deeptrio_tmp/{wildcards.family} \. --postprocess_variants_parent1_extra_args=""--haploid_contigs=""chrX,chrY"",--par_regions_bed={input.PAR}"" \. --output_gvcf_child {output}/{params.child_name}.g.vcf.gz \. --output_gvcf_parent1 {output}/{params.dad_name}.g.vcf.gz \. --output_gvcf_parent2 {output}/{params.mom_name}.g.vcf.gz \. --novcf_stats_report. ```. but I got the following error:. ```. FATAL Flags parsing error: Unknown command line flag 'postprocess_variants_parent1_extra_args'. Did you mean: postprocess_variants_extra_args ? Pass --helpshort or --helpfull to see help on flags. ```. checking --helpfull, it seems only --postprocess_variants_extra_args is available. Did I misunderstand the use of postprocess_variants_parent1_extra_args? Thank you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/816
https://github.com/google/deepvariant/issues/816:1403,usability,help,helpshort,1403,"Hi @pichuan,. I tried to use `postprocess_variants_child_extra_args, postprocess_variants_parent1_extra_args, postprocess_variants_parent2_extra_args.` with deeptrio by running the docker: docker://google/deepvariant:deeptrio-1.6.1. here's my example code where parent1 is the father in the trio. ```. /opt/deepvariant/bin/deeptrio/run_deeptrio \. --model_type ""WGS"" \. --ref {params.ref_genome} \. --reads_child {input.child_cram} \. --reads_parent1 {input.dad_cram} \. --reads_parent2 {input.mom_cram} \. --output_vcf_child {output}/{params.child_name}.vcf.gz \. --output_vcf_parent1 {output}/{params.dad_name}.vcf.gz \. --output_vcf_parent2 {output}/{params.mom_name}.vcf.gz \. --sample_name_child {params.child_name} \. --sample_name_parent1 {params.dad_name} \. --sample_name_parent2 {params.mom_name} \. --num_shards {threads} \. --intermediate_results_dir deeptrio_tmp/{wildcards.family} \. --postprocess_variants_parent1_extra_args=""--haploid_contigs=""chrX,chrY"",--par_regions_bed={input.PAR}"" \. --output_gvcf_child {output}/{params.child_name}.g.vcf.gz \. --output_gvcf_parent1 {output}/{params.dad_name}.g.vcf.gz \. --output_gvcf_parent2 {output}/{params.mom_name}.g.vcf.gz \. --novcf_stats_report. ```. but I got the following error:. ```. FATAL Flags parsing error: Unknown command line flag 'postprocess_variants_parent1_extra_args'. Did you mean: postprocess_variants_extra_args ? Pass --helpshort or --helpfull to see help on flags. ```. checking --helpfull, it seems only --postprocess_variants_extra_args is available. Did I misunderstand the use of postprocess_variants_parent1_extra_args? Thank you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/816
https://github.com/google/deepvariant/issues/816:1418,usability,help,helpfull,1418,"Hi @pichuan,. I tried to use `postprocess_variants_child_extra_args, postprocess_variants_parent1_extra_args, postprocess_variants_parent2_extra_args.` with deeptrio by running the docker: docker://google/deepvariant:deeptrio-1.6.1. here's my example code where parent1 is the father in the trio. ```. /opt/deepvariant/bin/deeptrio/run_deeptrio \. --model_type ""WGS"" \. --ref {params.ref_genome} \. --reads_child {input.child_cram} \. --reads_parent1 {input.dad_cram} \. --reads_parent2 {input.mom_cram} \. --output_vcf_child {output}/{params.child_name}.vcf.gz \. --output_vcf_parent1 {output}/{params.dad_name}.vcf.gz \. --output_vcf_parent2 {output}/{params.mom_name}.vcf.gz \. --sample_name_child {params.child_name} \. --sample_name_parent1 {params.dad_name} \. --sample_name_parent2 {params.mom_name} \. --num_shards {threads} \. --intermediate_results_dir deeptrio_tmp/{wildcards.family} \. --postprocess_variants_parent1_extra_args=""--haploid_contigs=""chrX,chrY"",--par_regions_bed={input.PAR}"" \. --output_gvcf_child {output}/{params.child_name}.g.vcf.gz \. --output_gvcf_parent1 {output}/{params.dad_name}.g.vcf.gz \. --output_gvcf_parent2 {output}/{params.mom_name}.g.vcf.gz \. --novcf_stats_report. ```. but I got the following error:. ```. FATAL Flags parsing error: Unknown command line flag 'postprocess_variants_parent1_extra_args'. Did you mean: postprocess_variants_extra_args ? Pass --helpshort or --helpfull to see help on flags. ```. checking --helpfull, it seems only --postprocess_variants_extra_args is available. Did I misunderstand the use of postprocess_variants_parent1_extra_args? Thank you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/816
https://github.com/google/deepvariant/issues/816:1434,usability,help,help,1434,"Hi @pichuan,. I tried to use `postprocess_variants_child_extra_args, postprocess_variants_parent1_extra_args, postprocess_variants_parent2_extra_args.` with deeptrio by running the docker: docker://google/deepvariant:deeptrio-1.6.1. here's my example code where parent1 is the father in the trio. ```. /opt/deepvariant/bin/deeptrio/run_deeptrio \. --model_type ""WGS"" \. --ref {params.ref_genome} \. --reads_child {input.child_cram} \. --reads_parent1 {input.dad_cram} \. --reads_parent2 {input.mom_cram} \. --output_vcf_child {output}/{params.child_name}.vcf.gz \. --output_vcf_parent1 {output}/{params.dad_name}.vcf.gz \. --output_vcf_parent2 {output}/{params.mom_name}.vcf.gz \. --sample_name_child {params.child_name} \. --sample_name_parent1 {params.dad_name} \. --sample_name_parent2 {params.mom_name} \. --num_shards {threads} \. --intermediate_results_dir deeptrio_tmp/{wildcards.family} \. --postprocess_variants_parent1_extra_args=""--haploid_contigs=""chrX,chrY"",--par_regions_bed={input.PAR}"" \. --output_gvcf_child {output}/{params.child_name}.g.vcf.gz \. --output_gvcf_parent1 {output}/{params.dad_name}.g.vcf.gz \. --output_gvcf_parent2 {output}/{params.mom_name}.g.vcf.gz \. --novcf_stats_report. ```. but I got the following error:. ```. FATAL Flags parsing error: Unknown command line flag 'postprocess_variants_parent1_extra_args'. Did you mean: postprocess_variants_extra_args ? Pass --helpshort or --helpfull to see help on flags. ```. checking --helpfull, it seems only --postprocess_variants_extra_args is available. Did I misunderstand the use of postprocess_variants_parent1_extra_args? Thank you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/816
https://github.com/google/deepvariant/issues/816:1465,usability,help,helpfull,1465,"Hi @pichuan,. I tried to use `postprocess_variants_child_extra_args, postprocess_variants_parent1_extra_args, postprocess_variants_parent2_extra_args.` with deeptrio by running the docker: docker://google/deepvariant:deeptrio-1.6.1. here's my example code where parent1 is the father in the trio. ```. /opt/deepvariant/bin/deeptrio/run_deeptrio \. --model_type ""WGS"" \. --ref {params.ref_genome} \. --reads_child {input.child_cram} \. --reads_parent1 {input.dad_cram} \. --reads_parent2 {input.mom_cram} \. --output_vcf_child {output}/{params.child_name}.vcf.gz \. --output_vcf_parent1 {output}/{params.dad_name}.vcf.gz \. --output_vcf_parent2 {output}/{params.mom_name}.vcf.gz \. --sample_name_child {params.child_name} \. --sample_name_parent1 {params.dad_name} \. --sample_name_parent2 {params.mom_name} \. --num_shards {threads} \. --intermediate_results_dir deeptrio_tmp/{wildcards.family} \. --postprocess_variants_parent1_extra_args=""--haploid_contigs=""chrX,chrY"",--par_regions_bed={input.PAR}"" \. --output_gvcf_child {output}/{params.child_name}.g.vcf.gz \. --output_gvcf_parent1 {output}/{params.dad_name}.g.vcf.gz \. --output_gvcf_parent2 {output}/{params.mom_name}.g.vcf.gz \. --novcf_stats_report. ```. but I got the following error:. ```. FATAL Flags parsing error: Unknown command line flag 'postprocess_variants_parent1_extra_args'. Did you mean: postprocess_variants_extra_args ? Pass --helpshort or --helpfull to see help on flags. ```. checking --helpfull, it seems only --postprocess_variants_extra_args is available. Did I misunderstand the use of postprocess_variants_parent1_extra_args? Thank you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/816
https://github.com/google/deepvariant/issues/816:526,availability,avail,available,526,"Hi @zihhuafang ,. The flag was added after 1.6.1 was released. Sorry about that. Note that `run_deeptrio` is a wrapper script that just runs the underlying binaries. So, for now you can run `run_deeptrio` with the `--dry_run` flag, which will print out all the commands it is going to run with each of the steps. From there, you can modify to make sure the postprocess_variants has the correct flag for the corresponding samples. Sorry for the inconvenience for now. The `postprocess_variants_parent1_extra_args` flag will be available in the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/816
https://github.com/google/deepvariant/issues/816:53,deployability,releas,released,53,"Hi @zihhuafang ,. The flag was added after 1.6.1 was released. Sorry about that. Note that `run_deeptrio` is a wrapper script that just runs the underlying binaries. So, for now you can run `run_deeptrio` with the `--dry_run` flag, which will print out all the commands it is going to run with each of the steps. From there, you can modify to make sure the postprocess_variants has the correct flag for the corresponding samples. Sorry for the inconvenience for now. The `postprocess_variants_parent1_extra_args` flag will be available in the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/816
https://github.com/google/deepvariant/issues/816:548,deployability,releas,release,548,"Hi @zihhuafang ,. The flag was added after 1.6.1 was released. Sorry about that. Note that `run_deeptrio` is a wrapper script that just runs the underlying binaries. So, for now you can run `run_deeptrio` with the `--dry_run` flag, which will print out all the commands it is going to run with each of the steps. From there, you can modify to make sure the postprocess_variants has the correct flag for the corresponding samples. Sorry for the inconvenience for now. The `postprocess_variants_parent1_extra_args` flag will be available in the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/816
https://github.com/google/deepvariant/issues/816:111,integrability,wrap,wrapper,111,"Hi @zihhuafang ,. The flag was added after 1.6.1 was released. Sorry about that. Note that `run_deeptrio` is a wrapper script that just runs the underlying binaries. So, for now you can run `run_deeptrio` with the `--dry_run` flag, which will print out all the commands it is going to run with each of the steps. From there, you can modify to make sure the postprocess_variants has the correct flag for the corresponding samples. Sorry for the inconvenience for now. The `postprocess_variants_parent1_extra_args` flag will be available in the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/816
https://github.com/google/deepvariant/issues/816:111,interoperability,wrapper,wrapper,111,"Hi @zihhuafang ,. The flag was added after 1.6.1 was released. Sorry about that. Note that `run_deeptrio` is a wrapper script that just runs the underlying binaries. So, for now you can run `run_deeptrio` with the `--dry_run` flag, which will print out all the commands it is going to run with each of the steps. From there, you can modify to make sure the postprocess_variants has the correct flag for the corresponding samples. Sorry for the inconvenience for now. The `postprocess_variants_parent1_extra_args` flag will be available in the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/816
https://github.com/google/deepvariant/issues/816:526,reliability,availab,available,526,"Hi @zihhuafang ,. The flag was added after 1.6.1 was released. Sorry about that. Note that `run_deeptrio` is a wrapper script that just runs the underlying binaries. So, for now you can run `run_deeptrio` with the `--dry_run` flag, which will print out all the commands it is going to run with each of the steps. From there, you can modify to make sure the postprocess_variants has the correct flag for the corresponding samples. Sorry for the inconvenience for now. The `postprocess_variants_parent1_extra_args` flag will be available in the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/816
https://github.com/google/deepvariant/issues/816:526,safety,avail,available,526,"Hi @zihhuafang ,. The flag was added after 1.6.1 was released. Sorry about that. Note that `run_deeptrio` is a wrapper script that just runs the underlying binaries. So, for now you can run `run_deeptrio` with the `--dry_run` flag, which will print out all the commands it is going to run with each of the steps. From there, you can modify to make sure the postprocess_variants has the correct flag for the corresponding samples. Sorry for the inconvenience for now. The `postprocess_variants_parent1_extra_args` flag will be available in the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/816
https://github.com/google/deepvariant/issues/816:333,security,modif,modify,333,"Hi @zihhuafang ,. The flag was added after 1.6.1 was released. Sorry about that. Note that `run_deeptrio` is a wrapper script that just runs the underlying binaries. So, for now you can run `run_deeptrio` with the `--dry_run` flag, which will print out all the commands it is going to run with each of the steps. From there, you can modify to make sure the postprocess_variants has the correct flag for the corresponding samples. Sorry for the inconvenience for now. The `postprocess_variants_parent1_extra_args` flag will be available in the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/816
https://github.com/google/deepvariant/issues/816:526,security,availab,available,526,"Hi @zihhuafang ,. The flag was added after 1.6.1 was released. Sorry about that. Note that `run_deeptrio` is a wrapper script that just runs the underlying binaries. So, for now you can run `run_deeptrio` with the `--dry_run` flag, which will print out all the commands it is going to run with each of the steps. From there, you can modify to make sure the postprocess_variants has the correct flag for the corresponding samples. Sorry for the inconvenience for now. The `postprocess_variants_parent1_extra_args` flag will be available in the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/816
https://github.com/google/deepvariant/issues/816:261,usability,command,commands,261,"Hi @zihhuafang ,. The flag was added after 1.6.1 was released. Sorry about that. Note that `run_deeptrio` is a wrapper script that just runs the underlying binaries. So, for now you can run `run_deeptrio` with the `--dry_run` flag, which will print out all the commands it is going to run with each of the steps. From there, you can modify to make sure the postprocess_variants has the correct flag for the corresponding samples. Sorry for the inconvenience for now. The `postprocess_variants_parent1_extra_args` flag will be available in the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/816
https://github.com/google/deepvariant/issues/816:244,deployability,automat,automatically,244,"If I generated a VCF file for a trio (with a father using deeptrio) or a solo male (using deepvariant) without chrX,chrY option. Can I fix the VCF after the run is finished? As a suggestion, it would be nice if the algorithm takes care of this automatically :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/816
https://github.com/google/deepvariant/issues/816:244,testability,automat,automatically,244,"If I generated a VCF file for a trio (with a father using deeptrio) or a solo male (using deepvariant) without chrX,chrY option. Can I fix the VCF after the run is finished? As a suggestion, it would be nice if the algorithm takes care of this automatically :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/816
https://github.com/google/deepvariant/issues/817:256,usability,help,help,256,"Hi Ilaria, I think it would be appropriate to open this issue on either sniffles or bcftools repo as this is not DeepVariant related. If you don't find any resolution from those forums, please email me at shafin@google.com with the files and I will try to help you from personal experience of using bcftools consensus.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/817
https://github.com/google/deepvariant/issues/817:270,usability,person,personal,270,"Hi Ilaria, I think it would be appropriate to open this issue on either sniffles or bcftools repo as this is not DeepVariant related. If you don't find any resolution from those forums, please email me at shafin@google.com with the files and I will try to help you from personal experience of using bcftools consensus.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/817
https://github.com/google/deepvariant/issues/817:279,usability,experien,experience,279,"Hi Ilaria, I think it would be appropriate to open this issue on either sniffles or bcftools repo as this is not DeepVariant related. If you don't find any resolution from those forums, please email me at shafin@google.com with the files and I will try to help you from personal experience of using bcftools consensus.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/817
https://github.com/google/deepvariant/issues/818:39,usability,command,command,39,"@MiWitt , can you please send the full command here for each step? It seems like you have 8 files are you are setting @1?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/818
https://github.com/google/deepvariant/issues/818:1024,energy efficiency,cpu,cpus,1024,"I do not run it step by step. I run ""run_deepvariant"". This is my command:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. /my/path/software/deepVariant/deepvariant_${BIN_VERSION}.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=${THEREF} \. --reads=""${ALIGNMENTNAME}.bam"" \. --sample_name=${SAMPLENAME} \. --output_vcf=""./${ALIGNMENTNAME}.deepVariant.vcf.gz"" \. --output_gvcf=""./${ALIGNMENTNAME}.deepVariant.g.vcf.gz"" \. --intermediate_results_dir . \. --num_shards=8 \. --logging_dir=. ```. . . I have now added the following command, which is a workaround for the problem ... ```. if ! [ -f ""./${ALIGNMENTNAME}.deepVariant.vcf.gz"" ]. then. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. /my/path/software/deepVariant/deepvariant_${BIN_VERSION}.sif \. /opt/deepvariant/bin/postprocess_variants \. --ref=""${THEREF}"" \. --infile ""./call_variants_output@$(ls ./call_variants_output*.tfrecord.gz | wc -l).tfrecord.gz"" \. --outfile ""./${ALIGNMENTNAME}.deepVariant.vcf.gz"" \. --cpus ""8"" \. --gvcf_outfile ""./${ALIGNMENTNAME}.deepVariant.g.vcf.gz"" \. --nonvariant_site_tfrecord_path ""./gvcf.tfrecord@$(ls ./gvcf.tfrecord*.gz | wc -l).gz"" \. --sample_name=${SAMPLENAME}. fi. ```. Eventually this workaround sets --infile to ""./call_variants_output@1.tfrecord.gz"" and --nonvariant_site_tfrecord_path to ""./gvcf.tfrecord@8.gz"" (see directory listing above).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/818
https://github.com/google/deepvariant/issues/818:1224,integrability,Event,Eventually,1224,"I do not run it step by step. I run ""run_deepvariant"". This is my command:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. /my/path/software/deepVariant/deepvariant_${BIN_VERSION}.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=${THEREF} \. --reads=""${ALIGNMENTNAME}.bam"" \. --sample_name=${SAMPLENAME} \. --output_vcf=""./${ALIGNMENTNAME}.deepVariant.vcf.gz"" \. --output_gvcf=""./${ALIGNMENTNAME}.deepVariant.g.vcf.gz"" \. --intermediate_results_dir . \. --num_shards=8 \. --logging_dir=. ```. . . I have now added the following command, which is a workaround for the problem ... ```. if ! [ -f ""./${ALIGNMENTNAME}.deepVariant.vcf.gz"" ]. then. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. /my/path/software/deepVariant/deepvariant_${BIN_VERSION}.sif \. /opt/deepvariant/bin/postprocess_variants \. --ref=""${THEREF}"" \. --infile ""./call_variants_output@$(ls ./call_variants_output*.tfrecord.gz | wc -l).tfrecord.gz"" \. --outfile ""./${ALIGNMENTNAME}.deepVariant.vcf.gz"" \. --cpus ""8"" \. --gvcf_outfile ""./${ALIGNMENTNAME}.deepVariant.g.vcf.gz"" \. --nonvariant_site_tfrecord_path ""./gvcf.tfrecord@$(ls ./gvcf.tfrecord*.gz | wc -l).gz"" \. --sample_name=${SAMPLENAME}. fi. ```. Eventually this workaround sets --infile to ""./call_variants_output@1.tfrecord.gz"" and --nonvariant_site_tfrecord_path to ""./gvcf.tfrecord@8.gz"" (see directory listing above).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/818
https://github.com/google/deepvariant/issues/818:254,modifiability,PAC,PACBIO,254,"I do not run it step by step. I run ""run_deepvariant"". This is my command:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. /my/path/software/deepVariant/deepvariant_${BIN_VERSION}.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=${THEREF} \. --reads=""${ALIGNMENTNAME}.bam"" \. --sample_name=${SAMPLENAME} \. --output_vcf=""./${ALIGNMENTNAME}.deepVariant.vcf.gz"" \. --output_gvcf=""./${ALIGNMENTNAME}.deepVariant.g.vcf.gz"" \. --intermediate_results_dir . \. --num_shards=8 \. --logging_dir=. ```. . . I have now added the following command, which is a workaround for the problem ... ```. if ! [ -f ""./${ALIGNMENTNAME}.deepVariant.vcf.gz"" ]. then. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. /my/path/software/deepVariant/deepvariant_${BIN_VERSION}.sif \. /opt/deepvariant/bin/postprocess_variants \. --ref=""${THEREF}"" \. --infile ""./call_variants_output@$(ls ./call_variants_output*.tfrecord.gz | wc -l).tfrecord.gz"" \. --outfile ""./${ALIGNMENTNAME}.deepVariant.vcf.gz"" \. --cpus ""8"" \. --gvcf_outfile ""./${ALIGNMENTNAME}.deepVariant.g.vcf.gz"" \. --nonvariant_site_tfrecord_path ""./gvcf.tfrecord@$(ls ./gvcf.tfrecord*.gz | wc -l).gz"" \. --sample_name=${SAMPLENAME}. fi. ```. Eventually this workaround sets --infile to ""./call_variants_output@1.tfrecord.gz"" and --nonvariant_site_tfrecord_path to ""./gvcf.tfrecord@8.gz"" (see directory listing above).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/818
https://github.com/google/deepvariant/issues/818:1024,performance,cpu,cpus,1024,"I do not run it step by step. I run ""run_deepvariant"". This is my command:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. /my/path/software/deepVariant/deepvariant_${BIN_VERSION}.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=${THEREF} \. --reads=""${ALIGNMENTNAME}.bam"" \. --sample_name=${SAMPLENAME} \. --output_vcf=""./${ALIGNMENTNAME}.deepVariant.vcf.gz"" \. --output_gvcf=""./${ALIGNMENTNAME}.deepVariant.g.vcf.gz"" \. --intermediate_results_dir . \. --num_shards=8 \. --logging_dir=. ```. . . I have now added the following command, which is a workaround for the problem ... ```. if ! [ -f ""./${ALIGNMENTNAME}.deepVariant.vcf.gz"" ]. then. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. /my/path/software/deepVariant/deepvariant_${BIN_VERSION}.sif \. /opt/deepvariant/bin/postprocess_variants \. --ref=""${THEREF}"" \. --infile ""./call_variants_output@$(ls ./call_variants_output*.tfrecord.gz | wc -l).tfrecord.gz"" \. --outfile ""./${ALIGNMENTNAME}.deepVariant.vcf.gz"" \. --cpus ""8"" \. --gvcf_outfile ""./${ALIGNMENTNAME}.deepVariant.g.vcf.gz"" \. --nonvariant_site_tfrecord_path ""./gvcf.tfrecord@$(ls ./gvcf.tfrecord*.gz | wc -l).gz"" \. --sample_name=${SAMPLENAME}. fi. ```. Eventually this workaround sets --infile to ""./call_variants_output@1.tfrecord.gz"" and --nonvariant_site_tfrecord_path to ""./gvcf.tfrecord@8.gz"" (see directory listing above).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/818
https://github.com/google/deepvariant/issues/818:66,usability,command,command,66,"I do not run it step by step. I run ""run_deepvariant"". This is my command:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. /my/path/software/deepVariant/deepvariant_${BIN_VERSION}.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=${THEREF} \. --reads=""${ALIGNMENTNAME}.bam"" \. --sample_name=${SAMPLENAME} \. --output_vcf=""./${ALIGNMENTNAME}.deepVariant.vcf.gz"" \. --output_gvcf=""./${ALIGNMENTNAME}.deepVariant.g.vcf.gz"" \. --intermediate_results_dir . \. --num_shards=8 \. --logging_dir=. ```. . . I have now added the following command, which is a workaround for the problem ... ```. if ! [ -f ""./${ALIGNMENTNAME}.deepVariant.vcf.gz"" ]. then. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. /my/path/software/deepVariant/deepvariant_${BIN_VERSION}.sif \. /opt/deepvariant/bin/postprocess_variants \. --ref=""${THEREF}"" \. --infile ""./call_variants_output@$(ls ./call_variants_output*.tfrecord.gz | wc -l).tfrecord.gz"" \. --outfile ""./${ALIGNMENTNAME}.deepVariant.vcf.gz"" \. --cpus ""8"" \. --gvcf_outfile ""./${ALIGNMENTNAME}.deepVariant.g.vcf.gz"" \. --nonvariant_site_tfrecord_path ""./gvcf.tfrecord@$(ls ./gvcf.tfrecord*.gz | wc -l).gz"" \. --sample_name=${SAMPLENAME}. fi. ```. Eventually this workaround sets --infile to ""./call_variants_output@1.tfrecord.gz"" and --nonvariant_site_tfrecord_path to ""./gvcf.tfrecord@8.gz"" (see directory listing above).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/818
https://github.com/google/deepvariant/issues/818:569,usability,command,command,569,"I do not run it step by step. I run ""run_deepvariant"". This is my command:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. /my/path/software/deepVariant/deepvariant_${BIN_VERSION}.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=${THEREF} \. --reads=""${ALIGNMENTNAME}.bam"" \. --sample_name=${SAMPLENAME} \. --output_vcf=""./${ALIGNMENTNAME}.deepVariant.vcf.gz"" \. --output_gvcf=""./${ALIGNMENTNAME}.deepVariant.g.vcf.gz"" \. --intermediate_results_dir . \. --num_shards=8 \. --logging_dir=. ```. . . I have now added the following command, which is a workaround for the problem ... ```. if ! [ -f ""./${ALIGNMENTNAME}.deepVariant.vcf.gz"" ]. then. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. /my/path/software/deepVariant/deepvariant_${BIN_VERSION}.sif \. /opt/deepvariant/bin/postprocess_variants \. --ref=""${THEREF}"" \. --infile ""./call_variants_output@$(ls ./call_variants_output*.tfrecord.gz | wc -l).tfrecord.gz"" \. --outfile ""./${ALIGNMENTNAME}.deepVariant.vcf.gz"" \. --cpus ""8"" \. --gvcf_outfile ""./${ALIGNMENTNAME}.deepVariant.g.vcf.gz"" \. --nonvariant_site_tfrecord_path ""./gvcf.tfrecord@$(ls ./gvcf.tfrecord*.gz | wc -l).gz"" \. --sample_name=${SAMPLENAME}. fi. ```. Eventually this workaround sets --infile to ""./call_variants_output@1.tfrecord.gz"" and --nonvariant_site_tfrecord_path to ""./gvcf.tfrecord@8.gz"" (see directory listing above).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/818
https://github.com/google/deepvariant/issues/818:799,availability,checkpoint,checkpoint,799,"I could extract the three commands make_examples, call_variants and postprocess_variants from the output. Here it is:. ```. seq 0 7 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""stdchroms.hg38.fa"" --reads ""SAMPLENAME.bam"" --examples ""./make_examples.tfrecord@8.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --gvcf ""./gvcf.tfrecord@8.gz"" --max_reads_per_partition ""600"" --min_mapping_quality ""1"" --parse_sam_aux_fields --partition_size ""25000"" --phase_reads --pileup_image_width ""199"" --norealign_reads --sample_name ""SAMPLENAME"" --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels ""0.12"" --task {}. /opt/deepvariant/bin/call_variants --outfile ""./call_variants_output.tfrecord.gz"" --examples ""./make_examples.tfrecord@8.gz"" --checkpoint ""/opt/models/pacbio"". /opt/deepvariant/bin/postprocess_variants --ref ""stdchroms.hg38.fa"" --infile ""./call_variants_output.tfrecord.gz"" --outfile ""./SAMPLENAME.deepVariant.vcf.gz"" --cpus ""8"" --gvcf_outfile ""./SAMPLENAME.deepVariant.g.vcf.gz"" --nonvariant_site_tfrecord_path ""./gvcf.tfrecord@8.gz"" --sample_name ""SAMPLENAME"". ```. And here are the two last commands with std out ... ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""./call_variants_output.tfrecord.gz"" --examples ""./make_examples.tfrecord@8.gz"" --checkpoint ""/opt/models/pacbio"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). . For more information see: https://github.com/tensorflow/addons/issues/2807 . warnings.warn(. I0510 12:13:42.483308 47501039724352 call_variants.py:563] Total 1 writing processes start",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/818
https://github.com/google/deepvariant/issues/818:1362,availability,checkpoint,checkpoint,1362,"--gvcf ""./gvcf.tfrecord@8.gz"" --max_reads_per_partition ""600"" --min_mapping_quality ""1"" --parse_sam_aux_fields --partition_size ""25000"" --phase_reads --pileup_image_width ""199"" --norealign_reads --sample_name ""SAMPLENAME"" --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels ""0.12"" --task {}. /opt/deepvariant/bin/call_variants --outfile ""./call_variants_output.tfrecord.gz"" --examples ""./make_examples.tfrecord@8.gz"" --checkpoint ""/opt/models/pacbio"". /opt/deepvariant/bin/postprocess_variants --ref ""stdchroms.hg38.fa"" --infile ""./call_variants_output.tfrecord.gz"" --outfile ""./SAMPLENAME.deepVariant.vcf.gz"" --cpus ""8"" --gvcf_outfile ""./SAMPLENAME.deepVariant.g.vcf.gz"" --nonvariant_site_tfrecord_path ""./gvcf.tfrecord@8.gz"" --sample_name ""SAMPLENAME"". ```. And here are the two last commands with std out ... ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""./call_variants_output.tfrecord.gz"" --examples ""./make_examples.tfrecord@8.gz"" --checkpoint ""/opt/models/pacbio"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). . For more information see: https://github.com/tensorflow/addons/issues/2807 . warnings.warn(. I0510 12:13:42.483308 47501039724352 call_variants.py:563] Total 1 writing processes started. I0510 12:13:42.487790 47501039724352 dv_utils.py:370] From ./make_examples.tfrecord-00000-of-00008.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0510 12:13:42.487916 47501039724352 call_variants.py:588] Shape of input examples: [100, 199, 9]. I0510 12:13:42.488451 47501039724352 call_var",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/818
https://github.com/google/deepvariant/issues/818:1598,availability,mainten,maintenance,1598,"types --track_ref_reads --vsc_min_fraction_indels ""0.12"" --task {}. /opt/deepvariant/bin/call_variants --outfile ""./call_variants_output.tfrecord.gz"" --examples ""./make_examples.tfrecord@8.gz"" --checkpoint ""/opt/models/pacbio"". /opt/deepvariant/bin/postprocess_variants --ref ""stdchroms.hg38.fa"" --infile ""./call_variants_output.tfrecord.gz"" --outfile ""./SAMPLENAME.deepVariant.vcf.gz"" --cpus ""8"" --gvcf_outfile ""./SAMPLENAME.deepVariant.g.vcf.gz"" --nonvariant_site_tfrecord_path ""./gvcf.tfrecord@8.gz"" --sample_name ""SAMPLENAME"". ```. And here are the two last commands with std out ... ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""./call_variants_output.tfrecord.gz"" --examples ""./make_examples.tfrecord@8.gz"" --checkpoint ""/opt/models/pacbio"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). . For more information see: https://github.com/tensorflow/addons/issues/2807 . warnings.warn(. I0510 12:13:42.483308 47501039724352 call_variants.py:563] Total 1 writing processes started. I0510 12:13:42.487790 47501039724352 dv_utils.py:370] From ./make_examples.tfrecord-00000-of-00008.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0510 12:13:42.487916 47501039724352 call_variants.py:588] Shape of input examples: [100, 199, 9]. I0510 12:13:42.488451 47501039724352 call_variants.py:592] Use saved model: True. I0510 12:13:52.162126 47501039724352 dv_utils.py:370] From /opt/models/pacbio/example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0510",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/818
https://github.com/google/deepvariant/issues/818:1682,availability,down,downstream,1682,"t/bin/call_variants --outfile ""./call_variants_output.tfrecord.gz"" --examples ""./make_examples.tfrecord@8.gz"" --checkpoint ""/opt/models/pacbio"". /opt/deepvariant/bin/postprocess_variants --ref ""stdchroms.hg38.fa"" --infile ""./call_variants_output.tfrecord.gz"" --outfile ""./SAMPLENAME.deepVariant.vcf.gz"" --cpus ""8"" --gvcf_outfile ""./SAMPLENAME.deepVariant.g.vcf.gz"" --nonvariant_site_tfrecord_path ""./gvcf.tfrecord@8.gz"" --sample_name ""SAMPLENAME"". ```. And here are the two last commands with std out ... ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""./call_variants_output.tfrecord.gz"" --examples ""./make_examples.tfrecord@8.gz"" --checkpoint ""/opt/models/pacbio"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). . For more information see: https://github.com/tensorflow/addons/issues/2807 . warnings.warn(. I0510 12:13:42.483308 47501039724352 call_variants.py:563] Total 1 writing processes started. I0510 12:13:42.487790 47501039724352 dv_utils.py:370] From ./make_examples.tfrecord-00000-of-00008.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0510 12:13:42.487916 47501039724352 call_variants.py:588] Shape of input examples: [100, 199, 9]. I0510 12:13:42.488451 47501039724352 call_variants.py:592] Use saved model: True. I0510 12:13:52.162126 47501039724352 dv_utils.py:370] From /opt/models/pacbio/example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0510 12:13:52.163805 47501039724352 dv_utils.py:370] From ./make_examples.tfrecord-0000",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/818
https://github.com/google/deepvariant/issues/818:1614,deployability,releas,release,1614,"ref_reads --vsc_min_fraction_indels ""0.12"" --task {}. /opt/deepvariant/bin/call_variants --outfile ""./call_variants_output.tfrecord.gz"" --examples ""./make_examples.tfrecord@8.gz"" --checkpoint ""/opt/models/pacbio"". /opt/deepvariant/bin/postprocess_variants --ref ""stdchroms.hg38.fa"" --infile ""./call_variants_output.tfrecord.gz"" --outfile ""./SAMPLENAME.deepVariant.vcf.gz"" --cpus ""8"" --gvcf_outfile ""./SAMPLENAME.deepVariant.g.vcf.gz"" --nonvariant_site_tfrecord_path ""./gvcf.tfrecord@8.gz"" --sample_name ""SAMPLENAME"". ```. And here are the two last commands with std out ... ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""./call_variants_output.tfrecord.gz"" --examples ""./make_examples.tfrecord@8.gz"" --checkpoint ""/opt/models/pacbio"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). . For more information see: https://github.com/tensorflow/addons/issues/2807 . warnings.warn(. I0510 12:13:42.483308 47501039724352 call_variants.py:563] Total 1 writing processes started. I0510 12:13:42.487790 47501039724352 dv_utils.py:370] From ./make_examples.tfrecord-00000-of-00008.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0510 12:13:42.487916 47501039724352 call_variants.py:588] Shape of input examples: [100, 199, 9]. I0510 12:13:42.488451 47501039724352 call_variants.py:592] Use saved model: True. I0510 12:13:52.162126 47501039724352 dv_utils.py:370] From /opt/models/pacbio/example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0510 12:13:52.1638",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/818
https://github.com/google/deepvariant/issues/818:1711,deployability,depend,dependencies,1711,"""./call_variants_output.tfrecord.gz"" --examples ""./make_examples.tfrecord@8.gz"" --checkpoint ""/opt/models/pacbio"". /opt/deepvariant/bin/postprocess_variants --ref ""stdchroms.hg38.fa"" --infile ""./call_variants_output.tfrecord.gz"" --outfile ""./SAMPLENAME.deepVariant.vcf.gz"" --cpus ""8"" --gvcf_outfile ""./SAMPLENAME.deepVariant.g.vcf.gz"" --nonvariant_site_tfrecord_path ""./gvcf.tfrecord@8.gz"" --sample_name ""SAMPLENAME"". ```. And here are the two last commands with std out ... ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""./call_variants_output.tfrecord.gz"" --examples ""./make_examples.tfrecord@8.gz"" --checkpoint ""/opt/models/pacbio"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). . For more information see: https://github.com/tensorflow/addons/issues/2807 . warnings.warn(. I0510 12:13:42.483308 47501039724352 call_variants.py:563] Total 1 writing processes started. I0510 12:13:42.487790 47501039724352 dv_utils.py:370] From ./make_examples.tfrecord-00000-of-00008.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0510 12:13:42.487916 47501039724352 call_variants.py:588] Shape of input examples: [100, 199, 9]. I0510 12:13:42.488451 47501039724352 call_variants.py:592] Use saved model: True. I0510 12:13:52.162126 47501039724352 dv_utils.py:370] From /opt/models/pacbio/example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0510 12:13:52.163805 47501039724352 dv_utils.py:370] From ./make_examples.tfrecord-00000-of-00008.gz.example_info.jso",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/818
https://github.com/google/deepvariant/issues/818:3562,deployability,modul,module,3562,"ls/pacbio/example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0510 12:13:52.163805 47501039724352 dv_utils.py:370] From ./make_examples.tfrecord-00000-of-00008.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0510 12:13:56.551032 47501039724352 call_variants.py:716] Predicted 982 examples in 1 batches [0.419 sec per 100]. I0510 12:13:57.403082 47501039724352 call_variants.py:779] Complete: call_variants. real	0m21.581s. user	1m40.583s. sys	0m15.744s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""stdchroms.hg38.fa"" --infile ""./call_variants_output.tfrecord.gz"" --outfile ""./SAMPLENAME.deepVariant.vcf.gz"" --cpus ""8"" --gvcf_outfile ""./SAMPLENAME.deepVariant.g.vcf.gz"" --nonvariant_site_tfrecord_path ""./gvcf.tfrecord@8.gz"" --sample_name ""SAMPLENAME"". Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_0t8uq2zt/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1419, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_0t8uq2zt/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_0t8uq2zt/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_0t8uq2zt/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1300, in main. sample_name = get_sample_name(). File ""/tmp/Bazel.runfiles_0t8uq2zt/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1203, in get_sample_name. _, record = get_cvo_paths_and_first_record(). File ""/tmp/Bazel.runfiles_0t8uq2zt/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1179, in get_cvo_paths_and_first_record. raise ValueError(. ValueError: ('Found multiple file patterns in input filename space: ', './call_variants_output.tfrecord.gz'). real	0m4.925s. user	0m8.815s. sys	0m7.379s. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/818
https://github.com/google/deepvariant/issues/818:816,energy efficiency,model,models,816,"I could extract the three commands make_examples, call_variants and postprocess_variants from the output. Here it is:. ```. seq 0 7 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""stdchroms.hg38.fa"" --reads ""SAMPLENAME.bam"" --examples ""./make_examples.tfrecord@8.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --gvcf ""./gvcf.tfrecord@8.gz"" --max_reads_per_partition ""600"" --min_mapping_quality ""1"" --parse_sam_aux_fields --partition_size ""25000"" --phase_reads --pileup_image_width ""199"" --norealign_reads --sample_name ""SAMPLENAME"" --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels ""0.12"" --task {}. /opt/deepvariant/bin/call_variants --outfile ""./call_variants_output.tfrecord.gz"" --examples ""./make_examples.tfrecord@8.gz"" --checkpoint ""/opt/models/pacbio"". /opt/deepvariant/bin/postprocess_variants --ref ""stdchroms.hg38.fa"" --infile ""./call_variants_output.tfrecord.gz"" --outfile ""./SAMPLENAME.deepVariant.vcf.gz"" --cpus ""8"" --gvcf_outfile ""./SAMPLENAME.deepVariant.g.vcf.gz"" --nonvariant_site_tfrecord_path ""./gvcf.tfrecord@8.gz"" --sample_name ""SAMPLENAME"". ```. And here are the two last commands with std out ... ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""./call_variants_output.tfrecord.gz"" --examples ""./make_examples.tfrecord@8.gz"" --checkpoint ""/opt/models/pacbio"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). . For more information see: https://github.com/tensorflow/addons/issues/2807 . warnings.warn(. I0510 12:13:42.483308 47501039724352 call_variants.py:563] Total 1 writing processes start",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/818
https://github.com/google/deepvariant/issues/818:992,energy efficiency,cpu,cpus,992,"I could extract the three commands make_examples, call_variants and postprocess_variants from the output. Here it is:. ```. seq 0 7 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""stdchroms.hg38.fa"" --reads ""SAMPLENAME.bam"" --examples ""./make_examples.tfrecord@8.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --gvcf ""./gvcf.tfrecord@8.gz"" --max_reads_per_partition ""600"" --min_mapping_quality ""1"" --parse_sam_aux_fields --partition_size ""25000"" --phase_reads --pileup_image_width ""199"" --norealign_reads --sample_name ""SAMPLENAME"" --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels ""0.12"" --task {}. /opt/deepvariant/bin/call_variants --outfile ""./call_variants_output.tfrecord.gz"" --examples ""./make_examples.tfrecord@8.gz"" --checkpoint ""/opt/models/pacbio"". /opt/deepvariant/bin/postprocess_variants --ref ""stdchroms.hg38.fa"" --infile ""./call_variants_output.tfrecord.gz"" --outfile ""./SAMPLENAME.deepVariant.vcf.gz"" --cpus ""8"" --gvcf_outfile ""./SAMPLENAME.deepVariant.g.vcf.gz"" --nonvariant_site_tfrecord_path ""./gvcf.tfrecord@8.gz"" --sample_name ""SAMPLENAME"". ```. And here are the two last commands with std out ... ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""./call_variants_output.tfrecord.gz"" --examples ""./make_examples.tfrecord@8.gz"" --checkpoint ""/opt/models/pacbio"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). . For more information see: https://github.com/tensorflow/addons/issues/2807 . warnings.warn(. I0510 12:13:42.483308 47501039724352 call_variants.py:563] Total 1 writing processes start",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/818
https://github.com/google/deepvariant/issues/818:1379,energy efficiency,model,models,1379,"tfrecord@8.gz"" --max_reads_per_partition ""600"" --min_mapping_quality ""1"" --parse_sam_aux_fields --partition_size ""25000"" --phase_reads --pileup_image_width ""199"" --norealign_reads --sample_name ""SAMPLENAME"" --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels ""0.12"" --task {}. /opt/deepvariant/bin/call_variants --outfile ""./call_variants_output.tfrecord.gz"" --examples ""./make_examples.tfrecord@8.gz"" --checkpoint ""/opt/models/pacbio"". /opt/deepvariant/bin/postprocess_variants --ref ""stdchroms.hg38.fa"" --infile ""./call_variants_output.tfrecord.gz"" --outfile ""./SAMPLENAME.deepVariant.vcf.gz"" --cpus ""8"" --gvcf_outfile ""./SAMPLENAME.deepVariant.g.vcf.gz"" --nonvariant_site_tfrecord_path ""./gvcf.tfrecord@8.gz"" --sample_name ""SAMPLENAME"". ```. And here are the two last commands with std out ... ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""./call_variants_output.tfrecord.gz"" --examples ""./make_examples.tfrecord@8.gz"" --checkpoint ""/opt/models/pacbio"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). . For more information see: https://github.com/tensorflow/addons/issues/2807 . warnings.warn(. I0510 12:13:42.483308 47501039724352 call_variants.py:563] Total 1 writing processes started. I0510 12:13:42.487790 47501039724352 dv_utils.py:370] From ./make_examples.tfrecord-00000-of-00008.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0510 12:13:42.487916 47501039724352 call_variants.py:588] Shape of input examples: [100, 199, 9]. I0510 12:13:42.488451 47501039724352 call_variants.py:592] U",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/818
https://github.com/google/deepvariant/issues/818:2391,energy efficiency,model,model,2391," /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). . For more information see: https://github.com/tensorflow/addons/issues/2807 . warnings.warn(. I0510 12:13:42.483308 47501039724352 call_variants.py:563] Total 1 writing processes started. I0510 12:13:42.487790 47501039724352 dv_utils.py:370] From ./make_examples.tfrecord-00000-of-00008.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0510 12:13:42.487916 47501039724352 call_variants.py:588] Shape of input examples: [100, 199, 9]. I0510 12:13:42.488451 47501039724352 call_variants.py:592] Use saved model: True. I0510 12:13:52.162126 47501039724352 dv_utils.py:370] From /opt/models/pacbio/example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0510 12:13:52.163805 47501039724352 dv_utils.py:370] From ./make_examples.tfrecord-00000-of-00008.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0510 12:13:56.551032 47501039724352 call_variants.py:716] Predicted 982 examples in 1 batches [0.419 sec per 100]. I0510 12:13:57.403082 47501039724352 call_variants.py:779] Complete: call_variants. real	0m21.581s. user	1m40.583s. sys	0m15.744s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""stdchroms.hg38.fa"" --infile ""./call_variants_output.tfrecord.gz"" --outfile ""./SAMPLENAME.deepVariant.vcf.gz"" --cpus ""8"" --gvcf_outfile ""./SAMPLENAME.deepVariant.g.vcf.gz"" --nonvariant_site_tfrecord_path ""./gvcf.tfrecord@8.gz"" --sample_name ""S",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/818
https://github.com/google/deepvariant/issues/818:2468,energy efficiency,model,models,2468,"y:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). . For more information see: https://github.com/tensorflow/addons/issues/2807 . warnings.warn(. I0510 12:13:42.483308 47501039724352 call_variants.py:563] Total 1 writing processes started. I0510 12:13:42.487790 47501039724352 dv_utils.py:370] From ./make_examples.tfrecord-00000-of-00008.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0510 12:13:42.487916 47501039724352 call_variants.py:588] Shape of input examples: [100, 199, 9]. I0510 12:13:42.488451 47501039724352 call_variants.py:592] Use saved model: True. I0510 12:13:52.162126 47501039724352 dv_utils.py:370] From /opt/models/pacbio/example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0510 12:13:52.163805 47501039724352 dv_utils.py:370] From ./make_examples.tfrecord-00000-of-00008.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0510 12:13:56.551032 47501039724352 call_variants.py:716] Predicted 982 examples in 1 batches [0.419 sec per 100]. I0510 12:13:57.403082 47501039724352 call_variants.py:779] Complete: call_variants. real	0m21.581s. user	1m40.583s. sys	0m15.744s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""stdchroms.hg38.fa"" --infile ""./call_variants_output.tfrecord.gz"" --outfile ""./SAMPLENAME.deepVariant.vcf.gz"" --cpus ""8"" --gvcf_outfile ""./SAMPLENAME.deepVariant.g.vcf.gz"" --nonvariant_site_tfrecord_path ""./gvcf.tfrecord@8.gz"" --sample_name ""SAMPLENAME"". Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_0t8",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/818
https://github.com/google/deepvariant/issues/818:2877,energy efficiency,Predict,Predicted,2877,"sues/2807 . warnings.warn(. I0510 12:13:42.483308 47501039724352 call_variants.py:563] Total 1 writing processes started. I0510 12:13:42.487790 47501039724352 dv_utils.py:370] From ./make_examples.tfrecord-00000-of-00008.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0510 12:13:42.487916 47501039724352 call_variants.py:588] Shape of input examples: [100, 199, 9]. I0510 12:13:42.488451 47501039724352 call_variants.py:592] Use saved model: True. I0510 12:13:52.162126 47501039724352 dv_utils.py:370] From /opt/models/pacbio/example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0510 12:13:52.163805 47501039724352 dv_utils.py:370] From ./make_examples.tfrecord-00000-of-00008.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0510 12:13:56.551032 47501039724352 call_variants.py:716] Predicted 982 examples in 1 batches [0.419 sec per 100]. I0510 12:13:57.403082 47501039724352 call_variants.py:779] Complete: call_variants. real	0m21.581s. user	1m40.583s. sys	0m15.744s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""stdchroms.hg38.fa"" --infile ""./call_variants_output.tfrecord.gz"" --outfile ""./SAMPLENAME.deepVariant.vcf.gz"" --cpus ""8"" --gvcf_outfile ""./SAMPLENAME.deepVariant.g.vcf.gz"" --nonvariant_site_tfrecord_path ""./gvcf.tfrecord@8.gz"" --sample_name ""SAMPLENAME"". Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_0t8uq2zt/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1419, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_0t8uq2zt/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_0t8uq2zt/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_0t8uq2zt/runfiles/com_google_deepvariant/deepvarian",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/818
https://github.com/google/deepvariant/issues/818:3263,energy efficiency,cpu,cpus,3263,"variants.py:588] Shape of input examples: [100, 199, 9]. I0510 12:13:42.488451 47501039724352 call_variants.py:592] Use saved model: True. I0510 12:13:52.162126 47501039724352 dv_utils.py:370] From /opt/models/pacbio/example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0510 12:13:52.163805 47501039724352 dv_utils.py:370] From ./make_examples.tfrecord-00000-of-00008.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0510 12:13:56.551032 47501039724352 call_variants.py:716] Predicted 982 examples in 1 batches [0.419 sec per 100]. I0510 12:13:57.403082 47501039724352 call_variants.py:779] Complete: call_variants. real	0m21.581s. user	1m40.583s. sys	0m15.744s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""stdchroms.hg38.fa"" --infile ""./call_variants_output.tfrecord.gz"" --outfile ""./SAMPLENAME.deepVariant.vcf.gz"" --cpus ""8"" --gvcf_outfile ""./SAMPLENAME.deepVariant.g.vcf.gz"" --nonvariant_site_tfrecord_path ""./gvcf.tfrecord@8.gz"" --sample_name ""SAMPLENAME"". Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_0t8uq2zt/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1419, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_0t8uq2zt/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_0t8uq2zt/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_0t8uq2zt/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1300, in main. sample_name = get_sample_name(). File ""/tmp/Bazel.runfiles_0t8uq2zt/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1203, in get_sample_name. _, record = get_cvo_paths_and_first_record(). File ""/tmp/Bazel.runfiles_0t8uq2zt/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1179, in g",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/818
https://github.com/google/deepvariant/issues/818:162,integrability,buffer,buffer,162,"I could extract the three commands make_examples, call_variants and postprocess_variants from the output. Here it is:. ```. seq 0 7 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""stdchroms.hg38.fa"" --reads ""SAMPLENAME.bam"" --examples ""./make_examples.tfrecord@8.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --gvcf ""./gvcf.tfrecord@8.gz"" --max_reads_per_partition ""600"" --min_mapping_quality ""1"" --parse_sam_aux_fields --partition_size ""25000"" --phase_reads --pileup_image_width ""199"" --norealign_reads --sample_name ""SAMPLENAME"" --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels ""0.12"" --task {}. /opt/deepvariant/bin/call_variants --outfile ""./call_variants_output.tfrecord.gz"" --examples ""./make_examples.tfrecord@8.gz"" --checkpoint ""/opt/models/pacbio"". /opt/deepvariant/bin/postprocess_variants --ref ""stdchroms.hg38.fa"" --infile ""./call_variants_output.tfrecord.gz"" --outfile ""./SAMPLENAME.deepVariant.vcf.gz"" --cpus ""8"" --gvcf_outfile ""./SAMPLENAME.deepVariant.g.vcf.gz"" --nonvariant_site_tfrecord_path ""./gvcf.tfrecord@8.gz"" --sample_name ""SAMPLENAME"". ```. And here are the two last commands with std out ... ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""./call_variants_output.tfrecord.gz"" --examples ""./make_examples.tfrecord@8.gz"" --checkpoint ""/opt/models/pacbio"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). . For more information see: https://github.com/tensorflow/addons/issues/2807 . warnings.warn(. I0510 12:13:42.483308 47501039724352 call_variants.py:563] Total 1 writing processes start",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/818
https://github.com/google/deepvariant/issues/818:1711,integrability,depend,dependencies,1711,"""./call_variants_output.tfrecord.gz"" --examples ""./make_examples.tfrecord@8.gz"" --checkpoint ""/opt/models/pacbio"". /opt/deepvariant/bin/postprocess_variants --ref ""stdchroms.hg38.fa"" --infile ""./call_variants_output.tfrecord.gz"" --outfile ""./SAMPLENAME.deepVariant.vcf.gz"" --cpus ""8"" --gvcf_outfile ""./SAMPLENAME.deepVariant.g.vcf.gz"" --nonvariant_site_tfrecord_path ""./gvcf.tfrecord@8.gz"" --sample_name ""SAMPLENAME"". ```. And here are the two last commands with std out ... ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""./call_variants_output.tfrecord.gz"" --examples ""./make_examples.tfrecord@8.gz"" --checkpoint ""/opt/models/pacbio"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). . For more information see: https://github.com/tensorflow/addons/issues/2807 . warnings.warn(. I0510 12:13:42.483308 47501039724352 call_variants.py:563] Total 1 writing processes started. I0510 12:13:42.487790 47501039724352 dv_utils.py:370] From ./make_examples.tfrecord-00000-of-00008.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0510 12:13:42.487916 47501039724352 call_variants.py:588] Shape of input examples: [100, 199, 9]. I0510 12:13:42.488451 47501039724352 call_variants.py:592] Use saved model: True. I0510 12:13:52.162126 47501039724352 dv_utils.py:370] From /opt/models/pacbio/example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0510 12:13:52.163805 47501039724352 dv_utils.py:370] From ./make_examples.tfrecord-00000-of-00008.gz.example_info.jso",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/818
https://github.com/google/deepvariant/issues/818:1735,integrability,repositor,repositories,1735,"tfrecord.gz"" --examples ""./make_examples.tfrecord@8.gz"" --checkpoint ""/opt/models/pacbio"". /opt/deepvariant/bin/postprocess_variants --ref ""stdchroms.hg38.fa"" --infile ""./call_variants_output.tfrecord.gz"" --outfile ""./SAMPLENAME.deepVariant.vcf.gz"" --cpus ""8"" --gvcf_outfile ""./SAMPLENAME.deepVariant.g.vcf.gz"" --nonvariant_site_tfrecord_path ""./gvcf.tfrecord@8.gz"" --sample_name ""SAMPLENAME"". ```. And here are the two last commands with std out ... ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""./call_variants_output.tfrecord.gz"" --examples ""./make_examples.tfrecord@8.gz"" --checkpoint ""/opt/models/pacbio"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). . For more information see: https://github.com/tensorflow/addons/issues/2807 . warnings.warn(. I0510 12:13:42.483308 47501039724352 call_variants.py:563] Total 1 writing processes started. I0510 12:13:42.487790 47501039724352 dv_utils.py:370] From ./make_examples.tfrecord-00000-of-00008.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0510 12:13:42.487916 47501039724352 call_variants.py:588] Shape of input examples: [100, 199, 9]. I0510 12:13:42.488451 47501039724352 call_variants.py:592] Use saved model: True. I0510 12:13:52.162126 47501039724352 dv_utils.py:370] From /opt/models/pacbio/example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0510 12:13:52.163805 47501039724352 dv_utils.py:370] From ./make_examples.tfrecord-00000-of-00008.gz.example_info.json: Shape of input exampl",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/818
https://github.com/google/deepvariant/issues/818:2905,integrability,batch,batches,2905," I0510 12:13:42.483308 47501039724352 call_variants.py:563] Total 1 writing processes started. I0510 12:13:42.487790 47501039724352 dv_utils.py:370] From ./make_examples.tfrecord-00000-of-00008.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0510 12:13:42.487916 47501039724352 call_variants.py:588] Shape of input examples: [100, 199, 9]. I0510 12:13:42.488451 47501039724352 call_variants.py:592] Use saved model: True. I0510 12:13:52.162126 47501039724352 dv_utils.py:370] From /opt/models/pacbio/example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0510 12:13:52.163805 47501039724352 dv_utils.py:370] From ./make_examples.tfrecord-00000-of-00008.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0510 12:13:56.551032 47501039724352 call_variants.py:716] Predicted 982 examples in 1 batches [0.419 sec per 100]. I0510 12:13:57.403082 47501039724352 call_variants.py:779] Complete: call_variants. real	0m21.581s. user	1m40.583s. sys	0m15.744s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""stdchroms.hg38.fa"" --infile ""./call_variants_output.tfrecord.gz"" --outfile ""./SAMPLENAME.deepVariant.vcf.gz"" --cpus ""8"" --gvcf_outfile ""./SAMPLENAME.deepVariant.g.vcf.gz"" --nonvariant_site_tfrecord_path ""./gvcf.tfrecord@8.gz"" --sample_name ""SAMPLENAME"". Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_0t8uq2zt/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1419, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_0t8uq2zt/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_0t8uq2zt/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_0t8uq2zt/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"",",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/818
https://github.com/google/deepvariant/issues/818:1735,interoperability,repositor,repositories,1735,"tfrecord.gz"" --examples ""./make_examples.tfrecord@8.gz"" --checkpoint ""/opt/models/pacbio"". /opt/deepvariant/bin/postprocess_variants --ref ""stdchroms.hg38.fa"" --infile ""./call_variants_output.tfrecord.gz"" --outfile ""./SAMPLENAME.deepVariant.vcf.gz"" --cpus ""8"" --gvcf_outfile ""./SAMPLENAME.deepVariant.g.vcf.gz"" --nonvariant_site_tfrecord_path ""./gvcf.tfrecord@8.gz"" --sample_name ""SAMPLENAME"". ```. And here are the two last commands with std out ... ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""./call_variants_output.tfrecord.gz"" --examples ""./make_examples.tfrecord@8.gz"" --checkpoint ""/opt/models/pacbio"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). . For more information see: https://github.com/tensorflow/addons/issues/2807 . warnings.warn(. I0510 12:13:42.483308 47501039724352 call_variants.py:563] Total 1 writing processes started. I0510 12:13:42.487790 47501039724352 dv_utils.py:370] From ./make_examples.tfrecord-00000-of-00008.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0510 12:13:42.487916 47501039724352 call_variants.py:588] Shape of input examples: [100, 199, 9]. I0510 12:13:42.488451 47501039724352 call_variants.py:592] Use saved model: True. I0510 12:13:52.162126 47501039724352 dv_utils.py:370] From /opt/models/pacbio/example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0510 12:13:52.163805 47501039724352 dv_utils.py:370] From ./make_examples.tfrecord-00000-of-00008.gz.example_info.json: Shape of input exampl",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/818
https://github.com/google/deepvariant/issues/818:823,modifiability,pac,pacbio,823,"I could extract the three commands make_examples, call_variants and postprocess_variants from the output. Here it is:. ```. seq 0 7 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""stdchroms.hg38.fa"" --reads ""SAMPLENAME.bam"" --examples ""./make_examples.tfrecord@8.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --gvcf ""./gvcf.tfrecord@8.gz"" --max_reads_per_partition ""600"" --min_mapping_quality ""1"" --parse_sam_aux_fields --partition_size ""25000"" --phase_reads --pileup_image_width ""199"" --norealign_reads --sample_name ""SAMPLENAME"" --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels ""0.12"" --task {}. /opt/deepvariant/bin/call_variants --outfile ""./call_variants_output.tfrecord.gz"" --examples ""./make_examples.tfrecord@8.gz"" --checkpoint ""/opt/models/pacbio"". /opt/deepvariant/bin/postprocess_variants --ref ""stdchroms.hg38.fa"" --infile ""./call_variants_output.tfrecord.gz"" --outfile ""./SAMPLENAME.deepVariant.vcf.gz"" --cpus ""8"" --gvcf_outfile ""./SAMPLENAME.deepVariant.g.vcf.gz"" --nonvariant_site_tfrecord_path ""./gvcf.tfrecord@8.gz"" --sample_name ""SAMPLENAME"". ```. And here are the two last commands with std out ... ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""./call_variants_output.tfrecord.gz"" --examples ""./make_examples.tfrecord@8.gz"" --checkpoint ""/opt/models/pacbio"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). . For more information see: https://github.com/tensorflow/addons/issues/2807 . warnings.warn(. I0510 12:13:42.483308 47501039724352 call_variants.py:563] Total 1 writing processes start",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/818
https://github.com/google/deepvariant/issues/818:1386,modifiability,pac,pacbio,1386,"d@8.gz"" --max_reads_per_partition ""600"" --min_mapping_quality ""1"" --parse_sam_aux_fields --partition_size ""25000"" --phase_reads --pileup_image_width ""199"" --norealign_reads --sample_name ""SAMPLENAME"" --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels ""0.12"" --task {}. /opt/deepvariant/bin/call_variants --outfile ""./call_variants_output.tfrecord.gz"" --examples ""./make_examples.tfrecord@8.gz"" --checkpoint ""/opt/models/pacbio"". /opt/deepvariant/bin/postprocess_variants --ref ""stdchroms.hg38.fa"" --infile ""./call_variants_output.tfrecord.gz"" --outfile ""./SAMPLENAME.deepVariant.vcf.gz"" --cpus ""8"" --gvcf_outfile ""./SAMPLENAME.deepVariant.g.vcf.gz"" --nonvariant_site_tfrecord_path ""./gvcf.tfrecord@8.gz"" --sample_name ""SAMPLENAME"". ```. And here are the two last commands with std out ... ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""./call_variants_output.tfrecord.gz"" --examples ""./make_examples.tfrecord@8.gz"" --checkpoint ""/opt/models/pacbio"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). . For more information see: https://github.com/tensorflow/addons/issues/2807 . warnings.warn(. I0510 12:13:42.483308 47501039724352 call_variants.py:563] Total 1 writing processes started. I0510 12:13:42.487790 47501039724352 dv_utils.py:370] From ./make_examples.tfrecord-00000-of-00008.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0510 12:13:42.487916 47501039724352 call_variants.py:588] Shape of input examples: [100, 199, 9]. I0510 12:13:42.488451 47501039724352 call_variants.py:592] Use save",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/818
https://github.com/google/deepvariant/issues/818:1425,modifiability,pac,packages,1425,"--min_mapping_quality ""1"" --parse_sam_aux_fields --partition_size ""25000"" --phase_reads --pileup_image_width ""199"" --norealign_reads --sample_name ""SAMPLENAME"" --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels ""0.12"" --task {}. /opt/deepvariant/bin/call_variants --outfile ""./call_variants_output.tfrecord.gz"" --examples ""./make_examples.tfrecord@8.gz"" --checkpoint ""/opt/models/pacbio"". /opt/deepvariant/bin/postprocess_variants --ref ""stdchroms.hg38.fa"" --infile ""./call_variants_output.tfrecord.gz"" --outfile ""./SAMPLENAME.deepVariant.vcf.gz"" --cpus ""8"" --gvcf_outfile ""./SAMPLENAME.deepVariant.g.vcf.gz"" --nonvariant_site_tfrecord_path ""./gvcf.tfrecord@8.gz"" --sample_name ""SAMPLENAME"". ```. And here are the two last commands with std out ... ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""./call_variants_output.tfrecord.gz"" --examples ""./make_examples.tfrecord@8.gz"" --checkpoint ""/opt/models/pacbio"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). . For more information see: https://github.com/tensorflow/addons/issues/2807 . warnings.warn(. I0510 12:13:42.483308 47501039724352 call_variants.py:563] Total 1 writing processes started. I0510 12:13:42.487790 47501039724352 dv_utils.py:370] From ./make_examples.tfrecord-00000-of-00008.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0510 12:13:42.487916 47501039724352 call_variants.py:588] Shape of input examples: [100, 199, 9]. I0510 12:13:42.488451 47501039724352 call_variants.py:592] Use saved model: True. I0510 12:13:52.162126 475",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/818
https://github.com/google/deepvariant/issues/818:1711,modifiability,depend,dependencies,1711,"""./call_variants_output.tfrecord.gz"" --examples ""./make_examples.tfrecord@8.gz"" --checkpoint ""/opt/models/pacbio"". /opt/deepvariant/bin/postprocess_variants --ref ""stdchroms.hg38.fa"" --infile ""./call_variants_output.tfrecord.gz"" --outfile ""./SAMPLENAME.deepVariant.vcf.gz"" --cpus ""8"" --gvcf_outfile ""./SAMPLENAME.deepVariant.g.vcf.gz"" --nonvariant_site_tfrecord_path ""./gvcf.tfrecord@8.gz"" --sample_name ""SAMPLENAME"". ```. And here are the two last commands with std out ... ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""./call_variants_output.tfrecord.gz"" --examples ""./make_examples.tfrecord@8.gz"" --checkpoint ""/opt/models/pacbio"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). . For more information see: https://github.com/tensorflow/addons/issues/2807 . warnings.warn(. I0510 12:13:42.483308 47501039724352 call_variants.py:563] Total 1 writing processes started. I0510 12:13:42.487790 47501039724352 dv_utils.py:370] From ./make_examples.tfrecord-00000-of-00008.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0510 12:13:42.487916 47501039724352 call_variants.py:588] Shape of input examples: [100, 199, 9]. I0510 12:13:42.488451 47501039724352 call_variants.py:592] Use saved model: True. I0510 12:13:52.162126 47501039724352 dv_utils.py:370] From /opt/models/pacbio/example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0510 12:13:52.163805 47501039724352 dv_utils.py:370] From ./make_examples.tfrecord-00000-of-00008.gz.example_info.jso",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/818
https://github.com/google/deepvariant/issues/818:2475,modifiability,pac,pacbio,2475,"serWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). . For more information see: https://github.com/tensorflow/addons/issues/2807 . warnings.warn(. I0510 12:13:42.483308 47501039724352 call_variants.py:563] Total 1 writing processes started. I0510 12:13:42.487790 47501039724352 dv_utils.py:370] From ./make_examples.tfrecord-00000-of-00008.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0510 12:13:42.487916 47501039724352 call_variants.py:588] Shape of input examples: [100, 199, 9]. I0510 12:13:42.488451 47501039724352 call_variants.py:592] Use saved model: True. I0510 12:13:52.162126 47501039724352 dv_utils.py:370] From /opt/models/pacbio/example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0510 12:13:52.163805 47501039724352 dv_utils.py:370] From ./make_examples.tfrecord-00000-of-00008.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0510 12:13:56.551032 47501039724352 call_variants.py:716] Predicted 982 examples in 1 batches [0.419 sec per 100]. I0510 12:13:57.403082 47501039724352 call_variants.py:779] Complete: call_variants. real	0m21.581s. user	1m40.583s. sys	0m15.744s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""stdchroms.hg38.fa"" --infile ""./call_variants_output.tfrecord.gz"" --outfile ""./SAMPLENAME.deepVariant.vcf.gz"" --cpus ""8"" --gvcf_outfile ""./SAMPLENAME.deepVariant.g.vcf.gz"" --nonvariant_site_tfrecord_path ""./gvcf.tfrecord@8.gz"" --sample_name ""SAMPLENAME"". Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_0t8uq2zt/r",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/818
https://github.com/google/deepvariant/issues/818:3562,modifiability,modul,module,3562,"ls/pacbio/example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0510 12:13:52.163805 47501039724352 dv_utils.py:370] From ./make_examples.tfrecord-00000-of-00008.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0510 12:13:56.551032 47501039724352 call_variants.py:716] Predicted 982 examples in 1 batches [0.419 sec per 100]. I0510 12:13:57.403082 47501039724352 call_variants.py:779] Complete: call_variants. real	0m21.581s. user	1m40.583s. sys	0m15.744s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""stdchroms.hg38.fa"" --infile ""./call_variants_output.tfrecord.gz"" --outfile ""./SAMPLENAME.deepVariant.vcf.gz"" --cpus ""8"" --gvcf_outfile ""./SAMPLENAME.deepVariant.g.vcf.gz"" --nonvariant_site_tfrecord_path ""./gvcf.tfrecord@8.gz"" --sample_name ""SAMPLENAME"". Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_0t8uq2zt/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1419, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_0t8uq2zt/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_0t8uq2zt/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_0t8uq2zt/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1300, in main. sample_name = get_sample_name(). File ""/tmp/Bazel.runfiles_0t8uq2zt/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1203, in get_sample_name. _, record = get_cvo_paths_and_first_record(). File ""/tmp/Bazel.runfiles_0t8uq2zt/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1179, in get_cvo_paths_and_first_record. raise ValueError(. ValueError: ('Found multiple file patterns in input filename space: ', './call_variants_output.tfrecord.gz'). real	0m4.925s. user	0m8.815s. sys	0m7.379s. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/818
https://github.com/google/deepvariant/issues/818:134,performance,parallel,parallel,134,"I could extract the three commands make_examples, call_variants and postprocess_variants from the output. Here it is:. ```. seq 0 7 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""stdchroms.hg38.fa"" --reads ""SAMPLENAME.bam"" --examples ""./make_examples.tfrecord@8.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --gvcf ""./gvcf.tfrecord@8.gz"" --max_reads_per_partition ""600"" --min_mapping_quality ""1"" --parse_sam_aux_fields --partition_size ""25000"" --phase_reads --pileup_image_width ""199"" --norealign_reads --sample_name ""SAMPLENAME"" --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels ""0.12"" --task {}. /opt/deepvariant/bin/call_variants --outfile ""./call_variants_output.tfrecord.gz"" --examples ""./make_examples.tfrecord@8.gz"" --checkpoint ""/opt/models/pacbio"". /opt/deepvariant/bin/postprocess_variants --ref ""stdchroms.hg38.fa"" --infile ""./call_variants_output.tfrecord.gz"" --outfile ""./SAMPLENAME.deepVariant.vcf.gz"" --cpus ""8"" --gvcf_outfile ""./SAMPLENAME.deepVariant.g.vcf.gz"" --nonvariant_site_tfrecord_path ""./gvcf.tfrecord@8.gz"" --sample_name ""SAMPLENAME"". ```. And here are the two last commands with std out ... ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""./call_variants_output.tfrecord.gz"" --examples ""./make_examples.tfrecord@8.gz"" --checkpoint ""/opt/models/pacbio"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). . For more information see: https://github.com/tensorflow/addons/issues/2807 . warnings.warn(. I0510 12:13:42.483308 47501039724352 call_variants.py:563] Total 1 writing processes start",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/818
https://github.com/google/deepvariant/issues/818:992,performance,cpu,cpus,992,"I could extract the three commands make_examples, call_variants and postprocess_variants from the output. Here it is:. ```. seq 0 7 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""stdchroms.hg38.fa"" --reads ""SAMPLENAME.bam"" --examples ""./make_examples.tfrecord@8.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --gvcf ""./gvcf.tfrecord@8.gz"" --max_reads_per_partition ""600"" --min_mapping_quality ""1"" --parse_sam_aux_fields --partition_size ""25000"" --phase_reads --pileup_image_width ""199"" --norealign_reads --sample_name ""SAMPLENAME"" --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels ""0.12"" --task {}. /opt/deepvariant/bin/call_variants --outfile ""./call_variants_output.tfrecord.gz"" --examples ""./make_examples.tfrecord@8.gz"" --checkpoint ""/opt/models/pacbio"". /opt/deepvariant/bin/postprocess_variants --ref ""stdchroms.hg38.fa"" --infile ""./call_variants_output.tfrecord.gz"" --outfile ""./SAMPLENAME.deepVariant.vcf.gz"" --cpus ""8"" --gvcf_outfile ""./SAMPLENAME.deepVariant.g.vcf.gz"" --nonvariant_site_tfrecord_path ""./gvcf.tfrecord@8.gz"" --sample_name ""SAMPLENAME"". ```. And here are the two last commands with std out ... ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""./call_variants_output.tfrecord.gz"" --examples ""./make_examples.tfrecord@8.gz"" --checkpoint ""/opt/models/pacbio"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). . For more information see: https://github.com/tensorflow/addons/issues/2807 . warnings.warn(. I0510 12:13:42.483308 47501039724352 call_variants.py:563] Total 1 writing processes start",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/818
https://github.com/google/deepvariant/issues/818:1230,performance,time,time,1230,"oms.hg38.fa"" --reads ""SAMPLENAME.bam"" --examples ""./make_examples.tfrecord@8.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --gvcf ""./gvcf.tfrecord@8.gz"" --max_reads_per_partition ""600"" --min_mapping_quality ""1"" --parse_sam_aux_fields --partition_size ""25000"" --phase_reads --pileup_image_width ""199"" --norealign_reads --sample_name ""SAMPLENAME"" --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels ""0.12"" --task {}. /opt/deepvariant/bin/call_variants --outfile ""./call_variants_output.tfrecord.gz"" --examples ""./make_examples.tfrecord@8.gz"" --checkpoint ""/opt/models/pacbio"". /opt/deepvariant/bin/postprocess_variants --ref ""stdchroms.hg38.fa"" --infile ""./call_variants_output.tfrecord.gz"" --outfile ""./SAMPLENAME.deepVariant.vcf.gz"" --cpus ""8"" --gvcf_outfile ""./SAMPLENAME.deepVariant.g.vcf.gz"" --nonvariant_site_tfrecord_path ""./gvcf.tfrecord@8.gz"" --sample_name ""SAMPLENAME"". ```. And here are the two last commands with std out ... ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""./call_variants_output.tfrecord.gz"" --examples ""./make_examples.tfrecord@8.gz"" --checkpoint ""/opt/models/pacbio"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). . For more information see: https://github.com/tensorflow/addons/issues/2807 . warnings.warn(. I0510 12:13:42.483308 47501039724352 call_variants.py:563] Total 1 writing processes started. I0510 12:13:42.487790 47501039724352 dv_utils.py:370] From ./make_examples.tfrecord-00000-of-00008.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0510 12:",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/818
https://github.com/google/deepvariant/issues/818:2905,performance,batch,batches,2905," I0510 12:13:42.483308 47501039724352 call_variants.py:563] Total 1 writing processes started. I0510 12:13:42.487790 47501039724352 dv_utils.py:370] From ./make_examples.tfrecord-00000-of-00008.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0510 12:13:42.487916 47501039724352 call_variants.py:588] Shape of input examples: [100, 199, 9]. I0510 12:13:42.488451 47501039724352 call_variants.py:592] Use saved model: True. I0510 12:13:52.162126 47501039724352 dv_utils.py:370] From /opt/models/pacbio/example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0510 12:13:52.163805 47501039724352 dv_utils.py:370] From ./make_examples.tfrecord-00000-of-00008.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0510 12:13:56.551032 47501039724352 call_variants.py:716] Predicted 982 examples in 1 batches [0.419 sec per 100]. I0510 12:13:57.403082 47501039724352 call_variants.py:779] Complete: call_variants. real	0m21.581s. user	1m40.583s. sys	0m15.744s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""stdchroms.hg38.fa"" --infile ""./call_variants_output.tfrecord.gz"" --outfile ""./SAMPLENAME.deepVariant.vcf.gz"" --cpus ""8"" --gvcf_outfile ""./SAMPLENAME.deepVariant.g.vcf.gz"" --nonvariant_site_tfrecord_path ""./gvcf.tfrecord@8.gz"" --sample_name ""SAMPLENAME"". Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_0t8uq2zt/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1419, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_0t8uq2zt/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_0t8uq2zt/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_0t8uq2zt/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"",",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/818
https://github.com/google/deepvariant/issues/818:3098,performance,time,time,3098,"08.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0510 12:13:42.487916 47501039724352 call_variants.py:588] Shape of input examples: [100, 199, 9]. I0510 12:13:42.488451 47501039724352 call_variants.py:592] Use saved model: True. I0510 12:13:52.162126 47501039724352 dv_utils.py:370] From /opt/models/pacbio/example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0510 12:13:52.163805 47501039724352 dv_utils.py:370] From ./make_examples.tfrecord-00000-of-00008.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0510 12:13:56.551032 47501039724352 call_variants.py:716] Predicted 982 examples in 1 batches [0.419 sec per 100]. I0510 12:13:57.403082 47501039724352 call_variants.py:779] Complete: call_variants. real	0m21.581s. user	1m40.583s. sys	0m15.744s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""stdchroms.hg38.fa"" --infile ""./call_variants_output.tfrecord.gz"" --outfile ""./SAMPLENAME.deepVariant.vcf.gz"" --cpus ""8"" --gvcf_outfile ""./SAMPLENAME.deepVariant.g.vcf.gz"" --nonvariant_site_tfrecord_path ""./gvcf.tfrecord@8.gz"" --sample_name ""SAMPLENAME"". Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_0t8uq2zt/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1419, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_0t8uq2zt/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_0t8uq2zt/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_0t8uq2zt/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1300, in main. sample_name = get_sample_name(). File ""/tmp/Bazel.runfiles_0t8uq2zt/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1203, in get_sample_name. _",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/818
https://github.com/google/deepvariant/issues/818:3263,performance,cpu,cpus,3263,"variants.py:588] Shape of input examples: [100, 199, 9]. I0510 12:13:42.488451 47501039724352 call_variants.py:592] Use saved model: True. I0510 12:13:52.162126 47501039724352 dv_utils.py:370] From /opt/models/pacbio/example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0510 12:13:52.163805 47501039724352 dv_utils.py:370] From ./make_examples.tfrecord-00000-of-00008.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0510 12:13:56.551032 47501039724352 call_variants.py:716] Predicted 982 examples in 1 batches [0.419 sec per 100]. I0510 12:13:57.403082 47501039724352 call_variants.py:779] Complete: call_variants. real	0m21.581s. user	1m40.583s. sys	0m15.744s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""stdchroms.hg38.fa"" --infile ""./call_variants_output.tfrecord.gz"" --outfile ""./SAMPLENAME.deepVariant.vcf.gz"" --cpus ""8"" --gvcf_outfile ""./SAMPLENAME.deepVariant.g.vcf.gz"" --nonvariant_site_tfrecord_path ""./gvcf.tfrecord@8.gz"" --sample_name ""SAMPLENAME"". Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_0t8uq2zt/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1419, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_0t8uq2zt/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_0t8uq2zt/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_0t8uq2zt/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1300, in main. sample_name = get_sample_name(). File ""/tmp/Bazel.runfiles_0t8uq2zt/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1203, in get_sample_name. _, record = get_cvo_paths_and_first_record(). File ""/tmp/Bazel.runfiles_0t8uq2zt/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1179, in g",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/818
https://github.com/google/deepvariant/issues/818:799,reliability,checkpoint,checkpoint,799,"I could extract the three commands make_examples, call_variants and postprocess_variants from the output. Here it is:. ```. seq 0 7 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""stdchroms.hg38.fa"" --reads ""SAMPLENAME.bam"" --examples ""./make_examples.tfrecord@8.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --gvcf ""./gvcf.tfrecord@8.gz"" --max_reads_per_partition ""600"" --min_mapping_quality ""1"" --parse_sam_aux_fields --partition_size ""25000"" --phase_reads --pileup_image_width ""199"" --norealign_reads --sample_name ""SAMPLENAME"" --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels ""0.12"" --task {}. /opt/deepvariant/bin/call_variants --outfile ""./call_variants_output.tfrecord.gz"" --examples ""./make_examples.tfrecord@8.gz"" --checkpoint ""/opt/models/pacbio"". /opt/deepvariant/bin/postprocess_variants --ref ""stdchroms.hg38.fa"" --infile ""./call_variants_output.tfrecord.gz"" --outfile ""./SAMPLENAME.deepVariant.vcf.gz"" --cpus ""8"" --gvcf_outfile ""./SAMPLENAME.deepVariant.g.vcf.gz"" --nonvariant_site_tfrecord_path ""./gvcf.tfrecord@8.gz"" --sample_name ""SAMPLENAME"". ```. And here are the two last commands with std out ... ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""./call_variants_output.tfrecord.gz"" --examples ""./make_examples.tfrecord@8.gz"" --checkpoint ""/opt/models/pacbio"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). . For more information see: https://github.com/tensorflow/addons/issues/2807 . warnings.warn(. I0510 12:13:42.483308 47501039724352 call_variants.py:563] Total 1 writing processes start",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/818
https://github.com/google/deepvariant/issues/818:1362,reliability,checkpoint,checkpoint,1362,"--gvcf ""./gvcf.tfrecord@8.gz"" --max_reads_per_partition ""600"" --min_mapping_quality ""1"" --parse_sam_aux_fields --partition_size ""25000"" --phase_reads --pileup_image_width ""199"" --norealign_reads --sample_name ""SAMPLENAME"" --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels ""0.12"" --task {}. /opt/deepvariant/bin/call_variants --outfile ""./call_variants_output.tfrecord.gz"" --examples ""./make_examples.tfrecord@8.gz"" --checkpoint ""/opt/models/pacbio"". /opt/deepvariant/bin/postprocess_variants --ref ""stdchroms.hg38.fa"" --infile ""./call_variants_output.tfrecord.gz"" --outfile ""./SAMPLENAME.deepVariant.vcf.gz"" --cpus ""8"" --gvcf_outfile ""./SAMPLENAME.deepVariant.g.vcf.gz"" --nonvariant_site_tfrecord_path ""./gvcf.tfrecord@8.gz"" --sample_name ""SAMPLENAME"". ```. And here are the two last commands with std out ... ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""./call_variants_output.tfrecord.gz"" --examples ""./make_examples.tfrecord@8.gz"" --checkpoint ""/opt/models/pacbio"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). . For more information see: https://github.com/tensorflow/addons/issues/2807 . warnings.warn(. I0510 12:13:42.483308 47501039724352 call_variants.py:563] Total 1 writing processes started. I0510 12:13:42.487790 47501039724352 dv_utils.py:370] From ./make_examples.tfrecord-00000-of-00008.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0510 12:13:42.487916 47501039724352 call_variants.py:588] Shape of input examples: [100, 199, 9]. I0510 12:13:42.488451 47501039724352 call_var",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/818
https://github.com/google/deepvariant/issues/818:1598,reliability,mainten,maintenance,1598,"types --track_ref_reads --vsc_min_fraction_indels ""0.12"" --task {}. /opt/deepvariant/bin/call_variants --outfile ""./call_variants_output.tfrecord.gz"" --examples ""./make_examples.tfrecord@8.gz"" --checkpoint ""/opt/models/pacbio"". /opt/deepvariant/bin/postprocess_variants --ref ""stdchroms.hg38.fa"" --infile ""./call_variants_output.tfrecord.gz"" --outfile ""./SAMPLENAME.deepVariant.vcf.gz"" --cpus ""8"" --gvcf_outfile ""./SAMPLENAME.deepVariant.g.vcf.gz"" --nonvariant_site_tfrecord_path ""./gvcf.tfrecord@8.gz"" --sample_name ""SAMPLENAME"". ```. And here are the two last commands with std out ... ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""./call_variants_output.tfrecord.gz"" --examples ""./make_examples.tfrecord@8.gz"" --checkpoint ""/opt/models/pacbio"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). . For more information see: https://github.com/tensorflow/addons/issues/2807 . warnings.warn(. I0510 12:13:42.483308 47501039724352 call_variants.py:563] Total 1 writing processes started. I0510 12:13:42.487790 47501039724352 dv_utils.py:370] From ./make_examples.tfrecord-00000-of-00008.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0510 12:13:42.487916 47501039724352 call_variants.py:588] Shape of input examples: [100, 199, 9]. I0510 12:13:42.488451 47501039724352 call_variants.py:592] Use saved model: True. I0510 12:13:52.162126 47501039724352 dv_utils.py:370] From /opt/models/pacbio/example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0510",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/818
https://github.com/google/deepvariant/issues/818:1711,safety,depend,dependencies,1711,"""./call_variants_output.tfrecord.gz"" --examples ""./make_examples.tfrecord@8.gz"" --checkpoint ""/opt/models/pacbio"". /opt/deepvariant/bin/postprocess_variants --ref ""stdchroms.hg38.fa"" --infile ""./call_variants_output.tfrecord.gz"" --outfile ""./SAMPLENAME.deepVariant.vcf.gz"" --cpus ""8"" --gvcf_outfile ""./SAMPLENAME.deepVariant.g.vcf.gz"" --nonvariant_site_tfrecord_path ""./gvcf.tfrecord@8.gz"" --sample_name ""SAMPLENAME"". ```. And here are the two last commands with std out ... ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""./call_variants_output.tfrecord.gz"" --examples ""./make_examples.tfrecord@8.gz"" --checkpoint ""/opt/models/pacbio"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). . For more information see: https://github.com/tensorflow/addons/issues/2807 . warnings.warn(. I0510 12:13:42.483308 47501039724352 call_variants.py:563] Total 1 writing processes started. I0510 12:13:42.487790 47501039724352 dv_utils.py:370] From ./make_examples.tfrecord-00000-of-00008.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0510 12:13:42.487916 47501039724352 call_variants.py:588] Shape of input examples: [100, 199, 9]. I0510 12:13:42.488451 47501039724352 call_variants.py:592] Use saved model: True. I0510 12:13:52.162126 47501039724352 dv_utils.py:370] From /opt/models/pacbio/example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0510 12:13:52.163805 47501039724352 dv_utils.py:370] From ./make_examples.tfrecord-00000-of-00008.gz.example_info.jso",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/818
https://github.com/google/deepvariant/issues/818:2134,safety,input,input,2134,"`. And here are the two last commands with std out ... ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""./call_variants_output.tfrecord.gz"" --examples ""./make_examples.tfrecord@8.gz"" --checkpoint ""/opt/models/pacbio"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). . For more information see: https://github.com/tensorflow/addons/issues/2807 . warnings.warn(. I0510 12:13:42.483308 47501039724352 call_variants.py:563] Total 1 writing processes started. I0510 12:13:42.487790 47501039724352 dv_utils.py:370] From ./make_examples.tfrecord-00000-of-00008.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0510 12:13:42.487916 47501039724352 call_variants.py:588] Shape of input examples: [100, 199, 9]. I0510 12:13:42.488451 47501039724352 call_variants.py:592] Use saved model: True. I0510 12:13:52.162126 47501039724352 dv_utils.py:370] From /opt/models/pacbio/example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0510 12:13:52.163805 47501039724352 dv_utils.py:370] From ./make_examples.tfrecord-00000-of-00008.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0510 12:13:56.551032 47501039724352 call_variants.py:716] Predicted 982 examples in 1 batches [0.419 sec per 100]. I0510 12:13:57.403082 47501039724352 call_variants.py:779] Complete: call_variants. real	0m21.581s. user	1m40.583s. sys	0m15.744s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_v",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/818
https://github.com/google/deepvariant/issues/818:2177,safety,input,input,2177,"std out ... ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""./call_variants_output.tfrecord.gz"" --examples ""./make_examples.tfrecord@8.gz"" --checkpoint ""/opt/models/pacbio"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). . For more information see: https://github.com/tensorflow/addons/issues/2807 . warnings.warn(. I0510 12:13:42.483308 47501039724352 call_variants.py:563] Total 1 writing processes started. I0510 12:13:42.487790 47501039724352 dv_utils.py:370] From ./make_examples.tfrecord-00000-of-00008.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0510 12:13:42.487916 47501039724352 call_variants.py:588] Shape of input examples: [100, 199, 9]. I0510 12:13:42.488451 47501039724352 call_variants.py:592] Use saved model: True. I0510 12:13:52.162126 47501039724352 dv_utils.py:370] From /opt/models/pacbio/example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0510 12:13:52.163805 47501039724352 dv_utils.py:370] From ./make_examples.tfrecord-00000-of-00008.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0510 12:13:56.551032 47501039724352 call_variants.py:716] Predicted 982 examples in 1 batches [0.419 sec per 100]. I0510 12:13:57.403082 47501039724352 call_variants.py:779] Complete: call_variants. real	0m21.581s. user	1m40.583s. sys	0m15.744s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""stdchroms.hg38.fa"" --infile ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/818
https://github.com/google/deepvariant/issues/818:2291,safety,input,input,2291,"ts_output.tfrecord.gz"" --examples ""./make_examples.tfrecord@8.gz"" --checkpoint ""/opt/models/pacbio"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). . For more information see: https://github.com/tensorflow/addons/issues/2807 . warnings.warn(. I0510 12:13:42.483308 47501039724352 call_variants.py:563] Total 1 writing processes started. I0510 12:13:42.487790 47501039724352 dv_utils.py:370] From ./make_examples.tfrecord-00000-of-00008.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0510 12:13:42.487916 47501039724352 call_variants.py:588] Shape of input examples: [100, 199, 9]. I0510 12:13:42.488451 47501039724352 call_variants.py:592] Use saved model: True. I0510 12:13:52.162126 47501039724352 dv_utils.py:370] From /opt/models/pacbio/example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0510 12:13:52.163805 47501039724352 dv_utils.py:370] From ./make_examples.tfrecord-00000-of-00008.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0510 12:13:56.551032 47501039724352 call_variants.py:716] Predicted 982 examples in 1 batches [0.419 sec per 100]. I0510 12:13:57.403082 47501039724352 call_variants.py:779] Complete: call_variants. real	0m21.581s. user	1m40.583s. sys	0m15.744s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""stdchroms.hg38.fa"" --infile ""./call_variants_output.tfrecord.gz"" --outfile ""./SAMPLENAME.deepVariant.vcf.gz"" --cpus ""8"" --gvcf_outfile ""./SAMP",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/818
https://github.com/google/deepvariant/issues/818:2510,safety,input,input,2510,"A) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). . For more information see: https://github.com/tensorflow/addons/issues/2807 . warnings.warn(. I0510 12:13:42.483308 47501039724352 call_variants.py:563] Total 1 writing processes started. I0510 12:13:42.487790 47501039724352 dv_utils.py:370] From ./make_examples.tfrecord-00000-of-00008.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0510 12:13:42.487916 47501039724352 call_variants.py:588] Shape of input examples: [100, 199, 9]. I0510 12:13:42.488451 47501039724352 call_variants.py:592] Use saved model: True. I0510 12:13:52.162126 47501039724352 dv_utils.py:370] From /opt/models/pacbio/example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0510 12:13:52.163805 47501039724352 dv_utils.py:370] From ./make_examples.tfrecord-00000-of-00008.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0510 12:13:56.551032 47501039724352 call_variants.py:716] Predicted 982 examples in 1 batches [0.419 sec per 100]. I0510 12:13:57.403082 47501039724352 call_variants.py:779] Complete: call_variants. real	0m21.581s. user	1m40.583s. sys	0m15.744s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""stdchroms.hg38.fa"" --infile ""./call_variants_output.tfrecord.gz"" --outfile ""./SAMPLENAME.deepVariant.vcf.gz"" --cpus ""8"" --gvcf_outfile ""./SAMPLENAME.deepVariant.g.vcf.gz"" --nonvariant_site_tfrecord_path ""./gvcf.tfrecord@8.gz"" --sample_name ""SAMPLENAME"". Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_0t8uq2zt/runfiles/com_google_deepvariant/deep",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/818
https://github.com/google/deepvariant/issues/818:2553,safety,input,input,2553,"f new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). . For more information see: https://github.com/tensorflow/addons/issues/2807 . warnings.warn(. I0510 12:13:42.483308 47501039724352 call_variants.py:563] Total 1 writing processes started. I0510 12:13:42.487790 47501039724352 dv_utils.py:370] From ./make_examples.tfrecord-00000-of-00008.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0510 12:13:42.487916 47501039724352 call_variants.py:588] Shape of input examples: [100, 199, 9]. I0510 12:13:42.488451 47501039724352 call_variants.py:592] Use saved model: True. I0510 12:13:52.162126 47501039724352 dv_utils.py:370] From /opt/models/pacbio/example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0510 12:13:52.163805 47501039724352 dv_utils.py:370] From ./make_examples.tfrecord-00000-of-00008.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0510 12:13:56.551032 47501039724352 call_variants.py:716] Predicted 982 examples in 1 batches [0.419 sec per 100]. I0510 12:13:57.403082 47501039724352 call_variants.py:779] Complete: call_variants. real	0m21.581s. user	1m40.583s. sys	0m15.744s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""stdchroms.hg38.fa"" --infile ""./call_variants_output.tfrecord.gz"" --outfile ""./SAMPLENAME.deepVariant.vcf.gz"" --cpus ""8"" --gvcf_outfile ""./SAMPLENAME.deepVariant.g.vcf.gz"" --nonvariant_site_tfrecord_path ""./gvcf.tfrecord@8.gz"" --sample_name ""SAMPLENAME"". Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_0t8uq2zt/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1419",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/818
https://github.com/google/deepvariant/issues/818:2729,safety,input,input,2729,"er repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). . For more information see: https://github.com/tensorflow/addons/issues/2807 . warnings.warn(. I0510 12:13:42.483308 47501039724352 call_variants.py:563] Total 1 writing processes started. I0510 12:13:42.487790 47501039724352 dv_utils.py:370] From ./make_examples.tfrecord-00000-of-00008.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0510 12:13:42.487916 47501039724352 call_variants.py:588] Shape of input examples: [100, 199, 9]. I0510 12:13:42.488451 47501039724352 call_variants.py:592] Use saved model: True. I0510 12:13:52.162126 47501039724352 dv_utils.py:370] From /opt/models/pacbio/example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0510 12:13:52.163805 47501039724352 dv_utils.py:370] From ./make_examples.tfrecord-00000-of-00008.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0510 12:13:56.551032 47501039724352 call_variants.py:716] Predicted 982 examples in 1 batches [0.419 sec per 100]. I0510 12:13:57.403082 47501039724352 call_variants.py:779] Complete: call_variants. real	0m21.581s. user	1m40.583s. sys	0m15.744s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""stdchroms.hg38.fa"" --infile ""./call_variants_output.tfrecord.gz"" --outfile ""./SAMPLENAME.deepVariant.vcf.gz"" --cpus ""8"" --gvcf_outfile ""./SAMPLENAME.deepVariant.g.vcf.gz"" --nonvariant_site_tfrecord_path ""./gvcf.tfrecord@8.gz"" --sample_name ""SAMPLENAME"". Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_0t8uq2zt/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1419, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_0t8uq2zt/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_0t8uq2zt/runf",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/818
https://github.com/google/deepvariant/issues/818:2772,safety,input,input,2772," (e.g. Keras, Keras-CV, and Keras-NLP). . For more information see: https://github.com/tensorflow/addons/issues/2807 . warnings.warn(. I0510 12:13:42.483308 47501039724352 call_variants.py:563] Total 1 writing processes started. I0510 12:13:42.487790 47501039724352 dv_utils.py:370] From ./make_examples.tfrecord-00000-of-00008.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0510 12:13:42.487916 47501039724352 call_variants.py:588] Shape of input examples: [100, 199, 9]. I0510 12:13:42.488451 47501039724352 call_variants.py:592] Use saved model: True. I0510 12:13:52.162126 47501039724352 dv_utils.py:370] From /opt/models/pacbio/example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0510 12:13:52.163805 47501039724352 dv_utils.py:370] From ./make_examples.tfrecord-00000-of-00008.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0510 12:13:56.551032 47501039724352 call_variants.py:716] Predicted 982 examples in 1 batches [0.419 sec per 100]. I0510 12:13:57.403082 47501039724352 call_variants.py:779] Complete: call_variants. real	0m21.581s. user	1m40.583s. sys	0m15.744s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""stdchroms.hg38.fa"" --infile ""./call_variants_output.tfrecord.gz"" --outfile ""./SAMPLENAME.deepVariant.vcf.gz"" --cpus ""8"" --gvcf_outfile ""./SAMPLENAME.deepVariant.g.vcf.gz"" --nonvariant_site_tfrecord_path ""./gvcf.tfrecord@8.gz"" --sample_name ""SAMPLENAME"". Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_0t8uq2zt/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1419, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_0t8uq2zt/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_0t8uq2zt/runfiles/absl_py/absl/app.py"", line 258, in _ru",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/818
https://github.com/google/deepvariant/issues/818:2877,safety,Predict,Predicted,2877,"sues/2807 . warnings.warn(. I0510 12:13:42.483308 47501039724352 call_variants.py:563] Total 1 writing processes started. I0510 12:13:42.487790 47501039724352 dv_utils.py:370] From ./make_examples.tfrecord-00000-of-00008.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0510 12:13:42.487916 47501039724352 call_variants.py:588] Shape of input examples: [100, 199, 9]. I0510 12:13:42.488451 47501039724352 call_variants.py:592] Use saved model: True. I0510 12:13:52.162126 47501039724352 dv_utils.py:370] From /opt/models/pacbio/example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0510 12:13:52.163805 47501039724352 dv_utils.py:370] From ./make_examples.tfrecord-00000-of-00008.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0510 12:13:56.551032 47501039724352 call_variants.py:716] Predicted 982 examples in 1 batches [0.419 sec per 100]. I0510 12:13:57.403082 47501039724352 call_variants.py:779] Complete: call_variants. real	0m21.581s. user	1m40.583s. sys	0m15.744s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""stdchroms.hg38.fa"" --infile ""./call_variants_output.tfrecord.gz"" --outfile ""./SAMPLENAME.deepVariant.vcf.gz"" --cpus ""8"" --gvcf_outfile ""./SAMPLENAME.deepVariant.g.vcf.gz"" --nonvariant_site_tfrecord_path ""./gvcf.tfrecord@8.gz"" --sample_name ""SAMPLENAME"". Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_0t8uq2zt/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1419, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_0t8uq2zt/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_0t8uq2zt/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_0t8uq2zt/runfiles/com_google_deepvariant/deepvarian",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/818
https://github.com/google/deepvariant/issues/818:2993,safety,Compl,Complete,2993,"arted. I0510 12:13:42.487790 47501039724352 dv_utils.py:370] From ./make_examples.tfrecord-00000-of-00008.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0510 12:13:42.487916 47501039724352 call_variants.py:588] Shape of input examples: [100, 199, 9]. I0510 12:13:42.488451 47501039724352 call_variants.py:592] Use saved model: True. I0510 12:13:52.162126 47501039724352 dv_utils.py:370] From /opt/models/pacbio/example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0510 12:13:52.163805 47501039724352 dv_utils.py:370] From ./make_examples.tfrecord-00000-of-00008.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0510 12:13:56.551032 47501039724352 call_variants.py:716] Predicted 982 examples in 1 batches [0.419 sec per 100]. I0510 12:13:57.403082 47501039724352 call_variants.py:779] Complete: call_variants. real	0m21.581s. user	1m40.583s. sys	0m15.744s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""stdchroms.hg38.fa"" --infile ""./call_variants_output.tfrecord.gz"" --outfile ""./SAMPLENAME.deepVariant.vcf.gz"" --cpus ""8"" --gvcf_outfile ""./SAMPLENAME.deepVariant.g.vcf.gz"" --nonvariant_site_tfrecord_path ""./gvcf.tfrecord@8.gz"" --sample_name ""SAMPLENAME"". Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_0t8uq2zt/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1419, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_0t8uq2zt/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_0t8uq2zt/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_0t8uq2zt/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1300, in main. sample_name = get_sample_name(). File ""/tmp/Bazel.runfiles_0t8uq2zt",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/818
https://github.com/google/deepvariant/issues/818:3562,safety,modul,module,3562,"ls/pacbio/example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0510 12:13:52.163805 47501039724352 dv_utils.py:370] From ./make_examples.tfrecord-00000-of-00008.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0510 12:13:56.551032 47501039724352 call_variants.py:716] Predicted 982 examples in 1 batches [0.419 sec per 100]. I0510 12:13:57.403082 47501039724352 call_variants.py:779] Complete: call_variants. real	0m21.581s. user	1m40.583s. sys	0m15.744s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""stdchroms.hg38.fa"" --infile ""./call_variants_output.tfrecord.gz"" --outfile ""./SAMPLENAME.deepVariant.vcf.gz"" --cpus ""8"" --gvcf_outfile ""./SAMPLENAME.deepVariant.g.vcf.gz"" --nonvariant_site_tfrecord_path ""./gvcf.tfrecord@8.gz"" --sample_name ""SAMPLENAME"". Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_0t8uq2zt/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1419, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_0t8uq2zt/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_0t8uq2zt/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_0t8uq2zt/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1300, in main. sample_name = get_sample_name(). File ""/tmp/Bazel.runfiles_0t8uq2zt/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1203, in get_sample_name. _, record = get_cvo_paths_and_first_record(). File ""/tmp/Bazel.runfiles_0t8uq2zt/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1179, in get_cvo_paths_and_first_record. raise ValueError(. ValueError: ('Found multiple file patterns in input filename space: ', './call_variants_output.tfrecord.gz'). real	0m4.925s. user	0m8.815s. sys	0m7.379s. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/818
https://github.com/google/deepvariant/issues/818:4361,safety,input,input,4361,"ls/pacbio/example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0510 12:13:52.163805 47501039724352 dv_utils.py:370] From ./make_examples.tfrecord-00000-of-00008.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0510 12:13:56.551032 47501039724352 call_variants.py:716] Predicted 982 examples in 1 batches [0.419 sec per 100]. I0510 12:13:57.403082 47501039724352 call_variants.py:779] Complete: call_variants. real	0m21.581s. user	1m40.583s. sys	0m15.744s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""stdchroms.hg38.fa"" --infile ""./call_variants_output.tfrecord.gz"" --outfile ""./SAMPLENAME.deepVariant.vcf.gz"" --cpus ""8"" --gvcf_outfile ""./SAMPLENAME.deepVariant.g.vcf.gz"" --nonvariant_site_tfrecord_path ""./gvcf.tfrecord@8.gz"" --sample_name ""SAMPLENAME"". Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_0t8uq2zt/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1419, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_0t8uq2zt/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_0t8uq2zt/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_0t8uq2zt/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1300, in main. sample_name = get_sample_name(). File ""/tmp/Bazel.runfiles_0t8uq2zt/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1203, in get_sample_name. _, record = get_cvo_paths_and_first_record(). File ""/tmp/Bazel.runfiles_0t8uq2zt/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1179, in get_cvo_paths_and_first_record. raise ValueError(. ValueError: ('Found multiple file patterns in input filename space: ', './call_variants_output.tfrecord.gz'). real	0m4.925s. user	0m8.815s. sys	0m7.379s. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/818
https://github.com/google/deepvariant/issues/818:816,security,model,models,816,"I could extract the three commands make_examples, call_variants and postprocess_variants from the output. Here it is:. ```. seq 0 7 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""stdchroms.hg38.fa"" --reads ""SAMPLENAME.bam"" --examples ""./make_examples.tfrecord@8.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --gvcf ""./gvcf.tfrecord@8.gz"" --max_reads_per_partition ""600"" --min_mapping_quality ""1"" --parse_sam_aux_fields --partition_size ""25000"" --phase_reads --pileup_image_width ""199"" --norealign_reads --sample_name ""SAMPLENAME"" --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels ""0.12"" --task {}. /opt/deepvariant/bin/call_variants --outfile ""./call_variants_output.tfrecord.gz"" --examples ""./make_examples.tfrecord@8.gz"" --checkpoint ""/opt/models/pacbio"". /opt/deepvariant/bin/postprocess_variants --ref ""stdchroms.hg38.fa"" --infile ""./call_variants_output.tfrecord.gz"" --outfile ""./SAMPLENAME.deepVariant.vcf.gz"" --cpus ""8"" --gvcf_outfile ""./SAMPLENAME.deepVariant.g.vcf.gz"" --nonvariant_site_tfrecord_path ""./gvcf.tfrecord@8.gz"" --sample_name ""SAMPLENAME"". ```. And here are the two last commands with std out ... ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""./call_variants_output.tfrecord.gz"" --examples ""./make_examples.tfrecord@8.gz"" --checkpoint ""/opt/models/pacbio"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). . For more information see: https://github.com/tensorflow/addons/issues/2807 . warnings.warn(. I0510 12:13:42.483308 47501039724352 call_variants.py:563] Total 1 writing processes start",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/818
https://github.com/google/deepvariant/issues/818:1379,security,model,models,1379,"tfrecord@8.gz"" --max_reads_per_partition ""600"" --min_mapping_quality ""1"" --parse_sam_aux_fields --partition_size ""25000"" --phase_reads --pileup_image_width ""199"" --norealign_reads --sample_name ""SAMPLENAME"" --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels ""0.12"" --task {}. /opt/deepvariant/bin/call_variants --outfile ""./call_variants_output.tfrecord.gz"" --examples ""./make_examples.tfrecord@8.gz"" --checkpoint ""/opt/models/pacbio"". /opt/deepvariant/bin/postprocess_variants --ref ""stdchroms.hg38.fa"" --infile ""./call_variants_output.tfrecord.gz"" --outfile ""./SAMPLENAME.deepVariant.vcf.gz"" --cpus ""8"" --gvcf_outfile ""./SAMPLENAME.deepVariant.g.vcf.gz"" --nonvariant_site_tfrecord_path ""./gvcf.tfrecord@8.gz"" --sample_name ""SAMPLENAME"". ```. And here are the two last commands with std out ... ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""./call_variants_output.tfrecord.gz"" --examples ""./make_examples.tfrecord@8.gz"" --checkpoint ""/opt/models/pacbio"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). . For more information see: https://github.com/tensorflow/addons/issues/2807 . warnings.warn(. I0510 12:13:42.483308 47501039724352 call_variants.py:563] Total 1 writing processes started. I0510 12:13:42.487790 47501039724352 dv_utils.py:370] From ./make_examples.tfrecord-00000-of-00008.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0510 12:13:42.487916 47501039724352 call_variants.py:588] Shape of input examples: [100, 199, 9]. I0510 12:13:42.488451 47501039724352 call_variants.py:592] U",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/818
https://github.com/google/deepvariant/issues/818:1675,security,modif,modify,1675,"eepvariant/bin/call_variants --outfile ""./call_variants_output.tfrecord.gz"" --examples ""./make_examples.tfrecord@8.gz"" --checkpoint ""/opt/models/pacbio"". /opt/deepvariant/bin/postprocess_variants --ref ""stdchroms.hg38.fa"" --infile ""./call_variants_output.tfrecord.gz"" --outfile ""./SAMPLENAME.deepVariant.vcf.gz"" --cpus ""8"" --gvcf_outfile ""./SAMPLENAME.deepVariant.g.vcf.gz"" --nonvariant_site_tfrecord_path ""./gvcf.tfrecord@8.gz"" --sample_name ""SAMPLENAME"". ```. And here are the two last commands with std out ... ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""./call_variants_output.tfrecord.gz"" --examples ""./make_examples.tfrecord@8.gz"" --checkpoint ""/opt/models/pacbio"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). . For more information see: https://github.com/tensorflow/addons/issues/2807 . warnings.warn(. I0510 12:13:42.483308 47501039724352 call_variants.py:563] Total 1 writing processes started. I0510 12:13:42.487790 47501039724352 dv_utils.py:370] From ./make_examples.tfrecord-00000-of-00008.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0510 12:13:42.487916 47501039724352 call_variants.py:588] Shape of input examples: [100, 199, 9]. I0510 12:13:42.488451 47501039724352 call_variants.py:592] Use saved model: True. I0510 12:13:52.162126 47501039724352 dv_utils.py:370] From /opt/models/pacbio/example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0510 12:13:52.163805 47501039724352 dv_utils.py:370] From ./make_examples.tfre",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/818
https://github.com/google/deepvariant/issues/818:2391,security,model,model,2391," /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). . For more information see: https://github.com/tensorflow/addons/issues/2807 . warnings.warn(. I0510 12:13:42.483308 47501039724352 call_variants.py:563] Total 1 writing processes started. I0510 12:13:42.487790 47501039724352 dv_utils.py:370] From ./make_examples.tfrecord-00000-of-00008.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0510 12:13:42.487916 47501039724352 call_variants.py:588] Shape of input examples: [100, 199, 9]. I0510 12:13:42.488451 47501039724352 call_variants.py:592] Use saved model: True. I0510 12:13:52.162126 47501039724352 dv_utils.py:370] From /opt/models/pacbio/example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0510 12:13:52.163805 47501039724352 dv_utils.py:370] From ./make_examples.tfrecord-00000-of-00008.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0510 12:13:56.551032 47501039724352 call_variants.py:716] Predicted 982 examples in 1 batches [0.419 sec per 100]. I0510 12:13:57.403082 47501039724352 call_variants.py:779] Complete: call_variants. real	0m21.581s. user	1m40.583s. sys	0m15.744s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""stdchroms.hg38.fa"" --infile ""./call_variants_output.tfrecord.gz"" --outfile ""./SAMPLENAME.deepVariant.vcf.gz"" --cpus ""8"" --gvcf_outfile ""./SAMPLENAME.deepVariant.g.vcf.gz"" --nonvariant_site_tfrecord_path ""./gvcf.tfrecord@8.gz"" --sample_name ""S",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/818
https://github.com/google/deepvariant/issues/818:2468,security,model,models,2468,"y:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). . For more information see: https://github.com/tensorflow/addons/issues/2807 . warnings.warn(. I0510 12:13:42.483308 47501039724352 call_variants.py:563] Total 1 writing processes started. I0510 12:13:42.487790 47501039724352 dv_utils.py:370] From ./make_examples.tfrecord-00000-of-00008.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0510 12:13:42.487916 47501039724352 call_variants.py:588] Shape of input examples: [100, 199, 9]. I0510 12:13:42.488451 47501039724352 call_variants.py:592] Use saved model: True. I0510 12:13:52.162126 47501039724352 dv_utils.py:370] From /opt/models/pacbio/example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0510 12:13:52.163805 47501039724352 dv_utils.py:370] From ./make_examples.tfrecord-00000-of-00008.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0510 12:13:56.551032 47501039724352 call_variants.py:716] Predicted 982 examples in 1 batches [0.419 sec per 100]. I0510 12:13:57.403082 47501039724352 call_variants.py:779] Complete: call_variants. real	0m21.581s. user	1m40.583s. sys	0m15.744s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""stdchroms.hg38.fa"" --infile ""./call_variants_output.tfrecord.gz"" --outfile ""./SAMPLENAME.deepVariant.vcf.gz"" --cpus ""8"" --gvcf_outfile ""./SAMPLENAME.deepVariant.g.vcf.gz"" --nonvariant_site_tfrecord_path ""./gvcf.tfrecord@8.gz"" --sample_name ""SAMPLENAME"". Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_0t8",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/818
https://github.com/google/deepvariant/issues/818:2993,security,Compl,Complete,2993,"arted. I0510 12:13:42.487790 47501039724352 dv_utils.py:370] From ./make_examples.tfrecord-00000-of-00008.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0510 12:13:42.487916 47501039724352 call_variants.py:588] Shape of input examples: [100, 199, 9]. I0510 12:13:42.488451 47501039724352 call_variants.py:592] Use saved model: True. I0510 12:13:52.162126 47501039724352 dv_utils.py:370] From /opt/models/pacbio/example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0510 12:13:52.163805 47501039724352 dv_utils.py:370] From ./make_examples.tfrecord-00000-of-00008.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0510 12:13:56.551032 47501039724352 call_variants.py:716] Predicted 982 examples in 1 batches [0.419 sec per 100]. I0510 12:13:57.403082 47501039724352 call_variants.py:779] Complete: call_variants. real	0m21.581s. user	1m40.583s. sys	0m15.744s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""stdchroms.hg38.fa"" --infile ""./call_variants_output.tfrecord.gz"" --outfile ""./SAMPLENAME.deepVariant.vcf.gz"" --cpus ""8"" --gvcf_outfile ""./SAMPLENAME.deepVariant.g.vcf.gz"" --nonvariant_site_tfrecord_path ""./gvcf.tfrecord@8.gz"" --sample_name ""SAMPLENAME"". Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_0t8uq2zt/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1419, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_0t8uq2zt/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_0t8uq2zt/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_0t8uq2zt/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1300, in main. sample_name = get_sample_name(). File ""/tmp/Bazel.runfiles_0t8uq2zt",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/818
https://github.com/google/deepvariant/issues/818:1635,testability,plan,planned,1635,"raction_indels ""0.12"" --task {}. /opt/deepvariant/bin/call_variants --outfile ""./call_variants_output.tfrecord.gz"" --examples ""./make_examples.tfrecord@8.gz"" --checkpoint ""/opt/models/pacbio"". /opt/deepvariant/bin/postprocess_variants --ref ""stdchroms.hg38.fa"" --infile ""./call_variants_output.tfrecord.gz"" --outfile ""./SAMPLENAME.deepVariant.vcf.gz"" --cpus ""8"" --gvcf_outfile ""./SAMPLENAME.deepVariant.g.vcf.gz"" --nonvariant_site_tfrecord_path ""./gvcf.tfrecord@8.gz"" --sample_name ""SAMPLENAME"". ```. And here are the two last commands with std out ... ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""./call_variants_output.tfrecord.gz"" --examples ""./make_examples.tfrecord@8.gz"" --checkpoint ""/opt/models/pacbio"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). . For more information see: https://github.com/tensorflow/addons/issues/2807 . warnings.warn(. I0510 12:13:42.483308 47501039724352 call_variants.py:563] Total 1 writing processes started. I0510 12:13:42.487790 47501039724352 dv_utils.py:370] From ./make_examples.tfrecord-00000-of-00008.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0510 12:13:42.487916 47501039724352 call_variants.py:588] Shape of input examples: [100, 199, 9]. I0510 12:13:42.488451 47501039724352 call_variants.py:592] Use saved model: True. I0510 12:13:52.162126 47501039724352 dv_utils.py:370] From /opt/models/pacbio/example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0510 12:13:52.163805 47501039724352 dv_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/818
https://github.com/google/deepvariant/issues/818:1711,testability,depend,dependencies,1711,"""./call_variants_output.tfrecord.gz"" --examples ""./make_examples.tfrecord@8.gz"" --checkpoint ""/opt/models/pacbio"". /opt/deepvariant/bin/postprocess_variants --ref ""stdchroms.hg38.fa"" --infile ""./call_variants_output.tfrecord.gz"" --outfile ""./SAMPLENAME.deepVariant.vcf.gz"" --cpus ""8"" --gvcf_outfile ""./SAMPLENAME.deepVariant.g.vcf.gz"" --nonvariant_site_tfrecord_path ""./gvcf.tfrecord@8.gz"" --sample_name ""SAMPLENAME"". ```. And here are the two last commands with std out ... ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""./call_variants_output.tfrecord.gz"" --examples ""./make_examples.tfrecord@8.gz"" --checkpoint ""/opt/models/pacbio"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). . For more information see: https://github.com/tensorflow/addons/issues/2807 . warnings.warn(. I0510 12:13:42.483308 47501039724352 call_variants.py:563] Total 1 writing processes started. I0510 12:13:42.487790 47501039724352 dv_utils.py:370] From ./make_examples.tfrecord-00000-of-00008.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0510 12:13:42.487916 47501039724352 call_variants.py:588] Shape of input examples: [100, 199, 9]. I0510 12:13:42.488451 47501039724352 call_variants.py:592] Use saved model: True. I0510 12:13:52.162126 47501039724352 dv_utils.py:370] From /opt/models/pacbio/example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0510 12:13:52.163805 47501039724352 dv_utils.py:370] From ./make_examples.tfrecord-00000-of-00008.gz.example_info.jso",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/818
https://github.com/google/deepvariant/issues/818:3406,testability,Trace,Traceback,3406,"2:13:52.162126 47501039724352 dv_utils.py:370] From /opt/models/pacbio/example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0510 12:13:52.163805 47501039724352 dv_utils.py:370] From ./make_examples.tfrecord-00000-of-00008.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0510 12:13:56.551032 47501039724352 call_variants.py:716] Predicted 982 examples in 1 batches [0.419 sec per 100]. I0510 12:13:57.403082 47501039724352 call_variants.py:779] Complete: call_variants. real	0m21.581s. user	1m40.583s. sys	0m15.744s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""stdchroms.hg38.fa"" --infile ""./call_variants_output.tfrecord.gz"" --outfile ""./SAMPLENAME.deepVariant.vcf.gz"" --cpus ""8"" --gvcf_outfile ""./SAMPLENAME.deepVariant.g.vcf.gz"" --nonvariant_site_tfrecord_path ""./gvcf.tfrecord@8.gz"" --sample_name ""SAMPLENAME"". Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_0t8uq2zt/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1419, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_0t8uq2zt/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_0t8uq2zt/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_0t8uq2zt/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1300, in main. sample_name = get_sample_name(). File ""/tmp/Bazel.runfiles_0t8uq2zt/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1203, in get_sample_name. _, record = get_cvo_paths_and_first_record(). File ""/tmp/Bazel.runfiles_0t8uq2zt/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1179, in get_cvo_paths_and_first_record. raise ValueError(. ValueError: ('Found multiple file patterns in input filename space: ', './call_variants_output.t",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/818
https://github.com/google/deepvariant/issues/818:26,usability,command,commands,26,"I could extract the three commands make_examples, call_variants and postprocess_variants from the output. Here it is:. ```. seq 0 7 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""stdchroms.hg38.fa"" --reads ""SAMPLENAME.bam"" --examples ""./make_examples.tfrecord@8.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --gvcf ""./gvcf.tfrecord@8.gz"" --max_reads_per_partition ""600"" --min_mapping_quality ""1"" --parse_sam_aux_fields --partition_size ""25000"" --phase_reads --pileup_image_width ""199"" --norealign_reads --sample_name ""SAMPLENAME"" --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels ""0.12"" --task {}. /opt/deepvariant/bin/call_variants --outfile ""./call_variants_output.tfrecord.gz"" --examples ""./make_examples.tfrecord@8.gz"" --checkpoint ""/opt/models/pacbio"". /opt/deepvariant/bin/postprocess_variants --ref ""stdchroms.hg38.fa"" --infile ""./call_variants_output.tfrecord.gz"" --outfile ""./SAMPLENAME.deepVariant.vcf.gz"" --cpus ""8"" --gvcf_outfile ""./SAMPLENAME.deepVariant.g.vcf.gz"" --nonvariant_site_tfrecord_path ""./gvcf.tfrecord@8.gz"" --sample_name ""SAMPLENAME"". ```. And here are the two last commands with std out ... ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""./call_variants_output.tfrecord.gz"" --examples ""./make_examples.tfrecord@8.gz"" --checkpoint ""/opt/models/pacbio"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). . For more information see: https://github.com/tensorflow/addons/issues/2807 . warnings.warn(. I0510 12:13:42.483308 47501039724352 call_variants.py:563] Total 1 writing processes start",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/818
https://github.com/google/deepvariant/issues/818:1166,usability,command,commands,1166,"opt/deepvariant/bin/make_examples --mode calling --ref ""stdchroms.hg38.fa"" --reads ""SAMPLENAME.bam"" --examples ""./make_examples.tfrecord@8.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --gvcf ""./gvcf.tfrecord@8.gz"" --max_reads_per_partition ""600"" --min_mapping_quality ""1"" --parse_sam_aux_fields --partition_size ""25000"" --phase_reads --pileup_image_width ""199"" --norealign_reads --sample_name ""SAMPLENAME"" --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels ""0.12"" --task {}. /opt/deepvariant/bin/call_variants --outfile ""./call_variants_output.tfrecord.gz"" --examples ""./make_examples.tfrecord@8.gz"" --checkpoint ""/opt/models/pacbio"". /opt/deepvariant/bin/postprocess_variants --ref ""stdchroms.hg38.fa"" --infile ""./call_variants_output.tfrecord.gz"" --outfile ""./SAMPLENAME.deepVariant.vcf.gz"" --cpus ""8"" --gvcf_outfile ""./SAMPLENAME.deepVariant.g.vcf.gz"" --nonvariant_site_tfrecord_path ""./gvcf.tfrecord@8.gz"" --sample_name ""SAMPLENAME"". ```. And here are the two last commands with std out ... ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""./call_variants_output.tfrecord.gz"" --examples ""./make_examples.tfrecord@8.gz"" --checkpoint ""/opt/models/pacbio"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). . For more information see: https://github.com/tensorflow/addons/issues/2807 . warnings.warn(. I0510 12:13:42.483308 47501039724352 call_variants.py:563] Total 1 writing processes started. I0510 12:13:42.487790 47501039724352 dv_utils.py:370] From ./make_examples.tfrecord-00000-of-00008.gz.example_info.json: Shape of input examples: [100, 199, 9], Chann",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/818
https://github.com/google/deepvariant/issues/818:1215,usability,command,command,1215,"--ref ""stdchroms.hg38.fa"" --reads ""SAMPLENAME.bam"" --examples ""./make_examples.tfrecord@8.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --gvcf ""./gvcf.tfrecord@8.gz"" --max_reads_per_partition ""600"" --min_mapping_quality ""1"" --parse_sam_aux_fields --partition_size ""25000"" --phase_reads --pileup_image_width ""199"" --norealign_reads --sample_name ""SAMPLENAME"" --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels ""0.12"" --task {}. /opt/deepvariant/bin/call_variants --outfile ""./call_variants_output.tfrecord.gz"" --examples ""./make_examples.tfrecord@8.gz"" --checkpoint ""/opt/models/pacbio"". /opt/deepvariant/bin/postprocess_variants --ref ""stdchroms.hg38.fa"" --infile ""./call_variants_output.tfrecord.gz"" --outfile ""./SAMPLENAME.deepVariant.vcf.gz"" --cpus ""8"" --gvcf_outfile ""./SAMPLENAME.deepVariant.g.vcf.gz"" --nonvariant_site_tfrecord_path ""./gvcf.tfrecord@8.gz"" --sample_name ""SAMPLENAME"". ```. And here are the two last commands with std out ... ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""./call_variants_output.tfrecord.gz"" --examples ""./make_examples.tfrecord@8.gz"" --checkpoint ""/opt/models/pacbio"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). . For more information see: https://github.com/tensorflow/addons/issues/2807 . warnings.warn(. I0510 12:13:42.483308 47501039724352 call_variants.py:563] Total 1 writing processes started. I0510 12:13:42.487790 47501039724352 dv_utils.py:370] From ./make_examples.tfrecord-00000-of-00008.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 1",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/818
https://github.com/google/deepvariant/issues/818:1477,usability,User,UserWarning,1477,"tition_size ""25000"" --phase_reads --pileup_image_width ""199"" --norealign_reads --sample_name ""SAMPLENAME"" --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels ""0.12"" --task {}. /opt/deepvariant/bin/call_variants --outfile ""./call_variants_output.tfrecord.gz"" --examples ""./make_examples.tfrecord@8.gz"" --checkpoint ""/opt/models/pacbio"". /opt/deepvariant/bin/postprocess_variants --ref ""stdchroms.hg38.fa"" --infile ""./call_variants_output.tfrecord.gz"" --outfile ""./SAMPLENAME.deepVariant.vcf.gz"" --cpus ""8"" --gvcf_outfile ""./SAMPLENAME.deepVariant.g.vcf.gz"" --nonvariant_site_tfrecord_path ""./gvcf.tfrecord@8.gz"" --sample_name ""SAMPLENAME"". ```. And here are the two last commands with std out ... ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""./call_variants_output.tfrecord.gz"" --examples ""./make_examples.tfrecord@8.gz"" --checkpoint ""/opt/models/pacbio"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). . For more information see: https://github.com/tensorflow/addons/issues/2807 . warnings.warn(. I0510 12:13:42.483308 47501039724352 call_variants.py:563] Total 1 writing processes started. I0510 12:13:42.487790 47501039724352 dv_utils.py:370] From ./make_examples.tfrecord-00000-of-00008.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0510 12:13:42.487916 47501039724352 call_variants.py:588] Shape of input examples: [100, 199, 9]. I0510 12:13:42.488451 47501039724352 call_variants.py:592] Use saved model: True. I0510 12:13:52.162126 47501039724352 dv_utils.py:370] From /opt/models/pacbio/e",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/818
https://github.com/google/deepvariant/issues/818:1590,usability,minim,minimal,1590,"t_by_haplotypes --track_ref_reads --vsc_min_fraction_indels ""0.12"" --task {}. /opt/deepvariant/bin/call_variants --outfile ""./call_variants_output.tfrecord.gz"" --examples ""./make_examples.tfrecord@8.gz"" --checkpoint ""/opt/models/pacbio"". /opt/deepvariant/bin/postprocess_variants --ref ""stdchroms.hg38.fa"" --infile ""./call_variants_output.tfrecord.gz"" --outfile ""./SAMPLENAME.deepVariant.vcf.gz"" --cpus ""8"" --gvcf_outfile ""./SAMPLENAME.deepVariant.g.vcf.gz"" --nonvariant_site_tfrecord_path ""./gvcf.tfrecord@8.gz"" --sample_name ""SAMPLENAME"". ```. And here are the two last commands with std out ... ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""./call_variants_output.tfrecord.gz"" --examples ""./make_examples.tfrecord@8.gz"" --checkpoint ""/opt/models/pacbio"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). . For more information see: https://github.com/tensorflow/addons/issues/2807 . warnings.warn(. I0510 12:13:42.483308 47501039724352 call_variants.py:563] Total 1 writing processes started. I0510 12:13:42.487790 47501039724352 dv_utils.py:370] From ./make_examples.tfrecord-00000-of-00008.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0510 12:13:42.487916 47501039724352 call_variants.py:588] Shape of input examples: [100, 199, 9]. I0510 12:13:42.488451 47501039724352 call_variants.py:592] Use saved model: True. I0510 12:13:52.162126 47501039724352 dv_utils.py:370] From /opt/models/pacbio/example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/818
https://github.com/google/deepvariant/issues/818:2134,usability,input,input,2134,"`. And here are the two last commands with std out ... ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""./call_variants_output.tfrecord.gz"" --examples ""./make_examples.tfrecord@8.gz"" --checkpoint ""/opt/models/pacbio"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). . For more information see: https://github.com/tensorflow/addons/issues/2807 . warnings.warn(. I0510 12:13:42.483308 47501039724352 call_variants.py:563] Total 1 writing processes started. I0510 12:13:42.487790 47501039724352 dv_utils.py:370] From ./make_examples.tfrecord-00000-of-00008.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0510 12:13:42.487916 47501039724352 call_variants.py:588] Shape of input examples: [100, 199, 9]. I0510 12:13:42.488451 47501039724352 call_variants.py:592] Use saved model: True. I0510 12:13:52.162126 47501039724352 dv_utils.py:370] From /opt/models/pacbio/example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0510 12:13:52.163805 47501039724352 dv_utils.py:370] From ./make_examples.tfrecord-00000-of-00008.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0510 12:13:56.551032 47501039724352 call_variants.py:716] Predicted 982 examples in 1 batches [0.419 sec per 100]. I0510 12:13:57.403082 47501039724352 call_variants.py:779] Complete: call_variants. real	0m21.581s. user	1m40.583s. sys	0m15.744s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_v",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/818
https://github.com/google/deepvariant/issues/818:2177,usability,input,input,2177,"std out ... ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""./call_variants_output.tfrecord.gz"" --examples ""./make_examples.tfrecord@8.gz"" --checkpoint ""/opt/models/pacbio"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). . For more information see: https://github.com/tensorflow/addons/issues/2807 . warnings.warn(. I0510 12:13:42.483308 47501039724352 call_variants.py:563] Total 1 writing processes started. I0510 12:13:42.487790 47501039724352 dv_utils.py:370] From ./make_examples.tfrecord-00000-of-00008.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0510 12:13:42.487916 47501039724352 call_variants.py:588] Shape of input examples: [100, 199, 9]. I0510 12:13:42.488451 47501039724352 call_variants.py:592] Use saved model: True. I0510 12:13:52.162126 47501039724352 dv_utils.py:370] From /opt/models/pacbio/example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0510 12:13:52.163805 47501039724352 dv_utils.py:370] From ./make_examples.tfrecord-00000-of-00008.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0510 12:13:56.551032 47501039724352 call_variants.py:716] Predicted 982 examples in 1 batches [0.419 sec per 100]. I0510 12:13:57.403082 47501039724352 call_variants.py:779] Complete: call_variants. real	0m21.581s. user	1m40.583s. sys	0m15.744s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""stdchroms.hg38.fa"" --infile ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/818
https://github.com/google/deepvariant/issues/818:2291,usability,input,input,2291,"ts_output.tfrecord.gz"" --examples ""./make_examples.tfrecord@8.gz"" --checkpoint ""/opt/models/pacbio"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). . For more information see: https://github.com/tensorflow/addons/issues/2807 . warnings.warn(. I0510 12:13:42.483308 47501039724352 call_variants.py:563] Total 1 writing processes started. I0510 12:13:42.487790 47501039724352 dv_utils.py:370] From ./make_examples.tfrecord-00000-of-00008.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0510 12:13:42.487916 47501039724352 call_variants.py:588] Shape of input examples: [100, 199, 9]. I0510 12:13:42.488451 47501039724352 call_variants.py:592] Use saved model: True. I0510 12:13:52.162126 47501039724352 dv_utils.py:370] From /opt/models/pacbio/example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0510 12:13:52.163805 47501039724352 dv_utils.py:370] From ./make_examples.tfrecord-00000-of-00008.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0510 12:13:56.551032 47501039724352 call_variants.py:716] Predicted 982 examples in 1 batches [0.419 sec per 100]. I0510 12:13:57.403082 47501039724352 call_variants.py:779] Complete: call_variants. real	0m21.581s. user	1m40.583s. sys	0m15.744s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""stdchroms.hg38.fa"" --infile ""./call_variants_output.tfrecord.gz"" --outfile ""./SAMPLENAME.deepVariant.vcf.gz"" --cpus ""8"" --gvcf_outfile ""./SAMP",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/818
https://github.com/google/deepvariant/issues/818:2510,usability,input,input,2510,"A) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). . For more information see: https://github.com/tensorflow/addons/issues/2807 . warnings.warn(. I0510 12:13:42.483308 47501039724352 call_variants.py:563] Total 1 writing processes started. I0510 12:13:42.487790 47501039724352 dv_utils.py:370] From ./make_examples.tfrecord-00000-of-00008.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0510 12:13:42.487916 47501039724352 call_variants.py:588] Shape of input examples: [100, 199, 9]. I0510 12:13:42.488451 47501039724352 call_variants.py:592] Use saved model: True. I0510 12:13:52.162126 47501039724352 dv_utils.py:370] From /opt/models/pacbio/example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0510 12:13:52.163805 47501039724352 dv_utils.py:370] From ./make_examples.tfrecord-00000-of-00008.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0510 12:13:56.551032 47501039724352 call_variants.py:716] Predicted 982 examples in 1 batches [0.419 sec per 100]. I0510 12:13:57.403082 47501039724352 call_variants.py:779] Complete: call_variants. real	0m21.581s. user	1m40.583s. sys	0m15.744s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""stdchroms.hg38.fa"" --infile ""./call_variants_output.tfrecord.gz"" --outfile ""./SAMPLENAME.deepVariant.vcf.gz"" --cpus ""8"" --gvcf_outfile ""./SAMPLENAME.deepVariant.g.vcf.gz"" --nonvariant_site_tfrecord_path ""./gvcf.tfrecord@8.gz"" --sample_name ""SAMPLENAME"". Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_0t8uq2zt/runfiles/com_google_deepvariant/deep",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/818
https://github.com/google/deepvariant/issues/818:2553,usability,input,input,2553,"f new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). . For more information see: https://github.com/tensorflow/addons/issues/2807 . warnings.warn(. I0510 12:13:42.483308 47501039724352 call_variants.py:563] Total 1 writing processes started. I0510 12:13:42.487790 47501039724352 dv_utils.py:370] From ./make_examples.tfrecord-00000-of-00008.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0510 12:13:42.487916 47501039724352 call_variants.py:588] Shape of input examples: [100, 199, 9]. I0510 12:13:42.488451 47501039724352 call_variants.py:592] Use saved model: True. I0510 12:13:52.162126 47501039724352 dv_utils.py:370] From /opt/models/pacbio/example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0510 12:13:52.163805 47501039724352 dv_utils.py:370] From ./make_examples.tfrecord-00000-of-00008.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0510 12:13:56.551032 47501039724352 call_variants.py:716] Predicted 982 examples in 1 batches [0.419 sec per 100]. I0510 12:13:57.403082 47501039724352 call_variants.py:779] Complete: call_variants. real	0m21.581s. user	1m40.583s. sys	0m15.744s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""stdchroms.hg38.fa"" --infile ""./call_variants_output.tfrecord.gz"" --outfile ""./SAMPLENAME.deepVariant.vcf.gz"" --cpus ""8"" --gvcf_outfile ""./SAMPLENAME.deepVariant.g.vcf.gz"" --nonvariant_site_tfrecord_path ""./gvcf.tfrecord@8.gz"" --sample_name ""SAMPLENAME"". Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_0t8uq2zt/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1419",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/818
https://github.com/google/deepvariant/issues/818:2729,usability,input,input,2729,"er repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). . For more information see: https://github.com/tensorflow/addons/issues/2807 . warnings.warn(. I0510 12:13:42.483308 47501039724352 call_variants.py:563] Total 1 writing processes started. I0510 12:13:42.487790 47501039724352 dv_utils.py:370] From ./make_examples.tfrecord-00000-of-00008.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0510 12:13:42.487916 47501039724352 call_variants.py:588] Shape of input examples: [100, 199, 9]. I0510 12:13:42.488451 47501039724352 call_variants.py:592] Use saved model: True. I0510 12:13:52.162126 47501039724352 dv_utils.py:370] From /opt/models/pacbio/example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0510 12:13:52.163805 47501039724352 dv_utils.py:370] From ./make_examples.tfrecord-00000-of-00008.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0510 12:13:56.551032 47501039724352 call_variants.py:716] Predicted 982 examples in 1 batches [0.419 sec per 100]. I0510 12:13:57.403082 47501039724352 call_variants.py:779] Complete: call_variants. real	0m21.581s. user	1m40.583s. sys	0m15.744s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""stdchroms.hg38.fa"" --infile ""./call_variants_output.tfrecord.gz"" --outfile ""./SAMPLENAME.deepVariant.vcf.gz"" --cpus ""8"" --gvcf_outfile ""./SAMPLENAME.deepVariant.g.vcf.gz"" --nonvariant_site_tfrecord_path ""./gvcf.tfrecord@8.gz"" --sample_name ""SAMPLENAME"". Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_0t8uq2zt/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1419, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_0t8uq2zt/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_0t8uq2zt/runf",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/818
https://github.com/google/deepvariant/issues/818:2772,usability,input,input,2772," (e.g. Keras, Keras-CV, and Keras-NLP). . For more information see: https://github.com/tensorflow/addons/issues/2807 . warnings.warn(. I0510 12:13:42.483308 47501039724352 call_variants.py:563] Total 1 writing processes started. I0510 12:13:42.487790 47501039724352 dv_utils.py:370] From ./make_examples.tfrecord-00000-of-00008.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0510 12:13:42.487916 47501039724352 call_variants.py:588] Shape of input examples: [100, 199, 9]. I0510 12:13:42.488451 47501039724352 call_variants.py:592] Use saved model: True. I0510 12:13:52.162126 47501039724352 dv_utils.py:370] From /opt/models/pacbio/example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0510 12:13:52.163805 47501039724352 dv_utils.py:370] From ./make_examples.tfrecord-00000-of-00008.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0510 12:13:56.551032 47501039724352 call_variants.py:716] Predicted 982 examples in 1 batches [0.419 sec per 100]. I0510 12:13:57.403082 47501039724352 call_variants.py:779] Complete: call_variants. real	0m21.581s. user	1m40.583s. sys	0m15.744s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""stdchroms.hg38.fa"" --infile ""./call_variants_output.tfrecord.gz"" --outfile ""./SAMPLENAME.deepVariant.vcf.gz"" --cpus ""8"" --gvcf_outfile ""./SAMPLENAME.deepVariant.g.vcf.gz"" --nonvariant_site_tfrecord_path ""./gvcf.tfrecord@8.gz"" --sample_name ""SAMPLENAME"". Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_0t8uq2zt/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1419, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_0t8uq2zt/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_0t8uq2zt/runfiles/absl_py/absl/app.py"", line 258, in _ru",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/818
https://github.com/google/deepvariant/issues/818:3034,usability,user,user,3034,"4352 dv_utils.py:370] From ./make_examples.tfrecord-00000-of-00008.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0510 12:13:42.487916 47501039724352 call_variants.py:588] Shape of input examples: [100, 199, 9]. I0510 12:13:42.488451 47501039724352 call_variants.py:592] Use saved model: True. I0510 12:13:52.162126 47501039724352 dv_utils.py:370] From /opt/models/pacbio/example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0510 12:13:52.163805 47501039724352 dv_utils.py:370] From ./make_examples.tfrecord-00000-of-00008.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0510 12:13:56.551032 47501039724352 call_variants.py:716] Predicted 982 examples in 1 batches [0.419 sec per 100]. I0510 12:13:57.403082 47501039724352 call_variants.py:779] Complete: call_variants. real	0m21.581s. user	1m40.583s. sys	0m15.744s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""stdchroms.hg38.fa"" --infile ""./call_variants_output.tfrecord.gz"" --outfile ""./SAMPLENAME.deepVariant.vcf.gz"" --cpus ""8"" --gvcf_outfile ""./SAMPLENAME.deepVariant.g.vcf.gz"" --nonvariant_site_tfrecord_path ""./gvcf.tfrecord@8.gz"" --sample_name ""SAMPLENAME"". Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_0t8uq2zt/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1419, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_0t8uq2zt/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_0t8uq2zt/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_0t8uq2zt/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1300, in main. sample_name = get_sample_name(). File ""/tmp/Bazel.runfiles_0t8uq2zt/runfiles/com_google_deepvariant/deepva",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/818
https://github.com/google/deepvariant/issues/818:3083,usability,command,command,3083,"-00000-of-00008.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0510 12:13:42.487916 47501039724352 call_variants.py:588] Shape of input examples: [100, 199, 9]. I0510 12:13:42.488451 47501039724352 call_variants.py:592] Use saved model: True. I0510 12:13:52.162126 47501039724352 dv_utils.py:370] From /opt/models/pacbio/example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0510 12:13:52.163805 47501039724352 dv_utils.py:370] From ./make_examples.tfrecord-00000-of-00008.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0510 12:13:56.551032 47501039724352 call_variants.py:716] Predicted 982 examples in 1 batches [0.419 sec per 100]. I0510 12:13:57.403082 47501039724352 call_variants.py:779] Complete: call_variants. real	0m21.581s. user	1m40.583s. sys	0m15.744s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""stdchroms.hg38.fa"" --infile ""./call_variants_output.tfrecord.gz"" --outfile ""./SAMPLENAME.deepVariant.vcf.gz"" --cpus ""8"" --gvcf_outfile ""./SAMPLENAME.deepVariant.g.vcf.gz"" --nonvariant_site_tfrecord_path ""./gvcf.tfrecord@8.gz"" --sample_name ""SAMPLENAME"". Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_0t8uq2zt/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1419, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_0t8uq2zt/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_0t8uq2zt/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_0t8uq2zt/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1300, in main. sample_name = get_sample_name(). File ""/tmp/Bazel.runfiles_0t8uq2zt/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1203, in get_s",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/818
https://github.com/google/deepvariant/issues/818:4361,usability,input,input,4361,"ls/pacbio/example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0510 12:13:52.163805 47501039724352 dv_utils.py:370] From ./make_examples.tfrecord-00000-of-00008.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0510 12:13:56.551032 47501039724352 call_variants.py:716] Predicted 982 examples in 1 batches [0.419 sec per 100]. I0510 12:13:57.403082 47501039724352 call_variants.py:779] Complete: call_variants. real	0m21.581s. user	1m40.583s. sys	0m15.744s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""stdchroms.hg38.fa"" --infile ""./call_variants_output.tfrecord.gz"" --outfile ""./SAMPLENAME.deepVariant.vcf.gz"" --cpus ""8"" --gvcf_outfile ""./SAMPLENAME.deepVariant.g.vcf.gz"" --nonvariant_site_tfrecord_path ""./gvcf.tfrecord@8.gz"" --sample_name ""SAMPLENAME"". Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_0t8uq2zt/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1419, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_0t8uq2zt/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_0t8uq2zt/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_0t8uq2zt/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1300, in main. sample_name = get_sample_name(). File ""/tmp/Bazel.runfiles_0t8uq2zt/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1203, in get_sample_name. _, record = get_cvo_paths_and_first_record(). File ""/tmp/Bazel.runfiles_0t8uq2zt/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1179, in get_cvo_paths_and_first_record. raise ValueError(. ValueError: ('Found multiple file patterns in input filename space: ', './call_variants_output.tfrecord.gz'). real	0m4.925s. user	0m8.815s. sys	0m7.379s. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/818
https://github.com/google/deepvariant/issues/818:4440,usability,user,user,4440,"ls/pacbio/example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0510 12:13:52.163805 47501039724352 dv_utils.py:370] From ./make_examples.tfrecord-00000-of-00008.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0510 12:13:56.551032 47501039724352 call_variants.py:716] Predicted 982 examples in 1 batches [0.419 sec per 100]. I0510 12:13:57.403082 47501039724352 call_variants.py:779] Complete: call_variants. real	0m21.581s. user	1m40.583s. sys	0m15.744s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""stdchroms.hg38.fa"" --infile ""./call_variants_output.tfrecord.gz"" --outfile ""./SAMPLENAME.deepVariant.vcf.gz"" --cpus ""8"" --gvcf_outfile ""./SAMPLENAME.deepVariant.g.vcf.gz"" --nonvariant_site_tfrecord_path ""./gvcf.tfrecord@8.gz"" --sample_name ""SAMPLENAME"". Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_0t8uq2zt/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1419, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_0t8uq2zt/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_0t8uq2zt/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_0t8uq2zt/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1300, in main. sample_name = get_sample_name(). File ""/tmp/Bazel.runfiles_0t8uq2zt/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1203, in get_sample_name. _, record = get_cvo_paths_and_first_record(). File ""/tmp/Bazel.runfiles_0t8uq2zt/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1179, in get_cvo_paths_and_first_record. raise ValueError(. ValueError: ('Found multiple file patterns in input filename space: ', './call_variants_output.tfrecord.gz'). real	0m4.925s. user	0m8.815s. sys	0m7.379s. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/818
https://github.com/google/deepvariant/issues/818:86,modifiability,interm,intermediate,86,"@MiWitt ,. Given that you are using `--intermediate_results_dir . \` which writes all intermediate files to your directory, if you run the same command multiple times then it will create multiple patterns. Can you please create a clean intermediate directory and use that for `--intermediate_results_dir /path/to/intermediate_dir`? That should resolve the issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/818
https://github.com/google/deepvariant/issues/818:236,modifiability,interm,intermediate,236,"@MiWitt ,. Given that you are using `--intermediate_results_dir . \` which writes all intermediate files to your directory, if you run the same command multiple times then it will create multiple patterns. Can you please create a clean intermediate directory and use that for `--intermediate_results_dir /path/to/intermediate_dir`? That should resolve the issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/818
https://github.com/google/deepvariant/issues/818:161,performance,time,times,161,"@MiWitt ,. Given that you are using `--intermediate_results_dir . \` which writes all intermediate files to your directory, if you run the same command multiple times then it will create multiple patterns. Can you please create a clean intermediate directory and use that for `--intermediate_results_dir /path/to/intermediate_dir`? That should resolve the issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/818
https://github.com/google/deepvariant/issues/818:144,usability,command,command,144,"@MiWitt ,. Given that you are using `--intermediate_results_dir . \` which writes all intermediate files to your directory, if you run the same command multiple times then it will create multiple patterns. Can you please create a clean intermediate directory and use that for `--intermediate_results_dir /path/to/intermediate_dir`? That should resolve the issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/818
https://github.com/google/deepvariant/issues/818:45,availability,cluster,cluster,45,"This can not be the point. I am working in a cluster environment using slurm and the dir ""."" is the job specific scratch dir, which is located at ""/scratch/SlurmTMP/JobSpecificFolder"" (${TMPDIR}). ```. cd ${TMPDIR}. BIN_VERSION=""1.6.1"". module load singularity/3.5.2. #####################################################################. # singularity pull docker://google/deepvariant:""${BIN_VERSION}"". ulimit -u 10000 # https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable/54746150#54746150. # --model_type=PACBIO \ ##Replace this string with exactly one of the following [WGS,WES,PACBIO,HYBRID_PACBIO_ILLUMINA]**. # docker://google/deepvariant:""${BIN_VERSION}"" \. if ! [ -f ""${WORKINDIR}/${ALIGNMENTNAME}.deepVariant.vcf.gz"" ]. then. cp ""${THEREF}""* ./. cp ""${WORKINDIR}/${ALIGNMENTNAME}.bam""* . chmod 666 `basename ""${THEREF}""`*. chmod 666 ""${ALIGNMENTNAME}.bam""*. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. /my/path/software/deepVariant/deepvariant_${BIN_VERSION}.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=`basename ""${THEREF}""` \. --reads=""${ALIGNMENTNAME}.bam"" \. --sample_name=${SAMPLENAME} \. --output_vcf=""./${ALIGNMENTNAME}.deepVariant.vcf.gz"" \. --output_gvcf=""./${ALIGNMENTNAME}.deepVariant.g.vcf.gz"" \. --intermediate_results_dir . \. --num_shards=8 \. --logging_dir=. . if ! [ -f ""./${ALIGNMENTNAME}.deepVariant.vcf.gz"" ]. then. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. /my/path/software/deepVariant/deepvariant_${BIN_VERSION}.sif \. /opt/deepvariant/bin/postprocess_variants \. --ref=`basename ""${THEREF}""` \. --infile ""./call_variants_output@$(ls ./call_variants_output*.tfrecord.gz | wc -l).tfrecord.gz"" \. --outfile ""./${ALIGNMENTNAME}.deepVariant.vcf.gz"" \. --cpus ""8"" \. --gvcf_outfile ""./${ALIGNMENTNAME}.deepVariant.g.vcf.gz"" \. --nonvariant_site_tfrecord_path ""./gvcf.tfrecord@$(ls ./gvcf.tfrecord*.gz | wc -l).gz"" \. --sample_name=${SAMPLENAME}. f",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/818
https://github.com/google/deepvariant/issues/818:529,availability,unavail,unavailable,529,"This can not be the point. I am working in a cluster environment using slurm and the dir ""."" is the job specific scratch dir, which is located at ""/scratch/SlurmTMP/JobSpecificFolder"" (${TMPDIR}). ```. cd ${TMPDIR}. BIN_VERSION=""1.6.1"". module load singularity/3.5.2. #####################################################################. # singularity pull docker://google/deepvariant:""${BIN_VERSION}"". ulimit -u 10000 # https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable/54746150#54746150. # --model_type=PACBIO \ ##Replace this string with exactly one of the following [WGS,WES,PACBIO,HYBRID_PACBIO_ILLUMINA]**. # docker://google/deepvariant:""${BIN_VERSION}"" \. if ! [ -f ""${WORKINDIR}/${ALIGNMENTNAME}.deepVariant.vcf.gz"" ]. then. cp ""${THEREF}""* ./. cp ""${WORKINDIR}/${ALIGNMENTNAME}.bam""* . chmod 666 `basename ""${THEREF}""`*. chmod 666 ""${ALIGNMENTNAME}.bam""*. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. /my/path/software/deepVariant/deepvariant_${BIN_VERSION}.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=`basename ""${THEREF}""` \. --reads=""${ALIGNMENTNAME}.bam"" \. --sample_name=${SAMPLENAME} \. --output_vcf=""./${ALIGNMENTNAME}.deepVariant.vcf.gz"" \. --output_gvcf=""./${ALIGNMENTNAME}.deepVariant.g.vcf.gz"" \. --intermediate_results_dir . \. --num_shards=8 \. --logging_dir=. . if ! [ -f ""./${ALIGNMENTNAME}.deepVariant.vcf.gz"" ]. then. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. /my/path/software/deepVariant/deepvariant_${BIN_VERSION}.sif \. /opt/deepvariant/bin/postprocess_variants \. --ref=`basename ""${THEREF}""` \. --infile ""./call_variants_output@$(ls ./call_variants_output*.tfrecord.gz | wc -l).tfrecord.gz"" \. --outfile ""./${ALIGNMENTNAME}.deepVariant.vcf.gz"" \. --cpus ""8"" \. --gvcf_outfile ""./${ALIGNMENTNAME}.deepVariant.g.vcf.gz"" \. --nonvariant_site_tfrecord_path ""./gvcf.tfrecord@$(ls ./gvcf.tfrecord*.gz | wc -l).gz"" \. --sample_name=${SAMPLENAME}. f",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/818
https://github.com/google/deepvariant/issues/818:45,deployability,cluster,cluster,45,"This can not be the point. I am working in a cluster environment using slurm and the dir ""."" is the job specific scratch dir, which is located at ""/scratch/SlurmTMP/JobSpecificFolder"" (${TMPDIR}). ```. cd ${TMPDIR}. BIN_VERSION=""1.6.1"". module load singularity/3.5.2. #####################################################################. # singularity pull docker://google/deepvariant:""${BIN_VERSION}"". ulimit -u 10000 # https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable/54746150#54746150. # --model_type=PACBIO \ ##Replace this string with exactly one of the following [WGS,WES,PACBIO,HYBRID_PACBIO_ILLUMINA]**. # docker://google/deepvariant:""${BIN_VERSION}"" \. if ! [ -f ""${WORKINDIR}/${ALIGNMENTNAME}.deepVariant.vcf.gz"" ]. then. cp ""${THEREF}""* ./. cp ""${WORKINDIR}/${ALIGNMENTNAME}.bam""* . chmod 666 `basename ""${THEREF}""`*. chmod 666 ""${ALIGNMENTNAME}.bam""*. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. /my/path/software/deepVariant/deepvariant_${BIN_VERSION}.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=`basename ""${THEREF}""` \. --reads=""${ALIGNMENTNAME}.bam"" \. --sample_name=${SAMPLENAME} \. --output_vcf=""./${ALIGNMENTNAME}.deepVariant.vcf.gz"" \. --output_gvcf=""./${ALIGNMENTNAME}.deepVariant.g.vcf.gz"" \. --intermediate_results_dir . \. --num_shards=8 \. --logging_dir=. . if ! [ -f ""./${ALIGNMENTNAME}.deepVariant.vcf.gz"" ]. then. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. /my/path/software/deepVariant/deepvariant_${BIN_VERSION}.sif \. /opt/deepvariant/bin/postprocess_variants \. --ref=`basename ""${THEREF}""` \. --infile ""./call_variants_output@$(ls ./call_variants_output*.tfrecord.gz | wc -l).tfrecord.gz"" \. --outfile ""./${ALIGNMENTNAME}.deepVariant.vcf.gz"" \. --cpus ""8"" \. --gvcf_outfile ""./${ALIGNMENTNAME}.deepVariant.g.vcf.gz"" \. --nonvariant_site_tfrecord_path ""./gvcf.tfrecord@$(ls ./gvcf.tfrecord*.gz | wc -l).gz"" \. --sample_name=${SAMPLENAME}. f",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/818
https://github.com/google/deepvariant/issues/818:237,deployability,modul,module,237,"This can not be the point. I am working in a cluster environment using slurm and the dir ""."" is the job specific scratch dir, which is located at ""/scratch/SlurmTMP/JobSpecificFolder"" (${TMPDIR}). ```. cd ${TMPDIR}. BIN_VERSION=""1.6.1"". module load singularity/3.5.2. #####################################################################. # singularity pull docker://google/deepvariant:""${BIN_VERSION}"". ulimit -u 10000 # https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable/54746150#54746150. # --model_type=PACBIO \ ##Replace this string with exactly one of the following [WGS,WES,PACBIO,HYBRID_PACBIO_ILLUMINA]**. # docker://google/deepvariant:""${BIN_VERSION}"" \. if ! [ -f ""${WORKINDIR}/${ALIGNMENTNAME}.deepVariant.vcf.gz"" ]. then. cp ""${THEREF}""* ./. cp ""${WORKINDIR}/${ALIGNMENTNAME}.bam""* . chmod 666 `basename ""${THEREF}""`*. chmod 666 ""${ALIGNMENTNAME}.bam""*. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. /my/path/software/deepVariant/deepvariant_${BIN_VERSION}.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=`basename ""${THEREF}""` \. --reads=""${ALIGNMENTNAME}.bam"" \. --sample_name=${SAMPLENAME} \. --output_vcf=""./${ALIGNMENTNAME}.deepVariant.vcf.gz"" \. --output_gvcf=""./${ALIGNMENTNAME}.deepVariant.g.vcf.gz"" \. --intermediate_results_dir . \. --num_shards=8 \. --logging_dir=. . if ! [ -f ""./${ALIGNMENTNAME}.deepVariant.vcf.gz"" ]. then. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. /my/path/software/deepVariant/deepvariant_${BIN_VERSION}.sif \. /opt/deepvariant/bin/postprocess_variants \. --ref=`basename ""${THEREF}""` \. --infile ""./call_variants_output@$(ls ./call_variants_output*.tfrecord.gz | wc -l).tfrecord.gz"" \. --outfile ""./${ALIGNMENTNAME}.deepVariant.vcf.gz"" \. --cpus ""8"" \. --gvcf_outfile ""./${ALIGNMENTNAME}.deepVariant.g.vcf.gz"" \. --nonvariant_site_tfrecord_path ""./gvcf.tfrecord@$(ls ./gvcf.tfrecord*.gz | wc -l).gz"" \. --sample_name=${SAMPLENAME}. f",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/818
https://github.com/google/deepvariant/issues/818:430,deployability,stack,stackoverflow,430,"This can not be the point. I am working in a cluster environment using slurm and the dir ""."" is the job specific scratch dir, which is located at ""/scratch/SlurmTMP/JobSpecificFolder"" (${TMPDIR}). ```. cd ${TMPDIR}. BIN_VERSION=""1.6.1"". module load singularity/3.5.2. #####################################################################. # singularity pull docker://google/deepvariant:""${BIN_VERSION}"". ulimit -u 10000 # https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable/54746150#54746150. # --model_type=PACBIO \ ##Replace this string with exactly one of the following [WGS,WES,PACBIO,HYBRID_PACBIO_ILLUMINA]**. # docker://google/deepvariant:""${BIN_VERSION}"" \. if ! [ -f ""${WORKINDIR}/${ALIGNMENTNAME}.deepVariant.vcf.gz"" ]. then. cp ""${THEREF}""* ./. cp ""${WORKINDIR}/${ALIGNMENTNAME}.bam""* . chmod 666 `basename ""${THEREF}""`*. chmod 666 ""${ALIGNMENTNAME}.bam""*. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. /my/path/software/deepVariant/deepvariant_${BIN_VERSION}.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=`basename ""${THEREF}""` \. --reads=""${ALIGNMENTNAME}.bam"" \. --sample_name=${SAMPLENAME} \. --output_vcf=""./${ALIGNMENTNAME}.deepVariant.vcf.gz"" \. --output_gvcf=""./${ALIGNMENTNAME}.deepVariant.g.vcf.gz"" \. --intermediate_results_dir . \. --num_shards=8 \. --logging_dir=. . if ! [ -f ""./${ALIGNMENTNAME}.deepVariant.vcf.gz"" ]. then. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. /my/path/software/deepVariant/deepvariant_${BIN_VERSION}.sif \. /opt/deepvariant/bin/postprocess_variants \. --ref=`basename ""${THEREF}""` \. --infile ""./call_variants_output@$(ls ./call_variants_output*.tfrecord.gz | wc -l).tfrecord.gz"" \. --outfile ""./${ALIGNMENTNAME}.deepVariant.vcf.gz"" \. --cpus ""8"" \. --gvcf_outfile ""./${ALIGNMENTNAME}.deepVariant.g.vcf.gz"" \. --nonvariant_site_tfrecord_path ""./gvcf.tfrecord@$(ls ./gvcf.tfrecord*.gz | wc -l).gz"" \. --sample_name=${SAMPLENAME}. f",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/818
https://github.com/google/deepvariant/issues/818:508,deployability,resourc,resource-temporarily-unavailable,508,"This can not be the point. I am working in a cluster environment using slurm and the dir ""."" is the job specific scratch dir, which is located at ""/scratch/SlurmTMP/JobSpecificFolder"" (${TMPDIR}). ```. cd ${TMPDIR}. BIN_VERSION=""1.6.1"". module load singularity/3.5.2. #####################################################################. # singularity pull docker://google/deepvariant:""${BIN_VERSION}"". ulimit -u 10000 # https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable/54746150#54746150. # --model_type=PACBIO \ ##Replace this string with exactly one of the following [WGS,WES,PACBIO,HYBRID_PACBIO_ILLUMINA]**. # docker://google/deepvariant:""${BIN_VERSION}"" \. if ! [ -f ""${WORKINDIR}/${ALIGNMENTNAME}.deepVariant.vcf.gz"" ]. then. cp ""${THEREF}""* ./. cp ""${WORKINDIR}/${ALIGNMENTNAME}.bam""* . chmod 666 `basename ""${THEREF}""`*. chmod 666 ""${ALIGNMENTNAME}.bam""*. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. /my/path/software/deepVariant/deepvariant_${BIN_VERSION}.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=`basename ""${THEREF}""` \. --reads=""${ALIGNMENTNAME}.bam"" \. --sample_name=${SAMPLENAME} \. --output_vcf=""./${ALIGNMENTNAME}.deepVariant.vcf.gz"" \. --output_gvcf=""./${ALIGNMENTNAME}.deepVariant.g.vcf.gz"" \. --intermediate_results_dir . \. --num_shards=8 \. --logging_dir=. . if ! [ -f ""./${ALIGNMENTNAME}.deepVariant.vcf.gz"" ]. then. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. /my/path/software/deepVariant/deepvariant_${BIN_VERSION}.sif \. /opt/deepvariant/bin/postprocess_variants \. --ref=`basename ""${THEREF}""` \. --infile ""./call_variants_output@$(ls ./call_variants_output*.tfrecord.gz | wc -l).tfrecord.gz"" \. --outfile ""./${ALIGNMENTNAME}.deepVariant.vcf.gz"" \. --cpus ""8"" \. --gvcf_outfile ""./${ALIGNMENTNAME}.deepVariant.g.vcf.gz"" \. --nonvariant_site_tfrecord_path ""./gvcf.tfrecord@$(ls ./gvcf.tfrecord*.gz | wc -l).gz"" \. --sample_name=${SAMPLENAME}. f",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/818
https://github.com/google/deepvariant/issues/818:2008,deployability,log,log,2008,"SlurmTMP/JobSpecificFolder"" (${TMPDIR}). ```. cd ${TMPDIR}. BIN_VERSION=""1.6.1"". module load singularity/3.5.2. #####################################################################. # singularity pull docker://google/deepvariant:""${BIN_VERSION}"". ulimit -u 10000 # https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable/54746150#54746150. # --model_type=PACBIO \ ##Replace this string with exactly one of the following [WGS,WES,PACBIO,HYBRID_PACBIO_ILLUMINA]**. # docker://google/deepvariant:""${BIN_VERSION}"" \. if ! [ -f ""${WORKINDIR}/${ALIGNMENTNAME}.deepVariant.vcf.gz"" ]. then. cp ""${THEREF}""* ./. cp ""${WORKINDIR}/${ALIGNMENTNAME}.bam""* . chmod 666 `basename ""${THEREF}""`*. chmod 666 ""${ALIGNMENTNAME}.bam""*. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. /my/path/software/deepVariant/deepvariant_${BIN_VERSION}.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=`basename ""${THEREF}""` \. --reads=""${ALIGNMENTNAME}.bam"" \. --sample_name=${SAMPLENAME} \. --output_vcf=""./${ALIGNMENTNAME}.deepVariant.vcf.gz"" \. --output_gvcf=""./${ALIGNMENTNAME}.deepVariant.g.vcf.gz"" \. --intermediate_results_dir . \. --num_shards=8 \. --logging_dir=. . if ! [ -f ""./${ALIGNMENTNAME}.deepVariant.vcf.gz"" ]. then. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. /my/path/software/deepVariant/deepvariant_${BIN_VERSION}.sif \. /opt/deepvariant/bin/postprocess_variants \. --ref=`basename ""${THEREF}""` \. --infile ""./call_variants_output@$(ls ./call_variants_output*.tfrecord.gz | wc -l).tfrecord.gz"" \. --outfile ""./${ALIGNMENTNAME}.deepVariant.vcf.gz"" \. --cpus ""8"" \. --gvcf_outfile ""./${ALIGNMENTNAME}.deepVariant.g.vcf.gz"" \. --nonvariant_site_tfrecord_path ""./gvcf.tfrecord@$(ls ./gvcf.tfrecord*.gz | wc -l).gz"" \. --sample_name=${SAMPLENAME}. fi. cp *.log ${WORKINDIR}/. cp ""./${ALIGNMENTNAME}.deepVariant.vcf.gz""* ${WORKINDIR}/. else. cp ""${WORKINDIR}/${ALIGNMENTNAME}.deepVariant.vcf.gz""* . fi. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/818
https://github.com/google/deepvariant/issues/818:244,energy efficiency,load,load,244,"This can not be the point. I am working in a cluster environment using slurm and the dir ""."" is the job specific scratch dir, which is located at ""/scratch/SlurmTMP/JobSpecificFolder"" (${TMPDIR}). ```. cd ${TMPDIR}. BIN_VERSION=""1.6.1"". module load singularity/3.5.2. #####################################################################. # singularity pull docker://google/deepvariant:""${BIN_VERSION}"". ulimit -u 10000 # https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable/54746150#54746150. # --model_type=PACBIO \ ##Replace this string with exactly one of the following [WGS,WES,PACBIO,HYBRID_PACBIO_ILLUMINA]**. # docker://google/deepvariant:""${BIN_VERSION}"" \. if ! [ -f ""${WORKINDIR}/${ALIGNMENTNAME}.deepVariant.vcf.gz"" ]. then. cp ""${THEREF}""* ./. cp ""${WORKINDIR}/${ALIGNMENTNAME}.bam""* . chmod 666 `basename ""${THEREF}""`*. chmod 666 ""${ALIGNMENTNAME}.bam""*. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. /my/path/software/deepVariant/deepvariant_${BIN_VERSION}.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=`basename ""${THEREF}""` \. --reads=""${ALIGNMENTNAME}.bam"" \. --sample_name=${SAMPLENAME} \. --output_vcf=""./${ALIGNMENTNAME}.deepVariant.vcf.gz"" \. --output_gvcf=""./${ALIGNMENTNAME}.deepVariant.g.vcf.gz"" \. --intermediate_results_dir . \. --num_shards=8 \. --logging_dir=. . if ! [ -f ""./${ALIGNMENTNAME}.deepVariant.vcf.gz"" ]. then. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. /my/path/software/deepVariant/deepvariant_${BIN_VERSION}.sif \. /opt/deepvariant/bin/postprocess_variants \. --ref=`basename ""${THEREF}""` \. --infile ""./call_variants_output@$(ls ./call_variants_output*.tfrecord.gz | wc -l).tfrecord.gz"" \. --outfile ""./${ALIGNMENTNAME}.deepVariant.vcf.gz"" \. --cpus ""8"" \. --gvcf_outfile ""./${ALIGNMENTNAME}.deepVariant.g.vcf.gz"" \. --nonvariant_site_tfrecord_path ""./gvcf.tfrecord@$(ls ./gvcf.tfrecord*.gz | wc -l).gz"" \. --sample_name=${SAMPLENAME}. f",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/818
https://github.com/google/deepvariant/issues/818:508,energy efficiency,resourc,resource-temporarily-unavailable,508,"This can not be the point. I am working in a cluster environment using slurm and the dir ""."" is the job specific scratch dir, which is located at ""/scratch/SlurmTMP/JobSpecificFolder"" (${TMPDIR}). ```. cd ${TMPDIR}. BIN_VERSION=""1.6.1"". module load singularity/3.5.2. #####################################################################. # singularity pull docker://google/deepvariant:""${BIN_VERSION}"". ulimit -u 10000 # https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable/54746150#54746150. # --model_type=PACBIO \ ##Replace this string with exactly one of the following [WGS,WES,PACBIO,HYBRID_PACBIO_ILLUMINA]**. # docker://google/deepvariant:""${BIN_VERSION}"" \. if ! [ -f ""${WORKINDIR}/${ALIGNMENTNAME}.deepVariant.vcf.gz"" ]. then. cp ""${THEREF}""* ./. cp ""${WORKINDIR}/${ALIGNMENTNAME}.bam""* . chmod 666 `basename ""${THEREF}""`*. chmod 666 ""${ALIGNMENTNAME}.bam""*. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. /my/path/software/deepVariant/deepvariant_${BIN_VERSION}.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=`basename ""${THEREF}""` \. --reads=""${ALIGNMENTNAME}.bam"" \. --sample_name=${SAMPLENAME} \. --output_vcf=""./${ALIGNMENTNAME}.deepVariant.vcf.gz"" \. --output_gvcf=""./${ALIGNMENTNAME}.deepVariant.g.vcf.gz"" \. --intermediate_results_dir . \. --num_shards=8 \. --logging_dir=. . if ! [ -f ""./${ALIGNMENTNAME}.deepVariant.vcf.gz"" ]. then. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. /my/path/software/deepVariant/deepvariant_${BIN_VERSION}.sif \. /opt/deepvariant/bin/postprocess_variants \. --ref=`basename ""${THEREF}""` \. --infile ""./call_variants_output@$(ls ./call_variants_output*.tfrecord.gz | wc -l).tfrecord.gz"" \. --outfile ""./${ALIGNMENTNAME}.deepVariant.vcf.gz"" \. --cpus ""8"" \. --gvcf_outfile ""./${ALIGNMENTNAME}.deepVariant.g.vcf.gz"" \. --nonvariant_site_tfrecord_path ""./gvcf.tfrecord@$(ls ./gvcf.tfrecord*.gz | wc -l).gz"" \. --sample_name=${SAMPLENAME}. f",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/818
https://github.com/google/deepvariant/issues/818:1808,energy efficiency,cpu,cpus,1808,"SlurmTMP/JobSpecificFolder"" (${TMPDIR}). ```. cd ${TMPDIR}. BIN_VERSION=""1.6.1"". module load singularity/3.5.2. #####################################################################. # singularity pull docker://google/deepvariant:""${BIN_VERSION}"". ulimit -u 10000 # https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable/54746150#54746150. # --model_type=PACBIO \ ##Replace this string with exactly one of the following [WGS,WES,PACBIO,HYBRID_PACBIO_ILLUMINA]**. # docker://google/deepvariant:""${BIN_VERSION}"" \. if ! [ -f ""${WORKINDIR}/${ALIGNMENTNAME}.deepVariant.vcf.gz"" ]. then. cp ""${THEREF}""* ./. cp ""${WORKINDIR}/${ALIGNMENTNAME}.bam""* . chmod 666 `basename ""${THEREF}""`*. chmod 666 ""${ALIGNMENTNAME}.bam""*. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. /my/path/software/deepVariant/deepvariant_${BIN_VERSION}.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=`basename ""${THEREF}""` \. --reads=""${ALIGNMENTNAME}.bam"" \. --sample_name=${SAMPLENAME} \. --output_vcf=""./${ALIGNMENTNAME}.deepVariant.vcf.gz"" \. --output_gvcf=""./${ALIGNMENTNAME}.deepVariant.g.vcf.gz"" \. --intermediate_results_dir . \. --num_shards=8 \. --logging_dir=. . if ! [ -f ""./${ALIGNMENTNAME}.deepVariant.vcf.gz"" ]. then. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. /my/path/software/deepVariant/deepvariant_${BIN_VERSION}.sif \. /opt/deepvariant/bin/postprocess_variants \. --ref=`basename ""${THEREF}""` \. --infile ""./call_variants_output@$(ls ./call_variants_output*.tfrecord.gz | wc -l).tfrecord.gz"" \. --outfile ""./${ALIGNMENTNAME}.deepVariant.vcf.gz"" \. --cpus ""8"" \. --gvcf_outfile ""./${ALIGNMENTNAME}.deepVariant.g.vcf.gz"" \. --nonvariant_site_tfrecord_path ""./gvcf.tfrecord@$(ls ./gvcf.tfrecord*.gz | wc -l).gz"" \. --sample_name=${SAMPLENAME}. fi. cp *.log ${WORKINDIR}/. cp ""./${ALIGNMENTNAME}.deepVariant.vcf.gz""* ${WORKINDIR}/. else. cp ""${WORKINDIR}/${ALIGNMENTNAME}.deepVariant.vcf.gz""* . fi. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/818
https://github.com/google/deepvariant/issues/818:104,interoperability,specif,specific,104,"This can not be the point. I am working in a cluster environment using slurm and the dir ""."" is the job specific scratch dir, which is located at ""/scratch/SlurmTMP/JobSpecificFolder"" (${TMPDIR}). ```. cd ${TMPDIR}. BIN_VERSION=""1.6.1"". module load singularity/3.5.2. #####################################################################. # singularity pull docker://google/deepvariant:""${BIN_VERSION}"". ulimit -u 10000 # https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable/54746150#54746150. # --model_type=PACBIO \ ##Replace this string with exactly one of the following [WGS,WES,PACBIO,HYBRID_PACBIO_ILLUMINA]**. # docker://google/deepvariant:""${BIN_VERSION}"" \. if ! [ -f ""${WORKINDIR}/${ALIGNMENTNAME}.deepVariant.vcf.gz"" ]. then. cp ""${THEREF}""* ./. cp ""${WORKINDIR}/${ALIGNMENTNAME}.bam""* . chmod 666 `basename ""${THEREF}""`*. chmod 666 ""${ALIGNMENTNAME}.bam""*. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. /my/path/software/deepVariant/deepvariant_${BIN_VERSION}.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=`basename ""${THEREF}""` \. --reads=""${ALIGNMENTNAME}.bam"" \. --sample_name=${SAMPLENAME} \. --output_vcf=""./${ALIGNMENTNAME}.deepVariant.vcf.gz"" \. --output_gvcf=""./${ALIGNMENTNAME}.deepVariant.g.vcf.gz"" \. --intermediate_results_dir . \. --num_shards=8 \. --logging_dir=. . if ! [ -f ""./${ALIGNMENTNAME}.deepVariant.vcf.gz"" ]. then. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. /my/path/software/deepVariant/deepvariant_${BIN_VERSION}.sif \. /opt/deepvariant/bin/postprocess_variants \. --ref=`basename ""${THEREF}""` \. --infile ""./call_variants_output@$(ls ./call_variants_output*.tfrecord.gz | wc -l).tfrecord.gz"" \. --outfile ""./${ALIGNMENTNAME}.deepVariant.vcf.gz"" \. --cpus ""8"" \. --gvcf_outfile ""./${ALIGNMENTNAME}.deepVariant.g.vcf.gz"" \. --nonvariant_site_tfrecord_path ""./gvcf.tfrecord@$(ls ./gvcf.tfrecord*.gz | wc -l).gz"" \. --sample_name=${SAMPLENAME}. f",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/818
https://github.com/google/deepvariant/issues/818:237,modifiability,modul,module,237,"This can not be the point. I am working in a cluster environment using slurm and the dir ""."" is the job specific scratch dir, which is located at ""/scratch/SlurmTMP/JobSpecificFolder"" (${TMPDIR}). ```. cd ${TMPDIR}. BIN_VERSION=""1.6.1"". module load singularity/3.5.2. #####################################################################. # singularity pull docker://google/deepvariant:""${BIN_VERSION}"". ulimit -u 10000 # https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable/54746150#54746150. # --model_type=PACBIO \ ##Replace this string with exactly one of the following [WGS,WES,PACBIO,HYBRID_PACBIO_ILLUMINA]**. # docker://google/deepvariant:""${BIN_VERSION}"" \. if ! [ -f ""${WORKINDIR}/${ALIGNMENTNAME}.deepVariant.vcf.gz"" ]. then. cp ""${THEREF}""* ./. cp ""${WORKINDIR}/${ALIGNMENTNAME}.bam""* . chmod 666 `basename ""${THEREF}""`*. chmod 666 ""${ALIGNMENTNAME}.bam""*. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. /my/path/software/deepVariant/deepvariant_${BIN_VERSION}.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=`basename ""${THEREF}""` \. --reads=""${ALIGNMENTNAME}.bam"" \. --sample_name=${SAMPLENAME} \. --output_vcf=""./${ALIGNMENTNAME}.deepVariant.vcf.gz"" \. --output_gvcf=""./${ALIGNMENTNAME}.deepVariant.g.vcf.gz"" \. --intermediate_results_dir . \. --num_shards=8 \. --logging_dir=. . if ! [ -f ""./${ALIGNMENTNAME}.deepVariant.vcf.gz"" ]. then. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. /my/path/software/deepVariant/deepvariant_${BIN_VERSION}.sif \. /opt/deepvariant/bin/postprocess_variants \. --ref=`basename ""${THEREF}""` \. --infile ""./call_variants_output@$(ls ./call_variants_output*.tfrecord.gz | wc -l).tfrecord.gz"" \. --outfile ""./${ALIGNMENTNAME}.deepVariant.vcf.gz"" \. --cpus ""8"" \. --gvcf_outfile ""./${ALIGNMENTNAME}.deepVariant.g.vcf.gz"" \. --nonvariant_site_tfrecord_path ""./gvcf.tfrecord@$(ls ./gvcf.tfrecord*.gz | wc -l).gz"" \. --sample_name=${SAMPLENAME}. f",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/818
https://github.com/google/deepvariant/issues/818:575,modifiability,PAC,PACBIO,575,"This can not be the point. I am working in a cluster environment using slurm and the dir ""."" is the job specific scratch dir, which is located at ""/scratch/SlurmTMP/JobSpecificFolder"" (${TMPDIR}). ```. cd ${TMPDIR}. BIN_VERSION=""1.6.1"". module load singularity/3.5.2. #####################################################################. # singularity pull docker://google/deepvariant:""${BIN_VERSION}"". ulimit -u 10000 # https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable/54746150#54746150. # --model_type=PACBIO \ ##Replace this string with exactly one of the following [WGS,WES,PACBIO,HYBRID_PACBIO_ILLUMINA]**. # docker://google/deepvariant:""${BIN_VERSION}"" \. if ! [ -f ""${WORKINDIR}/${ALIGNMENTNAME}.deepVariant.vcf.gz"" ]. then. cp ""${THEREF}""* ./. cp ""${WORKINDIR}/${ALIGNMENTNAME}.bam""* . chmod 666 `basename ""${THEREF}""`*. chmod 666 ""${ALIGNMENTNAME}.bam""*. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. /my/path/software/deepVariant/deepvariant_${BIN_VERSION}.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=`basename ""${THEREF}""` \. --reads=""${ALIGNMENTNAME}.bam"" \. --sample_name=${SAMPLENAME} \. --output_vcf=""./${ALIGNMENTNAME}.deepVariant.vcf.gz"" \. --output_gvcf=""./${ALIGNMENTNAME}.deepVariant.g.vcf.gz"" \. --intermediate_results_dir . \. --num_shards=8 \. --logging_dir=. . if ! [ -f ""./${ALIGNMENTNAME}.deepVariant.vcf.gz"" ]. then. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. /my/path/software/deepVariant/deepvariant_${BIN_VERSION}.sif \. /opt/deepvariant/bin/postprocess_variants \. --ref=`basename ""${THEREF}""` \. --infile ""./call_variants_output@$(ls ./call_variants_output*.tfrecord.gz | wc -l).tfrecord.gz"" \. --outfile ""./${ALIGNMENTNAME}.deepVariant.vcf.gz"" \. --cpus ""8"" \. --gvcf_outfile ""./${ALIGNMENTNAME}.deepVariant.g.vcf.gz"" \. --nonvariant_site_tfrecord_path ""./gvcf.tfrecord@$(ls ./gvcf.tfrecord*.gz | wc -l).gz"" \. --sample_name=${SAMPLENAME}. f",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/818
https://github.com/google/deepvariant/issues/818:649,modifiability,PAC,PACBIO,649,"This can not be the point. I am working in a cluster environment using slurm and the dir ""."" is the job specific scratch dir, which is located at ""/scratch/SlurmTMP/JobSpecificFolder"" (${TMPDIR}). ```. cd ${TMPDIR}. BIN_VERSION=""1.6.1"". module load singularity/3.5.2. #####################################################################. # singularity pull docker://google/deepvariant:""${BIN_VERSION}"". ulimit -u 10000 # https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable/54746150#54746150. # --model_type=PACBIO \ ##Replace this string with exactly one of the following [WGS,WES,PACBIO,HYBRID_PACBIO_ILLUMINA]**. # docker://google/deepvariant:""${BIN_VERSION}"" \. if ! [ -f ""${WORKINDIR}/${ALIGNMENTNAME}.deepVariant.vcf.gz"" ]. then. cp ""${THEREF}""* ./. cp ""${WORKINDIR}/${ALIGNMENTNAME}.bam""* . chmod 666 `basename ""${THEREF}""`*. chmod 666 ""${ALIGNMENTNAME}.bam""*. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. /my/path/software/deepVariant/deepvariant_${BIN_VERSION}.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=`basename ""${THEREF}""` \. --reads=""${ALIGNMENTNAME}.bam"" \. --sample_name=${SAMPLENAME} \. --output_vcf=""./${ALIGNMENTNAME}.deepVariant.vcf.gz"" \. --output_gvcf=""./${ALIGNMENTNAME}.deepVariant.g.vcf.gz"" \. --intermediate_results_dir . \. --num_shards=8 \. --logging_dir=. . if ! [ -f ""./${ALIGNMENTNAME}.deepVariant.vcf.gz"" ]. then. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. /my/path/software/deepVariant/deepvariant_${BIN_VERSION}.sif \. /opt/deepvariant/bin/postprocess_variants \. --ref=`basename ""${THEREF}""` \. --infile ""./call_variants_output@$(ls ./call_variants_output*.tfrecord.gz | wc -l).tfrecord.gz"" \. --outfile ""./${ALIGNMENTNAME}.deepVariant.vcf.gz"" \. --cpus ""8"" \. --gvcf_outfile ""./${ALIGNMENTNAME}.deepVariant.g.vcf.gz"" \. --nonvariant_site_tfrecord_path ""./gvcf.tfrecord@$(ls ./gvcf.tfrecord*.gz | wc -l).gz"" \. --sample_name=${SAMPLENAME}. f",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/818
https://github.com/google/deepvariant/issues/818:1108,modifiability,PAC,PACBIO,1108,"c scratch dir, which is located at ""/scratch/SlurmTMP/JobSpecificFolder"" (${TMPDIR}). ```. cd ${TMPDIR}. BIN_VERSION=""1.6.1"". module load singularity/3.5.2. #####################################################################. # singularity pull docker://google/deepvariant:""${BIN_VERSION}"". ulimit -u 10000 # https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable/54746150#54746150. # --model_type=PACBIO \ ##Replace this string with exactly one of the following [WGS,WES,PACBIO,HYBRID_PACBIO_ILLUMINA]**. # docker://google/deepvariant:""${BIN_VERSION}"" \. if ! [ -f ""${WORKINDIR}/${ALIGNMENTNAME}.deepVariant.vcf.gz"" ]. then. cp ""${THEREF}""* ./. cp ""${WORKINDIR}/${ALIGNMENTNAME}.bam""* . chmod 666 `basename ""${THEREF}""`*. chmod 666 ""${ALIGNMENTNAME}.bam""*. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. /my/path/software/deepVariant/deepvariant_${BIN_VERSION}.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=`basename ""${THEREF}""` \. --reads=""${ALIGNMENTNAME}.bam"" \. --sample_name=${SAMPLENAME} \. --output_vcf=""./${ALIGNMENTNAME}.deepVariant.vcf.gz"" \. --output_gvcf=""./${ALIGNMENTNAME}.deepVariant.g.vcf.gz"" \. --intermediate_results_dir . \. --num_shards=8 \. --logging_dir=. . if ! [ -f ""./${ALIGNMENTNAME}.deepVariant.vcf.gz"" ]. then. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. /my/path/software/deepVariant/deepvariant_${BIN_VERSION}.sif \. /opt/deepvariant/bin/postprocess_variants \. --ref=`basename ""${THEREF}""` \. --infile ""./call_variants_output@$(ls ./call_variants_output*.tfrecord.gz | wc -l).tfrecord.gz"" \. --outfile ""./${ALIGNMENTNAME}.deepVariant.vcf.gz"" \. --cpus ""8"" \. --gvcf_outfile ""./${ALIGNMENTNAME}.deepVariant.g.vcf.gz"" \. --nonvariant_site_tfrecord_path ""./gvcf.tfrecord@$(ls ./gvcf.tfrecord*.gz | wc -l).gz"" \. --sample_name=${SAMPLENAME}. fi. cp *.log ${WORKINDIR}/. cp ""./${ALIGNMENTNAME}.deepVariant.vcf.gz""* ${WORKINDIR}/. else. cp ""${WORKINDIR}/${",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/818
https://github.com/google/deepvariant/issues/818:244,performance,load,load,244,"This can not be the point. I am working in a cluster environment using slurm and the dir ""."" is the job specific scratch dir, which is located at ""/scratch/SlurmTMP/JobSpecificFolder"" (${TMPDIR}). ```. cd ${TMPDIR}. BIN_VERSION=""1.6.1"". module load singularity/3.5.2. #####################################################################. # singularity pull docker://google/deepvariant:""${BIN_VERSION}"". ulimit -u 10000 # https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable/54746150#54746150. # --model_type=PACBIO \ ##Replace this string with exactly one of the following [WGS,WES,PACBIO,HYBRID_PACBIO_ILLUMINA]**. # docker://google/deepvariant:""${BIN_VERSION}"" \. if ! [ -f ""${WORKINDIR}/${ALIGNMENTNAME}.deepVariant.vcf.gz"" ]. then. cp ""${THEREF}""* ./. cp ""${WORKINDIR}/${ALIGNMENTNAME}.bam""* . chmod 666 `basename ""${THEREF}""`*. chmod 666 ""${ALIGNMENTNAME}.bam""*. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. /my/path/software/deepVariant/deepvariant_${BIN_VERSION}.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=`basename ""${THEREF}""` \. --reads=""${ALIGNMENTNAME}.bam"" \. --sample_name=${SAMPLENAME} \. --output_vcf=""./${ALIGNMENTNAME}.deepVariant.vcf.gz"" \. --output_gvcf=""./${ALIGNMENTNAME}.deepVariant.g.vcf.gz"" \. --intermediate_results_dir . \. --num_shards=8 \. --logging_dir=. . if ! [ -f ""./${ALIGNMENTNAME}.deepVariant.vcf.gz"" ]. then. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. /my/path/software/deepVariant/deepvariant_${BIN_VERSION}.sif \. /opt/deepvariant/bin/postprocess_variants \. --ref=`basename ""${THEREF}""` \. --infile ""./call_variants_output@$(ls ./call_variants_output*.tfrecord.gz | wc -l).tfrecord.gz"" \. --outfile ""./${ALIGNMENTNAME}.deepVariant.vcf.gz"" \. --cpus ""8"" \. --gvcf_outfile ""./${ALIGNMENTNAME}.deepVariant.g.vcf.gz"" \. --nonvariant_site_tfrecord_path ""./gvcf.tfrecord@$(ls ./gvcf.tfrecord*.gz | wc -l).gz"" \. --sample_name=${SAMPLENAME}. f",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/818
https://github.com/google/deepvariant/issues/818:508,performance,resourc,resource-temporarily-unavailable,508,"This can not be the point. I am working in a cluster environment using slurm and the dir ""."" is the job specific scratch dir, which is located at ""/scratch/SlurmTMP/JobSpecificFolder"" (${TMPDIR}). ```. cd ${TMPDIR}. BIN_VERSION=""1.6.1"". module load singularity/3.5.2. #####################################################################. # singularity pull docker://google/deepvariant:""${BIN_VERSION}"". ulimit -u 10000 # https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable/54746150#54746150. # --model_type=PACBIO \ ##Replace this string with exactly one of the following [WGS,WES,PACBIO,HYBRID_PACBIO_ILLUMINA]**. # docker://google/deepvariant:""${BIN_VERSION}"" \. if ! [ -f ""${WORKINDIR}/${ALIGNMENTNAME}.deepVariant.vcf.gz"" ]. then. cp ""${THEREF}""* ./. cp ""${WORKINDIR}/${ALIGNMENTNAME}.bam""* . chmod 666 `basename ""${THEREF}""`*. chmod 666 ""${ALIGNMENTNAME}.bam""*. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. /my/path/software/deepVariant/deepvariant_${BIN_VERSION}.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=`basename ""${THEREF}""` \. --reads=""${ALIGNMENTNAME}.bam"" \. --sample_name=${SAMPLENAME} \. --output_vcf=""./${ALIGNMENTNAME}.deepVariant.vcf.gz"" \. --output_gvcf=""./${ALIGNMENTNAME}.deepVariant.g.vcf.gz"" \. --intermediate_results_dir . \. --num_shards=8 \. --logging_dir=. . if ! [ -f ""./${ALIGNMENTNAME}.deepVariant.vcf.gz"" ]. then. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. /my/path/software/deepVariant/deepvariant_${BIN_VERSION}.sif \. /opt/deepvariant/bin/postprocess_variants \. --ref=`basename ""${THEREF}""` \. --infile ""./call_variants_output@$(ls ./call_variants_output*.tfrecord.gz | wc -l).tfrecord.gz"" \. --outfile ""./${ALIGNMENTNAME}.deepVariant.vcf.gz"" \. --cpus ""8"" \. --gvcf_outfile ""./${ALIGNMENTNAME}.deepVariant.g.vcf.gz"" \. --nonvariant_site_tfrecord_path ""./gvcf.tfrecord@$(ls ./gvcf.tfrecord*.gz | wc -l).gz"" \. --sample_name=${SAMPLENAME}. f",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/818
https://github.com/google/deepvariant/issues/818:1808,performance,cpu,cpus,1808,"SlurmTMP/JobSpecificFolder"" (${TMPDIR}). ```. cd ${TMPDIR}. BIN_VERSION=""1.6.1"". module load singularity/3.5.2. #####################################################################. # singularity pull docker://google/deepvariant:""${BIN_VERSION}"". ulimit -u 10000 # https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable/54746150#54746150. # --model_type=PACBIO \ ##Replace this string with exactly one of the following [WGS,WES,PACBIO,HYBRID_PACBIO_ILLUMINA]**. # docker://google/deepvariant:""${BIN_VERSION}"" \. if ! [ -f ""${WORKINDIR}/${ALIGNMENTNAME}.deepVariant.vcf.gz"" ]. then. cp ""${THEREF}""* ./. cp ""${WORKINDIR}/${ALIGNMENTNAME}.bam""* . chmod 666 `basename ""${THEREF}""`*. chmod 666 ""${ALIGNMENTNAME}.bam""*. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. /my/path/software/deepVariant/deepvariant_${BIN_VERSION}.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=`basename ""${THEREF}""` \. --reads=""${ALIGNMENTNAME}.bam"" \. --sample_name=${SAMPLENAME} \. --output_vcf=""./${ALIGNMENTNAME}.deepVariant.vcf.gz"" \. --output_gvcf=""./${ALIGNMENTNAME}.deepVariant.g.vcf.gz"" \. --intermediate_results_dir . \. --num_shards=8 \. --logging_dir=. . if ! [ -f ""./${ALIGNMENTNAME}.deepVariant.vcf.gz"" ]. then. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. /my/path/software/deepVariant/deepvariant_${BIN_VERSION}.sif \. /opt/deepvariant/bin/postprocess_variants \. --ref=`basename ""${THEREF}""` \. --infile ""./call_variants_output@$(ls ./call_variants_output*.tfrecord.gz | wc -l).tfrecord.gz"" \. --outfile ""./${ALIGNMENTNAME}.deepVariant.vcf.gz"" \. --cpus ""8"" \. --gvcf_outfile ""./${ALIGNMENTNAME}.deepVariant.g.vcf.gz"" \. --nonvariant_site_tfrecord_path ""./gvcf.tfrecord@$(ls ./gvcf.tfrecord*.gz | wc -l).gz"" \. --sample_name=${SAMPLENAME}. fi. cp *.log ${WORKINDIR}/. cp ""./${ALIGNMENTNAME}.deepVariant.vcf.gz""* ${WORKINDIR}/. else. cp ""${WORKINDIR}/${ALIGNMENTNAME}.deepVariant.vcf.gz""* . fi. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/818
https://github.com/google/deepvariant/issues/818:237,safety,modul,module,237,"This can not be the point. I am working in a cluster environment using slurm and the dir ""."" is the job specific scratch dir, which is located at ""/scratch/SlurmTMP/JobSpecificFolder"" (${TMPDIR}). ```. cd ${TMPDIR}. BIN_VERSION=""1.6.1"". module load singularity/3.5.2. #####################################################################. # singularity pull docker://google/deepvariant:""${BIN_VERSION}"". ulimit -u 10000 # https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable/54746150#54746150. # --model_type=PACBIO \ ##Replace this string with exactly one of the following [WGS,WES,PACBIO,HYBRID_PACBIO_ILLUMINA]**. # docker://google/deepvariant:""${BIN_VERSION}"" \. if ! [ -f ""${WORKINDIR}/${ALIGNMENTNAME}.deepVariant.vcf.gz"" ]. then. cp ""${THEREF}""* ./. cp ""${WORKINDIR}/${ALIGNMENTNAME}.bam""* . chmod 666 `basename ""${THEREF}""`*. chmod 666 ""${ALIGNMENTNAME}.bam""*. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. /my/path/software/deepVariant/deepvariant_${BIN_VERSION}.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=`basename ""${THEREF}""` \. --reads=""${ALIGNMENTNAME}.bam"" \. --sample_name=${SAMPLENAME} \. --output_vcf=""./${ALIGNMENTNAME}.deepVariant.vcf.gz"" \. --output_gvcf=""./${ALIGNMENTNAME}.deepVariant.g.vcf.gz"" \. --intermediate_results_dir . \. --num_shards=8 \. --logging_dir=. . if ! [ -f ""./${ALIGNMENTNAME}.deepVariant.vcf.gz"" ]. then. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. /my/path/software/deepVariant/deepvariant_${BIN_VERSION}.sif \. /opt/deepvariant/bin/postprocess_variants \. --ref=`basename ""${THEREF}""` \. --infile ""./call_variants_output@$(ls ./call_variants_output*.tfrecord.gz | wc -l).tfrecord.gz"" \. --outfile ""./${ALIGNMENTNAME}.deepVariant.vcf.gz"" \. --cpus ""8"" \. --gvcf_outfile ""./${ALIGNMENTNAME}.deepVariant.g.vcf.gz"" \. --nonvariant_site_tfrecord_path ""./gvcf.tfrecord@$(ls ./gvcf.tfrecord*.gz | wc -l).gz"" \. --sample_name=${SAMPLENAME}. f",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/818
https://github.com/google/deepvariant/issues/818:508,safety,resourc,resource-temporarily-unavailable,508,"This can not be the point. I am working in a cluster environment using slurm and the dir ""."" is the job specific scratch dir, which is located at ""/scratch/SlurmTMP/JobSpecificFolder"" (${TMPDIR}). ```. cd ${TMPDIR}. BIN_VERSION=""1.6.1"". module load singularity/3.5.2. #####################################################################. # singularity pull docker://google/deepvariant:""${BIN_VERSION}"". ulimit -u 10000 # https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable/54746150#54746150. # --model_type=PACBIO \ ##Replace this string with exactly one of the following [WGS,WES,PACBIO,HYBRID_PACBIO_ILLUMINA]**. # docker://google/deepvariant:""${BIN_VERSION}"" \. if ! [ -f ""${WORKINDIR}/${ALIGNMENTNAME}.deepVariant.vcf.gz"" ]. then. cp ""${THEREF}""* ./. cp ""${WORKINDIR}/${ALIGNMENTNAME}.bam""* . chmod 666 `basename ""${THEREF}""`*. chmod 666 ""${ALIGNMENTNAME}.bam""*. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. /my/path/software/deepVariant/deepvariant_${BIN_VERSION}.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=`basename ""${THEREF}""` \. --reads=""${ALIGNMENTNAME}.bam"" \. --sample_name=${SAMPLENAME} \. --output_vcf=""./${ALIGNMENTNAME}.deepVariant.vcf.gz"" \. --output_gvcf=""./${ALIGNMENTNAME}.deepVariant.g.vcf.gz"" \. --intermediate_results_dir . \. --num_shards=8 \. --logging_dir=. . if ! [ -f ""./${ALIGNMENTNAME}.deepVariant.vcf.gz"" ]. then. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. /my/path/software/deepVariant/deepvariant_${BIN_VERSION}.sif \. /opt/deepvariant/bin/postprocess_variants \. --ref=`basename ""${THEREF}""` \. --infile ""./call_variants_output@$(ls ./call_variants_output*.tfrecord.gz | wc -l).tfrecord.gz"" \. --outfile ""./${ALIGNMENTNAME}.deepVariant.vcf.gz"" \. --cpus ""8"" \. --gvcf_outfile ""./${ALIGNMENTNAME}.deepVariant.g.vcf.gz"" \. --nonvariant_site_tfrecord_path ""./gvcf.tfrecord@$(ls ./gvcf.tfrecord*.gz | wc -l).gz"" \. --sample_name=${SAMPLENAME}. f",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/818
https://github.com/google/deepvariant/issues/818:2008,safety,log,log,2008,"SlurmTMP/JobSpecificFolder"" (${TMPDIR}). ```. cd ${TMPDIR}. BIN_VERSION=""1.6.1"". module load singularity/3.5.2. #####################################################################. # singularity pull docker://google/deepvariant:""${BIN_VERSION}"". ulimit -u 10000 # https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable/54746150#54746150. # --model_type=PACBIO \ ##Replace this string with exactly one of the following [WGS,WES,PACBIO,HYBRID_PACBIO_ILLUMINA]**. # docker://google/deepvariant:""${BIN_VERSION}"" \. if ! [ -f ""${WORKINDIR}/${ALIGNMENTNAME}.deepVariant.vcf.gz"" ]. then. cp ""${THEREF}""* ./. cp ""${WORKINDIR}/${ALIGNMENTNAME}.bam""* . chmod 666 `basename ""${THEREF}""`*. chmod 666 ""${ALIGNMENTNAME}.bam""*. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. /my/path/software/deepVariant/deepvariant_${BIN_VERSION}.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=`basename ""${THEREF}""` \. --reads=""${ALIGNMENTNAME}.bam"" \. --sample_name=${SAMPLENAME} \. --output_vcf=""./${ALIGNMENTNAME}.deepVariant.vcf.gz"" \. --output_gvcf=""./${ALIGNMENTNAME}.deepVariant.g.vcf.gz"" \. --intermediate_results_dir . \. --num_shards=8 \. --logging_dir=. . if ! [ -f ""./${ALIGNMENTNAME}.deepVariant.vcf.gz"" ]. then. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. /my/path/software/deepVariant/deepvariant_${BIN_VERSION}.sif \. /opt/deepvariant/bin/postprocess_variants \. --ref=`basename ""${THEREF}""` \. --infile ""./call_variants_output@$(ls ./call_variants_output*.tfrecord.gz | wc -l).tfrecord.gz"" \. --outfile ""./${ALIGNMENTNAME}.deepVariant.vcf.gz"" \. --cpus ""8"" \. --gvcf_outfile ""./${ALIGNMENTNAME}.deepVariant.g.vcf.gz"" \. --nonvariant_site_tfrecord_path ""./gvcf.tfrecord@$(ls ./gvcf.tfrecord*.gz | wc -l).gz"" \. --sample_name=${SAMPLENAME}. fi. cp *.log ${WORKINDIR}/. cp ""./${ALIGNMENTNAME}.deepVariant.vcf.gz""* ${WORKINDIR}/. else. cp ""${WORKINDIR}/${ALIGNMENTNAME}.deepVariant.vcf.gz""* . fi. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/818
https://github.com/google/deepvariant/issues/818:2008,security,log,log,2008,"SlurmTMP/JobSpecificFolder"" (${TMPDIR}). ```. cd ${TMPDIR}. BIN_VERSION=""1.6.1"". module load singularity/3.5.2. #####################################################################. # singularity pull docker://google/deepvariant:""${BIN_VERSION}"". ulimit -u 10000 # https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable/54746150#54746150. # --model_type=PACBIO \ ##Replace this string with exactly one of the following [WGS,WES,PACBIO,HYBRID_PACBIO_ILLUMINA]**. # docker://google/deepvariant:""${BIN_VERSION}"" \. if ! [ -f ""${WORKINDIR}/${ALIGNMENTNAME}.deepVariant.vcf.gz"" ]. then. cp ""${THEREF}""* ./. cp ""${WORKINDIR}/${ALIGNMENTNAME}.bam""* . chmod 666 `basename ""${THEREF}""`*. chmod 666 ""${ALIGNMENTNAME}.bam""*. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. /my/path/software/deepVariant/deepvariant_${BIN_VERSION}.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=`basename ""${THEREF}""` \. --reads=""${ALIGNMENTNAME}.bam"" \. --sample_name=${SAMPLENAME} \. --output_vcf=""./${ALIGNMENTNAME}.deepVariant.vcf.gz"" \. --output_gvcf=""./${ALIGNMENTNAME}.deepVariant.g.vcf.gz"" \. --intermediate_results_dir . \. --num_shards=8 \. --logging_dir=. . if ! [ -f ""./${ALIGNMENTNAME}.deepVariant.vcf.gz"" ]. then. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. /my/path/software/deepVariant/deepvariant_${BIN_VERSION}.sif \. /opt/deepvariant/bin/postprocess_variants \. --ref=`basename ""${THEREF}""` \. --infile ""./call_variants_output@$(ls ./call_variants_output*.tfrecord.gz | wc -l).tfrecord.gz"" \. --outfile ""./${ALIGNMENTNAME}.deepVariant.vcf.gz"" \. --cpus ""8"" \. --gvcf_outfile ""./${ALIGNMENTNAME}.deepVariant.g.vcf.gz"" \. --nonvariant_site_tfrecord_path ""./gvcf.tfrecord@$(ls ./gvcf.tfrecord*.gz | wc -l).gz"" \. --sample_name=${SAMPLENAME}. fi. cp *.log ${WORKINDIR}/. cp ""./${ALIGNMENTNAME}.deepVariant.vcf.gz""* ${WORKINDIR}/. else. cp ""${WORKINDIR}/${ALIGNMENTNAME}.deepVariant.vcf.gz""* . fi. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/818
https://github.com/google/deepvariant/issues/818:508,testability,resourc,resource-temporarily-unavailable,508,"This can not be the point. I am working in a cluster environment using slurm and the dir ""."" is the job specific scratch dir, which is located at ""/scratch/SlurmTMP/JobSpecificFolder"" (${TMPDIR}). ```. cd ${TMPDIR}. BIN_VERSION=""1.6.1"". module load singularity/3.5.2. #####################################################################. # singularity pull docker://google/deepvariant:""${BIN_VERSION}"". ulimit -u 10000 # https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable/54746150#54746150. # --model_type=PACBIO \ ##Replace this string with exactly one of the following [WGS,WES,PACBIO,HYBRID_PACBIO_ILLUMINA]**. # docker://google/deepvariant:""${BIN_VERSION}"" \. if ! [ -f ""${WORKINDIR}/${ALIGNMENTNAME}.deepVariant.vcf.gz"" ]. then. cp ""${THEREF}""* ./. cp ""${WORKINDIR}/${ALIGNMENTNAME}.bam""* . chmod 666 `basename ""${THEREF}""`*. chmod 666 ""${ALIGNMENTNAME}.bam""*. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. /my/path/software/deepVariant/deepvariant_${BIN_VERSION}.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=`basename ""${THEREF}""` \. --reads=""${ALIGNMENTNAME}.bam"" \. --sample_name=${SAMPLENAME} \. --output_vcf=""./${ALIGNMENTNAME}.deepVariant.vcf.gz"" \. --output_gvcf=""./${ALIGNMENTNAME}.deepVariant.g.vcf.gz"" \. --intermediate_results_dir . \. --num_shards=8 \. --logging_dir=. . if ! [ -f ""./${ALIGNMENTNAME}.deepVariant.vcf.gz"" ]. then. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. /my/path/software/deepVariant/deepvariant_${BIN_VERSION}.sif \. /opt/deepvariant/bin/postprocess_variants \. --ref=`basename ""${THEREF}""` \. --infile ""./call_variants_output@$(ls ./call_variants_output*.tfrecord.gz | wc -l).tfrecord.gz"" \. --outfile ""./${ALIGNMENTNAME}.deepVariant.vcf.gz"" \. --cpus ""8"" \. --gvcf_outfile ""./${ALIGNMENTNAME}.deepVariant.g.vcf.gz"" \. --nonvariant_site_tfrecord_path ""./gvcf.tfrecord@$(ls ./gvcf.tfrecord*.gz | wc -l).gz"" \. --sample_name=${SAMPLENAME}. f",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/818
https://github.com/google/deepvariant/issues/818:2008,testability,log,log,2008,"SlurmTMP/JobSpecificFolder"" (${TMPDIR}). ```. cd ${TMPDIR}. BIN_VERSION=""1.6.1"". module load singularity/3.5.2. #####################################################################. # singularity pull docker://google/deepvariant:""${BIN_VERSION}"". ulimit -u 10000 # https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable/54746150#54746150. # --model_type=PACBIO \ ##Replace this string with exactly one of the following [WGS,WES,PACBIO,HYBRID_PACBIO_ILLUMINA]**. # docker://google/deepvariant:""${BIN_VERSION}"" \. if ! [ -f ""${WORKINDIR}/${ALIGNMENTNAME}.deepVariant.vcf.gz"" ]. then. cp ""${THEREF}""* ./. cp ""${WORKINDIR}/${ALIGNMENTNAME}.bam""* . chmod 666 `basename ""${THEREF}""`*. chmod 666 ""${ALIGNMENTNAME}.bam""*. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. /my/path/software/deepVariant/deepvariant_${BIN_VERSION}.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=`basename ""${THEREF}""` \. --reads=""${ALIGNMENTNAME}.bam"" \. --sample_name=${SAMPLENAME} \. --output_vcf=""./${ALIGNMENTNAME}.deepVariant.vcf.gz"" \. --output_gvcf=""./${ALIGNMENTNAME}.deepVariant.g.vcf.gz"" \. --intermediate_results_dir . \. --num_shards=8 \. --logging_dir=. . if ! [ -f ""./${ALIGNMENTNAME}.deepVariant.vcf.gz"" ]. then. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. /my/path/software/deepVariant/deepvariant_${BIN_VERSION}.sif \. /opt/deepvariant/bin/postprocess_variants \. --ref=`basename ""${THEREF}""` \. --infile ""./call_variants_output@$(ls ./call_variants_output*.tfrecord.gz | wc -l).tfrecord.gz"" \. --outfile ""./${ALIGNMENTNAME}.deepVariant.vcf.gz"" \. --cpus ""8"" \. --gvcf_outfile ""./${ALIGNMENTNAME}.deepVariant.g.vcf.gz"" \. --nonvariant_site_tfrecord_path ""./gvcf.tfrecord@$(ls ./gvcf.tfrecord*.gz | wc -l).gz"" \. --sample_name=${SAMPLENAME}. fi. cp *.log ${WORKINDIR}/. cp ""./${ALIGNMENTNAME}.deepVariant.vcf.gz""* ${WORKINDIR}/. else. cp ""${WORKINDIR}/${ALIGNMENTNAME}.deepVariant.vcf.gz""* . fi. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/818
https://github.com/google/deepvariant/issues/818:469,availability,replic,replicate,469,"@MiWitt , . Can you use `--intermediate_results_dir ./intermediate_results_ ${ALIGNMENTNAME}`. I am unsure why you are running postprocessing separately, but, something must be overwriting the files or generating multiple file patterns in the same directory where you are saving everything. One way to better debug is to set `--dry_run=true` for each command and look at the outputs and see if they match with each other. Unfortunately I don't have access to an HPC to replicate this issue. I tried running your script but it has many missing variables.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/818
https://github.com/google/deepvariant/issues/818:543,modifiability,variab,variables,543,"@MiWitt , . Can you use `--intermediate_results_dir ./intermediate_results_ ${ALIGNMENTNAME}`. I am unsure why you are running postprocessing separately, but, something must be overwriting the files or generating multiple file patterns in the same directory where you are saving everything. One way to better debug is to set `--dry_run=true` for each command and look at the outputs and see if they match with each other. Unfortunately I don't have access to an HPC to replicate this issue. I tried running your script but it has many missing variables.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/818
https://github.com/google/deepvariant/issues/818:449,security,access,access,449,"@MiWitt , . Can you use `--intermediate_results_dir ./intermediate_results_ ${ALIGNMENTNAME}`. I am unsure why you are running postprocessing separately, but, something must be overwriting the files or generating multiple file patterns in the same directory where you are saving everything. One way to better debug is to set `--dry_run=true` for each command and look at the outputs and see if they match with each other. Unfortunately I don't have access to an HPC to replicate this issue. I tried running your script but it has many missing variables.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/818
https://github.com/google/deepvariant/issues/818:351,usability,command,command,351,"@MiWitt , . Can you use `--intermediate_results_dir ./intermediate_results_ ${ALIGNMENTNAME}`. I am unsure why you are running postprocessing separately, but, something must be overwriting the files or generating multiple file patterns in the same directory where you are saving everything. One way to better debug is to set `--dry_run=true` for each command and look at the outputs and see if they match with each other. Unfortunately I don't have access to an HPC to replicate this issue. I tried running your script but it has many missing variables.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/818
https://github.com/google/deepvariant/issues/818:30,deployability,updat,updates,30,"@MiWitt . Hi, do you have any updates on this issue?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/818
https://github.com/google/deepvariant/issues/818:30,safety,updat,updates,30,"@MiWitt . Hi, do you have any updates on this issue?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/818
https://github.com/google/deepvariant/issues/818:30,security,updat,updates,30,"@MiWitt . Hi, do you have any updates on this issue?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/818
https://github.com/google/deepvariant/issues/818:95,deployability,updat,updates,95,"@MiWitt , I am closing the issue due to inactivity. Please feel free to reopen if you have any updates.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/818
https://github.com/google/deepvariant/issues/818:95,safety,updat,updates,95,"@MiWitt , I am closing the issue due to inactivity. Please feel free to reopen if you have any updates.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/818
https://github.com/google/deepvariant/issues/818:95,security,updat,updates,95,"@MiWitt , I am closing the issue due to inactivity. Please feel free to reopen if you have any updates.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/818
https://github.com/google/deepvariant/issues/818:639,energy efficiency,cpu,cpus,639,"@EgorGuga. You can use my workround above, which solved the problem for me. . If the --output_vcf from /opt/deepvariant/bin/run_deepvariant does not exist, run /opt/deepvariant/bin/postprocess_variants in a separate step. . ```. if ! [ -f ""./${ALIGNMENTNAME}.deepVariant.vcf.gz"" ]. then. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. /my/path/software/deepVariant/deepvariant_${BIN_VERSION}.sif \. /opt/deepvariant/bin/postprocess_variants \. --ref=`basename ""${THEREF}""` \. --infile ""./call_variants_output@$(ls ./call_variants_output*.tfrecord.gz | wc -l).tfrecord.gz"" \. --outfile ""./${ALIGNMENTNAME}.deepVariant.vcf.gz"" \. --cpus ""8"" \. --gvcf_outfile ""./${ALIGNMENTNAME}.deepVariant.g.vcf.gz"" \. --nonvariant_site_tfrecord_path ""./gvcf.tfrecord@$(ls ./gvcf.tfrecord*.gz | wc -l).gz"" \. --sample_name=${SAMPLENAME}. fi. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/818
https://github.com/google/deepvariant/issues/818:639,performance,cpu,cpus,639,"@EgorGuga. You can use my workround above, which solved the problem for me. . If the --output_vcf from /opt/deepvariant/bin/run_deepvariant does not exist, run /opt/deepvariant/bin/postprocess_variants in a separate step. . ```. if ! [ -f ""./${ALIGNMENTNAME}.deepVariant.vcf.gz"" ]. then. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. /my/path/software/deepVariant/deepvariant_${BIN_VERSION}.sif \. /opt/deepvariant/bin/postprocess_variants \. --ref=`basename ""${THEREF}""` \. --infile ""./call_variants_output@$(ls ./call_variants_output*.tfrecord.gz | wc -l).tfrecord.gz"" \. --outfile ""./${ALIGNMENTNAME}.deepVariant.vcf.gz"" \. --cpus ""8"" \. --gvcf_outfile ""./${ALIGNMENTNAME}.deepVariant.g.vcf.gz"" \. --nonvariant_site_tfrecord_path ""./gvcf.tfrecord@$(ls ./gvcf.tfrecord*.gz | wc -l).gz"" \. --sample_name=${SAMPLENAME}. fi. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/818
https://github.com/google/deepvariant/issues/818:140,reliability,doe,does,140,"@EgorGuga. You can use my workround above, which solved the problem for me. . If the --output_vcf from /opt/deepvariant/bin/run_deepvariant does not exist, run /opt/deepvariant/bin/postprocess_variants in a separate step. . ```. if ! [ -f ""./${ALIGNMENTNAME}.deepVariant.vcf.gz"" ]. then. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. /my/path/software/deepVariant/deepvariant_${BIN_VERSION}.sif \. /opt/deepvariant/bin/postprocess_variants \. --ref=`basename ""${THEREF}""` \. --infile ""./call_variants_output@$(ls ./call_variants_output*.tfrecord.gz | wc -l).tfrecord.gz"" \. --outfile ""./${ALIGNMENTNAME}.deepVariant.vcf.gz"" \. --cpus ""8"" \. --gvcf_outfile ""./${ALIGNMENTNAME}.deepVariant.g.vcf.gz"" \. --nonvariant_site_tfrecord_path ""./gvcf.tfrecord@$(ls ./gvcf.tfrecord*.gz | wc -l).gz"" \. --sample_name=${SAMPLENAME}. fi. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/818
https://github.com/google/deepvariant/issues/819:21,deployability,observ,observe,21,"@nlopez94 ,. We also observe these warnings, but, DeepVariant does not use any TensorRT apis for training or inference. So these warning are usually non actionable for the deepvariant pipeline. Are you running inference and seeing the machine's GPU is not being utilized?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/819
https://github.com/google/deepvariant/issues/819:88,deployability,api,apis,88,"@nlopez94 ,. We also observe these warnings, but, DeepVariant does not use any TensorRT apis for training or inference. So these warning are usually non actionable for the deepvariant pipeline. Are you running inference and seeing the machine's GPU is not being utilized?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/819
https://github.com/google/deepvariant/issues/819:184,deployability,pipelin,pipeline,184,"@nlopez94 ,. We also observe these warnings, but, DeepVariant does not use any TensorRT apis for training or inference. So these warning are usually non actionable for the deepvariant pipeline. Are you running inference and seeing the machine's GPU is not being utilized?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/819
https://github.com/google/deepvariant/issues/819:245,energy efficiency,GPU,GPU,245,"@nlopez94 ,. We also observe these warnings, but, DeepVariant does not use any TensorRT apis for training or inference. So these warning are usually non actionable for the deepvariant pipeline. Are you running inference and seeing the machine's GPU is not being utilized?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/819
https://github.com/google/deepvariant/issues/819:88,integrability,api,apis,88,"@nlopez94 ,. We also observe these warnings, but, DeepVariant does not use any TensorRT apis for training or inference. So these warning are usually non actionable for the deepvariant pipeline. Are you running inference and seeing the machine's GPU is not being utilized?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/819
https://github.com/google/deepvariant/issues/819:184,integrability,pipelin,pipeline,184,"@nlopez94 ,. We also observe these warnings, but, DeepVariant does not use any TensorRT apis for training or inference. So these warning are usually non actionable for the deepvariant pipeline. Are you running inference and seeing the machine's GPU is not being utilized?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/819
https://github.com/google/deepvariant/issues/819:88,interoperability,api,apis,88,"@nlopez94 ,. We also observe these warnings, but, DeepVariant does not use any TensorRT apis for training or inference. So these warning are usually non actionable for the deepvariant pipeline. Are you running inference and seeing the machine's GPU is not being utilized?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/819
https://github.com/google/deepvariant/issues/819:245,performance,GPU,GPU,245,"@nlopez94 ,. We also observe these warnings, but, DeepVariant does not use any TensorRT apis for training or inference. So these warning are usually non actionable for the deepvariant pipeline. Are you running inference and seeing the machine's GPU is not being utilized?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/819
https://github.com/google/deepvariant/issues/819:62,reliability,doe,does,62,"@nlopez94 ,. We also observe these warnings, but, DeepVariant does not use any TensorRT apis for training or inference. So these warning are usually non actionable for the deepvariant pipeline. Are you running inference and seeing the machine's GPU is not being utilized?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/819
https://github.com/google/deepvariant/issues/819:21,testability,observ,observe,21,"@nlopez94 ,. We also observe these warnings, but, DeepVariant does not use any TensorRT apis for training or inference. So these warning are usually non actionable for the deepvariant pipeline. Are you running inference and seeing the machine's GPU is not being utilized?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/819
https://github.com/google/deepvariant/issues/819:121,availability,error,error,121,"Hi @kishwarshafin,. The training process starts as expected with GPU activity visible, but it abruptly stops without any error message while processing the first epoch and determining the best checkpoint metric (code snippet). This step completes as expected when using the CPU image with the same dataset and parameters. Initially, I thought TensorRT issues might be causing this stop, but I'll share the logs with you to get your perspective and an extra set of eyes on the problem. **Command:** . ```. ( time sudo docker run --runtime=nvidia --gpus 1\. -v ${HOME}:${HOME} \. -w ${HOME} \. google/deepvariant:1.6.1-gpu \. train \. --config=""${BASE}/dv_config.py"":base \. --config.train_dataset_pbtxt=""${BASE}/training_set.pbtxt"" \. --config.tune_dataset_pbtxt=""${BASE}/validation_set.pbtxt"" \. --config.init_checkpoint=""${BASE}/checkpoint/deepvariant.wgs.ckpt"" \. --config.num_epochs=10 \. --config.learning_rate=0.0001 \. --config.num_validation_examples=0 \. --experiment_dir=${TRAINING_DIR} \. --strategy=mirrored \. --config.batch_size=512 \. ) > ""${LOG_DIR}/train.log"" 2>&1 &. ```. ```. I0508 17:53:46.544947 140534986602304 train.py:384] Starting epoch 0. I0508 17:53:46.545100 140534986602304 train.py:391] Performing initial evaluation of warmstart model. I0508 17:53:46.545171 140534986602304 train.py:361] Running tune at step=0 epoch=0. I0508 17:53:46.545287 140534986602304 train.py:366] Tune step 0 / 15 (0.0%). I0508 17:54:10.069682 140512707213056 logging_writer.py:48] [0] tune/categorical_accuracy=0.22617188096046448, tune/categorical_crossentropy=1.3209192752838135, tune/f1_het=0.02283571846783161, tune/f1_homalt=0.09889934211969376, tune/f1_homref=0.843934178352356, tune/f1_macro=0.3218897581100464, tune/f1_micro=0.22617188096046448, tune/f1_weighted=0.21346084773540497, tune/false_negatives_1=6123.0, tune/false_positives_1=5727.0, tune/loss=1.3209190368652344, tune/precision_1=0.21375617384910583, tune/precision_het=0.19323670864105225, tune/precision_homalt=0.05127762",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/819
https://github.com/google/deepvariant/issues/819:193,availability,checkpoint,checkpoint,193,"Hi @kishwarshafin,. The training process starts as expected with GPU activity visible, but it abruptly stops without any error message while processing the first epoch and determining the best checkpoint metric (code snippet). This step completes as expected when using the CPU image with the same dataset and parameters. Initially, I thought TensorRT issues might be causing this stop, but I'll share the logs with you to get your perspective and an extra set of eyes on the problem. **Command:** . ```. ( time sudo docker run --runtime=nvidia --gpus 1\. -v ${HOME}:${HOME} \. -w ${HOME} \. google/deepvariant:1.6.1-gpu \. train \. --config=""${BASE}/dv_config.py"":base \. --config.train_dataset_pbtxt=""${BASE}/training_set.pbtxt"" \. --config.tune_dataset_pbtxt=""${BASE}/validation_set.pbtxt"" \. --config.init_checkpoint=""${BASE}/checkpoint/deepvariant.wgs.ckpt"" \. --config.num_epochs=10 \. --config.learning_rate=0.0001 \. --config.num_validation_examples=0 \. --experiment_dir=${TRAINING_DIR} \. --strategy=mirrored \. --config.batch_size=512 \. ) > ""${LOG_DIR}/train.log"" 2>&1 &. ```. ```. I0508 17:53:46.544947 140534986602304 train.py:384] Starting epoch 0. I0508 17:53:46.545100 140534986602304 train.py:391] Performing initial evaluation of warmstart model. I0508 17:53:46.545171 140534986602304 train.py:361] Running tune at step=0 epoch=0. I0508 17:53:46.545287 140534986602304 train.py:366] Tune step 0 / 15 (0.0%). I0508 17:54:10.069682 140512707213056 logging_writer.py:48] [0] tune/categorical_accuracy=0.22617188096046448, tune/categorical_crossentropy=1.3209192752838135, tune/f1_het=0.02283571846783161, tune/f1_homalt=0.09889934211969376, tune/f1_homref=0.843934178352356, tune/f1_macro=0.3218897581100464, tune/f1_micro=0.22617188096046448, tune/f1_weighted=0.21346084773540497, tune/false_negatives_1=6123.0, tune/false_positives_1=5727.0, tune/loss=1.3209190368652344, tune/precision_1=0.21375617384910583, tune/precision_het=0.19323670864105225, tune/precision_homalt=0.05127762",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/819
https://github.com/google/deepvariant/issues/819:830,availability,checkpoint,checkpoint,830,"Hi @kishwarshafin,. The training process starts as expected with GPU activity visible, but it abruptly stops without any error message while processing the first epoch and determining the best checkpoint metric (code snippet). This step completes as expected when using the CPU image with the same dataset and parameters. Initially, I thought TensorRT issues might be causing this stop, but I'll share the logs with you to get your perspective and an extra set of eyes on the problem. **Command:** . ```. ( time sudo docker run --runtime=nvidia --gpus 1\. -v ${HOME}:${HOME} \. -w ${HOME} \. google/deepvariant:1.6.1-gpu \. train \. --config=""${BASE}/dv_config.py"":base \. --config.train_dataset_pbtxt=""${BASE}/training_set.pbtxt"" \. --config.tune_dataset_pbtxt=""${BASE}/validation_set.pbtxt"" \. --config.init_checkpoint=""${BASE}/checkpoint/deepvariant.wgs.ckpt"" \. --config.num_epochs=10 \. --config.learning_rate=0.0001 \. --config.num_validation_examples=0 \. --experiment_dir=${TRAINING_DIR} \. --strategy=mirrored \. --config.batch_size=512 \. ) > ""${LOG_DIR}/train.log"" 2>&1 &. ```. ```. I0508 17:53:46.544947 140534986602304 train.py:384] Starting epoch 0. I0508 17:53:46.545100 140534986602304 train.py:391] Performing initial evaluation of warmstart model. I0508 17:53:46.545171 140534986602304 train.py:361] Running tune at step=0 epoch=0. I0508 17:53:46.545287 140534986602304 train.py:366] Tune step 0 / 15 (0.0%). I0508 17:54:10.069682 140512707213056 logging_writer.py:48] [0] tune/categorical_accuracy=0.22617188096046448, tune/categorical_crossentropy=1.3209192752838135, tune/f1_het=0.02283571846783161, tune/f1_homalt=0.09889934211969376, tune/f1_homref=0.843934178352356, tune/f1_macro=0.3218897581100464, tune/f1_micro=0.22617188096046448, tune/f1_weighted=0.21346084773540497, tune/false_negatives_1=6123.0, tune/false_positives_1=5727.0, tune/loss=1.3209190368652344, tune/precision_1=0.21375617384910583, tune/precision_het=0.19323670864105225, tune/precision_homalt=0.05127762",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/819
https://github.com/google/deepvariant/issues/819:2325,availability,checkpoint,checkpoint,2325,"o docker run --runtime=nvidia --gpus 1\. -v ${HOME}:${HOME} \. -w ${HOME} \. google/deepvariant:1.6.1-gpu \. train \. --config=""${BASE}/dv_config.py"":base \. --config.train_dataset_pbtxt=""${BASE}/training_set.pbtxt"" \. --config.tune_dataset_pbtxt=""${BASE}/validation_set.pbtxt"" \. --config.init_checkpoint=""${BASE}/checkpoint/deepvariant.wgs.ckpt"" \. --config.num_epochs=10 \. --config.learning_rate=0.0001 \. --config.num_validation_examples=0 \. --experiment_dir=${TRAINING_DIR} \. --strategy=mirrored \. --config.batch_size=512 \. ) > ""${LOG_DIR}/train.log"" 2>&1 &. ```. ```. I0508 17:53:46.544947 140534986602304 train.py:384] Starting epoch 0. I0508 17:53:46.545100 140534986602304 train.py:391] Performing initial evaluation of warmstart model. I0508 17:53:46.545171 140534986602304 train.py:361] Running tune at step=0 epoch=0. I0508 17:53:46.545287 140534986602304 train.py:366] Tune step 0 / 15 (0.0%). I0508 17:54:10.069682 140512707213056 logging_writer.py:48] [0] tune/categorical_accuracy=0.22617188096046448, tune/categorical_crossentropy=1.3209192752838135, tune/f1_het=0.02283571846783161, tune/f1_homalt=0.09889934211969376, tune/f1_homref=0.843934178352356, tune/f1_macro=0.3218897581100464, tune/f1_micro=0.22617188096046448, tune/f1_weighted=0.21346084773540497, tune/false_negatives_1=6123.0, tune/false_positives_1=5727.0, tune/loss=1.3209190368652344, tune/precision_1=0.21375617384910583, tune/precision_het=0.19323670864105225, tune/precision_homalt=0.05127762258052826, tune/precision_homref=0.9494163393974304, tune/recall_1=0.20273438096046448, tune/recall_het=0.007176175247877836, tune/recall_homalt=0.834269642829895, tune/recall_homref=0.6971428394317627, tune/true_negatives_1=9633.0, tune/true_positives_1=1557.0. I0508 17:54:10.083408 140534986602304 train.py:394] Warmstart checkpoint best checkpoint metric: tune/f1_weighted=0.21346085. real 1m12.933s. user 0m0.037s. sys 0m0.013s. ```. [train.log](https://github.com/google/deepvariant/files/15253217/train.log).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/819
https://github.com/google/deepvariant/issues/819:2341,availability,checkpoint,checkpoint,2341,"o docker run --runtime=nvidia --gpus 1\. -v ${HOME}:${HOME} \. -w ${HOME} \. google/deepvariant:1.6.1-gpu \. train \. --config=""${BASE}/dv_config.py"":base \. --config.train_dataset_pbtxt=""${BASE}/training_set.pbtxt"" \. --config.tune_dataset_pbtxt=""${BASE}/validation_set.pbtxt"" \. --config.init_checkpoint=""${BASE}/checkpoint/deepvariant.wgs.ckpt"" \. --config.num_epochs=10 \. --config.learning_rate=0.0001 \. --config.num_validation_examples=0 \. --experiment_dir=${TRAINING_DIR} \. --strategy=mirrored \. --config.batch_size=512 \. ) > ""${LOG_DIR}/train.log"" 2>&1 &. ```. ```. I0508 17:53:46.544947 140534986602304 train.py:384] Starting epoch 0. I0508 17:53:46.545100 140534986602304 train.py:391] Performing initial evaluation of warmstart model. I0508 17:53:46.545171 140534986602304 train.py:361] Running tune at step=0 epoch=0. I0508 17:53:46.545287 140534986602304 train.py:366] Tune step 0 / 15 (0.0%). I0508 17:54:10.069682 140512707213056 logging_writer.py:48] [0] tune/categorical_accuracy=0.22617188096046448, tune/categorical_crossentropy=1.3209192752838135, tune/f1_het=0.02283571846783161, tune/f1_homalt=0.09889934211969376, tune/f1_homref=0.843934178352356, tune/f1_macro=0.3218897581100464, tune/f1_micro=0.22617188096046448, tune/f1_weighted=0.21346084773540497, tune/false_negatives_1=6123.0, tune/false_positives_1=5727.0, tune/loss=1.3209190368652344, tune/precision_1=0.21375617384910583, tune/precision_het=0.19323670864105225, tune/precision_homalt=0.05127762258052826, tune/precision_homref=0.9494163393974304, tune/recall_1=0.20273438096046448, tune/recall_het=0.007176175247877836, tune/recall_homalt=0.834269642829895, tune/recall_homref=0.6971428394317627, tune/true_negatives_1=9633.0, tune/true_positives_1=1557.0. I0508 17:54:10.083408 140534986602304 train.py:394] Warmstart checkpoint best checkpoint metric: tune/f1_weighted=0.21346085. real 1m12.933s. user 0m0.037s. sys 0m0.013s. ```. [train.log](https://github.com/google/deepvariant/files/15253217/train.log).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/819
https://github.com/google/deepvariant/issues/819:406,deployability,log,logs,406,"Hi @kishwarshafin,. The training process starts as expected with GPU activity visible, but it abruptly stops without any error message while processing the first epoch and determining the best checkpoint metric (code snippet). This step completes as expected when using the CPU image with the same dataset and parameters. Initially, I thought TensorRT issues might be causing this stop, but I'll share the logs with you to get your perspective and an extra set of eyes on the problem. **Command:** . ```. ( time sudo docker run --runtime=nvidia --gpus 1\. -v ${HOME}:${HOME} \. -w ${HOME} \. google/deepvariant:1.6.1-gpu \. train \. --config=""${BASE}/dv_config.py"":base \. --config.train_dataset_pbtxt=""${BASE}/training_set.pbtxt"" \. --config.tune_dataset_pbtxt=""${BASE}/validation_set.pbtxt"" \. --config.init_checkpoint=""${BASE}/checkpoint/deepvariant.wgs.ckpt"" \. --config.num_epochs=10 \. --config.learning_rate=0.0001 \. --config.num_validation_examples=0 \. --experiment_dir=${TRAINING_DIR} \. --strategy=mirrored \. --config.batch_size=512 \. ) > ""${LOG_DIR}/train.log"" 2>&1 &. ```. ```. I0508 17:53:46.544947 140534986602304 train.py:384] Starting epoch 0. I0508 17:53:46.545100 140534986602304 train.py:391] Performing initial evaluation of warmstart model. I0508 17:53:46.545171 140534986602304 train.py:361] Running tune at step=0 epoch=0. I0508 17:53:46.545287 140534986602304 train.py:366] Tune step 0 / 15 (0.0%). I0508 17:54:10.069682 140512707213056 logging_writer.py:48] [0] tune/categorical_accuracy=0.22617188096046448, tune/categorical_crossentropy=1.3209192752838135, tune/f1_het=0.02283571846783161, tune/f1_homalt=0.09889934211969376, tune/f1_homref=0.843934178352356, tune/f1_macro=0.3218897581100464, tune/f1_micro=0.22617188096046448, tune/f1_weighted=0.21346084773540497, tune/false_negatives_1=6123.0, tune/false_positives_1=5727.0, tune/loss=1.3209190368652344, tune/precision_1=0.21375617384910583, tune/precision_het=0.19323670864105225, tune/precision_homalt=0.05127762",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/819
https://github.com/google/deepvariant/issues/819:1071,deployability,log,log,1071,"vity visible, but it abruptly stops without any error message while processing the first epoch and determining the best checkpoint metric (code snippet). This step completes as expected when using the CPU image with the same dataset and parameters. Initially, I thought TensorRT issues might be causing this stop, but I'll share the logs with you to get your perspective and an extra set of eyes on the problem. **Command:** . ```. ( time sudo docker run --runtime=nvidia --gpus 1\. -v ${HOME}:${HOME} \. -w ${HOME} \. google/deepvariant:1.6.1-gpu \. train \. --config=""${BASE}/dv_config.py"":base \. --config.train_dataset_pbtxt=""${BASE}/training_set.pbtxt"" \. --config.tune_dataset_pbtxt=""${BASE}/validation_set.pbtxt"" \. --config.init_checkpoint=""${BASE}/checkpoint/deepvariant.wgs.ckpt"" \. --config.num_epochs=10 \. --config.learning_rate=0.0001 \. --config.num_validation_examples=0 \. --experiment_dir=${TRAINING_DIR} \. --strategy=mirrored \. --config.batch_size=512 \. ) > ""${LOG_DIR}/train.log"" 2>&1 &. ```. ```. I0508 17:53:46.544947 140534986602304 train.py:384] Starting epoch 0. I0508 17:53:46.545100 140534986602304 train.py:391] Performing initial evaluation of warmstart model. I0508 17:53:46.545171 140534986602304 train.py:361] Running tune at step=0 epoch=0. I0508 17:53:46.545287 140534986602304 train.py:366] Tune step 0 / 15 (0.0%). I0508 17:54:10.069682 140512707213056 logging_writer.py:48] [0] tune/categorical_accuracy=0.22617188096046448, tune/categorical_crossentropy=1.3209192752838135, tune/f1_het=0.02283571846783161, tune/f1_homalt=0.09889934211969376, tune/f1_homref=0.843934178352356, tune/f1_macro=0.3218897581100464, tune/f1_micro=0.22617188096046448, tune/f1_weighted=0.21346084773540497, tune/false_negatives_1=6123.0, tune/false_positives_1=5727.0, tune/loss=1.3209190368652344, tune/precision_1=0.21375617384910583, tune/precision_het=0.19323670864105225, tune/precision_homalt=0.05127762258052826, tune/precision_homref=0.9494163393974304, tune/recall_1=0.2027",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/819
https://github.com/google/deepvariant/issues/819:2446,deployability,log,log,2446,"o docker run --runtime=nvidia --gpus 1\. -v ${HOME}:${HOME} \. -w ${HOME} \. google/deepvariant:1.6.1-gpu \. train \. --config=""${BASE}/dv_config.py"":base \. --config.train_dataset_pbtxt=""${BASE}/training_set.pbtxt"" \. --config.tune_dataset_pbtxt=""${BASE}/validation_set.pbtxt"" \. --config.init_checkpoint=""${BASE}/checkpoint/deepvariant.wgs.ckpt"" \. --config.num_epochs=10 \. --config.learning_rate=0.0001 \. --config.num_validation_examples=0 \. --experiment_dir=${TRAINING_DIR} \. --strategy=mirrored \. --config.batch_size=512 \. ) > ""${LOG_DIR}/train.log"" 2>&1 &. ```. ```. I0508 17:53:46.544947 140534986602304 train.py:384] Starting epoch 0. I0508 17:53:46.545100 140534986602304 train.py:391] Performing initial evaluation of warmstart model. I0508 17:53:46.545171 140534986602304 train.py:361] Running tune at step=0 epoch=0. I0508 17:53:46.545287 140534986602304 train.py:366] Tune step 0 / 15 (0.0%). I0508 17:54:10.069682 140512707213056 logging_writer.py:48] [0] tune/categorical_accuracy=0.22617188096046448, tune/categorical_crossentropy=1.3209192752838135, tune/f1_het=0.02283571846783161, tune/f1_homalt=0.09889934211969376, tune/f1_homref=0.843934178352356, tune/f1_macro=0.3218897581100464, tune/f1_micro=0.22617188096046448, tune/f1_weighted=0.21346084773540497, tune/false_negatives_1=6123.0, tune/false_positives_1=5727.0, tune/loss=1.3209190368652344, tune/precision_1=0.21375617384910583, tune/precision_het=0.19323670864105225, tune/precision_homalt=0.05127762258052826, tune/precision_homref=0.9494163393974304, tune/recall_1=0.20273438096046448, tune/recall_het=0.007176175247877836, tune/recall_homalt=0.834269642829895, tune/recall_homref=0.6971428394317627, tune/true_negatives_1=9633.0, tune/true_positives_1=1557.0. I0508 17:54:10.083408 140534986602304 train.py:394] Warmstart checkpoint best checkpoint metric: tune/f1_weighted=0.21346085. real 1m12.933s. user 0m0.037s. sys 0m0.013s. ```. [train.log](https://github.com/google/deepvariant/files/15253217/train.log).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/819
https://github.com/google/deepvariant/issues/819:2510,deployability,log,log,2510,"o docker run --runtime=nvidia --gpus 1\. -v ${HOME}:${HOME} \. -w ${HOME} \. google/deepvariant:1.6.1-gpu \. train \. --config=""${BASE}/dv_config.py"":base \. --config.train_dataset_pbtxt=""${BASE}/training_set.pbtxt"" \. --config.tune_dataset_pbtxt=""${BASE}/validation_set.pbtxt"" \. --config.init_checkpoint=""${BASE}/checkpoint/deepvariant.wgs.ckpt"" \. --config.num_epochs=10 \. --config.learning_rate=0.0001 \. --config.num_validation_examples=0 \. --experiment_dir=${TRAINING_DIR} \. --strategy=mirrored \. --config.batch_size=512 \. ) > ""${LOG_DIR}/train.log"" 2>&1 &. ```. ```. I0508 17:53:46.544947 140534986602304 train.py:384] Starting epoch 0. I0508 17:53:46.545100 140534986602304 train.py:391] Performing initial evaluation of warmstart model. I0508 17:53:46.545171 140534986602304 train.py:361] Running tune at step=0 epoch=0. I0508 17:53:46.545287 140534986602304 train.py:366] Tune step 0 / 15 (0.0%). I0508 17:54:10.069682 140512707213056 logging_writer.py:48] [0] tune/categorical_accuracy=0.22617188096046448, tune/categorical_crossentropy=1.3209192752838135, tune/f1_het=0.02283571846783161, tune/f1_homalt=0.09889934211969376, tune/f1_homref=0.843934178352356, tune/f1_macro=0.3218897581100464, tune/f1_micro=0.22617188096046448, tune/f1_weighted=0.21346084773540497, tune/false_negatives_1=6123.0, tune/false_positives_1=5727.0, tune/loss=1.3209190368652344, tune/precision_1=0.21375617384910583, tune/precision_het=0.19323670864105225, tune/precision_homalt=0.05127762258052826, tune/precision_homref=0.9494163393974304, tune/recall_1=0.20273438096046448, tune/recall_het=0.007176175247877836, tune/recall_homalt=0.834269642829895, tune/recall_homref=0.6971428394317627, tune/true_negatives_1=9633.0, tune/true_positives_1=1557.0. I0508 17:54:10.083408 140534986602304 train.py:394] Warmstart checkpoint best checkpoint metric: tune/f1_weighted=0.21346085. real 1m12.933s. user 0m0.037s. sys 0m0.013s. ```. [train.log](https://github.com/google/deepvariant/files/15253217/train.log).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/819
https://github.com/google/deepvariant/issues/819:65,energy efficiency,GPU,GPU,65,"Hi @kishwarshafin,. The training process starts as expected with GPU activity visible, but it abruptly stops without any error message while processing the first epoch and determining the best checkpoint metric (code snippet). This step completes as expected when using the CPU image with the same dataset and parameters. Initially, I thought TensorRT issues might be causing this stop, but I'll share the logs with you to get your perspective and an extra set of eyes on the problem. **Command:** . ```. ( time sudo docker run --runtime=nvidia --gpus 1\. -v ${HOME}:${HOME} \. -w ${HOME} \. google/deepvariant:1.6.1-gpu \. train \. --config=""${BASE}/dv_config.py"":base \. --config.train_dataset_pbtxt=""${BASE}/training_set.pbtxt"" \. --config.tune_dataset_pbtxt=""${BASE}/validation_set.pbtxt"" \. --config.init_checkpoint=""${BASE}/checkpoint/deepvariant.wgs.ckpt"" \. --config.num_epochs=10 \. --config.learning_rate=0.0001 \. --config.num_validation_examples=0 \. --experiment_dir=${TRAINING_DIR} \. --strategy=mirrored \. --config.batch_size=512 \. ) > ""${LOG_DIR}/train.log"" 2>&1 &. ```. ```. I0508 17:53:46.544947 140534986602304 train.py:384] Starting epoch 0. I0508 17:53:46.545100 140534986602304 train.py:391] Performing initial evaluation of warmstart model. I0508 17:53:46.545171 140534986602304 train.py:361] Running tune at step=0 epoch=0. I0508 17:53:46.545287 140534986602304 train.py:366] Tune step 0 / 15 (0.0%). I0508 17:54:10.069682 140512707213056 logging_writer.py:48] [0] tune/categorical_accuracy=0.22617188096046448, tune/categorical_crossentropy=1.3209192752838135, tune/f1_het=0.02283571846783161, tune/f1_homalt=0.09889934211969376, tune/f1_homref=0.843934178352356, tune/f1_macro=0.3218897581100464, tune/f1_micro=0.22617188096046448, tune/f1_weighted=0.21346084773540497, tune/false_negatives_1=6123.0, tune/false_positives_1=5727.0, tune/loss=1.3209190368652344, tune/precision_1=0.21375617384910583, tune/precision_het=0.19323670864105225, tune/precision_homalt=0.05127762",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/819
https://github.com/google/deepvariant/issues/819:274,energy efficiency,CPU,CPU,274,"Hi @kishwarshafin,. The training process starts as expected with GPU activity visible, but it abruptly stops without any error message while processing the first epoch and determining the best checkpoint metric (code snippet). This step completes as expected when using the CPU image with the same dataset and parameters. Initially, I thought TensorRT issues might be causing this stop, but I'll share the logs with you to get your perspective and an extra set of eyes on the problem. **Command:** . ```. ( time sudo docker run --runtime=nvidia --gpus 1\. -v ${HOME}:${HOME} \. -w ${HOME} \. google/deepvariant:1.6.1-gpu \. train \. --config=""${BASE}/dv_config.py"":base \. --config.train_dataset_pbtxt=""${BASE}/training_set.pbtxt"" \. --config.tune_dataset_pbtxt=""${BASE}/validation_set.pbtxt"" \. --config.init_checkpoint=""${BASE}/checkpoint/deepvariant.wgs.ckpt"" \. --config.num_epochs=10 \. --config.learning_rate=0.0001 \. --config.num_validation_examples=0 \. --experiment_dir=${TRAINING_DIR} \. --strategy=mirrored \. --config.batch_size=512 \. ) > ""${LOG_DIR}/train.log"" 2>&1 &. ```. ```. I0508 17:53:46.544947 140534986602304 train.py:384] Starting epoch 0. I0508 17:53:46.545100 140534986602304 train.py:391] Performing initial evaluation of warmstart model. I0508 17:53:46.545171 140534986602304 train.py:361] Running tune at step=0 epoch=0. I0508 17:53:46.545287 140534986602304 train.py:366] Tune step 0 / 15 (0.0%). I0508 17:54:10.069682 140512707213056 logging_writer.py:48] [0] tune/categorical_accuracy=0.22617188096046448, tune/categorical_crossentropy=1.3209192752838135, tune/f1_het=0.02283571846783161, tune/f1_homalt=0.09889934211969376, tune/f1_homref=0.843934178352356, tune/f1_macro=0.3218897581100464, tune/f1_micro=0.22617188096046448, tune/f1_weighted=0.21346084773540497, tune/false_negatives_1=6123.0, tune/false_positives_1=5727.0, tune/loss=1.3209190368652344, tune/precision_1=0.21375617384910583, tune/precision_het=0.19323670864105225, tune/precision_homalt=0.05127762",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/819
https://github.com/google/deepvariant/issues/819:547,energy efficiency,gpu,gpus,547,"Hi @kishwarshafin,. The training process starts as expected with GPU activity visible, but it abruptly stops without any error message while processing the first epoch and determining the best checkpoint metric (code snippet). This step completes as expected when using the CPU image with the same dataset and parameters. Initially, I thought TensorRT issues might be causing this stop, but I'll share the logs with you to get your perspective and an extra set of eyes on the problem. **Command:** . ```. ( time sudo docker run --runtime=nvidia --gpus 1\. -v ${HOME}:${HOME} \. -w ${HOME} \. google/deepvariant:1.6.1-gpu \. train \. --config=""${BASE}/dv_config.py"":base \. --config.train_dataset_pbtxt=""${BASE}/training_set.pbtxt"" \. --config.tune_dataset_pbtxt=""${BASE}/validation_set.pbtxt"" \. --config.init_checkpoint=""${BASE}/checkpoint/deepvariant.wgs.ckpt"" \. --config.num_epochs=10 \. --config.learning_rate=0.0001 \. --config.num_validation_examples=0 \. --experiment_dir=${TRAINING_DIR} \. --strategy=mirrored \. --config.batch_size=512 \. ) > ""${LOG_DIR}/train.log"" 2>&1 &. ```. ```. I0508 17:53:46.544947 140534986602304 train.py:384] Starting epoch 0. I0508 17:53:46.545100 140534986602304 train.py:391] Performing initial evaluation of warmstart model. I0508 17:53:46.545171 140534986602304 train.py:361] Running tune at step=0 epoch=0. I0508 17:53:46.545287 140534986602304 train.py:366] Tune step 0 / 15 (0.0%). I0508 17:54:10.069682 140512707213056 logging_writer.py:48] [0] tune/categorical_accuracy=0.22617188096046448, tune/categorical_crossentropy=1.3209192752838135, tune/f1_het=0.02283571846783161, tune/f1_homalt=0.09889934211969376, tune/f1_homref=0.843934178352356, tune/f1_macro=0.3218897581100464, tune/f1_micro=0.22617188096046448, tune/f1_weighted=0.21346084773540497, tune/false_negatives_1=6123.0, tune/false_positives_1=5727.0, tune/loss=1.3209190368652344, tune/precision_1=0.21375617384910583, tune/precision_het=0.19323670864105225, tune/precision_homalt=0.05127762",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/819
https://github.com/google/deepvariant/issues/819:617,energy efficiency,gpu,gpu,617,"Hi @kishwarshafin,. The training process starts as expected with GPU activity visible, but it abruptly stops without any error message while processing the first epoch and determining the best checkpoint metric (code snippet). This step completes as expected when using the CPU image with the same dataset and parameters. Initially, I thought TensorRT issues might be causing this stop, but I'll share the logs with you to get your perspective and an extra set of eyes on the problem. **Command:** . ```. ( time sudo docker run --runtime=nvidia --gpus 1\. -v ${HOME}:${HOME} \. -w ${HOME} \. google/deepvariant:1.6.1-gpu \. train \. --config=""${BASE}/dv_config.py"":base \. --config.train_dataset_pbtxt=""${BASE}/training_set.pbtxt"" \. --config.tune_dataset_pbtxt=""${BASE}/validation_set.pbtxt"" \. --config.init_checkpoint=""${BASE}/checkpoint/deepvariant.wgs.ckpt"" \. --config.num_epochs=10 \. --config.learning_rate=0.0001 \. --config.num_validation_examples=0 \. --experiment_dir=${TRAINING_DIR} \. --strategy=mirrored \. --config.batch_size=512 \. ) > ""${LOG_DIR}/train.log"" 2>&1 &. ```. ```. I0508 17:53:46.544947 140534986602304 train.py:384] Starting epoch 0. I0508 17:53:46.545100 140534986602304 train.py:391] Performing initial evaluation of warmstart model. I0508 17:53:46.545171 140534986602304 train.py:361] Running tune at step=0 epoch=0. I0508 17:53:46.545287 140534986602304 train.py:366] Tune step 0 / 15 (0.0%). I0508 17:54:10.069682 140512707213056 logging_writer.py:48] [0] tune/categorical_accuracy=0.22617188096046448, tune/categorical_crossentropy=1.3209192752838135, tune/f1_het=0.02283571846783161, tune/f1_homalt=0.09889934211969376, tune/f1_homref=0.843934178352356, tune/f1_macro=0.3218897581100464, tune/f1_micro=0.22617188096046448, tune/f1_weighted=0.21346084773540497, tune/false_negatives_1=6123.0, tune/false_positives_1=5727.0, tune/loss=1.3209190368652344, tune/precision_1=0.21375617384910583, tune/precision_het=0.19323670864105225, tune/precision_homalt=0.05127762",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/819
https://github.com/google/deepvariant/issues/819:1259,energy efficiency,model,model,1259,"n using the CPU image with the same dataset and parameters. Initially, I thought TensorRT issues might be causing this stop, but I'll share the logs with you to get your perspective and an extra set of eyes on the problem. **Command:** . ```. ( time sudo docker run --runtime=nvidia --gpus 1\. -v ${HOME}:${HOME} \. -w ${HOME} \. google/deepvariant:1.6.1-gpu \. train \. --config=""${BASE}/dv_config.py"":base \. --config.train_dataset_pbtxt=""${BASE}/training_set.pbtxt"" \. --config.tune_dataset_pbtxt=""${BASE}/validation_set.pbtxt"" \. --config.init_checkpoint=""${BASE}/checkpoint/deepvariant.wgs.ckpt"" \. --config.num_epochs=10 \. --config.learning_rate=0.0001 \. --config.num_validation_examples=0 \. --experiment_dir=${TRAINING_DIR} \. --strategy=mirrored \. --config.batch_size=512 \. ) > ""${LOG_DIR}/train.log"" 2>&1 &. ```. ```. I0508 17:53:46.544947 140534986602304 train.py:384] Starting epoch 0. I0508 17:53:46.545100 140534986602304 train.py:391] Performing initial evaluation of warmstart model. I0508 17:53:46.545171 140534986602304 train.py:361] Running tune at step=0 epoch=0. I0508 17:53:46.545287 140534986602304 train.py:366] Tune step 0 / 15 (0.0%). I0508 17:54:10.069682 140512707213056 logging_writer.py:48] [0] tune/categorical_accuracy=0.22617188096046448, tune/categorical_crossentropy=1.3209192752838135, tune/f1_het=0.02283571846783161, tune/f1_homalt=0.09889934211969376, tune/f1_homref=0.843934178352356, tune/f1_macro=0.3218897581100464, tune/f1_micro=0.22617188096046448, tune/f1_weighted=0.21346084773540497, tune/false_negatives_1=6123.0, tune/false_positives_1=5727.0, tune/loss=1.3209190368652344, tune/precision_1=0.21375617384910583, tune/precision_het=0.19323670864105225, tune/precision_homalt=0.05127762258052826, tune/precision_homref=0.9494163393974304, tune/recall_1=0.20273438096046448, tune/recall_het=0.007176175247877836, tune/recall_homalt=0.834269642829895, tune/recall_homref=0.6971428394317627, tune/true_negatives_1=9633.0, tune/true_positives_1=1557.0.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/819
https://github.com/google/deepvariant/issues/819:127,integrability,messag,message,127,"Hi @kishwarshafin,. The training process starts as expected with GPU activity visible, but it abruptly stops without any error message while processing the first epoch and determining the best checkpoint metric (code snippet). This step completes as expected when using the CPU image with the same dataset and parameters. Initially, I thought TensorRT issues might be causing this stop, but I'll share the logs with you to get your perspective and an extra set of eyes on the problem. **Command:** . ```. ( time sudo docker run --runtime=nvidia --gpus 1\. -v ${HOME}:${HOME} \. -w ${HOME} \. google/deepvariant:1.6.1-gpu \. train \. --config=""${BASE}/dv_config.py"":base \. --config.train_dataset_pbtxt=""${BASE}/training_set.pbtxt"" \. --config.tune_dataset_pbtxt=""${BASE}/validation_set.pbtxt"" \. --config.init_checkpoint=""${BASE}/checkpoint/deepvariant.wgs.ckpt"" \. --config.num_epochs=10 \. --config.learning_rate=0.0001 \. --config.num_validation_examples=0 \. --experiment_dir=${TRAINING_DIR} \. --strategy=mirrored \. --config.batch_size=512 \. ) > ""${LOG_DIR}/train.log"" 2>&1 &. ```. ```. I0508 17:53:46.544947 140534986602304 train.py:384] Starting epoch 0. I0508 17:53:46.545100 140534986602304 train.py:391] Performing initial evaluation of warmstart model. I0508 17:53:46.545171 140534986602304 train.py:361] Running tune at step=0 epoch=0. I0508 17:53:46.545287 140534986602304 train.py:366] Tune step 0 / 15 (0.0%). I0508 17:54:10.069682 140512707213056 logging_writer.py:48] [0] tune/categorical_accuracy=0.22617188096046448, tune/categorical_crossentropy=1.3209192752838135, tune/f1_het=0.02283571846783161, tune/f1_homalt=0.09889934211969376, tune/f1_homref=0.843934178352356, tune/f1_macro=0.3218897581100464, tune/f1_micro=0.22617188096046448, tune/f1_weighted=0.21346084773540497, tune/false_negatives_1=6123.0, tune/false_positives_1=5727.0, tune/loss=1.3209190368652344, tune/precision_1=0.21375617384910583, tune/precision_het=0.19323670864105225, tune/precision_homalt=0.05127762",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/819
https://github.com/google/deepvariant/issues/819:127,interoperability,messag,message,127,"Hi @kishwarshafin,. The training process starts as expected with GPU activity visible, but it abruptly stops without any error message while processing the first epoch and determining the best checkpoint metric (code snippet). This step completes as expected when using the CPU image with the same dataset and parameters. Initially, I thought TensorRT issues might be causing this stop, but I'll share the logs with you to get your perspective and an extra set of eyes on the problem. **Command:** . ```. ( time sudo docker run --runtime=nvidia --gpus 1\. -v ${HOME}:${HOME} \. -w ${HOME} \. google/deepvariant:1.6.1-gpu \. train \. --config=""${BASE}/dv_config.py"":base \. --config.train_dataset_pbtxt=""${BASE}/training_set.pbtxt"" \. --config.tune_dataset_pbtxt=""${BASE}/validation_set.pbtxt"" \. --config.init_checkpoint=""${BASE}/checkpoint/deepvariant.wgs.ckpt"" \. --config.num_epochs=10 \. --config.learning_rate=0.0001 \. --config.num_validation_examples=0 \. --experiment_dir=${TRAINING_DIR} \. --strategy=mirrored \. --config.batch_size=512 \. ) > ""${LOG_DIR}/train.log"" 2>&1 &. ```. ```. I0508 17:53:46.544947 140534986602304 train.py:384] Starting epoch 0. I0508 17:53:46.545100 140534986602304 train.py:391] Performing initial evaluation of warmstart model. I0508 17:53:46.545171 140534986602304 train.py:361] Running tune at step=0 epoch=0. I0508 17:53:46.545287 140534986602304 train.py:366] Tune step 0 / 15 (0.0%). I0508 17:54:10.069682 140512707213056 logging_writer.py:48] [0] tune/categorical_accuracy=0.22617188096046448, tune/categorical_crossentropy=1.3209192752838135, tune/f1_het=0.02283571846783161, tune/f1_homalt=0.09889934211969376, tune/f1_homref=0.843934178352356, tune/f1_macro=0.3218897581100464, tune/f1_micro=0.22617188096046448, tune/f1_weighted=0.21346084773540497, tune/false_negatives_1=6123.0, tune/false_positives_1=5727.0, tune/loss=1.3209190368652344, tune/precision_1=0.21375617384910583, tune/precision_het=0.19323670864105225, tune/precision_homalt=0.05127762",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/819
https://github.com/google/deepvariant/issues/819:396,interoperability,share,share,396,"Hi @kishwarshafin,. The training process starts as expected with GPU activity visible, but it abruptly stops without any error message while processing the first epoch and determining the best checkpoint metric (code snippet). This step completes as expected when using the CPU image with the same dataset and parameters. Initially, I thought TensorRT issues might be causing this stop, but I'll share the logs with you to get your perspective and an extra set of eyes on the problem. **Command:** . ```. ( time sudo docker run --runtime=nvidia --gpus 1\. -v ${HOME}:${HOME} \. -w ${HOME} \. google/deepvariant:1.6.1-gpu \. train \. --config=""${BASE}/dv_config.py"":base \. --config.train_dataset_pbtxt=""${BASE}/training_set.pbtxt"" \. --config.tune_dataset_pbtxt=""${BASE}/validation_set.pbtxt"" \. --config.init_checkpoint=""${BASE}/checkpoint/deepvariant.wgs.ckpt"" \. --config.num_epochs=10 \. --config.learning_rate=0.0001 \. --config.num_validation_examples=0 \. --experiment_dir=${TRAINING_DIR} \. --strategy=mirrored \. --config.batch_size=512 \. ) > ""${LOG_DIR}/train.log"" 2>&1 &. ```. ```. I0508 17:53:46.544947 140534986602304 train.py:384] Starting epoch 0. I0508 17:53:46.545100 140534986602304 train.py:391] Performing initial evaluation of warmstart model. I0508 17:53:46.545171 140534986602304 train.py:361] Running tune at step=0 epoch=0. I0508 17:53:46.545287 140534986602304 train.py:366] Tune step 0 / 15 (0.0%). I0508 17:54:10.069682 140512707213056 logging_writer.py:48] [0] tune/categorical_accuracy=0.22617188096046448, tune/categorical_crossentropy=1.3209192752838135, tune/f1_het=0.02283571846783161, tune/f1_homalt=0.09889934211969376, tune/f1_homref=0.843934178352356, tune/f1_macro=0.3218897581100464, tune/f1_micro=0.22617188096046448, tune/f1_weighted=0.21346084773540497, tune/false_negatives_1=6123.0, tune/false_positives_1=5727.0, tune/loss=1.3209190368652344, tune/precision_1=0.21375617384910583, tune/precision_het=0.19323670864105225, tune/precision_homalt=0.05127762",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/819
https://github.com/google/deepvariant/issues/819:310,modifiability,paramet,parameters,310,"Hi @kishwarshafin,. The training process starts as expected with GPU activity visible, but it abruptly stops without any error message while processing the first epoch and determining the best checkpoint metric (code snippet). This step completes as expected when using the CPU image with the same dataset and parameters. Initially, I thought TensorRT issues might be causing this stop, but I'll share the logs with you to get your perspective and an extra set of eyes on the problem. **Command:** . ```. ( time sudo docker run --runtime=nvidia --gpus 1\. -v ${HOME}:${HOME} \. -w ${HOME} \. google/deepvariant:1.6.1-gpu \. train \. --config=""${BASE}/dv_config.py"":base \. --config.train_dataset_pbtxt=""${BASE}/training_set.pbtxt"" \. --config.tune_dataset_pbtxt=""${BASE}/validation_set.pbtxt"" \. --config.init_checkpoint=""${BASE}/checkpoint/deepvariant.wgs.ckpt"" \. --config.num_epochs=10 \. --config.learning_rate=0.0001 \. --config.num_validation_examples=0 \. --experiment_dir=${TRAINING_DIR} \. --strategy=mirrored \. --config.batch_size=512 \. ) > ""${LOG_DIR}/train.log"" 2>&1 &. ```. ```. I0508 17:53:46.544947 140534986602304 train.py:384] Starting epoch 0. I0508 17:53:46.545100 140534986602304 train.py:391] Performing initial evaluation of warmstart model. I0508 17:53:46.545171 140534986602304 train.py:361] Running tune at step=0 epoch=0. I0508 17:53:46.545287 140534986602304 train.py:366] Tune step 0 / 15 (0.0%). I0508 17:54:10.069682 140512707213056 logging_writer.py:48] [0] tune/categorical_accuracy=0.22617188096046448, tune/categorical_crossentropy=1.3209192752838135, tune/f1_het=0.02283571846783161, tune/f1_homalt=0.09889934211969376, tune/f1_homref=0.843934178352356, tune/f1_macro=0.3218897581100464, tune/f1_micro=0.22617188096046448, tune/f1_weighted=0.21346084773540497, tune/false_negatives_1=6123.0, tune/false_positives_1=5727.0, tune/loss=1.3209190368652344, tune/precision_1=0.21375617384910583, tune/precision_het=0.19323670864105225, tune/precision_homalt=0.05127762",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/819
https://github.com/google/deepvariant/issues/819:65,performance,GPU,GPU,65,"Hi @kishwarshafin,. The training process starts as expected with GPU activity visible, but it abruptly stops without any error message while processing the first epoch and determining the best checkpoint metric (code snippet). This step completes as expected when using the CPU image with the same dataset and parameters. Initially, I thought TensorRT issues might be causing this stop, but I'll share the logs with you to get your perspective and an extra set of eyes on the problem. **Command:** . ```. ( time sudo docker run --runtime=nvidia --gpus 1\. -v ${HOME}:${HOME} \. -w ${HOME} \. google/deepvariant:1.6.1-gpu \. train \. --config=""${BASE}/dv_config.py"":base \. --config.train_dataset_pbtxt=""${BASE}/training_set.pbtxt"" \. --config.tune_dataset_pbtxt=""${BASE}/validation_set.pbtxt"" \. --config.init_checkpoint=""${BASE}/checkpoint/deepvariant.wgs.ckpt"" \. --config.num_epochs=10 \. --config.learning_rate=0.0001 \. --config.num_validation_examples=0 \. --experiment_dir=${TRAINING_DIR} \. --strategy=mirrored \. --config.batch_size=512 \. ) > ""${LOG_DIR}/train.log"" 2>&1 &. ```. ```. I0508 17:53:46.544947 140534986602304 train.py:384] Starting epoch 0. I0508 17:53:46.545100 140534986602304 train.py:391] Performing initial evaluation of warmstart model. I0508 17:53:46.545171 140534986602304 train.py:361] Running tune at step=0 epoch=0. I0508 17:53:46.545287 140534986602304 train.py:366] Tune step 0 / 15 (0.0%). I0508 17:54:10.069682 140512707213056 logging_writer.py:48] [0] tune/categorical_accuracy=0.22617188096046448, tune/categorical_crossentropy=1.3209192752838135, tune/f1_het=0.02283571846783161, tune/f1_homalt=0.09889934211969376, tune/f1_homref=0.843934178352356, tune/f1_macro=0.3218897581100464, tune/f1_micro=0.22617188096046448, tune/f1_weighted=0.21346084773540497, tune/false_negatives_1=6123.0, tune/false_positives_1=5727.0, tune/loss=1.3209190368652344, tune/precision_1=0.21375617384910583, tune/precision_het=0.19323670864105225, tune/precision_homalt=0.05127762",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/819
https://github.com/google/deepvariant/issues/819:121,performance,error,error,121,"Hi @kishwarshafin,. The training process starts as expected with GPU activity visible, but it abruptly stops without any error message while processing the first epoch and determining the best checkpoint metric (code snippet). This step completes as expected when using the CPU image with the same dataset and parameters. Initially, I thought TensorRT issues might be causing this stop, but I'll share the logs with you to get your perspective and an extra set of eyes on the problem. **Command:** . ```. ( time sudo docker run --runtime=nvidia --gpus 1\. -v ${HOME}:${HOME} \. -w ${HOME} \. google/deepvariant:1.6.1-gpu \. train \. --config=""${BASE}/dv_config.py"":base \. --config.train_dataset_pbtxt=""${BASE}/training_set.pbtxt"" \. --config.tune_dataset_pbtxt=""${BASE}/validation_set.pbtxt"" \. --config.init_checkpoint=""${BASE}/checkpoint/deepvariant.wgs.ckpt"" \. --config.num_epochs=10 \. --config.learning_rate=0.0001 \. --config.num_validation_examples=0 \. --experiment_dir=${TRAINING_DIR} \. --strategy=mirrored \. --config.batch_size=512 \. ) > ""${LOG_DIR}/train.log"" 2>&1 &. ```. ```. I0508 17:53:46.544947 140534986602304 train.py:384] Starting epoch 0. I0508 17:53:46.545100 140534986602304 train.py:391] Performing initial evaluation of warmstart model. I0508 17:53:46.545171 140534986602304 train.py:361] Running tune at step=0 epoch=0. I0508 17:53:46.545287 140534986602304 train.py:366] Tune step 0 / 15 (0.0%). I0508 17:54:10.069682 140512707213056 logging_writer.py:48] [0] tune/categorical_accuracy=0.22617188096046448, tune/categorical_crossentropy=1.3209192752838135, tune/f1_het=0.02283571846783161, tune/f1_homalt=0.09889934211969376, tune/f1_homref=0.843934178352356, tune/f1_macro=0.3218897581100464, tune/f1_micro=0.22617188096046448, tune/f1_weighted=0.21346084773540497, tune/false_negatives_1=6123.0, tune/false_positives_1=5727.0, tune/loss=1.3209190368652344, tune/precision_1=0.21375617384910583, tune/precision_het=0.19323670864105225, tune/precision_homalt=0.05127762",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/819
https://github.com/google/deepvariant/issues/819:274,performance,CPU,CPU,274,"Hi @kishwarshafin,. The training process starts as expected with GPU activity visible, but it abruptly stops without any error message while processing the first epoch and determining the best checkpoint metric (code snippet). This step completes as expected when using the CPU image with the same dataset and parameters. Initially, I thought TensorRT issues might be causing this stop, but I'll share the logs with you to get your perspective and an extra set of eyes on the problem. **Command:** . ```. ( time sudo docker run --runtime=nvidia --gpus 1\. -v ${HOME}:${HOME} \. -w ${HOME} \. google/deepvariant:1.6.1-gpu \. train \. --config=""${BASE}/dv_config.py"":base \. --config.train_dataset_pbtxt=""${BASE}/training_set.pbtxt"" \. --config.tune_dataset_pbtxt=""${BASE}/validation_set.pbtxt"" \. --config.init_checkpoint=""${BASE}/checkpoint/deepvariant.wgs.ckpt"" \. --config.num_epochs=10 \. --config.learning_rate=0.0001 \. --config.num_validation_examples=0 \. --experiment_dir=${TRAINING_DIR} \. --strategy=mirrored \. --config.batch_size=512 \. ) > ""${LOG_DIR}/train.log"" 2>&1 &. ```. ```. I0508 17:53:46.544947 140534986602304 train.py:384] Starting epoch 0. I0508 17:53:46.545100 140534986602304 train.py:391] Performing initial evaluation of warmstart model. I0508 17:53:46.545171 140534986602304 train.py:361] Running tune at step=0 epoch=0. I0508 17:53:46.545287 140534986602304 train.py:366] Tune step 0 / 15 (0.0%). I0508 17:54:10.069682 140512707213056 logging_writer.py:48] [0] tune/categorical_accuracy=0.22617188096046448, tune/categorical_crossentropy=1.3209192752838135, tune/f1_het=0.02283571846783161, tune/f1_homalt=0.09889934211969376, tune/f1_homref=0.843934178352356, tune/f1_macro=0.3218897581100464, tune/f1_micro=0.22617188096046448, tune/f1_weighted=0.21346084773540497, tune/false_negatives_1=6123.0, tune/false_positives_1=5727.0, tune/loss=1.3209190368652344, tune/precision_1=0.21375617384910583, tune/precision_het=0.19323670864105225, tune/precision_homalt=0.05127762",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/819
https://github.com/google/deepvariant/issues/819:507,performance,time,time,507,"Hi @kishwarshafin,. The training process starts as expected with GPU activity visible, but it abruptly stops without any error message while processing the first epoch and determining the best checkpoint metric (code snippet). This step completes as expected when using the CPU image with the same dataset and parameters. Initially, I thought TensorRT issues might be causing this stop, but I'll share the logs with you to get your perspective and an extra set of eyes on the problem. **Command:** . ```. ( time sudo docker run --runtime=nvidia --gpus 1\. -v ${HOME}:${HOME} \. -w ${HOME} \. google/deepvariant:1.6.1-gpu \. train \. --config=""${BASE}/dv_config.py"":base \. --config.train_dataset_pbtxt=""${BASE}/training_set.pbtxt"" \. --config.tune_dataset_pbtxt=""${BASE}/validation_set.pbtxt"" \. --config.init_checkpoint=""${BASE}/checkpoint/deepvariant.wgs.ckpt"" \. --config.num_epochs=10 \. --config.learning_rate=0.0001 \. --config.num_validation_examples=0 \. --experiment_dir=${TRAINING_DIR} \. --strategy=mirrored \. --config.batch_size=512 \. ) > ""${LOG_DIR}/train.log"" 2>&1 &. ```. ```. I0508 17:53:46.544947 140534986602304 train.py:384] Starting epoch 0. I0508 17:53:46.545100 140534986602304 train.py:391] Performing initial evaluation of warmstart model. I0508 17:53:46.545171 140534986602304 train.py:361] Running tune at step=0 epoch=0. I0508 17:53:46.545287 140534986602304 train.py:366] Tune step 0 / 15 (0.0%). I0508 17:54:10.069682 140512707213056 logging_writer.py:48] [0] tune/categorical_accuracy=0.22617188096046448, tune/categorical_crossentropy=1.3209192752838135, tune/f1_het=0.02283571846783161, tune/f1_homalt=0.09889934211969376, tune/f1_homref=0.843934178352356, tune/f1_macro=0.3218897581100464, tune/f1_micro=0.22617188096046448, tune/f1_weighted=0.21346084773540497, tune/false_negatives_1=6123.0, tune/false_positives_1=5727.0, tune/loss=1.3209190368652344, tune/precision_1=0.21375617384910583, tune/precision_het=0.19323670864105225, tune/precision_homalt=0.05127762",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/819
https://github.com/google/deepvariant/issues/819:547,performance,gpu,gpus,547,"Hi @kishwarshafin,. The training process starts as expected with GPU activity visible, but it abruptly stops without any error message while processing the first epoch and determining the best checkpoint metric (code snippet). This step completes as expected when using the CPU image with the same dataset and parameters. Initially, I thought TensorRT issues might be causing this stop, but I'll share the logs with you to get your perspective and an extra set of eyes on the problem. **Command:** . ```. ( time sudo docker run --runtime=nvidia --gpus 1\. -v ${HOME}:${HOME} \. -w ${HOME} \. google/deepvariant:1.6.1-gpu \. train \. --config=""${BASE}/dv_config.py"":base \. --config.train_dataset_pbtxt=""${BASE}/training_set.pbtxt"" \. --config.tune_dataset_pbtxt=""${BASE}/validation_set.pbtxt"" \. --config.init_checkpoint=""${BASE}/checkpoint/deepvariant.wgs.ckpt"" \. --config.num_epochs=10 \. --config.learning_rate=0.0001 \. --config.num_validation_examples=0 \. --experiment_dir=${TRAINING_DIR} \. --strategy=mirrored \. --config.batch_size=512 \. ) > ""${LOG_DIR}/train.log"" 2>&1 &. ```. ```. I0508 17:53:46.544947 140534986602304 train.py:384] Starting epoch 0. I0508 17:53:46.545100 140534986602304 train.py:391] Performing initial evaluation of warmstart model. I0508 17:53:46.545171 140534986602304 train.py:361] Running tune at step=0 epoch=0. I0508 17:53:46.545287 140534986602304 train.py:366] Tune step 0 / 15 (0.0%). I0508 17:54:10.069682 140512707213056 logging_writer.py:48] [0] tune/categorical_accuracy=0.22617188096046448, tune/categorical_crossentropy=1.3209192752838135, tune/f1_het=0.02283571846783161, tune/f1_homalt=0.09889934211969376, tune/f1_homref=0.843934178352356, tune/f1_macro=0.3218897581100464, tune/f1_micro=0.22617188096046448, tune/f1_weighted=0.21346084773540497, tune/false_negatives_1=6123.0, tune/false_positives_1=5727.0, tune/loss=1.3209190368652344, tune/precision_1=0.21375617384910583, tune/precision_het=0.19323670864105225, tune/precision_homalt=0.05127762",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/819
https://github.com/google/deepvariant/issues/819:617,performance,gpu,gpu,617,"Hi @kishwarshafin,. The training process starts as expected with GPU activity visible, but it abruptly stops without any error message while processing the first epoch and determining the best checkpoint metric (code snippet). This step completes as expected when using the CPU image with the same dataset and parameters. Initially, I thought TensorRT issues might be causing this stop, but I'll share the logs with you to get your perspective and an extra set of eyes on the problem. **Command:** . ```. ( time sudo docker run --runtime=nvidia --gpus 1\. -v ${HOME}:${HOME} \. -w ${HOME} \. google/deepvariant:1.6.1-gpu \. train \. --config=""${BASE}/dv_config.py"":base \. --config.train_dataset_pbtxt=""${BASE}/training_set.pbtxt"" \. --config.tune_dataset_pbtxt=""${BASE}/validation_set.pbtxt"" \. --config.init_checkpoint=""${BASE}/checkpoint/deepvariant.wgs.ckpt"" \. --config.num_epochs=10 \. --config.learning_rate=0.0001 \. --config.num_validation_examples=0 \. --experiment_dir=${TRAINING_DIR} \. --strategy=mirrored \. --config.batch_size=512 \. ) > ""${LOG_DIR}/train.log"" 2>&1 &. ```. ```. I0508 17:53:46.544947 140534986602304 train.py:384] Starting epoch 0. I0508 17:53:46.545100 140534986602304 train.py:391] Performing initial evaluation of warmstart model. I0508 17:53:46.545171 140534986602304 train.py:361] Running tune at step=0 epoch=0. I0508 17:53:46.545287 140534986602304 train.py:366] Tune step 0 / 15 (0.0%). I0508 17:54:10.069682 140512707213056 logging_writer.py:48] [0] tune/categorical_accuracy=0.22617188096046448, tune/categorical_crossentropy=1.3209192752838135, tune/f1_het=0.02283571846783161, tune/f1_homalt=0.09889934211969376, tune/f1_homref=0.843934178352356, tune/f1_macro=0.3218897581100464, tune/f1_micro=0.22617188096046448, tune/f1_weighted=0.21346084773540497, tune/false_negatives_1=6123.0, tune/false_positives_1=5727.0, tune/loss=1.3209190368652344, tune/precision_1=0.21375617384910583, tune/precision_het=0.19323670864105225, tune/precision_homalt=0.05127762",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/819
https://github.com/google/deepvariant/issues/819:1216,performance,Perform,Performing,1216,"pet). This step completes as expected when using the CPU image with the same dataset and parameters. Initially, I thought TensorRT issues might be causing this stop, but I'll share the logs with you to get your perspective and an extra set of eyes on the problem. **Command:** . ```. ( time sudo docker run --runtime=nvidia --gpus 1\. -v ${HOME}:${HOME} \. -w ${HOME} \. google/deepvariant:1.6.1-gpu \. train \. --config=""${BASE}/dv_config.py"":base \. --config.train_dataset_pbtxt=""${BASE}/training_set.pbtxt"" \. --config.tune_dataset_pbtxt=""${BASE}/validation_set.pbtxt"" \. --config.init_checkpoint=""${BASE}/checkpoint/deepvariant.wgs.ckpt"" \. --config.num_epochs=10 \. --config.learning_rate=0.0001 \. --config.num_validation_examples=0 \. --experiment_dir=${TRAINING_DIR} \. --strategy=mirrored \. --config.batch_size=512 \. ) > ""${LOG_DIR}/train.log"" 2>&1 &. ```. ```. I0508 17:53:46.544947 140534986602304 train.py:384] Starting epoch 0. I0508 17:53:46.545100 140534986602304 train.py:391] Performing initial evaluation of warmstart model. I0508 17:53:46.545171 140534986602304 train.py:361] Running tune at step=0 epoch=0. I0508 17:53:46.545287 140534986602304 train.py:366] Tune step 0 / 15 (0.0%). I0508 17:54:10.069682 140512707213056 logging_writer.py:48] [0] tune/categorical_accuracy=0.22617188096046448, tune/categorical_crossentropy=1.3209192752838135, tune/f1_het=0.02283571846783161, tune/f1_homalt=0.09889934211969376, tune/f1_homref=0.843934178352356, tune/f1_macro=0.3218897581100464, tune/f1_micro=0.22617188096046448, tune/f1_weighted=0.21346084773540497, tune/false_negatives_1=6123.0, tune/false_positives_1=5727.0, tune/loss=1.3209190368652344, tune/precision_1=0.21375617384910583, tune/precision_het=0.19323670864105225, tune/precision_homalt=0.05127762258052826, tune/precision_homref=0.9494163393974304, tune/recall_1=0.20273438096046448, tune/recall_het=0.007176175247877836, tune/recall_homalt=0.834269642829895, tune/recall_homref=0.6971428394317627, tune/true_negative",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/819
https://github.com/google/deepvariant/issues/819:1326,performance,tune,tune,1326,"lly, I thought TensorRT issues might be causing this stop, but I'll share the logs with you to get your perspective and an extra set of eyes on the problem. **Command:** . ```. ( time sudo docker run --runtime=nvidia --gpus 1\. -v ${HOME}:${HOME} \. -w ${HOME} \. google/deepvariant:1.6.1-gpu \. train \. --config=""${BASE}/dv_config.py"":base \. --config.train_dataset_pbtxt=""${BASE}/training_set.pbtxt"" \. --config.tune_dataset_pbtxt=""${BASE}/validation_set.pbtxt"" \. --config.init_checkpoint=""${BASE}/checkpoint/deepvariant.wgs.ckpt"" \. --config.num_epochs=10 \. --config.learning_rate=0.0001 \. --config.num_validation_examples=0 \. --experiment_dir=${TRAINING_DIR} \. --strategy=mirrored \. --config.batch_size=512 \. ) > ""${LOG_DIR}/train.log"" 2>&1 &. ```. ```. I0508 17:53:46.544947 140534986602304 train.py:384] Starting epoch 0. I0508 17:53:46.545100 140534986602304 train.py:391] Performing initial evaluation of warmstart model. I0508 17:53:46.545171 140534986602304 train.py:361] Running tune at step=0 epoch=0. I0508 17:53:46.545287 140534986602304 train.py:366] Tune step 0 / 15 (0.0%). I0508 17:54:10.069682 140512707213056 logging_writer.py:48] [0] tune/categorical_accuracy=0.22617188096046448, tune/categorical_crossentropy=1.3209192752838135, tune/f1_het=0.02283571846783161, tune/f1_homalt=0.09889934211969376, tune/f1_homref=0.843934178352356, tune/f1_macro=0.3218897581100464, tune/f1_micro=0.22617188096046448, tune/f1_weighted=0.21346084773540497, tune/false_negatives_1=6123.0, tune/false_positives_1=5727.0, tune/loss=1.3209190368652344, tune/precision_1=0.21375617384910583, tune/precision_het=0.19323670864105225, tune/precision_homalt=0.05127762258052826, tune/precision_homref=0.9494163393974304, tune/recall_1=0.20273438096046448, tune/recall_het=0.007176175247877836, tune/recall_homalt=0.834269642829895, tune/recall_homref=0.6971428394317627, tune/true_negatives_1=9633.0, tune/true_positives_1=1557.0. I0508 17:54:10.083408 140534986602304 train.py:394] Warmstart che",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/819
https://github.com/google/deepvariant/issues/819:1402,performance,Tune,Tune,1402,"e logs with you to get your perspective and an extra set of eyes on the problem. **Command:** . ```. ( time sudo docker run --runtime=nvidia --gpus 1\. -v ${HOME}:${HOME} \. -w ${HOME} \. google/deepvariant:1.6.1-gpu \. train \. --config=""${BASE}/dv_config.py"":base \. --config.train_dataset_pbtxt=""${BASE}/training_set.pbtxt"" \. --config.tune_dataset_pbtxt=""${BASE}/validation_set.pbtxt"" \. --config.init_checkpoint=""${BASE}/checkpoint/deepvariant.wgs.ckpt"" \. --config.num_epochs=10 \. --config.learning_rate=0.0001 \. --config.num_validation_examples=0 \. --experiment_dir=${TRAINING_DIR} \. --strategy=mirrored \. --config.batch_size=512 \. ) > ""${LOG_DIR}/train.log"" 2>&1 &. ```. ```. I0508 17:53:46.544947 140534986602304 train.py:384] Starting epoch 0. I0508 17:53:46.545100 140534986602304 train.py:391] Performing initial evaluation of warmstart model. I0508 17:53:46.545171 140534986602304 train.py:361] Running tune at step=0 epoch=0. I0508 17:53:46.545287 140534986602304 train.py:366] Tune step 0 / 15 (0.0%). I0508 17:54:10.069682 140512707213056 logging_writer.py:48] [0] tune/categorical_accuracy=0.22617188096046448, tune/categorical_crossentropy=1.3209192752838135, tune/f1_het=0.02283571846783161, tune/f1_homalt=0.09889934211969376, tune/f1_homref=0.843934178352356, tune/f1_macro=0.3218897581100464, tune/f1_micro=0.22617188096046448, tune/f1_weighted=0.21346084773540497, tune/false_negatives_1=6123.0, tune/false_positives_1=5727.0, tune/loss=1.3209190368652344, tune/precision_1=0.21375617384910583, tune/precision_het=0.19323670864105225, tune/precision_homalt=0.05127762258052826, tune/precision_homref=0.9494163393974304, tune/recall_1=0.20273438096046448, tune/recall_het=0.007176175247877836, tune/recall_homalt=0.834269642829895, tune/recall_homref=0.6971428394317627, tune/true_negatives_1=9633.0, tune/true_positives_1=1557.0. I0508 17:54:10.083408 140534986602304 train.py:394] Warmstart checkpoint best checkpoint metric: tune/f1_weighted=0.21346085. real 1m12.933s.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/819
https://github.com/google/deepvariant/issues/819:1491,performance,tune,tune,1491,"d:** . ```. ( time sudo docker run --runtime=nvidia --gpus 1\. -v ${HOME}:${HOME} \. -w ${HOME} \. google/deepvariant:1.6.1-gpu \. train \. --config=""${BASE}/dv_config.py"":base \. --config.train_dataset_pbtxt=""${BASE}/training_set.pbtxt"" \. --config.tune_dataset_pbtxt=""${BASE}/validation_set.pbtxt"" \. --config.init_checkpoint=""${BASE}/checkpoint/deepvariant.wgs.ckpt"" \. --config.num_epochs=10 \. --config.learning_rate=0.0001 \. --config.num_validation_examples=0 \. --experiment_dir=${TRAINING_DIR} \. --strategy=mirrored \. --config.batch_size=512 \. ) > ""${LOG_DIR}/train.log"" 2>&1 &. ```. ```. I0508 17:53:46.544947 140534986602304 train.py:384] Starting epoch 0. I0508 17:53:46.545100 140534986602304 train.py:391] Performing initial evaluation of warmstart model. I0508 17:53:46.545171 140534986602304 train.py:361] Running tune at step=0 epoch=0. I0508 17:53:46.545287 140534986602304 train.py:366] Tune step 0 / 15 (0.0%). I0508 17:54:10.069682 140512707213056 logging_writer.py:48] [0] tune/categorical_accuracy=0.22617188096046448, tune/categorical_crossentropy=1.3209192752838135, tune/f1_het=0.02283571846783161, tune/f1_homalt=0.09889934211969376, tune/f1_homref=0.843934178352356, tune/f1_macro=0.3218897581100464, tune/f1_micro=0.22617188096046448, tune/f1_weighted=0.21346084773540497, tune/false_negatives_1=6123.0, tune/false_positives_1=5727.0, tune/loss=1.3209190368652344, tune/precision_1=0.21375617384910583, tune/precision_het=0.19323670864105225, tune/precision_homalt=0.05127762258052826, tune/precision_homref=0.9494163393974304, tune/recall_1=0.20273438096046448, tune/recall_het=0.007176175247877836, tune/recall_homalt=0.834269642829895, tune/recall_homref=0.6971428394317627, tune/true_negatives_1=9633.0, tune/true_positives_1=1557.0. I0508 17:54:10.083408 140534986602304 train.py:394] Warmstart checkpoint best checkpoint metric: tune/f1_weighted=0.21346085. real 1m12.933s. user 0m0.037s. sys 0m0.013s. ```. [train.log](https://github.com/google/deepvariant/file",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/819
https://github.com/google/deepvariant/issues/819:1538,performance,tune,tune,1538,"o docker run --runtime=nvidia --gpus 1\. -v ${HOME}:${HOME} \. -w ${HOME} \. google/deepvariant:1.6.1-gpu \. train \. --config=""${BASE}/dv_config.py"":base \. --config.train_dataset_pbtxt=""${BASE}/training_set.pbtxt"" \. --config.tune_dataset_pbtxt=""${BASE}/validation_set.pbtxt"" \. --config.init_checkpoint=""${BASE}/checkpoint/deepvariant.wgs.ckpt"" \. --config.num_epochs=10 \. --config.learning_rate=0.0001 \. --config.num_validation_examples=0 \. --experiment_dir=${TRAINING_DIR} \. --strategy=mirrored \. --config.batch_size=512 \. ) > ""${LOG_DIR}/train.log"" 2>&1 &. ```. ```. I0508 17:53:46.544947 140534986602304 train.py:384] Starting epoch 0. I0508 17:53:46.545100 140534986602304 train.py:391] Performing initial evaluation of warmstart model. I0508 17:53:46.545171 140534986602304 train.py:361] Running tune at step=0 epoch=0. I0508 17:53:46.545287 140534986602304 train.py:366] Tune step 0 / 15 (0.0%). I0508 17:54:10.069682 140512707213056 logging_writer.py:48] [0] tune/categorical_accuracy=0.22617188096046448, tune/categorical_crossentropy=1.3209192752838135, tune/f1_het=0.02283571846783161, tune/f1_homalt=0.09889934211969376, tune/f1_homref=0.843934178352356, tune/f1_macro=0.3218897581100464, tune/f1_micro=0.22617188096046448, tune/f1_weighted=0.21346084773540497, tune/false_negatives_1=6123.0, tune/false_positives_1=5727.0, tune/loss=1.3209190368652344, tune/precision_1=0.21375617384910583, tune/precision_het=0.19323670864105225, tune/precision_homalt=0.05127762258052826, tune/precision_homref=0.9494163393974304, tune/recall_1=0.20273438096046448, tune/recall_het=0.007176175247877836, tune/recall_homalt=0.834269642829895, tune/recall_homref=0.6971428394317627, tune/true_negatives_1=9633.0, tune/true_positives_1=1557.0. I0508 17:54:10.083408 140534986602304 train.py:394] Warmstart checkpoint best checkpoint metric: tune/f1_weighted=0.21346085. real 1m12.933s. user 0m0.037s. sys 0m0.013s. ```. [train.log](https://github.com/google/deepvariant/files/15253217/train.log).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/819
https://github.com/google/deepvariant/issues/819:1588,performance,tune,tune,1588,"o docker run --runtime=nvidia --gpus 1\. -v ${HOME}:${HOME} \. -w ${HOME} \. google/deepvariant:1.6.1-gpu \. train \. --config=""${BASE}/dv_config.py"":base \. --config.train_dataset_pbtxt=""${BASE}/training_set.pbtxt"" \. --config.tune_dataset_pbtxt=""${BASE}/validation_set.pbtxt"" \. --config.init_checkpoint=""${BASE}/checkpoint/deepvariant.wgs.ckpt"" \. --config.num_epochs=10 \. --config.learning_rate=0.0001 \. --config.num_validation_examples=0 \. --experiment_dir=${TRAINING_DIR} \. --strategy=mirrored \. --config.batch_size=512 \. ) > ""${LOG_DIR}/train.log"" 2>&1 &. ```. ```. I0508 17:53:46.544947 140534986602304 train.py:384] Starting epoch 0. I0508 17:53:46.545100 140534986602304 train.py:391] Performing initial evaluation of warmstart model. I0508 17:53:46.545171 140534986602304 train.py:361] Running tune at step=0 epoch=0. I0508 17:53:46.545287 140534986602304 train.py:366] Tune step 0 / 15 (0.0%). I0508 17:54:10.069682 140512707213056 logging_writer.py:48] [0] tune/categorical_accuracy=0.22617188096046448, tune/categorical_crossentropy=1.3209192752838135, tune/f1_het=0.02283571846783161, tune/f1_homalt=0.09889934211969376, tune/f1_homref=0.843934178352356, tune/f1_macro=0.3218897581100464, tune/f1_micro=0.22617188096046448, tune/f1_weighted=0.21346084773540497, tune/false_negatives_1=6123.0, tune/false_positives_1=5727.0, tune/loss=1.3209190368652344, tune/precision_1=0.21375617384910583, tune/precision_het=0.19323670864105225, tune/precision_homalt=0.05127762258052826, tune/precision_homref=0.9494163393974304, tune/recall_1=0.20273438096046448, tune/recall_het=0.007176175247877836, tune/recall_homalt=0.834269642829895, tune/recall_homref=0.6971428394317627, tune/true_negatives_1=9633.0, tune/true_positives_1=1557.0. I0508 17:54:10.083408 140534986602304 train.py:394] Warmstart checkpoint best checkpoint metric: tune/f1_weighted=0.21346085. real 1m12.933s. user 0m0.037s. sys 0m0.013s. ```. [train.log](https://github.com/google/deepvariant/files/15253217/train.log).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/819
https://github.com/google/deepvariant/issues/819:1621,performance,tune,tune,1621,"o docker run --runtime=nvidia --gpus 1\. -v ${HOME}:${HOME} \. -w ${HOME} \. google/deepvariant:1.6.1-gpu \. train \. --config=""${BASE}/dv_config.py"":base \. --config.train_dataset_pbtxt=""${BASE}/training_set.pbtxt"" \. --config.tune_dataset_pbtxt=""${BASE}/validation_set.pbtxt"" \. --config.init_checkpoint=""${BASE}/checkpoint/deepvariant.wgs.ckpt"" \. --config.num_epochs=10 \. --config.learning_rate=0.0001 \. --config.num_validation_examples=0 \. --experiment_dir=${TRAINING_DIR} \. --strategy=mirrored \. --config.batch_size=512 \. ) > ""${LOG_DIR}/train.log"" 2>&1 &. ```. ```. I0508 17:53:46.544947 140534986602304 train.py:384] Starting epoch 0. I0508 17:53:46.545100 140534986602304 train.py:391] Performing initial evaluation of warmstart model. I0508 17:53:46.545171 140534986602304 train.py:361] Running tune at step=0 epoch=0. I0508 17:53:46.545287 140534986602304 train.py:366] Tune step 0 / 15 (0.0%). I0508 17:54:10.069682 140512707213056 logging_writer.py:48] [0] tune/categorical_accuracy=0.22617188096046448, tune/categorical_crossentropy=1.3209192752838135, tune/f1_het=0.02283571846783161, tune/f1_homalt=0.09889934211969376, tune/f1_homref=0.843934178352356, tune/f1_macro=0.3218897581100464, tune/f1_micro=0.22617188096046448, tune/f1_weighted=0.21346084773540497, tune/false_negatives_1=6123.0, tune/false_positives_1=5727.0, tune/loss=1.3209190368652344, tune/precision_1=0.21375617384910583, tune/precision_het=0.19323670864105225, tune/precision_homalt=0.05127762258052826, tune/precision_homref=0.9494163393974304, tune/recall_1=0.20273438096046448, tune/recall_het=0.007176175247877836, tune/recall_homalt=0.834269642829895, tune/recall_homref=0.6971428394317627, tune/true_negatives_1=9633.0, tune/true_positives_1=1557.0. I0508 17:54:10.083408 140534986602304 train.py:394] Warmstart checkpoint best checkpoint metric: tune/f1_weighted=0.21346085. real 1m12.933s. user 0m0.037s. sys 0m0.013s. ```. [train.log](https://github.com/google/deepvariant/files/15253217/train.log).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/819
https://github.com/google/deepvariant/issues/819:1657,performance,tune,tune,1657,"o docker run --runtime=nvidia --gpus 1\. -v ${HOME}:${HOME} \. -w ${HOME} \. google/deepvariant:1.6.1-gpu \. train \. --config=""${BASE}/dv_config.py"":base \. --config.train_dataset_pbtxt=""${BASE}/training_set.pbtxt"" \. --config.tune_dataset_pbtxt=""${BASE}/validation_set.pbtxt"" \. --config.init_checkpoint=""${BASE}/checkpoint/deepvariant.wgs.ckpt"" \. --config.num_epochs=10 \. --config.learning_rate=0.0001 \. --config.num_validation_examples=0 \. --experiment_dir=${TRAINING_DIR} \. --strategy=mirrored \. --config.batch_size=512 \. ) > ""${LOG_DIR}/train.log"" 2>&1 &. ```. ```. I0508 17:53:46.544947 140534986602304 train.py:384] Starting epoch 0. I0508 17:53:46.545100 140534986602304 train.py:391] Performing initial evaluation of warmstart model. I0508 17:53:46.545171 140534986602304 train.py:361] Running tune at step=0 epoch=0. I0508 17:53:46.545287 140534986602304 train.py:366] Tune step 0 / 15 (0.0%). I0508 17:54:10.069682 140512707213056 logging_writer.py:48] [0] tune/categorical_accuracy=0.22617188096046448, tune/categorical_crossentropy=1.3209192752838135, tune/f1_het=0.02283571846783161, tune/f1_homalt=0.09889934211969376, tune/f1_homref=0.843934178352356, tune/f1_macro=0.3218897581100464, tune/f1_micro=0.22617188096046448, tune/f1_weighted=0.21346084773540497, tune/false_negatives_1=6123.0, tune/false_positives_1=5727.0, tune/loss=1.3209190368652344, tune/precision_1=0.21375617384910583, tune/precision_het=0.19323670864105225, tune/precision_homalt=0.05127762258052826, tune/precision_homref=0.9494163393974304, tune/recall_1=0.20273438096046448, tune/recall_het=0.007176175247877836, tune/recall_homalt=0.834269642829895, tune/recall_homref=0.6971428394317627, tune/true_negatives_1=9633.0, tune/true_positives_1=1557.0. I0508 17:54:10.083408 140534986602304 train.py:394] Warmstart checkpoint best checkpoint metric: tune/f1_weighted=0.21346085. real 1m12.933s. user 0m0.037s. sys 0m0.013s. ```. [train.log](https://github.com/google/deepvariant/files/15253217/train.log).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/819
https://github.com/google/deepvariant/issues/819:1691,performance,tune,tune,1691,"o docker run --runtime=nvidia --gpus 1\. -v ${HOME}:${HOME} \. -w ${HOME} \. google/deepvariant:1.6.1-gpu \. train \. --config=""${BASE}/dv_config.py"":base \. --config.train_dataset_pbtxt=""${BASE}/training_set.pbtxt"" \. --config.tune_dataset_pbtxt=""${BASE}/validation_set.pbtxt"" \. --config.init_checkpoint=""${BASE}/checkpoint/deepvariant.wgs.ckpt"" \. --config.num_epochs=10 \. --config.learning_rate=0.0001 \. --config.num_validation_examples=0 \. --experiment_dir=${TRAINING_DIR} \. --strategy=mirrored \. --config.batch_size=512 \. ) > ""${LOG_DIR}/train.log"" 2>&1 &. ```. ```. I0508 17:53:46.544947 140534986602304 train.py:384] Starting epoch 0. I0508 17:53:46.545100 140534986602304 train.py:391] Performing initial evaluation of warmstart model. I0508 17:53:46.545171 140534986602304 train.py:361] Running tune at step=0 epoch=0. I0508 17:53:46.545287 140534986602304 train.py:366] Tune step 0 / 15 (0.0%). I0508 17:54:10.069682 140512707213056 logging_writer.py:48] [0] tune/categorical_accuracy=0.22617188096046448, tune/categorical_crossentropy=1.3209192752838135, tune/f1_het=0.02283571846783161, tune/f1_homalt=0.09889934211969376, tune/f1_homref=0.843934178352356, tune/f1_macro=0.3218897581100464, tune/f1_micro=0.22617188096046448, tune/f1_weighted=0.21346084773540497, tune/false_negatives_1=6123.0, tune/false_positives_1=5727.0, tune/loss=1.3209190368652344, tune/precision_1=0.21375617384910583, tune/precision_het=0.19323670864105225, tune/precision_homalt=0.05127762258052826, tune/precision_homref=0.9494163393974304, tune/recall_1=0.20273438096046448, tune/recall_het=0.007176175247877836, tune/recall_homalt=0.834269642829895, tune/recall_homref=0.6971428394317627, tune/true_negatives_1=9633.0, tune/true_positives_1=1557.0. I0508 17:54:10.083408 140534986602304 train.py:394] Warmstart checkpoint best checkpoint metric: tune/f1_weighted=0.21346085. real 1m12.933s. user 0m0.037s. sys 0m0.013s. ```. [train.log](https://github.com/google/deepvariant/files/15253217/train.log).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/819
https://github.com/google/deepvariant/issues/819:1725,performance,tune,tune,1725,"o docker run --runtime=nvidia --gpus 1\. -v ${HOME}:${HOME} \. -w ${HOME} \. google/deepvariant:1.6.1-gpu \. train \. --config=""${BASE}/dv_config.py"":base \. --config.train_dataset_pbtxt=""${BASE}/training_set.pbtxt"" \. --config.tune_dataset_pbtxt=""${BASE}/validation_set.pbtxt"" \. --config.init_checkpoint=""${BASE}/checkpoint/deepvariant.wgs.ckpt"" \. --config.num_epochs=10 \. --config.learning_rate=0.0001 \. --config.num_validation_examples=0 \. --experiment_dir=${TRAINING_DIR} \. --strategy=mirrored \. --config.batch_size=512 \. ) > ""${LOG_DIR}/train.log"" 2>&1 &. ```. ```. I0508 17:53:46.544947 140534986602304 train.py:384] Starting epoch 0. I0508 17:53:46.545100 140534986602304 train.py:391] Performing initial evaluation of warmstart model. I0508 17:53:46.545171 140534986602304 train.py:361] Running tune at step=0 epoch=0. I0508 17:53:46.545287 140534986602304 train.py:366] Tune step 0 / 15 (0.0%). I0508 17:54:10.069682 140512707213056 logging_writer.py:48] [0] tune/categorical_accuracy=0.22617188096046448, tune/categorical_crossentropy=1.3209192752838135, tune/f1_het=0.02283571846783161, tune/f1_homalt=0.09889934211969376, tune/f1_homref=0.843934178352356, tune/f1_macro=0.3218897581100464, tune/f1_micro=0.22617188096046448, tune/f1_weighted=0.21346084773540497, tune/false_negatives_1=6123.0, tune/false_positives_1=5727.0, tune/loss=1.3209190368652344, tune/precision_1=0.21375617384910583, tune/precision_het=0.19323670864105225, tune/precision_homalt=0.05127762258052826, tune/precision_homref=0.9494163393974304, tune/recall_1=0.20273438096046448, tune/recall_het=0.007176175247877836, tune/recall_homalt=0.834269642829895, tune/recall_homref=0.6971428394317627, tune/true_negatives_1=9633.0, tune/true_positives_1=1557.0. I0508 17:54:10.083408 140534986602304 train.py:394] Warmstart checkpoint best checkpoint metric: tune/f1_weighted=0.21346085. real 1m12.933s. user 0m0.037s. sys 0m0.013s. ```. [train.log](https://github.com/google/deepvariant/files/15253217/train.log).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/819
https://github.com/google/deepvariant/issues/819:1760,performance,tune,tune,1760,"o docker run --runtime=nvidia --gpus 1\. -v ${HOME}:${HOME} \. -w ${HOME} \. google/deepvariant:1.6.1-gpu \. train \. --config=""${BASE}/dv_config.py"":base \. --config.train_dataset_pbtxt=""${BASE}/training_set.pbtxt"" \. --config.tune_dataset_pbtxt=""${BASE}/validation_set.pbtxt"" \. --config.init_checkpoint=""${BASE}/checkpoint/deepvariant.wgs.ckpt"" \. --config.num_epochs=10 \. --config.learning_rate=0.0001 \. --config.num_validation_examples=0 \. --experiment_dir=${TRAINING_DIR} \. --strategy=mirrored \. --config.batch_size=512 \. ) > ""${LOG_DIR}/train.log"" 2>&1 &. ```. ```. I0508 17:53:46.544947 140534986602304 train.py:384] Starting epoch 0. I0508 17:53:46.545100 140534986602304 train.py:391] Performing initial evaluation of warmstart model. I0508 17:53:46.545171 140534986602304 train.py:361] Running tune at step=0 epoch=0. I0508 17:53:46.545287 140534986602304 train.py:366] Tune step 0 / 15 (0.0%). I0508 17:54:10.069682 140512707213056 logging_writer.py:48] [0] tune/categorical_accuracy=0.22617188096046448, tune/categorical_crossentropy=1.3209192752838135, tune/f1_het=0.02283571846783161, tune/f1_homalt=0.09889934211969376, tune/f1_homref=0.843934178352356, tune/f1_macro=0.3218897581100464, tune/f1_micro=0.22617188096046448, tune/f1_weighted=0.21346084773540497, tune/false_negatives_1=6123.0, tune/false_positives_1=5727.0, tune/loss=1.3209190368652344, tune/precision_1=0.21375617384910583, tune/precision_het=0.19323670864105225, tune/precision_homalt=0.05127762258052826, tune/precision_homref=0.9494163393974304, tune/recall_1=0.20273438096046448, tune/recall_het=0.007176175247877836, tune/recall_homalt=0.834269642829895, tune/recall_homref=0.6971428394317627, tune/true_negatives_1=9633.0, tune/true_positives_1=1557.0. I0508 17:54:10.083408 140534986602304 train.py:394] Warmstart checkpoint best checkpoint metric: tune/f1_weighted=0.21346085. real 1m12.933s. user 0m0.037s. sys 0m0.013s. ```. [train.log](https://github.com/google/deepvariant/files/15253217/train.log).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/819
https://github.com/google/deepvariant/issues/819:1798,performance,tune,tune,1798,"o docker run --runtime=nvidia --gpus 1\. -v ${HOME}:${HOME} \. -w ${HOME} \. google/deepvariant:1.6.1-gpu \. train \. --config=""${BASE}/dv_config.py"":base \. --config.train_dataset_pbtxt=""${BASE}/training_set.pbtxt"" \. --config.tune_dataset_pbtxt=""${BASE}/validation_set.pbtxt"" \. --config.init_checkpoint=""${BASE}/checkpoint/deepvariant.wgs.ckpt"" \. --config.num_epochs=10 \. --config.learning_rate=0.0001 \. --config.num_validation_examples=0 \. --experiment_dir=${TRAINING_DIR} \. --strategy=mirrored \. --config.batch_size=512 \. ) > ""${LOG_DIR}/train.log"" 2>&1 &. ```. ```. I0508 17:53:46.544947 140534986602304 train.py:384] Starting epoch 0. I0508 17:53:46.545100 140534986602304 train.py:391] Performing initial evaluation of warmstart model. I0508 17:53:46.545171 140534986602304 train.py:361] Running tune at step=0 epoch=0. I0508 17:53:46.545287 140534986602304 train.py:366] Tune step 0 / 15 (0.0%). I0508 17:54:10.069682 140512707213056 logging_writer.py:48] [0] tune/categorical_accuracy=0.22617188096046448, tune/categorical_crossentropy=1.3209192752838135, tune/f1_het=0.02283571846783161, tune/f1_homalt=0.09889934211969376, tune/f1_homref=0.843934178352356, tune/f1_macro=0.3218897581100464, tune/f1_micro=0.22617188096046448, tune/f1_weighted=0.21346084773540497, tune/false_negatives_1=6123.0, tune/false_positives_1=5727.0, tune/loss=1.3209190368652344, tune/precision_1=0.21375617384910583, tune/precision_het=0.19323670864105225, tune/precision_homalt=0.05127762258052826, tune/precision_homref=0.9494163393974304, tune/recall_1=0.20273438096046448, tune/recall_het=0.007176175247877836, tune/recall_homalt=0.834269642829895, tune/recall_homref=0.6971428394317627, tune/true_negatives_1=9633.0, tune/true_positives_1=1557.0. I0508 17:54:10.083408 140534986602304 train.py:394] Warmstart checkpoint best checkpoint metric: tune/f1_weighted=0.21346085. real 1m12.933s. user 0m0.037s. sys 0m0.013s. ```. [train.log](https://github.com/google/deepvariant/files/15253217/train.log).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/819
https://github.com/google/deepvariant/issues/819:1829,performance,tune,tune,1829,"o docker run --runtime=nvidia --gpus 1\. -v ${HOME}:${HOME} \. -w ${HOME} \. google/deepvariant:1.6.1-gpu \. train \. --config=""${BASE}/dv_config.py"":base \. --config.train_dataset_pbtxt=""${BASE}/training_set.pbtxt"" \. --config.tune_dataset_pbtxt=""${BASE}/validation_set.pbtxt"" \. --config.init_checkpoint=""${BASE}/checkpoint/deepvariant.wgs.ckpt"" \. --config.num_epochs=10 \. --config.learning_rate=0.0001 \. --config.num_validation_examples=0 \. --experiment_dir=${TRAINING_DIR} \. --strategy=mirrored \. --config.batch_size=512 \. ) > ""${LOG_DIR}/train.log"" 2>&1 &. ```. ```. I0508 17:53:46.544947 140534986602304 train.py:384] Starting epoch 0. I0508 17:53:46.545100 140534986602304 train.py:391] Performing initial evaluation of warmstart model. I0508 17:53:46.545171 140534986602304 train.py:361] Running tune at step=0 epoch=0. I0508 17:53:46.545287 140534986602304 train.py:366] Tune step 0 / 15 (0.0%). I0508 17:54:10.069682 140512707213056 logging_writer.py:48] [0] tune/categorical_accuracy=0.22617188096046448, tune/categorical_crossentropy=1.3209192752838135, tune/f1_het=0.02283571846783161, tune/f1_homalt=0.09889934211969376, tune/f1_homref=0.843934178352356, tune/f1_macro=0.3218897581100464, tune/f1_micro=0.22617188096046448, tune/f1_weighted=0.21346084773540497, tune/false_negatives_1=6123.0, tune/false_positives_1=5727.0, tune/loss=1.3209190368652344, tune/precision_1=0.21375617384910583, tune/precision_het=0.19323670864105225, tune/precision_homalt=0.05127762258052826, tune/precision_homref=0.9494163393974304, tune/recall_1=0.20273438096046448, tune/recall_het=0.007176175247877836, tune/recall_homalt=0.834269642829895, tune/recall_homref=0.6971428394317627, tune/true_negatives_1=9633.0, tune/true_positives_1=1557.0. I0508 17:54:10.083408 140534986602304 train.py:394] Warmstart checkpoint best checkpoint metric: tune/f1_weighted=0.21346085. real 1m12.933s. user 0m0.037s. sys 0m0.013s. ```. [train.log](https://github.com/google/deepvariant/files/15253217/train.log).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/819
https://github.com/google/deepvariant/issues/819:1860,performance,tune,tune,1860,"o docker run --runtime=nvidia --gpus 1\. -v ${HOME}:${HOME} \. -w ${HOME} \. google/deepvariant:1.6.1-gpu \. train \. --config=""${BASE}/dv_config.py"":base \. --config.train_dataset_pbtxt=""${BASE}/training_set.pbtxt"" \. --config.tune_dataset_pbtxt=""${BASE}/validation_set.pbtxt"" \. --config.init_checkpoint=""${BASE}/checkpoint/deepvariant.wgs.ckpt"" \. --config.num_epochs=10 \. --config.learning_rate=0.0001 \. --config.num_validation_examples=0 \. --experiment_dir=${TRAINING_DIR} \. --strategy=mirrored \. --config.batch_size=512 \. ) > ""${LOG_DIR}/train.log"" 2>&1 &. ```. ```. I0508 17:53:46.544947 140534986602304 train.py:384] Starting epoch 0. I0508 17:53:46.545100 140534986602304 train.py:391] Performing initial evaluation of warmstart model. I0508 17:53:46.545171 140534986602304 train.py:361] Running tune at step=0 epoch=0. I0508 17:53:46.545287 140534986602304 train.py:366] Tune step 0 / 15 (0.0%). I0508 17:54:10.069682 140512707213056 logging_writer.py:48] [0] tune/categorical_accuracy=0.22617188096046448, tune/categorical_crossentropy=1.3209192752838135, tune/f1_het=0.02283571846783161, tune/f1_homalt=0.09889934211969376, tune/f1_homref=0.843934178352356, tune/f1_macro=0.3218897581100464, tune/f1_micro=0.22617188096046448, tune/f1_weighted=0.21346084773540497, tune/false_negatives_1=6123.0, tune/false_positives_1=5727.0, tune/loss=1.3209190368652344, tune/precision_1=0.21375617384910583, tune/precision_het=0.19323670864105225, tune/precision_homalt=0.05127762258052826, tune/precision_homref=0.9494163393974304, tune/recall_1=0.20273438096046448, tune/recall_het=0.007176175247877836, tune/recall_homalt=0.834269642829895, tune/recall_homref=0.6971428394317627, tune/true_negatives_1=9633.0, tune/true_positives_1=1557.0. I0508 17:54:10.083408 140534986602304 train.py:394] Warmstart checkpoint best checkpoint metric: tune/f1_weighted=0.21346085. real 1m12.933s. user 0m0.037s. sys 0m0.013s. ```. [train.log](https://github.com/google/deepvariant/files/15253217/train.log).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/819
https://github.com/google/deepvariant/issues/819:1890,performance,tune,tune,1890,"o docker run --runtime=nvidia --gpus 1\. -v ${HOME}:${HOME} \. -w ${HOME} \. google/deepvariant:1.6.1-gpu \. train \. --config=""${BASE}/dv_config.py"":base \. --config.train_dataset_pbtxt=""${BASE}/training_set.pbtxt"" \. --config.tune_dataset_pbtxt=""${BASE}/validation_set.pbtxt"" \. --config.init_checkpoint=""${BASE}/checkpoint/deepvariant.wgs.ckpt"" \. --config.num_epochs=10 \. --config.learning_rate=0.0001 \. --config.num_validation_examples=0 \. --experiment_dir=${TRAINING_DIR} \. --strategy=mirrored \. --config.batch_size=512 \. ) > ""${LOG_DIR}/train.log"" 2>&1 &. ```. ```. I0508 17:53:46.544947 140534986602304 train.py:384] Starting epoch 0. I0508 17:53:46.545100 140534986602304 train.py:391] Performing initial evaluation of warmstart model. I0508 17:53:46.545171 140534986602304 train.py:361] Running tune at step=0 epoch=0. I0508 17:53:46.545287 140534986602304 train.py:366] Tune step 0 / 15 (0.0%). I0508 17:54:10.069682 140512707213056 logging_writer.py:48] [0] tune/categorical_accuracy=0.22617188096046448, tune/categorical_crossentropy=1.3209192752838135, tune/f1_het=0.02283571846783161, tune/f1_homalt=0.09889934211969376, tune/f1_homref=0.843934178352356, tune/f1_macro=0.3218897581100464, tune/f1_micro=0.22617188096046448, tune/f1_weighted=0.21346084773540497, tune/false_negatives_1=6123.0, tune/false_positives_1=5727.0, tune/loss=1.3209190368652344, tune/precision_1=0.21375617384910583, tune/precision_het=0.19323670864105225, tune/precision_homalt=0.05127762258052826, tune/precision_homref=0.9494163393974304, tune/recall_1=0.20273438096046448, tune/recall_het=0.007176175247877836, tune/recall_homalt=0.834269642829895, tune/recall_homref=0.6971428394317627, tune/true_negatives_1=9633.0, tune/true_positives_1=1557.0. I0508 17:54:10.083408 140534986602304 train.py:394] Warmstart checkpoint best checkpoint metric: tune/f1_weighted=0.21346085. real 1m12.933s. user 0m0.037s. sys 0m0.013s. ```. [train.log](https://github.com/google/deepvariant/files/15253217/train.log).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/819
https://github.com/google/deepvariant/issues/819:1928,performance,tune,tune,1928,"o docker run --runtime=nvidia --gpus 1\. -v ${HOME}:${HOME} \. -w ${HOME} \. google/deepvariant:1.6.1-gpu \. train \. --config=""${BASE}/dv_config.py"":base \. --config.train_dataset_pbtxt=""${BASE}/training_set.pbtxt"" \. --config.tune_dataset_pbtxt=""${BASE}/validation_set.pbtxt"" \. --config.init_checkpoint=""${BASE}/checkpoint/deepvariant.wgs.ckpt"" \. --config.num_epochs=10 \. --config.learning_rate=0.0001 \. --config.num_validation_examples=0 \. --experiment_dir=${TRAINING_DIR} \. --strategy=mirrored \. --config.batch_size=512 \. ) > ""${LOG_DIR}/train.log"" 2>&1 &. ```. ```. I0508 17:53:46.544947 140534986602304 train.py:384] Starting epoch 0. I0508 17:53:46.545100 140534986602304 train.py:391] Performing initial evaluation of warmstart model. I0508 17:53:46.545171 140534986602304 train.py:361] Running tune at step=0 epoch=0. I0508 17:53:46.545287 140534986602304 train.py:366] Tune step 0 / 15 (0.0%). I0508 17:54:10.069682 140512707213056 logging_writer.py:48] [0] tune/categorical_accuracy=0.22617188096046448, tune/categorical_crossentropy=1.3209192752838135, tune/f1_het=0.02283571846783161, tune/f1_homalt=0.09889934211969376, tune/f1_homref=0.843934178352356, tune/f1_macro=0.3218897581100464, tune/f1_micro=0.22617188096046448, tune/f1_weighted=0.21346084773540497, tune/false_negatives_1=6123.0, tune/false_positives_1=5727.0, tune/loss=1.3209190368652344, tune/precision_1=0.21375617384910583, tune/precision_het=0.19323670864105225, tune/precision_homalt=0.05127762258052826, tune/precision_homref=0.9494163393974304, tune/recall_1=0.20273438096046448, tune/recall_het=0.007176175247877836, tune/recall_homalt=0.834269642829895, tune/recall_homref=0.6971428394317627, tune/true_negatives_1=9633.0, tune/true_positives_1=1557.0. I0508 17:54:10.083408 140534986602304 train.py:394] Warmstart checkpoint best checkpoint metric: tune/f1_weighted=0.21346085. real 1m12.933s. user 0m0.037s. sys 0m0.013s. ```. [train.log](https://github.com/google/deepvariant/files/15253217/train.log).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/819
https://github.com/google/deepvariant/issues/819:1968,performance,tune,tune,1968,"o docker run --runtime=nvidia --gpus 1\. -v ${HOME}:${HOME} \. -w ${HOME} \. google/deepvariant:1.6.1-gpu \. train \. --config=""${BASE}/dv_config.py"":base \. --config.train_dataset_pbtxt=""${BASE}/training_set.pbtxt"" \. --config.tune_dataset_pbtxt=""${BASE}/validation_set.pbtxt"" \. --config.init_checkpoint=""${BASE}/checkpoint/deepvariant.wgs.ckpt"" \. --config.num_epochs=10 \. --config.learning_rate=0.0001 \. --config.num_validation_examples=0 \. --experiment_dir=${TRAINING_DIR} \. --strategy=mirrored \. --config.batch_size=512 \. ) > ""${LOG_DIR}/train.log"" 2>&1 &. ```. ```. I0508 17:53:46.544947 140534986602304 train.py:384] Starting epoch 0. I0508 17:53:46.545100 140534986602304 train.py:391] Performing initial evaluation of warmstart model. I0508 17:53:46.545171 140534986602304 train.py:361] Running tune at step=0 epoch=0. I0508 17:53:46.545287 140534986602304 train.py:366] Tune step 0 / 15 (0.0%). I0508 17:54:10.069682 140512707213056 logging_writer.py:48] [0] tune/categorical_accuracy=0.22617188096046448, tune/categorical_crossentropy=1.3209192752838135, tune/f1_het=0.02283571846783161, tune/f1_homalt=0.09889934211969376, tune/f1_homref=0.843934178352356, tune/f1_macro=0.3218897581100464, tune/f1_micro=0.22617188096046448, tune/f1_weighted=0.21346084773540497, tune/false_negatives_1=6123.0, tune/false_positives_1=5727.0, tune/loss=1.3209190368652344, tune/precision_1=0.21375617384910583, tune/precision_het=0.19323670864105225, tune/precision_homalt=0.05127762258052826, tune/precision_homref=0.9494163393974304, tune/recall_1=0.20273438096046448, tune/recall_het=0.007176175247877836, tune/recall_homalt=0.834269642829895, tune/recall_homref=0.6971428394317627, tune/true_negatives_1=9633.0, tune/true_positives_1=1557.0. I0508 17:54:10.083408 140534986602304 train.py:394] Warmstart checkpoint best checkpoint metric: tune/f1_weighted=0.21346085. real 1m12.933s. user 0m0.037s. sys 0m0.013s. ```. [train.log](https://github.com/google/deepvariant/files/15253217/train.log).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/819
https://github.com/google/deepvariant/issues/819:2011,performance,tune,tune,2011,"o docker run --runtime=nvidia --gpus 1\. -v ${HOME}:${HOME} \. -w ${HOME} \. google/deepvariant:1.6.1-gpu \. train \. --config=""${BASE}/dv_config.py"":base \. --config.train_dataset_pbtxt=""${BASE}/training_set.pbtxt"" \. --config.tune_dataset_pbtxt=""${BASE}/validation_set.pbtxt"" \. --config.init_checkpoint=""${BASE}/checkpoint/deepvariant.wgs.ckpt"" \. --config.num_epochs=10 \. --config.learning_rate=0.0001 \. --config.num_validation_examples=0 \. --experiment_dir=${TRAINING_DIR} \. --strategy=mirrored \. --config.batch_size=512 \. ) > ""${LOG_DIR}/train.log"" 2>&1 &. ```. ```. I0508 17:53:46.544947 140534986602304 train.py:384] Starting epoch 0. I0508 17:53:46.545100 140534986602304 train.py:391] Performing initial evaluation of warmstart model. I0508 17:53:46.545171 140534986602304 train.py:361] Running tune at step=0 epoch=0. I0508 17:53:46.545287 140534986602304 train.py:366] Tune step 0 / 15 (0.0%). I0508 17:54:10.069682 140512707213056 logging_writer.py:48] [0] tune/categorical_accuracy=0.22617188096046448, tune/categorical_crossentropy=1.3209192752838135, tune/f1_het=0.02283571846783161, tune/f1_homalt=0.09889934211969376, tune/f1_homref=0.843934178352356, tune/f1_macro=0.3218897581100464, tune/f1_micro=0.22617188096046448, tune/f1_weighted=0.21346084773540497, tune/false_negatives_1=6123.0, tune/false_positives_1=5727.0, tune/loss=1.3209190368652344, tune/precision_1=0.21375617384910583, tune/precision_het=0.19323670864105225, tune/precision_homalt=0.05127762258052826, tune/precision_homref=0.9494163393974304, tune/recall_1=0.20273438096046448, tune/recall_het=0.007176175247877836, tune/recall_homalt=0.834269642829895, tune/recall_homref=0.6971428394317627, tune/true_negatives_1=9633.0, tune/true_positives_1=1557.0. I0508 17:54:10.083408 140534986602304 train.py:394] Warmstart checkpoint best checkpoint metric: tune/f1_weighted=0.21346085. real 1m12.933s. user 0m0.037s. sys 0m0.013s. ```. [train.log](https://github.com/google/deepvariant/files/15253217/train.log).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/819
https://github.com/google/deepvariant/issues/819:2053,performance,tune,tune,2053,"o docker run --runtime=nvidia --gpus 1\. -v ${HOME}:${HOME} \. -w ${HOME} \. google/deepvariant:1.6.1-gpu \. train \. --config=""${BASE}/dv_config.py"":base \. --config.train_dataset_pbtxt=""${BASE}/training_set.pbtxt"" \. --config.tune_dataset_pbtxt=""${BASE}/validation_set.pbtxt"" \. --config.init_checkpoint=""${BASE}/checkpoint/deepvariant.wgs.ckpt"" \. --config.num_epochs=10 \. --config.learning_rate=0.0001 \. --config.num_validation_examples=0 \. --experiment_dir=${TRAINING_DIR} \. --strategy=mirrored \. --config.batch_size=512 \. ) > ""${LOG_DIR}/train.log"" 2>&1 &. ```. ```. I0508 17:53:46.544947 140534986602304 train.py:384] Starting epoch 0. I0508 17:53:46.545100 140534986602304 train.py:391] Performing initial evaluation of warmstart model. I0508 17:53:46.545171 140534986602304 train.py:361] Running tune at step=0 epoch=0. I0508 17:53:46.545287 140534986602304 train.py:366] Tune step 0 / 15 (0.0%). I0508 17:54:10.069682 140512707213056 logging_writer.py:48] [0] tune/categorical_accuracy=0.22617188096046448, tune/categorical_crossentropy=1.3209192752838135, tune/f1_het=0.02283571846783161, tune/f1_homalt=0.09889934211969376, tune/f1_homref=0.843934178352356, tune/f1_macro=0.3218897581100464, tune/f1_micro=0.22617188096046448, tune/f1_weighted=0.21346084773540497, tune/false_negatives_1=6123.0, tune/false_positives_1=5727.0, tune/loss=1.3209190368652344, tune/precision_1=0.21375617384910583, tune/precision_het=0.19323670864105225, tune/precision_homalt=0.05127762258052826, tune/precision_homref=0.9494163393974304, tune/recall_1=0.20273438096046448, tune/recall_het=0.007176175247877836, tune/recall_homalt=0.834269642829895, tune/recall_homref=0.6971428394317627, tune/true_negatives_1=9633.0, tune/true_positives_1=1557.0. I0508 17:54:10.083408 140534986602304 train.py:394] Warmstart checkpoint best checkpoint metric: tune/f1_weighted=0.21346085. real 1m12.933s. user 0m0.037s. sys 0m0.013s. ```. [train.log](https://github.com/google/deepvariant/files/15253217/train.log).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/819
https://github.com/google/deepvariant/issues/819:2088,performance,tune,tune,2088,"o docker run --runtime=nvidia --gpus 1\. -v ${HOME}:${HOME} \. -w ${HOME} \. google/deepvariant:1.6.1-gpu \. train \. --config=""${BASE}/dv_config.py"":base \. --config.train_dataset_pbtxt=""${BASE}/training_set.pbtxt"" \. --config.tune_dataset_pbtxt=""${BASE}/validation_set.pbtxt"" \. --config.init_checkpoint=""${BASE}/checkpoint/deepvariant.wgs.ckpt"" \. --config.num_epochs=10 \. --config.learning_rate=0.0001 \. --config.num_validation_examples=0 \. --experiment_dir=${TRAINING_DIR} \. --strategy=mirrored \. --config.batch_size=512 \. ) > ""${LOG_DIR}/train.log"" 2>&1 &. ```. ```. I0508 17:53:46.544947 140534986602304 train.py:384] Starting epoch 0. I0508 17:53:46.545100 140534986602304 train.py:391] Performing initial evaluation of warmstart model. I0508 17:53:46.545171 140534986602304 train.py:361] Running tune at step=0 epoch=0. I0508 17:53:46.545287 140534986602304 train.py:366] Tune step 0 / 15 (0.0%). I0508 17:54:10.069682 140512707213056 logging_writer.py:48] [0] tune/categorical_accuracy=0.22617188096046448, tune/categorical_crossentropy=1.3209192752838135, tune/f1_het=0.02283571846783161, tune/f1_homalt=0.09889934211969376, tune/f1_homref=0.843934178352356, tune/f1_macro=0.3218897581100464, tune/f1_micro=0.22617188096046448, tune/f1_weighted=0.21346084773540497, tune/false_negatives_1=6123.0, tune/false_positives_1=5727.0, tune/loss=1.3209190368652344, tune/precision_1=0.21375617384910583, tune/precision_het=0.19323670864105225, tune/precision_homalt=0.05127762258052826, tune/precision_homref=0.9494163393974304, tune/recall_1=0.20273438096046448, tune/recall_het=0.007176175247877836, tune/recall_homalt=0.834269642829895, tune/recall_homref=0.6971428394317627, tune/true_negatives_1=9633.0, tune/true_positives_1=1557.0. I0508 17:54:10.083408 140534986602304 train.py:394] Warmstart checkpoint best checkpoint metric: tune/f1_weighted=0.21346085. real 1m12.933s. user 0m0.037s. sys 0m0.013s. ```. [train.log](https://github.com/google/deepvariant/files/15253217/train.log).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/819
https://github.com/google/deepvariant/issues/819:2126,performance,tune,tune,2126,"o docker run --runtime=nvidia --gpus 1\. -v ${HOME}:${HOME} \. -w ${HOME} \. google/deepvariant:1.6.1-gpu \. train \. --config=""${BASE}/dv_config.py"":base \. --config.train_dataset_pbtxt=""${BASE}/training_set.pbtxt"" \. --config.tune_dataset_pbtxt=""${BASE}/validation_set.pbtxt"" \. --config.init_checkpoint=""${BASE}/checkpoint/deepvariant.wgs.ckpt"" \. --config.num_epochs=10 \. --config.learning_rate=0.0001 \. --config.num_validation_examples=0 \. --experiment_dir=${TRAINING_DIR} \. --strategy=mirrored \. --config.batch_size=512 \. ) > ""${LOG_DIR}/train.log"" 2>&1 &. ```. ```. I0508 17:53:46.544947 140534986602304 train.py:384] Starting epoch 0. I0508 17:53:46.545100 140534986602304 train.py:391] Performing initial evaluation of warmstart model. I0508 17:53:46.545171 140534986602304 train.py:361] Running tune at step=0 epoch=0. I0508 17:53:46.545287 140534986602304 train.py:366] Tune step 0 / 15 (0.0%). I0508 17:54:10.069682 140512707213056 logging_writer.py:48] [0] tune/categorical_accuracy=0.22617188096046448, tune/categorical_crossentropy=1.3209192752838135, tune/f1_het=0.02283571846783161, tune/f1_homalt=0.09889934211969376, tune/f1_homref=0.843934178352356, tune/f1_macro=0.3218897581100464, tune/f1_micro=0.22617188096046448, tune/f1_weighted=0.21346084773540497, tune/false_negatives_1=6123.0, tune/false_positives_1=5727.0, tune/loss=1.3209190368652344, tune/precision_1=0.21375617384910583, tune/precision_het=0.19323670864105225, tune/precision_homalt=0.05127762258052826, tune/precision_homref=0.9494163393974304, tune/recall_1=0.20273438096046448, tune/recall_het=0.007176175247877836, tune/recall_homalt=0.834269642829895, tune/recall_homref=0.6971428394317627, tune/true_negatives_1=9633.0, tune/true_positives_1=1557.0. I0508 17:54:10.083408 140534986602304 train.py:394] Warmstart checkpoint best checkpoint metric: tune/f1_weighted=0.21346085. real 1m12.933s. user 0m0.037s. sys 0m0.013s. ```. [train.log](https://github.com/google/deepvariant/files/15253217/train.log).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/819
https://github.com/google/deepvariant/issues/819:2164,performance,tune,tune,2164,"o docker run --runtime=nvidia --gpus 1\. -v ${HOME}:${HOME} \. -w ${HOME} \. google/deepvariant:1.6.1-gpu \. train \. --config=""${BASE}/dv_config.py"":base \. --config.train_dataset_pbtxt=""${BASE}/training_set.pbtxt"" \. --config.tune_dataset_pbtxt=""${BASE}/validation_set.pbtxt"" \. --config.init_checkpoint=""${BASE}/checkpoint/deepvariant.wgs.ckpt"" \. --config.num_epochs=10 \. --config.learning_rate=0.0001 \. --config.num_validation_examples=0 \. --experiment_dir=${TRAINING_DIR} \. --strategy=mirrored \. --config.batch_size=512 \. ) > ""${LOG_DIR}/train.log"" 2>&1 &. ```. ```. I0508 17:53:46.544947 140534986602304 train.py:384] Starting epoch 0. I0508 17:53:46.545100 140534986602304 train.py:391] Performing initial evaluation of warmstart model. I0508 17:53:46.545171 140534986602304 train.py:361] Running tune at step=0 epoch=0. I0508 17:53:46.545287 140534986602304 train.py:366] Tune step 0 / 15 (0.0%). I0508 17:54:10.069682 140512707213056 logging_writer.py:48] [0] tune/categorical_accuracy=0.22617188096046448, tune/categorical_crossentropy=1.3209192752838135, tune/f1_het=0.02283571846783161, tune/f1_homalt=0.09889934211969376, tune/f1_homref=0.843934178352356, tune/f1_macro=0.3218897581100464, tune/f1_micro=0.22617188096046448, tune/f1_weighted=0.21346084773540497, tune/false_negatives_1=6123.0, tune/false_positives_1=5727.0, tune/loss=1.3209190368652344, tune/precision_1=0.21375617384910583, tune/precision_het=0.19323670864105225, tune/precision_homalt=0.05127762258052826, tune/precision_homref=0.9494163393974304, tune/recall_1=0.20273438096046448, tune/recall_het=0.007176175247877836, tune/recall_homalt=0.834269642829895, tune/recall_homref=0.6971428394317627, tune/true_negatives_1=9633.0, tune/true_positives_1=1557.0. I0508 17:54:10.083408 140534986602304 train.py:394] Warmstart checkpoint best checkpoint metric: tune/f1_weighted=0.21346085. real 1m12.933s. user 0m0.037s. sys 0m0.013s. ```. [train.log](https://github.com/google/deepvariant/files/15253217/train.log).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/819
https://github.com/google/deepvariant/issues/819:2203,performance,tune,tune,2203,"o docker run --runtime=nvidia --gpus 1\. -v ${HOME}:${HOME} \. -w ${HOME} \. google/deepvariant:1.6.1-gpu \. train \. --config=""${BASE}/dv_config.py"":base \. --config.train_dataset_pbtxt=""${BASE}/training_set.pbtxt"" \. --config.tune_dataset_pbtxt=""${BASE}/validation_set.pbtxt"" \. --config.init_checkpoint=""${BASE}/checkpoint/deepvariant.wgs.ckpt"" \. --config.num_epochs=10 \. --config.learning_rate=0.0001 \. --config.num_validation_examples=0 \. --experiment_dir=${TRAINING_DIR} \. --strategy=mirrored \. --config.batch_size=512 \. ) > ""${LOG_DIR}/train.log"" 2>&1 &. ```. ```. I0508 17:53:46.544947 140534986602304 train.py:384] Starting epoch 0. I0508 17:53:46.545100 140534986602304 train.py:391] Performing initial evaluation of warmstart model. I0508 17:53:46.545171 140534986602304 train.py:361] Running tune at step=0 epoch=0. I0508 17:53:46.545287 140534986602304 train.py:366] Tune step 0 / 15 (0.0%). I0508 17:54:10.069682 140512707213056 logging_writer.py:48] [0] tune/categorical_accuracy=0.22617188096046448, tune/categorical_crossentropy=1.3209192752838135, tune/f1_het=0.02283571846783161, tune/f1_homalt=0.09889934211969376, tune/f1_homref=0.843934178352356, tune/f1_macro=0.3218897581100464, tune/f1_micro=0.22617188096046448, tune/f1_weighted=0.21346084773540497, tune/false_negatives_1=6123.0, tune/false_positives_1=5727.0, tune/loss=1.3209190368652344, tune/precision_1=0.21375617384910583, tune/precision_het=0.19323670864105225, tune/precision_homalt=0.05127762258052826, tune/precision_homref=0.9494163393974304, tune/recall_1=0.20273438096046448, tune/recall_het=0.007176175247877836, tune/recall_homalt=0.834269642829895, tune/recall_homref=0.6971428394317627, tune/true_negatives_1=9633.0, tune/true_positives_1=1557.0. I0508 17:54:10.083408 140534986602304 train.py:394] Warmstart checkpoint best checkpoint metric: tune/f1_weighted=0.21346085. real 1m12.933s. user 0m0.037s. sys 0m0.013s. ```. [train.log](https://github.com/google/deepvariant/files/15253217/train.log).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/819
https://github.com/google/deepvariant/issues/819:2233,performance,tune,tune,2233,"o docker run --runtime=nvidia --gpus 1\. -v ${HOME}:${HOME} \. -w ${HOME} \. google/deepvariant:1.6.1-gpu \. train \. --config=""${BASE}/dv_config.py"":base \. --config.train_dataset_pbtxt=""${BASE}/training_set.pbtxt"" \. --config.tune_dataset_pbtxt=""${BASE}/validation_set.pbtxt"" \. --config.init_checkpoint=""${BASE}/checkpoint/deepvariant.wgs.ckpt"" \. --config.num_epochs=10 \. --config.learning_rate=0.0001 \. --config.num_validation_examples=0 \. --experiment_dir=${TRAINING_DIR} \. --strategy=mirrored \. --config.batch_size=512 \. ) > ""${LOG_DIR}/train.log"" 2>&1 &. ```. ```. I0508 17:53:46.544947 140534986602304 train.py:384] Starting epoch 0. I0508 17:53:46.545100 140534986602304 train.py:391] Performing initial evaluation of warmstart model. I0508 17:53:46.545171 140534986602304 train.py:361] Running tune at step=0 epoch=0. I0508 17:53:46.545287 140534986602304 train.py:366] Tune step 0 / 15 (0.0%). I0508 17:54:10.069682 140512707213056 logging_writer.py:48] [0] tune/categorical_accuracy=0.22617188096046448, tune/categorical_crossentropy=1.3209192752838135, tune/f1_het=0.02283571846783161, tune/f1_homalt=0.09889934211969376, tune/f1_homref=0.843934178352356, tune/f1_macro=0.3218897581100464, tune/f1_micro=0.22617188096046448, tune/f1_weighted=0.21346084773540497, tune/false_negatives_1=6123.0, tune/false_positives_1=5727.0, tune/loss=1.3209190368652344, tune/precision_1=0.21375617384910583, tune/precision_het=0.19323670864105225, tune/precision_homalt=0.05127762258052826, tune/precision_homref=0.9494163393974304, tune/recall_1=0.20273438096046448, tune/recall_het=0.007176175247877836, tune/recall_homalt=0.834269642829895, tune/recall_homref=0.6971428394317627, tune/true_negatives_1=9633.0, tune/true_positives_1=1557.0. I0508 17:54:10.083408 140534986602304 train.py:394] Warmstart checkpoint best checkpoint metric: tune/f1_weighted=0.21346085. real 1m12.933s. user 0m0.037s. sys 0m0.013s. ```. [train.log](https://github.com/google/deepvariant/files/15253217/train.log).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/819
https://github.com/google/deepvariant/issues/819:2360,performance,tune,tune,2360,"o docker run --runtime=nvidia --gpus 1\. -v ${HOME}:${HOME} \. -w ${HOME} \. google/deepvariant:1.6.1-gpu \. train \. --config=""${BASE}/dv_config.py"":base \. --config.train_dataset_pbtxt=""${BASE}/training_set.pbtxt"" \. --config.tune_dataset_pbtxt=""${BASE}/validation_set.pbtxt"" \. --config.init_checkpoint=""${BASE}/checkpoint/deepvariant.wgs.ckpt"" \. --config.num_epochs=10 \. --config.learning_rate=0.0001 \. --config.num_validation_examples=0 \. --experiment_dir=${TRAINING_DIR} \. --strategy=mirrored \. --config.batch_size=512 \. ) > ""${LOG_DIR}/train.log"" 2>&1 &. ```. ```. I0508 17:53:46.544947 140534986602304 train.py:384] Starting epoch 0. I0508 17:53:46.545100 140534986602304 train.py:391] Performing initial evaluation of warmstart model. I0508 17:53:46.545171 140534986602304 train.py:361] Running tune at step=0 epoch=0. I0508 17:53:46.545287 140534986602304 train.py:366] Tune step 0 / 15 (0.0%). I0508 17:54:10.069682 140512707213056 logging_writer.py:48] [0] tune/categorical_accuracy=0.22617188096046448, tune/categorical_crossentropy=1.3209192752838135, tune/f1_het=0.02283571846783161, tune/f1_homalt=0.09889934211969376, tune/f1_homref=0.843934178352356, tune/f1_macro=0.3218897581100464, tune/f1_micro=0.22617188096046448, tune/f1_weighted=0.21346084773540497, tune/false_negatives_1=6123.0, tune/false_positives_1=5727.0, tune/loss=1.3209190368652344, tune/precision_1=0.21375617384910583, tune/precision_het=0.19323670864105225, tune/precision_homalt=0.05127762258052826, tune/precision_homref=0.9494163393974304, tune/recall_1=0.20273438096046448, tune/recall_het=0.007176175247877836, tune/recall_homalt=0.834269642829895, tune/recall_homref=0.6971428394317627, tune/true_negatives_1=9633.0, tune/true_positives_1=1557.0. I0508 17:54:10.083408 140534986602304 train.py:394] Warmstart checkpoint best checkpoint metric: tune/f1_weighted=0.21346085. real 1m12.933s. user 0m0.037s. sys 0m0.013s. ```. [train.log](https://github.com/google/deepvariant/files/15253217/train.log).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/819
https://github.com/google/deepvariant/issues/819:193,reliability,checkpoint,checkpoint,193,"Hi @kishwarshafin,. The training process starts as expected with GPU activity visible, but it abruptly stops without any error message while processing the first epoch and determining the best checkpoint metric (code snippet). This step completes as expected when using the CPU image with the same dataset and parameters. Initially, I thought TensorRT issues might be causing this stop, but I'll share the logs with you to get your perspective and an extra set of eyes on the problem. **Command:** . ```. ( time sudo docker run --runtime=nvidia --gpus 1\. -v ${HOME}:${HOME} \. -w ${HOME} \. google/deepvariant:1.6.1-gpu \. train \. --config=""${BASE}/dv_config.py"":base \. --config.train_dataset_pbtxt=""${BASE}/training_set.pbtxt"" \. --config.tune_dataset_pbtxt=""${BASE}/validation_set.pbtxt"" \. --config.init_checkpoint=""${BASE}/checkpoint/deepvariant.wgs.ckpt"" \. --config.num_epochs=10 \. --config.learning_rate=0.0001 \. --config.num_validation_examples=0 \. --experiment_dir=${TRAINING_DIR} \. --strategy=mirrored \. --config.batch_size=512 \. ) > ""${LOG_DIR}/train.log"" 2>&1 &. ```. ```. I0508 17:53:46.544947 140534986602304 train.py:384] Starting epoch 0. I0508 17:53:46.545100 140534986602304 train.py:391] Performing initial evaluation of warmstart model. I0508 17:53:46.545171 140534986602304 train.py:361] Running tune at step=0 epoch=0. I0508 17:53:46.545287 140534986602304 train.py:366] Tune step 0 / 15 (0.0%). I0508 17:54:10.069682 140512707213056 logging_writer.py:48] [0] tune/categorical_accuracy=0.22617188096046448, tune/categorical_crossentropy=1.3209192752838135, tune/f1_het=0.02283571846783161, tune/f1_homalt=0.09889934211969376, tune/f1_homref=0.843934178352356, tune/f1_macro=0.3218897581100464, tune/f1_micro=0.22617188096046448, tune/f1_weighted=0.21346084773540497, tune/false_negatives_1=6123.0, tune/false_positives_1=5727.0, tune/loss=1.3209190368652344, tune/precision_1=0.21375617384910583, tune/precision_het=0.19323670864105225, tune/precision_homalt=0.05127762",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/819
https://github.com/google/deepvariant/issues/819:830,reliability,checkpoint,checkpoint,830,"Hi @kishwarshafin,. The training process starts as expected with GPU activity visible, but it abruptly stops without any error message while processing the first epoch and determining the best checkpoint metric (code snippet). This step completes as expected when using the CPU image with the same dataset and parameters. Initially, I thought TensorRT issues might be causing this stop, but I'll share the logs with you to get your perspective and an extra set of eyes on the problem. **Command:** . ```. ( time sudo docker run --runtime=nvidia --gpus 1\. -v ${HOME}:${HOME} \. -w ${HOME} \. google/deepvariant:1.6.1-gpu \. train \. --config=""${BASE}/dv_config.py"":base \. --config.train_dataset_pbtxt=""${BASE}/training_set.pbtxt"" \. --config.tune_dataset_pbtxt=""${BASE}/validation_set.pbtxt"" \. --config.init_checkpoint=""${BASE}/checkpoint/deepvariant.wgs.ckpt"" \. --config.num_epochs=10 \. --config.learning_rate=0.0001 \. --config.num_validation_examples=0 \. --experiment_dir=${TRAINING_DIR} \. --strategy=mirrored \. --config.batch_size=512 \. ) > ""${LOG_DIR}/train.log"" 2>&1 &. ```. ```. I0508 17:53:46.544947 140534986602304 train.py:384] Starting epoch 0. I0508 17:53:46.545100 140534986602304 train.py:391] Performing initial evaluation of warmstart model. I0508 17:53:46.545171 140534986602304 train.py:361] Running tune at step=0 epoch=0. I0508 17:53:46.545287 140534986602304 train.py:366] Tune step 0 / 15 (0.0%). I0508 17:54:10.069682 140512707213056 logging_writer.py:48] [0] tune/categorical_accuracy=0.22617188096046448, tune/categorical_crossentropy=1.3209192752838135, tune/f1_het=0.02283571846783161, tune/f1_homalt=0.09889934211969376, tune/f1_homref=0.843934178352356, tune/f1_macro=0.3218897581100464, tune/f1_micro=0.22617188096046448, tune/f1_weighted=0.21346084773540497, tune/false_negatives_1=6123.0, tune/false_positives_1=5727.0, tune/loss=1.3209190368652344, tune/precision_1=0.21375617384910583, tune/precision_het=0.19323670864105225, tune/precision_homalt=0.05127762",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/819
https://github.com/google/deepvariant/issues/819:2325,reliability,checkpoint,checkpoint,2325,"o docker run --runtime=nvidia --gpus 1\. -v ${HOME}:${HOME} \. -w ${HOME} \. google/deepvariant:1.6.1-gpu \. train \. --config=""${BASE}/dv_config.py"":base \. --config.train_dataset_pbtxt=""${BASE}/training_set.pbtxt"" \. --config.tune_dataset_pbtxt=""${BASE}/validation_set.pbtxt"" \. --config.init_checkpoint=""${BASE}/checkpoint/deepvariant.wgs.ckpt"" \. --config.num_epochs=10 \. --config.learning_rate=0.0001 \. --config.num_validation_examples=0 \. --experiment_dir=${TRAINING_DIR} \. --strategy=mirrored \. --config.batch_size=512 \. ) > ""${LOG_DIR}/train.log"" 2>&1 &. ```. ```. I0508 17:53:46.544947 140534986602304 train.py:384] Starting epoch 0. I0508 17:53:46.545100 140534986602304 train.py:391] Performing initial evaluation of warmstart model. I0508 17:53:46.545171 140534986602304 train.py:361] Running tune at step=0 epoch=0. I0508 17:53:46.545287 140534986602304 train.py:366] Tune step 0 / 15 (0.0%). I0508 17:54:10.069682 140512707213056 logging_writer.py:48] [0] tune/categorical_accuracy=0.22617188096046448, tune/categorical_crossentropy=1.3209192752838135, tune/f1_het=0.02283571846783161, tune/f1_homalt=0.09889934211969376, tune/f1_homref=0.843934178352356, tune/f1_macro=0.3218897581100464, tune/f1_micro=0.22617188096046448, tune/f1_weighted=0.21346084773540497, tune/false_negatives_1=6123.0, tune/false_positives_1=5727.0, tune/loss=1.3209190368652344, tune/precision_1=0.21375617384910583, tune/precision_het=0.19323670864105225, tune/precision_homalt=0.05127762258052826, tune/precision_homref=0.9494163393974304, tune/recall_1=0.20273438096046448, tune/recall_het=0.007176175247877836, tune/recall_homalt=0.834269642829895, tune/recall_homref=0.6971428394317627, tune/true_negatives_1=9633.0, tune/true_positives_1=1557.0. I0508 17:54:10.083408 140534986602304 train.py:394] Warmstart checkpoint best checkpoint metric: tune/f1_weighted=0.21346085. real 1m12.933s. user 0m0.037s. sys 0m0.013s. ```. [train.log](https://github.com/google/deepvariant/files/15253217/train.log).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/819
https://github.com/google/deepvariant/issues/819:2341,reliability,checkpoint,checkpoint,2341,"o docker run --runtime=nvidia --gpus 1\. -v ${HOME}:${HOME} \. -w ${HOME} \. google/deepvariant:1.6.1-gpu \. train \. --config=""${BASE}/dv_config.py"":base \. --config.train_dataset_pbtxt=""${BASE}/training_set.pbtxt"" \. --config.tune_dataset_pbtxt=""${BASE}/validation_set.pbtxt"" \. --config.init_checkpoint=""${BASE}/checkpoint/deepvariant.wgs.ckpt"" \. --config.num_epochs=10 \. --config.learning_rate=0.0001 \. --config.num_validation_examples=0 \. --experiment_dir=${TRAINING_DIR} \. --strategy=mirrored \. --config.batch_size=512 \. ) > ""${LOG_DIR}/train.log"" 2>&1 &. ```. ```. I0508 17:53:46.544947 140534986602304 train.py:384] Starting epoch 0. I0508 17:53:46.545100 140534986602304 train.py:391] Performing initial evaluation of warmstart model. I0508 17:53:46.545171 140534986602304 train.py:361] Running tune at step=0 epoch=0. I0508 17:53:46.545287 140534986602304 train.py:366] Tune step 0 / 15 (0.0%). I0508 17:54:10.069682 140512707213056 logging_writer.py:48] [0] tune/categorical_accuracy=0.22617188096046448, tune/categorical_crossentropy=1.3209192752838135, tune/f1_het=0.02283571846783161, tune/f1_homalt=0.09889934211969376, tune/f1_homref=0.843934178352356, tune/f1_macro=0.3218897581100464, tune/f1_micro=0.22617188096046448, tune/f1_weighted=0.21346084773540497, tune/false_negatives_1=6123.0, tune/false_positives_1=5727.0, tune/loss=1.3209190368652344, tune/precision_1=0.21375617384910583, tune/precision_het=0.19323670864105225, tune/precision_homalt=0.05127762258052826, tune/precision_homref=0.9494163393974304, tune/recall_1=0.20273438096046448, tune/recall_het=0.007176175247877836, tune/recall_homalt=0.834269642829895, tune/recall_homref=0.6971428394317627, tune/true_negatives_1=9633.0, tune/true_positives_1=1557.0. I0508 17:54:10.083408 140534986602304 train.py:394] Warmstart checkpoint best checkpoint metric: tune/f1_weighted=0.21346085. real 1m12.933s. user 0m0.037s. sys 0m0.013s. ```. [train.log](https://github.com/google/deepvariant/files/15253217/train.log).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/819
https://github.com/google/deepvariant/issues/819:121,safety,error,error,121,"Hi @kishwarshafin,. The training process starts as expected with GPU activity visible, but it abruptly stops without any error message while processing the first epoch and determining the best checkpoint metric (code snippet). This step completes as expected when using the CPU image with the same dataset and parameters. Initially, I thought TensorRT issues might be causing this stop, but I'll share the logs with you to get your perspective and an extra set of eyes on the problem. **Command:** . ```. ( time sudo docker run --runtime=nvidia --gpus 1\. -v ${HOME}:${HOME} \. -w ${HOME} \. google/deepvariant:1.6.1-gpu \. train \. --config=""${BASE}/dv_config.py"":base \. --config.train_dataset_pbtxt=""${BASE}/training_set.pbtxt"" \. --config.tune_dataset_pbtxt=""${BASE}/validation_set.pbtxt"" \. --config.init_checkpoint=""${BASE}/checkpoint/deepvariant.wgs.ckpt"" \. --config.num_epochs=10 \. --config.learning_rate=0.0001 \. --config.num_validation_examples=0 \. --experiment_dir=${TRAINING_DIR} \. --strategy=mirrored \. --config.batch_size=512 \. ) > ""${LOG_DIR}/train.log"" 2>&1 &. ```. ```. I0508 17:53:46.544947 140534986602304 train.py:384] Starting epoch 0. I0508 17:53:46.545100 140534986602304 train.py:391] Performing initial evaluation of warmstart model. I0508 17:53:46.545171 140534986602304 train.py:361] Running tune at step=0 epoch=0. I0508 17:53:46.545287 140534986602304 train.py:366] Tune step 0 / 15 (0.0%). I0508 17:54:10.069682 140512707213056 logging_writer.py:48] [0] tune/categorical_accuracy=0.22617188096046448, tune/categorical_crossentropy=1.3209192752838135, tune/f1_het=0.02283571846783161, tune/f1_homalt=0.09889934211969376, tune/f1_homref=0.843934178352356, tune/f1_macro=0.3218897581100464, tune/f1_micro=0.22617188096046448, tune/f1_weighted=0.21346084773540497, tune/false_negatives_1=6123.0, tune/false_positives_1=5727.0, tune/loss=1.3209190368652344, tune/precision_1=0.21375617384910583, tune/precision_het=0.19323670864105225, tune/precision_homalt=0.05127762",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/819
https://github.com/google/deepvariant/issues/819:237,safety,compl,completes,237,"Hi @kishwarshafin,. The training process starts as expected with GPU activity visible, but it abruptly stops without any error message while processing the first epoch and determining the best checkpoint metric (code snippet). This step completes as expected when using the CPU image with the same dataset and parameters. Initially, I thought TensorRT issues might be causing this stop, but I'll share the logs with you to get your perspective and an extra set of eyes on the problem. **Command:** . ```. ( time sudo docker run --runtime=nvidia --gpus 1\. -v ${HOME}:${HOME} \. -w ${HOME} \. google/deepvariant:1.6.1-gpu \. train \. --config=""${BASE}/dv_config.py"":base \. --config.train_dataset_pbtxt=""${BASE}/training_set.pbtxt"" \. --config.tune_dataset_pbtxt=""${BASE}/validation_set.pbtxt"" \. --config.init_checkpoint=""${BASE}/checkpoint/deepvariant.wgs.ckpt"" \. --config.num_epochs=10 \. --config.learning_rate=0.0001 \. --config.num_validation_examples=0 \. --experiment_dir=${TRAINING_DIR} \. --strategy=mirrored \. --config.batch_size=512 \. ) > ""${LOG_DIR}/train.log"" 2>&1 &. ```. ```. I0508 17:53:46.544947 140534986602304 train.py:384] Starting epoch 0. I0508 17:53:46.545100 140534986602304 train.py:391] Performing initial evaluation of warmstart model. I0508 17:53:46.545171 140534986602304 train.py:361] Running tune at step=0 epoch=0. I0508 17:53:46.545287 140534986602304 train.py:366] Tune step 0 / 15 (0.0%). I0508 17:54:10.069682 140512707213056 logging_writer.py:48] [0] tune/categorical_accuracy=0.22617188096046448, tune/categorical_crossentropy=1.3209192752838135, tune/f1_het=0.02283571846783161, tune/f1_homalt=0.09889934211969376, tune/f1_homref=0.843934178352356, tune/f1_macro=0.3218897581100464, tune/f1_micro=0.22617188096046448, tune/f1_weighted=0.21346084773540497, tune/false_negatives_1=6123.0, tune/false_positives_1=5727.0, tune/loss=1.3209190368652344, tune/precision_1=0.21375617384910583, tune/precision_het=0.19323670864105225, tune/precision_homalt=0.05127762",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/819
https://github.com/google/deepvariant/issues/819:406,safety,log,logs,406,"Hi @kishwarshafin,. The training process starts as expected with GPU activity visible, but it abruptly stops without any error message while processing the first epoch and determining the best checkpoint metric (code snippet). This step completes as expected when using the CPU image with the same dataset and parameters. Initially, I thought TensorRT issues might be causing this stop, but I'll share the logs with you to get your perspective and an extra set of eyes on the problem. **Command:** . ```. ( time sudo docker run --runtime=nvidia --gpus 1\. -v ${HOME}:${HOME} \. -w ${HOME} \. google/deepvariant:1.6.1-gpu \. train \. --config=""${BASE}/dv_config.py"":base \. --config.train_dataset_pbtxt=""${BASE}/training_set.pbtxt"" \. --config.tune_dataset_pbtxt=""${BASE}/validation_set.pbtxt"" \. --config.init_checkpoint=""${BASE}/checkpoint/deepvariant.wgs.ckpt"" \. --config.num_epochs=10 \. --config.learning_rate=0.0001 \. --config.num_validation_examples=0 \. --experiment_dir=${TRAINING_DIR} \. --strategy=mirrored \. --config.batch_size=512 \. ) > ""${LOG_DIR}/train.log"" 2>&1 &. ```. ```. I0508 17:53:46.544947 140534986602304 train.py:384] Starting epoch 0. I0508 17:53:46.545100 140534986602304 train.py:391] Performing initial evaluation of warmstart model. I0508 17:53:46.545171 140534986602304 train.py:361] Running tune at step=0 epoch=0. I0508 17:53:46.545287 140534986602304 train.py:366] Tune step 0 / 15 (0.0%). I0508 17:54:10.069682 140512707213056 logging_writer.py:48] [0] tune/categorical_accuracy=0.22617188096046448, tune/categorical_crossentropy=1.3209192752838135, tune/f1_het=0.02283571846783161, tune/f1_homalt=0.09889934211969376, tune/f1_homref=0.843934178352356, tune/f1_macro=0.3218897581100464, tune/f1_micro=0.22617188096046448, tune/f1_weighted=0.21346084773540497, tune/false_negatives_1=6123.0, tune/false_positives_1=5727.0, tune/loss=1.3209190368652344, tune/precision_1=0.21375617384910583, tune/precision_het=0.19323670864105225, tune/precision_homalt=0.05127762",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/819
https://github.com/google/deepvariant/issues/819:1071,safety,log,log,1071,"vity visible, but it abruptly stops without any error message while processing the first epoch and determining the best checkpoint metric (code snippet). This step completes as expected when using the CPU image with the same dataset and parameters. Initially, I thought TensorRT issues might be causing this stop, but I'll share the logs with you to get your perspective and an extra set of eyes on the problem. **Command:** . ```. ( time sudo docker run --runtime=nvidia --gpus 1\. -v ${HOME}:${HOME} \. -w ${HOME} \. google/deepvariant:1.6.1-gpu \. train \. --config=""${BASE}/dv_config.py"":base \. --config.train_dataset_pbtxt=""${BASE}/training_set.pbtxt"" \. --config.tune_dataset_pbtxt=""${BASE}/validation_set.pbtxt"" \. --config.init_checkpoint=""${BASE}/checkpoint/deepvariant.wgs.ckpt"" \. --config.num_epochs=10 \. --config.learning_rate=0.0001 \. --config.num_validation_examples=0 \. --experiment_dir=${TRAINING_DIR} \. --strategy=mirrored \. --config.batch_size=512 \. ) > ""${LOG_DIR}/train.log"" 2>&1 &. ```. ```. I0508 17:53:46.544947 140534986602304 train.py:384] Starting epoch 0. I0508 17:53:46.545100 140534986602304 train.py:391] Performing initial evaluation of warmstart model. I0508 17:53:46.545171 140534986602304 train.py:361] Running tune at step=0 epoch=0. I0508 17:53:46.545287 140534986602304 train.py:366] Tune step 0 / 15 (0.0%). I0508 17:54:10.069682 140512707213056 logging_writer.py:48] [0] tune/categorical_accuracy=0.22617188096046448, tune/categorical_crossentropy=1.3209192752838135, tune/f1_het=0.02283571846783161, tune/f1_homalt=0.09889934211969376, tune/f1_homref=0.843934178352356, tune/f1_macro=0.3218897581100464, tune/f1_micro=0.22617188096046448, tune/f1_weighted=0.21346084773540497, tune/false_negatives_1=6123.0, tune/false_positives_1=5727.0, tune/loss=1.3209190368652344, tune/precision_1=0.21375617384910583, tune/precision_het=0.19323670864105225, tune/precision_homalt=0.05127762258052826, tune/precision_homref=0.9494163393974304, tune/recall_1=0.2027",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/819
https://github.com/google/deepvariant/issues/819:2446,safety,log,log,2446,"o docker run --runtime=nvidia --gpus 1\. -v ${HOME}:${HOME} \. -w ${HOME} \. google/deepvariant:1.6.1-gpu \. train \. --config=""${BASE}/dv_config.py"":base \. --config.train_dataset_pbtxt=""${BASE}/training_set.pbtxt"" \. --config.tune_dataset_pbtxt=""${BASE}/validation_set.pbtxt"" \. --config.init_checkpoint=""${BASE}/checkpoint/deepvariant.wgs.ckpt"" \. --config.num_epochs=10 \. --config.learning_rate=0.0001 \. --config.num_validation_examples=0 \. --experiment_dir=${TRAINING_DIR} \. --strategy=mirrored \. --config.batch_size=512 \. ) > ""${LOG_DIR}/train.log"" 2>&1 &. ```. ```. I0508 17:53:46.544947 140534986602304 train.py:384] Starting epoch 0. I0508 17:53:46.545100 140534986602304 train.py:391] Performing initial evaluation of warmstart model. I0508 17:53:46.545171 140534986602304 train.py:361] Running tune at step=0 epoch=0. I0508 17:53:46.545287 140534986602304 train.py:366] Tune step 0 / 15 (0.0%). I0508 17:54:10.069682 140512707213056 logging_writer.py:48] [0] tune/categorical_accuracy=0.22617188096046448, tune/categorical_crossentropy=1.3209192752838135, tune/f1_het=0.02283571846783161, tune/f1_homalt=0.09889934211969376, tune/f1_homref=0.843934178352356, tune/f1_macro=0.3218897581100464, tune/f1_micro=0.22617188096046448, tune/f1_weighted=0.21346084773540497, tune/false_negatives_1=6123.0, tune/false_positives_1=5727.0, tune/loss=1.3209190368652344, tune/precision_1=0.21375617384910583, tune/precision_het=0.19323670864105225, tune/precision_homalt=0.05127762258052826, tune/precision_homref=0.9494163393974304, tune/recall_1=0.20273438096046448, tune/recall_het=0.007176175247877836, tune/recall_homalt=0.834269642829895, tune/recall_homref=0.6971428394317627, tune/true_negatives_1=9633.0, tune/true_positives_1=1557.0. I0508 17:54:10.083408 140534986602304 train.py:394] Warmstart checkpoint best checkpoint metric: tune/f1_weighted=0.21346085. real 1m12.933s. user 0m0.037s. sys 0m0.013s. ```. [train.log](https://github.com/google/deepvariant/files/15253217/train.log).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/819
https://github.com/google/deepvariant/issues/819:2510,safety,log,log,2510,"o docker run --runtime=nvidia --gpus 1\. -v ${HOME}:${HOME} \. -w ${HOME} \. google/deepvariant:1.6.1-gpu \. train \. --config=""${BASE}/dv_config.py"":base \. --config.train_dataset_pbtxt=""${BASE}/training_set.pbtxt"" \. --config.tune_dataset_pbtxt=""${BASE}/validation_set.pbtxt"" \. --config.init_checkpoint=""${BASE}/checkpoint/deepvariant.wgs.ckpt"" \. --config.num_epochs=10 \. --config.learning_rate=0.0001 \. --config.num_validation_examples=0 \. --experiment_dir=${TRAINING_DIR} \. --strategy=mirrored \. --config.batch_size=512 \. ) > ""${LOG_DIR}/train.log"" 2>&1 &. ```. ```. I0508 17:53:46.544947 140534986602304 train.py:384] Starting epoch 0. I0508 17:53:46.545100 140534986602304 train.py:391] Performing initial evaluation of warmstart model. I0508 17:53:46.545171 140534986602304 train.py:361] Running tune at step=0 epoch=0. I0508 17:53:46.545287 140534986602304 train.py:366] Tune step 0 / 15 (0.0%). I0508 17:54:10.069682 140512707213056 logging_writer.py:48] [0] tune/categorical_accuracy=0.22617188096046448, tune/categorical_crossentropy=1.3209192752838135, tune/f1_het=0.02283571846783161, tune/f1_homalt=0.09889934211969376, tune/f1_homref=0.843934178352356, tune/f1_macro=0.3218897581100464, tune/f1_micro=0.22617188096046448, tune/f1_weighted=0.21346084773540497, tune/false_negatives_1=6123.0, tune/false_positives_1=5727.0, tune/loss=1.3209190368652344, tune/precision_1=0.21375617384910583, tune/precision_het=0.19323670864105225, tune/precision_homalt=0.05127762258052826, tune/precision_homref=0.9494163393974304, tune/recall_1=0.20273438096046448, tune/recall_het=0.007176175247877836, tune/recall_homalt=0.834269642829895, tune/recall_homref=0.6971428394317627, tune/true_negatives_1=9633.0, tune/true_positives_1=1557.0. I0508 17:54:10.083408 140534986602304 train.py:394] Warmstart checkpoint best checkpoint metric: tune/f1_weighted=0.21346085. real 1m12.933s. user 0m0.037s. sys 0m0.013s. ```. [train.log](https://github.com/google/deepvariant/files/15253217/train.log).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/819
https://github.com/google/deepvariant/issues/819:237,security,compl,completes,237,"Hi @kishwarshafin,. The training process starts as expected with GPU activity visible, but it abruptly stops without any error message while processing the first epoch and determining the best checkpoint metric (code snippet). This step completes as expected when using the CPU image with the same dataset and parameters. Initially, I thought TensorRT issues might be causing this stop, but I'll share the logs with you to get your perspective and an extra set of eyes on the problem. **Command:** . ```. ( time sudo docker run --runtime=nvidia --gpus 1\. -v ${HOME}:${HOME} \. -w ${HOME} \. google/deepvariant:1.6.1-gpu \. train \. --config=""${BASE}/dv_config.py"":base \. --config.train_dataset_pbtxt=""${BASE}/training_set.pbtxt"" \. --config.tune_dataset_pbtxt=""${BASE}/validation_set.pbtxt"" \. --config.init_checkpoint=""${BASE}/checkpoint/deepvariant.wgs.ckpt"" \. --config.num_epochs=10 \. --config.learning_rate=0.0001 \. --config.num_validation_examples=0 \. --experiment_dir=${TRAINING_DIR} \. --strategy=mirrored \. --config.batch_size=512 \. ) > ""${LOG_DIR}/train.log"" 2>&1 &. ```. ```. I0508 17:53:46.544947 140534986602304 train.py:384] Starting epoch 0. I0508 17:53:46.545100 140534986602304 train.py:391] Performing initial evaluation of warmstart model. I0508 17:53:46.545171 140534986602304 train.py:361] Running tune at step=0 epoch=0. I0508 17:53:46.545287 140534986602304 train.py:366] Tune step 0 / 15 (0.0%). I0508 17:54:10.069682 140512707213056 logging_writer.py:48] [0] tune/categorical_accuracy=0.22617188096046448, tune/categorical_crossentropy=1.3209192752838135, tune/f1_het=0.02283571846783161, tune/f1_homalt=0.09889934211969376, tune/f1_homref=0.843934178352356, tune/f1_macro=0.3218897581100464, tune/f1_micro=0.22617188096046448, tune/f1_weighted=0.21346084773540497, tune/false_negatives_1=6123.0, tune/false_positives_1=5727.0, tune/loss=1.3209190368652344, tune/precision_1=0.21375617384910583, tune/precision_het=0.19323670864105225, tune/precision_homalt=0.05127762",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/819
https://github.com/google/deepvariant/issues/819:406,security,log,logs,406,"Hi @kishwarshafin,. The training process starts as expected with GPU activity visible, but it abruptly stops without any error message while processing the first epoch and determining the best checkpoint metric (code snippet). This step completes as expected when using the CPU image with the same dataset and parameters. Initially, I thought TensorRT issues might be causing this stop, but I'll share the logs with you to get your perspective and an extra set of eyes on the problem. **Command:** . ```. ( time sudo docker run --runtime=nvidia --gpus 1\. -v ${HOME}:${HOME} \. -w ${HOME} \. google/deepvariant:1.6.1-gpu \. train \. --config=""${BASE}/dv_config.py"":base \. --config.train_dataset_pbtxt=""${BASE}/training_set.pbtxt"" \. --config.tune_dataset_pbtxt=""${BASE}/validation_set.pbtxt"" \. --config.init_checkpoint=""${BASE}/checkpoint/deepvariant.wgs.ckpt"" \. --config.num_epochs=10 \. --config.learning_rate=0.0001 \. --config.num_validation_examples=0 \. --experiment_dir=${TRAINING_DIR} \. --strategy=mirrored \. --config.batch_size=512 \. ) > ""${LOG_DIR}/train.log"" 2>&1 &. ```. ```. I0508 17:53:46.544947 140534986602304 train.py:384] Starting epoch 0. I0508 17:53:46.545100 140534986602304 train.py:391] Performing initial evaluation of warmstart model. I0508 17:53:46.545171 140534986602304 train.py:361] Running tune at step=0 epoch=0. I0508 17:53:46.545287 140534986602304 train.py:366] Tune step 0 / 15 (0.0%). I0508 17:54:10.069682 140512707213056 logging_writer.py:48] [0] tune/categorical_accuracy=0.22617188096046448, tune/categorical_crossentropy=1.3209192752838135, tune/f1_het=0.02283571846783161, tune/f1_homalt=0.09889934211969376, tune/f1_homref=0.843934178352356, tune/f1_macro=0.3218897581100464, tune/f1_micro=0.22617188096046448, tune/f1_weighted=0.21346084773540497, tune/false_negatives_1=6123.0, tune/false_positives_1=5727.0, tune/loss=1.3209190368652344, tune/precision_1=0.21375617384910583, tune/precision_het=0.19323670864105225, tune/precision_homalt=0.05127762",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/819
https://github.com/google/deepvariant/issues/819:1071,security,log,log,1071,"vity visible, but it abruptly stops without any error message while processing the first epoch and determining the best checkpoint metric (code snippet). This step completes as expected when using the CPU image with the same dataset and parameters. Initially, I thought TensorRT issues might be causing this stop, but I'll share the logs with you to get your perspective and an extra set of eyes on the problem. **Command:** . ```. ( time sudo docker run --runtime=nvidia --gpus 1\. -v ${HOME}:${HOME} \. -w ${HOME} \. google/deepvariant:1.6.1-gpu \. train \. --config=""${BASE}/dv_config.py"":base \. --config.train_dataset_pbtxt=""${BASE}/training_set.pbtxt"" \. --config.tune_dataset_pbtxt=""${BASE}/validation_set.pbtxt"" \. --config.init_checkpoint=""${BASE}/checkpoint/deepvariant.wgs.ckpt"" \. --config.num_epochs=10 \. --config.learning_rate=0.0001 \. --config.num_validation_examples=0 \. --experiment_dir=${TRAINING_DIR} \. --strategy=mirrored \. --config.batch_size=512 \. ) > ""${LOG_DIR}/train.log"" 2>&1 &. ```. ```. I0508 17:53:46.544947 140534986602304 train.py:384] Starting epoch 0. I0508 17:53:46.545100 140534986602304 train.py:391] Performing initial evaluation of warmstart model. I0508 17:53:46.545171 140534986602304 train.py:361] Running tune at step=0 epoch=0. I0508 17:53:46.545287 140534986602304 train.py:366] Tune step 0 / 15 (0.0%). I0508 17:54:10.069682 140512707213056 logging_writer.py:48] [0] tune/categorical_accuracy=0.22617188096046448, tune/categorical_crossentropy=1.3209192752838135, tune/f1_het=0.02283571846783161, tune/f1_homalt=0.09889934211969376, tune/f1_homref=0.843934178352356, tune/f1_macro=0.3218897581100464, tune/f1_micro=0.22617188096046448, tune/f1_weighted=0.21346084773540497, tune/false_negatives_1=6123.0, tune/false_positives_1=5727.0, tune/loss=1.3209190368652344, tune/precision_1=0.21375617384910583, tune/precision_het=0.19323670864105225, tune/precision_homalt=0.05127762258052826, tune/precision_homref=0.9494163393974304, tune/recall_1=0.2027",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/819
https://github.com/google/deepvariant/issues/819:1259,security,model,model,1259,"n using the CPU image with the same dataset and parameters. Initially, I thought TensorRT issues might be causing this stop, but I'll share the logs with you to get your perspective and an extra set of eyes on the problem. **Command:** . ```. ( time sudo docker run --runtime=nvidia --gpus 1\. -v ${HOME}:${HOME} \. -w ${HOME} \. google/deepvariant:1.6.1-gpu \. train \. --config=""${BASE}/dv_config.py"":base \. --config.train_dataset_pbtxt=""${BASE}/training_set.pbtxt"" \. --config.tune_dataset_pbtxt=""${BASE}/validation_set.pbtxt"" \. --config.init_checkpoint=""${BASE}/checkpoint/deepvariant.wgs.ckpt"" \. --config.num_epochs=10 \. --config.learning_rate=0.0001 \. --config.num_validation_examples=0 \. --experiment_dir=${TRAINING_DIR} \. --strategy=mirrored \. --config.batch_size=512 \. ) > ""${LOG_DIR}/train.log"" 2>&1 &. ```. ```. I0508 17:53:46.544947 140534986602304 train.py:384] Starting epoch 0. I0508 17:53:46.545100 140534986602304 train.py:391] Performing initial evaluation of warmstart model. I0508 17:53:46.545171 140534986602304 train.py:361] Running tune at step=0 epoch=0. I0508 17:53:46.545287 140534986602304 train.py:366] Tune step 0 / 15 (0.0%). I0508 17:54:10.069682 140512707213056 logging_writer.py:48] [0] tune/categorical_accuracy=0.22617188096046448, tune/categorical_crossentropy=1.3209192752838135, tune/f1_het=0.02283571846783161, tune/f1_homalt=0.09889934211969376, tune/f1_homref=0.843934178352356, tune/f1_macro=0.3218897581100464, tune/f1_micro=0.22617188096046448, tune/f1_weighted=0.21346084773540497, tune/false_negatives_1=6123.0, tune/false_positives_1=5727.0, tune/loss=1.3209190368652344, tune/precision_1=0.21375617384910583, tune/precision_het=0.19323670864105225, tune/precision_homalt=0.05127762258052826, tune/precision_homref=0.9494163393974304, tune/recall_1=0.20273438096046448, tune/recall_het=0.007176175247877836, tune/recall_homalt=0.834269642829895, tune/recall_homref=0.6971428394317627, tune/true_negatives_1=9633.0, tune/true_positives_1=1557.0.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/819
https://github.com/google/deepvariant/issues/819:1865,security,loss,loss,1865,"o docker run --runtime=nvidia --gpus 1\. -v ${HOME}:${HOME} \. -w ${HOME} \. google/deepvariant:1.6.1-gpu \. train \. --config=""${BASE}/dv_config.py"":base \. --config.train_dataset_pbtxt=""${BASE}/training_set.pbtxt"" \. --config.tune_dataset_pbtxt=""${BASE}/validation_set.pbtxt"" \. --config.init_checkpoint=""${BASE}/checkpoint/deepvariant.wgs.ckpt"" \. --config.num_epochs=10 \. --config.learning_rate=0.0001 \. --config.num_validation_examples=0 \. --experiment_dir=${TRAINING_DIR} \. --strategy=mirrored \. --config.batch_size=512 \. ) > ""${LOG_DIR}/train.log"" 2>&1 &. ```. ```. I0508 17:53:46.544947 140534986602304 train.py:384] Starting epoch 0. I0508 17:53:46.545100 140534986602304 train.py:391] Performing initial evaluation of warmstart model. I0508 17:53:46.545171 140534986602304 train.py:361] Running tune at step=0 epoch=0. I0508 17:53:46.545287 140534986602304 train.py:366] Tune step 0 / 15 (0.0%). I0508 17:54:10.069682 140512707213056 logging_writer.py:48] [0] tune/categorical_accuracy=0.22617188096046448, tune/categorical_crossentropy=1.3209192752838135, tune/f1_het=0.02283571846783161, tune/f1_homalt=0.09889934211969376, tune/f1_homref=0.843934178352356, tune/f1_macro=0.3218897581100464, tune/f1_micro=0.22617188096046448, tune/f1_weighted=0.21346084773540497, tune/false_negatives_1=6123.0, tune/false_positives_1=5727.0, tune/loss=1.3209190368652344, tune/precision_1=0.21375617384910583, tune/precision_het=0.19323670864105225, tune/precision_homalt=0.05127762258052826, tune/precision_homref=0.9494163393974304, tune/recall_1=0.20273438096046448, tune/recall_het=0.007176175247877836, tune/recall_homalt=0.834269642829895, tune/recall_homref=0.6971428394317627, tune/true_negatives_1=9633.0, tune/true_positives_1=1557.0. I0508 17:54:10.083408 140534986602304 train.py:394] Warmstart checkpoint best checkpoint metric: tune/f1_weighted=0.21346085. real 1m12.933s. user 0m0.037s. sys 0m0.013s. ```. [train.log](https://github.com/google/deepvariant/files/15253217/train.log).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/819
https://github.com/google/deepvariant/issues/819:2446,security,log,log,2446,"o docker run --runtime=nvidia --gpus 1\. -v ${HOME}:${HOME} \. -w ${HOME} \. google/deepvariant:1.6.1-gpu \. train \. --config=""${BASE}/dv_config.py"":base \. --config.train_dataset_pbtxt=""${BASE}/training_set.pbtxt"" \. --config.tune_dataset_pbtxt=""${BASE}/validation_set.pbtxt"" \. --config.init_checkpoint=""${BASE}/checkpoint/deepvariant.wgs.ckpt"" \. --config.num_epochs=10 \. --config.learning_rate=0.0001 \. --config.num_validation_examples=0 \. --experiment_dir=${TRAINING_DIR} \. --strategy=mirrored \. --config.batch_size=512 \. ) > ""${LOG_DIR}/train.log"" 2>&1 &. ```. ```. I0508 17:53:46.544947 140534986602304 train.py:384] Starting epoch 0. I0508 17:53:46.545100 140534986602304 train.py:391] Performing initial evaluation of warmstart model. I0508 17:53:46.545171 140534986602304 train.py:361] Running tune at step=0 epoch=0. I0508 17:53:46.545287 140534986602304 train.py:366] Tune step 0 / 15 (0.0%). I0508 17:54:10.069682 140512707213056 logging_writer.py:48] [0] tune/categorical_accuracy=0.22617188096046448, tune/categorical_crossentropy=1.3209192752838135, tune/f1_het=0.02283571846783161, tune/f1_homalt=0.09889934211969376, tune/f1_homref=0.843934178352356, tune/f1_macro=0.3218897581100464, tune/f1_micro=0.22617188096046448, tune/f1_weighted=0.21346084773540497, tune/false_negatives_1=6123.0, tune/false_positives_1=5727.0, tune/loss=1.3209190368652344, tune/precision_1=0.21375617384910583, tune/precision_het=0.19323670864105225, tune/precision_homalt=0.05127762258052826, tune/precision_homref=0.9494163393974304, tune/recall_1=0.20273438096046448, tune/recall_het=0.007176175247877836, tune/recall_homalt=0.834269642829895, tune/recall_homref=0.6971428394317627, tune/true_negatives_1=9633.0, tune/true_positives_1=1557.0. I0508 17:54:10.083408 140534986602304 train.py:394] Warmstart checkpoint best checkpoint metric: tune/f1_weighted=0.21346085. real 1m12.933s. user 0m0.037s. sys 0m0.013s. ```. [train.log](https://github.com/google/deepvariant/files/15253217/train.log).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/819
https://github.com/google/deepvariant/issues/819:2510,security,log,log,2510,"o docker run --runtime=nvidia --gpus 1\. -v ${HOME}:${HOME} \. -w ${HOME} \. google/deepvariant:1.6.1-gpu \. train \. --config=""${BASE}/dv_config.py"":base \. --config.train_dataset_pbtxt=""${BASE}/training_set.pbtxt"" \. --config.tune_dataset_pbtxt=""${BASE}/validation_set.pbtxt"" \. --config.init_checkpoint=""${BASE}/checkpoint/deepvariant.wgs.ckpt"" \. --config.num_epochs=10 \. --config.learning_rate=0.0001 \. --config.num_validation_examples=0 \. --experiment_dir=${TRAINING_DIR} \. --strategy=mirrored \. --config.batch_size=512 \. ) > ""${LOG_DIR}/train.log"" 2>&1 &. ```. ```. I0508 17:53:46.544947 140534986602304 train.py:384] Starting epoch 0. I0508 17:53:46.545100 140534986602304 train.py:391] Performing initial evaluation of warmstart model. I0508 17:53:46.545171 140534986602304 train.py:361] Running tune at step=0 epoch=0. I0508 17:53:46.545287 140534986602304 train.py:366] Tune step 0 / 15 (0.0%). I0508 17:54:10.069682 140512707213056 logging_writer.py:48] [0] tune/categorical_accuracy=0.22617188096046448, tune/categorical_crossentropy=1.3209192752838135, tune/f1_het=0.02283571846783161, tune/f1_homalt=0.09889934211969376, tune/f1_homref=0.843934178352356, tune/f1_macro=0.3218897581100464, tune/f1_micro=0.22617188096046448, tune/f1_weighted=0.21346084773540497, tune/false_negatives_1=6123.0, tune/false_positives_1=5727.0, tune/loss=1.3209190368652344, tune/precision_1=0.21375617384910583, tune/precision_het=0.19323670864105225, tune/precision_homalt=0.05127762258052826, tune/precision_homref=0.9494163393974304, tune/recall_1=0.20273438096046448, tune/recall_het=0.007176175247877836, tune/recall_homalt=0.834269642829895, tune/recall_homref=0.6971428394317627, tune/true_negatives_1=9633.0, tune/true_positives_1=1557.0. I0508 17:54:10.083408 140534986602304 train.py:394] Warmstart checkpoint best checkpoint metric: tune/f1_weighted=0.21346085. real 1m12.933s. user 0m0.037s. sys 0m0.013s. ```. [train.log](https://github.com/google/deepvariant/files/15253217/train.log).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/819
https://github.com/google/deepvariant/issues/819:406,testability,log,logs,406,"Hi @kishwarshafin,. The training process starts as expected with GPU activity visible, but it abruptly stops without any error message while processing the first epoch and determining the best checkpoint metric (code snippet). This step completes as expected when using the CPU image with the same dataset and parameters. Initially, I thought TensorRT issues might be causing this stop, but I'll share the logs with you to get your perspective and an extra set of eyes on the problem. **Command:** . ```. ( time sudo docker run --runtime=nvidia --gpus 1\. -v ${HOME}:${HOME} \. -w ${HOME} \. google/deepvariant:1.6.1-gpu \. train \. --config=""${BASE}/dv_config.py"":base \. --config.train_dataset_pbtxt=""${BASE}/training_set.pbtxt"" \. --config.tune_dataset_pbtxt=""${BASE}/validation_set.pbtxt"" \. --config.init_checkpoint=""${BASE}/checkpoint/deepvariant.wgs.ckpt"" \. --config.num_epochs=10 \. --config.learning_rate=0.0001 \. --config.num_validation_examples=0 \. --experiment_dir=${TRAINING_DIR} \. --strategy=mirrored \. --config.batch_size=512 \. ) > ""${LOG_DIR}/train.log"" 2>&1 &. ```. ```. I0508 17:53:46.544947 140534986602304 train.py:384] Starting epoch 0. I0508 17:53:46.545100 140534986602304 train.py:391] Performing initial evaluation of warmstart model. I0508 17:53:46.545171 140534986602304 train.py:361] Running tune at step=0 epoch=0. I0508 17:53:46.545287 140534986602304 train.py:366] Tune step 0 / 15 (0.0%). I0508 17:54:10.069682 140512707213056 logging_writer.py:48] [0] tune/categorical_accuracy=0.22617188096046448, tune/categorical_crossentropy=1.3209192752838135, tune/f1_het=0.02283571846783161, tune/f1_homalt=0.09889934211969376, tune/f1_homref=0.843934178352356, tune/f1_macro=0.3218897581100464, tune/f1_micro=0.22617188096046448, tune/f1_weighted=0.21346084773540497, tune/false_negatives_1=6123.0, tune/false_positives_1=5727.0, tune/loss=1.3209190368652344, tune/precision_1=0.21375617384910583, tune/precision_het=0.19323670864105225, tune/precision_homalt=0.05127762",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/819
https://github.com/google/deepvariant/issues/819:1071,testability,log,log,1071,"vity visible, but it abruptly stops without any error message while processing the first epoch and determining the best checkpoint metric (code snippet). This step completes as expected when using the CPU image with the same dataset and parameters. Initially, I thought TensorRT issues might be causing this stop, but I'll share the logs with you to get your perspective and an extra set of eyes on the problem. **Command:** . ```. ( time sudo docker run --runtime=nvidia --gpus 1\. -v ${HOME}:${HOME} \. -w ${HOME} \. google/deepvariant:1.6.1-gpu \. train \. --config=""${BASE}/dv_config.py"":base \. --config.train_dataset_pbtxt=""${BASE}/training_set.pbtxt"" \. --config.tune_dataset_pbtxt=""${BASE}/validation_set.pbtxt"" \. --config.init_checkpoint=""${BASE}/checkpoint/deepvariant.wgs.ckpt"" \. --config.num_epochs=10 \. --config.learning_rate=0.0001 \. --config.num_validation_examples=0 \. --experiment_dir=${TRAINING_DIR} \. --strategy=mirrored \. --config.batch_size=512 \. ) > ""${LOG_DIR}/train.log"" 2>&1 &. ```. ```. I0508 17:53:46.544947 140534986602304 train.py:384] Starting epoch 0. I0508 17:53:46.545100 140534986602304 train.py:391] Performing initial evaluation of warmstart model. I0508 17:53:46.545171 140534986602304 train.py:361] Running tune at step=0 epoch=0. I0508 17:53:46.545287 140534986602304 train.py:366] Tune step 0 / 15 (0.0%). I0508 17:54:10.069682 140512707213056 logging_writer.py:48] [0] tune/categorical_accuracy=0.22617188096046448, tune/categorical_crossentropy=1.3209192752838135, tune/f1_het=0.02283571846783161, tune/f1_homalt=0.09889934211969376, tune/f1_homref=0.843934178352356, tune/f1_macro=0.3218897581100464, tune/f1_micro=0.22617188096046448, tune/f1_weighted=0.21346084773540497, tune/false_negatives_1=6123.0, tune/false_positives_1=5727.0, tune/loss=1.3209190368652344, tune/precision_1=0.21375617384910583, tune/precision_het=0.19323670864105225, tune/precision_homalt=0.05127762258052826, tune/precision_homref=0.9494163393974304, tune/recall_1=0.2027",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/819
https://github.com/google/deepvariant/issues/819:2446,testability,log,log,2446,"o docker run --runtime=nvidia --gpus 1\. -v ${HOME}:${HOME} \. -w ${HOME} \. google/deepvariant:1.6.1-gpu \. train \. --config=""${BASE}/dv_config.py"":base \. --config.train_dataset_pbtxt=""${BASE}/training_set.pbtxt"" \. --config.tune_dataset_pbtxt=""${BASE}/validation_set.pbtxt"" \. --config.init_checkpoint=""${BASE}/checkpoint/deepvariant.wgs.ckpt"" \. --config.num_epochs=10 \. --config.learning_rate=0.0001 \. --config.num_validation_examples=0 \. --experiment_dir=${TRAINING_DIR} \. --strategy=mirrored \. --config.batch_size=512 \. ) > ""${LOG_DIR}/train.log"" 2>&1 &. ```. ```. I0508 17:53:46.544947 140534986602304 train.py:384] Starting epoch 0. I0508 17:53:46.545100 140534986602304 train.py:391] Performing initial evaluation of warmstart model. I0508 17:53:46.545171 140534986602304 train.py:361] Running tune at step=0 epoch=0. I0508 17:53:46.545287 140534986602304 train.py:366] Tune step 0 / 15 (0.0%). I0508 17:54:10.069682 140512707213056 logging_writer.py:48] [0] tune/categorical_accuracy=0.22617188096046448, tune/categorical_crossentropy=1.3209192752838135, tune/f1_het=0.02283571846783161, tune/f1_homalt=0.09889934211969376, tune/f1_homref=0.843934178352356, tune/f1_macro=0.3218897581100464, tune/f1_micro=0.22617188096046448, tune/f1_weighted=0.21346084773540497, tune/false_negatives_1=6123.0, tune/false_positives_1=5727.0, tune/loss=1.3209190368652344, tune/precision_1=0.21375617384910583, tune/precision_het=0.19323670864105225, tune/precision_homalt=0.05127762258052826, tune/precision_homref=0.9494163393974304, tune/recall_1=0.20273438096046448, tune/recall_het=0.007176175247877836, tune/recall_homalt=0.834269642829895, tune/recall_homref=0.6971428394317627, tune/true_negatives_1=9633.0, tune/true_positives_1=1557.0. I0508 17:54:10.083408 140534986602304 train.py:394] Warmstart checkpoint best checkpoint metric: tune/f1_weighted=0.21346085. real 1m12.933s. user 0m0.037s. sys 0m0.013s. ```. [train.log](https://github.com/google/deepvariant/files/15253217/train.log).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/819
https://github.com/google/deepvariant/issues/819:2510,testability,log,log,2510,"o docker run --runtime=nvidia --gpus 1\. -v ${HOME}:${HOME} \. -w ${HOME} \. google/deepvariant:1.6.1-gpu \. train \. --config=""${BASE}/dv_config.py"":base \. --config.train_dataset_pbtxt=""${BASE}/training_set.pbtxt"" \. --config.tune_dataset_pbtxt=""${BASE}/validation_set.pbtxt"" \. --config.init_checkpoint=""${BASE}/checkpoint/deepvariant.wgs.ckpt"" \. --config.num_epochs=10 \. --config.learning_rate=0.0001 \. --config.num_validation_examples=0 \. --experiment_dir=${TRAINING_DIR} \. --strategy=mirrored \. --config.batch_size=512 \. ) > ""${LOG_DIR}/train.log"" 2>&1 &. ```. ```. I0508 17:53:46.544947 140534986602304 train.py:384] Starting epoch 0. I0508 17:53:46.545100 140534986602304 train.py:391] Performing initial evaluation of warmstart model. I0508 17:53:46.545171 140534986602304 train.py:361] Running tune at step=0 epoch=0. I0508 17:53:46.545287 140534986602304 train.py:366] Tune step 0 / 15 (0.0%). I0508 17:54:10.069682 140512707213056 logging_writer.py:48] [0] tune/categorical_accuracy=0.22617188096046448, tune/categorical_crossentropy=1.3209192752838135, tune/f1_het=0.02283571846783161, tune/f1_homalt=0.09889934211969376, tune/f1_homref=0.843934178352356, tune/f1_macro=0.3218897581100464, tune/f1_micro=0.22617188096046448, tune/f1_weighted=0.21346084773540497, tune/false_negatives_1=6123.0, tune/false_positives_1=5727.0, tune/loss=1.3209190368652344, tune/precision_1=0.21375617384910583, tune/precision_het=0.19323670864105225, tune/precision_homalt=0.05127762258052826, tune/precision_homref=0.9494163393974304, tune/recall_1=0.20273438096046448, tune/recall_het=0.007176175247877836, tune/recall_homalt=0.834269642829895, tune/recall_homref=0.6971428394317627, tune/true_negatives_1=9633.0, tune/true_positives_1=1557.0. I0508 17:54:10.083408 140534986602304 train.py:394] Warmstart checkpoint best checkpoint metric: tune/f1_weighted=0.21346085. real 1m12.933s. user 0m0.037s. sys 0m0.013s. ```. [train.log](https://github.com/google/deepvariant/files/15253217/train.log).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/819
https://github.com/google/deepvariant/issues/819:103,usability,stop,stops,103,"Hi @kishwarshafin,. The training process starts as expected with GPU activity visible, but it abruptly stops without any error message while processing the first epoch and determining the best checkpoint metric (code snippet). This step completes as expected when using the CPU image with the same dataset and parameters. Initially, I thought TensorRT issues might be causing this stop, but I'll share the logs with you to get your perspective and an extra set of eyes on the problem. **Command:** . ```. ( time sudo docker run --runtime=nvidia --gpus 1\. -v ${HOME}:${HOME} \. -w ${HOME} \. google/deepvariant:1.6.1-gpu \. train \. --config=""${BASE}/dv_config.py"":base \. --config.train_dataset_pbtxt=""${BASE}/training_set.pbtxt"" \. --config.tune_dataset_pbtxt=""${BASE}/validation_set.pbtxt"" \. --config.init_checkpoint=""${BASE}/checkpoint/deepvariant.wgs.ckpt"" \. --config.num_epochs=10 \. --config.learning_rate=0.0001 \. --config.num_validation_examples=0 \. --experiment_dir=${TRAINING_DIR} \. --strategy=mirrored \. --config.batch_size=512 \. ) > ""${LOG_DIR}/train.log"" 2>&1 &. ```. ```. I0508 17:53:46.544947 140534986602304 train.py:384] Starting epoch 0. I0508 17:53:46.545100 140534986602304 train.py:391] Performing initial evaluation of warmstart model. I0508 17:53:46.545171 140534986602304 train.py:361] Running tune at step=0 epoch=0. I0508 17:53:46.545287 140534986602304 train.py:366] Tune step 0 / 15 (0.0%). I0508 17:54:10.069682 140512707213056 logging_writer.py:48] [0] tune/categorical_accuracy=0.22617188096046448, tune/categorical_crossentropy=1.3209192752838135, tune/f1_het=0.02283571846783161, tune/f1_homalt=0.09889934211969376, tune/f1_homref=0.843934178352356, tune/f1_macro=0.3218897581100464, tune/f1_micro=0.22617188096046448, tune/f1_weighted=0.21346084773540497, tune/false_negatives_1=6123.0, tune/false_positives_1=5727.0, tune/loss=1.3209190368652344, tune/precision_1=0.21375617384910583, tune/precision_het=0.19323670864105225, tune/precision_homalt=0.05127762",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/819
https://github.com/google/deepvariant/issues/819:121,usability,error,error,121,"Hi @kishwarshafin,. The training process starts as expected with GPU activity visible, but it abruptly stops without any error message while processing the first epoch and determining the best checkpoint metric (code snippet). This step completes as expected when using the CPU image with the same dataset and parameters. Initially, I thought TensorRT issues might be causing this stop, but I'll share the logs with you to get your perspective and an extra set of eyes on the problem. **Command:** . ```. ( time sudo docker run --runtime=nvidia --gpus 1\. -v ${HOME}:${HOME} \. -w ${HOME} \. google/deepvariant:1.6.1-gpu \. train \. --config=""${BASE}/dv_config.py"":base \. --config.train_dataset_pbtxt=""${BASE}/training_set.pbtxt"" \. --config.tune_dataset_pbtxt=""${BASE}/validation_set.pbtxt"" \. --config.init_checkpoint=""${BASE}/checkpoint/deepvariant.wgs.ckpt"" \. --config.num_epochs=10 \. --config.learning_rate=0.0001 \. --config.num_validation_examples=0 \. --experiment_dir=${TRAINING_DIR} \. --strategy=mirrored \. --config.batch_size=512 \. ) > ""${LOG_DIR}/train.log"" 2>&1 &. ```. ```. I0508 17:53:46.544947 140534986602304 train.py:384] Starting epoch 0. I0508 17:53:46.545100 140534986602304 train.py:391] Performing initial evaluation of warmstart model. I0508 17:53:46.545171 140534986602304 train.py:361] Running tune at step=0 epoch=0. I0508 17:53:46.545287 140534986602304 train.py:366] Tune step 0 / 15 (0.0%). I0508 17:54:10.069682 140512707213056 logging_writer.py:48] [0] tune/categorical_accuracy=0.22617188096046448, tune/categorical_crossentropy=1.3209192752838135, tune/f1_het=0.02283571846783161, tune/f1_homalt=0.09889934211969376, tune/f1_homref=0.843934178352356, tune/f1_macro=0.3218897581100464, tune/f1_micro=0.22617188096046448, tune/f1_weighted=0.21346084773540497, tune/false_negatives_1=6123.0, tune/false_positives_1=5727.0, tune/loss=1.3209190368652344, tune/precision_1=0.21375617384910583, tune/precision_het=0.19323670864105225, tune/precision_homalt=0.05127762",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/819
https://github.com/google/deepvariant/issues/819:381,usability,stop,stop,381,"Hi @kishwarshafin,. The training process starts as expected with GPU activity visible, but it abruptly stops without any error message while processing the first epoch and determining the best checkpoint metric (code snippet). This step completes as expected when using the CPU image with the same dataset and parameters. Initially, I thought TensorRT issues might be causing this stop, but I'll share the logs with you to get your perspective and an extra set of eyes on the problem. **Command:** . ```. ( time sudo docker run --runtime=nvidia --gpus 1\. -v ${HOME}:${HOME} \. -w ${HOME} \. google/deepvariant:1.6.1-gpu \. train \. --config=""${BASE}/dv_config.py"":base \. --config.train_dataset_pbtxt=""${BASE}/training_set.pbtxt"" \. --config.tune_dataset_pbtxt=""${BASE}/validation_set.pbtxt"" \. --config.init_checkpoint=""${BASE}/checkpoint/deepvariant.wgs.ckpt"" \. --config.num_epochs=10 \. --config.learning_rate=0.0001 \. --config.num_validation_examples=0 \. --experiment_dir=${TRAINING_DIR} \. --strategy=mirrored \. --config.batch_size=512 \. ) > ""${LOG_DIR}/train.log"" 2>&1 &. ```. ```. I0508 17:53:46.544947 140534986602304 train.py:384] Starting epoch 0. I0508 17:53:46.545100 140534986602304 train.py:391] Performing initial evaluation of warmstart model. I0508 17:53:46.545171 140534986602304 train.py:361] Running tune at step=0 epoch=0. I0508 17:53:46.545287 140534986602304 train.py:366] Tune step 0 / 15 (0.0%). I0508 17:54:10.069682 140512707213056 logging_writer.py:48] [0] tune/categorical_accuracy=0.22617188096046448, tune/categorical_crossentropy=1.3209192752838135, tune/f1_het=0.02283571846783161, tune/f1_homalt=0.09889934211969376, tune/f1_homref=0.843934178352356, tune/f1_macro=0.3218897581100464, tune/f1_micro=0.22617188096046448, tune/f1_weighted=0.21346084773540497, tune/false_negatives_1=6123.0, tune/false_positives_1=5727.0, tune/loss=1.3209190368652344, tune/precision_1=0.21375617384910583, tune/precision_het=0.19323670864105225, tune/precision_homalt=0.05127762",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/819
https://github.com/google/deepvariant/issues/819:487,usability,Command,Command,487,"Hi @kishwarshafin,. The training process starts as expected with GPU activity visible, but it abruptly stops without any error message while processing the first epoch and determining the best checkpoint metric (code snippet). This step completes as expected when using the CPU image with the same dataset and parameters. Initially, I thought TensorRT issues might be causing this stop, but I'll share the logs with you to get your perspective and an extra set of eyes on the problem. **Command:** . ```. ( time sudo docker run --runtime=nvidia --gpus 1\. -v ${HOME}:${HOME} \. -w ${HOME} \. google/deepvariant:1.6.1-gpu \. train \. --config=""${BASE}/dv_config.py"":base \. --config.train_dataset_pbtxt=""${BASE}/training_set.pbtxt"" \. --config.tune_dataset_pbtxt=""${BASE}/validation_set.pbtxt"" \. --config.init_checkpoint=""${BASE}/checkpoint/deepvariant.wgs.ckpt"" \. --config.num_epochs=10 \. --config.learning_rate=0.0001 \. --config.num_validation_examples=0 \. --experiment_dir=${TRAINING_DIR} \. --strategy=mirrored \. --config.batch_size=512 \. ) > ""${LOG_DIR}/train.log"" 2>&1 &. ```. ```. I0508 17:53:46.544947 140534986602304 train.py:384] Starting epoch 0. I0508 17:53:46.545100 140534986602304 train.py:391] Performing initial evaluation of warmstart model. I0508 17:53:46.545171 140534986602304 train.py:361] Running tune at step=0 epoch=0. I0508 17:53:46.545287 140534986602304 train.py:366] Tune step 0 / 15 (0.0%). I0508 17:54:10.069682 140512707213056 logging_writer.py:48] [0] tune/categorical_accuracy=0.22617188096046448, tune/categorical_crossentropy=1.3209192752838135, tune/f1_het=0.02283571846783161, tune/f1_homalt=0.09889934211969376, tune/f1_homref=0.843934178352356, tune/f1_macro=0.3218897581100464, tune/f1_micro=0.22617188096046448, tune/f1_weighted=0.21346084773540497, tune/false_negatives_1=6123.0, tune/false_positives_1=5727.0, tune/loss=1.3209190368652344, tune/precision_1=0.21375617384910583, tune/precision_het=0.19323670864105225, tune/precision_homalt=0.05127762",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/819
https://github.com/google/deepvariant/issues/819:1216,usability,Perform,Performing,1216,"pet). This step completes as expected when using the CPU image with the same dataset and parameters. Initially, I thought TensorRT issues might be causing this stop, but I'll share the logs with you to get your perspective and an extra set of eyes on the problem. **Command:** . ```. ( time sudo docker run --runtime=nvidia --gpus 1\. -v ${HOME}:${HOME} \. -w ${HOME} \. google/deepvariant:1.6.1-gpu \. train \. --config=""${BASE}/dv_config.py"":base \. --config.train_dataset_pbtxt=""${BASE}/training_set.pbtxt"" \. --config.tune_dataset_pbtxt=""${BASE}/validation_set.pbtxt"" \. --config.init_checkpoint=""${BASE}/checkpoint/deepvariant.wgs.ckpt"" \. --config.num_epochs=10 \. --config.learning_rate=0.0001 \. --config.num_validation_examples=0 \. --experiment_dir=${TRAINING_DIR} \. --strategy=mirrored \. --config.batch_size=512 \. ) > ""${LOG_DIR}/train.log"" 2>&1 &. ```. ```. I0508 17:53:46.544947 140534986602304 train.py:384] Starting epoch 0. I0508 17:53:46.545100 140534986602304 train.py:391] Performing initial evaluation of warmstart model. I0508 17:53:46.545171 140534986602304 train.py:361] Running tune at step=0 epoch=0. I0508 17:53:46.545287 140534986602304 train.py:366] Tune step 0 / 15 (0.0%). I0508 17:54:10.069682 140512707213056 logging_writer.py:48] [0] tune/categorical_accuracy=0.22617188096046448, tune/categorical_crossentropy=1.3209192752838135, tune/f1_het=0.02283571846783161, tune/f1_homalt=0.09889934211969376, tune/f1_homref=0.843934178352356, tune/f1_macro=0.3218897581100464, tune/f1_micro=0.22617188096046448, tune/f1_weighted=0.21346084773540497, tune/false_negatives_1=6123.0, tune/false_positives_1=5727.0, tune/loss=1.3209190368652344, tune/precision_1=0.21375617384910583, tune/precision_het=0.19323670864105225, tune/precision_homalt=0.05127762258052826, tune/precision_homref=0.9494163393974304, tune/recall_1=0.20273438096046448, tune/recall_het=0.007176175247877836, tune/recall_homalt=0.834269642829895, tune/recall_homref=0.6971428394317627, tune/true_negative",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/819
https://github.com/google/deepvariant/issues/819:2405,usability,user,user,2405,"o docker run --runtime=nvidia --gpus 1\. -v ${HOME}:${HOME} \. -w ${HOME} \. google/deepvariant:1.6.1-gpu \. train \. --config=""${BASE}/dv_config.py"":base \. --config.train_dataset_pbtxt=""${BASE}/training_set.pbtxt"" \. --config.tune_dataset_pbtxt=""${BASE}/validation_set.pbtxt"" \. --config.init_checkpoint=""${BASE}/checkpoint/deepvariant.wgs.ckpt"" \. --config.num_epochs=10 \. --config.learning_rate=0.0001 \. --config.num_validation_examples=0 \. --experiment_dir=${TRAINING_DIR} \. --strategy=mirrored \. --config.batch_size=512 \. ) > ""${LOG_DIR}/train.log"" 2>&1 &. ```. ```. I0508 17:53:46.544947 140534986602304 train.py:384] Starting epoch 0. I0508 17:53:46.545100 140534986602304 train.py:391] Performing initial evaluation of warmstart model. I0508 17:53:46.545171 140534986602304 train.py:361] Running tune at step=0 epoch=0. I0508 17:53:46.545287 140534986602304 train.py:366] Tune step 0 / 15 (0.0%). I0508 17:54:10.069682 140512707213056 logging_writer.py:48] [0] tune/categorical_accuracy=0.22617188096046448, tune/categorical_crossentropy=1.3209192752838135, tune/f1_het=0.02283571846783161, tune/f1_homalt=0.09889934211969376, tune/f1_homref=0.843934178352356, tune/f1_macro=0.3218897581100464, tune/f1_micro=0.22617188096046448, tune/f1_weighted=0.21346084773540497, tune/false_negatives_1=6123.0, tune/false_positives_1=5727.0, tune/loss=1.3209190368652344, tune/precision_1=0.21375617384910583, tune/precision_het=0.19323670864105225, tune/precision_homalt=0.05127762258052826, tune/precision_homref=0.9494163393974304, tune/recall_1=0.20273438096046448, tune/recall_het=0.007176175247877836, tune/recall_homalt=0.834269642829895, tune/recall_homref=0.6971428394317627, tune/true_negatives_1=9633.0, tune/true_positives_1=1557.0. I0508 17:54:10.083408 140534986602304 train.py:394] Warmstart checkpoint best checkpoint metric: tune/f1_weighted=0.21346085. real 1m12.933s. user 0m0.037s. sys 0m0.013s. ```. [train.log](https://github.com/google/deepvariant/files/15253217/train.log).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/819
https://github.com/google/deepvariant/issues/819:85,performance,tune,tune,85,@nlopez94 can you cat validation_set.pbtxt and see how many examples you have in the tune data? It looks like everything ended regularly but there's too little data.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/819
https://github.com/google/deepvariant/issues/819:145,deployability,continu,continued,145,"@kishwarshafin As I mentioned before, the same dataset and parameters were used when I ran this on CPU, and as I indicated earlier, this process continued without abruptly ending as it did when I ran it with GPU. Below you can find what's on my validation_set.pbtxt. ```. # Generated by shuffle_tfrecords_lowmem.py. name: ""ASM3060704"". tfrecord_path: ""/home/nlopez/training-case-study/customized_training/validation_set.with_label.shuffled-?????-of-?????.tfrecord.gz"". num_examples: 7762. #. # --input_pattern_list=/home/nlopez/training-case-study/customized_training/validation_set.with_label.tfrecord-?????-of-00024.gz. # --output_pattern_prefix=/home/nlopez/training-case-study/customized_training/validation_set.with_label.shuffled. #. # class1: 5628. # class0: 1774. # class2: 360. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/819
https://github.com/google/deepvariant/issues/819:99,energy efficiency,CPU,CPU,99,"@kishwarshafin As I mentioned before, the same dataset and parameters were used when I ran this on CPU, and as I indicated earlier, this process continued without abruptly ending as it did when I ran it with GPU. Below you can find what's on my validation_set.pbtxt. ```. # Generated by shuffle_tfrecords_lowmem.py. name: ""ASM3060704"". tfrecord_path: ""/home/nlopez/training-case-study/customized_training/validation_set.with_label.shuffled-?????-of-?????.tfrecord.gz"". num_examples: 7762. #. # --input_pattern_list=/home/nlopez/training-case-study/customized_training/validation_set.with_label.tfrecord-?????-of-00024.gz. # --output_pattern_prefix=/home/nlopez/training-case-study/customized_training/validation_set.with_label.shuffled. #. # class1: 5628. # class0: 1774. # class2: 360. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/819
https://github.com/google/deepvariant/issues/819:208,energy efficiency,GPU,GPU,208,"@kishwarshafin As I mentioned before, the same dataset and parameters were used when I ran this on CPU, and as I indicated earlier, this process continued without abruptly ending as it did when I ran it with GPU. Below you can find what's on my validation_set.pbtxt. ```. # Generated by shuffle_tfrecords_lowmem.py. name: ""ASM3060704"". tfrecord_path: ""/home/nlopez/training-case-study/customized_training/validation_set.with_label.shuffled-?????-of-?????.tfrecord.gz"". num_examples: 7762. #. # --input_pattern_list=/home/nlopez/training-case-study/customized_training/validation_set.with_label.tfrecord-?????-of-00024.gz. # --output_pattern_prefix=/home/nlopez/training-case-study/customized_training/validation_set.with_label.shuffled. #. # class1: 5628. # class0: 1774. # class2: 360. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/819
https://github.com/google/deepvariant/issues/819:59,modifiability,paramet,parameters,59,"@kishwarshafin As I mentioned before, the same dataset and parameters were used when I ran this on CPU, and as I indicated earlier, this process continued without abruptly ending as it did when I ran it with GPU. Below you can find what's on my validation_set.pbtxt. ```. # Generated by shuffle_tfrecords_lowmem.py. name: ""ASM3060704"". tfrecord_path: ""/home/nlopez/training-case-study/customized_training/validation_set.with_label.shuffled-?????-of-?????.tfrecord.gz"". num_examples: 7762. #. # --input_pattern_list=/home/nlopez/training-case-study/customized_training/validation_set.with_label.tfrecord-?????-of-00024.gz. # --output_pattern_prefix=/home/nlopez/training-case-study/customized_training/validation_set.with_label.shuffled. #. # class1: 5628. # class0: 1774. # class2: 360. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/819
https://github.com/google/deepvariant/issues/819:99,performance,CPU,CPU,99,"@kishwarshafin As I mentioned before, the same dataset and parameters were used when I ran this on CPU, and as I indicated earlier, this process continued without abruptly ending as it did when I ran it with GPU. Below you can find what's on my validation_set.pbtxt. ```. # Generated by shuffle_tfrecords_lowmem.py. name: ""ASM3060704"". tfrecord_path: ""/home/nlopez/training-case-study/customized_training/validation_set.with_label.shuffled-?????-of-?????.tfrecord.gz"". num_examples: 7762. #. # --input_pattern_list=/home/nlopez/training-case-study/customized_training/validation_set.with_label.tfrecord-?????-of-00024.gz. # --output_pattern_prefix=/home/nlopez/training-case-study/customized_training/validation_set.with_label.shuffled. #. # class1: 5628. # class0: 1774. # class2: 360. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/819
https://github.com/google/deepvariant/issues/819:208,performance,GPU,GPU,208,"@kishwarshafin As I mentioned before, the same dataset and parameters were used when I ran this on CPU, and as I indicated earlier, this process continued without abruptly ending as it did when I ran it with GPU. Below you can find what's on my validation_set.pbtxt. ```. # Generated by shuffle_tfrecords_lowmem.py. name: ""ASM3060704"". tfrecord_path: ""/home/nlopez/training-case-study/customized_training/validation_set.with_label.shuffled-?????-of-?????.tfrecord.gz"". num_examples: 7762. #. # --input_pattern_list=/home/nlopez/training-case-study/customized_training/validation_set.with_label.tfrecord-?????-of-00024.gz. # --output_pattern_prefix=/home/nlopez/training-case-study/customized_training/validation_set.with_label.shuffled. #. # class1: 5628. # class0: 1774. # class2: 360. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/819
https://github.com/google/deepvariant/issues/819:113,usability,indicat,indicated,113,"@kishwarshafin As I mentioned before, the same dataset and parameters were used when I ran this on CPU, and as I indicated earlier, this process continued without abruptly ending as it did when I ran it with GPU. Below you can find what's on my validation_set.pbtxt. ```. # Generated by shuffle_tfrecords_lowmem.py. name: ""ASM3060704"". tfrecord_path: ""/home/nlopez/training-case-study/customized_training/validation_set.with_label.shuffled-?????-of-?????.tfrecord.gz"". num_examples: 7762. #. # --input_pattern_list=/home/nlopez/training-case-study/customized_training/validation_set.with_label.tfrecord-?????-of-00024.gz. # --output_pattern_prefix=/home/nlopez/training-case-study/customized_training/validation_set.with_label.shuffled. #. # class1: 5628. # class0: 1774. # class2: 360. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/819
https://github.com/google/deepvariant/issues/819:30,modifiability,paramet,parameter,30,@nlopez94 can you remove this parameter: `--config.num_validation_examples=0` and rerun please,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/819
https://github.com/google/deepvariant/issues/819:35,deployability,updat,update,35,@kishwarshafin I will try this and update you on the results I get. Thank you so much for the support!,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/819
https://github.com/google/deepvariant/issues/819:35,safety,updat,update,35,@kishwarshafin I will try this and update you on the results I get. Thank you so much for the support!,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/819
https://github.com/google/deepvariant/issues/819:35,security,updat,update,35,@kishwarshafin I will try this and update you on the results I get. Thank you so much for the support!,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/819
https://github.com/google/deepvariant/issues/819:94,usability,support,support,94,@kishwarshafin I will try this and update you on the results I get. Thank you so much for the support!,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/819
https://github.com/google/deepvariant/issues/819:32,availability,error,error,32,"@kishwarshafin I just found the error that was causing this to abruptly exit without warning. I was running the script with insufficient memory, and after changing my instance type, everything ran as expected. Thank you very much for answering my questions!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/819
https://github.com/google/deepvariant/issues/819:32,performance,error,error,32,"@kishwarshafin I just found the error that was causing this to abruptly exit without warning. I was running the script with insufficient memory, and after changing my instance type, everything ran as expected. Thank you very much for answering my questions!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/819
https://github.com/google/deepvariant/issues/819:137,performance,memor,memory,137,"@kishwarshafin I just found the error that was causing this to abruptly exit without warning. I was running the script with insufficient memory, and after changing my instance type, everything ran as expected. Thank you very much for answering my questions!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/819
https://github.com/google/deepvariant/issues/819:32,safety,error,error,32,"@kishwarshafin I just found the error that was causing this to abruptly exit without warning. I was running the script with insufficient memory, and after changing my instance type, everything ran as expected. Thank you very much for answering my questions!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/819
https://github.com/google/deepvariant/issues/819:32,usability,error,error,32,"@kishwarshafin I just found the error that was causing this to abruptly exit without warning. I was running the script with insufficient memory, and after changing my instance type, everything ran as expected. Thank you very much for answering my questions!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/819
https://github.com/google/deepvariant/issues/819:137,usability,memor,memory,137,"@kishwarshafin I just found the error that was causing this to abruptly exit without warning. I was running the script with insufficient memory, and after changing my instance type, everything ran as expected. Thank you very much for answering my questions!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/819
https://github.com/google/deepvariant/issues/819:11,usability,confirm,confirming,11,"Thanks for confirming @nlopez94, I will close the issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/819
https://github.com/google/deepvariant/issues/819:40,usability,close,close,40,"Thanks for confirming @nlopez94, I will close the issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/819
https://github.com/google/deepvariant/issues/820:45,usability,command,command,45,"Hi @kunmonster ,. Could you please provide a command line that you use to run DeepVariant?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/820
https://github.com/google/deepvariant/issues/820:178,deployability,contain,container,178,"> Hi @kunmonster , Could you please provide a command line that you use to run DeepVariant? Sorry,the command line is in the top of the second picture. Actually,i run the docker container in interactive mode,then run the call_variants line within the container.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/820
https://github.com/google/deepvariant/issues/820:251,deployability,contain,container,251,"> Hi @kunmonster , Could you please provide a command line that you use to run DeepVariant? Sorry,the command line is in the top of the second picture. Actually,i run the docker container in interactive mode,then run the call_variants line within the container.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/820
https://github.com/google/deepvariant/issues/820:46,usability,command,command,46,"> Hi @kunmonster , Could you please provide a command line that you use to run DeepVariant? Sorry,the command line is in the top of the second picture. Actually,i run the docker container in interactive mode,then run the call_variants line within the container.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/820
https://github.com/google/deepvariant/issues/820:102,usability,command,command,102,"> Hi @kunmonster , Could you please provide a command line that you use to run DeepVariant? Sorry,the command line is in the top of the second picture. Actually,i run the docker container in interactive mode,then run the call_variants line within the container.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/820
https://github.com/google/deepvariant/issues/820:191,usability,interact,interactive,191,"> Hi @kunmonster , Could you please provide a command line that you use to run DeepVariant? Sorry,the command line is in the top of the second picture. Actually,i run the docker container in interactive mode,then run the call_variants line within the container.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/820
https://github.com/google/deepvariant/issues/820:132,availability,monitor,monitor,132,Since sometimes warning messages from Tensorflow may be misleading. Could you please try running call_variants and at the same time monitor the GPU load to make sure the GPU is not used? You can use `watch -n0.5 nvidia-smi` to check the GPU load in real time.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/820
https://github.com/google/deepvariant/issues/820:132,deployability,monitor,monitor,132,Since sometimes warning messages from Tensorflow may be misleading. Could you please try running call_variants and at the same time monitor the GPU load to make sure the GPU is not used? You can use `watch -n0.5 nvidia-smi` to check the GPU load in real time.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/820
https://github.com/google/deepvariant/issues/820:132,energy efficiency,monitor,monitor,132,Since sometimes warning messages from Tensorflow may be misleading. Could you please try running call_variants and at the same time monitor the GPU load to make sure the GPU is not used? You can use `watch -n0.5 nvidia-smi` to check the GPU load in real time.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/820
https://github.com/google/deepvariant/issues/820:144,energy efficiency,GPU,GPU,144,Since sometimes warning messages from Tensorflow may be misleading. Could you please try running call_variants and at the same time monitor the GPU load to make sure the GPU is not used? You can use `watch -n0.5 nvidia-smi` to check the GPU load in real time.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/820
https://github.com/google/deepvariant/issues/820:148,energy efficiency,load,load,148,Since sometimes warning messages from Tensorflow may be misleading. Could you please try running call_variants and at the same time monitor the GPU load to make sure the GPU is not used? You can use `watch -n0.5 nvidia-smi` to check the GPU load in real time.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/820
https://github.com/google/deepvariant/issues/820:170,energy efficiency,GPU,GPU,170,Since sometimes warning messages from Tensorflow may be misleading. Could you please try running call_variants and at the same time monitor the GPU load to make sure the GPU is not used? You can use `watch -n0.5 nvidia-smi` to check the GPU load in real time.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/820
https://github.com/google/deepvariant/issues/820:237,energy efficiency,GPU,GPU,237,Since sometimes warning messages from Tensorflow may be misleading. Could you please try running call_variants and at the same time monitor the GPU load to make sure the GPU is not used? You can use `watch -n0.5 nvidia-smi` to check the GPU load in real time.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/820
https://github.com/google/deepvariant/issues/820:241,energy efficiency,load,load,241,Since sometimes warning messages from Tensorflow may be misleading. Could you please try running call_variants and at the same time monitor the GPU load to make sure the GPU is not used? You can use `watch -n0.5 nvidia-smi` to check the GPU load in real time.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/820
https://github.com/google/deepvariant/issues/820:24,integrability,messag,messages,24,Since sometimes warning messages from Tensorflow may be misleading. Could you please try running call_variants and at the same time monitor the GPU load to make sure the GPU is not used? You can use `watch -n0.5 nvidia-smi` to check the GPU load in real time.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/820
https://github.com/google/deepvariant/issues/820:24,interoperability,messag,messages,24,Since sometimes warning messages from Tensorflow may be misleading. Could you please try running call_variants and at the same time monitor the GPU load to make sure the GPU is not used? You can use `watch -n0.5 nvidia-smi` to check the GPU load in real time.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/820
https://github.com/google/deepvariant/issues/820:127,performance,time,time,127,Since sometimes warning messages from Tensorflow may be misleading. Could you please try running call_variants and at the same time monitor the GPU load to make sure the GPU is not used? You can use `watch -n0.5 nvidia-smi` to check the GPU load in real time.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/820
https://github.com/google/deepvariant/issues/820:144,performance,GPU,GPU,144,Since sometimes warning messages from Tensorflow may be misleading. Could you please try running call_variants and at the same time monitor the GPU load to make sure the GPU is not used? You can use `watch -n0.5 nvidia-smi` to check the GPU load in real time.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/820
https://github.com/google/deepvariant/issues/820:148,performance,load,load,148,Since sometimes warning messages from Tensorflow may be misleading. Could you please try running call_variants and at the same time monitor the GPU load to make sure the GPU is not used? You can use `watch -n0.5 nvidia-smi` to check the GPU load in real time.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/820
https://github.com/google/deepvariant/issues/820:170,performance,GPU,GPU,170,Since sometimes warning messages from Tensorflow may be misleading. Could you please try running call_variants and at the same time monitor the GPU load to make sure the GPU is not used? You can use `watch -n0.5 nvidia-smi` to check the GPU load in real time.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/820
https://github.com/google/deepvariant/issues/820:237,performance,GPU,GPU,237,Since sometimes warning messages from Tensorflow may be misleading. Could you please try running call_variants and at the same time monitor the GPU load to make sure the GPU is not used? You can use `watch -n0.5 nvidia-smi` to check the GPU load in real time.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/820
https://github.com/google/deepvariant/issues/820:241,performance,load,load,241,Since sometimes warning messages from Tensorflow may be misleading. Could you please try running call_variants and at the same time monitor the GPU load to make sure the GPU is not used? You can use `watch -n0.5 nvidia-smi` to check the GPU load in real time.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/820
https://github.com/google/deepvariant/issues/820:254,performance,time,time,254,Since sometimes warning messages from Tensorflow may be misleading. Could you please try running call_variants and at the same time monitor the GPU load to make sure the GPU is not used? You can use `watch -n0.5 nvidia-smi` to check the GPU load in real time.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/820
https://github.com/google/deepvariant/issues/820:132,reliability,monitor,monitor,132,Since sometimes warning messages from Tensorflow may be misleading. Could you please try running call_variants and at the same time monitor the GPU load to make sure the GPU is not used? You can use `watch -n0.5 nvidia-smi` to check the GPU load in real time.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/820
https://github.com/google/deepvariant/issues/820:132,safety,monitor,monitor,132,Since sometimes warning messages from Tensorflow may be misleading. Could you please try running call_variants and at the same time monitor the GPU load to make sure the GPU is not used? You can use `watch -n0.5 nvidia-smi` to check the GPU load in real time.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/820
https://github.com/google/deepvariant/issues/820:132,testability,monitor,monitor,132,Since sometimes warning messages from Tensorflow may be misleading. Could you please try running call_variants and at the same time monitor the GPU load to make sure the GPU is not used? You can use `watch -n0.5 nvidia-smi` to check the GPU load in real time.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/820
https://github.com/google/deepvariant/issues/820:72,energy efficiency,gpu,gpu,72,"Happy to see the reply,actually, i run this on hpc and the usage of the gpu is very low , but the cpu and memory usage is extremely high , then i run this command line on my laptop with gtx 1050ti and compare the time of the prediction one batch ,the time in the hpc is longer than my laptop , but the truth is the performance of the hpc gpu is better than gtx1050ti. So, the gpu don't work. I will post what you want later.Thx!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/820
https://github.com/google/deepvariant/issues/820:98,energy efficiency,cpu,cpu,98,"Happy to see the reply,actually, i run this on hpc and the usage of the gpu is very low , but the cpu and memory usage is extremely high , then i run this command line on my laptop with gtx 1050ti and compare the time of the prediction one batch ,the time in the hpc is longer than my laptop , but the truth is the performance of the hpc gpu is better than gtx1050ti. So, the gpu don't work. I will post what you want later.Thx!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/820
https://github.com/google/deepvariant/issues/820:225,energy efficiency,predict,prediction,225,"Happy to see the reply,actually, i run this on hpc and the usage of the gpu is very low , but the cpu and memory usage is extremely high , then i run this command line on my laptop with gtx 1050ti and compare the time of the prediction one batch ,the time in the hpc is longer than my laptop , but the truth is the performance of the hpc gpu is better than gtx1050ti. So, the gpu don't work. I will post what you want later.Thx!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/820
https://github.com/google/deepvariant/issues/820:338,energy efficiency,gpu,gpu,338,"Happy to see the reply,actually, i run this on hpc and the usage of the gpu is very low , but the cpu and memory usage is extremely high , then i run this command line on my laptop with gtx 1050ti and compare the time of the prediction one batch ,the time in the hpc is longer than my laptop , but the truth is the performance of the hpc gpu is better than gtx1050ti. So, the gpu don't work. I will post what you want later.Thx!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/820
https://github.com/google/deepvariant/issues/820:376,energy efficiency,gpu,gpu,376,"Happy to see the reply,actually, i run this on hpc and the usage of the gpu is very low , but the cpu and memory usage is extremely high , then i run this command line on my laptop with gtx 1050ti and compare the time of the prediction one batch ,the time in the hpc is longer than my laptop , but the truth is the performance of the hpc gpu is better than gtx1050ti. So, the gpu don't work. I will post what you want later.Thx!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/820
https://github.com/google/deepvariant/issues/820:240,integrability,batch,batch,240,"Happy to see the reply,actually, i run this on hpc and the usage of the gpu is very low , but the cpu and memory usage is extremely high , then i run this command line on my laptop with gtx 1050ti and compare the time of the prediction one batch ,the time in the hpc is longer than my laptop , but the truth is the performance of the hpc gpu is better than gtx1050ti. So, the gpu don't work. I will post what you want later.Thx!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/820
https://github.com/google/deepvariant/issues/820:72,performance,gpu,gpu,72,"Happy to see the reply,actually, i run this on hpc and the usage of the gpu is very low , but the cpu and memory usage is extremely high , then i run this command line on my laptop with gtx 1050ti and compare the time of the prediction one batch ,the time in the hpc is longer than my laptop , but the truth is the performance of the hpc gpu is better than gtx1050ti. So, the gpu don't work. I will post what you want later.Thx!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/820
https://github.com/google/deepvariant/issues/820:98,performance,cpu,cpu,98,"Happy to see the reply,actually, i run this on hpc and the usage of the gpu is very low , but the cpu and memory usage is extremely high , then i run this command line on my laptop with gtx 1050ti and compare the time of the prediction one batch ,the time in the hpc is longer than my laptop , but the truth is the performance of the hpc gpu is better than gtx1050ti. So, the gpu don't work. I will post what you want later.Thx!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/820
https://github.com/google/deepvariant/issues/820:106,performance,memor,memory,106,"Happy to see the reply,actually, i run this on hpc and the usage of the gpu is very low , but the cpu and memory usage is extremely high , then i run this command line on my laptop with gtx 1050ti and compare the time of the prediction one batch ,the time in the hpc is longer than my laptop , but the truth is the performance of the hpc gpu is better than gtx1050ti. So, the gpu don't work. I will post what you want later.Thx!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/820
https://github.com/google/deepvariant/issues/820:213,performance,time,time,213,"Happy to see the reply,actually, i run this on hpc and the usage of the gpu is very low , but the cpu and memory usage is extremely high , then i run this command line on my laptop with gtx 1050ti and compare the time of the prediction one batch ,the time in the hpc is longer than my laptop , but the truth is the performance of the hpc gpu is better than gtx1050ti. So, the gpu don't work. I will post what you want later.Thx!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/820
https://github.com/google/deepvariant/issues/820:240,performance,batch,batch,240,"Happy to see the reply,actually, i run this on hpc and the usage of the gpu is very low , but the cpu and memory usage is extremely high , then i run this command line on my laptop with gtx 1050ti and compare the time of the prediction one batch ,the time in the hpc is longer than my laptop , but the truth is the performance of the hpc gpu is better than gtx1050ti. So, the gpu don't work. I will post what you want later.Thx!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/820
https://github.com/google/deepvariant/issues/820:251,performance,time,time,251,"Happy to see the reply,actually, i run this on hpc and the usage of the gpu is very low , but the cpu and memory usage is extremely high , then i run this command line on my laptop with gtx 1050ti and compare the time of the prediction one batch ,the time in the hpc is longer than my laptop , but the truth is the performance of the hpc gpu is better than gtx1050ti. So, the gpu don't work. I will post what you want later.Thx!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/820
https://github.com/google/deepvariant/issues/820:315,performance,perform,performance,315,"Happy to see the reply,actually, i run this on hpc and the usage of the gpu is very low , but the cpu and memory usage is extremely high , then i run this command line on my laptop with gtx 1050ti and compare the time of the prediction one batch ,the time in the hpc is longer than my laptop , but the truth is the performance of the hpc gpu is better than gtx1050ti. So, the gpu don't work. I will post what you want later.Thx!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/820
https://github.com/google/deepvariant/issues/820:338,performance,gpu,gpu,338,"Happy to see the reply,actually, i run this on hpc and the usage of the gpu is very low , but the cpu and memory usage is extremely high , then i run this command line on my laptop with gtx 1050ti and compare the time of the prediction one batch ,the time in the hpc is longer than my laptop , but the truth is the performance of the hpc gpu is better than gtx1050ti. So, the gpu don't work. I will post what you want later.Thx!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/820
https://github.com/google/deepvariant/issues/820:376,performance,gpu,gpu,376,"Happy to see the reply,actually, i run this on hpc and the usage of the gpu is very low , but the cpu and memory usage is extremely high , then i run this command line on my laptop with gtx 1050ti and compare the time of the prediction one batch ,the time in the hpc is longer than my laptop , but the truth is the performance of the hpc gpu is better than gtx1050ti. So, the gpu don't work. I will post what you want later.Thx!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/820
https://github.com/google/deepvariant/issues/820:225,safety,predict,prediction,225,"Happy to see the reply,actually, i run this on hpc and the usage of the gpu is very low , but the cpu and memory usage is extremely high , then i run this command line on my laptop with gtx 1050ti and compare the time of the prediction one batch ,the time in the hpc is longer than my laptop , but the truth is the performance of the hpc gpu is better than gtx1050ti. So, the gpu don't work. I will post what you want later.Thx!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/820
https://github.com/google/deepvariant/issues/820:106,usability,memor,memory,106,"Happy to see the reply,actually, i run this on hpc and the usage of the gpu is very low , but the cpu and memory usage is extremely high , then i run this command line on my laptop with gtx 1050ti and compare the time of the prediction one batch ,the time in the hpc is longer than my laptop , but the truth is the performance of the hpc gpu is better than gtx1050ti. So, the gpu don't work. I will post what you want later.Thx!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/820
https://github.com/google/deepvariant/issues/820:155,usability,command,command,155,"Happy to see the reply,actually, i run this on hpc and the usage of the gpu is very low , but the cpu and memory usage is extremely high , then i run this command line on my laptop with gtx 1050ti and compare the time of the prediction one batch ,the time in the hpc is longer than my laptop , but the truth is the performance of the hpc gpu is better than gtx1050ti. So, the gpu don't work. I will post what you want later.Thx!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/820
https://github.com/google/deepvariant/issues/820:315,usability,perform,performance,315,"Happy to see the reply,actually, i run this on hpc and the usage of the gpu is very low , but the cpu and memory usage is extremely high , then i run this command line on my laptop with gtx 1050ti and compare the time of the prediction one batch ,the time in the hpc is longer than my laptop , but the truth is the performance of the hpc gpu is better than gtx1050ti. So, the gpu don't work. I will post what you want later.Thx!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/820
https://github.com/google/deepvariant/issues/820:40,energy efficiency,gpu,gpu,40,There are the pictures for the usage of gpu. ![image](https://github.com/google/deepvariant/assets/71956115/84cd43f4-3828-4bcb-ac71-826c8dc2a06b). ![image](https://github.com/google/deepvariant/assets/71956115/803bad2e-7999-4309-b5fe-ddac360288b5).,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/820
https://github.com/google/deepvariant/issues/820:40,performance,gpu,gpu,40,There are the pictures for the usage of gpu. ![image](https://github.com/google/deepvariant/assets/71956115/84cd43f4-3828-4bcb-ac71-826c8dc2a06b). ![image](https://github.com/google/deepvariant/assets/71956115/803bad2e-7999-4309-b5fe-ddac360288b5).,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/820
https://github.com/google/deepvariant/issues/820:209,availability,error,error-no-device,209,Did you run `tensorflow.test.is_gpu_available()` from the DeepVariant docker? Could you try the suggestion from this [thread](https://stackoverflow.com/questions/48658204/tensorflow-failed-call-to-cuinit-cuda-error-no-device),MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/820
https://github.com/google/deepvariant/issues/820:134,deployability,stack,stackoverflow,134,Did you run `tensorflow.test.is_gpu_available()` from the DeepVariant docker? Could you try the suggestion from this [thread](https://stackoverflow.com/questions/48658204/tensorflow-failed-call-to-cuinit-cuda-error-no-device),MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/820
https://github.com/google/deepvariant/issues/820:182,deployability,fail,failed-call-to-cuinit-cuda-error-no-device,182,Did you run `tensorflow.test.is_gpu_available()` from the DeepVariant docker? Could you try the suggestion from this [thread](https://stackoverflow.com/questions/48658204/tensorflow-failed-call-to-cuinit-cuda-error-no-device),MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/820
https://github.com/google/deepvariant/issues/820:209,performance,error,error-no-device,209,Did you run `tensorflow.test.is_gpu_available()` from the DeepVariant docker? Could you try the suggestion from this [thread](https://stackoverflow.com/questions/48658204/tensorflow-failed-call-to-cuinit-cuda-error-no-device),MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/820
https://github.com/google/deepvariant/issues/820:182,reliability,fail,failed-call-to-cuinit-cuda-error-no-device,182,Did you run `tensorflow.test.is_gpu_available()` from the DeepVariant docker? Could you try the suggestion from this [thread](https://stackoverflow.com/questions/48658204/tensorflow-failed-call-to-cuinit-cuda-error-no-device),MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/820
https://github.com/google/deepvariant/issues/820:24,safety,test,test,24,Did you run `tensorflow.test.is_gpu_available()` from the DeepVariant docker? Could you try the suggestion from this [thread](https://stackoverflow.com/questions/48658204/tensorflow-failed-call-to-cuinit-cuda-error-no-device),MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/820
https://github.com/google/deepvariant/issues/820:209,safety,error,error-no-device,209,Did you run `tensorflow.test.is_gpu_available()` from the DeepVariant docker? Could you try the suggestion from this [thread](https://stackoverflow.com/questions/48658204/tensorflow-failed-call-to-cuinit-cuda-error-no-device),MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/820
https://github.com/google/deepvariant/issues/820:24,testability,test,test,24,Did you run `tensorflow.test.is_gpu_available()` from the DeepVariant docker? Could you try the suggestion from this [thread](https://stackoverflow.com/questions/48658204/tensorflow-failed-call-to-cuinit-cuda-error-no-device),MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/820
https://github.com/google/deepvariant/issues/820:209,usability,error,error-no-device,209,Did you run `tensorflow.test.is_gpu_available()` from the DeepVariant docker? Could you try the suggestion from this [thread](https://stackoverflow.com/questions/48658204/tensorflow-failed-call-to-cuinit-cuda-error-no-device),MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/820
https://github.com/google/deepvariant/issues/820:347,availability,error,error-no-device,347,"> Did you run `tensorflow.test.is_gpu_available()` from the DeepVariant docker? Yes i did , I have posted the result which shows that in python shell the gpu can be identified with tensorflow in the first comment. > Could you try the suggestion from this [thread](https://stackoverflow.com/questions/48658204/tensorflow-failed-call-to-cuinit-cuda-error-no-device). Actuallly i have tried to set ` CUDA_VISIBLE_DEVICES=0 ` in System ENV ,it did't work .So I tried to find the place where sets the value of the env in your code , and want to set the ` CUDA_VISIBLE_DEVICES=0 ` , but i did't find. So ,i turn to ask for your help. I think the reason why the error occurs may be in your code the value of the ` CUDA_VISIBLE_DEVICES` does't match with my device.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/820
https://github.com/google/deepvariant/issues/820:655,availability,error,error,655,"> Did you run `tensorflow.test.is_gpu_available()` from the DeepVariant docker? Yes i did , I have posted the result which shows that in python shell the gpu can be identified with tensorflow in the first comment. > Could you try the suggestion from this [thread](https://stackoverflow.com/questions/48658204/tensorflow-failed-call-to-cuinit-cuda-error-no-device). Actuallly i have tried to set ` CUDA_VISIBLE_DEVICES=0 ` in System ENV ,it did't work .So I tried to find the place where sets the value of the env in your code , and want to set the ` CUDA_VISIBLE_DEVICES=0 ` , but i did't find. So ,i turn to ask for your help. I think the reason why the error occurs may be in your code the value of the ` CUDA_VISIBLE_DEVICES` does't match with my device.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/820
https://github.com/google/deepvariant/issues/820:272,deployability,stack,stackoverflow,272,"> Did you run `tensorflow.test.is_gpu_available()` from the DeepVariant docker? Yes i did , I have posted the result which shows that in python shell the gpu can be identified with tensorflow in the first comment. > Could you try the suggestion from this [thread](https://stackoverflow.com/questions/48658204/tensorflow-failed-call-to-cuinit-cuda-error-no-device). Actuallly i have tried to set ` CUDA_VISIBLE_DEVICES=0 ` in System ENV ,it did't work .So I tried to find the place where sets the value of the env in your code , and want to set the ` CUDA_VISIBLE_DEVICES=0 ` , but i did't find. So ,i turn to ask for your help. I think the reason why the error occurs may be in your code the value of the ` CUDA_VISIBLE_DEVICES` does't match with my device.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/820
https://github.com/google/deepvariant/issues/820:320,deployability,fail,failed-call-to-cuinit-cuda-error-no-device,320,"> Did you run `tensorflow.test.is_gpu_available()` from the DeepVariant docker? Yes i did , I have posted the result which shows that in python shell the gpu can be identified with tensorflow in the first comment. > Could you try the suggestion from this [thread](https://stackoverflow.com/questions/48658204/tensorflow-failed-call-to-cuinit-cuda-error-no-device). Actuallly i have tried to set ` CUDA_VISIBLE_DEVICES=0 ` in System ENV ,it did't work .So I tried to find the place where sets the value of the env in your code , and want to set the ` CUDA_VISIBLE_DEVICES=0 ` , but i did't find. So ,i turn to ask for your help. I think the reason why the error occurs may be in your code the value of the ` CUDA_VISIBLE_DEVICES` does't match with my device.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/820
https://github.com/google/deepvariant/issues/820:154,energy efficiency,gpu,gpu,154,"> Did you run `tensorflow.test.is_gpu_available()` from the DeepVariant docker? Yes i did , I have posted the result which shows that in python shell the gpu can be identified with tensorflow in the first comment. > Could you try the suggestion from this [thread](https://stackoverflow.com/questions/48658204/tensorflow-failed-call-to-cuinit-cuda-error-no-device). Actuallly i have tried to set ` CUDA_VISIBLE_DEVICES=0 ` in System ENV ,it did't work .So I tried to find the place where sets the value of the env in your code , and want to set the ` CUDA_VISIBLE_DEVICES=0 ` , but i did't find. So ,i turn to ask for your help. I think the reason why the error occurs may be in your code the value of the ` CUDA_VISIBLE_DEVICES` does't match with my device.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/820
https://github.com/google/deepvariant/issues/820:154,performance,gpu,gpu,154,"> Did you run `tensorflow.test.is_gpu_available()` from the DeepVariant docker? Yes i did , I have posted the result which shows that in python shell the gpu can be identified with tensorflow in the first comment. > Could you try the suggestion from this [thread](https://stackoverflow.com/questions/48658204/tensorflow-failed-call-to-cuinit-cuda-error-no-device). Actuallly i have tried to set ` CUDA_VISIBLE_DEVICES=0 ` in System ENV ,it did't work .So I tried to find the place where sets the value of the env in your code , and want to set the ` CUDA_VISIBLE_DEVICES=0 ` , but i did't find. So ,i turn to ask for your help. I think the reason why the error occurs may be in your code the value of the ` CUDA_VISIBLE_DEVICES` does't match with my device.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/820
https://github.com/google/deepvariant/issues/820:347,performance,error,error-no-device,347,"> Did you run `tensorflow.test.is_gpu_available()` from the DeepVariant docker? Yes i did , I have posted the result which shows that in python shell the gpu can be identified with tensorflow in the first comment. > Could you try the suggestion from this [thread](https://stackoverflow.com/questions/48658204/tensorflow-failed-call-to-cuinit-cuda-error-no-device). Actuallly i have tried to set ` CUDA_VISIBLE_DEVICES=0 ` in System ENV ,it did't work .So I tried to find the place where sets the value of the env in your code , and want to set the ` CUDA_VISIBLE_DEVICES=0 ` , but i did't find. So ,i turn to ask for your help. I think the reason why the error occurs may be in your code the value of the ` CUDA_VISIBLE_DEVICES` does't match with my device.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/820
https://github.com/google/deepvariant/issues/820:655,performance,error,error,655,"> Did you run `tensorflow.test.is_gpu_available()` from the DeepVariant docker? Yes i did , I have posted the result which shows that in python shell the gpu can be identified with tensorflow in the first comment. > Could you try the suggestion from this [thread](https://stackoverflow.com/questions/48658204/tensorflow-failed-call-to-cuinit-cuda-error-no-device). Actuallly i have tried to set ` CUDA_VISIBLE_DEVICES=0 ` in System ENV ,it did't work .So I tried to find the place where sets the value of the env in your code , and want to set the ` CUDA_VISIBLE_DEVICES=0 ` , but i did't find. So ,i turn to ask for your help. I think the reason why the error occurs may be in your code the value of the ` CUDA_VISIBLE_DEVICES` does't match with my device.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/820
https://github.com/google/deepvariant/issues/820:320,reliability,fail,failed-call-to-cuinit-cuda-error-no-device,320,"> Did you run `tensorflow.test.is_gpu_available()` from the DeepVariant docker? Yes i did , I have posted the result which shows that in python shell the gpu can be identified with tensorflow in the first comment. > Could you try the suggestion from this [thread](https://stackoverflow.com/questions/48658204/tensorflow-failed-call-to-cuinit-cuda-error-no-device). Actuallly i have tried to set ` CUDA_VISIBLE_DEVICES=0 ` in System ENV ,it did't work .So I tried to find the place where sets the value of the env in your code , and want to set the ` CUDA_VISIBLE_DEVICES=0 ` , but i did't find. So ,i turn to ask for your help. I think the reason why the error occurs may be in your code the value of the ` CUDA_VISIBLE_DEVICES` does't match with my device.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/820
https://github.com/google/deepvariant/issues/820:729,reliability,doe,does,729,"> Did you run `tensorflow.test.is_gpu_available()` from the DeepVariant docker? Yes i did , I have posted the result which shows that in python shell the gpu can be identified with tensorflow in the first comment. > Could you try the suggestion from this [thread](https://stackoverflow.com/questions/48658204/tensorflow-failed-call-to-cuinit-cuda-error-no-device). Actuallly i have tried to set ` CUDA_VISIBLE_DEVICES=0 ` in System ENV ,it did't work .So I tried to find the place where sets the value of the env in your code , and want to set the ` CUDA_VISIBLE_DEVICES=0 ` , but i did't find. So ,i turn to ask for your help. I think the reason why the error occurs may be in your code the value of the ` CUDA_VISIBLE_DEVICES` does't match with my device.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/820
https://github.com/google/deepvariant/issues/820:26,safety,test,test,26,"> Did you run `tensorflow.test.is_gpu_available()` from the DeepVariant docker? Yes i did , I have posted the result which shows that in python shell the gpu can be identified with tensorflow in the first comment. > Could you try the suggestion from this [thread](https://stackoverflow.com/questions/48658204/tensorflow-failed-call-to-cuinit-cuda-error-no-device). Actuallly i have tried to set ` CUDA_VISIBLE_DEVICES=0 ` in System ENV ,it did't work .So I tried to find the place where sets the value of the env in your code , and want to set the ` CUDA_VISIBLE_DEVICES=0 ` , but i did't find. So ,i turn to ask for your help. I think the reason why the error occurs may be in your code the value of the ` CUDA_VISIBLE_DEVICES` does't match with my device.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/820
https://github.com/google/deepvariant/issues/820:347,safety,error,error-no-device,347,"> Did you run `tensorflow.test.is_gpu_available()` from the DeepVariant docker? Yes i did , I have posted the result which shows that in python shell the gpu can be identified with tensorflow in the first comment. > Could you try the suggestion from this [thread](https://stackoverflow.com/questions/48658204/tensorflow-failed-call-to-cuinit-cuda-error-no-device). Actuallly i have tried to set ` CUDA_VISIBLE_DEVICES=0 ` in System ENV ,it did't work .So I tried to find the place where sets the value of the env in your code , and want to set the ` CUDA_VISIBLE_DEVICES=0 ` , but i did't find. So ,i turn to ask for your help. I think the reason why the error occurs may be in your code the value of the ` CUDA_VISIBLE_DEVICES` does't match with my device.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/820
https://github.com/google/deepvariant/issues/820:655,safety,error,error,655,"> Did you run `tensorflow.test.is_gpu_available()` from the DeepVariant docker? Yes i did , I have posted the result which shows that in python shell the gpu can be identified with tensorflow in the first comment. > Could you try the suggestion from this [thread](https://stackoverflow.com/questions/48658204/tensorflow-failed-call-to-cuinit-cuda-error-no-device). Actuallly i have tried to set ` CUDA_VISIBLE_DEVICES=0 ` in System ENV ,it did't work .So I tried to find the place where sets the value of the env in your code , and want to set the ` CUDA_VISIBLE_DEVICES=0 ` , but i did't find. So ,i turn to ask for your help. I think the reason why the error occurs may be in your code the value of the ` CUDA_VISIBLE_DEVICES` does't match with my device.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/820
https://github.com/google/deepvariant/issues/820:165,security,ident,identified,165,"> Did you run `tensorflow.test.is_gpu_available()` from the DeepVariant docker? Yes i did , I have posted the result which shows that in python shell the gpu can be identified with tensorflow in the first comment. > Could you try the suggestion from this [thread](https://stackoverflow.com/questions/48658204/tensorflow-failed-call-to-cuinit-cuda-error-no-device). Actuallly i have tried to set ` CUDA_VISIBLE_DEVICES=0 ` in System ENV ,it did't work .So I tried to find the place where sets the value of the env in your code , and want to set the ` CUDA_VISIBLE_DEVICES=0 ` , but i did't find. So ,i turn to ask for your help. I think the reason why the error occurs may be in your code the value of the ` CUDA_VISIBLE_DEVICES` does't match with my device.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/820
https://github.com/google/deepvariant/issues/820:26,testability,test,test,26,"> Did you run `tensorflow.test.is_gpu_available()` from the DeepVariant docker? Yes i did , I have posted the result which shows that in python shell the gpu can be identified with tensorflow in the first comment. > Could you try the suggestion from this [thread](https://stackoverflow.com/questions/48658204/tensorflow-failed-call-to-cuinit-cuda-error-no-device). Actuallly i have tried to set ` CUDA_VISIBLE_DEVICES=0 ` in System ENV ,it did't work .So I tried to find the place where sets the value of the env in your code , and want to set the ` CUDA_VISIBLE_DEVICES=0 ` , but i did't find. So ,i turn to ask for your help. I think the reason why the error occurs may be in your code the value of the ` CUDA_VISIBLE_DEVICES` does't match with my device.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/820
https://github.com/google/deepvariant/issues/820:347,usability,error,error-no-device,347,"> Did you run `tensorflow.test.is_gpu_available()` from the DeepVariant docker? Yes i did , I have posted the result which shows that in python shell the gpu can be identified with tensorflow in the first comment. > Could you try the suggestion from this [thread](https://stackoverflow.com/questions/48658204/tensorflow-failed-call-to-cuinit-cuda-error-no-device). Actuallly i have tried to set ` CUDA_VISIBLE_DEVICES=0 ` in System ENV ,it did't work .So I tried to find the place where sets the value of the env in your code , and want to set the ` CUDA_VISIBLE_DEVICES=0 ` , but i did't find. So ,i turn to ask for your help. I think the reason why the error occurs may be in your code the value of the ` CUDA_VISIBLE_DEVICES` does't match with my device.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/820
https://github.com/google/deepvariant/issues/820:622,usability,help,help,622,"> Did you run `tensorflow.test.is_gpu_available()` from the DeepVariant docker? Yes i did , I have posted the result which shows that in python shell the gpu can be identified with tensorflow in the first comment. > Could you try the suggestion from this [thread](https://stackoverflow.com/questions/48658204/tensorflow-failed-call-to-cuinit-cuda-error-no-device). Actuallly i have tried to set ` CUDA_VISIBLE_DEVICES=0 ` in System ENV ,it did't work .So I tried to find the place where sets the value of the env in your code , and want to set the ` CUDA_VISIBLE_DEVICES=0 ` , but i did't find. So ,i turn to ask for your help. I think the reason why the error occurs may be in your code the value of the ` CUDA_VISIBLE_DEVICES` does't match with my device.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/820
https://github.com/google/deepvariant/issues/820:655,usability,error,error,655,"> Did you run `tensorflow.test.is_gpu_available()` from the DeepVariant docker? Yes i did , I have posted the result which shows that in python shell the gpu can be identified with tensorflow in the first comment. > Could you try the suggestion from this [thread](https://stackoverflow.com/questions/48658204/tensorflow-failed-call-to-cuinit-cuda-error-no-device). Actuallly i have tried to set ` CUDA_VISIBLE_DEVICES=0 ` in System ENV ,it did't work .So I tried to find the place where sets the value of the env in your code , and want to set the ` CUDA_VISIBLE_DEVICES=0 ` , but i did't find. So ,i turn to ask for your help. I think the reason why the error occurs may be in your code the value of the ` CUDA_VISIBLE_DEVICES` does't match with my device.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/820
https://github.com/google/deepvariant/issues/820:281,energy efficiency,gpu,gpus,281,"Hi @kunmonster ,. if I understand correctly, we haven't been able to reproduce the issue on our side, therefore it has been difficult for us to help. I know you've already run `tensorflow.test.is_gpu_available()`. can you also try this and see what you see? ```. sudo docker run --gpus all google/deepvariant:1.6.1-gpu python3 -c ""import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))"". ```. When I run this on my machine, I see:. ```. [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]. ```. Given that your is_gpu_available is True, I think it'll also list something. Just want to double check here. If there's any other information that you can provide, in order for us to reproduce on our side, please let us know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/820
https://github.com/google/deepvariant/issues/820:315,energy efficiency,gpu,gpu,315,"Hi @kunmonster ,. if I understand correctly, we haven't been able to reproduce the issue on our side, therefore it has been difficult for us to help. I know you've already run `tensorflow.test.is_gpu_available()`. can you also try this and see what you see? ```. sudo docker run --gpus all google/deepvariant:1.6.1-gpu python3 -c ""import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))"". ```. When I run this on my machine, I see:. ```. [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]. ```. Given that your is_gpu_available is True, I think it'll also list something. Just want to double check here. If there's any other information that you can provide, in order for us to reproduce on our side, please let us know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/820
https://github.com/google/deepvariant/issues/820:395,energy efficiency,GPU,GPU,395,"Hi @kunmonster ,. if I understand correctly, we haven't been able to reproduce the issue on our side, therefore it has been difficult for us to help. I know you've already run `tensorflow.test.is_gpu_available()`. can you also try this and see what you see? ```. sudo docker run --gpus all google/deepvariant:1.6.1-gpu python3 -c ""import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))"". ```. When I run this on my machine, I see:. ```. [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]. ```. Given that your is_gpu_available is True, I think it'll also list something. Just want to double check here. If there's any other information that you can provide, in order for us to reproduce on our side, please let us know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/820
https://github.com/google/deepvariant/issues/820:492,energy efficiency,GPU,GPU,492,"Hi @kunmonster ,. if I understand correctly, we haven't been able to reproduce the issue on our side, therefore it has been difficult for us to help. I know you've already run `tensorflow.test.is_gpu_available()`. can you also try this and see what you see? ```. sudo docker run --gpus all google/deepvariant:1.6.1-gpu python3 -c ""import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))"". ```. When I run this on my machine, I see:. ```. [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]. ```. Given that your is_gpu_available is True, I think it'll also list something. Just want to double check here. If there's any other information that you can provide, in order for us to reproduce on our side, please let us know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/820
https://github.com/google/deepvariant/issues/820:513,energy efficiency,GPU,GPU,513,"Hi @kunmonster ,. if I understand correctly, we haven't been able to reproduce the issue on our side, therefore it has been difficult for us to help. I know you've already run `tensorflow.test.is_gpu_available()`. can you also try this and see what you see? ```. sudo docker run --gpus all google/deepvariant:1.6.1-gpu python3 -c ""import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))"". ```. When I run this on my machine, I see:. ```. [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]. ```. Given that your is_gpu_available is True, I think it'll also list something. Just want to double check here. If there's any other information that you can provide, in order for us to reproduce on our side, please let us know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/820
https://github.com/google/deepvariant/issues/820:281,performance,gpu,gpus,281,"Hi @kunmonster ,. if I understand correctly, we haven't been able to reproduce the issue on our side, therefore it has been difficult for us to help. I know you've already run `tensorflow.test.is_gpu_available()`. can you also try this and see what you see? ```. sudo docker run --gpus all google/deepvariant:1.6.1-gpu python3 -c ""import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))"". ```. When I run this on my machine, I see:. ```. [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]. ```. Given that your is_gpu_available is True, I think it'll also list something. Just want to double check here. If there's any other information that you can provide, in order for us to reproduce on our side, please let us know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/820
https://github.com/google/deepvariant/issues/820:315,performance,gpu,gpu,315,"Hi @kunmonster ,. if I understand correctly, we haven't been able to reproduce the issue on our side, therefore it has been difficult for us to help. I know you've already run `tensorflow.test.is_gpu_available()`. can you also try this and see what you see? ```. sudo docker run --gpus all google/deepvariant:1.6.1-gpu python3 -c ""import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))"". ```. When I run this on my machine, I see:. ```. [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]. ```. Given that your is_gpu_available is True, I think it'll also list something. Just want to double check here. If there's any other information that you can provide, in order for us to reproduce on our side, please let us know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/820
https://github.com/google/deepvariant/issues/820:395,performance,GPU,GPU,395,"Hi @kunmonster ,. if I understand correctly, we haven't been able to reproduce the issue on our side, therefore it has been difficult for us to help. I know you've already run `tensorflow.test.is_gpu_available()`. can you also try this and see what you see? ```. sudo docker run --gpus all google/deepvariant:1.6.1-gpu python3 -c ""import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))"". ```. When I run this on my machine, I see:. ```. [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]. ```. Given that your is_gpu_available is True, I think it'll also list something. Just want to double check here. If there's any other information that you can provide, in order for us to reproduce on our side, please let us know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/820
https://github.com/google/deepvariant/issues/820:492,performance,GPU,GPU,492,"Hi @kunmonster ,. if I understand correctly, we haven't been able to reproduce the issue on our side, therefore it has been difficult for us to help. I know you've already run `tensorflow.test.is_gpu_available()`. can you also try this and see what you see? ```. sudo docker run --gpus all google/deepvariant:1.6.1-gpu python3 -c ""import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))"". ```. When I run this on my machine, I see:. ```. [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]. ```. Given that your is_gpu_available is True, I think it'll also list something. Just want to double check here. If there's any other information that you can provide, in order for us to reproduce on our side, please let us know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/820
https://github.com/google/deepvariant/issues/820:513,performance,GPU,GPU,513,"Hi @kunmonster ,. if I understand correctly, we haven't been able to reproduce the issue on our side, therefore it has been difficult for us to help. I know you've already run `tensorflow.test.is_gpu_available()`. can you also try this and see what you see? ```. sudo docker run --gpus all google/deepvariant:1.6.1-gpu python3 -c ""import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))"". ```. When I run this on my machine, I see:. ```. [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]. ```. Given that your is_gpu_available is True, I think it'll also list something. Just want to double check here. If there's any other information that you can provide, in order for us to reproduce on our side, please let us know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/820
https://github.com/google/deepvariant/issues/820:188,safety,test,test,188,"Hi @kunmonster ,. if I understand correctly, we haven't been able to reproduce the issue on our side, therefore it has been difficult for us to help. I know you've already run `tensorflow.test.is_gpu_available()`. can you also try this and see what you see? ```. sudo docker run --gpus all google/deepvariant:1.6.1-gpu python3 -c ""import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))"". ```. When I run this on my machine, I see:. ```. [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]. ```. Given that your is_gpu_available is True, I think it'll also list something. Just want to double check here. If there's any other information that you can provide, in order for us to reproduce on our side, please let us know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/820
https://github.com/google/deepvariant/issues/820:23,testability,understand,understand,23,"Hi @kunmonster ,. if I understand correctly, we haven't been able to reproduce the issue on our side, therefore it has been difficult for us to help. I know you've already run `tensorflow.test.is_gpu_available()`. can you also try this and see what you see? ```. sudo docker run --gpus all google/deepvariant:1.6.1-gpu python3 -c ""import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))"". ```. When I run this on my machine, I see:. ```. [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]. ```. Given that your is_gpu_available is True, I think it'll also list something. Just want to double check here. If there's any other information that you can provide, in order for us to reproduce on our side, please let us know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/820
https://github.com/google/deepvariant/issues/820:188,testability,test,test,188,"Hi @kunmonster ,. if I understand correctly, we haven't been able to reproduce the issue on our side, therefore it has been difficult for us to help. I know you've already run `tensorflow.test.is_gpu_available()`. can you also try this and see what you see? ```. sudo docker run --gpus all google/deepvariant:1.6.1-gpu python3 -c ""import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))"". ```. When I run this on my machine, I see:. ```. [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]. ```. Given that your is_gpu_available is True, I think it'll also list something. Just want to double check here. If there's any other information that you can provide, in order for us to reproduce on our side, please let us know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/820
https://github.com/google/deepvariant/issues/820:144,usability,help,help,144,"Hi @kunmonster ,. if I understand correctly, we haven't been able to reproduce the issue on our side, therefore it has been difficult for us to help. I know you've already run `tensorflow.test.is_gpu_available()`. can you also try this and see what you see? ```. sudo docker run --gpus all google/deepvariant:1.6.1-gpu python3 -c ""import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))"". ```. When I run this on my machine, I see:. ```. [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]. ```. Given that your is_gpu_available is True, I think it'll also list something. Just want to double check here. If there's any other information that you can provide, in order for us to reproduce on our side, please let us know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/820
https://github.com/google/deepvariant/issues/820:93,energy efficiency,gpu,gpus,93,"Thanks @pichuan,. I think this problem may be caused by the hpc what i am using,actually the gpus of this hpc are not from nvidia,instead they are made for deeplearning specifically in specific frame, and i can't run docker directly on the hpc. It doesn't matter, i will find another computer with gpu from nvidia to run this . Thx.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/820
https://github.com/google/deepvariant/issues/820:298,energy efficiency,gpu,gpu,298,"Thanks @pichuan,. I think this problem may be caused by the hpc what i am using,actually the gpus of this hpc are not from nvidia,instead they are made for deeplearning specifically in specific frame, and i can't run docker directly on the hpc. It doesn't matter, i will find another computer with gpu from nvidia to run this . Thx.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/820
https://github.com/google/deepvariant/issues/820:169,interoperability,specif,specifically,169,"Thanks @pichuan,. I think this problem may be caused by the hpc what i am using,actually the gpus of this hpc are not from nvidia,instead they are made for deeplearning specifically in specific frame, and i can't run docker directly on the hpc. It doesn't matter, i will find another computer with gpu from nvidia to run this . Thx.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/820
https://github.com/google/deepvariant/issues/820:185,interoperability,specif,specific,185,"Thanks @pichuan,. I think this problem may be caused by the hpc what i am using,actually the gpus of this hpc are not from nvidia,instead they are made for deeplearning specifically in specific frame, and i can't run docker directly on the hpc. It doesn't matter, i will find another computer with gpu from nvidia to run this . Thx.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/820
https://github.com/google/deepvariant/issues/820:93,performance,gpu,gpus,93,"Thanks @pichuan,. I think this problem may be caused by the hpc what i am using,actually the gpus of this hpc are not from nvidia,instead they are made for deeplearning specifically in specific frame, and i can't run docker directly on the hpc. It doesn't matter, i will find another computer with gpu from nvidia to run this . Thx.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/820
https://github.com/google/deepvariant/issues/820:298,performance,gpu,gpu,298,"Thanks @pichuan,. I think this problem may be caused by the hpc what i am using,actually the gpus of this hpc are not from nvidia,instead they are made for deeplearning specifically in specific frame, and i can't run docker directly on the hpc. It doesn't matter, i will find another computer with gpu from nvidia to run this . Thx.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/820
https://github.com/google/deepvariant/issues/820:248,reliability,doe,doesn,248,"Thanks @pichuan,. I think this problem may be caused by the hpc what i am using,actually the gpus of this hpc are not from nvidia,instead they are made for deeplearning specifically in specific frame, and i can't run docker directly on the hpc. It doesn't matter, i will find another computer with gpu from nvidia to run this . Thx.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/820
https://github.com/google/deepvariant/issues/820:205,availability,operat,operations,205,"```. I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2024-08-15 14:02:47.618984: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64. 2024-08-15 14:02:47.619048: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. 2024-08-15 14:02:50.434353: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NOT_FOUND: named symbol not found. []. ```I am encountering errors while testing deepvariants calling with gpu. It seems that some libraries for cuda are missing causing it to only work with cpu",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/820
https://github.com/google/deepvariant/issues/820:251,availability,operat,operations,251,"```. I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2024-08-15 14:02:47.618984: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64. 2024-08-15 14:02:47.619048: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. 2024-08-15 14:02:50.434353: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NOT_FOUND: named symbol not found. []. ```I am encountering errors while testing deepvariants calling with gpu. It seems that some libraries for cuda are missing causing it to only work with cpu",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/820
https://github.com/google/deepvariant/issues/820:1088,availability,error,errors,1088,"```. I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2024-08-15 14:02:47.618984: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64. 2024-08-15 14:02:47.619048: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. 2024-08-15 14:02:50.434353: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NOT_FOUND: named symbol not found. []. ```I am encountering errors while testing deepvariants calling with gpu. It seems that some libraries for cuda are missing causing it to only work with cpu",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/820
https://github.com/google/deepvariant/issues/820:879,deployability,instal,installed,879,"```. I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2024-08-15 14:02:47.618984: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64. 2024-08-15 14:02:47.619048: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. 2024-08-15 14:02:50.434353: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NOT_FOUND: named symbol not found. []. ```I am encountering errors while testing deepvariants calling with gpu. It seems that some libraries for cuda are missing causing it to only work with cpu",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/820
https://github.com/google/deepvariant/issues/820:994,deployability,fail,failed,994,"```. I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2024-08-15 14:02:47.618984: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64. 2024-08-15 14:02:47.619048: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. 2024-08-15 14:02:50.434353: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NOT_FOUND: named symbol not found. []. ```I am encountering errors while testing deepvariants calling with gpu. It seems that some libraries for cuda are missing causing it to only work with cpu",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/820
https://github.com/google/deepvariant/issues/820:18,energy efficiency,core,core,18,"```. I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2024-08-15 14:02:47.618984: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64. 2024-08-15 14:02:47.619048: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. 2024-08-15 14:02:50.434353: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NOT_FOUND: named symbol not found. []. ```I am encountering errors while testing deepvariants calling with gpu. It seems that some libraries for cuda are missing causing it to only work with cpu",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/820
https://github.com/google/deepvariant/issues/820:84,energy efficiency,optim,optimized,84,"```. I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2024-08-15 14:02:47.618984: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64. 2024-08-15 14:02:47.619048: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. 2024-08-15 14:02:50.434353: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NOT_FOUND: named symbol not found. []. ```I am encountering errors while testing deepvariants calling with gpu. It seems that some libraries for cuda are missing causing it to only work with cpu",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/820
https://github.com/google/deepvariant/issues/820:164,energy efficiency,CPU,CPU,164,"```. I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2024-08-15 14:02:47.618984: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64. 2024-08-15 14:02:47.619048: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. 2024-08-15 14:02:50.434353: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NOT_FOUND: named symbol not found. []. ```I am encountering errors while testing deepvariants calling with gpu. It seems that some libraries for cuda are missing causing it to only work with cpu",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/820
https://github.com/google/deepvariant/issues/820:434,energy efficiency,load,load,434,"```. I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2024-08-15 14:02:47.618984: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64. 2024-08-15 14:02:47.619048: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. 2024-08-15 14:02:50.434353: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NOT_FOUND: named symbol not found. []. ```I am encountering errors while testing deepvariants calling with gpu. It seems that some libraries for cuda are missing causing it to only work with cpu",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/820
https://github.com/google/deepvariant/issues/820:801,energy efficiency,GPU,GPU,801,"```. I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2024-08-15 14:02:47.618984: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64. 2024-08-15 14:02:47.619048: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. 2024-08-15 14:02:50.434353: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NOT_FOUND: named symbol not found. []. ```I am encountering errors while testing deepvariants calling with gpu. It seems that some libraries for cuda are missing causing it to only work with cpu",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/820
https://github.com/google/deepvariant/issues/820:1135,energy efficiency,gpu,gpu,1135,"```. I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2024-08-15 14:02:47.618984: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64. 2024-08-15 14:02:47.619048: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. 2024-08-15 14:02:50.434353: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NOT_FOUND: named symbol not found. []. ```I am encountering errors while testing deepvariants calling with gpu. It seems that some libraries for cuda are missing causing it to only work with cpu",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/820
https://github.com/google/deepvariant/issues/820:1219,energy efficiency,cpu,cpu,1219,"```. I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2024-08-15 14:02:47.618984: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64. 2024-08-15 14:02:47.619048: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. 2024-08-15 14:02:50.434353: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NOT_FOUND: named symbol not found. []. ```I am encountering errors while testing deepvariants calling with gpu. It seems that some libraries for cuda are missing causing it to only work with cpu",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/820
https://github.com/google/deepvariant/issues/820:23,interoperability,platform,platform,23,"```. I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2024-08-15 14:02:47.618984: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64. 2024-08-15 14:02:47.619048: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. 2024-08-15 14:02:50.434353: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NOT_FOUND: named symbol not found. []. ```I am encountering errors while testing deepvariants calling with gpu. It seems that some libraries for cuda are missing causing it to only work with cpu",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/820
https://github.com/google/deepvariant/issues/820:389,interoperability,platform,platform,389,"```. I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2024-08-15 14:02:47.618984: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64. 2024-08-15 14:02:47.619048: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. 2024-08-15 14:02:50.434353: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NOT_FOUND: named symbol not found. []. ```I am encountering errors while testing deepvariants calling with gpu. It seems that some libraries for cuda are missing causing it to only work with cpu",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/820
https://github.com/google/deepvariant/issues/820:519,interoperability,share,shared,519,"```. I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2024-08-15 14:02:47.618984: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64. 2024-08-15 14:02:47.619048: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. 2024-08-15 14:02:50.434353: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NOT_FOUND: named symbol not found. []. ```I am encountering errors while testing deepvariants calling with gpu. It seems that some libraries for cuda are missing causing it to only work with cpu",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/820
https://github.com/google/deepvariant/issues/820:84,performance,optimiz,optimized,84,"```. I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2024-08-15 14:02:47.618984: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64. 2024-08-15 14:02:47.619048: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. 2024-08-15 14:02:50.434353: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NOT_FOUND: named symbol not found. []. ```I am encountering errors while testing deepvariants calling with gpu. It seems that some libraries for cuda are missing causing it to only work with cpu",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/820
https://github.com/google/deepvariant/issues/820:118,performance,Network,Network,118,"```. I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2024-08-15 14:02:47.618984: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64. 2024-08-15 14:02:47.619048: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. 2024-08-15 14:02:50.434353: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NOT_FOUND: named symbol not found. []. ```I am encountering errors while testing deepvariants calling with gpu. It seems that some libraries for cuda are missing causing it to only work with cpu",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/820
https://github.com/google/deepvariant/issues/820:164,performance,CPU,CPU,164,"```. I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2024-08-15 14:02:47.618984: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64. 2024-08-15 14:02:47.619048: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. 2024-08-15 14:02:50.434353: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NOT_FOUND: named symbol not found. []. ```I am encountering errors while testing deepvariants calling with gpu. It seems that some libraries for cuda are missing causing it to only work with cpu",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/820
https://github.com/google/deepvariant/issues/820:184,performance,perform,performance-critical,184,"```. I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2024-08-15 14:02:47.618984: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64. 2024-08-15 14:02:47.619048: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. 2024-08-15 14:02:50.434353: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NOT_FOUND: named symbol not found. []. ```I am encountering errors while testing deepvariants calling with gpu. It seems that some libraries for cuda are missing causing it to only work with cpu",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/820
https://github.com/google/deepvariant/issues/820:434,performance,load,load,434,"```. I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2024-08-15 14:02:47.618984: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64. 2024-08-15 14:02:47.619048: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. 2024-08-15 14:02:50.434353: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NOT_FOUND: named symbol not found. []. ```I am encountering errors while testing deepvariants calling with gpu. It seems that some libraries for cuda are missing causing it to only work with cpu",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/820
https://github.com/google/deepvariant/issues/820:801,performance,GPU,GPU,801,"```. I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2024-08-15 14:02:47.618984: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64. 2024-08-15 14:02:47.619048: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. 2024-08-15 14:02:50.434353: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NOT_FOUND: named symbol not found. []. ```I am encountering errors while testing deepvariants calling with gpu. It seems that some libraries for cuda are missing causing it to only work with cpu",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/820
https://github.com/google/deepvariant/issues/820:1088,performance,error,errors,1088,"```. I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2024-08-15 14:02:47.618984: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64. 2024-08-15 14:02:47.619048: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. 2024-08-15 14:02:50.434353: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NOT_FOUND: named symbol not found. []. ```I am encountering errors while testing deepvariants calling with gpu. It seems that some libraries for cuda are missing causing it to only work with cpu",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/820
https://github.com/google/deepvariant/issues/820:1135,performance,gpu,gpu,1135,"```. I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2024-08-15 14:02:47.618984: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64. 2024-08-15 14:02:47.619048: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. 2024-08-15 14:02:50.434353: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NOT_FOUND: named symbol not found. []. ```I am encountering errors while testing deepvariants calling with gpu. It seems that some libraries for cuda are missing causing it to only work with cpu",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/820
https://github.com/google/deepvariant/issues/820:1219,performance,cpu,cpu,1219,"```. I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2024-08-15 14:02:47.618984: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64. 2024-08-15 14:02:47.619048: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. 2024-08-15 14:02:50.434353: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NOT_FOUND: named symbol not found. []. ```I am encountering errors while testing deepvariants calling with gpu. It seems that some libraries for cuda are missing causing it to only work with cpu",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/820
https://github.com/google/deepvariant/issues/820:994,reliability,fail,failed,994,"```. I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2024-08-15 14:02:47.618984: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64. 2024-08-15 14:02:47.619048: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. 2024-08-15 14:02:50.434353: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NOT_FOUND: named symbol not found. []. ```I am encountering errors while testing deepvariants calling with gpu. It seems that some libraries for cuda are missing causing it to only work with cpu",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/820
https://github.com/google/deepvariant/issues/820:1088,safety,error,errors,1088,"```. I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2024-08-15 14:02:47.618984: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64. 2024-08-15 14:02:47.619048: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. 2024-08-15 14:02:50.434353: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NOT_FOUND: named symbol not found. []. ```I am encountering errors while testing deepvariants calling with gpu. It seems that some libraries for cuda are missing causing it to only work with cpu",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/820
https://github.com/google/deepvariant/issues/820:1101,safety,test,testing,1101,"```. I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2024-08-15 14:02:47.618984: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64. 2024-08-15 14:02:47.619048: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. 2024-08-15 14:02:50.434353: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NOT_FOUND: named symbol not found. []. ```I am encountering errors while testing deepvariants calling with gpu. It seems that some libraries for cuda are missing causing it to only work with cpu",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/820
https://github.com/google/deepvariant/issues/820:118,security,Network,Network,118,"```. I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2024-08-15 14:02:47.618984: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64. 2024-08-15 14:02:47.619048: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. 2024-08-15 14:02:50.434353: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NOT_FOUND: named symbol not found. []. ```I am encountering errors while testing deepvariants calling with gpu. It seems that some libraries for cuda are missing causing it to only work with cpu",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/820
https://github.com/google/deepvariant/issues/820:1101,testability,test,testing,1101,"```. I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2024-08-15 14:02:47.618984: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64. 2024-08-15 14:02:47.619048: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. 2024-08-15 14:02:50.434353: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NOT_FOUND: named symbol not found. []. ```I am encountering errors while testing deepvariants calling with gpu. It seems that some libraries for cuda are missing causing it to only work with cpu",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/820
https://github.com/google/deepvariant/issues/820:184,usability,perform,performance-critical,184,"```. I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2024-08-15 14:02:47.618984: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64. 2024-08-15 14:02:47.619048: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. 2024-08-15 14:02:50.434353: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NOT_FOUND: named symbol not found. []. ```I am encountering errors while testing deepvariants calling with gpu. It seems that some libraries for cuda are missing causing it to only work with cpu",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/820
https://github.com/google/deepvariant/issues/820:1088,usability,error,errors,1088,"```. I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2024-08-15 14:02:47.618984: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64. 2024-08-15 14:02:47.619048: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. 2024-08-15 14:02:50.434353: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NOT_FOUND: named symbol not found. []. ```I am encountering errors while testing deepvariants calling with gpu. It seems that some libraries for cuda are missing causing it to only work with cpu",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/820
https://github.com/google/deepvariant/issues/821:143,integrability,sub,submissions,143,"Hello,. The ONT R10.4 training data came from the HPRC project. Specifically this bucket: https://s3-us-west-2.amazonaws.com/human-pangenomics/submissions/3b9be4f8-a269-4f89-ab9c-a0a0b0a10af6--UCSC_HG002_R1041_nanopore/HG002_Sheared_R1041/. Hope this helps. Please close the issue if it answers your query.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/821
https://github.com/google/deepvariant/issues/821:64,interoperability,Specif,Specifically,64,"Hello,. The ONT R10.4 training data came from the HPRC project. Specifically this bucket: https://s3-us-west-2.amazonaws.com/human-pangenomics/submissions/3b9be4f8-a269-4f89-ab9c-a0a0b0a10af6--UCSC_HG002_R1041_nanopore/HG002_Sheared_R1041/. Hope this helps. Please close the issue if it answers your query.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/821
https://github.com/google/deepvariant/issues/821:251,usability,help,helps,251,"Hello,. The ONT R10.4 training data came from the HPRC project. Specifically this bucket: https://s3-us-west-2.amazonaws.com/human-pangenomics/submissions/3b9be4f8-a269-4f89-ab9c-a0a0b0a10af6--UCSC_HG002_R1041_nanopore/HG002_Sheared_R1041/. Hope this helps. Please close the issue if it answers your query.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/821
https://github.com/google/deepvariant/issues/821:265,usability,close,close,265,"Hello,. The ONT R10.4 training data came from the HPRC project. Specifically this bucket: https://s3-us-west-2.amazonaws.com/human-pangenomics/submissions/3b9be4f8-a269-4f89-ab9c-a0a0b0a10af6--UCSC_HG002_R1041_nanopore/HG002_Sheared_R1041/. Hope this helps. Please close the issue if it answers your query.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/821
https://github.com/google/deepvariant/issues/821:21,availability,down,download,21,"The previous one was download link, if you want to see it on the web:. https://s3-us-west-2.amazonaws.com/human-pangenomics/index.html?prefix=submissions/3b9be4f8-a269-4f89-ab9c-a0a0b0a10af6--UCSC_HG002_R1041_nanopore/HG002_Sheared_R1041/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/821
https://github.com/google/deepvariant/issues/821:142,integrability,sub,submissions,142,"The previous one was download link, if you want to see it on the web:. https://s3-us-west-2.amazonaws.com/human-pangenomics/index.html?prefix=submissions/3b9be4f8-a269-4f89-ab9c-a0a0b0a10af6--UCSC_HG002_R1041_nanopore/HG002_Sheared_R1041/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/821
https://github.com/google/deepvariant/issues/823:235,energy efficiency,model,model-case-study,235,"Hi @eaooms ,. Starting in DeepVariant v1.4.0, wehave our read haplotagging built in. To use the flag you mentioned, please use `BIN_VERSION=1.3.0`. You can read:. https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/823
https://github.com/google/deepvariant/issues/823:228,modifiability,pac,pacbio-model-case-study,228,"Hi @eaooms ,. Starting in DeepVariant v1.4.0, wehave our read haplotagging built in. To use the flag you mentioned, please use `BIN_VERSION=1.3.0`. You can read:. https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/823
https://github.com/google/deepvariant/issues/823:235,security,model,model-case-study,235,"Hi @eaooms ,. Starting in DeepVariant v1.4.0, wehave our read haplotagging built in. To use the flag you mentioned, please use `BIN_VERSION=1.3.0`. You can read:. https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/823
https://github.com/google/deepvariant/issues/823:5,usability,close,close,5,I'll close this issue now. Please feel free to open as needed.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/823
https://github.com/google/deepvariant/issues/823:65,reliability,doe,does,65,Thank u for the quick respond. Where can i find what Deepvariant does with the phasing information in the bam file? Because it is still a bit unclear to me what Deepvariant does with the added HP channel that u get from Whatshap haplotag mentioned in the case study ( https://google.github.io/deepvariant/posts/2021-02-08-the-haplotype-channel/ ). In what way does it influence the variant caller in making its decisions for picking the found variants?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/823
https://github.com/google/deepvariant/issues/823:173,reliability,doe,does,173,Thank u for the quick respond. Where can i find what Deepvariant does with the phasing information in the bam file? Because it is still a bit unclear to me what Deepvariant does with the added HP channel that u get from Whatshap haplotag mentioned in the case study ( https://google.github.io/deepvariant/posts/2021-02-08-the-haplotype-channel/ ). In what way does it influence the variant caller in making its decisions for picking the found variants?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/823
https://github.com/google/deepvariant/issues/823:360,reliability,doe,does,360,Thank u for the quick respond. Where can i find what Deepvariant does with the phasing information in the bam file? Because it is still a bit unclear to me what Deepvariant does with the added HP channel that u get from Whatshap haplotag mentioned in the case study ( https://google.github.io/deepvariant/posts/2021-02-08-the-haplotype-channel/ ). In what way does it influence the variant caller in making its decisions for picking the found variants?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/823
https://github.com/google/deepvariant/issues/823:141,availability,avail,available,141,"Hi @eaooms , . DeepVariant adds an additional channel to represent the HP information. If you have some intermediate output of make_examples available, you can try using https://github.com/google/deepvariant/blob/r1.6.1/docs/show-examples.md to visualize them. In addition to the blog post, this preprint might provide the information that you need: https://doi.org/10.1101/2023.09.07.556731",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/823
https://github.com/google/deepvariant/issues/823:104,modifiability,interm,intermediate,104,"Hi @eaooms , . DeepVariant adds an additional channel to represent the HP information. If you have some intermediate output of make_examples available, you can try using https://github.com/google/deepvariant/blob/r1.6.1/docs/show-examples.md to visualize them. In addition to the blog post, this preprint might provide the information that you need: https://doi.org/10.1101/2023.09.07.556731",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/823
https://github.com/google/deepvariant/issues/823:141,reliability,availab,available,141,"Hi @eaooms , . DeepVariant adds an additional channel to represent the HP information. If you have some intermediate output of make_examples available, you can try using https://github.com/google/deepvariant/blob/r1.6.1/docs/show-examples.md to visualize them. In addition to the blog post, this preprint might provide the information that you need: https://doi.org/10.1101/2023.09.07.556731",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/823
https://github.com/google/deepvariant/issues/823:141,safety,avail,available,141,"Hi @eaooms , . DeepVariant adds an additional channel to represent the HP information. If you have some intermediate output of make_examples available, you can try using https://github.com/google/deepvariant/blob/r1.6.1/docs/show-examples.md to visualize them. In addition to the blog post, this preprint might provide the information that you need: https://doi.org/10.1101/2023.09.07.556731",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/823
https://github.com/google/deepvariant/issues/823:141,security,availab,available,141,"Hi @eaooms , . DeepVariant adds an additional channel to represent the HP information. If you have some intermediate output of make_examples available, you can try using https://github.com/google/deepvariant/blob/r1.6.1/docs/show-examples.md to visualize them. In addition to the blog post, this preprint might provide the information that you need: https://doi.org/10.1101/2023.09.07.556731",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/823
https://github.com/google/deepvariant/issues/823:245,usability,visual,visualize,245,"Hi @eaooms , . DeepVariant adds an additional channel to represent the HP information. If you have some intermediate output of make_examples available, you can try using https://github.com/google/deepvariant/blob/r1.6.1/docs/show-examples.md to visualize them. In addition to the blog post, this preprint might provide the information that you need: https://doi.org/10.1101/2023.09.07.556731",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/823
https://github.com/google/deepvariant/issues/824:545,interoperability,share,share,545,"Hi @asalimih ,. For WES, DeepVariant runs a realignment step. You can read this section about how realignment works: https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md#what-is-the-realigner-and-how-does-it-work. To understand your cases better, it'll be useful to generate the realigned reads in the region, which might explain why DeepVariant makes the call differently. You can take a look at https://github.com/google/deepvariant/issues/691#issuecomment-2014404465 for a more detailed example. Once you generate an example, please share the realigned reads in an IGV and we can check what it looks like? Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/824
https://github.com/google/deepvariant/issues/824:209,reliability,doe,does-it-work,209,"Hi @asalimih ,. For WES, DeepVariant runs a realignment step. You can read this section about how realignment works: https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md#what-is-the-realigner-and-how-does-it-work. To understand your cases better, it'll be useful to generate the realigned reads in the region, which might explain why DeepVariant makes the call differently. You can take a look at https://github.com/google/deepvariant/issues/691#issuecomment-2014404465 for a more detailed example. Once you generate an example, please share the realigned reads in an IGV and we can check what it looks like? Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/824
https://github.com/google/deepvariant/issues/824:226,testability,understand,understand,226,"Hi @asalimih ,. For WES, DeepVariant runs a realignment step. You can read this section about how realignment works: https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md#what-is-the-realigner-and-how-does-it-work. To understand your cases better, it'll be useful to generate the realigned reads in the region, which might explain why DeepVariant makes the call differently. You can take a look at https://github.com/google/deepvariant/issues/691#issuecomment-2014404465 for a more detailed example. Once you generate an example, please share the realigned reads in an IGV and we can check what it looks like? Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/824
https://github.com/google/deepvariant/issues/824:588,interoperability,share,share,588,"> Hi @asalimih , For WES, DeepVariant runs a realignment step. > . > You can read this section about how realignment works: https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md#what-is-the-realigner-and-how-does-it-work. > . > To understand your cases better, it'll be useful to generate the realigned reads in the region, which might explain why DeepVariant makes the call differently. > . > You can take a look at [#691 (comment)](https://github.com/google/deepvariant/issues/691#issuecomment-2014404465) for a more detailed example. > . > Once you generate an example, please share the realigned reads in an IGV and we can check what it looks like? Thank you! @pichuan . Thanks for the reply. here is the realigned bam in IGV for chr1-1668449-A-G:. ![realigned-chr1-1668449-A-G](https://github.com/google/deepvariant/assets/10653739/f0ec5545-43f8-4d79-bea9-bfccb2576569). The read counts matches what the AD field says in the vcf. however the GT value is 1/1 ! The weird thing is another variant just 76 bp to the left has the correct genotype in the vcf file:. ```. chr1 1668373 . C T 6.4 PASS . GT:GQ:DP:AD:VAF:PL 0/1:4:139:84,55:0.395683:4,0,5. ```. ![nearby_chr1](https://github.com/google/deepvariant/assets/10653739/72ad38bf-091b-4f4f-a72b-cdfd64411526).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/824
https://github.com/google/deepvariant/issues/824:216,reliability,doe,does-it-work,216,"> Hi @asalimih , For WES, DeepVariant runs a realignment step. > . > You can read this section about how realignment works: https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md#what-is-the-realigner-and-how-does-it-work. > . > To understand your cases better, it'll be useful to generate the realigned reads in the region, which might explain why DeepVariant makes the call differently. > . > You can take a look at [#691 (comment)](https://github.com/google/deepvariant/issues/691#issuecomment-2014404465) for a more detailed example. > . > Once you generate an example, please share the realigned reads in an IGV and we can check what it looks like? Thank you! @pichuan . Thanks for the reply. here is the realigned bam in IGV for chr1-1668449-A-G:. ![realigned-chr1-1668449-A-G](https://github.com/google/deepvariant/assets/10653739/f0ec5545-43f8-4d79-bea9-bfccb2576569). The read counts matches what the AD field says in the vcf. however the GT value is 1/1 ! The weird thing is another variant just 76 bp to the left has the correct genotype in the vcf file:. ```. chr1 1668373 . C T 6.4 PASS . GT:GQ:DP:AD:VAF:PL 0/1:4:139:84,55:0.395683:4,0,5. ```. ![nearby_chr1](https://github.com/google/deepvariant/assets/10653739/72ad38bf-091b-4f4f-a72b-cdfd64411526).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/824
https://github.com/google/deepvariant/issues/824:239,testability,understand,understand,239,"> Hi @asalimih , For WES, DeepVariant runs a realignment step. > . > You can read this section about how realignment works: https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md#what-is-the-realigner-and-how-does-it-work. > . > To understand your cases better, it'll be useful to generate the realigned reads in the region, which might explain why DeepVariant makes the call differently. > . > You can take a look at [#691 (comment)](https://github.com/google/deepvariant/issues/691#issuecomment-2014404465) for a more detailed example. > . > Once you generate an example, please share the realigned reads in an IGV and we can check what it looks like? Thank you! @pichuan . Thanks for the reply. here is the realigned bam in IGV for chr1-1668449-A-G:. ![realigned-chr1-1668449-A-G](https://github.com/google/deepvariant/assets/10653739/f0ec5545-43f8-4d79-bea9-bfccb2576569). The read counts matches what the AD field says in the vcf. however the GT value is 1/1 ! The weird thing is another variant just 76 bp to the left has the correct genotype in the vcf file:. ```. chr1 1668373 . C T 6.4 PASS . GT:GQ:DP:AD:VAF:PL 0/1:4:139:84,55:0.395683:4,0,5. ```. ![nearby_chr1](https://github.com/google/deepvariant/assets/10653739/72ad38bf-091b-4f4f-a72b-cdfd64411526).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/824
https://github.com/google/deepvariant/issues/824:24,availability,down,downsample,24,"Hi @asalimih ,. can you downsample your bam for this specific variant and run it again? ```bash. samtools view -s 0.5 -b -@16 YOU_BAM.bam > YOUR_BAM.downsampled.bam. ```. And run it one more time to see if this specifically has a different genotype?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/824
https://github.com/google/deepvariant/issues/824:149,availability,down,downsampled,149,"Hi @asalimih ,. can you downsample your bam for this specific variant and run it again? ```bash. samtools view -s 0.5 -b -@16 YOU_BAM.bam > YOUR_BAM.downsampled.bam. ```. And run it one more time to see if this specifically has a different genotype?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/824
https://github.com/google/deepvariant/issues/824:53,interoperability,specif,specific,53,"Hi @asalimih ,. can you downsample your bam for this specific variant and run it again? ```bash. samtools view -s 0.5 -b -@16 YOU_BAM.bam > YOUR_BAM.downsampled.bam. ```. And run it one more time to see if this specifically has a different genotype?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/824
https://github.com/google/deepvariant/issues/824:211,interoperability,specif,specifically,211,"Hi @asalimih ,. can you downsample your bam for this specific variant and run it again? ```bash. samtools view -s 0.5 -b -@16 YOU_BAM.bam > YOUR_BAM.downsampled.bam. ```. And run it one more time to see if this specifically has a different genotype?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/824
https://github.com/google/deepvariant/issues/824:191,performance,time,time,191,"Hi @asalimih ,. can you downsample your bam for this specific variant and run it again? ```bash. samtools view -s 0.5 -b -@16 YOU_BAM.bam > YOUR_BAM.downsampled.bam. ```. And run it one more time to see if this specifically has a different genotype?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/824
https://github.com/google/deepvariant/issues/824:32,availability,down,downsample,32,"> Hi @asalimih ,. > . > can you downsample your bam for this specific variant and run it again? > . > ```shell. > samtools view -s 0.5 -b -@16 YOU_BAM.bam > YOUR_BAM.downsampled.bam. > ```. > . > And run it one more time to see if this specifically has a different genotype? sure, no change in GT (using v1.6.1):. ```. chr1 1668449 . A G 42.1 PASS . GT:GQ:DP:AD:VAF:PL 1/1:27:77:36,41:0.532468:42,26,0. ```. I tried with `samtools view -s 0.25` too:. ```. chr1 1668449 . A G 35.5 PASS . GT:GQ:DP:AD:VAF:PL 1/1:16:44:22,22:0.5:35,16,0. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/824
https://github.com/google/deepvariant/issues/824:166,availability,down,downsampled,166,"> Hi @asalimih ,. > . > can you downsample your bam for this specific variant and run it again? > . > ```shell. > samtools view -s 0.5 -b -@16 YOU_BAM.bam > YOUR_BAM.downsampled.bam. > ```. > . > And run it one more time to see if this specifically has a different genotype? sure, no change in GT (using v1.6.1):. ```. chr1 1668449 . A G 42.1 PASS . GT:GQ:DP:AD:VAF:PL 1/1:27:77:36,41:0.532468:42,26,0. ```. I tried with `samtools view -s 0.25` too:. ```. chr1 1668449 . A G 35.5 PASS . GT:GQ:DP:AD:VAF:PL 1/1:16:44:22,22:0.5:35,16,0. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/824
https://github.com/google/deepvariant/issues/824:61,interoperability,specif,specific,61,"> Hi @asalimih ,. > . > can you downsample your bam for this specific variant and run it again? > . > ```shell. > samtools view -s 0.5 -b -@16 YOU_BAM.bam > YOUR_BAM.downsampled.bam. > ```. > . > And run it one more time to see if this specifically has a different genotype? sure, no change in GT (using v1.6.1):. ```. chr1 1668449 . A G 42.1 PASS . GT:GQ:DP:AD:VAF:PL 1/1:27:77:36,41:0.532468:42,26,0. ```. I tried with `samtools view -s 0.25` too:. ```. chr1 1668449 . A G 35.5 PASS . GT:GQ:DP:AD:VAF:PL 1/1:16:44:22,22:0.5:35,16,0. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/824
https://github.com/google/deepvariant/issues/824:236,interoperability,specif,specifically,236,"> Hi @asalimih ,. > . > can you downsample your bam for this specific variant and run it again? > . > ```shell. > samtools view -s 0.5 -b -@16 YOU_BAM.bam > YOUR_BAM.downsampled.bam. > ```. > . > And run it one more time to see if this specifically has a different genotype? sure, no change in GT (using v1.6.1):. ```. chr1 1668449 . A G 42.1 PASS . GT:GQ:DP:AD:VAF:PL 1/1:27:77:36,41:0.532468:42,26,0. ```. I tried with `samtools view -s 0.25` too:. ```. chr1 1668449 . A G 35.5 PASS . GT:GQ:DP:AD:VAF:PL 1/1:16:44:22,22:0.5:35,16,0. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/824
https://github.com/google/deepvariant/issues/824:216,performance,time,time,216,"> Hi @asalimih ,. > . > can you downsample your bam for this specific variant and run it again? > . > ```shell. > samtools view -s 0.5 -b -@16 YOU_BAM.bam > YOUR_BAM.downsampled.bam. > ```. > . > And run it one more time to see if this specifically has a different genotype? sure, no change in GT (using v1.6.1):. ```. chr1 1668449 . A G 42.1 PASS . GT:GQ:DP:AD:VAF:PL 1/1:27:77:36,41:0.532468:42,26,0. ```. I tried with `samtools view -s 0.25` too:. ```. chr1 1668449 . A G 35.5 PASS . GT:GQ:DP:AD:VAF:PL 1/1:16:44:22,22:0.5:35,16,0. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/824
https://github.com/google/deepvariant/issues/824:133,usability,help,help,133,@kishwarshafin @pichuan . I can provide the bam file for some of the variants if you give me an email. I would really appreciate any help,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/824
https://github.com/google/deepvariant/issues/824:95,usability,command,command,95,"Hi @asalimih ,. You can send to pichuan@google.com. Please start with a small BAM file and the command where you can reproduce the call on that BAM file.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/824
https://github.com/google/deepvariant/issues/824:44,usability,command,commands,44,"Thank you @pichuan, I sent the bam file and commands to your email.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/824
https://github.com/google/deepvariant/issues/824:70,deployability,updat,update,70,I'll close this bug for now. Later on I can plan to post a summarized update.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/824
https://github.com/google/deepvariant/issues/824:70,safety,updat,update,70,I'll close this bug for now. Later on I can plan to post a summarized update.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/824
https://github.com/google/deepvariant/issues/824:70,security,updat,update,70,I'll close this bug for now. Later on I can plan to post a summarized update.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/824
https://github.com/google/deepvariant/issues/824:44,testability,plan,plan,44,I'll close this bug for now. Later on I can plan to post a summarized update.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/824
https://github.com/google/deepvariant/issues/824:5,usability,close,close,5,I'll close this bug for now. Later on I can plan to post a summarized update.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/824
https://github.com/google/deepvariant/issues/825:32,modifiability,interm,intermediate,32,"@NourMarzouka , do you have the intermediate files saved by any chance?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/825
https://github.com/google/deepvariant/issues/825:124,modifiability,interm,intermediate,124,"@NourMarzouka ,. Closing this issue due to inactivity. Please feel free to reopen. . The conclusion is that if you have the intermediate files, then you can resume. Otherwise, you have to restart the run.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/825
https://github.com/google/deepvariant/issues/825:157,usability,resum,resume,157,"@NourMarzouka ,. Closing this issue due to inactivity. Please feel free to reopen. . The conclusion is that if you have the intermediate files, then you can resume. Otherwise, you have to restart the run.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/825
https://github.com/google/deepvariant/issues/826:579,deployability,observ,observed,579,"Hi Eva,. Thanks for the question. This is convention for gVCF compression. If every depth change creates a new line, the gVCF files will get incredibly large, to the point that it would be larger than the BAM file itself. At that point, it is easier to calculate the read depth using the BAM file. . In case it is helpful, there is a boolean flag `--include_med_dp` you can pass to `make_examples` as [make_examples_extra_args](https://github.com/google/deepvariant/blob/r1.6.1/scripts/run_deepvariant.py#L195-L203) when calling `run_deepvariant` that will include the median DP observed within the GVCF block rounded to the nearest integer. Hopefully that helps!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/826
https://github.com/google/deepvariant/issues/826:579,testability,observ,observed,579,"Hi Eva,. Thanks for the question. This is convention for gVCF compression. If every depth change creates a new line, the gVCF files will get incredibly large, to the point that it would be larger than the BAM file itself. At that point, it is easier to calculate the read depth using the BAM file. . In case it is helpful, there is a boolean flag `--include_med_dp` you can pass to `make_examples` as [make_examples_extra_args](https://github.com/google/deepvariant/blob/r1.6.1/scripts/run_deepvariant.py#L195-L203) when calling `run_deepvariant` that will include the median DP observed within the GVCF block rounded to the nearest integer. Hopefully that helps!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/826
https://github.com/google/deepvariant/issues/826:314,usability,help,helpful,314,"Hi Eva,. Thanks for the question. This is convention for gVCF compression. If every depth change creates a new line, the gVCF files will get incredibly large, to the point that it would be larger than the BAM file itself. At that point, it is easier to calculate the read depth using the BAM file. . In case it is helpful, there is a boolean flag `--include_med_dp` you can pass to `make_examples` as [make_examples_extra_args](https://github.com/google/deepvariant/blob/r1.6.1/scripts/run_deepvariant.py#L195-L203) when calling `run_deepvariant` that will include the median DP observed within the GVCF block rounded to the nearest integer. Hopefully that helps!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/826
https://github.com/google/deepvariant/issues/826:657,usability,help,helps,657,"Hi Eva,. Thanks for the question. This is convention for gVCF compression. If every depth change creates a new line, the gVCF files will get incredibly large, to the point that it would be larger than the BAM file itself. At that point, it is easier to calculate the read depth using the BAM file. . In case it is helpful, there is a boolean flag `--include_med_dp` you can pass to `make_examples` as [make_examples_extra_args](https://github.com/google/deepvariant/blob/r1.6.1/scripts/run_deepvariant.py#L195-L203) when calling `run_deepvariant` that will include the median DP observed within the GVCF block rounded to the nearest integer. Hopefully that helps!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/826
https://github.com/google/deepvariant/issues/826:179,energy efficiency,reduc,reduced,179,"Thank you Lucas for your answer. I understand it is for compression. I think I was looking for a similar function to GATK BP resolution. When you use targets, the output would be reduced by a lot. The gvcf becomes rather useless, since you cannot look at or filter individual sites on depth. Some sites might have a depth well over my threshold, but is filtered because the minimum depth is well under my threshold. I am thinking the median DP will have similar issues.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/826
https://github.com/google/deepvariant/issues/826:258,integrability,filter,filter,258,"Thank you Lucas for your answer. I understand it is for compression. I think I was looking for a similar function to GATK BP resolution. When you use targets, the output would be reduced by a lot. The gvcf becomes rather useless, since you cannot look at or filter individual sites on depth. Some sites might have a depth well over my threshold, but is filtered because the minimum depth is well under my threshold. I am thinking the median DP will have similar issues.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/826
https://github.com/google/deepvariant/issues/826:353,integrability,filter,filtered,353,"Thank you Lucas for your answer. I understand it is for compression. I think I was looking for a similar function to GATK BP resolution. When you use targets, the output would be reduced by a lot. The gvcf becomes rather useless, since you cannot look at or filter individual sites on depth. Some sites might have a depth well over my threshold, but is filtered because the minimum depth is well under my threshold. I am thinking the median DP will have similar issues.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/826
https://github.com/google/deepvariant/issues/826:35,testability,understand,understand,35,"Thank you Lucas for your answer. I understand it is for compression. I think I was looking for a similar function to GATK BP resolution. When you use targets, the output would be reduced by a lot. The gvcf becomes rather useless, since you cannot look at or filter individual sites on depth. Some sites might have a depth well over my threshold, but is filtered because the minimum depth is well under my threshold. I am thinking the median DP will have similar issues.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/826
https://github.com/google/deepvariant/issues/826:374,usability,minim,minimum,374,"Thank you Lucas for your answer. I understand it is for compression. I think I was looking for a similar function to GATK BP resolution. When you use targets, the output would be reduced by a lot. The gvcf becomes rather useless, since you cannot look at or filter individual sites on depth. Some sites might have a depth well over my threshold, but is filtered because the minimum depth is well under my threshold. I am thinking the median DP will have similar issues.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/826
https://github.com/google/deepvariant/issues/826:41,integrability,filter,filtering,41,"@ESDeutekom ,. If the purpose of this is filtering, you can run something like https://github.com/brentp/mosdepth to get bp resolution coverage. Then create a bed file based on your requirement. Finally, intersect the VCF file with the bed and it should give you the variants you are looking for.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/826
https://github.com/google/deepvariant/issues/826:135,testability,coverag,coverage,135,"@ESDeutekom ,. If the purpose of this is filtering, you can run something like https://github.com/brentp/mosdepth to get bp resolution coverage. Then create a bed file based on your requirement. Finally, intersect the VCF file with the bed and it should give you the variants you are looking for.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/826
https://github.com/google/deepvariant/issues/826:108,integrability,filter,filter,108,"Dear @kishwarshafin, thank you for the information. . I opted for samtools depth. It also gives some option filter on base and mapping quality and such, next to target filtering.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/826
https://github.com/google/deepvariant/issues/826:168,integrability,filter,filtering,168,"Dear @kishwarshafin, thank you for the information. . I opted for samtools depth. It also gives some option filter on base and mapping quality and such, next to target filtering.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/826
https://github.com/google/deepvariant/issues/826:28,usability,close,close,28,"@ESDeutekom , great, I will close the issue. Please feel free to reopen if you have further questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/826
https://github.com/google/deepvariant/issues/828:33,deployability,releas,released,33,"Hi @eaooms ,. Unfortunately, our released DeepVariant Illumina WES or WGS models will not work with phasing information. This use case could be possible if you collect training data on your own, and train your own model accordingly. But that's an advanced use case that our team won't be able to provide much support.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/828
https://github.com/google/deepvariant/issues/828:74,energy efficiency,model,models,74,"Hi @eaooms ,. Unfortunately, our released DeepVariant Illumina WES or WGS models will not work with phasing information. This use case could be possible if you collect training data on your own, and train your own model accordingly. But that's an advanced use case that our team won't be able to provide much support.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/828
https://github.com/google/deepvariant/issues/828:214,energy efficiency,model,model,214,"Hi @eaooms ,. Unfortunately, our released DeepVariant Illumina WES or WGS models will not work with phasing information. This use case could be possible if you collect training data on your own, and train your own model accordingly. But that's an advanced use case that our team won't be able to provide much support.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/828
https://github.com/google/deepvariant/issues/828:74,security,model,models,74,"Hi @eaooms ,. Unfortunately, our released DeepVariant Illumina WES or WGS models will not work with phasing information. This use case could be possible if you collect training data on your own, and train your own model accordingly. But that's an advanced use case that our team won't be able to provide much support.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/828
https://github.com/google/deepvariant/issues/828:214,security,model,model,214,"Hi @eaooms ,. Unfortunately, our released DeepVariant Illumina WES or WGS models will not work with phasing information. This use case could be possible if you collect training data on your own, and train your own model accordingly. But that's an advanced use case that our team won't be able to provide much support.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/828
https://github.com/google/deepvariant/issues/828:274,security,team,team,274,"Hi @eaooms ,. Unfortunately, our released DeepVariant Illumina WES or WGS models will not work with phasing information. This use case could be possible if you collect training data on your own, and train your own model accordingly. But that's an advanced use case that our team won't be able to provide much support.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/828
https://github.com/google/deepvariant/issues/828:309,usability,support,support,309,"Hi @eaooms ,. Unfortunately, our released DeepVariant Illumina WES or WGS models will not work with phasing information. This use case could be possible if you collect training data on your own, and train your own model accordingly. But that's an advanced use case that our team won't be able to provide much support.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/828
https://github.com/google/deepvariant/issues/828:102,energy efficiency,model,model,102,"Thanks for the quick response, . Is it possible to include phasing information with the ONT or PACBIO model?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/828
https://github.com/google/deepvariant/issues/828:95,modifiability,PAC,PACBIO,95,"Thanks for the quick response, . Is it possible to include phasing information with the ONT or PACBIO model?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/828
https://github.com/google/deepvariant/issues/828:102,security,model,model,102,"Thanks for the quick response, . Is it possible to include phasing information with the ONT or PACBIO model?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/828
https://github.com/google/deepvariant/issues/828:293,safety,input,input,293,"Yes, it is possible. In that case you would need to disable internal phasing. `--phase_reads=False`. You can pass this flag through `--make_examples_extra_args` if you use run_deepvariant.py script. With `phase_reads` set to False DeepVariants expects phasing information in `HP` field in the input BAM. Allowed HP values are 0 through 2.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/828
https://github.com/google/deepvariant/issues/828:293,usability,input,input,293,"Yes, it is possible. In that case you would need to disable internal phasing. `--phase_reads=False`. You can pass this flag through `--make_examples_extra_args` if you use run_deepvariant.py script. With `phase_reads` set to False DeepVariants expects phasing information in `HP` field in the input BAM. Allowed HP values are 0 through 2.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/828
https://github.com/google/deepvariant/issues/828:76,modifiability,paramet,parameter,76,"Thank you for this information, I have run deepvariant with this additional parameter: `--make_examples_extra_args=phase_reads=False` but do not see the phasing information in the vcf afterwards. Is the phasing information not preserved after running deepvariant in the vcf?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/828
https://github.com/google/deepvariant/issues/828:184,usability,tool,tools,184,"@eaooms ,. DeepVariant uses the haplotagging information of reads to improve the quality of the variants. It will not produce a phased VCF. If you want to get a phased VCF, please use tools like [margin](https://github.com/UCSC-nanopore-cgl/margin) or [whatshap](https://whatshap.readthedocs.io/en/latest/) on the output of DeepVariant to get a phased VCF.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/828
https://github.com/google/deepvariant/issues/828:80,deployability,releas,release,80,(We'll add @kishwarshafin 's answer in our clarification in our FAQ in the next release!),MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/828
https://github.com/google/deepvariant/issues/829:240,usability,confirm,confirm,240,"Can you check your FASTA file has corresponding index files? What do you see what you run:. ```. ls /media/manish/Data/Jyoti_Mridha_AIC/My_S/Nanopore_Mito/Mito_Genome_hg38/zip_ref_docker/hg38_chrM.fa.gz*. ```. And, just as a check, can you confirm whether you were able to run https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-quick-start.md ?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/829
https://github.com/google/deepvariant/issues/829:97,deployability,version,version,97,"Yes i do have a faidx index file too, actually i am doing it with. mitochondrial genome for hg38 version. Again i modified the command kept both input sorted bam and fasta file. along with faidx index in the same directory, By following the given format. https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-ont-r104-simplex-case-study.md. BIN_VERSION=""1.6.1"". sudo docker run \. -v ""${INPUT_DIR}"":""${INPUT_DIR}"" \. -v ""${OUTPUT_DIR}"":""${OUTPUT_DIR}"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type ONT_R104 \. --ref ""${INPUT_DIR}/${REF}"" \. --reads ""${INPUT_DIR}/${BAM}"" \. --output_vcf ""${OUTPUT_DIR}/${OUTPUT_VCF}"" \. --output_gvcf ""${OUTPUT_DIR}/${OUTPUT_GVCF}"" \. --num_shards ""${THREADS}"" \. --regions ""${REGION}"" \. --intermediate_results_dir ""${OUTPUT_DIR}/${INTERMEDIATE_DIRECTORY}"". => Here is my recent command used for docker run. It would be great help if. I am able to resolve this issue. Thank you in advance. sudo docker run -v. */media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3*. -v. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result. google/deepvariant:{BIN_VERSION=""1.6.1""} python. /media/manish/Data/Jyoti_Mridha_AIC/Program/deepvariant-1.6.1/scripts/run_deepvariant.py. *--reads. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3/MITO60_sorted.bam. --ref. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequenci",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/829
https://github.com/google/deepvariant/issues/829:97,integrability,version,version,97,"Yes i do have a faidx index file too, actually i am doing it with. mitochondrial genome for hg38 version. Again i modified the command kept both input sorted bam and fasta file. along with faidx index in the same directory, By following the given format. https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-ont-r104-simplex-case-study.md. BIN_VERSION=""1.6.1"". sudo docker run \. -v ""${INPUT_DIR}"":""${INPUT_DIR}"" \. -v ""${OUTPUT_DIR}"":""${OUTPUT_DIR}"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type ONT_R104 \. --ref ""${INPUT_DIR}/${REF}"" \. --reads ""${INPUT_DIR}/${BAM}"" \. --output_vcf ""${OUTPUT_DIR}/${OUTPUT_VCF}"" \. --output_gvcf ""${OUTPUT_DIR}/${OUTPUT_GVCF}"" \. --num_shards ""${THREADS}"" \. --regions ""${REGION}"" \. --intermediate_results_dir ""${OUTPUT_DIR}/${INTERMEDIATE_DIRECTORY}"". => Here is my recent command used for docker run. It would be great help if. I am able to resolve this issue. Thank you in advance. sudo docker run -v. */media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3*. -v. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result. google/deepvariant:{BIN_VERSION=""1.6.1""} python. /media/manish/Data/Jyoti_Mridha_AIC/Program/deepvariant-1.6.1/scripts/run_deepvariant.py. *--reads. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3/MITO60_sorted.bam. --ref. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequenci",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/829
https://github.com/google/deepvariant/issues/829:3171,integrability,Messag,Message,3171,"IC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3*. -v. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result. google/deepvariant:{BIN_VERSION=""1.6.1""} python. /media/manish/Data/Jyoti_Mridha_AIC/Program/deepvariant-1.6.1/scripts/run_deepvariant.py. *--reads. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3/MITO60_sorted.bam. --ref. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3/hg38_chrM.fa. *--report_title MITO60_Stats --sample_name MITO60 --output_vcf. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result/. --model_type ONT_R104. On Wed, Jun 12, 2024 at 11:20 AM Pi-Chuan Chang ***@***.***>. wrote:. > Can you check your FASTA file has corresponding index files? >. > What do you see what you run:. >. > ls /media/manish/Data/Jyoti_Mridha_AIC/My_S/Nanopore_Mito/Mito_Genome_hg38/zip_ref_docker/hg38_chrM.fa.gz*. >. > And, just as a check, can you confirm whether you were able to run. > https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-quick-start.md. > ? >. > —. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/829#issuecomment-2162157549>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/BDQL2ZEKDU5WYOAUIP24CJLZG7OSVAVCNFSM6AAAAABJFTFWYKVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDCNRSGE2TONJUHE>. > . > You are receiving this because you authored the thread.Message ID:. > ***@***.***>. >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/829
https://github.com/google/deepvariant/issues/829:247,interoperability,format,format,247,"Yes i do have a faidx index file too, actually i am doing it with. mitochondrial genome for hg38 version. Again i modified the command kept both input sorted bam and fasta file. along with faidx index in the same directory, By following the given format. https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-ont-r104-simplex-case-study.md. BIN_VERSION=""1.6.1"". sudo docker run \. -v ""${INPUT_DIR}"":""${INPUT_DIR}"" \. -v ""${OUTPUT_DIR}"":""${OUTPUT_DIR}"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type ONT_R104 \. --ref ""${INPUT_DIR}/${REF}"" \. --reads ""${INPUT_DIR}/${BAM}"" \. --output_vcf ""${OUTPUT_DIR}/${OUTPUT_VCF}"" \. --output_gvcf ""${OUTPUT_DIR}/${OUTPUT_GVCF}"" \. --num_shards ""${THREADS}"" \. --regions ""${REGION}"" \. --intermediate_results_dir ""${OUTPUT_DIR}/${INTERMEDIATE_DIRECTORY}"". => Here is my recent command used for docker run. It would be great help if. I am able to resolve this issue. Thank you in advance. sudo docker run -v. */media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3*. -v. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result. google/deepvariant:{BIN_VERSION=""1.6.1""} python. /media/manish/Data/Jyoti_Mridha_AIC/Program/deepvariant-1.6.1/scripts/run_deepvariant.py. *--reads. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3/MITO60_sorted.bam. --ref. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequenci",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/829
https://github.com/google/deepvariant/issues/829:3171,interoperability,Messag,Message,3171,"IC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3*. -v. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result. google/deepvariant:{BIN_VERSION=""1.6.1""} python. /media/manish/Data/Jyoti_Mridha_AIC/Program/deepvariant-1.6.1/scripts/run_deepvariant.py. *--reads. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3/MITO60_sorted.bam. --ref. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3/hg38_chrM.fa. *--report_title MITO60_Stats --sample_name MITO60 --output_vcf. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result/. --model_type ONT_R104. On Wed, Jun 12, 2024 at 11:20 AM Pi-Chuan Chang ***@***.***>. wrote:. > Can you check your FASTA file has corresponding index files? >. > What do you see what you run:. >. > ls /media/manish/Data/Jyoti_Mridha_AIC/My_S/Nanopore_Mito/Mito_Genome_hg38/zip_ref_docker/hg38_chrM.fa.gz*. >. > And, just as a check, can you confirm whether you were able to run. > https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-quick-start.md. > ? >. > —. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/829#issuecomment-2162157549>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/BDQL2ZEKDU5WYOAUIP24CJLZG7OSVAVCNFSM6AAAAABJFTFWYKVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDCNRSGE2TONJUHE>. > . > You are receiving this because you authored the thread.Message ID:. > ***@***.***>. >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/829
https://github.com/google/deepvariant/issues/829:97,modifiability,version,version,97,"Yes i do have a faidx index file too, actually i am doing it with. mitochondrial genome for hg38 version. Again i modified the command kept both input sorted bam and fasta file. along with faidx index in the same directory, By following the given format. https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-ont-r104-simplex-case-study.md. BIN_VERSION=""1.6.1"". sudo docker run \. -v ""${INPUT_DIR}"":""${INPUT_DIR}"" \. -v ""${OUTPUT_DIR}"":""${OUTPUT_DIR}"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type ONT_R104 \. --ref ""${INPUT_DIR}/${REF}"" \. --reads ""${INPUT_DIR}/${BAM}"" \. --output_vcf ""${OUTPUT_DIR}/${OUTPUT_VCF}"" \. --output_gvcf ""${OUTPUT_DIR}/${OUTPUT_GVCF}"" \. --num_shards ""${THREADS}"" \. --regions ""${REGION}"" \. --intermediate_results_dir ""${OUTPUT_DIR}/${INTERMEDIATE_DIRECTORY}"". => Here is my recent command used for docker run. It would be great help if. I am able to resolve this issue. Thank you in advance. sudo docker run -v. */media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3*. -v. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result. google/deepvariant:{BIN_VERSION=""1.6.1""} python. /media/manish/Data/Jyoti_Mridha_AIC/Program/deepvariant-1.6.1/scripts/run_deepvariant.py. *--reads. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3/MITO60_sorted.bam. --ref. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequenci",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/829
https://github.com/google/deepvariant/issues/829:145,safety,input,input,145,"Yes i do have a faidx index file too, actually i am doing it with. mitochondrial genome for hg38 version. Again i modified the command kept both input sorted bam and fasta file. along with faidx index in the same directory, By following the given format. https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-ont-r104-simplex-case-study.md. BIN_VERSION=""1.6.1"". sudo docker run \. -v ""${INPUT_DIR}"":""${INPUT_DIR}"" \. -v ""${OUTPUT_DIR}"":""${OUTPUT_DIR}"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type ONT_R104 \. --ref ""${INPUT_DIR}/${REF}"" \. --reads ""${INPUT_DIR}/${BAM}"" \. --output_vcf ""${OUTPUT_DIR}/${OUTPUT_VCF}"" \. --output_gvcf ""${OUTPUT_DIR}/${OUTPUT_GVCF}"" \. --num_shards ""${THREADS}"" \. --regions ""${REGION}"" \. --intermediate_results_dir ""${OUTPUT_DIR}/${INTERMEDIATE_DIRECTORY}"". => Here is my recent command used for docker run. It would be great help if. I am able to resolve this issue. Thank you in advance. sudo docker run -v. */media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3*. -v. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result. google/deepvariant:{BIN_VERSION=""1.6.1""} python. /media/manish/Data/Jyoti_Mridha_AIC/Program/deepvariant-1.6.1/scripts/run_deepvariant.py. *--reads. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3/MITO60_sorted.bam. --ref. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequenci",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/829
https://github.com/google/deepvariant/issues/829:114,security,modif,modified,114,"Yes i do have a faidx index file too, actually i am doing it with. mitochondrial genome for hg38 version. Again i modified the command kept both input sorted bam and fasta file. along with faidx index in the same directory, By following the given format. https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-ont-r104-simplex-case-study.md. BIN_VERSION=""1.6.1"". sudo docker run \. -v ""${INPUT_DIR}"":""${INPUT_DIR}"" \. -v ""${OUTPUT_DIR}"":""${OUTPUT_DIR}"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type ONT_R104 \. --ref ""${INPUT_DIR}/${REF}"" \. --reads ""${INPUT_DIR}/${BAM}"" \. --output_vcf ""${OUTPUT_DIR}/${OUTPUT_VCF}"" \. --output_gvcf ""${OUTPUT_DIR}/${OUTPUT_GVCF}"" \. --num_shards ""${THREADS}"" \. --regions ""${REGION}"" \. --intermediate_results_dir ""${OUTPUT_DIR}/${INTERMEDIATE_DIRECTORY}"". => Here is my recent command used for docker run. It would be great help if. I am able to resolve this issue. Thank you in advance. sudo docker run -v. */media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3*. -v. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result. google/deepvariant:{BIN_VERSION=""1.6.1""} python. /media/manish/Data/Jyoti_Mridha_AIC/Program/deepvariant-1.6.1/scripts/run_deepvariant.py. *--reads. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3/MITO60_sorted.bam. --ref. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequenci",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/829
https://github.com/google/deepvariant/issues/829:2996,security,auth,auth,2996,"IC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3*. -v. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result. google/deepvariant:{BIN_VERSION=""1.6.1""} python. /media/manish/Data/Jyoti_Mridha_AIC/Program/deepvariant-1.6.1/scripts/run_deepvariant.py. *--reads. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3/MITO60_sorted.bam. --ref. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3/hg38_chrM.fa. *--report_title MITO60_Stats --sample_name MITO60 --output_vcf. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result/. --model_type ONT_R104. On Wed, Jun 12, 2024 at 11:20 AM Pi-Chuan Chang ***@***.***>. wrote:. > Can you check your FASTA file has corresponding index files? >. > What do you see what you run:. >. > ls /media/manish/Data/Jyoti_Mridha_AIC/My_S/Nanopore_Mito/Mito_Genome_hg38/zip_ref_docker/hg38_chrM.fa.gz*. >. > And, just as a check, can you confirm whether you were able to run. > https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-quick-start.md. > ? >. > —. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/829#issuecomment-2162157549>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/BDQL2ZEKDU5WYOAUIP24CJLZG7OSVAVCNFSM6AAAAABJFTFWYKVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDCNRSGE2TONJUHE>. > . > You are receiving this because you authored the thread.Message ID:. > ***@***.***>. >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/829
https://github.com/google/deepvariant/issues/829:3151,security,auth,authored,3151,"IC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3*. -v. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result. google/deepvariant:{BIN_VERSION=""1.6.1""} python. /media/manish/Data/Jyoti_Mridha_AIC/Program/deepvariant-1.6.1/scripts/run_deepvariant.py. *--reads. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3/MITO60_sorted.bam. --ref. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3/hg38_chrM.fa. *--report_title MITO60_Stats --sample_name MITO60 --output_vcf. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result/. --model_type ONT_R104. On Wed, Jun 12, 2024 at 11:20 AM Pi-Chuan Chang ***@***.***>. wrote:. > Can you check your FASTA file has corresponding index files? >. > What do you see what you run:. >. > ls /media/manish/Data/Jyoti_Mridha_AIC/My_S/Nanopore_Mito/Mito_Genome_hg38/zip_ref_docker/hg38_chrM.fa.gz*. >. > And, just as a check, can you confirm whether you were able to run. > https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-quick-start.md. > ? >. > —. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/829#issuecomment-2162157549>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/BDQL2ZEKDU5WYOAUIP24CJLZG7OSVAVCNFSM6AAAAABJFTFWYKVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDCNRSGE2TONJUHE>. > . > You are receiving this because you authored the thread.Message ID:. > ***@***.***>. >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/829
https://github.com/google/deepvariant/issues/829:331,testability,simpl,simplex-case-study,331,"Yes i do have a faidx index file too, actually i am doing it with. mitochondrial genome for hg38 version. Again i modified the command kept both input sorted bam and fasta file. along with faidx index in the same directory, By following the given format. https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-ont-r104-simplex-case-study.md. BIN_VERSION=""1.6.1"". sudo docker run \. -v ""${INPUT_DIR}"":""${INPUT_DIR}"" \. -v ""${OUTPUT_DIR}"":""${OUTPUT_DIR}"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type ONT_R104 \. --ref ""${INPUT_DIR}/${REF}"" \. --reads ""${INPUT_DIR}/${BAM}"" \. --output_vcf ""${OUTPUT_DIR}/${OUTPUT_VCF}"" \. --output_gvcf ""${OUTPUT_DIR}/${OUTPUT_GVCF}"" \. --num_shards ""${THREADS}"" \. --regions ""${REGION}"" \. --intermediate_results_dir ""${OUTPUT_DIR}/${INTERMEDIATE_DIRECTORY}"". => Here is my recent command used for docker run. It would be great help if. I am able to resolve this issue. Thank you in advance. sudo docker run -v. */media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3*. -v. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result. google/deepvariant:{BIN_VERSION=""1.6.1""} python. /media/manish/Data/Jyoti_Mridha_AIC/Program/deepvariant-1.6.1/scripts/run_deepvariant.py. *--reads. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3/MITO60_sorted.bam. --ref. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequenci",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/829
https://github.com/google/deepvariant/issues/829:127,usability,command,command,127,"Yes i do have a faidx index file too, actually i am doing it with. mitochondrial genome for hg38 version. Again i modified the command kept both input sorted bam and fasta file. along with faidx index in the same directory, By following the given format. https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-ont-r104-simplex-case-study.md. BIN_VERSION=""1.6.1"". sudo docker run \. -v ""${INPUT_DIR}"":""${INPUT_DIR}"" \. -v ""${OUTPUT_DIR}"":""${OUTPUT_DIR}"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type ONT_R104 \. --ref ""${INPUT_DIR}/${REF}"" \. --reads ""${INPUT_DIR}/${BAM}"" \. --output_vcf ""${OUTPUT_DIR}/${OUTPUT_VCF}"" \. --output_gvcf ""${OUTPUT_DIR}/${OUTPUT_GVCF}"" \. --num_shards ""${THREADS}"" \. --regions ""${REGION}"" \. --intermediate_results_dir ""${OUTPUT_DIR}/${INTERMEDIATE_DIRECTORY}"". => Here is my recent command used for docker run. It would be great help if. I am able to resolve this issue. Thank you in advance. sudo docker run -v. */media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3*. -v. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result. google/deepvariant:{BIN_VERSION=""1.6.1""} python. /media/manish/Data/Jyoti_Mridha_AIC/Program/deepvariant-1.6.1/scripts/run_deepvariant.py. *--reads. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3/MITO60_sorted.bam. --ref. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequenci",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/829
https://github.com/google/deepvariant/issues/829:145,usability,input,input,145,"Yes i do have a faidx index file too, actually i am doing it with. mitochondrial genome for hg38 version. Again i modified the command kept both input sorted bam and fasta file. along with faidx index in the same directory, By following the given format. https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-ont-r104-simplex-case-study.md. BIN_VERSION=""1.6.1"". sudo docker run \. -v ""${INPUT_DIR}"":""${INPUT_DIR}"" \. -v ""${OUTPUT_DIR}"":""${OUTPUT_DIR}"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type ONT_R104 \. --ref ""${INPUT_DIR}/${REF}"" \. --reads ""${INPUT_DIR}/${BAM}"" \. --output_vcf ""${OUTPUT_DIR}/${OUTPUT_VCF}"" \. --output_gvcf ""${OUTPUT_DIR}/${OUTPUT_GVCF}"" \. --num_shards ""${THREADS}"" \. --regions ""${REGION}"" \. --intermediate_results_dir ""${OUTPUT_DIR}/${INTERMEDIATE_DIRECTORY}"". => Here is my recent command used for docker run. It would be great help if. I am able to resolve this issue. Thank you in advance. sudo docker run -v. */media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3*. -v. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result. google/deepvariant:{BIN_VERSION=""1.6.1""} python. /media/manish/Data/Jyoti_Mridha_AIC/Program/deepvariant-1.6.1/scripts/run_deepvariant.py. *--reads. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3/MITO60_sorted.bam. --ref. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequenci",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/829
https://github.com/google/deepvariant/issues/829:331,usability,simpl,simplex-case-study,331,"Yes i do have a faidx index file too, actually i am doing it with. mitochondrial genome for hg38 version. Again i modified the command kept both input sorted bam and fasta file. along with faidx index in the same directory, By following the given format. https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-ont-r104-simplex-case-study.md. BIN_VERSION=""1.6.1"". sudo docker run \. -v ""${INPUT_DIR}"":""${INPUT_DIR}"" \. -v ""${OUTPUT_DIR}"":""${OUTPUT_DIR}"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type ONT_R104 \. --ref ""${INPUT_DIR}/${REF}"" \. --reads ""${INPUT_DIR}/${BAM}"" \. --output_vcf ""${OUTPUT_DIR}/${OUTPUT_VCF}"" \. --output_gvcf ""${OUTPUT_DIR}/${OUTPUT_GVCF}"" \. --num_shards ""${THREADS}"" \. --regions ""${REGION}"" \. --intermediate_results_dir ""${OUTPUT_DIR}/${INTERMEDIATE_DIRECTORY}"". => Here is my recent command used for docker run. It would be great help if. I am able to resolve this issue. Thank you in advance. sudo docker run -v. */media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3*. -v. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result. google/deepvariant:{BIN_VERSION=""1.6.1""} python. /media/manish/Data/Jyoti_Mridha_AIC/Program/deepvariant-1.6.1/scripts/run_deepvariant.py. *--reads. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3/MITO60_sorted.bam. --ref. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequenci",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/829
https://github.com/google/deepvariant/issues/829:875,usability,command,command,875,"Yes i do have a faidx index file too, actually i am doing it with. mitochondrial genome for hg38 version. Again i modified the command kept both input sorted bam and fasta file. along with faidx index in the same directory, By following the given format. https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-ont-r104-simplex-case-study.md. BIN_VERSION=""1.6.1"". sudo docker run \. -v ""${INPUT_DIR}"":""${INPUT_DIR}"" \. -v ""${OUTPUT_DIR}"":""${OUTPUT_DIR}"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type ONT_R104 \. --ref ""${INPUT_DIR}/${REF}"" \. --reads ""${INPUT_DIR}/${BAM}"" \. --output_vcf ""${OUTPUT_DIR}/${OUTPUT_VCF}"" \. --output_gvcf ""${OUTPUT_DIR}/${OUTPUT_GVCF}"" \. --num_shards ""${THREADS}"" \. --regions ""${REGION}"" \. --intermediate_results_dir ""${OUTPUT_DIR}/${INTERMEDIATE_DIRECTORY}"". => Here is my recent command used for docker run. It would be great help if. I am able to resolve this issue. Thank you in advance. sudo docker run -v. */media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3*. -v. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result. google/deepvariant:{BIN_VERSION=""1.6.1""} python. /media/manish/Data/Jyoti_Mridha_AIC/Program/deepvariant-1.6.1/scripts/run_deepvariant.py. *--reads. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3/MITO60_sorted.bam. --ref. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequenci",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/829
https://github.com/google/deepvariant/issues/829:922,usability,help,help,922,"Yes i do have a faidx index file too, actually i am doing it with. mitochondrial genome for hg38 version. Again i modified the command kept both input sorted bam and fasta file. along with faidx index in the same directory, By following the given format. https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-ont-r104-simplex-case-study.md. BIN_VERSION=""1.6.1"". sudo docker run \. -v ""${INPUT_DIR}"":""${INPUT_DIR}"" \. -v ""${OUTPUT_DIR}"":""${OUTPUT_DIR}"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type ONT_R104 \. --ref ""${INPUT_DIR}/${REF}"" \. --reads ""${INPUT_DIR}/${BAM}"" \. --output_vcf ""${OUTPUT_DIR}/${OUTPUT_VCF}"" \. --output_gvcf ""${OUTPUT_DIR}/${OUTPUT_GVCF}"" \. --num_shards ""${THREADS}"" \. --regions ""${REGION}"" \. --intermediate_results_dir ""${OUTPUT_DIR}/${INTERMEDIATE_DIRECTORY}"". => Here is my recent command used for docker run. It would be great help if. I am able to resolve this issue. Thank you in advance. sudo docker run -v. */media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3*. -v. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result. google/deepvariant:{BIN_VERSION=""1.6.1""} python. /media/manish/Data/Jyoti_Mridha_AIC/Program/deepvariant-1.6.1/scripts/run_deepvariant.py. *--reads. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3/MITO60_sorted.bam. --ref. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequenci",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/829
https://github.com/google/deepvariant/issues/829:2665,usability,confirm,confirm,2665,"IC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3*. -v. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result. google/deepvariant:{BIN_VERSION=""1.6.1""} python. /media/manish/Data/Jyoti_Mridha_AIC/Program/deepvariant-1.6.1/scripts/run_deepvariant.py. *--reads. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3/MITO60_sorted.bam. --ref. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3/hg38_chrM.fa. *--report_title MITO60_Stats --sample_name MITO60 --output_vcf. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result/. --model_type ONT_R104. On Wed, Jun 12, 2024 at 11:20 AM Pi-Chuan Chang ***@***.***>. wrote:. > Can you check your FASTA file has corresponding index files? >. > What do you see what you run:. >. > ls /media/manish/Data/Jyoti_Mridha_AIC/My_S/Nanopore_Mito/Mito_Genome_hg38/zip_ref_docker/hg38_chrM.fa.gz*. >. > And, just as a check, can you confirm whether you were able to run. > https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-quick-start.md. > ? >. > —. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/829#issuecomment-2162157549>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/BDQL2ZEKDU5WYOAUIP24CJLZG7OSVAVCNFSM6AAAAABJFTFWYKVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDCNRSGE2TONJUHE>. > . > You are receiving this because you authored the thread.Message ID:. > ***@***.***>. >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/829
https://github.com/google/deepvariant/issues/829:37,availability,error,error,37,"Hi @Jyoti-Mridha ,. I looked at your error again more closely, and I realize that `docker: invalid reference format.` is not referring to the FASTA file. . It is something to do with your command, which doesn't have the correct format of how you would use Docker. For example, if I run:. ```bash. sudo docker run google/deepvariant:1.6.1 /opt/deepvariant/bin/run_deepvariant --helpshort. ```. which shows me the flags. But if I run something similar to your command:. ```bash. sudo docker run google/deepvariant:{BIN_VERSION=""1.6.1""} /opt/deepvariant/bin/run_deepvariant --helpshort. ```. It gave me this error:. ```. docker: invalid reference format. See 'docker run --help'. ```. I think there might be other formatting errors in your command. My suggestion: Can you make sure you follow our Quick Start step-by-step first? And, once that succeed, please make sure you use a command similar to https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-quick-start.md#run-deepvariant-with-one-command.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/829
https://github.com/google/deepvariant/issues/829:605,availability,error,error,605,"Hi @Jyoti-Mridha ,. I looked at your error again more closely, and I realize that `docker: invalid reference format.` is not referring to the FASTA file. . It is something to do with your command, which doesn't have the correct format of how you would use Docker. For example, if I run:. ```bash. sudo docker run google/deepvariant:1.6.1 /opt/deepvariant/bin/run_deepvariant --helpshort. ```. which shows me the flags. But if I run something similar to your command:. ```bash. sudo docker run google/deepvariant:{BIN_VERSION=""1.6.1""} /opt/deepvariant/bin/run_deepvariant --helpshort. ```. It gave me this error:. ```. docker: invalid reference format. See 'docker run --help'. ```. I think there might be other formatting errors in your command. My suggestion: Can you make sure you follow our Quick Start step-by-step first? And, once that succeed, please make sure you use a command similar to https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-quick-start.md#run-deepvariant-with-one-command.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/829
https://github.com/google/deepvariant/issues/829:722,availability,error,errors,722,"Hi @Jyoti-Mridha ,. I looked at your error again more closely, and I realize that `docker: invalid reference format.` is not referring to the FASTA file. . It is something to do with your command, which doesn't have the correct format of how you would use Docker. For example, if I run:. ```bash. sudo docker run google/deepvariant:1.6.1 /opt/deepvariant/bin/run_deepvariant --helpshort. ```. which shows me the flags. But if I run something similar to your command:. ```bash. sudo docker run google/deepvariant:{BIN_VERSION=""1.6.1""} /opt/deepvariant/bin/run_deepvariant --helpshort. ```. It gave me this error:. ```. docker: invalid reference format. See 'docker run --help'. ```. I think there might be other formatting errors in your command. My suggestion: Can you make sure you follow our Quick Start step-by-step first? And, once that succeed, please make sure you use a command similar to https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-quick-start.md#run-deepvariant-with-one-command.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/829
https://github.com/google/deepvariant/issues/829:109,interoperability,format,format,109,"Hi @Jyoti-Mridha ,. I looked at your error again more closely, and I realize that `docker: invalid reference format.` is not referring to the FASTA file. . It is something to do with your command, which doesn't have the correct format of how you would use Docker. For example, if I run:. ```bash. sudo docker run google/deepvariant:1.6.1 /opt/deepvariant/bin/run_deepvariant --helpshort. ```. which shows me the flags. But if I run something similar to your command:. ```bash. sudo docker run google/deepvariant:{BIN_VERSION=""1.6.1""} /opt/deepvariant/bin/run_deepvariant --helpshort. ```. It gave me this error:. ```. docker: invalid reference format. See 'docker run --help'. ```. I think there might be other formatting errors in your command. My suggestion: Can you make sure you follow our Quick Start step-by-step first? And, once that succeed, please make sure you use a command similar to https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-quick-start.md#run-deepvariant-with-one-command.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/829
https://github.com/google/deepvariant/issues/829:228,interoperability,format,format,228,"Hi @Jyoti-Mridha ,. I looked at your error again more closely, and I realize that `docker: invalid reference format.` is not referring to the FASTA file. . It is something to do with your command, which doesn't have the correct format of how you would use Docker. For example, if I run:. ```bash. sudo docker run google/deepvariant:1.6.1 /opt/deepvariant/bin/run_deepvariant --helpshort. ```. which shows me the flags. But if I run something similar to your command:. ```bash. sudo docker run google/deepvariant:{BIN_VERSION=""1.6.1""} /opt/deepvariant/bin/run_deepvariant --helpshort. ```. It gave me this error:. ```. docker: invalid reference format. See 'docker run --help'. ```. I think there might be other formatting errors in your command. My suggestion: Can you make sure you follow our Quick Start step-by-step first? And, once that succeed, please make sure you use a command similar to https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-quick-start.md#run-deepvariant-with-one-command.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/829
https://github.com/google/deepvariant/issues/829:644,interoperability,format,format,644,"Hi @Jyoti-Mridha ,. I looked at your error again more closely, and I realize that `docker: invalid reference format.` is not referring to the FASTA file. . It is something to do with your command, which doesn't have the correct format of how you would use Docker. For example, if I run:. ```bash. sudo docker run google/deepvariant:1.6.1 /opt/deepvariant/bin/run_deepvariant --helpshort. ```. which shows me the flags. But if I run something similar to your command:. ```bash. sudo docker run google/deepvariant:{BIN_VERSION=""1.6.1""} /opt/deepvariant/bin/run_deepvariant --helpshort. ```. It gave me this error:. ```. docker: invalid reference format. See 'docker run --help'. ```. I think there might be other formatting errors in your command. My suggestion: Can you make sure you follow our Quick Start step-by-step first? And, once that succeed, please make sure you use a command similar to https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-quick-start.md#run-deepvariant-with-one-command.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/829
https://github.com/google/deepvariant/issues/829:711,interoperability,format,formatting,711,"Hi @Jyoti-Mridha ,. I looked at your error again more closely, and I realize that `docker: invalid reference format.` is not referring to the FASTA file. . It is something to do with your command, which doesn't have the correct format of how you would use Docker. For example, if I run:. ```bash. sudo docker run google/deepvariant:1.6.1 /opt/deepvariant/bin/run_deepvariant --helpshort. ```. which shows me the flags. But if I run something similar to your command:. ```bash. sudo docker run google/deepvariant:{BIN_VERSION=""1.6.1""} /opt/deepvariant/bin/run_deepvariant --helpshort. ```. It gave me this error:. ```. docker: invalid reference format. See 'docker run --help'. ```. I think there might be other formatting errors in your command. My suggestion: Can you make sure you follow our Quick Start step-by-step first? And, once that succeed, please make sure you use a command similar to https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-quick-start.md#run-deepvariant-with-one-command.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/829
https://github.com/google/deepvariant/issues/829:37,performance,error,error,37,"Hi @Jyoti-Mridha ,. I looked at your error again more closely, and I realize that `docker: invalid reference format.` is not referring to the FASTA file. . It is something to do with your command, which doesn't have the correct format of how you would use Docker. For example, if I run:. ```bash. sudo docker run google/deepvariant:1.6.1 /opt/deepvariant/bin/run_deepvariant --helpshort. ```. which shows me the flags. But if I run something similar to your command:. ```bash. sudo docker run google/deepvariant:{BIN_VERSION=""1.6.1""} /opt/deepvariant/bin/run_deepvariant --helpshort. ```. It gave me this error:. ```. docker: invalid reference format. See 'docker run --help'. ```. I think there might be other formatting errors in your command. My suggestion: Can you make sure you follow our Quick Start step-by-step first? And, once that succeed, please make sure you use a command similar to https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-quick-start.md#run-deepvariant-with-one-command.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/829
https://github.com/google/deepvariant/issues/829:605,performance,error,error,605,"Hi @Jyoti-Mridha ,. I looked at your error again more closely, and I realize that `docker: invalid reference format.` is not referring to the FASTA file. . It is something to do with your command, which doesn't have the correct format of how you would use Docker. For example, if I run:. ```bash. sudo docker run google/deepvariant:1.6.1 /opt/deepvariant/bin/run_deepvariant --helpshort. ```. which shows me the flags. But if I run something similar to your command:. ```bash. sudo docker run google/deepvariant:{BIN_VERSION=""1.6.1""} /opt/deepvariant/bin/run_deepvariant --helpshort. ```. It gave me this error:. ```. docker: invalid reference format. See 'docker run --help'. ```. I think there might be other formatting errors in your command. My suggestion: Can you make sure you follow our Quick Start step-by-step first? And, once that succeed, please make sure you use a command similar to https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-quick-start.md#run-deepvariant-with-one-command.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/829
https://github.com/google/deepvariant/issues/829:722,performance,error,errors,722,"Hi @Jyoti-Mridha ,. I looked at your error again more closely, and I realize that `docker: invalid reference format.` is not referring to the FASTA file. . It is something to do with your command, which doesn't have the correct format of how you would use Docker. For example, if I run:. ```bash. sudo docker run google/deepvariant:1.6.1 /opt/deepvariant/bin/run_deepvariant --helpshort. ```. which shows me the flags. But if I run something similar to your command:. ```bash. sudo docker run google/deepvariant:{BIN_VERSION=""1.6.1""} /opt/deepvariant/bin/run_deepvariant --helpshort. ```. It gave me this error:. ```. docker: invalid reference format. See 'docker run --help'. ```. I think there might be other formatting errors in your command. My suggestion: Can you make sure you follow our Quick Start step-by-step first? And, once that succeed, please make sure you use a command similar to https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-quick-start.md#run-deepvariant-with-one-command.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/829
https://github.com/google/deepvariant/issues/829:203,reliability,doe,doesn,203,"Hi @Jyoti-Mridha ,. I looked at your error again more closely, and I realize that `docker: invalid reference format.` is not referring to the FASTA file. . It is something to do with your command, which doesn't have the correct format of how you would use Docker. For example, if I run:. ```bash. sudo docker run google/deepvariant:1.6.1 /opt/deepvariant/bin/run_deepvariant --helpshort. ```. which shows me the flags. But if I run something similar to your command:. ```bash. sudo docker run google/deepvariant:{BIN_VERSION=""1.6.1""} /opt/deepvariant/bin/run_deepvariant --helpshort. ```. It gave me this error:. ```. docker: invalid reference format. See 'docker run --help'. ```. I think there might be other formatting errors in your command. My suggestion: Can you make sure you follow our Quick Start step-by-step first? And, once that succeed, please make sure you use a command similar to https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-quick-start.md#run-deepvariant-with-one-command.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/829
https://github.com/google/deepvariant/issues/829:37,safety,error,error,37,"Hi @Jyoti-Mridha ,. I looked at your error again more closely, and I realize that `docker: invalid reference format.` is not referring to the FASTA file. . It is something to do with your command, which doesn't have the correct format of how you would use Docker. For example, if I run:. ```bash. sudo docker run google/deepvariant:1.6.1 /opt/deepvariant/bin/run_deepvariant --helpshort. ```. which shows me the flags. But if I run something similar to your command:. ```bash. sudo docker run google/deepvariant:{BIN_VERSION=""1.6.1""} /opt/deepvariant/bin/run_deepvariant --helpshort. ```. It gave me this error:. ```. docker: invalid reference format. See 'docker run --help'. ```. I think there might be other formatting errors in your command. My suggestion: Can you make sure you follow our Quick Start step-by-step first? And, once that succeed, please make sure you use a command similar to https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-quick-start.md#run-deepvariant-with-one-command.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/829
https://github.com/google/deepvariant/issues/829:605,safety,error,error,605,"Hi @Jyoti-Mridha ,. I looked at your error again more closely, and I realize that `docker: invalid reference format.` is not referring to the FASTA file. . It is something to do with your command, which doesn't have the correct format of how you would use Docker. For example, if I run:. ```bash. sudo docker run google/deepvariant:1.6.1 /opt/deepvariant/bin/run_deepvariant --helpshort. ```. which shows me the flags. But if I run something similar to your command:. ```bash. sudo docker run google/deepvariant:{BIN_VERSION=""1.6.1""} /opt/deepvariant/bin/run_deepvariant --helpshort. ```. It gave me this error:. ```. docker: invalid reference format. See 'docker run --help'. ```. I think there might be other formatting errors in your command. My suggestion: Can you make sure you follow our Quick Start step-by-step first? And, once that succeed, please make sure you use a command similar to https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-quick-start.md#run-deepvariant-with-one-command.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/829
https://github.com/google/deepvariant/issues/829:722,safety,error,errors,722,"Hi @Jyoti-Mridha ,. I looked at your error again more closely, and I realize that `docker: invalid reference format.` is not referring to the FASTA file. . It is something to do with your command, which doesn't have the correct format of how you would use Docker. For example, if I run:. ```bash. sudo docker run google/deepvariant:1.6.1 /opt/deepvariant/bin/run_deepvariant --helpshort. ```. which shows me the flags. But if I run something similar to your command:. ```bash. sudo docker run google/deepvariant:{BIN_VERSION=""1.6.1""} /opt/deepvariant/bin/run_deepvariant --helpshort. ```. It gave me this error:. ```. docker: invalid reference format. See 'docker run --help'. ```. I think there might be other formatting errors in your command. My suggestion: Can you make sure you follow our Quick Start step-by-step first? And, once that succeed, please make sure you use a command similar to https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-quick-start.md#run-deepvariant-with-one-command.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/829
https://github.com/google/deepvariant/issues/829:37,usability,error,error,37,"Hi @Jyoti-Mridha ,. I looked at your error again more closely, and I realize that `docker: invalid reference format.` is not referring to the FASTA file. . It is something to do with your command, which doesn't have the correct format of how you would use Docker. For example, if I run:. ```bash. sudo docker run google/deepvariant:1.6.1 /opt/deepvariant/bin/run_deepvariant --helpshort. ```. which shows me the flags. But if I run something similar to your command:. ```bash. sudo docker run google/deepvariant:{BIN_VERSION=""1.6.1""} /opt/deepvariant/bin/run_deepvariant --helpshort. ```. It gave me this error:. ```. docker: invalid reference format. See 'docker run --help'. ```. I think there might be other formatting errors in your command. My suggestion: Can you make sure you follow our Quick Start step-by-step first? And, once that succeed, please make sure you use a command similar to https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-quick-start.md#run-deepvariant-with-one-command.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/829
https://github.com/google/deepvariant/issues/829:54,usability,close,closely,54,"Hi @Jyoti-Mridha ,. I looked at your error again more closely, and I realize that `docker: invalid reference format.` is not referring to the FASTA file. . It is something to do with your command, which doesn't have the correct format of how you would use Docker. For example, if I run:. ```bash. sudo docker run google/deepvariant:1.6.1 /opt/deepvariant/bin/run_deepvariant --helpshort. ```. which shows me the flags. But if I run something similar to your command:. ```bash. sudo docker run google/deepvariant:{BIN_VERSION=""1.6.1""} /opt/deepvariant/bin/run_deepvariant --helpshort. ```. It gave me this error:. ```. docker: invalid reference format. See 'docker run --help'. ```. I think there might be other formatting errors in your command. My suggestion: Can you make sure you follow our Quick Start step-by-step first? And, once that succeed, please make sure you use a command similar to https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-quick-start.md#run-deepvariant-with-one-command.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/829
https://github.com/google/deepvariant/issues/829:188,usability,command,command,188,"Hi @Jyoti-Mridha ,. I looked at your error again more closely, and I realize that `docker: invalid reference format.` is not referring to the FASTA file. . It is something to do with your command, which doesn't have the correct format of how you would use Docker. For example, if I run:. ```bash. sudo docker run google/deepvariant:1.6.1 /opt/deepvariant/bin/run_deepvariant --helpshort. ```. which shows me the flags. But if I run something similar to your command:. ```bash. sudo docker run google/deepvariant:{BIN_VERSION=""1.6.1""} /opt/deepvariant/bin/run_deepvariant --helpshort. ```. It gave me this error:. ```. docker: invalid reference format. See 'docker run --help'. ```. I think there might be other formatting errors in your command. My suggestion: Can you make sure you follow our Quick Start step-by-step first? And, once that succeed, please make sure you use a command similar to https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-quick-start.md#run-deepvariant-with-one-command.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/829
https://github.com/google/deepvariant/issues/829:377,usability,help,helpshort,377,"Hi @Jyoti-Mridha ,. I looked at your error again more closely, and I realize that `docker: invalid reference format.` is not referring to the FASTA file. . It is something to do with your command, which doesn't have the correct format of how you would use Docker. For example, if I run:. ```bash. sudo docker run google/deepvariant:1.6.1 /opt/deepvariant/bin/run_deepvariant --helpshort. ```. which shows me the flags. But if I run something similar to your command:. ```bash. sudo docker run google/deepvariant:{BIN_VERSION=""1.6.1""} /opt/deepvariant/bin/run_deepvariant --helpshort. ```. It gave me this error:. ```. docker: invalid reference format. See 'docker run --help'. ```. I think there might be other formatting errors in your command. My suggestion: Can you make sure you follow our Quick Start step-by-step first? And, once that succeed, please make sure you use a command similar to https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-quick-start.md#run-deepvariant-with-one-command.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/829
https://github.com/google/deepvariant/issues/829:458,usability,command,command,458,"Hi @Jyoti-Mridha ,. I looked at your error again more closely, and I realize that `docker: invalid reference format.` is not referring to the FASTA file. . It is something to do with your command, which doesn't have the correct format of how you would use Docker. For example, if I run:. ```bash. sudo docker run google/deepvariant:1.6.1 /opt/deepvariant/bin/run_deepvariant --helpshort. ```. which shows me the flags. But if I run something similar to your command:. ```bash. sudo docker run google/deepvariant:{BIN_VERSION=""1.6.1""} /opt/deepvariant/bin/run_deepvariant --helpshort. ```. It gave me this error:. ```. docker: invalid reference format. See 'docker run --help'. ```. I think there might be other formatting errors in your command. My suggestion: Can you make sure you follow our Quick Start step-by-step first? And, once that succeed, please make sure you use a command similar to https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-quick-start.md#run-deepvariant-with-one-command.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/829
https://github.com/google/deepvariant/issues/829:573,usability,help,helpshort,573,"Hi @Jyoti-Mridha ,. I looked at your error again more closely, and I realize that `docker: invalid reference format.` is not referring to the FASTA file. . It is something to do with your command, which doesn't have the correct format of how you would use Docker. For example, if I run:. ```bash. sudo docker run google/deepvariant:1.6.1 /opt/deepvariant/bin/run_deepvariant --helpshort. ```. which shows me the flags. But if I run something similar to your command:. ```bash. sudo docker run google/deepvariant:{BIN_VERSION=""1.6.1""} /opt/deepvariant/bin/run_deepvariant --helpshort. ```. It gave me this error:. ```. docker: invalid reference format. See 'docker run --help'. ```. I think there might be other formatting errors in your command. My suggestion: Can you make sure you follow our Quick Start step-by-step first? And, once that succeed, please make sure you use a command similar to https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-quick-start.md#run-deepvariant-with-one-command.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/829
https://github.com/google/deepvariant/issues/829:605,usability,error,error,605,"Hi @Jyoti-Mridha ,. I looked at your error again more closely, and I realize that `docker: invalid reference format.` is not referring to the FASTA file. . It is something to do with your command, which doesn't have the correct format of how you would use Docker. For example, if I run:. ```bash. sudo docker run google/deepvariant:1.6.1 /opt/deepvariant/bin/run_deepvariant --helpshort. ```. which shows me the flags. But if I run something similar to your command:. ```bash. sudo docker run google/deepvariant:{BIN_VERSION=""1.6.1""} /opt/deepvariant/bin/run_deepvariant --helpshort. ```. It gave me this error:. ```. docker: invalid reference format. See 'docker run --help'. ```. I think there might be other formatting errors in your command. My suggestion: Can you make sure you follow our Quick Start step-by-step first? And, once that succeed, please make sure you use a command similar to https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-quick-start.md#run-deepvariant-with-one-command.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/829
https://github.com/google/deepvariant/issues/829:670,usability,help,help,670,"Hi @Jyoti-Mridha ,. I looked at your error again more closely, and I realize that `docker: invalid reference format.` is not referring to the FASTA file. . It is something to do with your command, which doesn't have the correct format of how you would use Docker. For example, if I run:. ```bash. sudo docker run google/deepvariant:1.6.1 /opt/deepvariant/bin/run_deepvariant --helpshort. ```. which shows me the flags. But if I run something similar to your command:. ```bash. sudo docker run google/deepvariant:{BIN_VERSION=""1.6.1""} /opt/deepvariant/bin/run_deepvariant --helpshort. ```. It gave me this error:. ```. docker: invalid reference format. See 'docker run --help'. ```. I think there might be other formatting errors in your command. My suggestion: Can you make sure you follow our Quick Start step-by-step first? And, once that succeed, please make sure you use a command similar to https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-quick-start.md#run-deepvariant-with-one-command.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/829
https://github.com/google/deepvariant/issues/829:722,usability,error,errors,722,"Hi @Jyoti-Mridha ,. I looked at your error again more closely, and I realize that `docker: invalid reference format.` is not referring to the FASTA file. . It is something to do with your command, which doesn't have the correct format of how you would use Docker. For example, if I run:. ```bash. sudo docker run google/deepvariant:1.6.1 /opt/deepvariant/bin/run_deepvariant --helpshort. ```. which shows me the flags. But if I run something similar to your command:. ```bash. sudo docker run google/deepvariant:{BIN_VERSION=""1.6.1""} /opt/deepvariant/bin/run_deepvariant --helpshort. ```. It gave me this error:. ```. docker: invalid reference format. See 'docker run --help'. ```. I think there might be other formatting errors in your command. My suggestion: Can you make sure you follow our Quick Start step-by-step first? And, once that succeed, please make sure you use a command similar to https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-quick-start.md#run-deepvariant-with-one-command.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/829
https://github.com/google/deepvariant/issues/829:737,usability,command,command,737,"Hi @Jyoti-Mridha ,. I looked at your error again more closely, and I realize that `docker: invalid reference format.` is not referring to the FASTA file. . It is something to do with your command, which doesn't have the correct format of how you would use Docker. For example, if I run:. ```bash. sudo docker run google/deepvariant:1.6.1 /opt/deepvariant/bin/run_deepvariant --helpshort. ```. which shows me the flags. But if I run something similar to your command:. ```bash. sudo docker run google/deepvariant:{BIN_VERSION=""1.6.1""} /opt/deepvariant/bin/run_deepvariant --helpshort. ```. It gave me this error:. ```. docker: invalid reference format. See 'docker run --help'. ```. I think there might be other formatting errors in your command. My suggestion: Can you make sure you follow our Quick Start step-by-step first? And, once that succeed, please make sure you use a command similar to https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-quick-start.md#run-deepvariant-with-one-command.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/829
https://github.com/google/deepvariant/issues/829:877,usability,command,command,877,"Hi @Jyoti-Mridha ,. I looked at your error again more closely, and I realize that `docker: invalid reference format.` is not referring to the FASTA file. . It is something to do with your command, which doesn't have the correct format of how you would use Docker. For example, if I run:. ```bash. sudo docker run google/deepvariant:1.6.1 /opt/deepvariant/bin/run_deepvariant --helpshort. ```. which shows me the flags. But if I run something similar to your command:. ```bash. sudo docker run google/deepvariant:{BIN_VERSION=""1.6.1""} /opt/deepvariant/bin/run_deepvariant --helpshort. ```. It gave me this error:. ```. docker: invalid reference format. See 'docker run --help'. ```. I think there might be other formatting errors in your command. My suggestion: Can you make sure you follow our Quick Start step-by-step first? And, once that succeed, please make sure you use a command similar to https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-quick-start.md#run-deepvariant-with-one-command.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/829
https://github.com/google/deepvariant/issues/829:1003,usability,command,command,1003,"Hi @Jyoti-Mridha ,. I looked at your error again more closely, and I realize that `docker: invalid reference format.` is not referring to the FASTA file. . It is something to do with your command, which doesn't have the correct format of how you would use Docker. For example, if I run:. ```bash. sudo docker run google/deepvariant:1.6.1 /opt/deepvariant/bin/run_deepvariant --helpshort. ```. which shows me the flags. But if I run something similar to your command:. ```bash. sudo docker run google/deepvariant:{BIN_VERSION=""1.6.1""} /opt/deepvariant/bin/run_deepvariant --helpshort. ```. It gave me this error:. ```. docker: invalid reference format. See 'docker run --help'. ```. I think there might be other formatting errors in your command. My suggestion: Can you make sure you follow our Quick Start step-by-step first? And, once that succeed, please make sure you use a command similar to https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-quick-start.md#run-deepvariant-with-one-command.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/829
https://github.com/google/deepvariant/issues/829:225,interoperability,specif,specified,225,"And, just in case the documentation isn't clear:. This part:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. ... ```. The variable BIN_VERSION was specified in earlier in the steps:. ```. BIN_VERSION=""1.6.1"". ```. So, in Unix command it's equivalent to:. ```. google/deepvariant:""1.6.1"" \. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/829
https://github.com/google/deepvariant/issues/829:200,modifiability,variab,variable,200,"And, just in case the documentation isn't clear:. This part:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. ... ```. The variable BIN_VERSION was specified in earlier in the steps:. ```. BIN_VERSION=""1.6.1"". ```. So, in Unix command it's equivalent to:. ```. google/deepvariant:""1.6.1"" \. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/829
https://github.com/google/deepvariant/issues/829:106,safety,input,input,106,"And, just in case the documentation isn't clear:. This part:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. ... ```. The variable BIN_VERSION was specified in earlier in the steps:. ```. BIN_VERSION=""1.6.1"". ```. So, in Unix command it's equivalent to:. ```. google/deepvariant:""1.6.1"" \. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/829
https://github.com/google/deepvariant/issues/829:22,usability,document,documentation,22,"And, just in case the documentation isn't clear:. This part:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. ... ```. The variable BIN_VERSION was specified in earlier in the steps:. ```. BIN_VERSION=""1.6.1"". ```. So, in Unix command it's equivalent to:. ```. google/deepvariant:""1.6.1"" \. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/829
https://github.com/google/deepvariant/issues/829:42,usability,clear,clear,42,"And, just in case the documentation isn't clear:. This part:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. ... ```. The variable BIN_VERSION was specified in earlier in the steps:. ```. BIN_VERSION=""1.6.1"". ```. So, in Unix command it's equivalent to:. ```. google/deepvariant:""1.6.1"" \. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/829
https://github.com/google/deepvariant/issues/829:106,usability,input,input,106,"And, just in case the documentation isn't clear:. This part:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. ... ```. The variable BIN_VERSION was specified in earlier in the steps:. ```. BIN_VERSION=""1.6.1"". ```. So, in Unix command it's equivalent to:. ```. google/deepvariant:""1.6.1"" \. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/829
https://github.com/google/deepvariant/issues/829:304,usability,command,command,304,"And, just in case the documentation isn't clear:. This part:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. ... ```. The variable BIN_VERSION was specified in earlier in the steps:. ```. BIN_VERSION=""1.6.1"". ```. So, in Unix command it's equivalent to:. ```. google/deepvariant:""1.6.1"" \. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/829
https://github.com/google/deepvariant/issues/829:171,availability,error,error,171,"Hi,. Thanks a lot for your immediate response, i have followed the above. instructions given by you, now the docker command is running fine, but I. have come across a new error. *[E::hts_open_format] Failed to open file. ""/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result"". : Is a directory*. *ValueError: UNKNOWN: Could not open variants_path:. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result*. here i have used the same path for docker run (-v. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result). and also same path for run_deepvariant (--output_vcf. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result. ) , Here i am not able to rectify what minor error in my command, i am. following the same pattern mentioned in the link shared, i might be. missing out something minute. Here is my command which is used. sudo docker run -v. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3. -v. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result. google/deepvariant:1.6.1 /opt/deepvariant/bin/run_deepvariant --reads. /media/manish/Da",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/829
https://github.com/google/deepvariant/issues/829:1131,availability,error,error,1131," fine, but I. have come across a new error. *[E::hts_open_format] Failed to open file. ""/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result"". : Is a directory*. *ValueError: UNKNOWN: Could not open variants_path:. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result*. here i have used the same path for docker run (-v. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result). and also same path for run_deepvariant (--output_vcf. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result. ) , Here i am not able to rectify what minor error in my command, i am. following the same pattern mentioned in the link shared, i might be. missing out something minute. Here is my command which is used. sudo docker run -v. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3. -v. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result. google/deepvariant:1.6.1 /opt/deepvariant/bin/run_deepvariant --reads. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minim",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/829
https://github.com/google/deepvariant/issues/829:200,deployability,Fail,Failed,200,"Hi,. Thanks a lot for your immediate response, i have followed the above. instructions given by you, now the docker command is running fine, but I. have come across a new error. *[E::hts_open_format] Failed to open file. ""/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result"". : Is a directory*. *ValueError: UNKNOWN: Could not open variants_path:. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result*. here i have used the same path for docker run (-v. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result). and also same path for run_deepvariant (--output_vcf. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result. ) , Here i am not able to rectify what minor error in my command, i am. following the same pattern mentioned in the link shared, i might be. missing out something minute. Here is my command which is used. sudo docker run -v. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3. -v. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result. google/deepvariant:1.6.1 /opt/deepvariant/bin/run_deepvariant --reads. /media/manish/Da",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/829
https://github.com/google/deepvariant/issues/829:3398,integrability,Messag,Message,3398,"ap2_freebayes_Final/Direct_minimap/Bam/run3:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3. -v. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result. google/deepvariant:1.6.1 /opt/deepvariant/bin/run_deepvariant --reads. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3/MITO60_sorted.bam. --ref. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3/hg38_chrM.fa. --report_title MITO60_Stats --sample_name MITO60 --output_vcf. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result. --model_type ONT_R104. On Wed, Jun 12, 2024 at 12:03 PM Pi-Chuan Chang ***@***.***>. wrote:. > And, just in case the documentation isn't clear:. >. > This part:. >. > sudo docker run \. > -v ""${INPUT_DIR}"":""/input"" \. > -v ""${OUTPUT_DIR}"":""/output"" \. > google/deepvariant:""${BIN_VERSION}"" \. > ... >. > The variable BIN_VERSION was specified in earlier in the steps:. >. > BIN_VERSION=""1.6.1"". >. > So, in Unix command it's equivalent to:. >. > google/deepvariant:""1.6.1"" \. >. > —. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/829#issuecomment-2162210763>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/BDQL2ZE35INJI4WP7BHRNK3ZG7TVPAVCNFSM6AAAAABJFTFWYKVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDCNRSGIYTANZWGM>. > . > You are receiving this because you were mentioned.Message ID:. > ***@***.***>. >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/829
https://github.com/google/deepvariant/issues/829:1207,interoperability,share,shared,1207,"open file. ""/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result"". : Is a directory*. *ValueError: UNKNOWN: Could not open variants_path:. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result*. here i have used the same path for docker run (-v. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result). and also same path for run_deepvariant (--output_vcf. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result. ) , Here i am not able to rectify what minor error in my command, i am. following the same pattern mentioned in the link shared, i might be. missing out something minute. Here is my command which is used. sudo docker run -v. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3. -v. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result. google/deepvariant:1.6.1 /opt/deepvariant/bin/run_deepvariant --reads. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3/MITO60_sorted.bam. --ref. /media/manish/Data/Jyoti_Mridha_AIC/Na",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/829
https://github.com/google/deepvariant/issues/829:2881,interoperability,specif,specified,2881,"ap2_freebayes_Final/Direct_minimap/Bam/run3:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3. -v. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result. google/deepvariant:1.6.1 /opt/deepvariant/bin/run_deepvariant --reads. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3/MITO60_sorted.bam. --ref. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3/hg38_chrM.fa. --report_title MITO60_Stats --sample_name MITO60 --output_vcf. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result. --model_type ONT_R104. On Wed, Jun 12, 2024 at 12:03 PM Pi-Chuan Chang ***@***.***>. wrote:. > And, just in case the documentation isn't clear:. >. > This part:. >. > sudo docker run \. > -v ""${INPUT_DIR}"":""/input"" \. > -v ""${OUTPUT_DIR}"":""/output"" \. > google/deepvariant:""${BIN_VERSION}"" \. > ... >. > The variable BIN_VERSION was specified in earlier in the steps:. >. > BIN_VERSION=""1.6.1"". >. > So, in Unix command it's equivalent to:. >. > google/deepvariant:""1.6.1"" \. >. > —. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/829#issuecomment-2162210763>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/BDQL2ZE35INJI4WP7BHRNK3ZG7TVPAVCNFSM6AAAAABJFTFWYKVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDCNRSGIYTANZWGM>. > . > You are receiving this because you were mentioned.Message ID:. > ***@***.***>. >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/829
https://github.com/google/deepvariant/issues/829:3398,interoperability,Messag,Message,3398,"ap2_freebayes_Final/Direct_minimap/Bam/run3:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3. -v. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result. google/deepvariant:1.6.1 /opt/deepvariant/bin/run_deepvariant --reads. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3/MITO60_sorted.bam. --ref. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3/hg38_chrM.fa. --report_title MITO60_Stats --sample_name MITO60 --output_vcf. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result. --model_type ONT_R104. On Wed, Jun 12, 2024 at 12:03 PM Pi-Chuan Chang ***@***.***>. wrote:. > And, just in case the documentation isn't clear:. >. > This part:. >. > sudo docker run \. > -v ""${INPUT_DIR}"":""/input"" \. > -v ""${OUTPUT_DIR}"":""/output"" \. > google/deepvariant:""${BIN_VERSION}"" \. > ... >. > The variable BIN_VERSION was specified in earlier in the steps:. >. > BIN_VERSION=""1.6.1"". >. > So, in Unix command it's equivalent to:. >. > google/deepvariant:""1.6.1"" \. >. > —. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/829#issuecomment-2162210763>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/BDQL2ZE35INJI4WP7BHRNK3ZG7TVPAVCNFSM6AAAAABJFTFWYKVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDCNRSGIYTANZWGM>. > . > You are receiving this because you were mentioned.Message ID:. > ***@***.***>. >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/829
https://github.com/google/deepvariant/issues/829:2856,modifiability,variab,variable,2856,"ap2_freebayes_Final/Direct_minimap/Bam/run3:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3. -v. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result. google/deepvariant:1.6.1 /opt/deepvariant/bin/run_deepvariant --reads. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3/MITO60_sorted.bam. --ref. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3/hg38_chrM.fa. --report_title MITO60_Stats --sample_name MITO60 --output_vcf. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result. --model_type ONT_R104. On Wed, Jun 12, 2024 at 12:03 PM Pi-Chuan Chang ***@***.***>. wrote:. > And, just in case the documentation isn't clear:. >. > This part:. >. > sudo docker run \. > -v ""${INPUT_DIR}"":""/input"" \. > -v ""${OUTPUT_DIR}"":""/output"" \. > google/deepvariant:""${BIN_VERSION}"" \. > ... >. > The variable BIN_VERSION was specified in earlier in the steps:. >. > BIN_VERSION=""1.6.1"". >. > So, in Unix command it's equivalent to:. >. > google/deepvariant:""1.6.1"" \. >. > —. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/829#issuecomment-2162210763>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/BDQL2ZE35INJI4WP7BHRNK3ZG7TVPAVCNFSM6AAAAABJFTFWYKVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDCNRSGIYTANZWGM>. > . > You are receiving this because you were mentioned.Message ID:. > ***@***.***>. >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/829
https://github.com/google/deepvariant/issues/829:171,performance,error,error,171,"Hi,. Thanks a lot for your immediate response, i have followed the above. instructions given by you, now the docker command is running fine, but I. have come across a new error. *[E::hts_open_format] Failed to open file. ""/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result"". : Is a directory*. *ValueError: UNKNOWN: Could not open variants_path:. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result*. here i have used the same path for docker run (-v. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result). and also same path for run_deepvariant (--output_vcf. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result. ) , Here i am not able to rectify what minor error in my command, i am. following the same pattern mentioned in the link shared, i might be. missing out something minute. Here is my command which is used. sudo docker run -v. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3. -v. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result. google/deepvariant:1.6.1 /opt/deepvariant/bin/run_deepvariant --reads. /media/manish/Da",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/829
https://github.com/google/deepvariant/issues/829:1131,performance,error,error,1131," fine, but I. have come across a new error. *[E::hts_open_format] Failed to open file. ""/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result"". : Is a directory*. *ValueError: UNKNOWN: Could not open variants_path:. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result*. here i have used the same path for docker run (-v. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result). and also same path for run_deepvariant (--output_vcf. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result. ) , Here i am not able to rectify what minor error in my command, i am. following the same pattern mentioned in the link shared, i might be. missing out something minute. Here is my command which is used. sudo docker run -v. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3. -v. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result. google/deepvariant:1.6.1 /opt/deepvariant/bin/run_deepvariant --reads. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minim",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/829
https://github.com/google/deepvariant/issues/829:200,reliability,Fail,Failed,200,"Hi,. Thanks a lot for your immediate response, i have followed the above. instructions given by you, now the docker command is running fine, but I. have come across a new error. *[E::hts_open_format] Failed to open file. ""/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result"". : Is a directory*. *ValueError: UNKNOWN: Could not open variants_path:. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result*. here i have used the same path for docker run (-v. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result). and also same path for run_deepvariant (--output_vcf. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result. ) , Here i am not able to rectify what minor error in my command, i am. following the same pattern mentioned in the link shared, i might be. missing out something minute. Here is my command which is used. sudo docker run -v. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3. -v. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result. google/deepvariant:1.6.1 /opt/deepvariant/bin/run_deepvariant --reads. /media/manish/Da",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/829
https://github.com/google/deepvariant/issues/829:171,safety,error,error,171,"Hi,. Thanks a lot for your immediate response, i have followed the above. instructions given by you, now the docker command is running fine, but I. have come across a new error. *[E::hts_open_format] Failed to open file. ""/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result"". : Is a directory*. *ValueError: UNKNOWN: Could not open variants_path:. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result*. here i have used the same path for docker run (-v. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result). and also same path for run_deepvariant (--output_vcf. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result. ) , Here i am not able to rectify what minor error in my command, i am. following the same pattern mentioned in the link shared, i might be. missing out something minute. Here is my command which is used. sudo docker run -v. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3. -v. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result. google/deepvariant:1.6.1 /opt/deepvariant/bin/run_deepvariant --reads. /media/manish/Da",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/829
https://github.com/google/deepvariant/issues/829:1131,safety,error,error,1131," fine, but I. have come across a new error. *[E::hts_open_format] Failed to open file. ""/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result"". : Is a directory*. *ValueError: UNKNOWN: Could not open variants_path:. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result*. here i have used the same path for docker run (-v. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result). and also same path for run_deepvariant (--output_vcf. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result. ) , Here i am not able to rectify what minor error in my command, i am. following the same pattern mentioned in the link shared, i might be. missing out something minute. Here is my command which is used. sudo docker run -v. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3. -v. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result. google/deepvariant:1.6.1 /opt/deepvariant/bin/run_deepvariant --reads. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minim",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/829
https://github.com/google/deepvariant/issues/829:2756,safety,input,input,2756,"ap2_freebayes_Final/Direct_minimap/Bam/run3:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3. -v. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result. google/deepvariant:1.6.1 /opt/deepvariant/bin/run_deepvariant --reads. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3/MITO60_sorted.bam. --ref. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3/hg38_chrM.fa. --report_title MITO60_Stats --sample_name MITO60 --output_vcf. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result. --model_type ONT_R104. On Wed, Jun 12, 2024 at 12:03 PM Pi-Chuan Chang ***@***.***>. wrote:. > And, just in case the documentation isn't clear:. >. > This part:. >. > sudo docker run \. > -v ""${INPUT_DIR}"":""/input"" \. > -v ""${OUTPUT_DIR}"":""/output"" \. > google/deepvariant:""${BIN_VERSION}"" \. > ... >. > The variable BIN_VERSION was specified in earlier in the steps:. >. > BIN_VERSION=""1.6.1"". >. > So, in Unix command it's equivalent to:. >. > google/deepvariant:""1.6.1"" \. >. > —. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/829#issuecomment-2162210763>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/BDQL2ZE35INJI4WP7BHRNK3ZG7TVPAVCNFSM6AAAAABJFTFWYKVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDCNRSGIYTANZWGM>. > . > You are receiving this because you were mentioned.Message ID:. > ***@***.***>. >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/829
https://github.com/google/deepvariant/issues/829:3228,security,auth,auth,3228,"ap2_freebayes_Final/Direct_minimap/Bam/run3:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3. -v. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result. google/deepvariant:1.6.1 /opt/deepvariant/bin/run_deepvariant --reads. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3/MITO60_sorted.bam. --ref. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3/hg38_chrM.fa. --report_title MITO60_Stats --sample_name MITO60 --output_vcf. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result. --model_type ONT_R104. On Wed, Jun 12, 2024 at 12:03 PM Pi-Chuan Chang ***@***.***>. wrote:. > And, just in case the documentation isn't clear:. >. > This part:. >. > sudo docker run \. > -v ""${INPUT_DIR}"":""/input"" \. > -v ""${OUTPUT_DIR}"":""/output"" \. > google/deepvariant:""${BIN_VERSION}"" \. > ... >. > The variable BIN_VERSION was specified in earlier in the steps:. >. > BIN_VERSION=""1.6.1"". >. > So, in Unix command it's equivalent to:. >. > google/deepvariant:""1.6.1"" \. >. > —. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/829#issuecomment-2162210763>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/BDQL2ZE35INJI4WP7BHRNK3ZG7TVPAVCNFSM6AAAAABJFTFWYKVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDCNRSGIYTANZWGM>. > . > You are receiving this because you were mentioned.Message ID:. > ***@***.***>. >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/829
https://github.com/google/deepvariant/issues/829:116,usability,command,command,116,"Hi,. Thanks a lot for your immediate response, i have followed the above. instructions given by you, now the docker command is running fine, but I. have come across a new error. *[E::hts_open_format] Failed to open file. ""/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result"". : Is a directory*. *ValueError: UNKNOWN: Could not open variants_path:. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result*. here i have used the same path for docker run (-v. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result). and also same path for run_deepvariant (--output_vcf. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result. ) , Here i am not able to rectify what minor error in my command, i am. following the same pattern mentioned in the link shared, i might be. missing out something minute. Here is my command which is used. sudo docker run -v. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3. -v. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result. google/deepvariant:1.6.1 /opt/deepvariant/bin/run_deepvariant --reads. /media/manish/Da",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/829
https://github.com/google/deepvariant/issues/829:171,usability,error,error,171,"Hi,. Thanks a lot for your immediate response, i have followed the above. instructions given by you, now the docker command is running fine, but I. have come across a new error. *[E::hts_open_format] Failed to open file. ""/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result"". : Is a directory*. *ValueError: UNKNOWN: Could not open variants_path:. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result*. here i have used the same path for docker run (-v. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result). and also same path for run_deepvariant (--output_vcf. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result. ) , Here i am not able to rectify what minor error in my command, i am. following the same pattern mentioned in the link shared, i might be. missing out something minute. Here is my command which is used. sudo docker run -v. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3. -v. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result. google/deepvariant:1.6.1 /opt/deepvariant/bin/run_deepvariant --reads. /media/manish/Da",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/829
https://github.com/google/deepvariant/issues/829:1131,usability,error,error,1131," fine, but I. have come across a new error. *[E::hts_open_format] Failed to open file. ""/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result"". : Is a directory*. *ValueError: UNKNOWN: Could not open variants_path:. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result*. here i have used the same path for docker run (-v. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result). and also same path for run_deepvariant (--output_vcf. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result. ) , Here i am not able to rectify what minor error in my command, i am. following the same pattern mentioned in the link shared, i might be. missing out something minute. Here is my command which is used. sudo docker run -v. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3. -v. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result. google/deepvariant:1.6.1 /opt/deepvariant/bin/run_deepvariant --reads. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minim",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/829
https://github.com/google/deepvariant/issues/829:1143,usability,command,command,1143," have come across a new error. *[E::hts_open_format] Failed to open file. ""/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result"". : Is a directory*. *ValueError: UNKNOWN: Could not open variants_path:. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result*. here i have used the same path for docker run (-v. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result). and also same path for run_deepvariant (--output_vcf. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result. ) , Here i am not able to rectify what minor error in my command, i am. following the same pattern mentioned in the link shared, i might be. missing out something minute. Here is my command which is used. sudo docker run -v. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3. -v. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result. google/deepvariant:1.6.1 /opt/deepvariant/bin/run_deepvariant --reads. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3/M",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/829
https://github.com/google/deepvariant/issues/829:1268,usability,command,command,1268,"ncing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result"". : Is a directory*. *ValueError: UNKNOWN: Could not open variants_path:. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result*. here i have used the same path for docker run (-v. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result). and also same path for run_deepvariant (--output_vcf. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result. ) , Here i am not able to rectify what minor error in my command, i am. following the same pattern mentioned in the link shared, i might be. missing out something minute. Here is my command which is used. sudo docker run -v. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3. -v. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result. google/deepvariant:1.6.1 /opt/deepvariant/bin/run_deepvariant --reads. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3/MITO60_sorted.bam. --ref. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Ba",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/829
https://github.com/google/deepvariant/issues/829:2665,usability,document,documentation,2665,"ap2_freebayes_Final/Direct_minimap/Bam/run3:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3. -v. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result. google/deepvariant:1.6.1 /opt/deepvariant/bin/run_deepvariant --reads. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3/MITO60_sorted.bam. --ref. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3/hg38_chrM.fa. --report_title MITO60_Stats --sample_name MITO60 --output_vcf. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result. --model_type ONT_R104. On Wed, Jun 12, 2024 at 12:03 PM Pi-Chuan Chang ***@***.***>. wrote:. > And, just in case the documentation isn't clear:. >. > This part:. >. > sudo docker run \. > -v ""${INPUT_DIR}"":""/input"" \. > -v ""${OUTPUT_DIR}"":""/output"" \. > google/deepvariant:""${BIN_VERSION}"" \. > ... >. > The variable BIN_VERSION was specified in earlier in the steps:. >. > BIN_VERSION=""1.6.1"". >. > So, in Unix command it's equivalent to:. >. > google/deepvariant:""1.6.1"" \. >. > —. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/829#issuecomment-2162210763>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/BDQL2ZE35INJI4WP7BHRNK3ZG7TVPAVCNFSM6AAAAABJFTFWYKVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDCNRSGIYTANZWGM>. > . > You are receiving this because you were mentioned.Message ID:. > ***@***.***>. >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/829
https://github.com/google/deepvariant/issues/829:2685,usability,clear,clear,2685,"ap2_freebayes_Final/Direct_minimap/Bam/run3:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3. -v. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result. google/deepvariant:1.6.1 /opt/deepvariant/bin/run_deepvariant --reads. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3/MITO60_sorted.bam. --ref. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3/hg38_chrM.fa. --report_title MITO60_Stats --sample_name MITO60 --output_vcf. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result. --model_type ONT_R104. On Wed, Jun 12, 2024 at 12:03 PM Pi-Chuan Chang ***@***.***>. wrote:. > And, just in case the documentation isn't clear:. >. > This part:. >. > sudo docker run \. > -v ""${INPUT_DIR}"":""/input"" \. > -v ""${OUTPUT_DIR}"":""/output"" \. > google/deepvariant:""${BIN_VERSION}"" \. > ... >. > The variable BIN_VERSION was specified in earlier in the steps:. >. > BIN_VERSION=""1.6.1"". >. > So, in Unix command it's equivalent to:. >. > google/deepvariant:""1.6.1"" \. >. > —. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/829#issuecomment-2162210763>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/BDQL2ZE35INJI4WP7BHRNK3ZG7TVPAVCNFSM6AAAAABJFTFWYKVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDCNRSGIYTANZWGM>. > . > You are receiving this because you were mentioned.Message ID:. > ***@***.***>. >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/829
https://github.com/google/deepvariant/issues/829:2756,usability,input,input,2756,"ap2_freebayes_Final/Direct_minimap/Bam/run3:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3. -v. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result. google/deepvariant:1.6.1 /opt/deepvariant/bin/run_deepvariant --reads. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3/MITO60_sorted.bam. --ref. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3/hg38_chrM.fa. --report_title MITO60_Stats --sample_name MITO60 --output_vcf. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result. --model_type ONT_R104. On Wed, Jun 12, 2024 at 12:03 PM Pi-Chuan Chang ***@***.***>. wrote:. > And, just in case the documentation isn't clear:. >. > This part:. >. > sudo docker run \. > -v ""${INPUT_DIR}"":""/input"" \. > -v ""${OUTPUT_DIR}"":""/output"" \. > google/deepvariant:""${BIN_VERSION}"" \. > ... >. > The variable BIN_VERSION was specified in earlier in the steps:. >. > BIN_VERSION=""1.6.1"". >. > So, in Unix command it's equivalent to:. >. > google/deepvariant:""1.6.1"" \. >. > —. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/829#issuecomment-2162210763>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/BDQL2ZE35INJI4WP7BHRNK3ZG7TVPAVCNFSM6AAAAABJFTFWYKVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDCNRSGIYTANZWGM>. > . > You are receiving this because you were mentioned.Message ID:. > ***@***.***>. >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/829
https://github.com/google/deepvariant/issues/829:2960,usability,command,command,2960,"ap2_freebayes_Final/Direct_minimap/Bam/run3:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3. -v. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result. google/deepvariant:1.6.1 /opt/deepvariant/bin/run_deepvariant --reads. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3/MITO60_sorted.bam. --ref. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3/hg38_chrM.fa. --report_title MITO60_Stats --sample_name MITO60 --output_vcf. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result. --model_type ONT_R104. On Wed, Jun 12, 2024 at 12:03 PM Pi-Chuan Chang ***@***.***>. wrote:. > And, just in case the documentation isn't clear:. >. > This part:. >. > sudo docker run \. > -v ""${INPUT_DIR}"":""/input"" \. > -v ""${OUTPUT_DIR}"":""/output"" \. > google/deepvariant:""${BIN_VERSION}"" \. > ... >. > The variable BIN_VERSION was specified in earlier in the steps:. >. > BIN_VERSION=""1.6.1"". >. > So, in Unix command it's equivalent to:. >. > google/deepvariant:""1.6.1"" \. >. > —. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/829#issuecomment-2162210763>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/BDQL2ZE35INJI4WP7BHRNK3ZG7TVPAVCNFSM6AAAAABJFTFWYKVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDCNRSGIYTANZWGM>. > . > You are receiving this because you were mentioned.Message ID:. > ***@***.***>. >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/829
https://github.com/google/deepvariant/issues/829:1641,availability,error,error,1641,"alysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result. google/deepvariant:1.6.1 /opt/deepvariant/bin/run_deepvariant --reads. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3/MITO60_sorted.bam. --ref. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3/hg38_chrM.fa. --report_title MITO60_Stats --sample_name MITO60 --output_vcf. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result/deepvariant_MITO60.vcf.gz. --model_type ONT_R104. On Wed, Jun 12, 2024 at 1:02 PM JYOTI MRIDHA ***@***.***> wrote:. > Hi,. > Thanks a lot for your immediate response, i have followed the above. > instructions given by you, now the docker command is running fine, but I. > have come across a new error. >. > *[E::hts_open_format] Failed to open file. > ""/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result"". > : Is a directory*. >. > *ValueError: UNKNOWN: Could not open variants_path:. > /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result*. >. > here i have used the same path for docker run (-v. > /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result). > and also same path for run_deepvariant (--output_vcf. > /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result. > ) , Here i am not able to rectify what minor error in my co",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/829
https://github.com/google/deepvariant/issues/829:2630,availability,error,error,2630,"s a new error. >. > *[E::hts_open_format] Failed to open file. > ""/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result"". > : Is a directory*. >. > *ValueError: UNKNOWN: Could not open variants_path:. > /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result*. >. > here i have used the same path for docker run (-v. > /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result). > and also same path for run_deepvariant (--output_vcf. > /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result. > ) , Here i am not able to rectify what minor error in my command, i am. > following the same pattern mentioned in the link shared, i might be. > missing out something minute. >. > Here is my command which is used. >. > sudo docker run -v. > /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3. > -v. > /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result. > google/deepvariant:1.6.1 /opt/deepvariant/bin/run_deepvariant --reads. > /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_free",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/829
https://github.com/google/deepvariant/issues/829:1675,deployability,Fail,Failed,1675,"ariants_Trial/deepvariant_result. google/deepvariant:1.6.1 /opt/deepvariant/bin/run_deepvariant --reads. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3/MITO60_sorted.bam. --ref. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3/hg38_chrM.fa. --report_title MITO60_Stats --sample_name MITO60 --output_vcf. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result/deepvariant_MITO60.vcf.gz. --model_type ONT_R104. On Wed, Jun 12, 2024 at 1:02 PM JYOTI MRIDHA ***@***.***> wrote:. > Hi,. > Thanks a lot for your immediate response, i have followed the above. > instructions given by you, now the docker command is running fine, but I. > have come across a new error. >. > *[E::hts_open_format] Failed to open file. > ""/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result"". > : Is a directory*. >. > *ValueError: UNKNOWN: Could not open variants_path:. > /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result*. >. > here i have used the same path for docker run (-v. > /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result). > and also same path for run_deepvariant (--output_vcf. > /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result. > ) , Here i am not able to rectify what minor error in my command, i am. > following the same ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/829
https://github.com/google/deepvariant/issues/829:4969,integrability,Messag,Message,4969,"a/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3. > -v. > /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result. > google/deepvariant:1.6.1 /opt/deepvariant/bin/run_deepvariant --reads. > /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3/MITO60_sorted.bam. > --ref. > /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3/hg38_chrM.fa. > --report_title MITO60_Stats --sample_name MITO60 --output_vcf. > /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result. > --model_type ONT_R104. >. >. > On Wed, Jun 12, 2024 at 12:03 PM Pi-Chuan Chang ***@***.***>. > wrote:. >. >> And, just in case the documentation isn't clear:. >>. >> This part:. >>. >> sudo docker run \. >> -v ""${INPUT_DIR}"":""/input"" \. >> -v ""${OUTPUT_DIR}"":""/output"" \. >> google/deepvariant:""${BIN_VERSION}"" \. >> ... >>. >> The variable BIN_VERSION was specified in earlier in the steps:. >>. >> BIN_VERSION=""1.6.1"". >>. >> So, in Unix command it's equivalent to:. >>. >> google/deepvariant:""1.6.1"" \. >>. >> —. >> Reply to this email directly, view it on GitHub. >> <https://github.com/google/deepvariant/issues/829#issuecomment-2162210763>,. >> or unsubscribe. >> <https://github.com/notifications/unsubscribe-auth/BDQL2ZE35INJI4WP7BHRNK3ZG7TVPAVCNFSM6AAAAABJFTFWYKVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDCNRSGIYTANZWGM>. >> . >> You are receiving this because you were mentioned.Message ID:. >> ***@***.***>. >>. >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/829
https://github.com/google/deepvariant/issues/829:2708,interoperability,share,shared,2708,"h/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result"". > : Is a directory*. >. > *ValueError: UNKNOWN: Could not open variants_path:. > /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result*. >. > here i have used the same path for docker run (-v. > /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result). > and also same path for run_deepvariant (--output_vcf. > /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result. > ) , Here i am not able to rectify what minor error in my command, i am. > following the same pattern mentioned in the link shared, i might be. > missing out something minute. >. > Here is my command which is used. >. > sudo docker run -v. > /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3. > -v. > /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result. > google/deepvariant:1.6.1 /opt/deepvariant/bin/run_deepvariant --reads. > /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3/MITO60_sorted.bam. > --ref. > /media/manis",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/829
https://github.com/google/deepvariant/issues/829:4438,interoperability,specif,specified,4438,"a/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3. > -v. > /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result. > google/deepvariant:1.6.1 /opt/deepvariant/bin/run_deepvariant --reads. > /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3/MITO60_sorted.bam. > --ref. > /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3/hg38_chrM.fa. > --report_title MITO60_Stats --sample_name MITO60 --output_vcf. > /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result. > --model_type ONT_R104. >. >. > On Wed, Jun 12, 2024 at 12:03 PM Pi-Chuan Chang ***@***.***>. > wrote:. >. >> And, just in case the documentation isn't clear:. >>. >> This part:. >>. >> sudo docker run \. >> -v ""${INPUT_DIR}"":""/input"" \. >> -v ""${OUTPUT_DIR}"":""/output"" \. >> google/deepvariant:""${BIN_VERSION}"" \. >> ... >>. >> The variable BIN_VERSION was specified in earlier in the steps:. >>. >> BIN_VERSION=""1.6.1"". >>. >> So, in Unix command it's equivalent to:. >>. >> google/deepvariant:""1.6.1"" \. >>. >> —. >> Reply to this email directly, view it on GitHub. >> <https://github.com/google/deepvariant/issues/829#issuecomment-2162210763>,. >> or unsubscribe. >> <https://github.com/notifications/unsubscribe-auth/BDQL2ZE35INJI4WP7BHRNK3ZG7TVPAVCNFSM6AAAAABJFTFWYKVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDCNRSGIYTANZWGM>. >> . >> You are receiving this because you were mentioned.Message ID:. >> ***@***.***>. >>. >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/829
https://github.com/google/deepvariant/issues/829:4969,interoperability,Messag,Message,4969,"a/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3. > -v. > /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result. > google/deepvariant:1.6.1 /opt/deepvariant/bin/run_deepvariant --reads. > /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3/MITO60_sorted.bam. > --ref. > /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3/hg38_chrM.fa. > --report_title MITO60_Stats --sample_name MITO60 --output_vcf. > /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result. > --model_type ONT_R104. >. >. > On Wed, Jun 12, 2024 at 12:03 PM Pi-Chuan Chang ***@***.***>. > wrote:. >. >> And, just in case the documentation isn't clear:. >>. >> This part:. >>. >> sudo docker run \. >> -v ""${INPUT_DIR}"":""/input"" \. >> -v ""${OUTPUT_DIR}"":""/output"" \. >> google/deepvariant:""${BIN_VERSION}"" \. >> ... >>. >> The variable BIN_VERSION was specified in earlier in the steps:. >>. >> BIN_VERSION=""1.6.1"". >>. >> So, in Unix command it's equivalent to:. >>. >> google/deepvariant:""1.6.1"" \. >>. >> —. >> Reply to this email directly, view it on GitHub. >> <https://github.com/google/deepvariant/issues/829#issuecomment-2162210763>,. >> or unsubscribe. >> <https://github.com/notifications/unsubscribe-auth/BDQL2ZE35INJI4WP7BHRNK3ZG7TVPAVCNFSM6AAAAABJFTFWYKVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDCNRSGIYTANZWGM>. >> . >> You are receiving this because you were mentioned.Message ID:. >> ***@***.***>. >>. >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/829
https://github.com/google/deepvariant/issues/829:4413,modifiability,variab,variable,4413,"a/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3. > -v. > /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result. > google/deepvariant:1.6.1 /opt/deepvariant/bin/run_deepvariant --reads. > /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3/MITO60_sorted.bam. > --ref. > /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3/hg38_chrM.fa. > --report_title MITO60_Stats --sample_name MITO60 --output_vcf. > /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result. > --model_type ONT_R104. >. >. > On Wed, Jun 12, 2024 at 12:03 PM Pi-Chuan Chang ***@***.***>. > wrote:. >. >> And, just in case the documentation isn't clear:. >>. >> This part:. >>. >> sudo docker run \. >> -v ""${INPUT_DIR}"":""/input"" \. >> -v ""${OUTPUT_DIR}"":""/output"" \. >> google/deepvariant:""${BIN_VERSION}"" \. >> ... >>. >> The variable BIN_VERSION was specified in earlier in the steps:. >>. >> BIN_VERSION=""1.6.1"". >>. >> So, in Unix command it's equivalent to:. >>. >> google/deepvariant:""1.6.1"" \. >>. >> —. >> Reply to this email directly, view it on GitHub. >> <https://github.com/google/deepvariant/issues/829#issuecomment-2162210763>,. >> or unsubscribe. >> <https://github.com/notifications/unsubscribe-auth/BDQL2ZE35INJI4WP7BHRNK3ZG7TVPAVCNFSM6AAAAABJFTFWYKVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDCNRSGIYTANZWGM>. >> . >> You are receiving this because you were mentioned.Message ID:. >> ***@***.***>. >>. >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/829
https://github.com/google/deepvariant/issues/829:1641,performance,error,error,1641,"alysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result. google/deepvariant:1.6.1 /opt/deepvariant/bin/run_deepvariant --reads. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3/MITO60_sorted.bam. --ref. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3/hg38_chrM.fa. --report_title MITO60_Stats --sample_name MITO60 --output_vcf. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result/deepvariant_MITO60.vcf.gz. --model_type ONT_R104. On Wed, Jun 12, 2024 at 1:02 PM JYOTI MRIDHA ***@***.***> wrote:. > Hi,. > Thanks a lot for your immediate response, i have followed the above. > instructions given by you, now the docker command is running fine, but I. > have come across a new error. >. > *[E::hts_open_format] Failed to open file. > ""/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result"". > : Is a directory*. >. > *ValueError: UNKNOWN: Could not open variants_path:. > /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result*. >. > here i have used the same path for docker run (-v. > /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result). > and also same path for run_deepvariant (--output_vcf. > /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result. > ) , Here i am not able to rectify what minor error in my co",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/829
https://github.com/google/deepvariant/issues/829:2630,performance,error,error,2630,"s a new error. >. > *[E::hts_open_format] Failed to open file. > ""/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result"". > : Is a directory*. >. > *ValueError: UNKNOWN: Could not open variants_path:. > /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result*. >. > here i have used the same path for docker run (-v. > /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result). > and also same path for run_deepvariant (--output_vcf. > /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result. > ) , Here i am not able to rectify what minor error in my command, i am. > following the same pattern mentioned in the link shared, i might be. > missing out something minute. >. > Here is my command which is used. >. > sudo docker run -v. > /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3. > -v. > /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result. > google/deepvariant:1.6.1 /opt/deepvariant/bin/run_deepvariant --reads. > /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_free",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/829
https://github.com/google/deepvariant/issues/829:1675,reliability,Fail,Failed,1675,"ariants_Trial/deepvariant_result. google/deepvariant:1.6.1 /opt/deepvariant/bin/run_deepvariant --reads. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3/MITO60_sorted.bam. --ref. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3/hg38_chrM.fa. --report_title MITO60_Stats --sample_name MITO60 --output_vcf. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result/deepvariant_MITO60.vcf.gz. --model_type ONT_R104. On Wed, Jun 12, 2024 at 1:02 PM JYOTI MRIDHA ***@***.***> wrote:. > Hi,. > Thanks a lot for your immediate response, i have followed the above. > instructions given by you, now the docker command is running fine, but I. > have come across a new error. >. > *[E::hts_open_format] Failed to open file. > ""/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result"". > : Is a directory*. >. > *ValueError: UNKNOWN: Could not open variants_path:. > /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result*. >. > here i have used the same path for docker run (-v. > /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result). > and also same path for run_deepvariant (--output_vcf. > /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result. > ) , Here i am not able to rectify what minor error in my command, i am. > following the same ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/829
https://github.com/google/deepvariant/issues/829:1641,safety,error,error,1641,"alysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result. google/deepvariant:1.6.1 /opt/deepvariant/bin/run_deepvariant --reads. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3/MITO60_sorted.bam. --ref. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3/hg38_chrM.fa. --report_title MITO60_Stats --sample_name MITO60 --output_vcf. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result/deepvariant_MITO60.vcf.gz. --model_type ONT_R104. On Wed, Jun 12, 2024 at 1:02 PM JYOTI MRIDHA ***@***.***> wrote:. > Hi,. > Thanks a lot for your immediate response, i have followed the above. > instructions given by you, now the docker command is running fine, but I. > have come across a new error. >. > *[E::hts_open_format] Failed to open file. > ""/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result"". > : Is a directory*. >. > *ValueError: UNKNOWN: Could not open variants_path:. > /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result*. >. > here i have used the same path for docker run (-v. > /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result). > and also same path for run_deepvariant (--output_vcf. > /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result. > ) , Here i am not able to rectify what minor error in my co",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/829
https://github.com/google/deepvariant/issues/829:2630,safety,error,error,2630,"s a new error. >. > *[E::hts_open_format] Failed to open file. > ""/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result"". > : Is a directory*. >. > *ValueError: UNKNOWN: Could not open variants_path:. > /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result*. >. > here i have used the same path for docker run (-v. > /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result). > and also same path for run_deepvariant (--output_vcf. > /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result. > ) , Here i am not able to rectify what minor error in my command, i am. > following the same pattern mentioned in the link shared, i might be. > missing out something minute. >. > Here is my command which is used. >. > sudo docker run -v. > /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3. > -v. > /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result. > google/deepvariant:1.6.1 /opt/deepvariant/bin/run_deepvariant --reads. > /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_free",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/829
https://github.com/google/deepvariant/issues/829:4308,safety,input,input,4308,"a/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3. > -v. > /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result. > google/deepvariant:1.6.1 /opt/deepvariant/bin/run_deepvariant --reads. > /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3/MITO60_sorted.bam. > --ref. > /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3/hg38_chrM.fa. > --report_title MITO60_Stats --sample_name MITO60 --output_vcf. > /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result. > --model_type ONT_R104. >. >. > On Wed, Jun 12, 2024 at 12:03 PM Pi-Chuan Chang ***@***.***>. > wrote:. >. >> And, just in case the documentation isn't clear:. >>. >> This part:. >>. >> sudo docker run \. >> -v ""${INPUT_DIR}"":""/input"" \. >> -v ""${OUTPUT_DIR}"":""/output"" \. >> google/deepvariant:""${BIN_VERSION}"" \. >> ... >>. >> The variable BIN_VERSION was specified in earlier in the steps:. >>. >> BIN_VERSION=""1.6.1"". >>. >> So, in Unix command it's equivalent to:. >>. >> google/deepvariant:""1.6.1"" \. >>. >> —. >> Reply to this email directly, view it on GitHub. >> <https://github.com/google/deepvariant/issues/829#issuecomment-2162210763>,. >> or unsubscribe. >> <https://github.com/notifications/unsubscribe-auth/BDQL2ZE35INJI4WP7BHRNK3ZG7TVPAVCNFSM6AAAAABJFTFWYKVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDCNRSGIYTANZWGM>. >> . >> You are receiving this because you were mentioned.Message ID:. >> ***@***.***>. >>. >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/829
https://github.com/google/deepvariant/issues/829:4797,security,auth,auth,4797,"a/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3. > -v. > /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result. > google/deepvariant:1.6.1 /opt/deepvariant/bin/run_deepvariant --reads. > /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3/MITO60_sorted.bam. > --ref. > /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3/hg38_chrM.fa. > --report_title MITO60_Stats --sample_name MITO60 --output_vcf. > /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result. > --model_type ONT_R104. >. >. > On Wed, Jun 12, 2024 at 12:03 PM Pi-Chuan Chang ***@***.***>. > wrote:. >. >> And, just in case the documentation isn't clear:. >>. >> This part:. >>. >> sudo docker run \. >> -v ""${INPUT_DIR}"":""/input"" \. >> -v ""${OUTPUT_DIR}"":""/output"" \. >> google/deepvariant:""${BIN_VERSION}"" \. >> ... >>. >> The variable BIN_VERSION was specified in earlier in the steps:. >>. >> BIN_VERSION=""1.6.1"". >>. >> So, in Unix command it's equivalent to:. >>. >> google/deepvariant:""1.6.1"" \. >>. >> —. >> Reply to this email directly, view it on GitHub. >> <https://github.com/google/deepvariant/issues/829#issuecomment-2162210763>,. >> or unsubscribe. >> <https://github.com/notifications/unsubscribe-auth/BDQL2ZE35INJI4WP7BHRNK3ZG7TVPAVCNFSM6AAAAABJFTFWYKVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDCNRSGIYTANZWGM>. >> . >> You are receiving this because you were mentioned.Message ID:. >> ***@***.***>. >>. >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/829
https://github.com/google/deepvariant/issues/829:33,usability,help,help,33,"Hi,. Thanks a lot for your great help, Now I am able to run the command and get. my results.👍. docker run -v. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3. -v. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result. google/deepvariant:1.6.1 /opt/deepvariant/bin/run_deepvariant --reads. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3/MITO60_sorted.bam. --ref. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3/hg38_chrM.fa. --report_title MITO60_Stats --sample_name MITO60 --output_vcf. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result/deepvariant_MITO60.vcf.gz. --model_type ONT_R104. On Wed, Jun 12, 2024 at 1:02 PM JYOTI MRIDHA ***@***.***> wrote:. > Hi,. > Thanks a lot for your immediate response, i have followed the above. > instructions given by you, now the docker command is running fine, but I. > have come across a new error. >. > *[E::hts_open_format] Failed to open file. > ""/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result"". > : Is a directory*. >. > *ValueError: UNKNOWN: Could not open variants_path:. > /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/829
https://github.com/google/deepvariant/issues/829:64,usability,command,command,64,"Hi,. Thanks a lot for your great help, Now I am able to run the command and get. my results.👍. docker run -v. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3. -v. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result. google/deepvariant:1.6.1 /opt/deepvariant/bin/run_deepvariant --reads. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3/MITO60_sorted.bam. --ref. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3/hg38_chrM.fa. --report_title MITO60_Stats --sample_name MITO60 --output_vcf. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result/deepvariant_MITO60.vcf.gz. --model_type ONT_R104. On Wed, Jun 12, 2024 at 1:02 PM JYOTI MRIDHA ***@***.***> wrote:. > Hi,. > Thanks a lot for your immediate response, i have followed the above. > instructions given by you, now the docker command is running fine, but I. > have come across a new error. >. > *[E::hts_open_format] Failed to open file. > ""/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result"". > : Is a directory*. >. > *ValueError: UNKNOWN: Could not open variants_path:. > /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/829
https://github.com/google/deepvariant/issues/829:1584,usability,command,command,1584,"/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result. google/deepvariant:1.6.1 /opt/deepvariant/bin/run_deepvariant --reads. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3/MITO60_sorted.bam. --ref. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3/hg38_chrM.fa. --report_title MITO60_Stats --sample_name MITO60 --output_vcf. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result/deepvariant_MITO60.vcf.gz. --model_type ONT_R104. On Wed, Jun 12, 2024 at 1:02 PM JYOTI MRIDHA ***@***.***> wrote:. > Hi,. > Thanks a lot for your immediate response, i have followed the above. > instructions given by you, now the docker command is running fine, but I. > have come across a new error. >. > *[E::hts_open_format] Failed to open file. > ""/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result"". > : Is a directory*. >. > *ValueError: UNKNOWN: Could not open variants_path:. > /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result*. >. > here i have used the same path for docker run (-v. > /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result). > and also same path for run_deepvariant (--output_vcf. > /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result. > ) ,",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/829
https://github.com/google/deepvariant/issues/829:1641,usability,error,error,1641,"alysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result. google/deepvariant:1.6.1 /opt/deepvariant/bin/run_deepvariant --reads. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3/MITO60_sorted.bam. --ref. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3/hg38_chrM.fa. --report_title MITO60_Stats --sample_name MITO60 --output_vcf. /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result/deepvariant_MITO60.vcf.gz. --model_type ONT_R104. On Wed, Jun 12, 2024 at 1:02 PM JYOTI MRIDHA ***@***.***> wrote:. > Hi,. > Thanks a lot for your immediate response, i have followed the above. > instructions given by you, now the docker command is running fine, but I. > have come across a new error. >. > *[E::hts_open_format] Failed to open file. > ""/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result"". > : Is a directory*. >. > *ValueError: UNKNOWN: Could not open variants_path:. > /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result*. >. > here i have used the same path for docker run (-v. > /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result). > and also same path for run_deepvariant (--output_vcf. > /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result. > ) , Here i am not able to rectify what minor error in my co",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/829
https://github.com/google/deepvariant/issues/829:2630,usability,error,error,2630,"s a new error. >. > *[E::hts_open_format] Failed to open file. > ""/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result"". > : Is a directory*. >. > *ValueError: UNKNOWN: Could not open variants_path:. > /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result*. >. > here i have used the same path for docker run (-v. > /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result). > and also same path for run_deepvariant (--output_vcf. > /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result. > ) , Here i am not able to rectify what minor error in my command, i am. > following the same pattern mentioned in the link shared, i might be. > missing out something minute. >. > Here is my command which is used. >. > sudo docker run -v. > /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3. > -v. > /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result. > google/deepvariant:1.6.1 /opt/deepvariant/bin/run_deepvariant --reads. > /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_free",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/829
https://github.com/google/deepvariant/issues/829:2642,usability,command,command,2642,". >. > *[E::hts_open_format] Failed to open file. > ""/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result"". > : Is a directory*. >. > *ValueError: UNKNOWN: Could not open variants_path:. > /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result*. >. > here i have used the same path for docker run (-v. > /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result). > and also same path for run_deepvariant (--output_vcf. > /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result. > ) , Here i am not able to rectify what minor error in my command, i am. > following the same pattern mentioned in the link shared, i might be. > missing out something minute. >. > Here is my command which is used. >. > sudo docker run -v. > /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3. > -v. > /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result. > google/deepvariant:1.6.1 /opt/deepvariant/bin/run_deepvariant --reads. > /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/D",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/829
https://github.com/google/deepvariant/issues/829:2776,usability,command,command,2776,"1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result"". > : Is a directory*. >. > *ValueError: UNKNOWN: Could not open variants_path:. > /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result*. >. > here i have used the same path for docker run (-v. > /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result). > and also same path for run_deepvariant (--output_vcf. > /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result. > ) , Here i am not able to rectify what minor error in my command, i am. > following the same pattern mentioned in the link shared, i might be. > missing out something minute. >. > Here is my command which is used. >. > sudo docker run -v. > /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3. > -v. > /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result. > google/deepvariant:1.6.1 /opt/deepvariant/bin/run_deepvariant --reads. > /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3/MITO60_sorted.bam. > --ref. > /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/829
https://github.com/google/deepvariant/issues/829:4212,usability,document,documentation,4212,"a/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3. > -v. > /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result. > google/deepvariant:1.6.1 /opt/deepvariant/bin/run_deepvariant --reads. > /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3/MITO60_sorted.bam. > --ref. > /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3/hg38_chrM.fa. > --report_title MITO60_Stats --sample_name MITO60 --output_vcf. > /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result. > --model_type ONT_R104. >. >. > On Wed, Jun 12, 2024 at 12:03 PM Pi-Chuan Chang ***@***.***>. > wrote:. >. >> And, just in case the documentation isn't clear:. >>. >> This part:. >>. >> sudo docker run \. >> -v ""${INPUT_DIR}"":""/input"" \. >> -v ""${OUTPUT_DIR}"":""/output"" \. >> google/deepvariant:""${BIN_VERSION}"" \. >> ... >>. >> The variable BIN_VERSION was specified in earlier in the steps:. >>. >> BIN_VERSION=""1.6.1"". >>. >> So, in Unix command it's equivalent to:. >>. >> google/deepvariant:""1.6.1"" \. >>. >> —. >> Reply to this email directly, view it on GitHub. >> <https://github.com/google/deepvariant/issues/829#issuecomment-2162210763>,. >> or unsubscribe. >> <https://github.com/notifications/unsubscribe-auth/BDQL2ZE35INJI4WP7BHRNK3ZG7TVPAVCNFSM6AAAAABJFTFWYKVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDCNRSGIYTANZWGM>. >> . >> You are receiving this because you were mentioned.Message ID:. >> ***@***.***>. >>. >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/829
https://github.com/google/deepvariant/issues/829:4232,usability,clear,clear,4232,"a/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3. > -v. > /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result. > google/deepvariant:1.6.1 /opt/deepvariant/bin/run_deepvariant --reads. > /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3/MITO60_sorted.bam. > --ref. > /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3/hg38_chrM.fa. > --report_title MITO60_Stats --sample_name MITO60 --output_vcf. > /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result. > --model_type ONT_R104. >. >. > On Wed, Jun 12, 2024 at 12:03 PM Pi-Chuan Chang ***@***.***>. > wrote:. >. >> And, just in case the documentation isn't clear:. >>. >> This part:. >>. >> sudo docker run \. >> -v ""${INPUT_DIR}"":""/input"" \. >> -v ""${OUTPUT_DIR}"":""/output"" \. >> google/deepvariant:""${BIN_VERSION}"" \. >> ... >>. >> The variable BIN_VERSION was specified in earlier in the steps:. >>. >> BIN_VERSION=""1.6.1"". >>. >> So, in Unix command it's equivalent to:. >>. >> google/deepvariant:""1.6.1"" \. >>. >> —. >> Reply to this email directly, view it on GitHub. >> <https://github.com/google/deepvariant/issues/829#issuecomment-2162210763>,. >> or unsubscribe. >> <https://github.com/notifications/unsubscribe-auth/BDQL2ZE35INJI4WP7BHRNK3ZG7TVPAVCNFSM6AAAAABJFTFWYKVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDCNRSGIYTANZWGM>. >> . >> You are receiving this because you were mentioned.Message ID:. >> ***@***.***>. >>. >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/829
https://github.com/google/deepvariant/issues/829:4308,usability,input,input,4308,"a/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3. > -v. > /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result. > google/deepvariant:1.6.1 /opt/deepvariant/bin/run_deepvariant --reads. > /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3/MITO60_sorted.bam. > --ref. > /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3/hg38_chrM.fa. > --report_title MITO60_Stats --sample_name MITO60 --output_vcf. > /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result. > --model_type ONT_R104. >. >. > On Wed, Jun 12, 2024 at 12:03 PM Pi-Chuan Chang ***@***.***>. > wrote:. >. >> And, just in case the documentation isn't clear:. >>. >> This part:. >>. >> sudo docker run \. >> -v ""${INPUT_DIR}"":""/input"" \. >> -v ""${OUTPUT_DIR}"":""/output"" \. >> google/deepvariant:""${BIN_VERSION}"" \. >> ... >>. >> The variable BIN_VERSION was specified in earlier in the steps:. >>. >> BIN_VERSION=""1.6.1"". >>. >> So, in Unix command it's equivalent to:. >>. >> google/deepvariant:""1.6.1"" \. >>. >> —. >> Reply to this email directly, view it on GitHub. >> <https://github.com/google/deepvariant/issues/829#issuecomment-2162210763>,. >> or unsubscribe. >> <https://github.com/notifications/unsubscribe-auth/BDQL2ZE35INJI4WP7BHRNK3ZG7TVPAVCNFSM6AAAAABJFTFWYKVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDCNRSGIYTANZWGM>. >> . >> You are receiving this because you were mentioned.Message ID:. >> ***@***.***>. >>. >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/829
https://github.com/google/deepvariant/issues/829:4521,usability,command,command,4521,"a/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3. > -v. > /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result. > google/deepvariant:1.6.1 /opt/deepvariant/bin/run_deepvariant --reads. > /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3/MITO60_sorted.bam. > --ref. > /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3/hg38_chrM.fa. > --report_title MITO60_Stats --sample_name MITO60 --output_vcf. > /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result. > --model_type ONT_R104. >. >. > On Wed, Jun 12, 2024 at 12:03 PM Pi-Chuan Chang ***@***.***>. > wrote:. >. >> And, just in case the documentation isn't clear:. >>. >> This part:. >>. >> sudo docker run \. >> -v ""${INPUT_DIR}"":""/input"" \. >> -v ""${OUTPUT_DIR}"":""/output"" \. >> google/deepvariant:""${BIN_VERSION}"" \. >> ... >>. >> The variable BIN_VERSION was specified in earlier in the steps:. >>. >> BIN_VERSION=""1.6.1"". >>. >> So, in Unix command it's equivalent to:. >>. >> google/deepvariant:""1.6.1"" \. >>. >> —. >> Reply to this email directly, view it on GitHub. >> <https://github.com/google/deepvariant/issues/829#issuecomment-2162210763>,. >> or unsubscribe. >> <https://github.com/notifications/unsubscribe-auth/BDQL2ZE35INJI4WP7BHRNK3ZG7TVPAVCNFSM6AAAAABJFTFWYKVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDCNRSGIYTANZWGM>. >> . >> You are receiving this because you were mentioned.Message ID:. >> ***@***.***>. >>. >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/829
https://github.com/google/deepvariant/issues/830:13,deployability,version,version,13,"@fellen31 ,. version 1.6.1 had a very small incremental change and the docker version was not updated. I can confirm that 1.6.1 docker has the code change required but the version update for printing the version wasn't changed. Is this a blocking issue for you? Otherwise we will update it the next version appropriately in the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:78,deployability,version,version,78,"@fellen31 ,. version 1.6.1 had a very small incremental change and the docker version was not updated. I can confirm that 1.6.1 docker has the code change required but the version update for printing the version wasn't changed. Is this a blocking issue for you? Otherwise we will update it the next version appropriately in the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:94,deployability,updat,updated,94,"@fellen31 ,. version 1.6.1 had a very small incremental change and the docker version was not updated. I can confirm that 1.6.1 docker has the code change required but the version update for printing the version wasn't changed. Is this a blocking issue for you? Otherwise we will update it the next version appropriately in the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:172,deployability,version,version,172,"@fellen31 ,. version 1.6.1 had a very small incremental change and the docker version was not updated. I can confirm that 1.6.1 docker has the code change required but the version update for printing the version wasn't changed. Is this a blocking issue for you? Otherwise we will update it the next version appropriately in the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:180,deployability,updat,update,180,"@fellen31 ,. version 1.6.1 had a very small incremental change and the docker version was not updated. I can confirm that 1.6.1 docker has the code change required but the version update for printing the version wasn't changed. Is this a blocking issue for you? Otherwise we will update it the next version appropriately in the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:204,deployability,version,version,204,"@fellen31 ,. version 1.6.1 had a very small incremental change and the docker version was not updated. I can confirm that 1.6.1 docker has the code change required but the version update for printing the version wasn't changed. Is this a blocking issue for you? Otherwise we will update it the next version appropriately in the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:280,deployability,updat,update,280,"@fellen31 ,. version 1.6.1 had a very small incremental change and the docker version was not updated. I can confirm that 1.6.1 docker has the code change required but the version update for printing the version wasn't changed. Is this a blocking issue for you? Otherwise we will update it the next version appropriately in the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:299,deployability,version,version,299,"@fellen31 ,. version 1.6.1 had a very small incremental change and the docker version was not updated. I can confirm that 1.6.1 docker has the code change required but the version update for printing the version wasn't changed. Is this a blocking issue for you? Otherwise we will update it the next version appropriately in the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:333,deployability,releas,release,333,"@fellen31 ,. version 1.6.1 had a very small incremental change and the docker version was not updated. I can confirm that 1.6.1 docker has the code change required but the version update for printing the version wasn't changed. Is this a blocking issue for you? Otherwise we will update it the next version appropriately in the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:13,integrability,version,version,13,"@fellen31 ,. version 1.6.1 had a very small incremental change and the docker version was not updated. I can confirm that 1.6.1 docker has the code change required but the version update for printing the version wasn't changed. Is this a blocking issue for you? Otherwise we will update it the next version appropriately in the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:78,integrability,version,version,78,"@fellen31 ,. version 1.6.1 had a very small incremental change and the docker version was not updated. I can confirm that 1.6.1 docker has the code change required but the version update for printing the version wasn't changed. Is this a blocking issue for you? Otherwise we will update it the next version appropriately in the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:172,integrability,version,version,172,"@fellen31 ,. version 1.6.1 had a very small incremental change and the docker version was not updated. I can confirm that 1.6.1 docker has the code change required but the version update for printing the version wasn't changed. Is this a blocking issue for you? Otherwise we will update it the next version appropriately in the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:204,integrability,version,version,204,"@fellen31 ,. version 1.6.1 had a very small incremental change and the docker version was not updated. I can confirm that 1.6.1 docker has the code change required but the version update for printing the version wasn't changed. Is this a blocking issue for you? Otherwise we will update it the next version appropriately in the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:299,integrability,version,version,299,"@fellen31 ,. version 1.6.1 had a very small incremental change and the docker version was not updated. I can confirm that 1.6.1 docker has the code change required but the version update for printing the version wasn't changed. Is this a blocking issue for you? Otherwise we will update it the next version appropriately in the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:13,modifiability,version,version,13,"@fellen31 ,. version 1.6.1 had a very small incremental change and the docker version was not updated. I can confirm that 1.6.1 docker has the code change required but the version update for printing the version wasn't changed. Is this a blocking issue for you? Otherwise we will update it the next version appropriately in the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:78,modifiability,version,version,78,"@fellen31 ,. version 1.6.1 had a very small incremental change and the docker version was not updated. I can confirm that 1.6.1 docker has the code change required but the version update for printing the version wasn't changed. Is this a blocking issue for you? Otherwise we will update it the next version appropriately in the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:172,modifiability,version,version,172,"@fellen31 ,. version 1.6.1 had a very small incremental change and the docker version was not updated. I can confirm that 1.6.1 docker has the code change required but the version update for printing the version wasn't changed. Is this a blocking issue for you? Otherwise we will update it the next version appropriately in the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:204,modifiability,version,version,204,"@fellen31 ,. version 1.6.1 had a very small incremental change and the docker version was not updated. I can confirm that 1.6.1 docker has the code change required but the version update for printing the version wasn't changed. Is this a blocking issue for you? Otherwise we will update it the next version appropriately in the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:299,modifiability,version,version,299,"@fellen31 ,. version 1.6.1 had a very small incremental change and the docker version was not updated. I can confirm that 1.6.1 docker has the code change required but the version update for printing the version wasn't changed. Is this a blocking issue for you? Otherwise we will update it the next version appropriately in the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:94,safety,updat,updated,94,"@fellen31 ,. version 1.6.1 had a very small incremental change and the docker version was not updated. I can confirm that 1.6.1 docker has the code change required but the version update for printing the version wasn't changed. Is this a blocking issue for you? Otherwise we will update it the next version appropriately in the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:180,safety,updat,update,180,"@fellen31 ,. version 1.6.1 had a very small incremental change and the docker version was not updated. I can confirm that 1.6.1 docker has the code change required but the version update for printing the version wasn't changed. Is this a blocking issue for you? Otherwise we will update it the next version appropriately in the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:280,safety,updat,update,280,"@fellen31 ,. version 1.6.1 had a very small incremental change and the docker version was not updated. I can confirm that 1.6.1 docker has the code change required but the version update for printing the version wasn't changed. Is this a blocking issue for you? Otherwise we will update it the next version appropriately in the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:94,security,updat,updated,94,"@fellen31 ,. version 1.6.1 had a very small incremental change and the docker version was not updated. I can confirm that 1.6.1 docker has the code change required but the version update for printing the version wasn't changed. Is this a blocking issue for you? Otherwise we will update it the next version appropriately in the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:180,security,updat,update,180,"@fellen31 ,. version 1.6.1 had a very small incremental change and the docker version was not updated. I can confirm that 1.6.1 docker has the code change required but the version update for printing the version wasn't changed. Is this a blocking issue for you? Otherwise we will update it the next version appropriately in the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:280,security,updat,update,280,"@fellen31 ,. version 1.6.1 had a very small incremental change and the docker version was not updated. I can confirm that 1.6.1 docker has the code change required but the version update for printing the version wasn't changed. Is this a blocking issue for you? Otherwise we will update it the next version appropriately in the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:109,usability,confirm,confirm,109,"@fellen31 ,. version 1.6.1 had a very small incremental change and the docker version was not updated. I can confirm that 1.6.1 docker has the code change required but the version update for printing the version wasn't changed. Is this a blocking issue for you? Otherwise we will update it the next version appropriately in the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:194,deployability,version,version,194,"I'd say it's mostly because the VCF also says `##DeepVariant_version=1.6.0`, which could be confusing. If it's not too much of a hassle to rebuild 1.6.1 and 1.6.1-gpu so they output the correct version, I think that would be great. Otherwise I could always refer to this issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:163,energy efficiency,gpu,gpu,163,"I'd say it's mostly because the VCF also says `##DeepVariant_version=1.6.0`, which could be confusing. If it's not too much of a hassle to rebuild 1.6.1 and 1.6.1-gpu so they output the correct version, I think that would be great. Otherwise I could always refer to this issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:194,integrability,version,version,194,"I'd say it's mostly because the VCF also says `##DeepVariant_version=1.6.0`, which could be confusing. If it's not too much of a hassle to rebuild 1.6.1 and 1.6.1-gpu so they output the correct version, I think that would be great. Otherwise I could always refer to this issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:194,modifiability,version,version,194,"I'd say it's mostly because the VCF also says `##DeepVariant_version=1.6.0`, which could be confusing. If it's not too much of a hassle to rebuild 1.6.1 and 1.6.1-gpu so they output the correct version, I think that would be great. Otherwise I could always refer to this issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:163,performance,gpu,gpu,163,"I'd say it's mostly because the VCF also says `##DeepVariant_version=1.6.0`, which could be confusing. If it's not too much of a hassle to rebuild 1.6.1 and 1.6.1-gpu so they output the correct version, I think that would be great. Otherwise I could always refer to this issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:445,deployability,updat,update,445,"Hi @fellen31 ,. Thanks for the suggestion. For the official Docker file, we usually don't like to modify it after we push it. This is mostly for reproducibility concern. Even when we don't change everything else on our side, rebuilding at a later time can cause confusion if some other components (not in our control) has changed, and then might result in unexpected reproducibility differences. A few possible solutions:. 1. We'll make sure to update the version number in future releases, and we'll be careful even with a PATCH version update. (But we'll do nothing right now). 2. Or, if it's absolutely necessary for you to access a Docker under our Docker Hub and/or gcr.io, I can build one for you, but it will be called something like `1.6.1-fix-version` (instead of overwriting the 1.6.1 images). To do 2, it will take some time and effort on my side, but it's doable if it's important for you. Let me know!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:456,deployability,version,version,456,"Hi @fellen31 ,. Thanks for the suggestion. For the official Docker file, we usually don't like to modify it after we push it. This is mostly for reproducibility concern. Even when we don't change everything else on our side, rebuilding at a later time can cause confusion if some other components (not in our control) has changed, and then might result in unexpected reproducibility differences. A few possible solutions:. 1. We'll make sure to update the version number in future releases, and we'll be careful even with a PATCH version update. (But we'll do nothing right now). 2. Or, if it's absolutely necessary for you to access a Docker under our Docker Hub and/or gcr.io, I can build one for you, but it will be called something like `1.6.1-fix-version` (instead of overwriting the 1.6.1 images). To do 2, it will take some time and effort on my side, but it's doable if it's important for you. Let me know!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:481,deployability,releas,releases,481,"Hi @fellen31 ,. Thanks for the suggestion. For the official Docker file, we usually don't like to modify it after we push it. This is mostly for reproducibility concern. Even when we don't change everything else on our side, rebuilding at a later time can cause confusion if some other components (not in our control) has changed, and then might result in unexpected reproducibility differences. A few possible solutions:. 1. We'll make sure to update the version number in future releases, and we'll be careful even with a PATCH version update. (But we'll do nothing right now). 2. Or, if it's absolutely necessary for you to access a Docker under our Docker Hub and/or gcr.io, I can build one for you, but it will be called something like `1.6.1-fix-version` (instead of overwriting the 1.6.1 images). To do 2, it will take some time and effort on my side, but it's doable if it's important for you. Let me know!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:524,deployability,PATCH,PATCH,524,"Hi @fellen31 ,. Thanks for the suggestion. For the official Docker file, we usually don't like to modify it after we push it. This is mostly for reproducibility concern. Even when we don't change everything else on our side, rebuilding at a later time can cause confusion if some other components (not in our control) has changed, and then might result in unexpected reproducibility differences. A few possible solutions:. 1. We'll make sure to update the version number in future releases, and we'll be careful even with a PATCH version update. (But we'll do nothing right now). 2. Or, if it's absolutely necessary for you to access a Docker under our Docker Hub and/or gcr.io, I can build one for you, but it will be called something like `1.6.1-fix-version` (instead of overwriting the 1.6.1 images). To do 2, it will take some time and effort on my side, but it's doable if it's important for you. Let me know!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:530,deployability,version,version,530,"Hi @fellen31 ,. Thanks for the suggestion. For the official Docker file, we usually don't like to modify it after we push it. This is mostly for reproducibility concern. Even when we don't change everything else on our side, rebuilding at a later time can cause confusion if some other components (not in our control) has changed, and then might result in unexpected reproducibility differences. A few possible solutions:. 1. We'll make sure to update the version number in future releases, and we'll be careful even with a PATCH version update. (But we'll do nothing right now). 2. Or, if it's absolutely necessary for you to access a Docker under our Docker Hub and/or gcr.io, I can build one for you, but it will be called something like `1.6.1-fix-version` (instead of overwriting the 1.6.1 images). To do 2, it will take some time and effort on my side, but it's doable if it's important for you. Let me know!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:538,deployability,updat,update,538,"Hi @fellen31 ,. Thanks for the suggestion. For the official Docker file, we usually don't like to modify it after we push it. This is mostly for reproducibility concern. Even when we don't change everything else on our side, rebuilding at a later time can cause confusion if some other components (not in our control) has changed, and then might result in unexpected reproducibility differences. A few possible solutions:. 1. We'll make sure to update the version number in future releases, and we'll be careful even with a PATCH version update. (But we'll do nothing right now). 2. Or, if it's absolutely necessary for you to access a Docker under our Docker Hub and/or gcr.io, I can build one for you, but it will be called something like `1.6.1-fix-version` (instead of overwriting the 1.6.1 images). To do 2, it will take some time and effort on my side, but it's doable if it's important for you. Let me know!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:685,deployability,build,build,685,"Hi @fellen31 ,. Thanks for the suggestion. For the official Docker file, we usually don't like to modify it after we push it. This is mostly for reproducibility concern. Even when we don't change everything else on our side, rebuilding at a later time can cause confusion if some other components (not in our control) has changed, and then might result in unexpected reproducibility differences. A few possible solutions:. 1. We'll make sure to update the version number in future releases, and we'll be careful even with a PATCH version update. (But we'll do nothing right now). 2. Or, if it's absolutely necessary for you to access a Docker under our Docker Hub and/or gcr.io, I can build one for you, but it will be called something like `1.6.1-fix-version` (instead of overwriting the 1.6.1 images). To do 2, it will take some time and effort on my side, but it's doable if it's important for you. Let me know!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:752,deployability,version,version,752,"Hi @fellen31 ,. Thanks for the suggestion. For the official Docker file, we usually don't like to modify it after we push it. This is mostly for reproducibility concern. Even when we don't change everything else on our side, rebuilding at a later time can cause confusion if some other components (not in our control) has changed, and then might result in unexpected reproducibility differences. A few possible solutions:. 1. We'll make sure to update the version number in future releases, and we'll be careful even with a PATCH version update. (But we'll do nothing right now). 2. Or, if it's absolutely necessary for you to access a Docker under our Docker Hub and/or gcr.io, I can build one for you, but it will be called something like `1.6.1-fix-version` (instead of overwriting the 1.6.1 images). To do 2, it will take some time and effort on my side, but it's doable if it's important for you. Let me know!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:286,integrability,compon,components,286,"Hi @fellen31 ,. Thanks for the suggestion. For the official Docker file, we usually don't like to modify it after we push it. This is mostly for reproducibility concern. Even when we don't change everything else on our side, rebuilding at a later time can cause confusion if some other components (not in our control) has changed, and then might result in unexpected reproducibility differences. A few possible solutions:. 1. We'll make sure to update the version number in future releases, and we'll be careful even with a PATCH version update. (But we'll do nothing right now). 2. Or, if it's absolutely necessary for you to access a Docker under our Docker Hub and/or gcr.io, I can build one for you, but it will be called something like `1.6.1-fix-version` (instead of overwriting the 1.6.1 images). To do 2, it will take some time and effort on my side, but it's doable if it's important for you. Let me know!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:456,integrability,version,version,456,"Hi @fellen31 ,. Thanks for the suggestion. For the official Docker file, we usually don't like to modify it after we push it. This is mostly for reproducibility concern. Even when we don't change everything else on our side, rebuilding at a later time can cause confusion if some other components (not in our control) has changed, and then might result in unexpected reproducibility differences. A few possible solutions:. 1. We'll make sure to update the version number in future releases, and we'll be careful even with a PATCH version update. (But we'll do nothing right now). 2. Or, if it's absolutely necessary for you to access a Docker under our Docker Hub and/or gcr.io, I can build one for you, but it will be called something like `1.6.1-fix-version` (instead of overwriting the 1.6.1 images). To do 2, it will take some time and effort on my side, but it's doable if it's important for you. Let me know!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:530,integrability,version,version,530,"Hi @fellen31 ,. Thanks for the suggestion. For the official Docker file, we usually don't like to modify it after we push it. This is mostly for reproducibility concern. Even when we don't change everything else on our side, rebuilding at a later time can cause confusion if some other components (not in our control) has changed, and then might result in unexpected reproducibility differences. A few possible solutions:. 1. We'll make sure to update the version number in future releases, and we'll be careful even with a PATCH version update. (But we'll do nothing right now). 2. Or, if it's absolutely necessary for you to access a Docker under our Docker Hub and/or gcr.io, I can build one for you, but it will be called something like `1.6.1-fix-version` (instead of overwriting the 1.6.1 images). To do 2, it will take some time and effort on my side, but it's doable if it's important for you. Let me know!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:752,integrability,version,version,752,"Hi @fellen31 ,. Thanks for the suggestion. For the official Docker file, we usually don't like to modify it after we push it. This is mostly for reproducibility concern. Even when we don't change everything else on our side, rebuilding at a later time can cause confusion if some other components (not in our control) has changed, and then might result in unexpected reproducibility differences. A few possible solutions:. 1. We'll make sure to update the version number in future releases, and we'll be careful even with a PATCH version update. (But we'll do nothing right now). 2. Or, if it's absolutely necessary for you to access a Docker under our Docker Hub and/or gcr.io, I can build one for you, but it will be called something like `1.6.1-fix-version` (instead of overwriting the 1.6.1 images). To do 2, it will take some time and effort on my side, but it's doable if it's important for you. Let me know!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:286,interoperability,compon,components,286,"Hi @fellen31 ,. Thanks for the suggestion. For the official Docker file, we usually don't like to modify it after we push it. This is mostly for reproducibility concern. Even when we don't change everything else on our side, rebuilding at a later time can cause confusion if some other components (not in our control) has changed, and then might result in unexpected reproducibility differences. A few possible solutions:. 1. We'll make sure to update the version number in future releases, and we'll be careful even with a PATCH version update. (But we'll do nothing right now). 2. Or, if it's absolutely necessary for you to access a Docker under our Docker Hub and/or gcr.io, I can build one for you, but it will be called something like `1.6.1-fix-version` (instead of overwriting the 1.6.1 images). To do 2, it will take some time and effort on my side, but it's doable if it's important for you. Let me know!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:161,modifiability,concern,concern,161,"Hi @fellen31 ,. Thanks for the suggestion. For the official Docker file, we usually don't like to modify it after we push it. This is mostly for reproducibility concern. Even when we don't change everything else on our side, rebuilding at a later time can cause confusion if some other components (not in our control) has changed, and then might result in unexpected reproducibility differences. A few possible solutions:. 1. We'll make sure to update the version number in future releases, and we'll be careful even with a PATCH version update. (But we'll do nothing right now). 2. Or, if it's absolutely necessary for you to access a Docker under our Docker Hub and/or gcr.io, I can build one for you, but it will be called something like `1.6.1-fix-version` (instead of overwriting the 1.6.1 images). To do 2, it will take some time and effort on my side, but it's doable if it's important for you. Let me know!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:286,modifiability,compon,components,286,"Hi @fellen31 ,. Thanks for the suggestion. For the official Docker file, we usually don't like to modify it after we push it. This is mostly for reproducibility concern. Even when we don't change everything else on our side, rebuilding at a later time can cause confusion if some other components (not in our control) has changed, and then might result in unexpected reproducibility differences. A few possible solutions:. 1. We'll make sure to update the version number in future releases, and we'll be careful even with a PATCH version update. (But we'll do nothing right now). 2. Or, if it's absolutely necessary for you to access a Docker under our Docker Hub and/or gcr.io, I can build one for you, but it will be called something like `1.6.1-fix-version` (instead of overwriting the 1.6.1 images). To do 2, it will take some time and effort on my side, but it's doable if it's important for you. Let me know!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:456,modifiability,version,version,456,"Hi @fellen31 ,. Thanks for the suggestion. For the official Docker file, we usually don't like to modify it after we push it. This is mostly for reproducibility concern. Even when we don't change everything else on our side, rebuilding at a later time can cause confusion if some other components (not in our control) has changed, and then might result in unexpected reproducibility differences. A few possible solutions:. 1. We'll make sure to update the version number in future releases, and we'll be careful even with a PATCH version update. (But we'll do nothing right now). 2. Or, if it's absolutely necessary for you to access a Docker under our Docker Hub and/or gcr.io, I can build one for you, but it will be called something like `1.6.1-fix-version` (instead of overwriting the 1.6.1 images). To do 2, it will take some time and effort on my side, but it's doable if it's important for you. Let me know!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:530,modifiability,version,version,530,"Hi @fellen31 ,. Thanks for the suggestion. For the official Docker file, we usually don't like to modify it after we push it. This is mostly for reproducibility concern. Even when we don't change everything else on our side, rebuilding at a later time can cause confusion if some other components (not in our control) has changed, and then might result in unexpected reproducibility differences. A few possible solutions:. 1. We'll make sure to update the version number in future releases, and we'll be careful even with a PATCH version update. (But we'll do nothing right now). 2. Or, if it's absolutely necessary for you to access a Docker under our Docker Hub and/or gcr.io, I can build one for you, but it will be called something like `1.6.1-fix-version` (instead of overwriting the 1.6.1 images). To do 2, it will take some time and effort on my side, but it's doable if it's important for you. Let me know!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:752,modifiability,version,version,752,"Hi @fellen31 ,. Thanks for the suggestion. For the official Docker file, we usually don't like to modify it after we push it. This is mostly for reproducibility concern. Even when we don't change everything else on our side, rebuilding at a later time can cause confusion if some other components (not in our control) has changed, and then might result in unexpected reproducibility differences. A few possible solutions:. 1. We'll make sure to update the version number in future releases, and we'll be careful even with a PATCH version update. (But we'll do nothing right now). 2. Or, if it's absolutely necessary for you to access a Docker under our Docker Hub and/or gcr.io, I can build one for you, but it will be called something like `1.6.1-fix-version` (instead of overwriting the 1.6.1 images). To do 2, it will take some time and effort on my side, but it's doable if it's important for you. Let me know!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:247,performance,time,time,247,"Hi @fellen31 ,. Thanks for the suggestion. For the official Docker file, we usually don't like to modify it after we push it. This is mostly for reproducibility concern. Even when we don't change everything else on our side, rebuilding at a later time can cause confusion if some other components (not in our control) has changed, and then might result in unexpected reproducibility differences. A few possible solutions:. 1. We'll make sure to update the version number in future releases, and we'll be careful even with a PATCH version update. (But we'll do nothing right now). 2. Or, if it's absolutely necessary for you to access a Docker under our Docker Hub and/or gcr.io, I can build one for you, but it will be called something like `1.6.1-fix-version` (instead of overwriting the 1.6.1 images). To do 2, it will take some time and effort on my side, but it's doable if it's important for you. Let me know!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:831,performance,time,time,831,"Hi @fellen31 ,. Thanks for the suggestion. For the official Docker file, we usually don't like to modify it after we push it. This is mostly for reproducibility concern. Even when we don't change everything else on our side, rebuilding at a later time can cause confusion if some other components (not in our control) has changed, and then might result in unexpected reproducibility differences. A few possible solutions:. 1. We'll make sure to update the version number in future releases, and we'll be careful even with a PATCH version update. (But we'll do nothing right now). 2. Or, if it's absolutely necessary for you to access a Docker under our Docker Hub and/or gcr.io, I can build one for you, but it will be called something like `1.6.1-fix-version` (instead of overwriting the 1.6.1 images). To do 2, it will take some time and effort on my side, but it's doable if it's important for you. Let me know!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:445,safety,updat,update,445,"Hi @fellen31 ,. Thanks for the suggestion. For the official Docker file, we usually don't like to modify it after we push it. This is mostly for reproducibility concern. Even when we don't change everything else on our side, rebuilding at a later time can cause confusion if some other components (not in our control) has changed, and then might result in unexpected reproducibility differences. A few possible solutions:. 1. We'll make sure to update the version number in future releases, and we'll be careful even with a PATCH version update. (But we'll do nothing right now). 2. Or, if it's absolutely necessary for you to access a Docker under our Docker Hub and/or gcr.io, I can build one for you, but it will be called something like `1.6.1-fix-version` (instead of overwriting the 1.6.1 images). To do 2, it will take some time and effort on my side, but it's doable if it's important for you. Let me know!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:524,safety,PATCH,PATCH,524,"Hi @fellen31 ,. Thanks for the suggestion. For the official Docker file, we usually don't like to modify it after we push it. This is mostly for reproducibility concern. Even when we don't change everything else on our side, rebuilding at a later time can cause confusion if some other components (not in our control) has changed, and then might result in unexpected reproducibility differences. A few possible solutions:. 1. We'll make sure to update the version number in future releases, and we'll be careful even with a PATCH version update. (But we'll do nothing right now). 2. Or, if it's absolutely necessary for you to access a Docker under our Docker Hub and/or gcr.io, I can build one for you, but it will be called something like `1.6.1-fix-version` (instead of overwriting the 1.6.1 images). To do 2, it will take some time and effort on my side, but it's doable if it's important for you. Let me know!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:538,safety,updat,update,538,"Hi @fellen31 ,. Thanks for the suggestion. For the official Docker file, we usually don't like to modify it after we push it. This is mostly for reproducibility concern. Even when we don't change everything else on our side, rebuilding at a later time can cause confusion if some other components (not in our control) has changed, and then might result in unexpected reproducibility differences. A few possible solutions:. 1. We'll make sure to update the version number in future releases, and we'll be careful even with a PATCH version update. (But we'll do nothing right now). 2. Or, if it's absolutely necessary for you to access a Docker under our Docker Hub and/or gcr.io, I can build one for you, but it will be called something like `1.6.1-fix-version` (instead of overwriting the 1.6.1 images). To do 2, it will take some time and effort on my side, but it's doable if it's important for you. Let me know!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:98,security,modif,modify,98,"Hi @fellen31 ,. Thanks for the suggestion. For the official Docker file, we usually don't like to modify it after we push it. This is mostly for reproducibility concern. Even when we don't change everything else on our side, rebuilding at a later time can cause confusion if some other components (not in our control) has changed, and then might result in unexpected reproducibility differences. A few possible solutions:. 1. We'll make sure to update the version number in future releases, and we'll be careful even with a PATCH version update. (But we'll do nothing right now). 2. Or, if it's absolutely necessary for you to access a Docker under our Docker Hub and/or gcr.io, I can build one for you, but it will be called something like `1.6.1-fix-version` (instead of overwriting the 1.6.1 images). To do 2, it will take some time and effort on my side, but it's doable if it's important for you. Let me know!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:309,security,control,control,309,"Hi @fellen31 ,. Thanks for the suggestion. For the official Docker file, we usually don't like to modify it after we push it. This is mostly for reproducibility concern. Even when we don't change everything else on our side, rebuilding at a later time can cause confusion if some other components (not in our control) has changed, and then might result in unexpected reproducibility differences. A few possible solutions:. 1. We'll make sure to update the version number in future releases, and we'll be careful even with a PATCH version update. (But we'll do nothing right now). 2. Or, if it's absolutely necessary for you to access a Docker under our Docker Hub and/or gcr.io, I can build one for you, but it will be called something like `1.6.1-fix-version` (instead of overwriting the 1.6.1 images). To do 2, it will take some time and effort on my side, but it's doable if it's important for you. Let me know!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:445,security,updat,update,445,"Hi @fellen31 ,. Thanks for the suggestion. For the official Docker file, we usually don't like to modify it after we push it. This is mostly for reproducibility concern. Even when we don't change everything else on our side, rebuilding at a later time can cause confusion if some other components (not in our control) has changed, and then might result in unexpected reproducibility differences. A few possible solutions:. 1. We'll make sure to update the version number in future releases, and we'll be careful even with a PATCH version update. (But we'll do nothing right now). 2. Or, if it's absolutely necessary for you to access a Docker under our Docker Hub and/or gcr.io, I can build one for you, but it will be called something like `1.6.1-fix-version` (instead of overwriting the 1.6.1 images). To do 2, it will take some time and effort on my side, but it's doable if it's important for you. Let me know!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:524,security,PATCH,PATCH,524,"Hi @fellen31 ,. Thanks for the suggestion. For the official Docker file, we usually don't like to modify it after we push it. This is mostly for reproducibility concern. Even when we don't change everything else on our side, rebuilding at a later time can cause confusion if some other components (not in our control) has changed, and then might result in unexpected reproducibility differences. A few possible solutions:. 1. We'll make sure to update the version number in future releases, and we'll be careful even with a PATCH version update. (But we'll do nothing right now). 2. Or, if it's absolutely necessary for you to access a Docker under our Docker Hub and/or gcr.io, I can build one for you, but it will be called something like `1.6.1-fix-version` (instead of overwriting the 1.6.1 images). To do 2, it will take some time and effort on my side, but it's doable if it's important for you. Let me know!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:538,security,updat,update,538,"Hi @fellen31 ,. Thanks for the suggestion. For the official Docker file, we usually don't like to modify it after we push it. This is mostly for reproducibility concern. Even when we don't change everything else on our side, rebuilding at a later time can cause confusion if some other components (not in our control) has changed, and then might result in unexpected reproducibility differences. A few possible solutions:. 1. We'll make sure to update the version number in future releases, and we'll be careful even with a PATCH version update. (But we'll do nothing right now). 2. Or, if it's absolutely necessary for you to access a Docker under our Docker Hub and/or gcr.io, I can build one for you, but it will be called something like `1.6.1-fix-version` (instead of overwriting the 1.6.1 images). To do 2, it will take some time and effort on my side, but it's doable if it's important for you. Let me know!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:627,security,access,access,627,"Hi @fellen31 ,. Thanks for the suggestion. For the official Docker file, we usually don't like to modify it after we push it. This is mostly for reproducibility concern. Even when we don't change everything else on our side, rebuilding at a later time can cause confusion if some other components (not in our control) has changed, and then might result in unexpected reproducibility differences. A few possible solutions:. 1. We'll make sure to update the version number in future releases, and we'll be careful even with a PATCH version update. (But we'll do nothing right now). 2. Or, if it's absolutely necessary for you to access a Docker under our Docker Hub and/or gcr.io, I can build one for you, but it will be called something like `1.6.1-fix-version` (instead of overwriting the 1.6.1 images). To do 2, it will take some time and effort on my side, but it's doable if it's important for you. Let me know!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:161,testability,concern,concern,161,"Hi @fellen31 ,. Thanks for the suggestion. For the official Docker file, we usually don't like to modify it after we push it. This is mostly for reproducibility concern. Even when we don't change everything else on our side, rebuilding at a later time can cause confusion if some other components (not in our control) has changed, and then might result in unexpected reproducibility differences. A few possible solutions:. 1. We'll make sure to update the version number in future releases, and we'll be careful even with a PATCH version update. (But we'll do nothing right now). 2. Or, if it's absolutely necessary for you to access a Docker under our Docker Hub and/or gcr.io, I can build one for you, but it will be called something like `1.6.1-fix-version` (instead of overwriting the 1.6.1 images). To do 2, it will take some time and effort on my side, but it's doable if it's important for you. Let me know!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:309,testability,control,control,309,"Hi @fellen31 ,. Thanks for the suggestion. For the official Docker file, we usually don't like to modify it after we push it. This is mostly for reproducibility concern. Even when we don't change everything else on our side, rebuilding at a later time can cause confusion if some other components (not in our control) has changed, and then might result in unexpected reproducibility differences. A few possible solutions:. 1. We'll make sure to update the version number in future releases, and we'll be careful even with a PATCH version update. (But we'll do nothing right now). 2. Or, if it's absolutely necessary for you to access a Docker under our Docker Hub and/or gcr.io, I can build one for you, but it will be called something like `1.6.1-fix-version` (instead of overwriting the 1.6.1 images). To do 2, it will take some time and effort on my side, but it's doable if it's important for you. Let me know!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:151,deployability,version,version,151,"Hi, sorry to be a bother. If it was just me that was going to use it I wouldn't care, but in this particular case it would be preferable to have a fix-version under your Docker Hub. If you could do it, I'd really appreciate it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:151,integrability,version,version,151,"Hi, sorry to be a bother. If it was just me that was going to use it I wouldn't care, but in this particular case it would be preferable to have a fix-version under your Docker Hub. If you could do it, I'd really appreciate it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:151,modifiability,version,version,151,"Hi, sorry to be a bother. If it was just me that was going to use it I wouldn't care, but in this particular case it would be preferable to have a fix-version under your Docker Hub. If you could do it, I'd really appreciate it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:126,usability,prefer,preferable,126,"Hi, sorry to be a bother. If it was just me that was going to use it I wouldn't care, but in this particular case it would be preferable to have a fix-version under your Docker Hub. If you could do it, I'd really appreciate it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:60,availability,down,downgrade,60,"@fellen31 , . The easiest solution here would be for you to downgrade to 1.6.0 as these two versions should generate identical outputs. The only fix is in `call_variants` in cases with empty shards which happens very rarely when you are running things broken into multiple steps. Unless you are experiencing the issue, you can continue using 1.6.0. Please let us know if that solution doesn't work for you and we will build a docker for you, but, it may take some time so you may need to wait.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:92,deployability,version,versions,92,"@fellen31 , . The easiest solution here would be for you to downgrade to 1.6.0 as these two versions should generate identical outputs. The only fix is in `call_variants` in cases with empty shards which happens very rarely when you are running things broken into multiple steps. Unless you are experiencing the issue, you can continue using 1.6.0. Please let us know if that solution doesn't work for you and we will build a docker for you, but, it may take some time so you may need to wait.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:327,deployability,continu,continue,327,"@fellen31 , . The easiest solution here would be for you to downgrade to 1.6.0 as these two versions should generate identical outputs. The only fix is in `call_variants` in cases with empty shards which happens very rarely when you are running things broken into multiple steps. Unless you are experiencing the issue, you can continue using 1.6.0. Please let us know if that solution doesn't work for you and we will build a docker for you, but, it may take some time so you may need to wait.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:418,deployability,build,build,418,"@fellen31 , . The easiest solution here would be for you to downgrade to 1.6.0 as these two versions should generate identical outputs. The only fix is in `call_variants` in cases with empty shards which happens very rarely when you are running things broken into multiple steps. Unless you are experiencing the issue, you can continue using 1.6.0. Please let us know if that solution doesn't work for you and we will build a docker for you, but, it may take some time so you may need to wait.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:92,integrability,version,versions,92,"@fellen31 , . The easiest solution here would be for you to downgrade to 1.6.0 as these two versions should generate identical outputs. The only fix is in `call_variants` in cases with empty shards which happens very rarely when you are running things broken into multiple steps. Unless you are experiencing the issue, you can continue using 1.6.0. Please let us know if that solution doesn't work for you and we will build a docker for you, but, it may take some time so you may need to wait.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:92,modifiability,version,versions,92,"@fellen31 , . The easiest solution here would be for you to downgrade to 1.6.0 as these two versions should generate identical outputs. The only fix is in `call_variants` in cases with empty shards which happens very rarely when you are running things broken into multiple steps. Unless you are experiencing the issue, you can continue using 1.6.0. Please let us know if that solution doesn't work for you and we will build a docker for you, but, it may take some time so you may need to wait.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:464,performance,time,time,464,"@fellen31 , . The easiest solution here would be for you to downgrade to 1.6.0 as these two versions should generate identical outputs. The only fix is in `call_variants` in cases with empty shards which happens very rarely when you are running things broken into multiple steps. Unless you are experiencing the issue, you can continue using 1.6.0. Please let us know if that solution doesn't work for you and we will build a docker for you, but, it may take some time so you may need to wait.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:385,reliability,doe,doesn,385,"@fellen31 , . The easiest solution here would be for you to downgrade to 1.6.0 as these two versions should generate identical outputs. The only fix is in `call_variants` in cases with empty shards which happens very rarely when you are running things broken into multiple steps. Unless you are experiencing the issue, you can continue using 1.6.0. Please let us know if that solution doesn't work for you and we will build a docker for you, but, it may take some time so you may need to wait.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:117,security,ident,identical,117,"@fellen31 , . The easiest solution here would be for you to downgrade to 1.6.0 as these two versions should generate identical outputs. The only fix is in `call_variants` in cases with empty shards which happens very rarely when you are running things broken into multiple steps. Unless you are experiencing the issue, you can continue using 1.6.0. Please let us know if that solution doesn't work for you and we will build a docker for you, but, it may take some time so you may need to wait.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:295,usability,experien,experiencing,295,"@fellen31 , . The easiest solution here would be for you to downgrade to 1.6.0 as these two versions should generate identical outputs. The only fix is in `call_variants` in cases with empty shards which happens very rarely when you are running things broken into multiple steps. Unless you are experiencing the issue, you can continue using 1.6.0. Please let us know if that solution doesn't work for you and we will build a docker for you, but, it may take some time so you may need to wait.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:48,deployability,build,build,48,"Sure @kishwarshafin, I'll try 1.6.0. If I would build the docker myself, is `ARG VERSION=1.6.0` the only thing I would need to change from the current Dockerfile?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:81,deployability,VERSION,VERSION,81,"Sure @kishwarshafin, I'll try 1.6.0. If I would build the docker myself, is `ARG VERSION=1.6.0` the only thing I would need to change from the current Dockerfile?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:143,energy efficiency,current,current,143,"Sure @kishwarshafin, I'll try 1.6.0. If I would build the docker myself, is `ARG VERSION=1.6.0` the only thing I would need to change from the current Dockerfile?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:81,integrability,VERSION,VERSION,81,"Sure @kishwarshafin, I'll try 1.6.0. If I would build the docker myself, is `ARG VERSION=1.6.0` the only thing I would need to change from the current Dockerfile?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:81,modifiability,VERSION,VERSION,81,"Sure @kishwarshafin, I'll try 1.6.0. If I would build the docker myself, is `ARG VERSION=1.6.0` the only thing I would need to change from the current Dockerfile?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:599,availability,Sli,Slim,599,"Hi @fellen31 ,. That's right. . I checked the files. Only the `ARG` in Dockerfiles are relevant. ```. $ find . -type f -exec grep -H '1\.6\.0' {} \; . ./scripts/inference_deeptrio.sh: # --use_slim_model is introduced only after 1.6.0. ./scripts/inference_deepvariant.sh: # --use_slim_model is introduced only after 1.6.0. ./Dockerfile.deeptrio:ARG VERSION_DEEPTRIO=1.6.0. ./Dockerfile:ARG VERSION=1.6.0. ./.git/packed-refs:6b167016a0e6b92ca5b492e2dac9000d97c8d3ee refs/tags/v1.6.0. ./docs/FAQ.md:## Do models from before r1.6.0 work with current inference code? ./docs/FAQ.md:No. We have moved from Slim to Keras. All models before `1.6.0` were trained in. ./docs/FAQ.md:Slim platform. So they are not compatible with `1.6.0` anymore. ./docs/deeptrio-details-training-data.md:1.6.0 | <sup>[(6)](#vfootnote6)</sup>4 HG001, 3 HG002, 3 HG003, 3 HG004, 7 HG005, 6 HG006, 6 HG007, 4 NA12891, 4 NA12892 | 704,394,556. ./docs/deeptrio-details-training-data.md:1.6.0 | <sup>[(6)](#vfootnote6)</sup>2 HG001, 3 HG002, 3 HG003, 3 HG004, 7 HG005, 6 HG006, 6 HG007, 2 NA12891, 2 NA12892 | 457,420,038. ./docs/deeptrio-details-training-data.md:| 1.6.0 | <sup>[(6)](#vfootnote6)</sup>9 HG001, 7 HG002, 7 HG003, 7 HG004, 8 HG005, 8 HG006, 8 HG007, 9 NA12891, 9 NA12892 | 27,783,324 |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | <sup>[(6)](#vfootnote6)</sup>6 HG002, 6 HG003, 6 HG004, 8 HG005, 8 HG006, 8 HG007 | 13,039,595 |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | 9 HG002, 5 HG003, 5 HG004, 1 HG005, 1 HG006, 1 HG007 | 890,016,014<sup>[(5)](#vfootnote5)</sup> |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | 9 HG002, 5 HG003, 5 HG004, 1 HG005, 1 HG006, 1 HG007 | 838,515,085<sup>[(5)](#vfootnote5)</sup> |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | 1 HG002, 1 HG002, 1 HG004 | 50,249,704<sup>[(5)](#vfootnote5)</sup> |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | 1 HG002, 1 HG002, 1 HG004 | 99,675,190<sup>[(5)](#vfootnote5)</sup> |. ./deeptrio/testdata/golden.calling_exa",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:671,availability,Sli,Slim,671,"Hi @fellen31 ,. That's right. . I checked the files. Only the `ARG` in Dockerfiles are relevant. ```. $ find . -type f -exec grep -H '1\.6\.0' {} \; . ./scripts/inference_deeptrio.sh: # --use_slim_model is introduced only after 1.6.0. ./scripts/inference_deepvariant.sh: # --use_slim_model is introduced only after 1.6.0. ./Dockerfile.deeptrio:ARG VERSION_DEEPTRIO=1.6.0. ./Dockerfile:ARG VERSION=1.6.0. ./.git/packed-refs:6b167016a0e6b92ca5b492e2dac9000d97c8d3ee refs/tags/v1.6.0. ./docs/FAQ.md:## Do models from before r1.6.0 work with current inference code? ./docs/FAQ.md:No. We have moved from Slim to Keras. All models before `1.6.0` were trained in. ./docs/FAQ.md:Slim platform. So they are not compatible with `1.6.0` anymore. ./docs/deeptrio-details-training-data.md:1.6.0 | <sup>[(6)](#vfootnote6)</sup>4 HG001, 3 HG002, 3 HG003, 3 HG004, 7 HG005, 6 HG006, 6 HG007, 4 NA12891, 4 NA12892 | 704,394,556. ./docs/deeptrio-details-training-data.md:1.6.0 | <sup>[(6)](#vfootnote6)</sup>2 HG001, 3 HG002, 3 HG003, 3 HG004, 7 HG005, 6 HG006, 6 HG007, 2 NA12891, 2 NA12892 | 457,420,038. ./docs/deeptrio-details-training-data.md:| 1.6.0 | <sup>[(6)](#vfootnote6)</sup>9 HG001, 7 HG002, 7 HG003, 7 HG004, 8 HG005, 8 HG006, 8 HG007, 9 NA12891, 9 NA12892 | 27,783,324 |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | <sup>[(6)](#vfootnote6)</sup>6 HG002, 6 HG003, 6 HG004, 8 HG005, 8 HG006, 8 HG007 | 13,039,595 |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | 9 HG002, 5 HG003, 5 HG004, 1 HG005, 1 HG006, 1 HG007 | 890,016,014<sup>[(5)](#vfootnote5)</sup> |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | 9 HG002, 5 HG003, 5 HG004, 1 HG005, 1 HG006, 1 HG007 | 838,515,085<sup>[(5)](#vfootnote5)</sup> |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | 1 HG002, 1 HG002, 1 HG004 | 50,249,704<sup>[(5)](#vfootnote5)</sup> |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | 1 HG002, 1 HG002, 1 HG004 | 99,675,190<sup>[(5)](#vfootnote5)</sup> |. ./deeptrio/testdata/golden.calling_exa",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:389,deployability,VERSION,VERSION,389,"Hi @fellen31 ,. That's right. . I checked the files. Only the `ARG` in Dockerfiles are relevant. ```. $ find . -type f -exec grep -H '1\.6\.0' {} \; . ./scripts/inference_deeptrio.sh: # --use_slim_model is introduced only after 1.6.0. ./scripts/inference_deepvariant.sh: # --use_slim_model is introduced only after 1.6.0. ./Dockerfile.deeptrio:ARG VERSION_DEEPTRIO=1.6.0. ./Dockerfile:ARG VERSION=1.6.0. ./.git/packed-refs:6b167016a0e6b92ca5b492e2dac9000d97c8d3ee refs/tags/v1.6.0. ./docs/FAQ.md:## Do models from before r1.6.0 work with current inference code? ./docs/FAQ.md:No. We have moved from Slim to Keras. All models before `1.6.0` were trained in. ./docs/FAQ.md:Slim platform. So they are not compatible with `1.6.0` anymore. ./docs/deeptrio-details-training-data.md:1.6.0 | <sup>[(6)](#vfootnote6)</sup>4 HG001, 3 HG002, 3 HG003, 3 HG004, 7 HG005, 6 HG006, 6 HG007, 4 NA12891, 4 NA12892 | 704,394,556. ./docs/deeptrio-details-training-data.md:1.6.0 | <sup>[(6)](#vfootnote6)</sup>2 HG001, 3 HG002, 3 HG003, 3 HG004, 7 HG005, 6 HG006, 6 HG007, 2 NA12891, 2 NA12892 | 457,420,038. ./docs/deeptrio-details-training-data.md:| 1.6.0 | <sup>[(6)](#vfootnote6)</sup>9 HG001, 7 HG002, 7 HG003, 7 HG004, 8 HG005, 8 HG006, 8 HG007, 9 NA12891, 9 NA12892 | 27,783,324 |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | <sup>[(6)](#vfootnote6)</sup>6 HG002, 6 HG003, 6 HG004, 8 HG005, 8 HG006, 8 HG007 | 13,039,595 |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | 9 HG002, 5 HG003, 5 HG004, 1 HG005, 1 HG006, 1 HG007 | 890,016,014<sup>[(5)](#vfootnote5)</sup> |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | 9 HG002, 5 HG003, 5 HG004, 1 HG005, 1 HG006, 1 HG007 | 838,515,085<sup>[(5)](#vfootnote5)</sup> |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | 1 HG002, 1 HG002, 1 HG004 | 50,249,704<sup>[(5)](#vfootnote5)</sup> |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | 1 HG002, 1 HG002, 1 HG004 | 99,675,190<sup>[(5)](#vfootnote5)</sup> |. ./deeptrio/testdata/golden.calling_exa",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:2053,deployability,version,version,2053,"12891, 2 NA12892 | 457,420,038. ./docs/deeptrio-details-training-data.md:| 1.6.0 | <sup>[(6)](#vfootnote6)</sup>9 HG001, 7 HG002, 7 HG003, 7 HG004, 8 HG005, 8 HG006, 8 HG007, 9 NA12891, 9 NA12892 | 27,783,324 |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | <sup>[(6)](#vfootnote6)</sup>6 HG002, 6 HG003, 6 HG004, 8 HG005, 8 HG006, 8 HG007 | 13,039,595 |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | 9 HG002, 5 HG003, 5 HG004, 1 HG005, 1 HG006, 1 HG007 | 890,016,014<sup>[(5)](#vfootnote5)</sup> |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | 9 HG002, 5 HG003, 5 HG004, 1 HG005, 1 HG006, 1 HG007 | 838,515,085<sup>[(5)](#vfootnote5)</sup> |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | 1 HG002, 1 HG002, 1 HG004 | 50,249,704<sup>[(5)](#vfootnote5)</sup> |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | 1 HG002, 1 HG002, 1 HG004 | 99,675,190<sup>[(5)](#vfootnote5)</sup> |. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00002-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/HG002_ONT_deeptrio.denovo.examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ./deeptrio/testdata/customized_classes.golden.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.postprocess_gvcf_output.g.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/golden.training_examples.tfrecord.gz-00002-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.vcf_candidate_importer.calling_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:2216,deployability,version,version,2216,"6, 8 HG007, 9 NA12891, 9 NA12892 | 27,783,324 |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | <sup>[(6)](#vfootnote6)</sup>6 HG002, 6 HG003, 6 HG004, 8 HG005, 8 HG006, 8 HG007 | 13,039,595 |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | 9 HG002, 5 HG003, 5 HG004, 1 HG005, 1 HG006, 1 HG007 | 890,016,014<sup>[(5)](#vfootnote5)</sup> |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | 9 HG002, 5 HG003, 5 HG004, 1 HG005, 1 HG006, 1 HG007 | 838,515,085<sup>[(5)](#vfootnote5)</sup> |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | 1 HG002, 1 HG002, 1 HG004 | 50,249,704<sup>[(5)](#vfootnote5)</sup> |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | 1 HG002, 1 HG002, 1 HG004 | 99,675,190<sup>[(5)](#vfootnote5)</sup> |. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00002-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/HG002_ONT_deeptrio.denovo.examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ./deeptrio/testdata/customized_classes.golden.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.postprocess_gvcf_output.g.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/golden.training_examples.tfrecord.gz-00002-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.vcf_candidate_importer.calling_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00000-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:2398,deployability,version,version,2398,"| 13,039,595 |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | 9 HG002, 5 HG003, 5 HG004, 1 HG005, 1 HG006, 1 HG007 | 890,016,014<sup>[(5)](#vfootnote5)</sup> |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | 9 HG002, 5 HG003, 5 HG004, 1 HG005, 1 HG006, 1 HG007 | 838,515,085<sup>[(5)](#vfootnote5)</sup> |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | 1 HG002, 1 HG002, 1 HG004 | 50,249,704<sup>[(5)](#vfootnote5)</sup> |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | 1 HG002, 1 HG002, 1 HG004 | 99,675,190<sup>[(5)](#vfootnote5)</sup> |. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00002-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/HG002_ONT_deeptrio.denovo.examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ./deeptrio/testdata/customized_classes.golden.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.postprocess_gvcf_output.g.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/golden.training_examples.tfrecord.gz-00002-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.vcf_candidate_importer.calling_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00000-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00001-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/te",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:2550,deployability,version,version,2550,"te5)</sup> |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | 9 HG002, 5 HG003, 5 HG004, 1 HG005, 1 HG006, 1 HG007 | 838,515,085<sup>[(5)](#vfootnote5)</sup> |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | 1 HG002, 1 HG002, 1 HG004 | 50,249,704<sup>[(5)](#vfootnote5)</sup> |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | 1 HG002, 1 HG002, 1 HG004 | 99,675,190<sup>[(5)](#vfootnote5)</sup> |. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00002-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/HG002_ONT_deeptrio.denovo.examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ./deeptrio/testdata/customized_classes.golden.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.postprocess_gvcf_output.g.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/golden.training_examples.tfrecord.gz-00002-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.vcf_candidate_importer.calling_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00000-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00001-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.postprocess_single_site_output.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/golden.training_examples.vcf:##DeepVariant_version=1.6",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:2804,deployability,version,version,2804,"sup>[(5)](#vfootnote5)</sup> |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | 1 HG002, 1 HG002, 1 HG004 | 99,675,190<sup>[(5)](#vfootnote5)</sup> |. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00002-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/HG002_ONT_deeptrio.denovo.examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ./deeptrio/testdata/customized_classes.golden.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.postprocess_gvcf_output.g.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/golden.training_examples.tfrecord.gz-00002-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.vcf_candidate_importer.calling_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00000-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00001-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.postprocess_single_site_output.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/golden.training_examples.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/HG002_ONT_deeptrio.examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ./deeptrio/testdata/golden.vcf_candidate_importer.training_examples.tfrecord.gz.exam",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:2979,deployability,version,version,2979,"olden.calling_examples.tfrecord.gz-00002-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/HG002_ONT_deeptrio.denovo.examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ./deeptrio/testdata/customized_classes.golden.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.postprocess_gvcf_output.g.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/golden.training_examples.tfrecord.gz-00002-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.vcf_candidate_importer.calling_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00000-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00001-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.postprocess_single_site_output.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/golden.training_examples.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/HG002_ONT_deeptrio.examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ./deeptrio/testdata/golden.vcf_candidate_importer.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/alt_aligned_pileup.golden.training_examples.tfrecord.gz.example",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:3146,deployability,version,version,3146,"G002_ONT_deeptrio.denovo.examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ./deeptrio/testdata/customized_classes.golden.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.postprocess_gvcf_output.g.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/golden.training_examples.tfrecord.gz-00002-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.vcf_candidate_importer.calling_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00000-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00001-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.postprocess_single_site_output.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/golden.training_examples.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/HG002_ONT_deeptrio.examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ./deeptrio/testdata/golden.vcf_candidate_importer.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/alt_aligned_pileup.golden.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 199, 8], ""channels"": [1, 2, 3, 4, 5, 6, 9, 10]}. ./deeptrio/testdata/golden.training_examples.tfrecord.gz-00000-of-00003",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:3313,deployability,version,version,3313,"data/customized_classes.golden.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.postprocess_gvcf_output.g.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/golden.training_examples.tfrecord.gz-00002-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.vcf_candidate_importer.calling_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00000-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00001-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.postprocess_single_site_output.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/golden.training_examples.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/HG002_ONT_deeptrio.examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ./deeptrio/testdata/golden.vcf_candidate_importer.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/alt_aligned_pileup.golden.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 199, 8], ""channels"": [1, 2, 3, 4, 5, 6, 9, 10]}. ./deeptrio/testdata/golden.training_examples.tfrecord.gz-00000-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.training_examples.tfrecord.gz.example_info.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:3638,deployability,version,version,3638,"ata/golden.postprocess_gvcf_output.g.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/golden.training_examples.tfrecord.gz-00002-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.vcf_candidate_importer.calling_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00000-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00001-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.postprocess_single_site_output.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/golden.training_examples.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/HG002_ONT_deeptrio.examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ./deeptrio/testdata/golden.vcf_candidate_importer.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/alt_aligned_pileup.golden.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 199, 8], ""channels"": [1, 2, 3, 4, 5, 6, 9, 10]}. ./deeptrio/testdata/golden.training_examples.tfrecord.gz-00000-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.training_examples.tfrecord.gz-00001-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./Dockerfile.deepsomatic:ARG VERSION_DEEPSOMATIC=1.6.0. ./deepvariant/test",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:3824,deployability,version,version,3824,"{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.vcf_candidate_importer.calling_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00000-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00001-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.postprocess_single_site_output.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/golden.training_examples.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/HG002_ONT_deeptrio.examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ./deeptrio/testdata/golden.vcf_candidate_importer.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/alt_aligned_pileup.golden.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 199, 8], ""channels"": [1, 2, 3, 4, 5, 6, 9, 10]}. ./deeptrio/testdata/golden.training_examples.tfrecord.gz-00000-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.training_examples.tfrecord.gz-00001-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./Dockerfile.deepsomatic:ARG VERSION_DEEPSOMATIC=1.6.0. ./deepvariant/testdata/golden.vcf_candidate_importer_calling_examples.tfrecord.example_info.json:{""version"": ""1.6.0"", ""shape"": [100, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:3996,deployability,version,version,3996,"{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.vcf_candidate_importer.calling_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00000-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00001-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.postprocess_single_site_output.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/golden.training_examples.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/HG002_ONT_deeptrio.examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ./deeptrio/testdata/golden.vcf_candidate_importer.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/alt_aligned_pileup.golden.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 199, 8], ""channels"": [1, 2, 3, 4, 5, 6, 9, 10]}. ./deeptrio/testdata/golden.training_examples.tfrecord.gz-00000-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.training_examples.tfrecord.gz-00001-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./Dockerfile.deepsomatic:ARG VERSION_DEEPSOMATIC=1.6.0. ./deepvariant/testdata/golden.vcf_candidate_importer_calling_examples.tfrecord.example_info.json:{""version"": ""1.6.0"", ""shape"": [100, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:4171,deployability,version,version,4171,"{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.vcf_candidate_importer.calling_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00000-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00001-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.postprocess_single_site_output.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/golden.training_examples.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/HG002_ONT_deeptrio.examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ./deeptrio/testdata/golden.vcf_candidate_importer.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/alt_aligned_pileup.golden.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 199, 8], ""channels"": [1, 2, 3, 4, 5, 6, 9, 10]}. ./deeptrio/testdata/golden.training_examples.tfrecord.gz-00000-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.training_examples.tfrecord.gz-00001-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./Dockerfile.deepsomatic:ARG VERSION_DEEPSOMATIC=1.6.0. ./deepvariant/testdata/golden.vcf_candidate_importer_calling_examples.tfrecord.example_info.json:{""version"": ""1.6.0"", ""shape"": [100, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:4324,deployability,version,version,4324,"{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.vcf_candidate_importer.calling_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00000-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00001-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.postprocess_single_site_output.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/golden.training_examples.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/HG002_ONT_deeptrio.examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ./deeptrio/testdata/golden.vcf_candidate_importer.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/alt_aligned_pileup.golden.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 199, 8], ""channels"": [1, 2, 3, 4, 5, 6, 9, 10]}. ./deeptrio/testdata/golden.training_examples.tfrecord.gz-00000-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.training_examples.tfrecord.gz-00001-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./Dockerfile.deepsomatic:ARG VERSION_DEEPSOMATIC=1.6.0. ./deepvariant/testdata/golden.vcf_candidate_importer_calling_examples.tfrecord.example_info.json:{""version"": ""1.6.0"", ""shape"": [100, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:4492,deployability,version,version,4492,"{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.vcf_candidate_importer.calling_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00000-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00001-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.postprocess_single_site_output.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/golden.training_examples.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/HG002_ONT_deeptrio.examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ./deeptrio/testdata/golden.vcf_candidate_importer.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/alt_aligned_pileup.golden.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 199, 8], ""channels"": [1, 2, 3, 4, 5, 6, 9, 10]}. ./deeptrio/testdata/golden.training_examples.tfrecord.gz-00000-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.training_examples.tfrecord.gz-00001-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./Dockerfile.deepsomatic:ARG VERSION_DEEPSOMATIC=1.6.0. ./deepvariant/testdata/golden.vcf_candidate_importer_calling_examples.tfrecord.example_info.json:{""version"": ""1.6.0"", ""shape"": [100, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:4723,deployability,version,version,4723,"{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.vcf_candidate_importer.calling_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00000-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00001-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.postprocess_single_site_output.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/golden.training_examples.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/HG002_ONT_deeptrio.examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ./deeptrio/testdata/golden.vcf_candidate_importer.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/alt_aligned_pileup.golden.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 199, 8], ""channels"": [1, 2, 3, 4, 5, 6, 9, 10]}. ./deeptrio/testdata/golden.training_examples.tfrecord.gz-00000-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.training_examples.tfrecord.gz-00001-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./Dockerfile.deepsomatic:ARG VERSION_DEEPSOMATIC=1.6.0. ./deepvariant/testdata/golden.vcf_candidate_importer_calling_examples.tfrecord.example_info.json:{""version"": ""1.6.0"", ""shape"": [100, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:502,energy efficiency,model,models,502,"Hi @fellen31 ,. That's right. . I checked the files. Only the `ARG` in Dockerfiles are relevant. ```. $ find . -type f -exec grep -H '1\.6\.0' {} \; . ./scripts/inference_deeptrio.sh: # --use_slim_model is introduced only after 1.6.0. ./scripts/inference_deepvariant.sh: # --use_slim_model is introduced only after 1.6.0. ./Dockerfile.deeptrio:ARG VERSION_DEEPTRIO=1.6.0. ./Dockerfile:ARG VERSION=1.6.0. ./.git/packed-refs:6b167016a0e6b92ca5b492e2dac9000d97c8d3ee refs/tags/v1.6.0. ./docs/FAQ.md:## Do models from before r1.6.0 work with current inference code? ./docs/FAQ.md:No. We have moved from Slim to Keras. All models before `1.6.0` were trained in. ./docs/FAQ.md:Slim platform. So they are not compatible with `1.6.0` anymore. ./docs/deeptrio-details-training-data.md:1.6.0 | <sup>[(6)](#vfootnote6)</sup>4 HG001, 3 HG002, 3 HG003, 3 HG004, 7 HG005, 6 HG006, 6 HG007, 4 NA12891, 4 NA12892 | 704,394,556. ./docs/deeptrio-details-training-data.md:1.6.0 | <sup>[(6)](#vfootnote6)</sup>2 HG001, 3 HG002, 3 HG003, 3 HG004, 7 HG005, 6 HG006, 6 HG007, 2 NA12891, 2 NA12892 | 457,420,038. ./docs/deeptrio-details-training-data.md:| 1.6.0 | <sup>[(6)](#vfootnote6)</sup>9 HG001, 7 HG002, 7 HG003, 7 HG004, 8 HG005, 8 HG006, 8 HG007, 9 NA12891, 9 NA12892 | 27,783,324 |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | <sup>[(6)](#vfootnote6)</sup>6 HG002, 6 HG003, 6 HG004, 8 HG005, 8 HG006, 8 HG007 | 13,039,595 |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | 9 HG002, 5 HG003, 5 HG004, 1 HG005, 1 HG006, 1 HG007 | 890,016,014<sup>[(5)](#vfootnote5)</sup> |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | 9 HG002, 5 HG003, 5 HG004, 1 HG005, 1 HG006, 1 HG007 | 838,515,085<sup>[(5)](#vfootnote5)</sup> |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | 1 HG002, 1 HG002, 1 HG004 | 50,249,704<sup>[(5)](#vfootnote5)</sup> |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | 1 HG002, 1 HG002, 1 HG004 | 99,675,190<sup>[(5)](#vfootnote5)</sup> |. ./deeptrio/testdata/golden.calling_exa",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:538,energy efficiency,current,current,538,"Hi @fellen31 ,. That's right. . I checked the files. Only the `ARG` in Dockerfiles are relevant. ```. $ find . -type f -exec grep -H '1\.6\.0' {} \; . ./scripts/inference_deeptrio.sh: # --use_slim_model is introduced only after 1.6.0. ./scripts/inference_deepvariant.sh: # --use_slim_model is introduced only after 1.6.0. ./Dockerfile.deeptrio:ARG VERSION_DEEPTRIO=1.6.0. ./Dockerfile:ARG VERSION=1.6.0. ./.git/packed-refs:6b167016a0e6b92ca5b492e2dac9000d97c8d3ee refs/tags/v1.6.0. ./docs/FAQ.md:## Do models from before r1.6.0 work with current inference code? ./docs/FAQ.md:No. We have moved from Slim to Keras. All models before `1.6.0` were trained in. ./docs/FAQ.md:Slim platform. So they are not compatible with `1.6.0` anymore. ./docs/deeptrio-details-training-data.md:1.6.0 | <sup>[(6)](#vfootnote6)</sup>4 HG001, 3 HG002, 3 HG003, 3 HG004, 7 HG005, 6 HG006, 6 HG007, 4 NA12891, 4 NA12892 | 704,394,556. ./docs/deeptrio-details-training-data.md:1.6.0 | <sup>[(6)](#vfootnote6)</sup>2 HG001, 3 HG002, 3 HG003, 3 HG004, 7 HG005, 6 HG006, 6 HG007, 2 NA12891, 2 NA12892 | 457,420,038. ./docs/deeptrio-details-training-data.md:| 1.6.0 | <sup>[(6)](#vfootnote6)</sup>9 HG001, 7 HG002, 7 HG003, 7 HG004, 8 HG005, 8 HG006, 8 HG007, 9 NA12891, 9 NA12892 | 27,783,324 |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | <sup>[(6)](#vfootnote6)</sup>6 HG002, 6 HG003, 6 HG004, 8 HG005, 8 HG006, 8 HG007 | 13,039,595 |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | 9 HG002, 5 HG003, 5 HG004, 1 HG005, 1 HG006, 1 HG007 | 890,016,014<sup>[(5)](#vfootnote5)</sup> |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | 9 HG002, 5 HG003, 5 HG004, 1 HG005, 1 HG006, 1 HG007 | 838,515,085<sup>[(5)](#vfootnote5)</sup> |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | 1 HG002, 1 HG002, 1 HG004 | 50,249,704<sup>[(5)](#vfootnote5)</sup> |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | 1 HG002, 1 HG002, 1 HG004 | 99,675,190<sup>[(5)](#vfootnote5)</sup> |. ./deeptrio/testdata/golden.calling_exa",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:618,energy efficiency,model,models,618,"Hi @fellen31 ,. That's right. . I checked the files. Only the `ARG` in Dockerfiles are relevant. ```. $ find . -type f -exec grep -H '1\.6\.0' {} \; . ./scripts/inference_deeptrio.sh: # --use_slim_model is introduced only after 1.6.0. ./scripts/inference_deepvariant.sh: # --use_slim_model is introduced only after 1.6.0. ./Dockerfile.deeptrio:ARG VERSION_DEEPTRIO=1.6.0. ./Dockerfile:ARG VERSION=1.6.0. ./.git/packed-refs:6b167016a0e6b92ca5b492e2dac9000d97c8d3ee refs/tags/v1.6.0. ./docs/FAQ.md:## Do models from before r1.6.0 work with current inference code? ./docs/FAQ.md:No. We have moved from Slim to Keras. All models before `1.6.0` were trained in. ./docs/FAQ.md:Slim platform. So they are not compatible with `1.6.0` anymore. ./docs/deeptrio-details-training-data.md:1.6.0 | <sup>[(6)](#vfootnote6)</sup>4 HG001, 3 HG002, 3 HG003, 3 HG004, 7 HG005, 6 HG006, 6 HG007, 4 NA12891, 4 NA12892 | 704,394,556. ./docs/deeptrio-details-training-data.md:1.6.0 | <sup>[(6)](#vfootnote6)</sup>2 HG001, 3 HG002, 3 HG003, 3 HG004, 7 HG005, 6 HG006, 6 HG007, 2 NA12891, 2 NA12892 | 457,420,038. ./docs/deeptrio-details-training-data.md:| 1.6.0 | <sup>[(6)](#vfootnote6)</sup>9 HG001, 7 HG002, 7 HG003, 7 HG004, 8 HG005, 8 HG006, 8 HG007, 9 NA12891, 9 NA12892 | 27,783,324 |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | <sup>[(6)](#vfootnote6)</sup>6 HG002, 6 HG003, 6 HG004, 8 HG005, 8 HG006, 8 HG007 | 13,039,595 |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | 9 HG002, 5 HG003, 5 HG004, 1 HG005, 1 HG006, 1 HG007 | 890,016,014<sup>[(5)](#vfootnote5)</sup> |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | 9 HG002, 5 HG003, 5 HG004, 1 HG005, 1 HG006, 1 HG007 | 838,515,085<sup>[(5)](#vfootnote5)</sup> |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | 1 HG002, 1 HG002, 1 HG004 | 50,249,704<sup>[(5)](#vfootnote5)</sup> |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | 1 HG002, 1 HG002, 1 HG004 | 99,675,190<sup>[(5)](#vfootnote5)</sup> |. ./deeptrio/testdata/golden.calling_exa",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:389,integrability,VERSION,VERSION,389,"Hi @fellen31 ,. That's right. . I checked the files. Only the `ARG` in Dockerfiles are relevant. ```. $ find . -type f -exec grep -H '1\.6\.0' {} \; . ./scripts/inference_deeptrio.sh: # --use_slim_model is introduced only after 1.6.0. ./scripts/inference_deepvariant.sh: # --use_slim_model is introduced only after 1.6.0. ./Dockerfile.deeptrio:ARG VERSION_DEEPTRIO=1.6.0. ./Dockerfile:ARG VERSION=1.6.0. ./.git/packed-refs:6b167016a0e6b92ca5b492e2dac9000d97c8d3ee refs/tags/v1.6.0. ./docs/FAQ.md:## Do models from before r1.6.0 work with current inference code? ./docs/FAQ.md:No. We have moved from Slim to Keras. All models before `1.6.0` were trained in. ./docs/FAQ.md:Slim platform. So they are not compatible with `1.6.0` anymore. ./docs/deeptrio-details-training-data.md:1.6.0 | <sup>[(6)](#vfootnote6)</sup>4 HG001, 3 HG002, 3 HG003, 3 HG004, 7 HG005, 6 HG006, 6 HG007, 4 NA12891, 4 NA12892 | 704,394,556. ./docs/deeptrio-details-training-data.md:1.6.0 | <sup>[(6)](#vfootnote6)</sup>2 HG001, 3 HG002, 3 HG003, 3 HG004, 7 HG005, 6 HG006, 6 HG007, 2 NA12891, 2 NA12892 | 457,420,038. ./docs/deeptrio-details-training-data.md:| 1.6.0 | <sup>[(6)](#vfootnote6)</sup>9 HG001, 7 HG002, 7 HG003, 7 HG004, 8 HG005, 8 HG006, 8 HG007, 9 NA12891, 9 NA12892 | 27,783,324 |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | <sup>[(6)](#vfootnote6)</sup>6 HG002, 6 HG003, 6 HG004, 8 HG005, 8 HG006, 8 HG007 | 13,039,595 |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | 9 HG002, 5 HG003, 5 HG004, 1 HG005, 1 HG006, 1 HG007 | 890,016,014<sup>[(5)](#vfootnote5)</sup> |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | 9 HG002, 5 HG003, 5 HG004, 1 HG005, 1 HG006, 1 HG007 | 838,515,085<sup>[(5)](#vfootnote5)</sup> |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | 1 HG002, 1 HG002, 1 HG004 | 50,249,704<sup>[(5)](#vfootnote5)</sup> |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | 1 HG002, 1 HG002, 1 HG004 | 99,675,190<sup>[(5)](#vfootnote5)</sup> |. ./deeptrio/testdata/golden.calling_exa",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:2053,integrability,version,version,2053,"12891, 2 NA12892 | 457,420,038. ./docs/deeptrio-details-training-data.md:| 1.6.0 | <sup>[(6)](#vfootnote6)</sup>9 HG001, 7 HG002, 7 HG003, 7 HG004, 8 HG005, 8 HG006, 8 HG007, 9 NA12891, 9 NA12892 | 27,783,324 |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | <sup>[(6)](#vfootnote6)</sup>6 HG002, 6 HG003, 6 HG004, 8 HG005, 8 HG006, 8 HG007 | 13,039,595 |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | 9 HG002, 5 HG003, 5 HG004, 1 HG005, 1 HG006, 1 HG007 | 890,016,014<sup>[(5)](#vfootnote5)</sup> |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | 9 HG002, 5 HG003, 5 HG004, 1 HG005, 1 HG006, 1 HG007 | 838,515,085<sup>[(5)](#vfootnote5)</sup> |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | 1 HG002, 1 HG002, 1 HG004 | 50,249,704<sup>[(5)](#vfootnote5)</sup> |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | 1 HG002, 1 HG002, 1 HG004 | 99,675,190<sup>[(5)](#vfootnote5)</sup> |. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00002-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/HG002_ONT_deeptrio.denovo.examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ./deeptrio/testdata/customized_classes.golden.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.postprocess_gvcf_output.g.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/golden.training_examples.tfrecord.gz-00002-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.vcf_candidate_importer.calling_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:2216,integrability,version,version,2216,"6, 8 HG007, 9 NA12891, 9 NA12892 | 27,783,324 |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | <sup>[(6)](#vfootnote6)</sup>6 HG002, 6 HG003, 6 HG004, 8 HG005, 8 HG006, 8 HG007 | 13,039,595 |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | 9 HG002, 5 HG003, 5 HG004, 1 HG005, 1 HG006, 1 HG007 | 890,016,014<sup>[(5)](#vfootnote5)</sup> |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | 9 HG002, 5 HG003, 5 HG004, 1 HG005, 1 HG006, 1 HG007 | 838,515,085<sup>[(5)](#vfootnote5)</sup> |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | 1 HG002, 1 HG002, 1 HG004 | 50,249,704<sup>[(5)](#vfootnote5)</sup> |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | 1 HG002, 1 HG002, 1 HG004 | 99,675,190<sup>[(5)](#vfootnote5)</sup> |. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00002-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/HG002_ONT_deeptrio.denovo.examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ./deeptrio/testdata/customized_classes.golden.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.postprocess_gvcf_output.g.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/golden.training_examples.tfrecord.gz-00002-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.vcf_candidate_importer.calling_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00000-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:2398,integrability,version,version,2398,"| 13,039,595 |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | 9 HG002, 5 HG003, 5 HG004, 1 HG005, 1 HG006, 1 HG007 | 890,016,014<sup>[(5)](#vfootnote5)</sup> |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | 9 HG002, 5 HG003, 5 HG004, 1 HG005, 1 HG006, 1 HG007 | 838,515,085<sup>[(5)](#vfootnote5)</sup> |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | 1 HG002, 1 HG002, 1 HG004 | 50,249,704<sup>[(5)](#vfootnote5)</sup> |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | 1 HG002, 1 HG002, 1 HG004 | 99,675,190<sup>[(5)](#vfootnote5)</sup> |. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00002-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/HG002_ONT_deeptrio.denovo.examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ./deeptrio/testdata/customized_classes.golden.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.postprocess_gvcf_output.g.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/golden.training_examples.tfrecord.gz-00002-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.vcf_candidate_importer.calling_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00000-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00001-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/te",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:2550,integrability,version,version,2550,"te5)</sup> |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | 9 HG002, 5 HG003, 5 HG004, 1 HG005, 1 HG006, 1 HG007 | 838,515,085<sup>[(5)](#vfootnote5)</sup> |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | 1 HG002, 1 HG002, 1 HG004 | 50,249,704<sup>[(5)](#vfootnote5)</sup> |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | 1 HG002, 1 HG002, 1 HG004 | 99,675,190<sup>[(5)](#vfootnote5)</sup> |. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00002-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/HG002_ONT_deeptrio.denovo.examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ./deeptrio/testdata/customized_classes.golden.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.postprocess_gvcf_output.g.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/golden.training_examples.tfrecord.gz-00002-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.vcf_candidate_importer.calling_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00000-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00001-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.postprocess_single_site_output.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/golden.training_examples.vcf:##DeepVariant_version=1.6",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:2804,integrability,version,version,2804,"sup>[(5)](#vfootnote5)</sup> |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | 1 HG002, 1 HG002, 1 HG004 | 99,675,190<sup>[(5)](#vfootnote5)</sup> |. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00002-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/HG002_ONT_deeptrio.denovo.examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ./deeptrio/testdata/customized_classes.golden.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.postprocess_gvcf_output.g.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/golden.training_examples.tfrecord.gz-00002-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.vcf_candidate_importer.calling_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00000-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00001-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.postprocess_single_site_output.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/golden.training_examples.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/HG002_ONT_deeptrio.examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ./deeptrio/testdata/golden.vcf_candidate_importer.training_examples.tfrecord.gz.exam",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:2979,integrability,version,version,2979,"olden.calling_examples.tfrecord.gz-00002-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/HG002_ONT_deeptrio.denovo.examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ./deeptrio/testdata/customized_classes.golden.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.postprocess_gvcf_output.g.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/golden.training_examples.tfrecord.gz-00002-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.vcf_candidate_importer.calling_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00000-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00001-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.postprocess_single_site_output.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/golden.training_examples.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/HG002_ONT_deeptrio.examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ./deeptrio/testdata/golden.vcf_candidate_importer.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/alt_aligned_pileup.golden.training_examples.tfrecord.gz.example",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:3146,integrability,version,version,3146,"G002_ONT_deeptrio.denovo.examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ./deeptrio/testdata/customized_classes.golden.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.postprocess_gvcf_output.g.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/golden.training_examples.tfrecord.gz-00002-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.vcf_candidate_importer.calling_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00000-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00001-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.postprocess_single_site_output.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/golden.training_examples.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/HG002_ONT_deeptrio.examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ./deeptrio/testdata/golden.vcf_candidate_importer.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/alt_aligned_pileup.golden.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 199, 8], ""channels"": [1, 2, 3, 4, 5, 6, 9, 10]}. ./deeptrio/testdata/golden.training_examples.tfrecord.gz-00000-of-00003",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:3313,integrability,version,version,3313,"data/customized_classes.golden.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.postprocess_gvcf_output.g.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/golden.training_examples.tfrecord.gz-00002-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.vcf_candidate_importer.calling_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00000-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00001-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.postprocess_single_site_output.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/golden.training_examples.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/HG002_ONT_deeptrio.examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ./deeptrio/testdata/golden.vcf_candidate_importer.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/alt_aligned_pileup.golden.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 199, 8], ""channels"": [1, 2, 3, 4, 5, 6, 9, 10]}. ./deeptrio/testdata/golden.training_examples.tfrecord.gz-00000-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.training_examples.tfrecord.gz.example_info.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:3638,integrability,version,version,3638,"ata/golden.postprocess_gvcf_output.g.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/golden.training_examples.tfrecord.gz-00002-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.vcf_candidate_importer.calling_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00000-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00001-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.postprocess_single_site_output.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/golden.training_examples.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/HG002_ONT_deeptrio.examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ./deeptrio/testdata/golden.vcf_candidate_importer.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/alt_aligned_pileup.golden.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 199, 8], ""channels"": [1, 2, 3, 4, 5, 6, 9, 10]}. ./deeptrio/testdata/golden.training_examples.tfrecord.gz-00000-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.training_examples.tfrecord.gz-00001-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./Dockerfile.deepsomatic:ARG VERSION_DEEPSOMATIC=1.6.0. ./deepvariant/test",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:3824,integrability,version,version,3824,"{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.vcf_candidate_importer.calling_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00000-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00001-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.postprocess_single_site_output.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/golden.training_examples.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/HG002_ONT_deeptrio.examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ./deeptrio/testdata/golden.vcf_candidate_importer.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/alt_aligned_pileup.golden.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 199, 8], ""channels"": [1, 2, 3, 4, 5, 6, 9, 10]}. ./deeptrio/testdata/golden.training_examples.tfrecord.gz-00000-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.training_examples.tfrecord.gz-00001-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./Dockerfile.deepsomatic:ARG VERSION_DEEPSOMATIC=1.6.0. ./deepvariant/testdata/golden.vcf_candidate_importer_calling_examples.tfrecord.example_info.json:{""version"": ""1.6.0"", ""shape"": [100, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:3996,integrability,version,version,3996,"{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.vcf_candidate_importer.calling_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00000-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00001-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.postprocess_single_site_output.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/golden.training_examples.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/HG002_ONT_deeptrio.examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ./deeptrio/testdata/golden.vcf_candidate_importer.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/alt_aligned_pileup.golden.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 199, 8], ""channels"": [1, 2, 3, 4, 5, 6, 9, 10]}. ./deeptrio/testdata/golden.training_examples.tfrecord.gz-00000-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.training_examples.tfrecord.gz-00001-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./Dockerfile.deepsomatic:ARG VERSION_DEEPSOMATIC=1.6.0. ./deepvariant/testdata/golden.vcf_candidate_importer_calling_examples.tfrecord.example_info.json:{""version"": ""1.6.0"", ""shape"": [100, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:4171,integrability,version,version,4171,"{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.vcf_candidate_importer.calling_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00000-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00001-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.postprocess_single_site_output.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/golden.training_examples.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/HG002_ONT_deeptrio.examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ./deeptrio/testdata/golden.vcf_candidate_importer.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/alt_aligned_pileup.golden.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 199, 8], ""channels"": [1, 2, 3, 4, 5, 6, 9, 10]}. ./deeptrio/testdata/golden.training_examples.tfrecord.gz-00000-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.training_examples.tfrecord.gz-00001-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./Dockerfile.deepsomatic:ARG VERSION_DEEPSOMATIC=1.6.0. ./deepvariant/testdata/golden.vcf_candidate_importer_calling_examples.tfrecord.example_info.json:{""version"": ""1.6.0"", ""shape"": [100, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:4324,integrability,version,version,4324,"{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.vcf_candidate_importer.calling_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00000-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00001-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.postprocess_single_site_output.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/golden.training_examples.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/HG002_ONT_deeptrio.examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ./deeptrio/testdata/golden.vcf_candidate_importer.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/alt_aligned_pileup.golden.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 199, 8], ""channels"": [1, 2, 3, 4, 5, 6, 9, 10]}. ./deeptrio/testdata/golden.training_examples.tfrecord.gz-00000-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.training_examples.tfrecord.gz-00001-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./Dockerfile.deepsomatic:ARG VERSION_DEEPSOMATIC=1.6.0. ./deepvariant/testdata/golden.vcf_candidate_importer_calling_examples.tfrecord.example_info.json:{""version"": ""1.6.0"", ""shape"": [100, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:4492,integrability,version,version,4492,"{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.vcf_candidate_importer.calling_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00000-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00001-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.postprocess_single_site_output.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/golden.training_examples.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/HG002_ONT_deeptrio.examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ./deeptrio/testdata/golden.vcf_candidate_importer.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/alt_aligned_pileup.golden.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 199, 8], ""channels"": [1, 2, 3, 4, 5, 6, 9, 10]}. ./deeptrio/testdata/golden.training_examples.tfrecord.gz-00000-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.training_examples.tfrecord.gz-00001-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./Dockerfile.deepsomatic:ARG VERSION_DEEPSOMATIC=1.6.0. ./deepvariant/testdata/golden.vcf_candidate_importer_calling_examples.tfrecord.example_info.json:{""version"": ""1.6.0"", ""shape"": [100, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:4723,integrability,version,version,4723,"{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.vcf_candidate_importer.calling_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00000-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00001-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.postprocess_single_site_output.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/golden.training_examples.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/HG002_ONT_deeptrio.examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ./deeptrio/testdata/golden.vcf_candidate_importer.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/alt_aligned_pileup.golden.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 199, 8], ""channels"": [1, 2, 3, 4, 5, 6, 9, 10]}. ./deeptrio/testdata/golden.training_examples.tfrecord.gz-00000-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.training_examples.tfrecord.gz-00001-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./Dockerfile.deepsomatic:ARG VERSION_DEEPSOMATIC=1.6.0. ./deepvariant/testdata/golden.vcf_candidate_importer_calling_examples.tfrecord.example_info.json:{""version"": ""1.6.0"", ""shape"": [100, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:676,interoperability,platform,platform,676,"Hi @fellen31 ,. That's right. . I checked the files. Only the `ARG` in Dockerfiles are relevant. ```. $ find . -type f -exec grep -H '1\.6\.0' {} \; . ./scripts/inference_deeptrio.sh: # --use_slim_model is introduced only after 1.6.0. ./scripts/inference_deepvariant.sh: # --use_slim_model is introduced only after 1.6.0. ./Dockerfile.deeptrio:ARG VERSION_DEEPTRIO=1.6.0. ./Dockerfile:ARG VERSION=1.6.0. ./.git/packed-refs:6b167016a0e6b92ca5b492e2dac9000d97c8d3ee refs/tags/v1.6.0. ./docs/FAQ.md:## Do models from before r1.6.0 work with current inference code? ./docs/FAQ.md:No. We have moved from Slim to Keras. All models before `1.6.0` were trained in. ./docs/FAQ.md:Slim platform. So they are not compatible with `1.6.0` anymore. ./docs/deeptrio-details-training-data.md:1.6.0 | <sup>[(6)](#vfootnote6)</sup>4 HG001, 3 HG002, 3 HG003, 3 HG004, 7 HG005, 6 HG006, 6 HG007, 4 NA12891, 4 NA12892 | 704,394,556. ./docs/deeptrio-details-training-data.md:1.6.0 | <sup>[(6)](#vfootnote6)</sup>2 HG001, 3 HG002, 3 HG003, 3 HG004, 7 HG005, 6 HG006, 6 HG007, 2 NA12891, 2 NA12892 | 457,420,038. ./docs/deeptrio-details-training-data.md:| 1.6.0 | <sup>[(6)](#vfootnote6)</sup>9 HG001, 7 HG002, 7 HG003, 7 HG004, 8 HG005, 8 HG006, 8 HG007, 9 NA12891, 9 NA12892 | 27,783,324 |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | <sup>[(6)](#vfootnote6)</sup>6 HG002, 6 HG003, 6 HG004, 8 HG005, 8 HG006, 8 HG007 | 13,039,595 |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | 9 HG002, 5 HG003, 5 HG004, 1 HG005, 1 HG006, 1 HG007 | 890,016,014<sup>[(5)](#vfootnote5)</sup> |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | 9 HG002, 5 HG003, 5 HG004, 1 HG005, 1 HG006, 1 HG007 | 838,515,085<sup>[(5)](#vfootnote5)</sup> |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | 1 HG002, 1 HG002, 1 HG004 | 50,249,704<sup>[(5)](#vfootnote5)</sup> |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | 1 HG002, 1 HG002, 1 HG004 | 99,675,190<sup>[(5)](#vfootnote5)</sup> |. ./deeptrio/testdata/golden.calling_exa",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:702,interoperability,compatib,compatible,702,"Hi @fellen31 ,. That's right. . I checked the files. Only the `ARG` in Dockerfiles are relevant. ```. $ find . -type f -exec grep -H '1\.6\.0' {} \; . ./scripts/inference_deeptrio.sh: # --use_slim_model is introduced only after 1.6.0. ./scripts/inference_deepvariant.sh: # --use_slim_model is introduced only after 1.6.0. ./Dockerfile.deeptrio:ARG VERSION_DEEPTRIO=1.6.0. ./Dockerfile:ARG VERSION=1.6.0. ./.git/packed-refs:6b167016a0e6b92ca5b492e2dac9000d97c8d3ee refs/tags/v1.6.0. ./docs/FAQ.md:## Do models from before r1.6.0 work with current inference code? ./docs/FAQ.md:No. We have moved from Slim to Keras. All models before `1.6.0` were trained in. ./docs/FAQ.md:Slim platform. So they are not compatible with `1.6.0` anymore. ./docs/deeptrio-details-training-data.md:1.6.0 | <sup>[(6)](#vfootnote6)</sup>4 HG001, 3 HG002, 3 HG003, 3 HG004, 7 HG005, 6 HG006, 6 HG007, 4 NA12891, 4 NA12892 | 704,394,556. ./docs/deeptrio-details-training-data.md:1.6.0 | <sup>[(6)](#vfootnote6)</sup>2 HG001, 3 HG002, 3 HG003, 3 HG004, 7 HG005, 6 HG006, 6 HG007, 2 NA12891, 2 NA12892 | 457,420,038. ./docs/deeptrio-details-training-data.md:| 1.6.0 | <sup>[(6)](#vfootnote6)</sup>9 HG001, 7 HG002, 7 HG003, 7 HG004, 8 HG005, 8 HG006, 8 HG007, 9 NA12891, 9 NA12892 | 27,783,324 |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | <sup>[(6)](#vfootnote6)</sup>6 HG002, 6 HG003, 6 HG004, 8 HG005, 8 HG006, 8 HG007 | 13,039,595 |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | 9 HG002, 5 HG003, 5 HG004, 1 HG005, 1 HG006, 1 HG007 | 890,016,014<sup>[(5)](#vfootnote5)</sup> |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | 9 HG002, 5 HG003, 5 HG004, 1 HG005, 1 HG006, 1 HG007 | 838,515,085<sup>[(5)](#vfootnote5)</sup> |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | 1 HG002, 1 HG002, 1 HG004 | 50,249,704<sup>[(5)](#vfootnote5)</sup> |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | 1 HG002, 1 HG002, 1 HG004 | 99,675,190<sup>[(5)](#vfootnote5)</sup> |. ./deeptrio/testdata/golden.calling_exa",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:389,modifiability,VERSION,VERSION,389,"Hi @fellen31 ,. That's right. . I checked the files. Only the `ARG` in Dockerfiles are relevant. ```. $ find . -type f -exec grep -H '1\.6\.0' {} \; . ./scripts/inference_deeptrio.sh: # --use_slim_model is introduced only after 1.6.0. ./scripts/inference_deepvariant.sh: # --use_slim_model is introduced only after 1.6.0. ./Dockerfile.deeptrio:ARG VERSION_DEEPTRIO=1.6.0. ./Dockerfile:ARG VERSION=1.6.0. ./.git/packed-refs:6b167016a0e6b92ca5b492e2dac9000d97c8d3ee refs/tags/v1.6.0. ./docs/FAQ.md:## Do models from before r1.6.0 work with current inference code? ./docs/FAQ.md:No. We have moved from Slim to Keras. All models before `1.6.0` were trained in. ./docs/FAQ.md:Slim platform. So they are not compatible with `1.6.0` anymore. ./docs/deeptrio-details-training-data.md:1.6.0 | <sup>[(6)](#vfootnote6)</sup>4 HG001, 3 HG002, 3 HG003, 3 HG004, 7 HG005, 6 HG006, 6 HG007, 4 NA12891, 4 NA12892 | 704,394,556. ./docs/deeptrio-details-training-data.md:1.6.0 | <sup>[(6)](#vfootnote6)</sup>2 HG001, 3 HG002, 3 HG003, 3 HG004, 7 HG005, 6 HG006, 6 HG007, 2 NA12891, 2 NA12892 | 457,420,038. ./docs/deeptrio-details-training-data.md:| 1.6.0 | <sup>[(6)](#vfootnote6)</sup>9 HG001, 7 HG002, 7 HG003, 7 HG004, 8 HG005, 8 HG006, 8 HG007, 9 NA12891, 9 NA12892 | 27,783,324 |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | <sup>[(6)](#vfootnote6)</sup>6 HG002, 6 HG003, 6 HG004, 8 HG005, 8 HG006, 8 HG007 | 13,039,595 |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | 9 HG002, 5 HG003, 5 HG004, 1 HG005, 1 HG006, 1 HG007 | 890,016,014<sup>[(5)](#vfootnote5)</sup> |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | 9 HG002, 5 HG003, 5 HG004, 1 HG005, 1 HG006, 1 HG007 | 838,515,085<sup>[(5)](#vfootnote5)</sup> |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | 1 HG002, 1 HG002, 1 HG004 | 50,249,704<sup>[(5)](#vfootnote5)</sup> |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | 1 HG002, 1 HG002, 1 HG004 | 99,675,190<sup>[(5)](#vfootnote5)</sup> |. ./deeptrio/testdata/golden.calling_exa",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:411,modifiability,pac,packed-refs,411,"Hi @fellen31 ,. That's right. . I checked the files. Only the `ARG` in Dockerfiles are relevant. ```. $ find . -type f -exec grep -H '1\.6\.0' {} \; . ./scripts/inference_deeptrio.sh: # --use_slim_model is introduced only after 1.6.0. ./scripts/inference_deepvariant.sh: # --use_slim_model is introduced only after 1.6.0. ./Dockerfile.deeptrio:ARG VERSION_DEEPTRIO=1.6.0. ./Dockerfile:ARG VERSION=1.6.0. ./.git/packed-refs:6b167016a0e6b92ca5b492e2dac9000d97c8d3ee refs/tags/v1.6.0. ./docs/FAQ.md:## Do models from before r1.6.0 work with current inference code? ./docs/FAQ.md:No. We have moved from Slim to Keras. All models before `1.6.0` were trained in. ./docs/FAQ.md:Slim platform. So they are not compatible with `1.6.0` anymore. ./docs/deeptrio-details-training-data.md:1.6.0 | <sup>[(6)](#vfootnote6)</sup>4 HG001, 3 HG002, 3 HG003, 3 HG004, 7 HG005, 6 HG006, 6 HG007, 4 NA12891, 4 NA12892 | 704,394,556. ./docs/deeptrio-details-training-data.md:1.6.0 | <sup>[(6)](#vfootnote6)</sup>2 HG001, 3 HG002, 3 HG003, 3 HG004, 7 HG005, 6 HG006, 6 HG007, 2 NA12891, 2 NA12892 | 457,420,038. ./docs/deeptrio-details-training-data.md:| 1.6.0 | <sup>[(6)](#vfootnote6)</sup>9 HG001, 7 HG002, 7 HG003, 7 HG004, 8 HG005, 8 HG006, 8 HG007, 9 NA12891, 9 NA12892 | 27,783,324 |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | <sup>[(6)](#vfootnote6)</sup>6 HG002, 6 HG003, 6 HG004, 8 HG005, 8 HG006, 8 HG007 | 13,039,595 |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | 9 HG002, 5 HG003, 5 HG004, 1 HG005, 1 HG006, 1 HG007 | 890,016,014<sup>[(5)](#vfootnote5)</sup> |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | 9 HG002, 5 HG003, 5 HG004, 1 HG005, 1 HG006, 1 HG007 | 838,515,085<sup>[(5)](#vfootnote5)</sup> |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | 1 HG002, 1 HG002, 1 HG004 | 50,249,704<sup>[(5)](#vfootnote5)</sup> |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | 1 HG002, 1 HG002, 1 HG004 | 99,675,190<sup>[(5)](#vfootnote5)</sup> |. ./deeptrio/testdata/golden.calling_exa",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:2053,modifiability,version,version,2053,"12891, 2 NA12892 | 457,420,038. ./docs/deeptrio-details-training-data.md:| 1.6.0 | <sup>[(6)](#vfootnote6)</sup>9 HG001, 7 HG002, 7 HG003, 7 HG004, 8 HG005, 8 HG006, 8 HG007, 9 NA12891, 9 NA12892 | 27,783,324 |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | <sup>[(6)](#vfootnote6)</sup>6 HG002, 6 HG003, 6 HG004, 8 HG005, 8 HG006, 8 HG007 | 13,039,595 |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | 9 HG002, 5 HG003, 5 HG004, 1 HG005, 1 HG006, 1 HG007 | 890,016,014<sup>[(5)](#vfootnote5)</sup> |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | 9 HG002, 5 HG003, 5 HG004, 1 HG005, 1 HG006, 1 HG007 | 838,515,085<sup>[(5)](#vfootnote5)</sup> |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | 1 HG002, 1 HG002, 1 HG004 | 50,249,704<sup>[(5)](#vfootnote5)</sup> |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | 1 HG002, 1 HG002, 1 HG004 | 99,675,190<sup>[(5)](#vfootnote5)</sup> |. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00002-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/HG002_ONT_deeptrio.denovo.examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ./deeptrio/testdata/customized_classes.golden.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.postprocess_gvcf_output.g.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/golden.training_examples.tfrecord.gz-00002-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.vcf_candidate_importer.calling_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:2216,modifiability,version,version,2216,"6, 8 HG007, 9 NA12891, 9 NA12892 | 27,783,324 |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | <sup>[(6)](#vfootnote6)</sup>6 HG002, 6 HG003, 6 HG004, 8 HG005, 8 HG006, 8 HG007 | 13,039,595 |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | 9 HG002, 5 HG003, 5 HG004, 1 HG005, 1 HG006, 1 HG007 | 890,016,014<sup>[(5)](#vfootnote5)</sup> |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | 9 HG002, 5 HG003, 5 HG004, 1 HG005, 1 HG006, 1 HG007 | 838,515,085<sup>[(5)](#vfootnote5)</sup> |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | 1 HG002, 1 HG002, 1 HG004 | 50,249,704<sup>[(5)](#vfootnote5)</sup> |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | 1 HG002, 1 HG002, 1 HG004 | 99,675,190<sup>[(5)](#vfootnote5)</sup> |. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00002-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/HG002_ONT_deeptrio.denovo.examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ./deeptrio/testdata/customized_classes.golden.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.postprocess_gvcf_output.g.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/golden.training_examples.tfrecord.gz-00002-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.vcf_candidate_importer.calling_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00000-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:2398,modifiability,version,version,2398,"| 13,039,595 |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | 9 HG002, 5 HG003, 5 HG004, 1 HG005, 1 HG006, 1 HG007 | 890,016,014<sup>[(5)](#vfootnote5)</sup> |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | 9 HG002, 5 HG003, 5 HG004, 1 HG005, 1 HG006, 1 HG007 | 838,515,085<sup>[(5)](#vfootnote5)</sup> |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | 1 HG002, 1 HG002, 1 HG004 | 50,249,704<sup>[(5)](#vfootnote5)</sup> |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | 1 HG002, 1 HG002, 1 HG004 | 99,675,190<sup>[(5)](#vfootnote5)</sup> |. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00002-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/HG002_ONT_deeptrio.denovo.examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ./deeptrio/testdata/customized_classes.golden.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.postprocess_gvcf_output.g.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/golden.training_examples.tfrecord.gz-00002-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.vcf_candidate_importer.calling_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00000-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00001-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/te",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:2550,modifiability,version,version,2550,"te5)</sup> |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | 9 HG002, 5 HG003, 5 HG004, 1 HG005, 1 HG006, 1 HG007 | 838,515,085<sup>[(5)](#vfootnote5)</sup> |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | 1 HG002, 1 HG002, 1 HG004 | 50,249,704<sup>[(5)](#vfootnote5)</sup> |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | 1 HG002, 1 HG002, 1 HG004 | 99,675,190<sup>[(5)](#vfootnote5)</sup> |. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00002-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/HG002_ONT_deeptrio.denovo.examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ./deeptrio/testdata/customized_classes.golden.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.postprocess_gvcf_output.g.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/golden.training_examples.tfrecord.gz-00002-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.vcf_candidate_importer.calling_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00000-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00001-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.postprocess_single_site_output.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/golden.training_examples.vcf:##DeepVariant_version=1.6",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:2804,modifiability,version,version,2804,"sup>[(5)](#vfootnote5)</sup> |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | 1 HG002, 1 HG002, 1 HG004 | 99,675,190<sup>[(5)](#vfootnote5)</sup> |. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00002-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/HG002_ONT_deeptrio.denovo.examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ./deeptrio/testdata/customized_classes.golden.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.postprocess_gvcf_output.g.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/golden.training_examples.tfrecord.gz-00002-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.vcf_candidate_importer.calling_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00000-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00001-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.postprocess_single_site_output.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/golden.training_examples.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/HG002_ONT_deeptrio.examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ./deeptrio/testdata/golden.vcf_candidate_importer.training_examples.tfrecord.gz.exam",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:2979,modifiability,version,version,2979,"olden.calling_examples.tfrecord.gz-00002-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/HG002_ONT_deeptrio.denovo.examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ./deeptrio/testdata/customized_classes.golden.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.postprocess_gvcf_output.g.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/golden.training_examples.tfrecord.gz-00002-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.vcf_candidate_importer.calling_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00000-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00001-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.postprocess_single_site_output.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/golden.training_examples.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/HG002_ONT_deeptrio.examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ./deeptrio/testdata/golden.vcf_candidate_importer.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/alt_aligned_pileup.golden.training_examples.tfrecord.gz.example",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:3146,modifiability,version,version,3146,"G002_ONT_deeptrio.denovo.examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ./deeptrio/testdata/customized_classes.golden.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.postprocess_gvcf_output.g.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/golden.training_examples.tfrecord.gz-00002-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.vcf_candidate_importer.calling_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00000-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00001-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.postprocess_single_site_output.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/golden.training_examples.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/HG002_ONT_deeptrio.examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ./deeptrio/testdata/golden.vcf_candidate_importer.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/alt_aligned_pileup.golden.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 199, 8], ""channels"": [1, 2, 3, 4, 5, 6, 9, 10]}. ./deeptrio/testdata/golden.training_examples.tfrecord.gz-00000-of-00003",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:3313,modifiability,version,version,3313,"data/customized_classes.golden.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.postprocess_gvcf_output.g.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/golden.training_examples.tfrecord.gz-00002-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.vcf_candidate_importer.calling_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00000-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00001-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.postprocess_single_site_output.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/golden.training_examples.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/HG002_ONT_deeptrio.examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ./deeptrio/testdata/golden.vcf_candidate_importer.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/alt_aligned_pileup.golden.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 199, 8], ""channels"": [1, 2, 3, 4, 5, 6, 9, 10]}. ./deeptrio/testdata/golden.training_examples.tfrecord.gz-00000-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.training_examples.tfrecord.gz.example_info.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:3638,modifiability,version,version,3638,"ata/golden.postprocess_gvcf_output.g.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/golden.training_examples.tfrecord.gz-00002-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.vcf_candidate_importer.calling_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00000-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00001-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.postprocess_single_site_output.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/golden.training_examples.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/HG002_ONT_deeptrio.examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ./deeptrio/testdata/golden.vcf_candidate_importer.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/alt_aligned_pileup.golden.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 199, 8], ""channels"": [1, 2, 3, 4, 5, 6, 9, 10]}. ./deeptrio/testdata/golden.training_examples.tfrecord.gz-00000-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.training_examples.tfrecord.gz-00001-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./Dockerfile.deepsomatic:ARG VERSION_DEEPSOMATIC=1.6.0. ./deepvariant/test",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:3824,modifiability,version,version,3824,"{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.vcf_candidate_importer.calling_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00000-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00001-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.postprocess_single_site_output.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/golden.training_examples.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/HG002_ONT_deeptrio.examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ./deeptrio/testdata/golden.vcf_candidate_importer.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/alt_aligned_pileup.golden.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 199, 8], ""channels"": [1, 2, 3, 4, 5, 6, 9, 10]}. ./deeptrio/testdata/golden.training_examples.tfrecord.gz-00000-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.training_examples.tfrecord.gz-00001-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./Dockerfile.deepsomatic:ARG VERSION_DEEPSOMATIC=1.6.0. ./deepvariant/testdata/golden.vcf_candidate_importer_calling_examples.tfrecord.example_info.json:{""version"": ""1.6.0"", ""shape"": [100, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:3996,modifiability,version,version,3996,"{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.vcf_candidate_importer.calling_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00000-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00001-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.postprocess_single_site_output.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/golden.training_examples.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/HG002_ONT_deeptrio.examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ./deeptrio/testdata/golden.vcf_candidate_importer.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/alt_aligned_pileup.golden.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 199, 8], ""channels"": [1, 2, 3, 4, 5, 6, 9, 10]}. ./deeptrio/testdata/golden.training_examples.tfrecord.gz-00000-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.training_examples.tfrecord.gz-00001-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./Dockerfile.deepsomatic:ARG VERSION_DEEPSOMATIC=1.6.0. ./deepvariant/testdata/golden.vcf_candidate_importer_calling_examples.tfrecord.example_info.json:{""version"": ""1.6.0"", ""shape"": [100, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:4171,modifiability,version,version,4171,"{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.vcf_candidate_importer.calling_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00000-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00001-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.postprocess_single_site_output.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/golden.training_examples.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/HG002_ONT_deeptrio.examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ./deeptrio/testdata/golden.vcf_candidate_importer.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/alt_aligned_pileup.golden.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 199, 8], ""channels"": [1, 2, 3, 4, 5, 6, 9, 10]}. ./deeptrio/testdata/golden.training_examples.tfrecord.gz-00000-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.training_examples.tfrecord.gz-00001-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./Dockerfile.deepsomatic:ARG VERSION_DEEPSOMATIC=1.6.0. ./deepvariant/testdata/golden.vcf_candidate_importer_calling_examples.tfrecord.example_info.json:{""version"": ""1.6.0"", ""shape"": [100, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:4324,modifiability,version,version,4324,"{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.vcf_candidate_importer.calling_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00000-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00001-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.postprocess_single_site_output.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/golden.training_examples.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/HG002_ONT_deeptrio.examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ./deeptrio/testdata/golden.vcf_candidate_importer.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/alt_aligned_pileup.golden.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 199, 8], ""channels"": [1, 2, 3, 4, 5, 6, 9, 10]}. ./deeptrio/testdata/golden.training_examples.tfrecord.gz-00000-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.training_examples.tfrecord.gz-00001-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./Dockerfile.deepsomatic:ARG VERSION_DEEPSOMATIC=1.6.0. ./deepvariant/testdata/golden.vcf_candidate_importer_calling_examples.tfrecord.example_info.json:{""version"": ""1.6.0"", ""shape"": [100, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:4492,modifiability,version,version,4492,"{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.vcf_candidate_importer.calling_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00000-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00001-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.postprocess_single_site_output.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/golden.training_examples.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/HG002_ONT_deeptrio.examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ./deeptrio/testdata/golden.vcf_candidate_importer.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/alt_aligned_pileup.golden.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 199, 8], ""channels"": [1, 2, 3, 4, 5, 6, 9, 10]}. ./deeptrio/testdata/golden.training_examples.tfrecord.gz-00000-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.training_examples.tfrecord.gz-00001-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./Dockerfile.deepsomatic:ARG VERSION_DEEPSOMATIC=1.6.0. ./deepvariant/testdata/golden.vcf_candidate_importer_calling_examples.tfrecord.example_info.json:{""version"": ""1.6.0"", ""shape"": [100, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:4723,modifiability,version,version,4723,"{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.vcf_candidate_importer.calling_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00000-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00001-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.postprocess_single_site_output.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/golden.training_examples.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/HG002_ONT_deeptrio.examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ./deeptrio/testdata/golden.vcf_candidate_importer.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/alt_aligned_pileup.golden.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 199, 8], ""channels"": [1, 2, 3, 4, 5, 6, 9, 10]}. ./deeptrio/testdata/golden.training_examples.tfrecord.gz-00000-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.training_examples.tfrecord.gz-00001-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./Dockerfile.deepsomatic:ARG VERSION_DEEPSOMATIC=1.6.0. ./deepvariant/testdata/golden.vcf_candidate_importer_calling_examples.tfrecord.example_info.json:{""version"": ""1.6.0"", ""shape"": [100, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:599,reliability,Sli,Slim,599,"Hi @fellen31 ,. That's right. . I checked the files. Only the `ARG` in Dockerfiles are relevant. ```. $ find . -type f -exec grep -H '1\.6\.0' {} \; . ./scripts/inference_deeptrio.sh: # --use_slim_model is introduced only after 1.6.0. ./scripts/inference_deepvariant.sh: # --use_slim_model is introduced only after 1.6.0. ./Dockerfile.deeptrio:ARG VERSION_DEEPTRIO=1.6.0. ./Dockerfile:ARG VERSION=1.6.0. ./.git/packed-refs:6b167016a0e6b92ca5b492e2dac9000d97c8d3ee refs/tags/v1.6.0. ./docs/FAQ.md:## Do models from before r1.6.0 work with current inference code? ./docs/FAQ.md:No. We have moved from Slim to Keras. All models before `1.6.0` were trained in. ./docs/FAQ.md:Slim platform. So they are not compatible with `1.6.0` anymore. ./docs/deeptrio-details-training-data.md:1.6.0 | <sup>[(6)](#vfootnote6)</sup>4 HG001, 3 HG002, 3 HG003, 3 HG004, 7 HG005, 6 HG006, 6 HG007, 4 NA12891, 4 NA12892 | 704,394,556. ./docs/deeptrio-details-training-data.md:1.6.0 | <sup>[(6)](#vfootnote6)</sup>2 HG001, 3 HG002, 3 HG003, 3 HG004, 7 HG005, 6 HG006, 6 HG007, 2 NA12891, 2 NA12892 | 457,420,038. ./docs/deeptrio-details-training-data.md:| 1.6.0 | <sup>[(6)](#vfootnote6)</sup>9 HG001, 7 HG002, 7 HG003, 7 HG004, 8 HG005, 8 HG006, 8 HG007, 9 NA12891, 9 NA12892 | 27,783,324 |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | <sup>[(6)](#vfootnote6)</sup>6 HG002, 6 HG003, 6 HG004, 8 HG005, 8 HG006, 8 HG007 | 13,039,595 |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | 9 HG002, 5 HG003, 5 HG004, 1 HG005, 1 HG006, 1 HG007 | 890,016,014<sup>[(5)](#vfootnote5)</sup> |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | 9 HG002, 5 HG003, 5 HG004, 1 HG005, 1 HG006, 1 HG007 | 838,515,085<sup>[(5)](#vfootnote5)</sup> |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | 1 HG002, 1 HG002, 1 HG004 | 50,249,704<sup>[(5)](#vfootnote5)</sup> |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | 1 HG002, 1 HG002, 1 HG004 | 99,675,190<sup>[(5)](#vfootnote5)</sup> |. ./deeptrio/testdata/golden.calling_exa",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:671,reliability,Sli,Slim,671,"Hi @fellen31 ,. That's right. . I checked the files. Only the `ARG` in Dockerfiles are relevant. ```. $ find . -type f -exec grep -H '1\.6\.0' {} \; . ./scripts/inference_deeptrio.sh: # --use_slim_model is introduced only after 1.6.0. ./scripts/inference_deepvariant.sh: # --use_slim_model is introduced only after 1.6.0. ./Dockerfile.deeptrio:ARG VERSION_DEEPTRIO=1.6.0. ./Dockerfile:ARG VERSION=1.6.0. ./.git/packed-refs:6b167016a0e6b92ca5b492e2dac9000d97c8d3ee refs/tags/v1.6.0. ./docs/FAQ.md:## Do models from before r1.6.0 work with current inference code? ./docs/FAQ.md:No. We have moved from Slim to Keras. All models before `1.6.0` were trained in. ./docs/FAQ.md:Slim platform. So they are not compatible with `1.6.0` anymore. ./docs/deeptrio-details-training-data.md:1.6.0 | <sup>[(6)](#vfootnote6)</sup>4 HG001, 3 HG002, 3 HG003, 3 HG004, 7 HG005, 6 HG006, 6 HG007, 4 NA12891, 4 NA12892 | 704,394,556. ./docs/deeptrio-details-training-data.md:1.6.0 | <sup>[(6)](#vfootnote6)</sup>2 HG001, 3 HG002, 3 HG003, 3 HG004, 7 HG005, 6 HG006, 6 HG007, 2 NA12891, 2 NA12892 | 457,420,038. ./docs/deeptrio-details-training-data.md:| 1.6.0 | <sup>[(6)](#vfootnote6)</sup>9 HG001, 7 HG002, 7 HG003, 7 HG004, 8 HG005, 8 HG006, 8 HG007, 9 NA12891, 9 NA12892 | 27,783,324 |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | <sup>[(6)](#vfootnote6)</sup>6 HG002, 6 HG003, 6 HG004, 8 HG005, 8 HG006, 8 HG007 | 13,039,595 |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | 9 HG002, 5 HG003, 5 HG004, 1 HG005, 1 HG006, 1 HG007 | 890,016,014<sup>[(5)](#vfootnote5)</sup> |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | 9 HG002, 5 HG003, 5 HG004, 1 HG005, 1 HG006, 1 HG007 | 838,515,085<sup>[(5)](#vfootnote5)</sup> |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | 1 HG002, 1 HG002, 1 HG004 | 50,249,704<sup>[(5)](#vfootnote5)</sup> |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | 1 HG002, 1 HG002, 1 HG004 | 99,675,190<sup>[(5)](#vfootnote5)</sup> |. ./deeptrio/testdata/golden.calling_exa",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:1973,safety,test,testdata,1973,"tnote6)</sup>2 HG001, 3 HG002, 3 HG003, 3 HG004, 7 HG005, 6 HG006, 6 HG007, 2 NA12891, 2 NA12892 | 457,420,038. ./docs/deeptrio-details-training-data.md:| 1.6.0 | <sup>[(6)](#vfootnote6)</sup>9 HG001, 7 HG002, 7 HG003, 7 HG004, 8 HG005, 8 HG006, 8 HG007, 9 NA12891, 9 NA12892 | 27,783,324 |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | <sup>[(6)](#vfootnote6)</sup>6 HG002, 6 HG003, 6 HG004, 8 HG005, 8 HG006, 8 HG007 | 13,039,595 |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | 9 HG002, 5 HG003, 5 HG004, 1 HG005, 1 HG006, 1 HG007 | 890,016,014<sup>[(5)](#vfootnote5)</sup> |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | 9 HG002, 5 HG003, 5 HG004, 1 HG005, 1 HG006, 1 HG007 | 838,515,085<sup>[(5)](#vfootnote5)</sup> |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | 1 HG002, 1 HG002, 1 HG004 | 50,249,704<sup>[(5)](#vfootnote5)</sup> |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | 1 HG002, 1 HG002, 1 HG004 | 99,675,190<sup>[(5)](#vfootnote5)</sup> |. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00002-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/HG002_ONT_deeptrio.denovo.examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ./deeptrio/testdata/customized_classes.golden.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.postprocess_gvcf_output.g.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/golden.training_examples.tfrecord.gz-00002-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.vcf_candidate_importer.calling_examples.tfrecord.gz.example_info.json:",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:2140,safety,test,testdata,2140,">[(6)](#vfootnote6)</sup>9 HG001, 7 HG002, 7 HG003, 7 HG004, 8 HG005, 8 HG006, 8 HG007, 9 NA12891, 9 NA12892 | 27,783,324 |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | <sup>[(6)](#vfootnote6)</sup>6 HG002, 6 HG003, 6 HG004, 8 HG005, 8 HG006, 8 HG007 | 13,039,595 |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | 9 HG002, 5 HG003, 5 HG004, 1 HG005, 1 HG006, 1 HG007 | 890,016,014<sup>[(5)](#vfootnote5)</sup> |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | 9 HG002, 5 HG003, 5 HG004, 1 HG005, 1 HG006, 1 HG007 | 838,515,085<sup>[(5)](#vfootnote5)</sup> |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | 1 HG002, 1 HG002, 1 HG004 | 50,249,704<sup>[(5)](#vfootnote5)</sup> |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | 1 HG002, 1 HG002, 1 HG004 | 99,675,190<sup>[(5)](#vfootnote5)</sup> |. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00002-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/HG002_ONT_deeptrio.denovo.examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ./deeptrio/testdata/customized_classes.golden.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.postprocess_gvcf_output.g.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/golden.training_examples.tfrecord.gz-00002-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.vcf_candidate_importer.calling_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00000-of-00003.example_info.json:",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:2313,safety,test,testdata,2313," | <sup>[(6)](#vfootnote6)</sup>6 HG002, 6 HG003, 6 HG004, 8 HG005, 8 HG006, 8 HG007 | 13,039,595 |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | 9 HG002, 5 HG003, 5 HG004, 1 HG005, 1 HG006, 1 HG007 | 890,016,014<sup>[(5)](#vfootnote5)</sup> |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | 9 HG002, 5 HG003, 5 HG004, 1 HG005, 1 HG006, 1 HG007 | 838,515,085<sup>[(5)](#vfootnote5)</sup> |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | 1 HG002, 1 HG002, 1 HG004 | 50,249,704<sup>[(5)](#vfootnote5)</sup> |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | 1 HG002, 1 HG002, 1 HG004 | 99,675,190<sup>[(5)](#vfootnote5)</sup> |. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00002-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/HG002_ONT_deeptrio.denovo.examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ./deeptrio/testdata/customized_classes.golden.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.postprocess_gvcf_output.g.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/golden.training_examples.tfrecord.gz-00002-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.vcf_candidate_importer.calling_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00000-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00001-of-00003.example_info.json:{""vers",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:2485,safety,test,testdata,2485,"HG004, 1 HG005, 1 HG006, 1 HG007 | 890,016,014<sup>[(5)](#vfootnote5)</sup> |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | 9 HG002, 5 HG003, 5 HG004, 1 HG005, 1 HG006, 1 HG007 | 838,515,085<sup>[(5)](#vfootnote5)</sup> |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | 1 HG002, 1 HG002, 1 HG004 | 50,249,704<sup>[(5)](#vfootnote5)</sup> |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | 1 HG002, 1 HG002, 1 HG004 | 99,675,190<sup>[(5)](#vfootnote5)</sup> |. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00002-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/HG002_ONT_deeptrio.denovo.examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ./deeptrio/testdata/customized_classes.golden.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.postprocess_gvcf_output.g.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/golden.training_examples.tfrecord.gz-00002-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.vcf_candidate_importer.calling_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00000-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00001-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.postprocess_single_site_output.vcf:##DeepVariant_version=1.6.0. ./deeptri",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:2637,safety,test,testdata,2637,"004, 1 HG005, 1 HG006, 1 HG007 | 838,515,085<sup>[(5)](#vfootnote5)</sup> |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | 1 HG002, 1 HG002, 1 HG004 | 50,249,704<sup>[(5)](#vfootnote5)</sup> |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | 1 HG002, 1 HG002, 1 HG004 | 99,675,190<sup>[(5)](#vfootnote5)</sup> |. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00002-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/HG002_ONT_deeptrio.denovo.examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ./deeptrio/testdata/customized_classes.golden.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.postprocess_gvcf_output.g.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/golden.training_examples.tfrecord.gz-00002-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.vcf_candidate_importer.calling_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00000-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00001-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.postprocess_single_site_output.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/golden.training_examples.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/HG002_ONT_deeptrio.examples.tfrecord.gz.example_info.json:{""ver",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:2723,safety,test,testdata,2723,"eptrio-details-training-data.md:| 1.6.0 | 1 HG002, 1 HG002, 1 HG004 | 50,249,704<sup>[(5)](#vfootnote5)</sup> |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | 1 HG002, 1 HG002, 1 HG004 | 99,675,190<sup>[(5)](#vfootnote5)</sup> |. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00002-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/HG002_ONT_deeptrio.denovo.examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ./deeptrio/testdata/customized_classes.golden.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.postprocess_gvcf_output.g.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/golden.training_examples.tfrecord.gz-00002-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.vcf_candidate_importer.calling_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00000-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00001-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.postprocess_single_site_output.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/golden.training_examples.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/HG002_ONT_deeptrio.examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ./d",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:2891,safety,test,testdata,2891,"002, 1 HG002, 1 HG004 | 99,675,190<sup>[(5)](#vfootnote5)</sup> |. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00002-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/HG002_ONT_deeptrio.denovo.examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ./deeptrio/testdata/customized_classes.golden.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.postprocess_gvcf_output.g.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/golden.training_examples.tfrecord.gz-00002-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.vcf_candidate_importer.calling_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00000-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00001-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.postprocess_single_site_output.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/golden.training_examples.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/HG002_ONT_deeptrio.examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ./deeptrio/testdata/golden.vcf_candidate_importer.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 221, 6], ""channels"": [1, 2, 3, 4, 5, ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:3066,safety,test,testdata,3066,", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/HG002_ONT_deeptrio.denovo.examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ./deeptrio/testdata/customized_classes.golden.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.postprocess_gvcf_output.g.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/golden.training_examples.tfrecord.gz-00002-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.vcf_candidate_importer.calling_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00000-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00001-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.postprocess_single_site_output.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/golden.training_examples.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/HG002_ONT_deeptrio.examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ./deeptrio/testdata/golden.vcf_candidate_importer.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/alt_aligned_pileup.golden.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 199, 8], ""channels"": [1, 2, 3, 4, 5, 6, ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:3233,safety,test,testdata,3233,"hape"": [300, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ./deeptrio/testdata/customized_classes.golden.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.postprocess_gvcf_output.g.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/golden.training_examples.tfrecord.gz-00002-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.vcf_candidate_importer.calling_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00000-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00001-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.postprocess_single_site_output.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/golden.training_examples.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/HG002_ONT_deeptrio.examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ./deeptrio/testdata/golden.vcf_candidate_importer.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/alt_aligned_pileup.golden.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 199, 8], ""channels"": [1, 2, 3, 4, 5, 6, 9, 10]}. ./deeptrio/testdata/golden.training_examples.tfrecord.gz-00000-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:3400,safety,test,testdata,3400,"n"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.postprocess_gvcf_output.g.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/golden.training_examples.tfrecord.gz-00002-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.vcf_candidate_importer.calling_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00000-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00001-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.postprocess_single_site_output.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/golden.training_examples.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/HG002_ONT_deeptrio.examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ./deeptrio/testdata/golden.vcf_candidate_importer.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/alt_aligned_pileup.golden.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 199, 8], ""channels"": [1, 2, 3, 4, 5, 6, 9, 10]}. ./deeptrio/testdata/golden.training_examples.tfrecord.gz-00000-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./de",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:3491,safety,test,testdata,3491,"olden.calling_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.postprocess_gvcf_output.g.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/golden.training_examples.tfrecord.gz-00002-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.vcf_candidate_importer.calling_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00000-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00001-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.postprocess_single_site_output.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/golden.training_examples.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/HG002_ONT_deeptrio.examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ./deeptrio/testdata/golden.vcf_candidate_importer.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/alt_aligned_pileup.golden.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 199, 8], ""channels"": [1, 2, 3, 4, 5, 6, 9, 10]}. ./deeptrio/testdata/golden.training_examples.tfrecord.gz-00000-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.training_examples.tfrecord.gz-00001-of-00003.example_info.json:{""ver",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:3569,safety,test,testdata,3569,"pe"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.postprocess_gvcf_output.g.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/golden.training_examples.tfrecord.gz-00002-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.vcf_candidate_importer.calling_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00000-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00001-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.postprocess_single_site_output.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/golden.training_examples.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/HG002_ONT_deeptrio.examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ./deeptrio/testdata/golden.vcf_candidate_importer.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/alt_aligned_pileup.golden.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 199, 8], ""channels"": [1, 2, 3, 4, 5, 6, 9, 10]}. ./deeptrio/testdata/golden.training_examples.tfrecord.gz-00000-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.training_examples.tfrecord.gz-00001-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./Doc",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:3735,safety,test,testdata,3735,"training_examples.tfrecord.gz-00002-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.vcf_candidate_importer.calling_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00000-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00001-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.postprocess_single_site_output.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/golden.training_examples.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/HG002_ONT_deeptrio.examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ./deeptrio/testdata/golden.vcf_candidate_importer.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/alt_aligned_pileup.golden.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 199, 8], ""channels"": [1, 2, 3, 4, 5, 6, 9, 10]}. ./deeptrio/testdata/golden.training_examples.tfrecord.gz-00000-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.training_examples.tfrecord.gz-00001-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./Dockerfile.deepsomatic:ARG VERSION_DEEPSOMATIC=1.6.0. ./deepvariant/testdata/golden.vcf_candidate_importer_calling_examples.tfrecord.example_info.json:{""version"": ""1.6.0",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:3911,safety,test,testdata,3911,"{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.vcf_candidate_importer.calling_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00000-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00001-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.postprocess_single_site_output.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/golden.training_examples.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/HG002_ONT_deeptrio.examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ./deeptrio/testdata/golden.vcf_candidate_importer.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/alt_aligned_pileup.golden.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 199, 8], ""channels"": [1, 2, 3, 4, 5, 6, 9, 10]}. ./deeptrio/testdata/golden.training_examples.tfrecord.gz-00000-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.training_examples.tfrecord.gz-00001-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./Dockerfile.deepsomatic:ARG VERSION_DEEPSOMATIC=1.6.0. ./deepvariant/testdata/golden.vcf_candidate_importer_calling_examples.tfrecord.example_info.json:{""version"": ""1.6.0"", ""shape"": [100, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:4090,safety,test,testdata,4090,"{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.vcf_candidate_importer.calling_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00000-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00001-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.postprocess_single_site_output.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/golden.training_examples.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/HG002_ONT_deeptrio.examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ./deeptrio/testdata/golden.vcf_candidate_importer.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/alt_aligned_pileup.golden.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 199, 8], ""channels"": [1, 2, 3, 4, 5, 6, 9, 10]}. ./deeptrio/testdata/golden.training_examples.tfrecord.gz-00000-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.training_examples.tfrecord.gz-00001-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./Dockerfile.deepsomatic:ARG VERSION_DEEPSOMATIC=1.6.0. ./deepvariant/testdata/golden.vcf_candidate_importer_calling_examples.tfrecord.example_info.json:{""version"": ""1.6.0"", ""shape"": [100, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:4258,safety,test,testdata,4258,"{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.vcf_candidate_importer.calling_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00000-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00001-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.postprocess_single_site_output.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/golden.training_examples.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/HG002_ONT_deeptrio.examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ./deeptrio/testdata/golden.vcf_candidate_importer.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/alt_aligned_pileup.golden.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 199, 8], ""channels"": [1, 2, 3, 4, 5, 6, 9, 10]}. ./deeptrio/testdata/golden.training_examples.tfrecord.gz-00000-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.training_examples.tfrecord.gz-00001-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./Dockerfile.deepsomatic:ARG VERSION_DEEPSOMATIC=1.6.0. ./deepvariant/testdata/golden.vcf_candidate_importer_calling_examples.tfrecord.example_info.json:{""version"": ""1.6.0"", ""shape"": [100, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:4411,safety,test,testdata,4411,"{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.vcf_candidate_importer.calling_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00000-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00001-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.postprocess_single_site_output.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/golden.training_examples.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/HG002_ONT_deeptrio.examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ./deeptrio/testdata/golden.vcf_candidate_importer.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/alt_aligned_pileup.golden.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 199, 8], ""channels"": [1, 2, 3, 4, 5, 6, 9, 10]}. ./deeptrio/testdata/golden.training_examples.tfrecord.gz-00000-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.training_examples.tfrecord.gz-00001-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./Dockerfile.deepsomatic:ARG VERSION_DEEPSOMATIC=1.6.0. ./deepvariant/testdata/golden.vcf_candidate_importer_calling_examples.tfrecord.example_info.json:{""version"": ""1.6.0"", ""shape"": [100, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:4638,safety,test,testdata,4638,"{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.vcf_candidate_importer.calling_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00000-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00001-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.postprocess_single_site_output.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/golden.training_examples.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/HG002_ONT_deeptrio.examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ./deeptrio/testdata/golden.vcf_candidate_importer.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/alt_aligned_pileup.golden.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 199, 8], ""channels"": [1, 2, 3, 4, 5, 6, 9, 10]}. ./deeptrio/testdata/golden.training_examples.tfrecord.gz-00000-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.training_examples.tfrecord.gz-00001-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./Dockerfile.deepsomatic:ARG VERSION_DEEPSOMATIC=1.6.0. ./deepvariant/testdata/golden.vcf_candidate_importer_calling_examples.tfrecord.example_info.json:{""version"": ""1.6.0"", ""shape"": [100, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:502,security,model,models,502,"Hi @fellen31 ,. That's right. . I checked the files. Only the `ARG` in Dockerfiles are relevant. ```. $ find . -type f -exec grep -H '1\.6\.0' {} \; . ./scripts/inference_deeptrio.sh: # --use_slim_model is introduced only after 1.6.0. ./scripts/inference_deepvariant.sh: # --use_slim_model is introduced only after 1.6.0. ./Dockerfile.deeptrio:ARG VERSION_DEEPTRIO=1.6.0. ./Dockerfile:ARG VERSION=1.6.0. ./.git/packed-refs:6b167016a0e6b92ca5b492e2dac9000d97c8d3ee refs/tags/v1.6.0. ./docs/FAQ.md:## Do models from before r1.6.0 work with current inference code? ./docs/FAQ.md:No. We have moved from Slim to Keras. All models before `1.6.0` were trained in. ./docs/FAQ.md:Slim platform. So they are not compatible with `1.6.0` anymore. ./docs/deeptrio-details-training-data.md:1.6.0 | <sup>[(6)](#vfootnote6)</sup>4 HG001, 3 HG002, 3 HG003, 3 HG004, 7 HG005, 6 HG006, 6 HG007, 4 NA12891, 4 NA12892 | 704,394,556. ./docs/deeptrio-details-training-data.md:1.6.0 | <sup>[(6)](#vfootnote6)</sup>2 HG001, 3 HG002, 3 HG003, 3 HG004, 7 HG005, 6 HG006, 6 HG007, 2 NA12891, 2 NA12892 | 457,420,038. ./docs/deeptrio-details-training-data.md:| 1.6.0 | <sup>[(6)](#vfootnote6)</sup>9 HG001, 7 HG002, 7 HG003, 7 HG004, 8 HG005, 8 HG006, 8 HG007, 9 NA12891, 9 NA12892 | 27,783,324 |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | <sup>[(6)](#vfootnote6)</sup>6 HG002, 6 HG003, 6 HG004, 8 HG005, 8 HG006, 8 HG007 | 13,039,595 |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | 9 HG002, 5 HG003, 5 HG004, 1 HG005, 1 HG006, 1 HG007 | 890,016,014<sup>[(5)](#vfootnote5)</sup> |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | 9 HG002, 5 HG003, 5 HG004, 1 HG005, 1 HG006, 1 HG007 | 838,515,085<sup>[(5)](#vfootnote5)</sup> |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | 1 HG002, 1 HG002, 1 HG004 | 50,249,704<sup>[(5)](#vfootnote5)</sup> |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | 1 HG002, 1 HG002, 1 HG004 | 99,675,190<sup>[(5)](#vfootnote5)</sup> |. ./deeptrio/testdata/golden.calling_exa",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:618,security,model,models,618,"Hi @fellen31 ,. That's right. . I checked the files. Only the `ARG` in Dockerfiles are relevant. ```. $ find . -type f -exec grep -H '1\.6\.0' {} \; . ./scripts/inference_deeptrio.sh: # --use_slim_model is introduced only after 1.6.0. ./scripts/inference_deepvariant.sh: # --use_slim_model is introduced only after 1.6.0. ./Dockerfile.deeptrio:ARG VERSION_DEEPTRIO=1.6.0. ./Dockerfile:ARG VERSION=1.6.0. ./.git/packed-refs:6b167016a0e6b92ca5b492e2dac9000d97c8d3ee refs/tags/v1.6.0. ./docs/FAQ.md:## Do models from before r1.6.0 work with current inference code? ./docs/FAQ.md:No. We have moved from Slim to Keras. All models before `1.6.0` were trained in. ./docs/FAQ.md:Slim platform. So they are not compatible with `1.6.0` anymore. ./docs/deeptrio-details-training-data.md:1.6.0 | <sup>[(6)](#vfootnote6)</sup>4 HG001, 3 HG002, 3 HG003, 3 HG004, 7 HG005, 6 HG006, 6 HG007, 4 NA12891, 4 NA12892 | 704,394,556. ./docs/deeptrio-details-training-data.md:1.6.0 | <sup>[(6)](#vfootnote6)</sup>2 HG001, 3 HG002, 3 HG003, 3 HG004, 7 HG005, 6 HG006, 6 HG007, 2 NA12891, 2 NA12892 | 457,420,038. ./docs/deeptrio-details-training-data.md:| 1.6.0 | <sup>[(6)](#vfootnote6)</sup>9 HG001, 7 HG002, 7 HG003, 7 HG004, 8 HG005, 8 HG006, 8 HG007, 9 NA12891, 9 NA12892 | 27,783,324 |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | <sup>[(6)](#vfootnote6)</sup>6 HG002, 6 HG003, 6 HG004, 8 HG005, 8 HG006, 8 HG007 | 13,039,595 |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | 9 HG002, 5 HG003, 5 HG004, 1 HG005, 1 HG006, 1 HG007 | 890,016,014<sup>[(5)](#vfootnote5)</sup> |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | 9 HG002, 5 HG003, 5 HG004, 1 HG005, 1 HG006, 1 HG007 | 838,515,085<sup>[(5)](#vfootnote5)</sup> |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | 1 HG002, 1 HG002, 1 HG004 | 50,249,704<sup>[(5)](#vfootnote5)</sup> |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | 1 HG002, 1 HG002, 1 HG004 | 99,675,190<sup>[(5)](#vfootnote5)</sup> |. ./deeptrio/testdata/golden.calling_exa",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:1973,testability,test,testdata,1973,"tnote6)</sup>2 HG001, 3 HG002, 3 HG003, 3 HG004, 7 HG005, 6 HG006, 6 HG007, 2 NA12891, 2 NA12892 | 457,420,038. ./docs/deeptrio-details-training-data.md:| 1.6.0 | <sup>[(6)](#vfootnote6)</sup>9 HG001, 7 HG002, 7 HG003, 7 HG004, 8 HG005, 8 HG006, 8 HG007, 9 NA12891, 9 NA12892 | 27,783,324 |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | <sup>[(6)](#vfootnote6)</sup>6 HG002, 6 HG003, 6 HG004, 8 HG005, 8 HG006, 8 HG007 | 13,039,595 |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | 9 HG002, 5 HG003, 5 HG004, 1 HG005, 1 HG006, 1 HG007 | 890,016,014<sup>[(5)](#vfootnote5)</sup> |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | 9 HG002, 5 HG003, 5 HG004, 1 HG005, 1 HG006, 1 HG007 | 838,515,085<sup>[(5)](#vfootnote5)</sup> |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | 1 HG002, 1 HG002, 1 HG004 | 50,249,704<sup>[(5)](#vfootnote5)</sup> |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | 1 HG002, 1 HG002, 1 HG004 | 99,675,190<sup>[(5)](#vfootnote5)</sup> |. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00002-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/HG002_ONT_deeptrio.denovo.examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ./deeptrio/testdata/customized_classes.golden.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.postprocess_gvcf_output.g.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/golden.training_examples.tfrecord.gz-00002-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.vcf_candidate_importer.calling_examples.tfrecord.gz.example_info.json:",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:2140,testability,test,testdata,2140,">[(6)](#vfootnote6)</sup>9 HG001, 7 HG002, 7 HG003, 7 HG004, 8 HG005, 8 HG006, 8 HG007, 9 NA12891, 9 NA12892 | 27,783,324 |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | <sup>[(6)](#vfootnote6)</sup>6 HG002, 6 HG003, 6 HG004, 8 HG005, 8 HG006, 8 HG007 | 13,039,595 |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | 9 HG002, 5 HG003, 5 HG004, 1 HG005, 1 HG006, 1 HG007 | 890,016,014<sup>[(5)](#vfootnote5)</sup> |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | 9 HG002, 5 HG003, 5 HG004, 1 HG005, 1 HG006, 1 HG007 | 838,515,085<sup>[(5)](#vfootnote5)</sup> |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | 1 HG002, 1 HG002, 1 HG004 | 50,249,704<sup>[(5)](#vfootnote5)</sup> |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | 1 HG002, 1 HG002, 1 HG004 | 99,675,190<sup>[(5)](#vfootnote5)</sup> |. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00002-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/HG002_ONT_deeptrio.denovo.examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ./deeptrio/testdata/customized_classes.golden.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.postprocess_gvcf_output.g.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/golden.training_examples.tfrecord.gz-00002-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.vcf_candidate_importer.calling_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00000-of-00003.example_info.json:",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:2313,testability,test,testdata,2313," | <sup>[(6)](#vfootnote6)</sup>6 HG002, 6 HG003, 6 HG004, 8 HG005, 8 HG006, 8 HG007 | 13,039,595 |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | 9 HG002, 5 HG003, 5 HG004, 1 HG005, 1 HG006, 1 HG007 | 890,016,014<sup>[(5)](#vfootnote5)</sup> |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | 9 HG002, 5 HG003, 5 HG004, 1 HG005, 1 HG006, 1 HG007 | 838,515,085<sup>[(5)](#vfootnote5)</sup> |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | 1 HG002, 1 HG002, 1 HG004 | 50,249,704<sup>[(5)](#vfootnote5)</sup> |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | 1 HG002, 1 HG002, 1 HG004 | 99,675,190<sup>[(5)](#vfootnote5)</sup> |. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00002-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/HG002_ONT_deeptrio.denovo.examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ./deeptrio/testdata/customized_classes.golden.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.postprocess_gvcf_output.g.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/golden.training_examples.tfrecord.gz-00002-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.vcf_candidate_importer.calling_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00000-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00001-of-00003.example_info.json:{""vers",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:2485,testability,test,testdata,2485,"HG004, 1 HG005, 1 HG006, 1 HG007 | 890,016,014<sup>[(5)](#vfootnote5)</sup> |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | 9 HG002, 5 HG003, 5 HG004, 1 HG005, 1 HG006, 1 HG007 | 838,515,085<sup>[(5)](#vfootnote5)</sup> |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | 1 HG002, 1 HG002, 1 HG004 | 50,249,704<sup>[(5)](#vfootnote5)</sup> |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | 1 HG002, 1 HG002, 1 HG004 | 99,675,190<sup>[(5)](#vfootnote5)</sup> |. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00002-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/HG002_ONT_deeptrio.denovo.examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ./deeptrio/testdata/customized_classes.golden.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.postprocess_gvcf_output.g.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/golden.training_examples.tfrecord.gz-00002-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.vcf_candidate_importer.calling_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00000-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00001-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.postprocess_single_site_output.vcf:##DeepVariant_version=1.6.0. ./deeptri",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:2637,testability,test,testdata,2637,"004, 1 HG005, 1 HG006, 1 HG007 | 838,515,085<sup>[(5)](#vfootnote5)</sup> |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | 1 HG002, 1 HG002, 1 HG004 | 50,249,704<sup>[(5)](#vfootnote5)</sup> |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | 1 HG002, 1 HG002, 1 HG004 | 99,675,190<sup>[(5)](#vfootnote5)</sup> |. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00002-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/HG002_ONT_deeptrio.denovo.examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ./deeptrio/testdata/customized_classes.golden.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.postprocess_gvcf_output.g.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/golden.training_examples.tfrecord.gz-00002-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.vcf_candidate_importer.calling_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00000-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00001-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.postprocess_single_site_output.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/golden.training_examples.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/HG002_ONT_deeptrio.examples.tfrecord.gz.example_info.json:{""ver",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:2723,testability,test,testdata,2723,"eptrio-details-training-data.md:| 1.6.0 | 1 HG002, 1 HG002, 1 HG004 | 50,249,704<sup>[(5)](#vfootnote5)</sup> |. ./docs/deeptrio-details-training-data.md:| 1.6.0 | 1 HG002, 1 HG002, 1 HG004 | 99,675,190<sup>[(5)](#vfootnote5)</sup> |. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00002-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/HG002_ONT_deeptrio.denovo.examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ./deeptrio/testdata/customized_classes.golden.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.postprocess_gvcf_output.g.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/golden.training_examples.tfrecord.gz-00002-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.vcf_candidate_importer.calling_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00000-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00001-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.postprocess_single_site_output.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/golden.training_examples.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/HG002_ONT_deeptrio.examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ./d",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:2891,testability,test,testdata,2891,"002, 1 HG002, 1 HG004 | 99,675,190<sup>[(5)](#vfootnote5)</sup> |. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00002-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/HG002_ONT_deeptrio.denovo.examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ./deeptrio/testdata/customized_classes.golden.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.postprocess_gvcf_output.g.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/golden.training_examples.tfrecord.gz-00002-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.vcf_candidate_importer.calling_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00000-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00001-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.postprocess_single_site_output.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/golden.training_examples.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/HG002_ONT_deeptrio.examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ./deeptrio/testdata/golden.vcf_candidate_importer.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 221, 6], ""channels"": [1, 2, 3, 4, 5, ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:3066,testability,test,testdata,3066,", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/HG002_ONT_deeptrio.denovo.examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ./deeptrio/testdata/customized_classes.golden.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.postprocess_gvcf_output.g.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/golden.training_examples.tfrecord.gz-00002-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.vcf_candidate_importer.calling_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00000-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00001-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.postprocess_single_site_output.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/golden.training_examples.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/HG002_ONT_deeptrio.examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ./deeptrio/testdata/golden.vcf_candidate_importer.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/alt_aligned_pileup.golden.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 199, 8], ""channels"": [1, 2, 3, 4, 5, 6, ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:3233,testability,test,testdata,3233,"hape"": [300, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ./deeptrio/testdata/customized_classes.golden.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.postprocess_gvcf_output.g.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/golden.training_examples.tfrecord.gz-00002-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.vcf_candidate_importer.calling_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00000-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00001-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.postprocess_single_site_output.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/golden.training_examples.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/HG002_ONT_deeptrio.examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ./deeptrio/testdata/golden.vcf_candidate_importer.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/alt_aligned_pileup.golden.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 199, 8], ""channels"": [1, 2, 3, 4, 5, 6, 9, 10]}. ./deeptrio/testdata/golden.training_examples.tfrecord.gz-00000-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:3400,testability,test,testdata,3400,"n"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.postprocess_gvcf_output.g.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/golden.training_examples.tfrecord.gz-00002-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.vcf_candidate_importer.calling_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00000-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00001-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.postprocess_single_site_output.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/golden.training_examples.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/HG002_ONT_deeptrio.examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ./deeptrio/testdata/golden.vcf_candidate_importer.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/alt_aligned_pileup.golden.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 199, 8], ""channels"": [1, 2, 3, 4, 5, 6, 9, 10]}. ./deeptrio/testdata/golden.training_examples.tfrecord.gz-00000-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./de",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:3491,testability,test,testdata,3491,"olden.calling_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.postprocess_gvcf_output.g.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/golden.training_examples.tfrecord.gz-00002-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.vcf_candidate_importer.calling_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00000-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00001-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.postprocess_single_site_output.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/golden.training_examples.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/HG002_ONT_deeptrio.examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ./deeptrio/testdata/golden.vcf_candidate_importer.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/alt_aligned_pileup.golden.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 199, 8], ""channels"": [1, 2, 3, 4, 5, 6, 9, 10]}. ./deeptrio/testdata/golden.training_examples.tfrecord.gz-00000-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.training_examples.tfrecord.gz-00001-of-00003.example_info.json:{""ver",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:3569,testability,test,testdata,3569,"pe"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.postprocess_gvcf_output.g.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/golden.training_examples.tfrecord.gz-00002-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.vcf_candidate_importer.calling_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00000-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00001-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.postprocess_single_site_output.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/golden.training_examples.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/HG002_ONT_deeptrio.examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ./deeptrio/testdata/golden.vcf_candidate_importer.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/alt_aligned_pileup.golden.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 199, 8], ""channels"": [1, 2, 3, 4, 5, 6, 9, 10]}. ./deeptrio/testdata/golden.training_examples.tfrecord.gz-00000-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.training_examples.tfrecord.gz-00001-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./Doc",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:3735,testability,test,testdata,3735,"training_examples.tfrecord.gz-00002-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.vcf_candidate_importer.calling_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00000-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00001-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.postprocess_single_site_output.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/golden.training_examples.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/HG002_ONT_deeptrio.examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ./deeptrio/testdata/golden.vcf_candidate_importer.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/alt_aligned_pileup.golden.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 199, 8], ""channels"": [1, 2, 3, 4, 5, 6, 9, 10]}. ./deeptrio/testdata/golden.training_examples.tfrecord.gz-00000-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.training_examples.tfrecord.gz-00001-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./Dockerfile.deepsomatic:ARG VERSION_DEEPSOMATIC=1.6.0. ./deepvariant/testdata/golden.vcf_candidate_importer_calling_examples.tfrecord.example_info.json:{""version"": ""1.6.0",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:3911,testability,test,testdata,3911,"{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.vcf_candidate_importer.calling_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00000-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00001-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.postprocess_single_site_output.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/golden.training_examples.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/HG002_ONT_deeptrio.examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ./deeptrio/testdata/golden.vcf_candidate_importer.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/alt_aligned_pileup.golden.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 199, 8], ""channels"": [1, 2, 3, 4, 5, 6, 9, 10]}. ./deeptrio/testdata/golden.training_examples.tfrecord.gz-00000-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.training_examples.tfrecord.gz-00001-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./Dockerfile.deepsomatic:ARG VERSION_DEEPSOMATIC=1.6.0. ./deepvariant/testdata/golden.vcf_candidate_importer_calling_examples.tfrecord.example_info.json:{""version"": ""1.6.0"", ""shape"": [100, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:4090,testability,test,testdata,4090,"{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.vcf_candidate_importer.calling_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00000-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00001-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.postprocess_single_site_output.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/golden.training_examples.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/HG002_ONT_deeptrio.examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ./deeptrio/testdata/golden.vcf_candidate_importer.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/alt_aligned_pileup.golden.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 199, 8], ""channels"": [1, 2, 3, 4, 5, 6, 9, 10]}. ./deeptrio/testdata/golden.training_examples.tfrecord.gz-00000-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.training_examples.tfrecord.gz-00001-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./Dockerfile.deepsomatic:ARG VERSION_DEEPSOMATIC=1.6.0. ./deepvariant/testdata/golden.vcf_candidate_importer_calling_examples.tfrecord.example_info.json:{""version"": ""1.6.0"", ""shape"": [100, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:4258,testability,test,testdata,4258,"{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.vcf_candidate_importer.calling_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00000-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00001-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.postprocess_single_site_output.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/golden.training_examples.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/HG002_ONT_deeptrio.examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ./deeptrio/testdata/golden.vcf_candidate_importer.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/alt_aligned_pileup.golden.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 199, 8], ""channels"": [1, 2, 3, 4, 5, 6, 9, 10]}. ./deeptrio/testdata/golden.training_examples.tfrecord.gz-00000-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.training_examples.tfrecord.gz-00001-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./Dockerfile.deepsomatic:ARG VERSION_DEEPSOMATIC=1.6.0. ./deepvariant/testdata/golden.vcf_candidate_importer_calling_examples.tfrecord.example_info.json:{""version"": ""1.6.0"", ""shape"": [100, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:4411,testability,test,testdata,4411,"{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.vcf_candidate_importer.calling_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00000-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00001-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.postprocess_single_site_output.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/golden.training_examples.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/HG002_ONT_deeptrio.examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ./deeptrio/testdata/golden.vcf_candidate_importer.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/alt_aligned_pileup.golden.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 199, 8], ""channels"": [1, 2, 3, 4, 5, 6, 9, 10]}. ./deeptrio/testdata/golden.training_examples.tfrecord.gz-00000-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.training_examples.tfrecord.gz-00001-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./Dockerfile.deepsomatic:ARG VERSION_DEEPSOMATIC=1.6.0. ./deepvariant/testdata/golden.vcf_candidate_importer_calling_examples.tfrecord.example_info.json:{""version"": ""1.6.0"", ""shape"": [100, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/830:4638,testability,test,testdata,4638,"{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.vcf_candidate_importer.calling_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00000-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00001-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.postprocess_single_site_output.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/golden.training_examples.vcf:##DeepVariant_version=1.6.0. ./deeptrio/testdata/HG002_ONT_deeptrio.examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ./deeptrio/testdata/golden.vcf_candidate_importer.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/alt_aligned_pileup.golden.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 199, 8], ""channels"": [1, 2, 3, 4, 5, 6, 9, 10]}. ./deeptrio/testdata/golden.training_examples.tfrecord.gz-00000-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./deeptrio/testdata/golden.training_examples.tfrecord.gz-00001-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ./Dockerfile.deepsomatic:ARG VERSION_DEEPSOMATIC=1.6.0. ./deepvariant/testdata/golden.vcf_candidate_importer_calling_examples.tfrecord.example_info.json:{""version"": ""1.6.0"", ""shape"": [100, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/830
https://github.com/google/deepvariant/issues/831:64,integrability,pub,pub,64,It seems like your command has `/slurm/home/yrd/sunlab/yangfeng/pub/WW/WGS/deepvirant/deepvariant_1.6.1.sif` which I'm guessing is a Singularity Image File that you created from pulling from DeepVariant before already. Two questions for you:. 1. Why are using using `-B` with it? 2. Is there a reason why you're specifying both the .sif file and the docker: path ?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/831
https://github.com/google/deepvariant/issues/831:312,interoperability,specif,specifying,312,It seems like your command has `/slurm/home/yrd/sunlab/yangfeng/pub/WW/WGS/deepvirant/deepvariant_1.6.1.sif` which I'm guessing is a Singularity Image File that you created from pulling from DeepVariant before already. Two questions for you:. 1. Why are using using `-B` with it? 2. Is there a reason why you're specifying both the .sif file and the docker: path ?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/831
https://github.com/google/deepvariant/issues/831:19,usability,command,command,19,It seems like your command has `/slurm/home/yrd/sunlab/yangfeng/pub/WW/WGS/deepvirant/deepvariant_1.6.1.sif` which I'm guessing is a Singularity Image File that you created from pulling from DeepVariant before already. Two questions for you:. 1. Why are using using `-B` with it? 2. Is there a reason why you're specifying both the .sif file and the docker: path ?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/831
https://github.com/google/deepvariant/issues/832:65,energy efficiency,GPU,GPU,65,"Hi @gambalab,. Unfortunately, DeepVariant does not support multi-GPU training (or inference) at this time.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/832
https://github.com/google/deepvariant/issues/832:65,performance,GPU,GPU,65,"Hi @gambalab,. Unfortunately, DeepVariant does not support multi-GPU training (or inference) at this time.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/832
https://github.com/google/deepvariant/issues/832:101,performance,time,time,101,"Hi @gambalab,. Unfortunately, DeepVariant does not support multi-GPU training (or inference) at this time.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/832
https://github.com/google/deepvariant/issues/832:42,reliability,doe,does,42,"Hi @gambalab,. Unfortunately, DeepVariant does not support multi-GPU training (or inference) at this time.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/832
https://github.com/google/deepvariant/issues/832:51,usability,support,support,51,"Hi @gambalab,. Unfortunately, DeepVariant does not support multi-GPU training (or inference) at this time.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/832
https://github.com/google/deepvariant/issues/833:256,deployability,pipelin,pipeline,256,"@snakesch can you please send the file to shafin@google.com. It's unclear from the output what exactly is happening, are you certain make_examples finished properly? Anyways, if you send me the file, I can run on my end to see if there's an issue with the pipeline or the file.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/833
https://github.com/google/deepvariant/issues/833:256,integrability,pipelin,pipeline,256,"@snakesch can you please send the file to shafin@google.com. It's unclear from the output what exactly is happening, are you certain make_examples finished properly? Anyways, if you send me the file, I can run on my end to see if there's an issue with the pipeline or the file.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/833
https://github.com/google/deepvariant/issues/833:15,availability,ping,pinging,15,"Hi @snakesch , pinging one more time to see if its possible for you to share the bam.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/833
https://github.com/google/deepvariant/issues/833:71,interoperability,share,share,71,"Hi @snakesch , pinging one more time to see if its possible for you to share the bam.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/833
https://github.com/google/deepvariant/issues/833:32,performance,time,time,32,"Hi @snakesch , pinging one more time to see if its possible for you to share the bam.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/833
https://github.com/google/deepvariant/issues/833:89,energy efficiency,CPU,CPU,89,"Hi Kishwar,. Apologies for the delayed response. I later discovered the issue was due to CPU memory issue, not with DeepVariant. I will close the issue for now, sorry for the oversight. Best,. Louis",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/833
https://github.com/google/deepvariant/issues/833:57,integrability,discover,discovered,57,"Hi Kishwar,. Apologies for the delayed response. I later discovered the issue was due to CPU memory issue, not with DeepVariant. I will close the issue for now, sorry for the oversight. Best,. Louis",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/833
https://github.com/google/deepvariant/issues/833:57,interoperability,discover,discovered,57,"Hi Kishwar,. Apologies for the delayed response. I later discovered the issue was due to CPU memory issue, not with DeepVariant. I will close the issue for now, sorry for the oversight. Best,. Louis",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/833
https://github.com/google/deepvariant/issues/833:89,performance,CPU,CPU,89,"Hi Kishwar,. Apologies for the delayed response. I later discovered the issue was due to CPU memory issue, not with DeepVariant. I will close the issue for now, sorry for the oversight. Best,. Louis",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/833
https://github.com/google/deepvariant/issues/833:93,performance,memor,memory,93,"Hi Kishwar,. Apologies for the delayed response. I later discovered the issue was due to CPU memory issue, not with DeepVariant. I will close the issue for now, sorry for the oversight. Best,. Louis",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/833
https://github.com/google/deepvariant/issues/833:57,usability,discov,discovered,57,"Hi Kishwar,. Apologies for the delayed response. I later discovered the issue was due to CPU memory issue, not with DeepVariant. I will close the issue for now, sorry for the oversight. Best,. Louis",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/833
https://github.com/google/deepvariant/issues/833:93,usability,memor,memory,93,"Hi Kishwar,. Apologies for the delayed response. I later discovered the issue was due to CPU memory issue, not with DeepVariant. I will close the issue for now, sorry for the oversight. Best,. Louis",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/833
https://github.com/google/deepvariant/issues/833:136,usability,close,close,136,"Hi Kishwar,. Apologies for the delayed response. I later discovered the issue was due to CPU memory issue, not with DeepVariant. I will close the issue for now, sorry for the oversight. Best,. Louis",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/833
https://github.com/google/deepvariant/issues/834:66,deployability,build,build,66,"Hi @0820LL . There is no docker for ARM64, however you may try to build binaries yourself. Please check [this](https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-build-test.md) page for instructions how to build from source. It will not work out of the box for ARM platform. The main obstacle would be the Bazel build (there might be other obstacles as well). I found [this](https://groups.google.com/g/bazel-discuss/c/BQCVxaNd5f8?pli=1) link that can be a starting point to make the Bazel work for ARM.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/834
https://github.com/google/deepvariant/issues/834:178,deployability,build,build-test,178,"Hi @0820LL . There is no docker for ARM64, however you may try to build binaries yourself. Please check [this](https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-build-test.md) page for instructions how to build from source. It will not work out of the box for ARM platform. The main obstacle would be the Bazel build (there might be other obstacles as well). I found [this](https://groups.google.com/g/bazel-discuss/c/BQCVxaNd5f8?pli=1) link that can be a starting point to make the Bazel work for ARM.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/834
https://github.com/google/deepvariant/issues/834:222,deployability,build,build,222,"Hi @0820LL . There is no docker for ARM64, however you may try to build binaries yourself. Please check [this](https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-build-test.md) page for instructions how to build from source. It will not work out of the box for ARM platform. The main obstacle would be the Bazel build (there might be other obstacles as well). I found [this](https://groups.google.com/g/bazel-discuss/c/BQCVxaNd5f8?pli=1) link that can be a starting point to make the Bazel work for ARM.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/834
https://github.com/google/deepvariant/issues/834:328,deployability,build,build,328,"Hi @0820LL . There is no docker for ARM64, however you may try to build binaries yourself. Please check [this](https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-build-test.md) page for instructions how to build from source. It will not work out of the box for ARM platform. The main obstacle would be the Bazel build (there might be other obstacles as well). I found [this](https://groups.google.com/g/bazel-discuss/c/BQCVxaNd5f8?pli=1) link that can be a starting point to make the Bazel work for ARM.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/834
https://github.com/google/deepvariant/issues/834:281,interoperability,platform,platform,281,"Hi @0820LL . There is no docker for ARM64, however you may try to build binaries yourself. Please check [this](https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-build-test.md) page for instructions how to build from source. It will not work out of the box for ARM platform. The main obstacle would be the Bazel build (there might be other obstacles as well). I found [this](https://groups.google.com/g/bazel-discuss/c/BQCVxaNd5f8?pli=1) link that can be a starting point to make the Bazel work for ARM.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/834
https://github.com/google/deepvariant/issues/834:184,safety,test,test,184,"Hi @0820LL . There is no docker for ARM64, however you may try to build binaries yourself. Please check [this](https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-build-test.md) page for instructions how to build from source. It will not work out of the box for ARM platform. The main obstacle would be the Bazel build (there might be other obstacles as well). I found [this](https://groups.google.com/g/bazel-discuss/c/BQCVxaNd5f8?pli=1) link that can be a starting point to make the Bazel work for ARM.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/834
https://github.com/google/deepvariant/issues/834:184,testability,test,test,184,"Hi @0820LL . There is no docker for ARM64, however you may try to build binaries yourself. Please check [this](https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-build-test.md) page for instructions how to build from source. It will not work out of the box for ARM platform. The main obstacle would be the Bazel build (there might be other obstacles as well). I found [this](https://groups.google.com/g/bazel-discuss/c/BQCVxaNd5f8?pli=1) link that can be a starting point to make the Bazel work for ARM.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/834
https://github.com/google/deepvariant/issues/834:7,usability,close,close,7,I will close this issue due to no activity since the last reply. Please feel free to reopen if necessary.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/834
https://github.com/google/deepvariant/issues/835:252,availability,error,error,252,"Hi,. Just to make sure I understand the issue: you are able to successfully install DeepVariant 0.7.0 with Python 2.7, but you are unable to install 1.15 or later; the goal is for you to install the latest version of DeepVariant, 1.6.0. . The relevant error that I see is:. ```. deepvariant [1.0.0|1.1.0|...|1.5.0] would require. │ └─ tensorflow-estimator 2.0.* , which conflicts with any installable versions previously reported;. ```. Is it possible to resolve this conflict? I will try myself to see if I can get it to work on GCP, and report back.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/835
https://github.com/google/deepvariant/issues/835:76,deployability,instal,install,76,"Hi,. Just to make sure I understand the issue: you are able to successfully install DeepVariant 0.7.0 with Python 2.7, but you are unable to install 1.15 or later; the goal is for you to install the latest version of DeepVariant, 1.6.0. . The relevant error that I see is:. ```. deepvariant [1.0.0|1.1.0|...|1.5.0] would require. │ └─ tensorflow-estimator 2.0.* , which conflicts with any installable versions previously reported;. ```. Is it possible to resolve this conflict? I will try myself to see if I can get it to work on GCP, and report back.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/835
https://github.com/google/deepvariant/issues/835:141,deployability,instal,install,141,"Hi,. Just to make sure I understand the issue: you are able to successfully install DeepVariant 0.7.0 with Python 2.7, but you are unable to install 1.15 or later; the goal is for you to install the latest version of DeepVariant, 1.6.0. . The relevant error that I see is:. ```. deepvariant [1.0.0|1.1.0|...|1.5.0] would require. │ └─ tensorflow-estimator 2.0.* , which conflicts with any installable versions previously reported;. ```. Is it possible to resolve this conflict? I will try myself to see if I can get it to work on GCP, and report back.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/835
https://github.com/google/deepvariant/issues/835:187,deployability,instal,install,187,"Hi,. Just to make sure I understand the issue: you are able to successfully install DeepVariant 0.7.0 with Python 2.7, but you are unable to install 1.15 or later; the goal is for you to install the latest version of DeepVariant, 1.6.0. . The relevant error that I see is:. ```. deepvariant [1.0.0|1.1.0|...|1.5.0] would require. │ └─ tensorflow-estimator 2.0.* , which conflicts with any installable versions previously reported;. ```. Is it possible to resolve this conflict? I will try myself to see if I can get it to work on GCP, and report back.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/835
https://github.com/google/deepvariant/issues/835:206,deployability,version,version,206,"Hi,. Just to make sure I understand the issue: you are able to successfully install DeepVariant 0.7.0 with Python 2.7, but you are unable to install 1.15 or later; the goal is for you to install the latest version of DeepVariant, 1.6.0. . The relevant error that I see is:. ```. deepvariant [1.0.0|1.1.0|...|1.5.0] would require. │ └─ tensorflow-estimator 2.0.* , which conflicts with any installable versions previously reported;. ```. Is it possible to resolve this conflict? I will try myself to see if I can get it to work on GCP, and report back.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/835
https://github.com/google/deepvariant/issues/835:389,deployability,instal,installable,389,"Hi,. Just to make sure I understand the issue: you are able to successfully install DeepVariant 0.7.0 with Python 2.7, but you are unable to install 1.15 or later; the goal is for you to install the latest version of DeepVariant, 1.6.0. . The relevant error that I see is:. ```. deepvariant [1.0.0|1.1.0|...|1.5.0] would require. │ └─ tensorflow-estimator 2.0.* , which conflicts with any installable versions previously reported;. ```. Is it possible to resolve this conflict? I will try myself to see if I can get it to work on GCP, and report back.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/835
https://github.com/google/deepvariant/issues/835:401,deployability,version,versions,401,"Hi,. Just to make sure I understand the issue: you are able to successfully install DeepVariant 0.7.0 with Python 2.7, but you are unable to install 1.15 or later; the goal is for you to install the latest version of DeepVariant, 1.6.0. . The relevant error that I see is:. ```. deepvariant [1.0.0|1.1.0|...|1.5.0] would require. │ └─ tensorflow-estimator 2.0.* , which conflicts with any installable versions previously reported;. ```. Is it possible to resolve this conflict? I will try myself to see if I can get it to work on GCP, and report back.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/835
https://github.com/google/deepvariant/issues/835:346,energy efficiency,estimat,estimator,346,"Hi,. Just to make sure I understand the issue: you are able to successfully install DeepVariant 0.7.0 with Python 2.7, but you are unable to install 1.15 or later; the goal is for you to install the latest version of DeepVariant, 1.6.0. . The relevant error that I see is:. ```. deepvariant [1.0.0|1.1.0|...|1.5.0] would require. │ └─ tensorflow-estimator 2.0.* , which conflicts with any installable versions previously reported;. ```. Is it possible to resolve this conflict? I will try myself to see if I can get it to work on GCP, and report back.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/835
https://github.com/google/deepvariant/issues/835:206,integrability,version,version,206,"Hi,. Just to make sure I understand the issue: you are able to successfully install DeepVariant 0.7.0 with Python 2.7, but you are unable to install 1.15 or later; the goal is for you to install the latest version of DeepVariant, 1.6.0. . The relevant error that I see is:. ```. deepvariant [1.0.0|1.1.0|...|1.5.0] would require. │ └─ tensorflow-estimator 2.0.* , which conflicts with any installable versions previously reported;. ```. Is it possible to resolve this conflict? I will try myself to see if I can get it to work on GCP, and report back.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/835
https://github.com/google/deepvariant/issues/835:401,integrability,version,versions,401,"Hi,. Just to make sure I understand the issue: you are able to successfully install DeepVariant 0.7.0 with Python 2.7, but you are unable to install 1.15 or later; the goal is for you to install the latest version of DeepVariant, 1.6.0. . The relevant error that I see is:. ```. deepvariant [1.0.0|1.1.0|...|1.5.0] would require. │ └─ tensorflow-estimator 2.0.* , which conflicts with any installable versions previously reported;. ```. Is it possible to resolve this conflict? I will try myself to see if I can get it to work on GCP, and report back.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/835
https://github.com/google/deepvariant/issues/835:370,interoperability,conflict,conflicts,370,"Hi,. Just to make sure I understand the issue: you are able to successfully install DeepVariant 0.7.0 with Python 2.7, but you are unable to install 1.15 or later; the goal is for you to install the latest version of DeepVariant, 1.6.0. . The relevant error that I see is:. ```. deepvariant [1.0.0|1.1.0|...|1.5.0] would require. │ └─ tensorflow-estimator 2.0.* , which conflicts with any installable versions previously reported;. ```. Is it possible to resolve this conflict? I will try myself to see if I can get it to work on GCP, and report back.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/835
https://github.com/google/deepvariant/issues/835:468,interoperability,conflict,conflict,468,"Hi,. Just to make sure I understand the issue: you are able to successfully install DeepVariant 0.7.0 with Python 2.7, but you are unable to install 1.15 or later; the goal is for you to install the latest version of DeepVariant, 1.6.0. . The relevant error that I see is:. ```. deepvariant [1.0.0|1.1.0|...|1.5.0] would require. │ └─ tensorflow-estimator 2.0.* , which conflicts with any installable versions previously reported;. ```. Is it possible to resolve this conflict? I will try myself to see if I can get it to work on GCP, and report back.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/835
https://github.com/google/deepvariant/issues/835:206,modifiability,version,version,206,"Hi,. Just to make sure I understand the issue: you are able to successfully install DeepVariant 0.7.0 with Python 2.7, but you are unable to install 1.15 or later; the goal is for you to install the latest version of DeepVariant, 1.6.0. . The relevant error that I see is:. ```. deepvariant [1.0.0|1.1.0|...|1.5.0] would require. │ └─ tensorflow-estimator 2.0.* , which conflicts with any installable versions previously reported;. ```. Is it possible to resolve this conflict? I will try myself to see if I can get it to work on GCP, and report back.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/835
https://github.com/google/deepvariant/issues/835:401,modifiability,version,versions,401,"Hi,. Just to make sure I understand the issue: you are able to successfully install DeepVariant 0.7.0 with Python 2.7, but you are unable to install 1.15 or later; the goal is for you to install the latest version of DeepVariant, 1.6.0. . The relevant error that I see is:. ```. deepvariant [1.0.0|1.1.0|...|1.5.0] would require. │ └─ tensorflow-estimator 2.0.* , which conflicts with any installable versions previously reported;. ```. Is it possible to resolve this conflict? I will try myself to see if I can get it to work on GCP, and report back.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/835
https://github.com/google/deepvariant/issues/835:252,performance,error,error,252,"Hi,. Just to make sure I understand the issue: you are able to successfully install DeepVariant 0.7.0 with Python 2.7, but you are unable to install 1.15 or later; the goal is for you to install the latest version of DeepVariant, 1.6.0. . The relevant error that I see is:. ```. deepvariant [1.0.0|1.1.0|...|1.5.0] would require. │ └─ tensorflow-estimator 2.0.* , which conflicts with any installable versions previously reported;. ```. Is it possible to resolve this conflict? I will try myself to see if I can get it to work on GCP, and report back.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/835
https://github.com/google/deepvariant/issues/835:252,safety,error,error,252,"Hi,. Just to make sure I understand the issue: you are able to successfully install DeepVariant 0.7.0 with Python 2.7, but you are unable to install 1.15 or later; the goal is for you to install the latest version of DeepVariant, 1.6.0. . The relevant error that I see is:. ```. deepvariant [1.0.0|1.1.0|...|1.5.0] would require. │ └─ tensorflow-estimator 2.0.* , which conflicts with any installable versions previously reported;. ```. Is it possible to resolve this conflict? I will try myself to see if I can get it to work on GCP, and report back.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/835
https://github.com/google/deepvariant/issues/835:25,testability,understand,understand,25,"Hi,. Just to make sure I understand the issue: you are able to successfully install DeepVariant 0.7.0 with Python 2.7, but you are unable to install 1.15 or later; the goal is for you to install the latest version of DeepVariant, 1.6.0. . The relevant error that I see is:. ```. deepvariant [1.0.0|1.1.0|...|1.5.0] would require. │ └─ tensorflow-estimator 2.0.* , which conflicts with any installable versions previously reported;. ```. Is it possible to resolve this conflict? I will try myself to see if I can get it to work on GCP, and report back.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/835
https://github.com/google/deepvariant/issues/835:252,usability,error,error,252,"Hi,. Just to make sure I understand the issue: you are able to successfully install DeepVariant 0.7.0 with Python 2.7, but you are unable to install 1.15 or later; the goal is for you to install the latest version of DeepVariant, 1.6.0. . The relevant error that I see is:. ```. deepvariant [1.0.0|1.1.0|...|1.5.0] would require. │ └─ tensorflow-estimator 2.0.* , which conflicts with any installable versions previously reported;. ```. Is it possible to resolve this conflict? I will try myself to see if I can get it to work on GCP, and report back.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/835
https://github.com/google/deepvariant/issues/835:145,availability,error,error,145,"Hi Lucas,. yes, the only version I can install is DeepVariant 0.7.0. I tried to install tensorflow-estimator first (v2.13.1) but still have same error as mentioned. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/835
https://github.com/google/deepvariant/issues/835:25,deployability,version,version,25,"Hi Lucas,. yes, the only version I can install is DeepVariant 0.7.0. I tried to install tensorflow-estimator first (v2.13.1) but still have same error as mentioned. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/835
https://github.com/google/deepvariant/issues/835:39,deployability,instal,install,39,"Hi Lucas,. yes, the only version I can install is DeepVariant 0.7.0. I tried to install tensorflow-estimator first (v2.13.1) but still have same error as mentioned. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/835
https://github.com/google/deepvariant/issues/835:80,deployability,instal,install,80,"Hi Lucas,. yes, the only version I can install is DeepVariant 0.7.0. I tried to install tensorflow-estimator first (v2.13.1) but still have same error as mentioned. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/835
https://github.com/google/deepvariant/issues/835:99,energy efficiency,estimat,estimator,99,"Hi Lucas,. yes, the only version I can install is DeepVariant 0.7.0. I tried to install tensorflow-estimator first (v2.13.1) but still have same error as mentioned. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/835
https://github.com/google/deepvariant/issues/835:25,integrability,version,version,25,"Hi Lucas,. yes, the only version I can install is DeepVariant 0.7.0. I tried to install tensorflow-estimator first (v2.13.1) but still have same error as mentioned. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/835
https://github.com/google/deepvariant/issues/835:25,modifiability,version,version,25,"Hi Lucas,. yes, the only version I can install is DeepVariant 0.7.0. I tried to install tensorflow-estimator first (v2.13.1) but still have same error as mentioned. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/835
https://github.com/google/deepvariant/issues/835:145,performance,error,error,145,"Hi Lucas,. yes, the only version I can install is DeepVariant 0.7.0. I tried to install tensorflow-estimator first (v2.13.1) but still have same error as mentioned. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/835
https://github.com/google/deepvariant/issues/835:145,safety,error,error,145,"Hi Lucas,. yes, the only version I can install is DeepVariant 0.7.0. I tried to install tensorflow-estimator first (v2.13.1) but still have same error as mentioned. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/835
https://github.com/google/deepvariant/issues/835:145,usability,error,error,145,"Hi Lucas,. yes, the only version I can install is DeepVariant 0.7.0. I tried to install tensorflow-estimator first (v2.13.1) but still have same error as mentioned. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/835
https://github.com/google/deepvariant/issues/835:77,deployability,version,version,77,"Hi @lok27395 ,. unfortunately, our team doesn't officially support the Conda version. I'll close this issue now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/835
https://github.com/google/deepvariant/issues/835:77,integrability,version,version,77,"Hi @lok27395 ,. unfortunately, our team doesn't officially support the Conda version. I'll close this issue now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/835
https://github.com/google/deepvariant/issues/835:77,modifiability,version,version,77,"Hi @lok27395 ,. unfortunately, our team doesn't officially support the Conda version. I'll close this issue now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/835
https://github.com/google/deepvariant/issues/835:40,reliability,doe,doesn,40,"Hi @lok27395 ,. unfortunately, our team doesn't officially support the Conda version. I'll close this issue now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/835
https://github.com/google/deepvariant/issues/835:35,security,team,team,35,"Hi @lok27395 ,. unfortunately, our team doesn't officially support the Conda version. I'll close this issue now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/835
https://github.com/google/deepvariant/issues/835:59,usability,support,support,59,"Hi @lok27395 ,. unfortunately, our team doesn't officially support the Conda version. I'll close this issue now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/835
https://github.com/google/deepvariant/issues/835:91,usability,close,close,91,"Hi @lok27395 ,. unfortunately, our team doesn't officially support the Conda version. I'll close this issue now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/835
https://github.com/google/deepvariant/issues/836:1065,availability,down,downsampling,1065,"Hi @may199128 ,. There can be several reasons for that:. > 1. PacBio Direct Data (top track): Despite having reads shown in both the IGV and VCF files, the genotype (GT) is marked as ""./.,"" and the genotype quality (GQ) is very low. This indel locus has higher coverage (141x) than the average coverage across the genome (128x). The lower GQ can be caused by several issues. If this is a variant dense region, then the model can be less confident in correctly calling the variant. Given that it looks like a structural variant, have you considered using a structural variant caller like [PBSV](https://github.com/PacificBiosciences/pbsv) for these cases? > 2. For the PacBio Capture data (bottom track) has an average genome coverage of 2897x. Even after setting the mapping quality to >30, the read count at this indel locus is still 3000x, which is significantly higher than the read count indicated in the VCF file. Why is there such a discrepancy between the read counts in IGV and VCF? The could be several reasons for seeing the difference: . a. We perform a downsampling of reads on a given window to make sure the processing time is manageable. So, some reads can get discounted during that process. b. For a specific variant, reads with lower mapping quality will not be counted. c. For a specific variant, reads that contain that variant with lower base quality then set is discounted. Please note that our pileup image height is 100, so we can only represent 100 reads per variant within an example. Having 3000x would mean the reads will be downsampled to a fraction that can be represented in the example. Please let me know if you have any other questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/836
https://github.com/google/deepvariant/issues/836:1553,availability,down,downsampled,1553,"Hi @may199128 ,. There can be several reasons for that:. > 1. PacBio Direct Data (top track): Despite having reads shown in both the IGV and VCF files, the genotype (GT) is marked as ""./.,"" and the genotype quality (GQ) is very low. This indel locus has higher coverage (141x) than the average coverage across the genome (128x). The lower GQ can be caused by several issues. If this is a variant dense region, then the model can be less confident in correctly calling the variant. Given that it looks like a structural variant, have you considered using a structural variant caller like [PBSV](https://github.com/PacificBiosciences/pbsv) for these cases? > 2. For the PacBio Capture data (bottom track) has an average genome coverage of 2897x. Even after setting the mapping quality to >30, the read count at this indel locus is still 3000x, which is significantly higher than the read count indicated in the VCF file. Why is there such a discrepancy between the read counts in IGV and VCF? The could be several reasons for seeing the difference: . a. We perform a downsampling of reads on a given window to make sure the processing time is manageable. So, some reads can get discounted during that process. b. For a specific variant, reads with lower mapping quality will not be counted. c. For a specific variant, reads that contain that variant with lower base quality then set is discounted. Please note that our pileup image height is 100, so we can only represent 100 reads per variant within an example. Having 3000x would mean the reads will be downsampled to a fraction that can be represented in the example. Please let me know if you have any other questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/836
https://github.com/google/deepvariant/issues/836:1141,deployability,manag,manageable,1141,"Hi @may199128 ,. There can be several reasons for that:. > 1. PacBio Direct Data (top track): Despite having reads shown in both the IGV and VCF files, the genotype (GT) is marked as ""./.,"" and the genotype quality (GQ) is very low. This indel locus has higher coverage (141x) than the average coverage across the genome (128x). The lower GQ can be caused by several issues. If this is a variant dense region, then the model can be less confident in correctly calling the variant. Given that it looks like a structural variant, have you considered using a structural variant caller like [PBSV](https://github.com/PacificBiosciences/pbsv) for these cases? > 2. For the PacBio Capture data (bottom track) has an average genome coverage of 2897x. Even after setting the mapping quality to >30, the read count at this indel locus is still 3000x, which is significantly higher than the read count indicated in the VCF file. Why is there such a discrepancy between the read counts in IGV and VCF? The could be several reasons for seeing the difference: . a. We perform a downsampling of reads on a given window to make sure the processing time is manageable. So, some reads can get discounted during that process. b. For a specific variant, reads with lower mapping quality will not be counted. c. For a specific variant, reads that contain that variant with lower base quality then set is discounted. Please note that our pileup image height is 100, so we can only represent 100 reads per variant within an example. Having 3000x would mean the reads will be downsampled to a fraction that can be represented in the example. Please let me know if you have any other questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/836
https://github.com/google/deepvariant/issues/836:1327,deployability,contain,contain,1327,"Hi @may199128 ,. There can be several reasons for that:. > 1. PacBio Direct Data (top track): Despite having reads shown in both the IGV and VCF files, the genotype (GT) is marked as ""./.,"" and the genotype quality (GQ) is very low. This indel locus has higher coverage (141x) than the average coverage across the genome (128x). The lower GQ can be caused by several issues. If this is a variant dense region, then the model can be less confident in correctly calling the variant. Given that it looks like a structural variant, have you considered using a structural variant caller like [PBSV](https://github.com/PacificBiosciences/pbsv) for these cases? > 2. For the PacBio Capture data (bottom track) has an average genome coverage of 2897x. Even after setting the mapping quality to >30, the read count at this indel locus is still 3000x, which is significantly higher than the read count indicated in the VCF file. Why is there such a discrepancy between the read counts in IGV and VCF? The could be several reasons for seeing the difference: . a. We perform a downsampling of reads on a given window to make sure the processing time is manageable. So, some reads can get discounted during that process. b. For a specific variant, reads with lower mapping quality will not be counted. c. For a specific variant, reads that contain that variant with lower base quality then set is discounted. Please note that our pileup image height is 100, so we can only represent 100 reads per variant within an example. Having 3000x would mean the reads will be downsampled to a fraction that can be represented in the example. Please let me know if you have any other questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/836
https://github.com/google/deepvariant/issues/836:419,energy efficiency,model,model,419,"Hi @may199128 ,. There can be several reasons for that:. > 1. PacBio Direct Data (top track): Despite having reads shown in both the IGV and VCF files, the genotype (GT) is marked as ""./.,"" and the genotype quality (GQ) is very low. This indel locus has higher coverage (141x) than the average coverage across the genome (128x). The lower GQ can be caused by several issues. If this is a variant dense region, then the model can be less confident in correctly calling the variant. Given that it looks like a structural variant, have you considered using a structural variant caller like [PBSV](https://github.com/PacificBiosciences/pbsv) for these cases? > 2. For the PacBio Capture data (bottom track) has an average genome coverage of 2897x. Even after setting the mapping quality to >30, the read count at this indel locus is still 3000x, which is significantly higher than the read count indicated in the VCF file. Why is there such a discrepancy between the read counts in IGV and VCF? The could be several reasons for seeing the difference: . a. We perform a downsampling of reads on a given window to make sure the processing time is manageable. So, some reads can get discounted during that process. b. For a specific variant, reads with lower mapping quality will not be counted. c. For a specific variant, reads that contain that variant with lower base quality then set is discounted. Please note that our pileup image height is 100, so we can only represent 100 reads per variant within an example. Having 3000x would mean the reads will be downsampled to a fraction that can be represented in the example. Please let me know if you have any other questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/836
https://github.com/google/deepvariant/issues/836:1141,energy efficiency,manag,manageable,1141,"Hi @may199128 ,. There can be several reasons for that:. > 1. PacBio Direct Data (top track): Despite having reads shown in both the IGV and VCF files, the genotype (GT) is marked as ""./.,"" and the genotype quality (GQ) is very low. This indel locus has higher coverage (141x) than the average coverage across the genome (128x). The lower GQ can be caused by several issues. If this is a variant dense region, then the model can be less confident in correctly calling the variant. Given that it looks like a structural variant, have you considered using a structural variant caller like [PBSV](https://github.com/PacificBiosciences/pbsv) for these cases? > 2. For the PacBio Capture data (bottom track) has an average genome coverage of 2897x. Even after setting the mapping quality to >30, the read count at this indel locus is still 3000x, which is significantly higher than the read count indicated in the VCF file. Why is there such a discrepancy between the read counts in IGV and VCF? The could be several reasons for seeing the difference: . a. We perform a downsampling of reads on a given window to make sure the processing time is manageable. So, some reads can get discounted during that process. b. For a specific variant, reads with lower mapping quality will not be counted. c. For a specific variant, reads that contain that variant with lower base quality then set is discounted. Please note that our pileup image height is 100, so we can only represent 100 reads per variant within an example. Having 3000x would mean the reads will be downsampled to a fraction that can be represented in the example. Please let me know if you have any other questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/836
https://github.com/google/deepvariant/issues/836:1217,interoperability,specif,specific,1217,"Hi @may199128 ,. There can be several reasons for that:. > 1. PacBio Direct Data (top track): Despite having reads shown in both the IGV and VCF files, the genotype (GT) is marked as ""./.,"" and the genotype quality (GQ) is very low. This indel locus has higher coverage (141x) than the average coverage across the genome (128x). The lower GQ can be caused by several issues. If this is a variant dense region, then the model can be less confident in correctly calling the variant. Given that it looks like a structural variant, have you considered using a structural variant caller like [PBSV](https://github.com/PacificBiosciences/pbsv) for these cases? > 2. For the PacBio Capture data (bottom track) has an average genome coverage of 2897x. Even after setting the mapping quality to >30, the read count at this indel locus is still 3000x, which is significantly higher than the read count indicated in the VCF file. Why is there such a discrepancy between the read counts in IGV and VCF? The could be several reasons for seeing the difference: . a. We perform a downsampling of reads on a given window to make sure the processing time is manageable. So, some reads can get discounted during that process. b. For a specific variant, reads with lower mapping quality will not be counted. c. For a specific variant, reads that contain that variant with lower base quality then set is discounted. Please note that our pileup image height is 100, so we can only represent 100 reads per variant within an example. Having 3000x would mean the reads will be downsampled to a fraction that can be represented in the example. Please let me know if you have any other questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/836
https://github.com/google/deepvariant/issues/836:1298,interoperability,specif,specific,1298,"Hi @may199128 ,. There can be several reasons for that:. > 1. PacBio Direct Data (top track): Despite having reads shown in both the IGV and VCF files, the genotype (GT) is marked as ""./.,"" and the genotype quality (GQ) is very low. This indel locus has higher coverage (141x) than the average coverage across the genome (128x). The lower GQ can be caused by several issues. If this is a variant dense region, then the model can be less confident in correctly calling the variant. Given that it looks like a structural variant, have you considered using a structural variant caller like [PBSV](https://github.com/PacificBiosciences/pbsv) for these cases? > 2. For the PacBio Capture data (bottom track) has an average genome coverage of 2897x. Even after setting the mapping quality to >30, the read count at this indel locus is still 3000x, which is significantly higher than the read count indicated in the VCF file. Why is there such a discrepancy between the read counts in IGV and VCF? The could be several reasons for seeing the difference: . a. We perform a downsampling of reads on a given window to make sure the processing time is manageable. So, some reads can get discounted during that process. b. For a specific variant, reads with lower mapping quality will not be counted. c. For a specific variant, reads that contain that variant with lower base quality then set is discounted. Please note that our pileup image height is 100, so we can only represent 100 reads per variant within an example. Having 3000x would mean the reads will be downsampled to a fraction that can be represented in the example. Please let me know if you have any other questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/836
https://github.com/google/deepvariant/issues/836:62,modifiability,Pac,PacBio,62,"Hi @may199128 ,. There can be several reasons for that:. > 1. PacBio Direct Data (top track): Despite having reads shown in both the IGV and VCF files, the genotype (GT) is marked as ""./.,"" and the genotype quality (GQ) is very low. This indel locus has higher coverage (141x) than the average coverage across the genome (128x). The lower GQ can be caused by several issues. If this is a variant dense region, then the model can be less confident in correctly calling the variant. Given that it looks like a structural variant, have you considered using a structural variant caller like [PBSV](https://github.com/PacificBiosciences/pbsv) for these cases? > 2. For the PacBio Capture data (bottom track) has an average genome coverage of 2897x. Even after setting the mapping quality to >30, the read count at this indel locus is still 3000x, which is significantly higher than the read count indicated in the VCF file. Why is there such a discrepancy between the read counts in IGV and VCF? The could be several reasons for seeing the difference: . a. We perform a downsampling of reads on a given window to make sure the processing time is manageable. So, some reads can get discounted during that process. b. For a specific variant, reads with lower mapping quality will not be counted. c. For a specific variant, reads that contain that variant with lower base quality then set is discounted. Please note that our pileup image height is 100, so we can only represent 100 reads per variant within an example. Having 3000x would mean the reads will be downsampled to a fraction that can be represented in the example. Please let me know if you have any other questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/836
https://github.com/google/deepvariant/issues/836:613,modifiability,Pac,PacificBiosciences,613,"Hi @may199128 ,. There can be several reasons for that:. > 1. PacBio Direct Data (top track): Despite having reads shown in both the IGV and VCF files, the genotype (GT) is marked as ""./.,"" and the genotype quality (GQ) is very low. This indel locus has higher coverage (141x) than the average coverage across the genome (128x). The lower GQ can be caused by several issues. If this is a variant dense region, then the model can be less confident in correctly calling the variant. Given that it looks like a structural variant, have you considered using a structural variant caller like [PBSV](https://github.com/PacificBiosciences/pbsv) for these cases? > 2. For the PacBio Capture data (bottom track) has an average genome coverage of 2897x. Even after setting the mapping quality to >30, the read count at this indel locus is still 3000x, which is significantly higher than the read count indicated in the VCF file. Why is there such a discrepancy between the read counts in IGV and VCF? The could be several reasons for seeing the difference: . a. We perform a downsampling of reads on a given window to make sure the processing time is manageable. So, some reads can get discounted during that process. b. For a specific variant, reads with lower mapping quality will not be counted. c. For a specific variant, reads that contain that variant with lower base quality then set is discounted. Please note that our pileup image height is 100, so we can only represent 100 reads per variant within an example. Having 3000x would mean the reads will be downsampled to a fraction that can be represented in the example. Please let me know if you have any other questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/836
https://github.com/google/deepvariant/issues/836:668,modifiability,Pac,PacBio,668,"Hi @may199128 ,. There can be several reasons for that:. > 1. PacBio Direct Data (top track): Despite having reads shown in both the IGV and VCF files, the genotype (GT) is marked as ""./.,"" and the genotype quality (GQ) is very low. This indel locus has higher coverage (141x) than the average coverage across the genome (128x). The lower GQ can be caused by several issues. If this is a variant dense region, then the model can be less confident in correctly calling the variant. Given that it looks like a structural variant, have you considered using a structural variant caller like [PBSV](https://github.com/PacificBiosciences/pbsv) for these cases? > 2. For the PacBio Capture data (bottom track) has an average genome coverage of 2897x. Even after setting the mapping quality to >30, the read count at this indel locus is still 3000x, which is significantly higher than the read count indicated in the VCF file. Why is there such a discrepancy between the read counts in IGV and VCF? The could be several reasons for seeing the difference: . a. We perform a downsampling of reads on a given window to make sure the processing time is manageable. So, some reads can get discounted during that process. b. For a specific variant, reads with lower mapping quality will not be counted. c. For a specific variant, reads that contain that variant with lower base quality then set is discounted. Please note that our pileup image height is 100, so we can only represent 100 reads per variant within an example. Having 3000x would mean the reads will be downsampled to a fraction that can be represented in the example. Please let me know if you have any other questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/836
https://github.com/google/deepvariant/issues/836:1055,performance,perform,perform,1055,"Hi @may199128 ,. There can be several reasons for that:. > 1. PacBio Direct Data (top track): Despite having reads shown in both the IGV and VCF files, the genotype (GT) is marked as ""./.,"" and the genotype quality (GQ) is very low. This indel locus has higher coverage (141x) than the average coverage across the genome (128x). The lower GQ can be caused by several issues. If this is a variant dense region, then the model can be less confident in correctly calling the variant. Given that it looks like a structural variant, have you considered using a structural variant caller like [PBSV](https://github.com/PacificBiosciences/pbsv) for these cases? > 2. For the PacBio Capture data (bottom track) has an average genome coverage of 2897x. Even after setting the mapping quality to >30, the read count at this indel locus is still 3000x, which is significantly higher than the read count indicated in the VCF file. Why is there such a discrepancy between the read counts in IGV and VCF? The could be several reasons for seeing the difference: . a. We perform a downsampling of reads on a given window to make sure the processing time is manageable. So, some reads can get discounted during that process. b. For a specific variant, reads with lower mapping quality will not be counted. c. For a specific variant, reads that contain that variant with lower base quality then set is discounted. Please note that our pileup image height is 100, so we can only represent 100 reads per variant within an example. Having 3000x would mean the reads will be downsampled to a fraction that can be represented in the example. Please let me know if you have any other questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/836
https://github.com/google/deepvariant/issues/836:1133,performance,time,time,1133,"Hi @may199128 ,. There can be several reasons for that:. > 1. PacBio Direct Data (top track): Despite having reads shown in both the IGV and VCF files, the genotype (GT) is marked as ""./.,"" and the genotype quality (GQ) is very low. This indel locus has higher coverage (141x) than the average coverage across the genome (128x). The lower GQ can be caused by several issues. If this is a variant dense region, then the model can be less confident in correctly calling the variant. Given that it looks like a structural variant, have you considered using a structural variant caller like [PBSV](https://github.com/PacificBiosciences/pbsv) for these cases? > 2. For the PacBio Capture data (bottom track) has an average genome coverage of 2897x. Even after setting the mapping quality to >30, the read count at this indel locus is still 3000x, which is significantly higher than the read count indicated in the VCF file. Why is there such a discrepancy between the read counts in IGV and VCF? The could be several reasons for seeing the difference: . a. We perform a downsampling of reads on a given window to make sure the processing time is manageable. So, some reads can get discounted during that process. b. For a specific variant, reads with lower mapping quality will not be counted. c. For a specific variant, reads that contain that variant with lower base quality then set is discounted. Please note that our pileup image height is 100, so we can only represent 100 reads per variant within an example. Having 3000x would mean the reads will be downsampled to a fraction that can be represented in the example. Please let me know if you have any other questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/836
https://github.com/google/deepvariant/issues/836:1141,safety,manag,manageable,1141,"Hi @may199128 ,. There can be several reasons for that:. > 1. PacBio Direct Data (top track): Despite having reads shown in both the IGV and VCF files, the genotype (GT) is marked as ""./.,"" and the genotype quality (GQ) is very low. This indel locus has higher coverage (141x) than the average coverage across the genome (128x). The lower GQ can be caused by several issues. If this is a variant dense region, then the model can be less confident in correctly calling the variant. Given that it looks like a structural variant, have you considered using a structural variant caller like [PBSV](https://github.com/PacificBiosciences/pbsv) for these cases? > 2. For the PacBio Capture data (bottom track) has an average genome coverage of 2897x. Even after setting the mapping quality to >30, the read count at this indel locus is still 3000x, which is significantly higher than the read count indicated in the VCF file. Why is there such a discrepancy between the read counts in IGV and VCF? The could be several reasons for seeing the difference: . a. We perform a downsampling of reads on a given window to make sure the processing time is manageable. So, some reads can get discounted during that process. b. For a specific variant, reads with lower mapping quality will not be counted. c. For a specific variant, reads that contain that variant with lower base quality then set is discounted. Please note that our pileup image height is 100, so we can only represent 100 reads per variant within an example. Having 3000x would mean the reads will be downsampled to a fraction that can be represented in the example. Please let me know if you have any other questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/836
https://github.com/google/deepvariant/issues/836:419,security,model,model,419,"Hi @may199128 ,. There can be several reasons for that:. > 1. PacBio Direct Data (top track): Despite having reads shown in both the IGV and VCF files, the genotype (GT) is marked as ""./.,"" and the genotype quality (GQ) is very low. This indel locus has higher coverage (141x) than the average coverage across the genome (128x). The lower GQ can be caused by several issues. If this is a variant dense region, then the model can be less confident in correctly calling the variant. Given that it looks like a structural variant, have you considered using a structural variant caller like [PBSV](https://github.com/PacificBiosciences/pbsv) for these cases? > 2. For the PacBio Capture data (bottom track) has an average genome coverage of 2897x. Even after setting the mapping quality to >30, the read count at this indel locus is still 3000x, which is significantly higher than the read count indicated in the VCF file. Why is there such a discrepancy between the read counts in IGV and VCF? The could be several reasons for seeing the difference: . a. We perform a downsampling of reads on a given window to make sure the processing time is manageable. So, some reads can get discounted during that process. b. For a specific variant, reads with lower mapping quality will not be counted. c. For a specific variant, reads that contain that variant with lower base quality then set is discounted. Please note that our pileup image height is 100, so we can only represent 100 reads per variant within an example. Having 3000x would mean the reads will be downsampled to a fraction that can be represented in the example. Please let me know if you have any other questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/836
https://github.com/google/deepvariant/issues/836:851,security,sign,significantly,851,"Hi @may199128 ,. There can be several reasons for that:. > 1. PacBio Direct Data (top track): Despite having reads shown in both the IGV and VCF files, the genotype (GT) is marked as ""./.,"" and the genotype quality (GQ) is very low. This indel locus has higher coverage (141x) than the average coverage across the genome (128x). The lower GQ can be caused by several issues. If this is a variant dense region, then the model can be less confident in correctly calling the variant. Given that it looks like a structural variant, have you considered using a structural variant caller like [PBSV](https://github.com/PacificBiosciences/pbsv) for these cases? > 2. For the PacBio Capture data (bottom track) has an average genome coverage of 2897x. Even after setting the mapping quality to >30, the read count at this indel locus is still 3000x, which is significantly higher than the read count indicated in the VCF file. Why is there such a discrepancy between the read counts in IGV and VCF? The could be several reasons for seeing the difference: . a. We perform a downsampling of reads on a given window to make sure the processing time is manageable. So, some reads can get discounted during that process. b. For a specific variant, reads with lower mapping quality will not be counted. c. For a specific variant, reads that contain that variant with lower base quality then set is discounted. Please note that our pileup image height is 100, so we can only represent 100 reads per variant within an example. Having 3000x would mean the reads will be downsampled to a fraction that can be represented in the example. Please let me know if you have any other questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/836
https://github.com/google/deepvariant/issues/836:261,testability,coverag,coverage,261,"Hi @may199128 ,. There can be several reasons for that:. > 1. PacBio Direct Data (top track): Despite having reads shown in both the IGV and VCF files, the genotype (GT) is marked as ""./.,"" and the genotype quality (GQ) is very low. This indel locus has higher coverage (141x) than the average coverage across the genome (128x). The lower GQ can be caused by several issues. If this is a variant dense region, then the model can be less confident in correctly calling the variant. Given that it looks like a structural variant, have you considered using a structural variant caller like [PBSV](https://github.com/PacificBiosciences/pbsv) for these cases? > 2. For the PacBio Capture data (bottom track) has an average genome coverage of 2897x. Even after setting the mapping quality to >30, the read count at this indel locus is still 3000x, which is significantly higher than the read count indicated in the VCF file. Why is there such a discrepancy between the read counts in IGV and VCF? The could be several reasons for seeing the difference: . a. We perform a downsampling of reads on a given window to make sure the processing time is manageable. So, some reads can get discounted during that process. b. For a specific variant, reads with lower mapping quality will not be counted. c. For a specific variant, reads that contain that variant with lower base quality then set is discounted. Please note that our pileup image height is 100, so we can only represent 100 reads per variant within an example. Having 3000x would mean the reads will be downsampled to a fraction that can be represented in the example. Please let me know if you have any other questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/836
https://github.com/google/deepvariant/issues/836:294,testability,coverag,coverage,294,"Hi @may199128 ,. There can be several reasons for that:. > 1. PacBio Direct Data (top track): Despite having reads shown in both the IGV and VCF files, the genotype (GT) is marked as ""./.,"" and the genotype quality (GQ) is very low. This indel locus has higher coverage (141x) than the average coverage across the genome (128x). The lower GQ can be caused by several issues. If this is a variant dense region, then the model can be less confident in correctly calling the variant. Given that it looks like a structural variant, have you considered using a structural variant caller like [PBSV](https://github.com/PacificBiosciences/pbsv) for these cases? > 2. For the PacBio Capture data (bottom track) has an average genome coverage of 2897x. Even after setting the mapping quality to >30, the read count at this indel locus is still 3000x, which is significantly higher than the read count indicated in the VCF file. Why is there such a discrepancy between the read counts in IGV and VCF? The could be several reasons for seeing the difference: . a. We perform a downsampling of reads on a given window to make sure the processing time is manageable. So, some reads can get discounted during that process. b. For a specific variant, reads with lower mapping quality will not be counted. c. For a specific variant, reads that contain that variant with lower base quality then set is discounted. Please note that our pileup image height is 100, so we can only represent 100 reads per variant within an example. Having 3000x would mean the reads will be downsampled to a fraction that can be represented in the example. Please let me know if you have any other questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/836
https://github.com/google/deepvariant/issues/836:725,testability,coverag,coverage,725,"Hi @may199128 ,. There can be several reasons for that:. > 1. PacBio Direct Data (top track): Despite having reads shown in both the IGV and VCF files, the genotype (GT) is marked as ""./.,"" and the genotype quality (GQ) is very low. This indel locus has higher coverage (141x) than the average coverage across the genome (128x). The lower GQ can be caused by several issues. If this is a variant dense region, then the model can be less confident in correctly calling the variant. Given that it looks like a structural variant, have you considered using a structural variant caller like [PBSV](https://github.com/PacificBiosciences/pbsv) for these cases? > 2. For the PacBio Capture data (bottom track) has an average genome coverage of 2897x. Even after setting the mapping quality to >30, the read count at this indel locus is still 3000x, which is significantly higher than the read count indicated in the VCF file. Why is there such a discrepancy between the read counts in IGV and VCF? The could be several reasons for seeing the difference: . a. We perform a downsampling of reads on a given window to make sure the processing time is manageable. So, some reads can get discounted during that process. b. For a specific variant, reads with lower mapping quality will not be counted. c. For a specific variant, reads that contain that variant with lower base quality then set is discounted. Please note that our pileup image height is 100, so we can only represent 100 reads per variant within an example. Having 3000x would mean the reads will be downsampled to a fraction that can be represented in the example. Please let me know if you have any other questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/836
https://github.com/google/deepvariant/issues/836:892,usability,indicat,indicated,892,"Hi @may199128 ,. There can be several reasons for that:. > 1. PacBio Direct Data (top track): Despite having reads shown in both the IGV and VCF files, the genotype (GT) is marked as ""./.,"" and the genotype quality (GQ) is very low. This indel locus has higher coverage (141x) than the average coverage across the genome (128x). The lower GQ can be caused by several issues. If this is a variant dense region, then the model can be less confident in correctly calling the variant. Given that it looks like a structural variant, have you considered using a structural variant caller like [PBSV](https://github.com/PacificBiosciences/pbsv) for these cases? > 2. For the PacBio Capture data (bottom track) has an average genome coverage of 2897x. Even after setting the mapping quality to >30, the read count at this indel locus is still 3000x, which is significantly higher than the read count indicated in the VCF file. Why is there such a discrepancy between the read counts in IGV and VCF? The could be several reasons for seeing the difference: . a. We perform a downsampling of reads on a given window to make sure the processing time is manageable. So, some reads can get discounted during that process. b. For a specific variant, reads with lower mapping quality will not be counted. c. For a specific variant, reads that contain that variant with lower base quality then set is discounted. Please note that our pileup image height is 100, so we can only represent 100 reads per variant within an example. Having 3000x would mean the reads will be downsampled to a fraction that can be represented in the example. Please let me know if you have any other questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/836
https://github.com/google/deepvariant/issues/836:1055,usability,perform,perform,1055,"Hi @may199128 ,. There can be several reasons for that:. > 1. PacBio Direct Data (top track): Despite having reads shown in both the IGV and VCF files, the genotype (GT) is marked as ""./.,"" and the genotype quality (GQ) is very low. This indel locus has higher coverage (141x) than the average coverage across the genome (128x). The lower GQ can be caused by several issues. If this is a variant dense region, then the model can be less confident in correctly calling the variant. Given that it looks like a structural variant, have you considered using a structural variant caller like [PBSV](https://github.com/PacificBiosciences/pbsv) for these cases? > 2. For the PacBio Capture data (bottom track) has an average genome coverage of 2897x. Even after setting the mapping quality to >30, the read count at this indel locus is still 3000x, which is significantly higher than the read count indicated in the VCF file. Why is there such a discrepancy between the read counts in IGV and VCF? The could be several reasons for seeing the difference: . a. We perform a downsampling of reads on a given window to make sure the processing time is manageable. So, some reads can get discounted during that process. b. For a specific variant, reads with lower mapping quality will not be counted. c. For a specific variant, reads that contain that variant with lower base quality then set is discounted. Please note that our pileup image height is 100, so we can only represent 100 reads per variant within an example. Having 3000x would mean the reads will be downsampled to a fraction that can be represented in the example. Please let me know if you have any other questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/836
https://github.com/google/deepvariant/issues/836:57,safety,review,review,57,"Hi @kishwarshafin,. Thank you for your response. We will review and discuss your comments. If we have any further questions, we will reach out to you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/836
https://github.com/google/deepvariant/issues/836:57,testability,review,review,57,"Hi @kishwarshafin,. Thank you for your response. We will review and discuss your comments. If we have any further questions, we will reach out to you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/836
https://github.com/google/deepvariant/issues/836:64,availability,down,down,64,"Hi @kishwarshafin,. Thank you for explaining the details of the down sampling process. - a. We perform a downsampling of reads on a given window to ensure the processing time is manageable. Some reads may be discounted during this process. - b. For a specific variant, reads with lower mapping quality will not be counted. - c. For a specific variant, reads that contain that variant with lower base quality than the set threshold are discounted. For positions with higher than 100x coverage, is the downsampling performed sequentially as mentioned above? . If not, is it randomly or specific process? Thank you for your assistance.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/836
https://github.com/google/deepvariant/issues/836:105,availability,down,downsampling,105,"Hi @kishwarshafin,. Thank you for explaining the details of the down sampling process. - a. We perform a downsampling of reads on a given window to ensure the processing time is manageable. Some reads may be discounted during this process. - b. For a specific variant, reads with lower mapping quality will not be counted. - c. For a specific variant, reads that contain that variant with lower base quality than the set threshold are discounted. For positions with higher than 100x coverage, is the downsampling performed sequentially as mentioned above? . If not, is it randomly or specific process? Thank you for your assistance.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/836
https://github.com/google/deepvariant/issues/836:500,availability,down,downsampling,500,"Hi @kishwarshafin,. Thank you for explaining the details of the down sampling process. - a. We perform a downsampling of reads on a given window to ensure the processing time is manageable. Some reads may be discounted during this process. - b. For a specific variant, reads with lower mapping quality will not be counted. - c. For a specific variant, reads that contain that variant with lower base quality than the set threshold are discounted. For positions with higher than 100x coverage, is the downsampling performed sequentially as mentioned above? . If not, is it randomly or specific process? Thank you for your assistance.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/836
https://github.com/google/deepvariant/issues/836:178,deployability,manag,manageable,178,"Hi @kishwarshafin,. Thank you for explaining the details of the down sampling process. - a. We perform a downsampling of reads on a given window to ensure the processing time is manageable. Some reads may be discounted during this process. - b. For a specific variant, reads with lower mapping quality will not be counted. - c. For a specific variant, reads that contain that variant with lower base quality than the set threshold are discounted. For positions with higher than 100x coverage, is the downsampling performed sequentially as mentioned above? . If not, is it randomly or specific process? Thank you for your assistance.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/836
https://github.com/google/deepvariant/issues/836:363,deployability,contain,contain,363,"Hi @kishwarshafin,. Thank you for explaining the details of the down sampling process. - a. We perform a downsampling of reads on a given window to ensure the processing time is manageable. Some reads may be discounted during this process. - b. For a specific variant, reads with lower mapping quality will not be counted. - c. For a specific variant, reads that contain that variant with lower base quality than the set threshold are discounted. For positions with higher than 100x coverage, is the downsampling performed sequentially as mentioned above? . If not, is it randomly or specific process? Thank you for your assistance.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/836
https://github.com/google/deepvariant/issues/836:178,energy efficiency,manag,manageable,178,"Hi @kishwarshafin,. Thank you for explaining the details of the down sampling process. - a. We perform a downsampling of reads on a given window to ensure the processing time is manageable. Some reads may be discounted during this process. - b. For a specific variant, reads with lower mapping quality will not be counted. - c. For a specific variant, reads that contain that variant with lower base quality than the set threshold are discounted. For positions with higher than 100x coverage, is the downsampling performed sequentially as mentioned above? . If not, is it randomly or specific process? Thank you for your assistance.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/836
https://github.com/google/deepvariant/issues/836:251,interoperability,specif,specific,251,"Hi @kishwarshafin,. Thank you for explaining the details of the down sampling process. - a. We perform a downsampling of reads on a given window to ensure the processing time is manageable. Some reads may be discounted during this process. - b. For a specific variant, reads with lower mapping quality will not be counted. - c. For a specific variant, reads that contain that variant with lower base quality than the set threshold are discounted. For positions with higher than 100x coverage, is the downsampling performed sequentially as mentioned above? . If not, is it randomly or specific process? Thank you for your assistance.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/836
https://github.com/google/deepvariant/issues/836:334,interoperability,specif,specific,334,"Hi @kishwarshafin,. Thank you for explaining the details of the down sampling process. - a. We perform a downsampling of reads on a given window to ensure the processing time is manageable. Some reads may be discounted during this process. - b. For a specific variant, reads with lower mapping quality will not be counted. - c. For a specific variant, reads that contain that variant with lower base quality than the set threshold are discounted. For positions with higher than 100x coverage, is the downsampling performed sequentially as mentioned above? . If not, is it randomly or specific process? Thank you for your assistance.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/836
https://github.com/google/deepvariant/issues/836:584,interoperability,specif,specific,584,"Hi @kishwarshafin,. Thank you for explaining the details of the down sampling process. - a. We perform a downsampling of reads on a given window to ensure the processing time is manageable. Some reads may be discounted during this process. - b. For a specific variant, reads with lower mapping quality will not be counted. - c. For a specific variant, reads that contain that variant with lower base quality than the set threshold are discounted. For positions with higher than 100x coverage, is the downsampling performed sequentially as mentioned above? . If not, is it randomly or specific process? Thank you for your assistance.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/836
https://github.com/google/deepvariant/issues/836:95,performance,perform,perform,95,"Hi @kishwarshafin,. Thank you for explaining the details of the down sampling process. - a. We perform a downsampling of reads on a given window to ensure the processing time is manageable. Some reads may be discounted during this process. - b. For a specific variant, reads with lower mapping quality will not be counted. - c. For a specific variant, reads that contain that variant with lower base quality than the set threshold are discounted. For positions with higher than 100x coverage, is the downsampling performed sequentially as mentioned above? . If not, is it randomly or specific process? Thank you for your assistance.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/836
https://github.com/google/deepvariant/issues/836:170,performance,time,time,170,"Hi @kishwarshafin,. Thank you for explaining the details of the down sampling process. - a. We perform a downsampling of reads on a given window to ensure the processing time is manageable. Some reads may be discounted during this process. - b. For a specific variant, reads with lower mapping quality will not be counted. - c. For a specific variant, reads that contain that variant with lower base quality than the set threshold are discounted. For positions with higher than 100x coverage, is the downsampling performed sequentially as mentioned above? . If not, is it randomly or specific process? Thank you for your assistance.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/836
https://github.com/google/deepvariant/issues/836:513,performance,perform,performed,513,"Hi @kishwarshafin,. Thank you for explaining the details of the down sampling process. - a. We perform a downsampling of reads on a given window to ensure the processing time is manageable. Some reads may be discounted during this process. - b. For a specific variant, reads with lower mapping quality will not be counted. - c. For a specific variant, reads that contain that variant with lower base quality than the set threshold are discounted. For positions with higher than 100x coverage, is the downsampling performed sequentially as mentioned above? . If not, is it randomly or specific process? Thank you for your assistance.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/836
https://github.com/google/deepvariant/issues/836:178,safety,manag,manageable,178,"Hi @kishwarshafin,. Thank you for explaining the details of the down sampling process. - a. We perform a downsampling of reads on a given window to ensure the processing time is manageable. Some reads may be discounted during this process. - b. For a specific variant, reads with lower mapping quality will not be counted. - c. For a specific variant, reads that contain that variant with lower base quality than the set threshold are discounted. For positions with higher than 100x coverage, is the downsampling performed sequentially as mentioned above? . If not, is it randomly or specific process? Thank you for your assistance.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/836
https://github.com/google/deepvariant/issues/836:483,testability,coverag,coverage,483,"Hi @kishwarshafin,. Thank you for explaining the details of the down sampling process. - a. We perform a downsampling of reads on a given window to ensure the processing time is manageable. Some reads may be discounted during this process. - b. For a specific variant, reads with lower mapping quality will not be counted. - c. For a specific variant, reads that contain that variant with lower base quality than the set threshold are discounted. For positions with higher than 100x coverage, is the downsampling performed sequentially as mentioned above? . If not, is it randomly or specific process? Thank you for your assistance.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/836
https://github.com/google/deepvariant/issues/836:95,usability,perform,perform,95,"Hi @kishwarshafin,. Thank you for explaining the details of the down sampling process. - a. We perform a downsampling of reads on a given window to ensure the processing time is manageable. Some reads may be discounted during this process. - b. For a specific variant, reads with lower mapping quality will not be counted. - c. For a specific variant, reads that contain that variant with lower base quality than the set threshold are discounted. For positions with higher than 100x coverage, is the downsampling performed sequentially as mentioned above? . If not, is it randomly or specific process? Thank you for your assistance.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/836
https://github.com/google/deepvariant/issues/836:513,usability,perform,performed,513,"Hi @kishwarshafin,. Thank you for explaining the details of the down sampling process. - a. We perform a downsampling of reads on a given window to ensure the processing time is manageable. Some reads may be discounted during this process. - b. For a specific variant, reads with lower mapping quality will not be counted. - c. For a specific variant, reads that contain that variant with lower base quality than the set threshold are discounted. For positions with higher than 100x coverage, is the downsampling performed sequentially as mentioned above? . If not, is it randomly or specific process? Thank you for your assistance.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/836
https://github.com/google/deepvariant/issues/837:32,availability,error,error,32,Hi! I'm not sure I've seen this error before. It seems that the file is accessible given that it appears to be failing on parsing of the `DeepVariantDatasetConfig` proto. . Is is possible for you to share 1 of the files in the mounted bucket? That would allow us to try this on our end. Thanks!,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/837
https://github.com/google/deepvariant/issues/837:111,deployability,fail,failing,111,Hi! I'm not sure I've seen this error before. It seems that the file is accessible given that it appears to be failing on parsing of the `DeepVariantDatasetConfig` proto. . Is is possible for you to share 1 of the files in the mounted bucket? That would allow us to try this on our end. Thanks!,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/837
https://github.com/google/deepvariant/issues/837:199,interoperability,share,share,199,Hi! I'm not sure I've seen this error before. It seems that the file is accessible given that it appears to be failing on parsing of the `DeepVariantDatasetConfig` proto. . Is is possible for you to share 1 of the files in the mounted bucket? That would allow us to try this on our end. Thanks!,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/837
https://github.com/google/deepvariant/issues/837:32,performance,error,error,32,Hi! I'm not sure I've seen this error before. It seems that the file is accessible given that it appears to be failing on parsing of the `DeepVariantDatasetConfig` proto. . Is is possible for you to share 1 of the files in the mounted bucket? That would allow us to try this on our end. Thanks!,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/837
https://github.com/google/deepvariant/issues/837:111,reliability,fail,failing,111,Hi! I'm not sure I've seen this error before. It seems that the file is accessible given that it appears to be failing on parsing of the `DeepVariantDatasetConfig` proto. . Is is possible for you to share 1 of the files in the mounted bucket? That would allow us to try this on our end. Thanks!,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/837
https://github.com/google/deepvariant/issues/837:32,safety,error,error,32,Hi! I'm not sure I've seen this error before. It seems that the file is accessible given that it appears to be failing on parsing of the `DeepVariantDatasetConfig` proto. . Is is possible for you to share 1 of the files in the mounted bucket? That would allow us to try this on our end. Thanks!,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/837
https://github.com/google/deepvariant/issues/837:72,security,access,accessible,72,Hi! I'm not sure I've seen this error before. It seems that the file is accessible given that it appears to be failing on parsing of the `DeepVariantDatasetConfig` proto. . Is is possible for you to share 1 of the files in the mounted bucket? That would allow us to try this on our end. Thanks!,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/837
https://github.com/google/deepvariant/issues/837:32,usability,error,error,32,Hi! I'm not sure I've seen this error before. It seems that the file is accessible given that it appears to be failing on parsing of the `DeepVariantDatasetConfig` proto. . Is is possible for you to share 1 of the files in the mounted bucket? That would allow us to try this on our end. Thanks!,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/837
https://github.com/google/deepvariant/issues/837:54,safety,valid,validation,54,nevermind... solved there was a text line more in the validation pbtxt.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/837
https://github.com/google/deepvariant/issues/837:54,security,validat,validation,54,nevermind... solved there was a text line more in the validation pbtxt.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/837
https://github.com/google/deepvariant/issues/838:1060,interoperability,specif,specific,1060,"Hi @MKaandemir,. ```bash. chr8	118316369	.	CA	C,CAAAAAAAAAAA,CAAAAAAAAAAAA	34.6	PASS	.	GT:GQ:DP:AD:VAF:PL:PS	2|1:4:36:5,9,7,5:0.25,0.194444,0.138889:32,12,45,12,0,46,6,4,46,46:118261886. ```. In this case, the genotype is `2|1`. You can interpret this as:. ```bash. allele0=ref (CA). allele1=alt1 (CA->C). allele2=alt2 (CA->CAAAAAAAAAAA). allele3=alt3 (CA->CAAAAAAAAAAAA). ```. As the genotype is `2|1` you can interpret it as:. `alt2|alt1`. So the first haplotype sees:. ```bash. CA->CAAAAAAAAAAA. ```. And second haplotype sees:. ```bash. CA->C. ```. So in haplotype-1 you have 10bp insertion of As and 2nd one you have a deletion of 1bp A. You can represent this many ways. However, if you left shift, it would become:. ```bash. chr8 118316369 . CA C,CAAAAAAAAAAA. ```. Which is equivalent to what you had in the VCF. You can use `bcftools norm` or something else if you are trying to normalize the variant call. Anyway you represent that gives you the right underlying haplotype should be the right way to represent it unless you are looking for something specific.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/838
https://github.com/google/deepvariant/issues/838:155,reliability,doe,doesn,155,"Thanks for the explanation! I'm also curious about why there are four allelic depths. Is the first one the reference allele's depth? In biallelic SNPs, it doesn't show the reference allele. Also, why do you show the 2 allele's genotype but put the 3 allele in the alt column?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/838
https://github.com/google/deepvariant/issues/838:356,deployability,observ,observed,356,"Hi @MKaandemir from the header you can see the description of each field:. ```. ##FORMAT=<ID=GT,Number=1,Type=String,Description=""Genotype"">. ##FORMAT=<ID=GQ,Number=1,Type=Integer,Description=""Conditional genotype quality"">. ##FORMAT=<ID=DP,Number=1,Type=Integer,Description=""Read depth"">. ##FORMAT=<ID=MIN_DP,Number=1,Type=Integer,Description=""Minimum DP observed within the GVCF block."">. ##FORMAT=<ID=AD,Number=R,Type=Integer,Description=""Read depth for each allele"">. ##FORMAT=<ID=VAF,Number=A,Type=Float,Description=""Variant allele fractions."">. ##FORMAT=<ID=PL,Number=G,Type=Integer,Description=""Phred-scaled genotype likelihoods rounded to the closest integer"">. ##FORMAT=<ID=MED_DP,Number=1,Type=Integer,Description=""Median DP observed within the GVCF block rounded to the nearest integer."". ```. Yes, the first value for `AD` is for the reference allele.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/838
https://github.com/google/deepvariant/issues/838:608,deployability,scale,scaled,608,"Hi @MKaandemir from the header you can see the description of each field:. ```. ##FORMAT=<ID=GT,Number=1,Type=String,Description=""Genotype"">. ##FORMAT=<ID=GQ,Number=1,Type=Integer,Description=""Conditional genotype quality"">. ##FORMAT=<ID=DP,Number=1,Type=Integer,Description=""Read depth"">. ##FORMAT=<ID=MIN_DP,Number=1,Type=Integer,Description=""Minimum DP observed within the GVCF block."">. ##FORMAT=<ID=AD,Number=R,Type=Integer,Description=""Read depth for each allele"">. ##FORMAT=<ID=VAF,Number=A,Type=Float,Description=""Variant allele fractions."">. ##FORMAT=<ID=PL,Number=G,Type=Integer,Description=""Phred-scaled genotype likelihoods rounded to the closest integer"">. ##FORMAT=<ID=MED_DP,Number=1,Type=Integer,Description=""Median DP observed within the GVCF block rounded to the nearest integer."". ```. Yes, the first value for `AD` is for the reference allele.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/838
https://github.com/google/deepvariant/issues/838:735,deployability,observ,observed,735,"Hi @MKaandemir from the header you can see the description of each field:. ```. ##FORMAT=<ID=GT,Number=1,Type=String,Description=""Genotype"">. ##FORMAT=<ID=GQ,Number=1,Type=Integer,Description=""Conditional genotype quality"">. ##FORMAT=<ID=DP,Number=1,Type=Integer,Description=""Read depth"">. ##FORMAT=<ID=MIN_DP,Number=1,Type=Integer,Description=""Minimum DP observed within the GVCF block."">. ##FORMAT=<ID=AD,Number=R,Type=Integer,Description=""Read depth for each allele"">. ##FORMAT=<ID=VAF,Number=A,Type=Float,Description=""Variant allele fractions."">. ##FORMAT=<ID=PL,Number=G,Type=Integer,Description=""Phred-scaled genotype likelihoods rounded to the closest integer"">. ##FORMAT=<ID=MED_DP,Number=1,Type=Integer,Description=""Median DP observed within the GVCF block rounded to the nearest integer."". ```. Yes, the first value for `AD` is for the reference allele.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/838
https://github.com/google/deepvariant/issues/838:608,energy efficiency,scale,scaled,608,"Hi @MKaandemir from the header you can see the description of each field:. ```. ##FORMAT=<ID=GT,Number=1,Type=String,Description=""Genotype"">. ##FORMAT=<ID=GQ,Number=1,Type=Integer,Description=""Conditional genotype quality"">. ##FORMAT=<ID=DP,Number=1,Type=Integer,Description=""Read depth"">. ##FORMAT=<ID=MIN_DP,Number=1,Type=Integer,Description=""Minimum DP observed within the GVCF block."">. ##FORMAT=<ID=AD,Number=R,Type=Integer,Description=""Read depth for each allele"">. ##FORMAT=<ID=VAF,Number=A,Type=Float,Description=""Variant allele fractions."">. ##FORMAT=<ID=PL,Number=G,Type=Integer,Description=""Phred-scaled genotype likelihoods rounded to the closest integer"">. ##FORMAT=<ID=MED_DP,Number=1,Type=Integer,Description=""Median DP observed within the GVCF block rounded to the nearest integer."". ```. Yes, the first value for `AD` is for the reference allele.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/838
https://github.com/google/deepvariant/issues/838:82,interoperability,FORMAT,FORMAT,82,"Hi @MKaandemir from the header you can see the description of each field:. ```. ##FORMAT=<ID=GT,Number=1,Type=String,Description=""Genotype"">. ##FORMAT=<ID=GQ,Number=1,Type=Integer,Description=""Conditional genotype quality"">. ##FORMAT=<ID=DP,Number=1,Type=Integer,Description=""Read depth"">. ##FORMAT=<ID=MIN_DP,Number=1,Type=Integer,Description=""Minimum DP observed within the GVCF block."">. ##FORMAT=<ID=AD,Number=R,Type=Integer,Description=""Read depth for each allele"">. ##FORMAT=<ID=VAF,Number=A,Type=Float,Description=""Variant allele fractions."">. ##FORMAT=<ID=PL,Number=G,Type=Integer,Description=""Phred-scaled genotype likelihoods rounded to the closest integer"">. ##FORMAT=<ID=MED_DP,Number=1,Type=Integer,Description=""Median DP observed within the GVCF block rounded to the nearest integer."". ```. Yes, the first value for `AD` is for the reference allele.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/838
https://github.com/google/deepvariant/issues/838:144,interoperability,FORMAT,FORMAT,144,"Hi @MKaandemir from the header you can see the description of each field:. ```. ##FORMAT=<ID=GT,Number=1,Type=String,Description=""Genotype"">. ##FORMAT=<ID=GQ,Number=1,Type=Integer,Description=""Conditional genotype quality"">. ##FORMAT=<ID=DP,Number=1,Type=Integer,Description=""Read depth"">. ##FORMAT=<ID=MIN_DP,Number=1,Type=Integer,Description=""Minimum DP observed within the GVCF block."">. ##FORMAT=<ID=AD,Number=R,Type=Integer,Description=""Read depth for each allele"">. ##FORMAT=<ID=VAF,Number=A,Type=Float,Description=""Variant allele fractions."">. ##FORMAT=<ID=PL,Number=G,Type=Integer,Description=""Phred-scaled genotype likelihoods rounded to the closest integer"">. ##FORMAT=<ID=MED_DP,Number=1,Type=Integer,Description=""Median DP observed within the GVCF block rounded to the nearest integer."". ```. Yes, the first value for `AD` is for the reference allele.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/838
https://github.com/google/deepvariant/issues/838:227,interoperability,FORMAT,FORMAT,227,"Hi @MKaandemir from the header you can see the description of each field:. ```. ##FORMAT=<ID=GT,Number=1,Type=String,Description=""Genotype"">. ##FORMAT=<ID=GQ,Number=1,Type=Integer,Description=""Conditional genotype quality"">. ##FORMAT=<ID=DP,Number=1,Type=Integer,Description=""Read depth"">. ##FORMAT=<ID=MIN_DP,Number=1,Type=Integer,Description=""Minimum DP observed within the GVCF block."">. ##FORMAT=<ID=AD,Number=R,Type=Integer,Description=""Read depth for each allele"">. ##FORMAT=<ID=VAF,Number=A,Type=Float,Description=""Variant allele fractions."">. ##FORMAT=<ID=PL,Number=G,Type=Integer,Description=""Phred-scaled genotype likelihoods rounded to the closest integer"">. ##FORMAT=<ID=MED_DP,Number=1,Type=Integer,Description=""Median DP observed within the GVCF block rounded to the nearest integer."". ```. Yes, the first value for `AD` is for the reference allele.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/838
https://github.com/google/deepvariant/issues/838:292,interoperability,FORMAT,FORMAT,292,"Hi @MKaandemir from the header you can see the description of each field:. ```. ##FORMAT=<ID=GT,Number=1,Type=String,Description=""Genotype"">. ##FORMAT=<ID=GQ,Number=1,Type=Integer,Description=""Conditional genotype quality"">. ##FORMAT=<ID=DP,Number=1,Type=Integer,Description=""Read depth"">. ##FORMAT=<ID=MIN_DP,Number=1,Type=Integer,Description=""Minimum DP observed within the GVCF block."">. ##FORMAT=<ID=AD,Number=R,Type=Integer,Description=""Read depth for each allele"">. ##FORMAT=<ID=VAF,Number=A,Type=Float,Description=""Variant allele fractions."">. ##FORMAT=<ID=PL,Number=G,Type=Integer,Description=""Phred-scaled genotype likelihoods rounded to the closest integer"">. ##FORMAT=<ID=MED_DP,Number=1,Type=Integer,Description=""Median DP observed within the GVCF block rounded to the nearest integer."". ```. Yes, the first value for `AD` is for the reference allele.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/838
https://github.com/google/deepvariant/issues/838:393,interoperability,FORMAT,FORMAT,393,"Hi @MKaandemir from the header you can see the description of each field:. ```. ##FORMAT=<ID=GT,Number=1,Type=String,Description=""Genotype"">. ##FORMAT=<ID=GQ,Number=1,Type=Integer,Description=""Conditional genotype quality"">. ##FORMAT=<ID=DP,Number=1,Type=Integer,Description=""Read depth"">. ##FORMAT=<ID=MIN_DP,Number=1,Type=Integer,Description=""Minimum DP observed within the GVCF block."">. ##FORMAT=<ID=AD,Number=R,Type=Integer,Description=""Read depth for each allele"">. ##FORMAT=<ID=VAF,Number=A,Type=Float,Description=""Variant allele fractions."">. ##FORMAT=<ID=PL,Number=G,Type=Integer,Description=""Phred-scaled genotype likelihoods rounded to the closest integer"">. ##FORMAT=<ID=MED_DP,Number=1,Type=Integer,Description=""Median DP observed within the GVCF block rounded to the nearest integer."". ```. Yes, the first value for `AD` is for the reference allele.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/838
https://github.com/google/deepvariant/issues/838:474,interoperability,FORMAT,FORMAT,474,"Hi @MKaandemir from the header you can see the description of each field:. ```. ##FORMAT=<ID=GT,Number=1,Type=String,Description=""Genotype"">. ##FORMAT=<ID=GQ,Number=1,Type=Integer,Description=""Conditional genotype quality"">. ##FORMAT=<ID=DP,Number=1,Type=Integer,Description=""Read depth"">. ##FORMAT=<ID=MIN_DP,Number=1,Type=Integer,Description=""Minimum DP observed within the GVCF block."">. ##FORMAT=<ID=AD,Number=R,Type=Integer,Description=""Read depth for each allele"">. ##FORMAT=<ID=VAF,Number=A,Type=Float,Description=""Variant allele fractions."">. ##FORMAT=<ID=PL,Number=G,Type=Integer,Description=""Phred-scaled genotype likelihoods rounded to the closest integer"">. ##FORMAT=<ID=MED_DP,Number=1,Type=Integer,Description=""Median DP observed within the GVCF block rounded to the nearest integer."". ```. Yes, the first value for `AD` is for the reference allele.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/838
https://github.com/google/deepvariant/issues/838:553,interoperability,FORMAT,FORMAT,553,"Hi @MKaandemir from the header you can see the description of each field:. ```. ##FORMAT=<ID=GT,Number=1,Type=String,Description=""Genotype"">. ##FORMAT=<ID=GQ,Number=1,Type=Integer,Description=""Conditional genotype quality"">. ##FORMAT=<ID=DP,Number=1,Type=Integer,Description=""Read depth"">. ##FORMAT=<ID=MIN_DP,Number=1,Type=Integer,Description=""Minimum DP observed within the GVCF block."">. ##FORMAT=<ID=AD,Number=R,Type=Integer,Description=""Read depth for each allele"">. ##FORMAT=<ID=VAF,Number=A,Type=Float,Description=""Variant allele fractions."">. ##FORMAT=<ID=PL,Number=G,Type=Integer,Description=""Phred-scaled genotype likelihoods rounded to the closest integer"">. ##FORMAT=<ID=MED_DP,Number=1,Type=Integer,Description=""Median DP observed within the GVCF block rounded to the nearest integer."". ```. Yes, the first value for `AD` is for the reference allele.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/838
https://github.com/google/deepvariant/issues/838:672,interoperability,FORMAT,FORMAT,672,"Hi @MKaandemir from the header you can see the description of each field:. ```. ##FORMAT=<ID=GT,Number=1,Type=String,Description=""Genotype"">. ##FORMAT=<ID=GQ,Number=1,Type=Integer,Description=""Conditional genotype quality"">. ##FORMAT=<ID=DP,Number=1,Type=Integer,Description=""Read depth"">. ##FORMAT=<ID=MIN_DP,Number=1,Type=Integer,Description=""Minimum DP observed within the GVCF block."">. ##FORMAT=<ID=AD,Number=R,Type=Integer,Description=""Read depth for each allele"">. ##FORMAT=<ID=VAF,Number=A,Type=Float,Description=""Variant allele fractions."">. ##FORMAT=<ID=PL,Number=G,Type=Integer,Description=""Phred-scaled genotype likelihoods rounded to the closest integer"">. ##FORMAT=<ID=MED_DP,Number=1,Type=Integer,Description=""Median DP observed within the GVCF block rounded to the nearest integer."". ```. Yes, the first value for `AD` is for the reference allele.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/838
https://github.com/google/deepvariant/issues/838:608,modifiability,scal,scaled,608,"Hi @MKaandemir from the header you can see the description of each field:. ```. ##FORMAT=<ID=GT,Number=1,Type=String,Description=""Genotype"">. ##FORMAT=<ID=GQ,Number=1,Type=Integer,Description=""Conditional genotype quality"">. ##FORMAT=<ID=DP,Number=1,Type=Integer,Description=""Read depth"">. ##FORMAT=<ID=MIN_DP,Number=1,Type=Integer,Description=""Minimum DP observed within the GVCF block."">. ##FORMAT=<ID=AD,Number=R,Type=Integer,Description=""Read depth for each allele"">. ##FORMAT=<ID=VAF,Number=A,Type=Float,Description=""Variant allele fractions."">. ##FORMAT=<ID=PL,Number=G,Type=Integer,Description=""Phred-scaled genotype likelihoods rounded to the closest integer"">. ##FORMAT=<ID=MED_DP,Number=1,Type=Integer,Description=""Median DP observed within the GVCF block rounded to the nearest integer."". ```. Yes, the first value for `AD` is for the reference allele.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/838
https://github.com/google/deepvariant/issues/838:608,performance,scale,scaled,608,"Hi @MKaandemir from the header you can see the description of each field:. ```. ##FORMAT=<ID=GT,Number=1,Type=String,Description=""Genotype"">. ##FORMAT=<ID=GQ,Number=1,Type=Integer,Description=""Conditional genotype quality"">. ##FORMAT=<ID=DP,Number=1,Type=Integer,Description=""Read depth"">. ##FORMAT=<ID=MIN_DP,Number=1,Type=Integer,Description=""Minimum DP observed within the GVCF block."">. ##FORMAT=<ID=AD,Number=R,Type=Integer,Description=""Read depth for each allele"">. ##FORMAT=<ID=VAF,Number=A,Type=Float,Description=""Variant allele fractions."">. ##FORMAT=<ID=PL,Number=G,Type=Integer,Description=""Phred-scaled genotype likelihoods rounded to the closest integer"">. ##FORMAT=<ID=MED_DP,Number=1,Type=Integer,Description=""Median DP observed within the GVCF block rounded to the nearest integer."". ```. Yes, the first value for `AD` is for the reference allele.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/838
https://github.com/google/deepvariant/issues/838:356,testability,observ,observed,356,"Hi @MKaandemir from the header you can see the description of each field:. ```. ##FORMAT=<ID=GT,Number=1,Type=String,Description=""Genotype"">. ##FORMAT=<ID=GQ,Number=1,Type=Integer,Description=""Conditional genotype quality"">. ##FORMAT=<ID=DP,Number=1,Type=Integer,Description=""Read depth"">. ##FORMAT=<ID=MIN_DP,Number=1,Type=Integer,Description=""Minimum DP observed within the GVCF block."">. ##FORMAT=<ID=AD,Number=R,Type=Integer,Description=""Read depth for each allele"">. ##FORMAT=<ID=VAF,Number=A,Type=Float,Description=""Variant allele fractions."">. ##FORMAT=<ID=PL,Number=G,Type=Integer,Description=""Phred-scaled genotype likelihoods rounded to the closest integer"">. ##FORMAT=<ID=MED_DP,Number=1,Type=Integer,Description=""Median DP observed within the GVCF block rounded to the nearest integer."". ```. Yes, the first value for `AD` is for the reference allele.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/838
https://github.com/google/deepvariant/issues/838:735,testability,observ,observed,735,"Hi @MKaandemir from the header you can see the description of each field:. ```. ##FORMAT=<ID=GT,Number=1,Type=String,Description=""Genotype"">. ##FORMAT=<ID=GQ,Number=1,Type=Integer,Description=""Conditional genotype quality"">. ##FORMAT=<ID=DP,Number=1,Type=Integer,Description=""Read depth"">. ##FORMAT=<ID=MIN_DP,Number=1,Type=Integer,Description=""Minimum DP observed within the GVCF block."">. ##FORMAT=<ID=AD,Number=R,Type=Integer,Description=""Read depth for each allele"">. ##FORMAT=<ID=VAF,Number=A,Type=Float,Description=""Variant allele fractions."">. ##FORMAT=<ID=PL,Number=G,Type=Integer,Description=""Phred-scaled genotype likelihoods rounded to the closest integer"">. ##FORMAT=<ID=MED_DP,Number=1,Type=Integer,Description=""Median DP observed within the GVCF block rounded to the nearest integer."". ```. Yes, the first value for `AD` is for the reference allele.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/838
https://github.com/google/deepvariant/issues/838:345,usability,Minim,Minimum,345,"Hi @MKaandemir from the header you can see the description of each field:. ```. ##FORMAT=<ID=GT,Number=1,Type=String,Description=""Genotype"">. ##FORMAT=<ID=GQ,Number=1,Type=Integer,Description=""Conditional genotype quality"">. ##FORMAT=<ID=DP,Number=1,Type=Integer,Description=""Read depth"">. ##FORMAT=<ID=MIN_DP,Number=1,Type=Integer,Description=""Minimum DP observed within the GVCF block."">. ##FORMAT=<ID=AD,Number=R,Type=Integer,Description=""Read depth for each allele"">. ##FORMAT=<ID=VAF,Number=A,Type=Float,Description=""Variant allele fractions."">. ##FORMAT=<ID=PL,Number=G,Type=Integer,Description=""Phred-scaled genotype likelihoods rounded to the closest integer"">. ##FORMAT=<ID=MED_DP,Number=1,Type=Integer,Description=""Median DP observed within the GVCF block rounded to the nearest integer."". ```. Yes, the first value for `AD` is for the reference allele.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/838
https://github.com/google/deepvariant/issues/838:651,usability,close,closest,651,"Hi @MKaandemir from the header you can see the description of each field:. ```. ##FORMAT=<ID=GT,Number=1,Type=String,Description=""Genotype"">. ##FORMAT=<ID=GQ,Number=1,Type=Integer,Description=""Conditional genotype quality"">. ##FORMAT=<ID=DP,Number=1,Type=Integer,Description=""Read depth"">. ##FORMAT=<ID=MIN_DP,Number=1,Type=Integer,Description=""Minimum DP observed within the GVCF block."">. ##FORMAT=<ID=AD,Number=R,Type=Integer,Description=""Read depth for each allele"">. ##FORMAT=<ID=VAF,Number=A,Type=Float,Description=""Variant allele fractions."">. ##FORMAT=<ID=PL,Number=G,Type=Integer,Description=""Phred-scaled genotype likelihoods rounded to the closest integer"">. ##FORMAT=<ID=MED_DP,Number=1,Type=Integer,Description=""Median DP observed within the GVCF block rounded to the nearest integer."". ```. Yes, the first value for `AD` is for the reference allele.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/838
https://github.com/google/deepvariant/issues/838:128,reliability,doe,does,128,"In the example line, the depth is listed as 36. However, the allelic depths are 5, 9, 7, and 5. The sum of these allelic depths does not equal the value in the DP field. ```. chr8	118316369	.	CA	C,CAAAAAAAAAAA,CAAAAAAAAAAAA	34.6	PASS	.	GT:GQ:DP:AD:VAF:PL:PS	2|1:4:36:5,9,7,5:0.25,0.194444,0.138889:32,12,45,12,0,46,6,4,46,46:118261886. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/838
https://github.com/google/deepvariant/issues/838:75,energy efficiency,frequenc,frequency,75,@MKaandemir that means there were more alleles in this position with lower frequency that were dropped by the candidate generation scheme as they do not meet all the heuristics set for an allele to be a candidate. You can read the [DeepVariant manuscript](https://www.nature.com/articles/nbt.4235) to understand the process fully.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/838
https://github.com/google/deepvariant/issues/838:301,testability,understand,understand,301,@MKaandemir that means there were more alleles in this position with lower frequency that were dropped by the candidate generation scheme as they do not meet all the heuristics set for an allele to be a candidate. You can read the [DeepVariant manuscript](https://www.nature.com/articles/nbt.4235) to understand the process fully.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/838
https://github.com/google/deepvariant/issues/839:35,deployability,log,log,35,"@hangy1 ,. 1) You can see from the log:. ```. Reading ../results/deepVariant/KO_PV/<sample_name>/vcf/<sample_name>.deepVariant.vcf.gz. ```. seems like sample_name is not set correctly? Unless you replaced them? Can you confirm if you have set the values correctly by printing them before running DeepVariant? 2) Please use absolute paths rather than relative paths when you are setting paths. so instead of using:. ```bash. gvcf = ""../results/deepVariant/{dataset}/{sample}/vcf/{sample}.deepVariant.g.vcf.gz"". ```. Use:. ```bash. gvcf = ""/path/to/vcf/{sample}.deepVariant.g.vcf.gz"". ```. Hope this helps. You can also run the [quickstart](https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-quick-start.md) to see if you can simply copy-paste and run the command fully and then adapt it to the command you are planning to run.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/839
https://github.com/google/deepvariant/issues/839:793,energy efficiency,adapt,adapt,793,"@hangy1 ,. 1) You can see from the log:. ```. Reading ../results/deepVariant/KO_PV/<sample_name>/vcf/<sample_name>.deepVariant.vcf.gz. ```. seems like sample_name is not set correctly? Unless you replaced them? Can you confirm if you have set the values correctly by printing them before running DeepVariant? 2) Please use absolute paths rather than relative paths when you are setting paths. so instead of using:. ```bash. gvcf = ""../results/deepVariant/{dataset}/{sample}/vcf/{sample}.deepVariant.g.vcf.gz"". ```. Use:. ```bash. gvcf = ""/path/to/vcf/{sample}.deepVariant.g.vcf.gz"". ```. Hope this helps. You can also run the [quickstart](https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-quick-start.md) to see if you can simply copy-paste and run the command fully and then adapt it to the command you are planning to run.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/839
https://github.com/google/deepvariant/issues/839:793,integrability,adapt,adapt,793,"@hangy1 ,. 1) You can see from the log:. ```. Reading ../results/deepVariant/KO_PV/<sample_name>/vcf/<sample_name>.deepVariant.vcf.gz. ```. seems like sample_name is not set correctly? Unless you replaced them? Can you confirm if you have set the values correctly by printing them before running DeepVariant? 2) Please use absolute paths rather than relative paths when you are setting paths. so instead of using:. ```bash. gvcf = ""../results/deepVariant/{dataset}/{sample}/vcf/{sample}.deepVariant.g.vcf.gz"". ```. Use:. ```bash. gvcf = ""/path/to/vcf/{sample}.deepVariant.g.vcf.gz"". ```. Hope this helps. You can also run the [quickstart](https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-quick-start.md) to see if you can simply copy-paste and run the command fully and then adapt it to the command you are planning to run.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/839
https://github.com/google/deepvariant/issues/839:793,interoperability,adapt,adapt,793,"@hangy1 ,. 1) You can see from the log:. ```. Reading ../results/deepVariant/KO_PV/<sample_name>/vcf/<sample_name>.deepVariant.vcf.gz. ```. seems like sample_name is not set correctly? Unless you replaced them? Can you confirm if you have set the values correctly by printing them before running DeepVariant? 2) Please use absolute paths rather than relative paths when you are setting paths. so instead of using:. ```bash. gvcf = ""../results/deepVariant/{dataset}/{sample}/vcf/{sample}.deepVariant.g.vcf.gz"". ```. Use:. ```bash. gvcf = ""/path/to/vcf/{sample}.deepVariant.g.vcf.gz"". ```. Hope this helps. You can also run the [quickstart](https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-quick-start.md) to see if you can simply copy-paste and run the command fully and then adapt it to the command you are planning to run.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/839
https://github.com/google/deepvariant/issues/839:793,modifiability,adapt,adapt,793,"@hangy1 ,. 1) You can see from the log:. ```. Reading ../results/deepVariant/KO_PV/<sample_name>/vcf/<sample_name>.deepVariant.vcf.gz. ```. seems like sample_name is not set correctly? Unless you replaced them? Can you confirm if you have set the values correctly by printing them before running DeepVariant? 2) Please use absolute paths rather than relative paths when you are setting paths. so instead of using:. ```bash. gvcf = ""../results/deepVariant/{dataset}/{sample}/vcf/{sample}.deepVariant.g.vcf.gz"". ```. Use:. ```bash. gvcf = ""/path/to/vcf/{sample}.deepVariant.g.vcf.gz"". ```. Hope this helps. You can also run the [quickstart](https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-quick-start.md) to see if you can simply copy-paste and run the command fully and then adapt it to the command you are planning to run.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/839
https://github.com/google/deepvariant/issues/839:35,safety,log,log,35,"@hangy1 ,. 1) You can see from the log:. ```. Reading ../results/deepVariant/KO_PV/<sample_name>/vcf/<sample_name>.deepVariant.vcf.gz. ```. seems like sample_name is not set correctly? Unless you replaced them? Can you confirm if you have set the values correctly by printing them before running DeepVariant? 2) Please use absolute paths rather than relative paths when you are setting paths. so instead of using:. ```bash. gvcf = ""../results/deepVariant/{dataset}/{sample}/vcf/{sample}.deepVariant.g.vcf.gz"". ```. Use:. ```bash. gvcf = ""/path/to/vcf/{sample}.deepVariant.g.vcf.gz"". ```. Hope this helps. You can also run the [quickstart](https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-quick-start.md) to see if you can simply copy-paste and run the command fully and then adapt it to the command you are planning to run.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/839
https://github.com/google/deepvariant/issues/839:35,security,log,log,35,"@hangy1 ,. 1) You can see from the log:. ```. Reading ../results/deepVariant/KO_PV/<sample_name>/vcf/<sample_name>.deepVariant.vcf.gz. ```. seems like sample_name is not set correctly? Unless you replaced them? Can you confirm if you have set the values correctly by printing them before running DeepVariant? 2) Please use absolute paths rather than relative paths when you are setting paths. so instead of using:. ```bash. gvcf = ""../results/deepVariant/{dataset}/{sample}/vcf/{sample}.deepVariant.g.vcf.gz"". ```. Use:. ```bash. gvcf = ""/path/to/vcf/{sample}.deepVariant.g.vcf.gz"". ```. Hope this helps. You can also run the [quickstart](https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-quick-start.md) to see if you can simply copy-paste and run the command fully and then adapt it to the command you are planning to run.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/839
https://github.com/google/deepvariant/issues/839:35,testability,log,log,35,"@hangy1 ,. 1) You can see from the log:. ```. Reading ../results/deepVariant/KO_PV/<sample_name>/vcf/<sample_name>.deepVariant.vcf.gz. ```. seems like sample_name is not set correctly? Unless you replaced them? Can you confirm if you have set the values correctly by printing them before running DeepVariant? 2) Please use absolute paths rather than relative paths when you are setting paths. so instead of using:. ```bash. gvcf = ""../results/deepVariant/{dataset}/{sample}/vcf/{sample}.deepVariant.g.vcf.gz"". ```. Use:. ```bash. gvcf = ""/path/to/vcf/{sample}.deepVariant.g.vcf.gz"". ```. Hope this helps. You can also run the [quickstart](https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-quick-start.md) to see if you can simply copy-paste and run the command fully and then adapt it to the command you are planning to run.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/839
https://github.com/google/deepvariant/issues/839:740,testability,simpl,simply,740,"@hangy1 ,. 1) You can see from the log:. ```. Reading ../results/deepVariant/KO_PV/<sample_name>/vcf/<sample_name>.deepVariant.vcf.gz. ```. seems like sample_name is not set correctly? Unless you replaced them? Can you confirm if you have set the values correctly by printing them before running DeepVariant? 2) Please use absolute paths rather than relative paths when you are setting paths. so instead of using:. ```bash. gvcf = ""../results/deepVariant/{dataset}/{sample}/vcf/{sample}.deepVariant.g.vcf.gz"". ```. Use:. ```bash. gvcf = ""/path/to/vcf/{sample}.deepVariant.g.vcf.gz"". ```. Hope this helps. You can also run the [quickstart](https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-quick-start.md) to see if you can simply copy-paste and run the command fully and then adapt it to the command you are planning to run.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/839
https://github.com/google/deepvariant/issues/839:825,testability,plan,planning,825,"@hangy1 ,. 1) You can see from the log:. ```. Reading ../results/deepVariant/KO_PV/<sample_name>/vcf/<sample_name>.deepVariant.vcf.gz. ```. seems like sample_name is not set correctly? Unless you replaced them? Can you confirm if you have set the values correctly by printing them before running DeepVariant? 2) Please use absolute paths rather than relative paths when you are setting paths. so instead of using:. ```bash. gvcf = ""../results/deepVariant/{dataset}/{sample}/vcf/{sample}.deepVariant.g.vcf.gz"". ```. Use:. ```bash. gvcf = ""/path/to/vcf/{sample}.deepVariant.g.vcf.gz"". ```. Hope this helps. You can also run the [quickstart](https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-quick-start.md) to see if you can simply copy-paste and run the command fully and then adapt it to the command you are planning to run.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/839
https://github.com/google/deepvariant/issues/839:219,usability,confirm,confirm,219,"@hangy1 ,. 1) You can see from the log:. ```. Reading ../results/deepVariant/KO_PV/<sample_name>/vcf/<sample_name>.deepVariant.vcf.gz. ```. seems like sample_name is not set correctly? Unless you replaced them? Can you confirm if you have set the values correctly by printing them before running DeepVariant? 2) Please use absolute paths rather than relative paths when you are setting paths. so instead of using:. ```bash. gvcf = ""../results/deepVariant/{dataset}/{sample}/vcf/{sample}.deepVariant.g.vcf.gz"". ```. Use:. ```bash. gvcf = ""/path/to/vcf/{sample}.deepVariant.g.vcf.gz"". ```. Hope this helps. You can also run the [quickstart](https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-quick-start.md) to see if you can simply copy-paste and run the command fully and then adapt it to the command you are planning to run.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/839
https://github.com/google/deepvariant/issues/839:598,usability,help,helps,598,"@hangy1 ,. 1) You can see from the log:. ```. Reading ../results/deepVariant/KO_PV/<sample_name>/vcf/<sample_name>.deepVariant.vcf.gz. ```. seems like sample_name is not set correctly? Unless you replaced them? Can you confirm if you have set the values correctly by printing them before running DeepVariant? 2) Please use absolute paths rather than relative paths when you are setting paths. so instead of using:. ```bash. gvcf = ""../results/deepVariant/{dataset}/{sample}/vcf/{sample}.deepVariant.g.vcf.gz"". ```. Use:. ```bash. gvcf = ""/path/to/vcf/{sample}.deepVariant.g.vcf.gz"". ```. Hope this helps. You can also run the [quickstart](https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-quick-start.md) to see if you can simply copy-paste and run the command fully and then adapt it to the command you are planning to run.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/839
https://github.com/google/deepvariant/issues/839:740,usability,simpl,simply,740,"@hangy1 ,. 1) You can see from the log:. ```. Reading ../results/deepVariant/KO_PV/<sample_name>/vcf/<sample_name>.deepVariant.vcf.gz. ```. seems like sample_name is not set correctly? Unless you replaced them? Can you confirm if you have set the values correctly by printing them before running DeepVariant? 2) Please use absolute paths rather than relative paths when you are setting paths. so instead of using:. ```bash. gvcf = ""../results/deepVariant/{dataset}/{sample}/vcf/{sample}.deepVariant.g.vcf.gz"". ```. Use:. ```bash. gvcf = ""/path/to/vcf/{sample}.deepVariant.g.vcf.gz"". ```. Hope this helps. You can also run the [quickstart](https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-quick-start.md) to see if you can simply copy-paste and run the command fully and then adapt it to the command you are planning to run.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/839
https://github.com/google/deepvariant/issues/839:770,usability,command,command,770,"@hangy1 ,. 1) You can see from the log:. ```. Reading ../results/deepVariant/KO_PV/<sample_name>/vcf/<sample_name>.deepVariant.vcf.gz. ```. seems like sample_name is not set correctly? Unless you replaced them? Can you confirm if you have set the values correctly by printing them before running DeepVariant? 2) Please use absolute paths rather than relative paths when you are setting paths. so instead of using:. ```bash. gvcf = ""../results/deepVariant/{dataset}/{sample}/vcf/{sample}.deepVariant.g.vcf.gz"". ```. Use:. ```bash. gvcf = ""/path/to/vcf/{sample}.deepVariant.g.vcf.gz"". ```. Hope this helps. You can also run the [quickstart](https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-quick-start.md) to see if you can simply copy-paste and run the command fully and then adapt it to the command you are planning to run.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/839
https://github.com/google/deepvariant/issues/839:809,usability,command,command,809,"@hangy1 ,. 1) You can see from the log:. ```. Reading ../results/deepVariant/KO_PV/<sample_name>/vcf/<sample_name>.deepVariant.vcf.gz. ```. seems like sample_name is not set correctly? Unless you replaced them? Can you confirm if you have set the values correctly by printing them before running DeepVariant? 2) Please use absolute paths rather than relative paths when you are setting paths. so instead of using:. ```bash. gvcf = ""../results/deepVariant/{dataset}/{sample}/vcf/{sample}.deepVariant.g.vcf.gz"". ```. Use:. ```bash. gvcf = ""/path/to/vcf/{sample}.deepVariant.g.vcf.gz"". ```. Hope this helps. You can also run the [quickstart](https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-quick-start.md) to see if you can simply copy-paste and run the command fully and then adapt it to the command you are planning to run.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/839
https://github.com/google/deepvariant/issues/839:292,deployability,log,log,292,"Sorry about the the confusion of <sample_name>, I edited out the actual name. The name was appeared correctly and It was being generated at the right path too (files were deleted by snakemake due to incomplete workflow). . It seems like both VCF and gVCF were generated successfully from the log but if failed to run vcf_stats_report.py: . ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/project/pi_robertmills_umass_edu/databases/bacteroides_genome/<ref_genome>"" --infile ""../../results/deepVariant/KO_PV/<sample_name>/intermediate/call_variants_output.tfrecord.gz"" --outfile ""../results/deepVariant/KO_PV/<sample_name>/vcf/<sample_name>.deepVariant.vcf.gz"" --cpus ""6"" --gvcf_outfile ""../results/deepVariant/KO_PV/<sample_name>/vcf/<sample_name>.deepVariant.vcf.gz"" --nonvariant_site_tfrecord_path ""../../results/deepVariant/KO_PV/<sample_name>/intermediate/gvcf.tfrecord@6.gz"". I0626 19:01:26.925776 139684790912832 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: default. 2024-06-26 19:01:26.928061: I deepvariant/postprocess_variants.cc:94] Read from: ../../results/deepVariant/KO_PV/<sample_name>/intermediate/call_variants_output-00000-of-00001.tfrecord.gz. 2024-06-26 19:01:26.930065: I deepvariant/postprocess_variants.cc:109] Total #entries in single_site_calls = 407. I0626 19:01:26.930917 139684790912832 postprocess_variants.py:1313] CVO sorting took 6.503661473592123e-05 minutes. I0626 19:01:26.931080 139684790912832 postprocess_variants.py:1316] Transforming call_variants_output to variants. I0626 19:01:26.931126 139684790912832 postprocess_variants.py:1318] Using 6 CPUs for parallelization of variant transformation. I0626 19:01:26.954008 139684790912832 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: default. I0626 19:01:26.991115 139684790912832 postprocess_variants.py:1386] Processing variants (and writing to temporary file) took 0.00046567519505818686 ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/839
https://github.com/google/deepvariant/issues/839:303,deployability,fail,failed,303,"Sorry about the the confusion of <sample_name>, I edited out the actual name. The name was appeared correctly and It was being generated at the right path too (files were deleted by snakemake due to incomplete workflow). . It seems like both VCF and gVCF were generated successfully from the log but if failed to run vcf_stats_report.py: . ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/project/pi_robertmills_umass_edu/databases/bacteroides_genome/<ref_genome>"" --infile ""../../results/deepVariant/KO_PV/<sample_name>/intermediate/call_variants_output.tfrecord.gz"" --outfile ""../results/deepVariant/KO_PV/<sample_name>/vcf/<sample_name>.deepVariant.vcf.gz"" --cpus ""6"" --gvcf_outfile ""../results/deepVariant/KO_PV/<sample_name>/vcf/<sample_name>.deepVariant.vcf.gz"" --nonvariant_site_tfrecord_path ""../../results/deepVariant/KO_PV/<sample_name>/intermediate/gvcf.tfrecord@6.gz"". I0626 19:01:26.925776 139684790912832 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: default. 2024-06-26 19:01:26.928061: I deepvariant/postprocess_variants.cc:94] Read from: ../../results/deepVariant/KO_PV/<sample_name>/intermediate/call_variants_output-00000-of-00001.tfrecord.gz. 2024-06-26 19:01:26.930065: I deepvariant/postprocess_variants.cc:109] Total #entries in single_site_calls = 407. I0626 19:01:26.930917 139684790912832 postprocess_variants.py:1313] CVO sorting took 6.503661473592123e-05 minutes. I0626 19:01:26.931080 139684790912832 postprocess_variants.py:1316] Transforming call_variants_output to variants. I0626 19:01:26.931126 139684790912832 postprocess_variants.py:1318] Using 6 CPUs for parallelization of variant transformation. I0626 19:01:26.954008 139684790912832 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: default. I0626 19:01:26.991115 139684790912832 postprocess_variants.py:1386] Processing variants (and writing to temporary file) took 0.00046567519505818686 ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/839
https://github.com/google/deepvariant/issues/839:701,energy efficiency,cpu,cpus,701,"Sorry about the the confusion of <sample_name>, I edited out the actual name. The name was appeared correctly and It was being generated at the right path too (files were deleted by snakemake due to incomplete workflow). . It seems like both VCF and gVCF were generated successfully from the log but if failed to run vcf_stats_report.py: . ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/project/pi_robertmills_umass_edu/databases/bacteroides_genome/<ref_genome>"" --infile ""../../results/deepVariant/KO_PV/<sample_name>/intermediate/call_variants_output.tfrecord.gz"" --outfile ""../results/deepVariant/KO_PV/<sample_name>/vcf/<sample_name>.deepVariant.vcf.gz"" --cpus ""6"" --gvcf_outfile ""../results/deepVariant/KO_PV/<sample_name>/vcf/<sample_name>.deepVariant.vcf.gz"" --nonvariant_site_tfrecord_path ""../../results/deepVariant/KO_PV/<sample_name>/intermediate/gvcf.tfrecord@6.gz"". I0626 19:01:26.925776 139684790912832 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: default. 2024-06-26 19:01:26.928061: I deepvariant/postprocess_variants.cc:94] Read from: ../../results/deepVariant/KO_PV/<sample_name>/intermediate/call_variants_output-00000-of-00001.tfrecord.gz. 2024-06-26 19:01:26.930065: I deepvariant/postprocess_variants.cc:109] Total #entries in single_site_calls = 407. I0626 19:01:26.930917 139684790912832 postprocess_variants.py:1313] CVO sorting took 6.503661473592123e-05 minutes. I0626 19:01:26.931080 139684790912832 postprocess_variants.py:1316] Transforming call_variants_output to variants. I0626 19:01:26.931126 139684790912832 postprocess_variants.py:1318] Using 6 CPUs for parallelization of variant transformation. I0626 19:01:26.954008 139684790912832 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: default. I0626 19:01:26.991115 139684790912832 postprocess_variants.py:1386] Processing variants (and writing to temporary file) took 0.00046567519505818686 ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/839
https://github.com/google/deepvariant/issues/839:1665,energy efficiency,CPU,CPUs,1665,"snakemake due to incomplete workflow). . It seems like both VCF and gVCF were generated successfully from the log but if failed to run vcf_stats_report.py: . ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/project/pi_robertmills_umass_edu/databases/bacteroides_genome/<ref_genome>"" --infile ""../../results/deepVariant/KO_PV/<sample_name>/intermediate/call_variants_output.tfrecord.gz"" --outfile ""../results/deepVariant/KO_PV/<sample_name>/vcf/<sample_name>.deepVariant.vcf.gz"" --cpus ""6"" --gvcf_outfile ""../results/deepVariant/KO_PV/<sample_name>/vcf/<sample_name>.deepVariant.vcf.gz"" --nonvariant_site_tfrecord_path ""../../results/deepVariant/KO_PV/<sample_name>/intermediate/gvcf.tfrecord@6.gz"". I0626 19:01:26.925776 139684790912832 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: default. 2024-06-26 19:01:26.928061: I deepvariant/postprocess_variants.cc:94] Read from: ../../results/deepVariant/KO_PV/<sample_name>/intermediate/call_variants_output-00000-of-00001.tfrecord.gz. 2024-06-26 19:01:26.930065: I deepvariant/postprocess_variants.cc:109] Total #entries in single_site_calls = 407. I0626 19:01:26.930917 139684790912832 postprocess_variants.py:1313] CVO sorting took 6.503661473592123e-05 minutes. I0626 19:01:26.931080 139684790912832 postprocess_variants.py:1316] Transforming call_variants_output to variants. I0626 19:01:26.931126 139684790912832 postprocess_variants.py:1318] Using 6 CPUs for parallelization of variant transformation. I0626 19:01:26.954008 139684790912832 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: default. I0626 19:01:26.991115 139684790912832 postprocess_variants.py:1386] Processing variants (and writing to temporary file) took 0.00046567519505818686 minutes. I0626 19:01:27.391298 139684790912832 postprocess_variants.py:1407] Finished writing VCF and gVCF in 0.006664212544759115 minutes. real 0m4.417s. user 0m2.938s. sys 0m0.743s",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/839
https://github.com/google/deepvariant/issues/839:1542,integrability,Transform,Transforming,1542,"snakemake due to incomplete workflow). . It seems like both VCF and gVCF were generated successfully from the log but if failed to run vcf_stats_report.py: . ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/project/pi_robertmills_umass_edu/databases/bacteroides_genome/<ref_genome>"" --infile ""../../results/deepVariant/KO_PV/<sample_name>/intermediate/call_variants_output.tfrecord.gz"" --outfile ""../results/deepVariant/KO_PV/<sample_name>/vcf/<sample_name>.deepVariant.vcf.gz"" --cpus ""6"" --gvcf_outfile ""../results/deepVariant/KO_PV/<sample_name>/vcf/<sample_name>.deepVariant.vcf.gz"" --nonvariant_site_tfrecord_path ""../../results/deepVariant/KO_PV/<sample_name>/intermediate/gvcf.tfrecord@6.gz"". I0626 19:01:26.925776 139684790912832 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: default. 2024-06-26 19:01:26.928061: I deepvariant/postprocess_variants.cc:94] Read from: ../../results/deepVariant/KO_PV/<sample_name>/intermediate/call_variants_output-00000-of-00001.tfrecord.gz. 2024-06-26 19:01:26.930065: I deepvariant/postprocess_variants.cc:109] Total #entries in single_site_calls = 407. I0626 19:01:26.930917 139684790912832 postprocess_variants.py:1313] CVO sorting took 6.503661473592123e-05 minutes. I0626 19:01:26.931080 139684790912832 postprocess_variants.py:1316] Transforming call_variants_output to variants. I0626 19:01:26.931126 139684790912832 postprocess_variants.py:1318] Using 6 CPUs for parallelization of variant transformation. I0626 19:01:26.954008 139684790912832 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: default. I0626 19:01:26.991115 139684790912832 postprocess_variants.py:1386] Processing variants (and writing to temporary file) took 0.00046567519505818686 minutes. I0626 19:01:27.391298 139684790912832 postprocess_variants.py:1407] Finished writing VCF and gVCF in 0.006664212544759115 minutes. real 0m4.417s. user 0m2.938s. sys 0m0.743s",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/839
https://github.com/google/deepvariant/issues/839:1701,integrability,transform,transformation,1701,"snakemake due to incomplete workflow). . It seems like both VCF and gVCF were generated successfully from the log but if failed to run vcf_stats_report.py: . ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/project/pi_robertmills_umass_edu/databases/bacteroides_genome/<ref_genome>"" --infile ""../../results/deepVariant/KO_PV/<sample_name>/intermediate/call_variants_output.tfrecord.gz"" --outfile ""../results/deepVariant/KO_PV/<sample_name>/vcf/<sample_name>.deepVariant.vcf.gz"" --cpus ""6"" --gvcf_outfile ""../results/deepVariant/KO_PV/<sample_name>/vcf/<sample_name>.deepVariant.vcf.gz"" --nonvariant_site_tfrecord_path ""../../results/deepVariant/KO_PV/<sample_name>/intermediate/gvcf.tfrecord@6.gz"". I0626 19:01:26.925776 139684790912832 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: default. 2024-06-26 19:01:26.928061: I deepvariant/postprocess_variants.cc:94] Read from: ../../results/deepVariant/KO_PV/<sample_name>/intermediate/call_variants_output-00000-of-00001.tfrecord.gz. 2024-06-26 19:01:26.930065: I deepvariant/postprocess_variants.cc:109] Total #entries in single_site_calls = 407. I0626 19:01:26.930917 139684790912832 postprocess_variants.py:1313] CVO sorting took 6.503661473592123e-05 minutes. I0626 19:01:26.931080 139684790912832 postprocess_variants.py:1316] Transforming call_variants_output to variants. I0626 19:01:26.931126 139684790912832 postprocess_variants.py:1318] Using 6 CPUs for parallelization of variant transformation. I0626 19:01:26.954008 139684790912832 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: default. I0626 19:01:26.991115 139684790912832 postprocess_variants.py:1386] Processing variants (and writing to temporary file) took 0.00046567519505818686 minutes. I0626 19:01:27.391298 139684790912832 postprocess_variants.py:1407] Finished writing VCF and gVCF in 0.006664212544759115 minutes. real 0m4.417s. user 0m2.938s. sys 0m0.743s",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/839
https://github.com/google/deepvariant/issues/839:1542,interoperability,Transform,Transforming,1542,"snakemake due to incomplete workflow). . It seems like both VCF and gVCF were generated successfully from the log but if failed to run vcf_stats_report.py: . ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/project/pi_robertmills_umass_edu/databases/bacteroides_genome/<ref_genome>"" --infile ""../../results/deepVariant/KO_PV/<sample_name>/intermediate/call_variants_output.tfrecord.gz"" --outfile ""../results/deepVariant/KO_PV/<sample_name>/vcf/<sample_name>.deepVariant.vcf.gz"" --cpus ""6"" --gvcf_outfile ""../results/deepVariant/KO_PV/<sample_name>/vcf/<sample_name>.deepVariant.vcf.gz"" --nonvariant_site_tfrecord_path ""../../results/deepVariant/KO_PV/<sample_name>/intermediate/gvcf.tfrecord@6.gz"". I0626 19:01:26.925776 139684790912832 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: default. 2024-06-26 19:01:26.928061: I deepvariant/postprocess_variants.cc:94] Read from: ../../results/deepVariant/KO_PV/<sample_name>/intermediate/call_variants_output-00000-of-00001.tfrecord.gz. 2024-06-26 19:01:26.930065: I deepvariant/postprocess_variants.cc:109] Total #entries in single_site_calls = 407. I0626 19:01:26.930917 139684790912832 postprocess_variants.py:1313] CVO sorting took 6.503661473592123e-05 minutes. I0626 19:01:26.931080 139684790912832 postprocess_variants.py:1316] Transforming call_variants_output to variants. I0626 19:01:26.931126 139684790912832 postprocess_variants.py:1318] Using 6 CPUs for parallelization of variant transformation. I0626 19:01:26.954008 139684790912832 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: default. I0626 19:01:26.991115 139684790912832 postprocess_variants.py:1386] Processing variants (and writing to temporary file) took 0.00046567519505818686 minutes. I0626 19:01:27.391298 139684790912832 postprocess_variants.py:1407] Finished writing VCF and gVCF in 0.006664212544759115 minutes. real 0m4.417s. user 0m2.938s. sys 0m0.743s",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/839
https://github.com/google/deepvariant/issues/839:1701,interoperability,transform,transformation,1701,"snakemake due to incomplete workflow). . It seems like both VCF and gVCF were generated successfully from the log but if failed to run vcf_stats_report.py: . ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/project/pi_robertmills_umass_edu/databases/bacteroides_genome/<ref_genome>"" --infile ""../../results/deepVariant/KO_PV/<sample_name>/intermediate/call_variants_output.tfrecord.gz"" --outfile ""../results/deepVariant/KO_PV/<sample_name>/vcf/<sample_name>.deepVariant.vcf.gz"" --cpus ""6"" --gvcf_outfile ""../results/deepVariant/KO_PV/<sample_name>/vcf/<sample_name>.deepVariant.vcf.gz"" --nonvariant_site_tfrecord_path ""../../results/deepVariant/KO_PV/<sample_name>/intermediate/gvcf.tfrecord@6.gz"". I0626 19:01:26.925776 139684790912832 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: default. 2024-06-26 19:01:26.928061: I deepvariant/postprocess_variants.cc:94] Read from: ../../results/deepVariant/KO_PV/<sample_name>/intermediate/call_variants_output-00000-of-00001.tfrecord.gz. 2024-06-26 19:01:26.930065: I deepvariant/postprocess_variants.cc:109] Total #entries in single_site_calls = 407. I0626 19:01:26.930917 139684790912832 postprocess_variants.py:1313] CVO sorting took 6.503661473592123e-05 minutes. I0626 19:01:26.931080 139684790912832 postprocess_variants.py:1316] Transforming call_variants_output to variants. I0626 19:01:26.931126 139684790912832 postprocess_variants.py:1318] Using 6 CPUs for parallelization of variant transformation. I0626 19:01:26.954008 139684790912832 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: default. I0626 19:01:26.991115 139684790912832 postprocess_variants.py:1386] Processing variants (and writing to temporary file) took 0.00046567519505818686 minutes. I0626 19:01:27.391298 139684790912832 postprocess_variants.py:1407] Finished writing VCF and gVCF in 0.006664212544759115 minutes. real 0m4.417s. user 0m2.938s. sys 0m0.743s",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/839
https://github.com/google/deepvariant/issues/839:560,modifiability,interm,intermediate,560,"Sorry about the the confusion of <sample_name>, I edited out the actual name. The name was appeared correctly and It was being generated at the right path too (files were deleted by snakemake due to incomplete workflow). . It seems like both VCF and gVCF were generated successfully from the log but if failed to run vcf_stats_report.py: . ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/project/pi_robertmills_umass_edu/databases/bacteroides_genome/<ref_genome>"" --infile ""../../results/deepVariant/KO_PV/<sample_name>/intermediate/call_variants_output.tfrecord.gz"" --outfile ""../results/deepVariant/KO_PV/<sample_name>/vcf/<sample_name>.deepVariant.vcf.gz"" --cpus ""6"" --gvcf_outfile ""../results/deepVariant/KO_PV/<sample_name>/vcf/<sample_name>.deepVariant.vcf.gz"" --nonvariant_site_tfrecord_path ""../../results/deepVariant/KO_PV/<sample_name>/intermediate/gvcf.tfrecord@6.gz"". I0626 19:01:26.925776 139684790912832 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: default. 2024-06-26 19:01:26.928061: I deepvariant/postprocess_variants.cc:94] Read from: ../../results/deepVariant/KO_PV/<sample_name>/intermediate/call_variants_output-00000-of-00001.tfrecord.gz. 2024-06-26 19:01:26.930065: I deepvariant/postprocess_variants.cc:109] Total #entries in single_site_calls = 407. I0626 19:01:26.930917 139684790912832 postprocess_variants.py:1313] CVO sorting took 6.503661473592123e-05 minutes. I0626 19:01:26.931080 139684790912832 postprocess_variants.py:1316] Transforming call_variants_output to variants. I0626 19:01:26.931126 139684790912832 postprocess_variants.py:1318] Using 6 CPUs for parallelization of variant transformation. I0626 19:01:26.954008 139684790912832 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: default. I0626 19:01:26.991115 139684790912832 postprocess_variants.py:1386] Processing variants (and writing to temporary file) took 0.00046567519505818686 ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/839
https://github.com/google/deepvariant/issues/839:886,modifiability,interm,intermediate,886,"Sorry about the the confusion of <sample_name>, I edited out the actual name. The name was appeared correctly and It was being generated at the right path too (files were deleted by snakemake due to incomplete workflow). . It seems like both VCF and gVCF were generated successfully from the log but if failed to run vcf_stats_report.py: . ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/project/pi_robertmills_umass_edu/databases/bacteroides_genome/<ref_genome>"" --infile ""../../results/deepVariant/KO_PV/<sample_name>/intermediate/call_variants_output.tfrecord.gz"" --outfile ""../results/deepVariant/KO_PV/<sample_name>/vcf/<sample_name>.deepVariant.vcf.gz"" --cpus ""6"" --gvcf_outfile ""../results/deepVariant/KO_PV/<sample_name>/vcf/<sample_name>.deepVariant.vcf.gz"" --nonvariant_site_tfrecord_path ""../../results/deepVariant/KO_PV/<sample_name>/intermediate/gvcf.tfrecord@6.gz"". I0626 19:01:26.925776 139684790912832 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: default. 2024-06-26 19:01:26.928061: I deepvariant/postprocess_variants.cc:94] Read from: ../../results/deepVariant/KO_PV/<sample_name>/intermediate/call_variants_output-00000-of-00001.tfrecord.gz. 2024-06-26 19:01:26.930065: I deepvariant/postprocess_variants.cc:109] Total #entries in single_site_calls = 407. I0626 19:01:26.930917 139684790912832 postprocess_variants.py:1313] CVO sorting took 6.503661473592123e-05 minutes. I0626 19:01:26.931080 139684790912832 postprocess_variants.py:1316] Transforming call_variants_output to variants. I0626 19:01:26.931126 139684790912832 postprocess_variants.py:1318] Using 6 CPUs for parallelization of variant transformation. I0626 19:01:26.954008 139684790912832 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: default. I0626 19:01:26.991115 139684790912832 postprocess_variants.py:1386] Processing variants (and writing to temporary file) took 0.00046567519505818686 ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/839
https://github.com/google/deepvariant/issues/839:1182,modifiability,interm,intermediate,1182,"snakemake due to incomplete workflow). . It seems like both VCF and gVCF were generated successfully from the log but if failed to run vcf_stats_report.py: . ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/project/pi_robertmills_umass_edu/databases/bacteroides_genome/<ref_genome>"" --infile ""../../results/deepVariant/KO_PV/<sample_name>/intermediate/call_variants_output.tfrecord.gz"" --outfile ""../results/deepVariant/KO_PV/<sample_name>/vcf/<sample_name>.deepVariant.vcf.gz"" --cpus ""6"" --gvcf_outfile ""../results/deepVariant/KO_PV/<sample_name>/vcf/<sample_name>.deepVariant.vcf.gz"" --nonvariant_site_tfrecord_path ""../../results/deepVariant/KO_PV/<sample_name>/intermediate/gvcf.tfrecord@6.gz"". I0626 19:01:26.925776 139684790912832 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: default. 2024-06-26 19:01:26.928061: I deepvariant/postprocess_variants.cc:94] Read from: ../../results/deepVariant/KO_PV/<sample_name>/intermediate/call_variants_output-00000-of-00001.tfrecord.gz. 2024-06-26 19:01:26.930065: I deepvariant/postprocess_variants.cc:109] Total #entries in single_site_calls = 407. I0626 19:01:26.930917 139684790912832 postprocess_variants.py:1313] CVO sorting took 6.503661473592123e-05 minutes. I0626 19:01:26.931080 139684790912832 postprocess_variants.py:1316] Transforming call_variants_output to variants. I0626 19:01:26.931126 139684790912832 postprocess_variants.py:1318] Using 6 CPUs for parallelization of variant transformation. I0626 19:01:26.954008 139684790912832 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: default. I0626 19:01:26.991115 139684790912832 postprocess_variants.py:1386] Processing variants (and writing to temporary file) took 0.00046567519505818686 minutes. I0626 19:01:27.391298 139684790912832 postprocess_variants.py:1407] Finished writing VCF and gVCF in 0.006664212544759115 minutes. real 0m4.417s. user 0m2.938s. sys 0m0.743s",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/839
https://github.com/google/deepvariant/issues/839:373,performance,time,time,373,"Sorry about the the confusion of <sample_name>, I edited out the actual name. The name was appeared correctly and It was being generated at the right path too (files were deleted by snakemake due to incomplete workflow). . It seems like both VCF and gVCF were generated successfully from the log but if failed to run vcf_stats_report.py: . ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/project/pi_robertmills_umass_edu/databases/bacteroides_genome/<ref_genome>"" --infile ""../../results/deepVariant/KO_PV/<sample_name>/intermediate/call_variants_output.tfrecord.gz"" --outfile ""../results/deepVariant/KO_PV/<sample_name>/vcf/<sample_name>.deepVariant.vcf.gz"" --cpus ""6"" --gvcf_outfile ""../results/deepVariant/KO_PV/<sample_name>/vcf/<sample_name>.deepVariant.vcf.gz"" --nonvariant_site_tfrecord_path ""../../results/deepVariant/KO_PV/<sample_name>/intermediate/gvcf.tfrecord@6.gz"". I0626 19:01:26.925776 139684790912832 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: default. 2024-06-26 19:01:26.928061: I deepvariant/postprocess_variants.cc:94] Read from: ../../results/deepVariant/KO_PV/<sample_name>/intermediate/call_variants_output-00000-of-00001.tfrecord.gz. 2024-06-26 19:01:26.930065: I deepvariant/postprocess_variants.cc:109] Total #entries in single_site_calls = 407. I0626 19:01:26.930917 139684790912832 postprocess_variants.py:1313] CVO sorting took 6.503661473592123e-05 minutes. I0626 19:01:26.931080 139684790912832 postprocess_variants.py:1316] Transforming call_variants_output to variants. I0626 19:01:26.931126 139684790912832 postprocess_variants.py:1318] Using 6 CPUs for parallelization of variant transformation. I0626 19:01:26.954008 139684790912832 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: default. I0626 19:01:26.991115 139684790912832 postprocess_variants.py:1386] Processing variants (and writing to temporary file) took 0.00046567519505818686 ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/839
https://github.com/google/deepvariant/issues/839:701,performance,cpu,cpus,701,"Sorry about the the confusion of <sample_name>, I edited out the actual name. The name was appeared correctly and It was being generated at the right path too (files were deleted by snakemake due to incomplete workflow). . It seems like both VCF and gVCF were generated successfully from the log but if failed to run vcf_stats_report.py: . ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/project/pi_robertmills_umass_edu/databases/bacteroides_genome/<ref_genome>"" --infile ""../../results/deepVariant/KO_PV/<sample_name>/intermediate/call_variants_output.tfrecord.gz"" --outfile ""../results/deepVariant/KO_PV/<sample_name>/vcf/<sample_name>.deepVariant.vcf.gz"" --cpus ""6"" --gvcf_outfile ""../results/deepVariant/KO_PV/<sample_name>/vcf/<sample_name>.deepVariant.vcf.gz"" --nonvariant_site_tfrecord_path ""../../results/deepVariant/KO_PV/<sample_name>/intermediate/gvcf.tfrecord@6.gz"". I0626 19:01:26.925776 139684790912832 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: default. 2024-06-26 19:01:26.928061: I deepvariant/postprocess_variants.cc:94] Read from: ../../results/deepVariant/KO_PV/<sample_name>/intermediate/call_variants_output-00000-of-00001.tfrecord.gz. 2024-06-26 19:01:26.930065: I deepvariant/postprocess_variants.cc:109] Total #entries in single_site_calls = 407. I0626 19:01:26.930917 139684790912832 postprocess_variants.py:1313] CVO sorting took 6.503661473592123e-05 minutes. I0626 19:01:26.931080 139684790912832 postprocess_variants.py:1316] Transforming call_variants_output to variants. I0626 19:01:26.931126 139684790912832 postprocess_variants.py:1318] Using 6 CPUs for parallelization of variant transformation. I0626 19:01:26.954008 139684790912832 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: default. I0626 19:01:26.991115 139684790912832 postprocess_variants.py:1386] Processing variants (and writing to temporary file) took 0.00046567519505818686 ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/839
https://github.com/google/deepvariant/issues/839:1665,performance,CPU,CPUs,1665,"snakemake due to incomplete workflow). . It seems like both VCF and gVCF were generated successfully from the log but if failed to run vcf_stats_report.py: . ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/project/pi_robertmills_umass_edu/databases/bacteroides_genome/<ref_genome>"" --infile ""../../results/deepVariant/KO_PV/<sample_name>/intermediate/call_variants_output.tfrecord.gz"" --outfile ""../results/deepVariant/KO_PV/<sample_name>/vcf/<sample_name>.deepVariant.vcf.gz"" --cpus ""6"" --gvcf_outfile ""../results/deepVariant/KO_PV/<sample_name>/vcf/<sample_name>.deepVariant.vcf.gz"" --nonvariant_site_tfrecord_path ""../../results/deepVariant/KO_PV/<sample_name>/intermediate/gvcf.tfrecord@6.gz"". I0626 19:01:26.925776 139684790912832 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: default. 2024-06-26 19:01:26.928061: I deepvariant/postprocess_variants.cc:94] Read from: ../../results/deepVariant/KO_PV/<sample_name>/intermediate/call_variants_output-00000-of-00001.tfrecord.gz. 2024-06-26 19:01:26.930065: I deepvariant/postprocess_variants.cc:109] Total #entries in single_site_calls = 407. I0626 19:01:26.930917 139684790912832 postprocess_variants.py:1313] CVO sorting took 6.503661473592123e-05 minutes. I0626 19:01:26.931080 139684790912832 postprocess_variants.py:1316] Transforming call_variants_output to variants. I0626 19:01:26.931126 139684790912832 postprocess_variants.py:1318] Using 6 CPUs for parallelization of variant transformation. I0626 19:01:26.954008 139684790912832 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: default. I0626 19:01:26.991115 139684790912832 postprocess_variants.py:1386] Processing variants (and writing to temporary file) took 0.00046567519505818686 minutes. I0626 19:01:27.391298 139684790912832 postprocess_variants.py:1407] Finished writing VCF and gVCF in 0.006664212544759115 minutes. real 0m4.417s. user 0m2.938s. sys 0m0.743s",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/839
https://github.com/google/deepvariant/issues/839:1674,performance,parallel,parallelization,1674,"snakemake due to incomplete workflow). . It seems like both VCF and gVCF were generated successfully from the log but if failed to run vcf_stats_report.py: . ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/project/pi_robertmills_umass_edu/databases/bacteroides_genome/<ref_genome>"" --infile ""../../results/deepVariant/KO_PV/<sample_name>/intermediate/call_variants_output.tfrecord.gz"" --outfile ""../results/deepVariant/KO_PV/<sample_name>/vcf/<sample_name>.deepVariant.vcf.gz"" --cpus ""6"" --gvcf_outfile ""../results/deepVariant/KO_PV/<sample_name>/vcf/<sample_name>.deepVariant.vcf.gz"" --nonvariant_site_tfrecord_path ""../../results/deepVariant/KO_PV/<sample_name>/intermediate/gvcf.tfrecord@6.gz"". I0626 19:01:26.925776 139684790912832 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: default. 2024-06-26 19:01:26.928061: I deepvariant/postprocess_variants.cc:94] Read from: ../../results/deepVariant/KO_PV/<sample_name>/intermediate/call_variants_output-00000-of-00001.tfrecord.gz. 2024-06-26 19:01:26.930065: I deepvariant/postprocess_variants.cc:109] Total #entries in single_site_calls = 407. I0626 19:01:26.930917 139684790912832 postprocess_variants.py:1313] CVO sorting took 6.503661473592123e-05 minutes. I0626 19:01:26.931080 139684790912832 postprocess_variants.py:1316] Transforming call_variants_output to variants. I0626 19:01:26.931126 139684790912832 postprocess_variants.py:1318] Using 6 CPUs for parallelization of variant transformation. I0626 19:01:26.954008 139684790912832 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: default. I0626 19:01:26.991115 139684790912832 postprocess_variants.py:1386] Processing variants (and writing to temporary file) took 0.00046567519505818686 minutes. I0626 19:01:27.391298 139684790912832 postprocess_variants.py:1407] Finished writing VCF and gVCF in 0.006664212544759115 minutes. real 0m4.417s. user 0m2.938s. sys 0m0.743s",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/839
https://github.com/google/deepvariant/issues/839:303,reliability,fail,failed,303,"Sorry about the the confusion of <sample_name>, I edited out the actual name. The name was appeared correctly and It was being generated at the right path too (files were deleted by snakemake due to incomplete workflow). . It seems like both VCF and gVCF were generated successfully from the log but if failed to run vcf_stats_report.py: . ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/project/pi_robertmills_umass_edu/databases/bacteroides_genome/<ref_genome>"" --infile ""../../results/deepVariant/KO_PV/<sample_name>/intermediate/call_variants_output.tfrecord.gz"" --outfile ""../results/deepVariant/KO_PV/<sample_name>/vcf/<sample_name>.deepVariant.vcf.gz"" --cpus ""6"" --gvcf_outfile ""../results/deepVariant/KO_PV/<sample_name>/vcf/<sample_name>.deepVariant.vcf.gz"" --nonvariant_site_tfrecord_path ""../../results/deepVariant/KO_PV/<sample_name>/intermediate/gvcf.tfrecord@6.gz"". I0626 19:01:26.925776 139684790912832 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: default. 2024-06-26 19:01:26.928061: I deepvariant/postprocess_variants.cc:94] Read from: ../../results/deepVariant/KO_PV/<sample_name>/intermediate/call_variants_output-00000-of-00001.tfrecord.gz. 2024-06-26 19:01:26.930065: I deepvariant/postprocess_variants.cc:109] Total #entries in single_site_calls = 407. I0626 19:01:26.930917 139684790912832 postprocess_variants.py:1313] CVO sorting took 6.503661473592123e-05 minutes. I0626 19:01:26.931080 139684790912832 postprocess_variants.py:1316] Transforming call_variants_output to variants. I0626 19:01:26.931126 139684790912832 postprocess_variants.py:1318] Using 6 CPUs for parallelization of variant transformation. I0626 19:01:26.954008 139684790912832 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: default. I0626 19:01:26.991115 139684790912832 postprocess_variants.py:1386] Processing variants (and writing to temporary file) took 0.00046567519505818686 ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/839
https://github.com/google/deepvariant/issues/839:292,safety,log,log,292,"Sorry about the the confusion of <sample_name>, I edited out the actual name. The name was appeared correctly and It was being generated at the right path too (files were deleted by snakemake due to incomplete workflow). . It seems like both VCF and gVCF were generated successfully from the log but if failed to run vcf_stats_report.py: . ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/project/pi_robertmills_umass_edu/databases/bacteroides_genome/<ref_genome>"" --infile ""../../results/deepVariant/KO_PV/<sample_name>/intermediate/call_variants_output.tfrecord.gz"" --outfile ""../results/deepVariant/KO_PV/<sample_name>/vcf/<sample_name>.deepVariant.vcf.gz"" --cpus ""6"" --gvcf_outfile ""../results/deepVariant/KO_PV/<sample_name>/vcf/<sample_name>.deepVariant.vcf.gz"" --nonvariant_site_tfrecord_path ""../../results/deepVariant/KO_PV/<sample_name>/intermediate/gvcf.tfrecord@6.gz"". I0626 19:01:26.925776 139684790912832 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: default. 2024-06-26 19:01:26.928061: I deepvariant/postprocess_variants.cc:94] Read from: ../../results/deepVariant/KO_PV/<sample_name>/intermediate/call_variants_output-00000-of-00001.tfrecord.gz. 2024-06-26 19:01:26.930065: I deepvariant/postprocess_variants.cc:109] Total #entries in single_site_calls = 407. I0626 19:01:26.930917 139684790912832 postprocess_variants.py:1313] CVO sorting took 6.503661473592123e-05 minutes. I0626 19:01:26.931080 139684790912832 postprocess_variants.py:1316] Transforming call_variants_output to variants. I0626 19:01:26.931126 139684790912832 postprocess_variants.py:1318] Using 6 CPUs for parallelization of variant transformation. I0626 19:01:26.954008 139684790912832 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: default. I0626 19:01:26.991115 139684790912832 postprocess_variants.py:1386] Processing variants (and writing to temporary file) took 0.00046567519505818686 ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/839
https://github.com/google/deepvariant/issues/839:292,security,log,log,292,"Sorry about the the confusion of <sample_name>, I edited out the actual name. The name was appeared correctly and It was being generated at the right path too (files were deleted by snakemake due to incomplete workflow). . It seems like both VCF and gVCF were generated successfully from the log but if failed to run vcf_stats_report.py: . ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/project/pi_robertmills_umass_edu/databases/bacteroides_genome/<ref_genome>"" --infile ""../../results/deepVariant/KO_PV/<sample_name>/intermediate/call_variants_output.tfrecord.gz"" --outfile ""../results/deepVariant/KO_PV/<sample_name>/vcf/<sample_name>.deepVariant.vcf.gz"" --cpus ""6"" --gvcf_outfile ""../results/deepVariant/KO_PV/<sample_name>/vcf/<sample_name>.deepVariant.vcf.gz"" --nonvariant_site_tfrecord_path ""../../results/deepVariant/KO_PV/<sample_name>/intermediate/gvcf.tfrecord@6.gz"". I0626 19:01:26.925776 139684790912832 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: default. 2024-06-26 19:01:26.928061: I deepvariant/postprocess_variants.cc:94] Read from: ../../results/deepVariant/KO_PV/<sample_name>/intermediate/call_variants_output-00000-of-00001.tfrecord.gz. 2024-06-26 19:01:26.930065: I deepvariant/postprocess_variants.cc:109] Total #entries in single_site_calls = 407. I0626 19:01:26.930917 139684790912832 postprocess_variants.py:1313] CVO sorting took 6.503661473592123e-05 minutes. I0626 19:01:26.931080 139684790912832 postprocess_variants.py:1316] Transforming call_variants_output to variants. I0626 19:01:26.931126 139684790912832 postprocess_variants.py:1318] Using 6 CPUs for parallelization of variant transformation. I0626 19:01:26.954008 139684790912832 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: default. I0626 19:01:26.991115 139684790912832 postprocess_variants.py:1386] Processing variants (and writing to temporary file) took 0.00046567519505818686 ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/839
https://github.com/google/deepvariant/issues/839:292,testability,log,log,292,"Sorry about the the confusion of <sample_name>, I edited out the actual name. The name was appeared correctly and It was being generated at the right path too (files were deleted by snakemake due to incomplete workflow). . It seems like both VCF and gVCF were generated successfully from the log but if failed to run vcf_stats_report.py: . ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/project/pi_robertmills_umass_edu/databases/bacteroides_genome/<ref_genome>"" --infile ""../../results/deepVariant/KO_PV/<sample_name>/intermediate/call_variants_output.tfrecord.gz"" --outfile ""../results/deepVariant/KO_PV/<sample_name>/vcf/<sample_name>.deepVariant.vcf.gz"" --cpus ""6"" --gvcf_outfile ""../results/deepVariant/KO_PV/<sample_name>/vcf/<sample_name>.deepVariant.vcf.gz"" --nonvariant_site_tfrecord_path ""../../results/deepVariant/KO_PV/<sample_name>/intermediate/gvcf.tfrecord@6.gz"". I0626 19:01:26.925776 139684790912832 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: default. 2024-06-26 19:01:26.928061: I deepvariant/postprocess_variants.cc:94] Read from: ../../results/deepVariant/KO_PV/<sample_name>/intermediate/call_variants_output-00000-of-00001.tfrecord.gz. 2024-06-26 19:01:26.930065: I deepvariant/postprocess_variants.cc:109] Total #entries in single_site_calls = 407. I0626 19:01:26.930917 139684790912832 postprocess_variants.py:1313] CVO sorting took 6.503661473592123e-05 minutes. I0626 19:01:26.931080 139684790912832 postprocess_variants.py:1316] Transforming call_variants_output to variants. I0626 19:01:26.931126 139684790912832 postprocess_variants.py:1318] Using 6 CPUs for parallelization of variant transformation. I0626 19:01:26.954008 139684790912832 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: default. I0626 19:01:26.991115 139684790912832 postprocess_variants.py:1386] Processing variants (and writing to temporary file) took 0.00046567519505818686 ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/839
https://github.com/google/deepvariant/issues/839:210,usability,workflow,workflow,210,"Sorry about the the confusion of <sample_name>, I edited out the actual name. The name was appeared correctly and It was being generated at the right path too (files were deleted by snakemake due to incomplete workflow). . It seems like both VCF and gVCF were generated successfully from the log but if failed to run vcf_stats_report.py: . ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/project/pi_robertmills_umass_edu/databases/bacteroides_genome/<ref_genome>"" --infile ""../../results/deepVariant/KO_PV/<sample_name>/intermediate/call_variants_output.tfrecord.gz"" --outfile ""../results/deepVariant/KO_PV/<sample_name>/vcf/<sample_name>.deepVariant.vcf.gz"" --cpus ""6"" --gvcf_outfile ""../results/deepVariant/KO_PV/<sample_name>/vcf/<sample_name>.deepVariant.vcf.gz"" --nonvariant_site_tfrecord_path ""../../results/deepVariant/KO_PV/<sample_name>/intermediate/gvcf.tfrecord@6.gz"". I0626 19:01:26.925776 139684790912832 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: default. 2024-06-26 19:01:26.928061: I deepvariant/postprocess_variants.cc:94] Read from: ../../results/deepVariant/KO_PV/<sample_name>/intermediate/call_variants_output-00000-of-00001.tfrecord.gz. 2024-06-26 19:01:26.930065: I deepvariant/postprocess_variants.cc:109] Total #entries in single_site_calls = 407. I0626 19:01:26.930917 139684790912832 postprocess_variants.py:1313] CVO sorting took 6.503661473592123e-05 minutes. I0626 19:01:26.931080 139684790912832 postprocess_variants.py:1316] Transforming call_variants_output to variants. I0626 19:01:26.931126 139684790912832 postprocess_variants.py:1318] Using 6 CPUs for parallelization of variant transformation. I0626 19:01:26.954008 139684790912832 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: default. I0626 19:01:26.991115 139684790912832 postprocess_variants.py:1386] Processing variants (and writing to temporary file) took 0.00046567519505818686 ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/839
https://github.com/google/deepvariant/issues/839:358,usability,command,command,358,"Sorry about the the confusion of <sample_name>, I edited out the actual name. The name was appeared correctly and It was being generated at the right path too (files were deleted by snakemake due to incomplete workflow). . It seems like both VCF and gVCF were generated successfully from the log but if failed to run vcf_stats_report.py: . ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/project/pi_robertmills_umass_edu/databases/bacteroides_genome/<ref_genome>"" --infile ""../../results/deepVariant/KO_PV/<sample_name>/intermediate/call_variants_output.tfrecord.gz"" --outfile ""../results/deepVariant/KO_PV/<sample_name>/vcf/<sample_name>.deepVariant.vcf.gz"" --cpus ""6"" --gvcf_outfile ""../results/deepVariant/KO_PV/<sample_name>/vcf/<sample_name>.deepVariant.vcf.gz"" --nonvariant_site_tfrecord_path ""../../results/deepVariant/KO_PV/<sample_name>/intermediate/gvcf.tfrecord@6.gz"". I0626 19:01:26.925776 139684790912832 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: default. 2024-06-26 19:01:26.928061: I deepvariant/postprocess_variants.cc:94] Read from: ../../results/deepVariant/KO_PV/<sample_name>/intermediate/call_variants_output-00000-of-00001.tfrecord.gz. 2024-06-26 19:01:26.930065: I deepvariant/postprocess_variants.cc:109] Total #entries in single_site_calls = 407. I0626 19:01:26.930917 139684790912832 postprocess_variants.py:1313] CVO sorting took 6.503661473592123e-05 minutes. I0626 19:01:26.931080 139684790912832 postprocess_variants.py:1316] Transforming call_variants_output to variants. I0626 19:01:26.931126 139684790912832 postprocess_variants.py:1318] Using 6 CPUs for parallelization of variant transformation. I0626 19:01:26.954008 139684790912832 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: default. I0626 19:01:26.991115 139684790912832 postprocess_variants.py:1386] Processing variants (and writing to temporary file) took 0.00046567519505818686 ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/839
https://github.com/google/deepvariant/issues/839:2155,usability,user,user,2155,"snakemake due to incomplete workflow). . It seems like both VCF and gVCF were generated successfully from the log but if failed to run vcf_stats_report.py: . ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/project/pi_robertmills_umass_edu/databases/bacteroides_genome/<ref_genome>"" --infile ""../../results/deepVariant/KO_PV/<sample_name>/intermediate/call_variants_output.tfrecord.gz"" --outfile ""../results/deepVariant/KO_PV/<sample_name>/vcf/<sample_name>.deepVariant.vcf.gz"" --cpus ""6"" --gvcf_outfile ""../results/deepVariant/KO_PV/<sample_name>/vcf/<sample_name>.deepVariant.vcf.gz"" --nonvariant_site_tfrecord_path ""../../results/deepVariant/KO_PV/<sample_name>/intermediate/gvcf.tfrecord@6.gz"". I0626 19:01:26.925776 139684790912832 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: default. 2024-06-26 19:01:26.928061: I deepvariant/postprocess_variants.cc:94] Read from: ../../results/deepVariant/KO_PV/<sample_name>/intermediate/call_variants_output-00000-of-00001.tfrecord.gz. 2024-06-26 19:01:26.930065: I deepvariant/postprocess_variants.cc:109] Total #entries in single_site_calls = 407. I0626 19:01:26.930917 139684790912832 postprocess_variants.py:1313] CVO sorting took 6.503661473592123e-05 minutes. I0626 19:01:26.931080 139684790912832 postprocess_variants.py:1316] Transforming call_variants_output to variants. I0626 19:01:26.931126 139684790912832 postprocess_variants.py:1318] Using 6 CPUs for parallelization of variant transformation. I0626 19:01:26.954008 139684790912832 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: default. I0626 19:01:26.991115 139684790912832 postprocess_variants.py:1386] Processing variants (and writing to temporary file) took 0.00046567519505818686 minutes. I0626 19:01:27.391298 139684790912832 postprocess_variants.py:1407] Finished writing VCF and gVCF in 0.006664212544759115 minutes. real 0m4.417s. user 0m2.938s. sys 0m0.743s",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/839
https://github.com/google/deepvariant/issues/839:191,usability,help,help,191,"only **<sample_name>.deepVariant.vcf.gz.tbi** (besides example and call_variant files) was generated, no vcf and gvcf at the output dir. I also changed the path to absolute dir and it didn't help",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/839
https://github.com/google/deepvariant/issues/839:223,security,control,controlled,223,"@hangy1 , can you please run the [quickstart](https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-quick-start.md) by simply copy-pasting the commands in your system? That way we could pinpoint the issue in a controlled case.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/839
https://github.com/google/deepvariant/issues/839:132,testability,simpl,simply,132,"@hangy1 , can you please run the [quickstart](https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-quick-start.md) by simply copy-pasting the commands in your system? That way we could pinpoint the issue in a controlled case.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/839
https://github.com/google/deepvariant/issues/839:223,testability,control,controlled,223,"@hangy1 , can you please run the [quickstart](https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-quick-start.md) by simply copy-pasting the commands in your system? That way we could pinpoint the issue in a controlled case.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/839
https://github.com/google/deepvariant/issues/839:132,usability,simpl,simply,132,"@hangy1 , can you please run the [quickstart](https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-quick-start.md) by simply copy-pasting the commands in your system? That way we could pinpoint the issue in a controlled case.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/839
https://github.com/google/deepvariant/issues/839:156,usability,command,commands,156,"@hangy1 , can you please run the [quickstart](https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-quick-start.md) by simply copy-pasting the commands in your system? That way we could pinpoint the issue in a controlled case.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/839
https://github.com/google/deepvariant/issues/839:71,usability,help,help,71,I am closing this due to inactivity. Please reopen if you need further help.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/839
https://github.com/google/deepvariant/issues/839:106,deployability,fail,fails,106,"Hello,. I have the exact same issue. In my case, I have my VCFs, gVCFs and the index files created but it fails at creating the ""vcf_stats_repot"". I have given the absolute paths as well. . When I ran the test data, I had the same issue. Tried runnig the VCF run report separate as documented here: https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-vcf-stats-report.md and that also didn't work. Any assistance would be greatly appreciated. Thank you so much.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/839
https://github.com/google/deepvariant/issues/839:106,reliability,fail,fails,106,"Hello,. I have the exact same issue. In my case, I have my VCFs, gVCFs and the index files created but it fails at creating the ""vcf_stats_repot"". I have given the absolute paths as well. . When I ran the test data, I had the same issue. Tried runnig the VCF run report separate as documented here: https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-vcf-stats-report.md and that also didn't work. Any assistance would be greatly appreciated. Thank you so much.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/839
https://github.com/google/deepvariant/issues/839:205,safety,test,test,205,"Hello,. I have the exact same issue. In my case, I have my VCFs, gVCFs and the index files created but it fails at creating the ""vcf_stats_repot"". I have given the absolute paths as well. . When I ran the test data, I had the same issue. Tried runnig the VCF run report separate as documented here: https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-vcf-stats-report.md and that also didn't work. Any assistance would be greatly appreciated. Thank you so much.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/839
https://github.com/google/deepvariant/issues/839:205,testability,test,test,205,"Hello,. I have the exact same issue. In my case, I have my VCFs, gVCFs and the index files created but it fails at creating the ""vcf_stats_repot"". I have given the absolute paths as well. . When I ran the test data, I had the same issue. Tried runnig the VCF run report separate as documented here: https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-vcf-stats-report.md and that also didn't work. Any assistance would be greatly appreciated. Thank you so much.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/839
https://github.com/google/deepvariant/issues/839:282,usability,document,documented,282,"Hello,. I have the exact same issue. In my case, I have my VCFs, gVCFs and the index files created but it fails at creating the ""vcf_stats_repot"". I have given the absolute paths as well. . When I ran the test data, I had the same issue. Tried runnig the VCF run report separate as documented here: https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-vcf-stats-report.md and that also didn't work. Any assistance would be greatly appreciated. Thank you so much.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/839
https://github.com/google/deepvariant/issues/839:45,availability,state,statement,45,"Hello @ayeshbond! Could you please add a log statement with the exact error that you're seeing? . Additionally, you can always disable the creation of the `vcf_stats_repot` in case it's blocking you from running DeepVariant.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/839
https://github.com/google/deepvariant/issues/839:70,availability,error,error,70,"Hello @ayeshbond! Could you please add a log statement with the exact error that you're seeing? . Additionally, you can always disable the creation of the `vcf_stats_repot` in case it's blocking you from running DeepVariant.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/839
https://github.com/google/deepvariant/issues/839:41,deployability,log,log,41,"Hello @ayeshbond! Could you please add a log statement with the exact error that you're seeing? . Additionally, you can always disable the creation of the `vcf_stats_repot` in case it's blocking you from running DeepVariant.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/839
https://github.com/google/deepvariant/issues/839:45,integrability,state,statement,45,"Hello @ayeshbond! Could you please add a log statement with the exact error that you're seeing? . Additionally, you can always disable the creation of the `vcf_stats_repot` in case it's blocking you from running DeepVariant.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/839
https://github.com/google/deepvariant/issues/839:70,performance,error,error,70,"Hello @ayeshbond! Could you please add a log statement with the exact error that you're seeing? . Additionally, you can always disable the creation of the `vcf_stats_repot` in case it's blocking you from running DeepVariant.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/839
https://github.com/google/deepvariant/issues/839:41,safety,log,log,41,"Hello @ayeshbond! Could you please add a log statement with the exact error that you're seeing? . Additionally, you can always disable the creation of the `vcf_stats_repot` in case it's blocking you from running DeepVariant.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/839
https://github.com/google/deepvariant/issues/839:70,safety,error,error,70,"Hello @ayeshbond! Could you please add a log statement with the exact error that you're seeing? . Additionally, you can always disable the creation of the `vcf_stats_repot` in case it's blocking you from running DeepVariant.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/839
https://github.com/google/deepvariant/issues/839:41,security,log,log,41,"Hello @ayeshbond! Could you please add a log statement with the exact error that you're seeing? . Additionally, you can always disable the creation of the `vcf_stats_repot` in case it's blocking you from running DeepVariant.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/839
https://github.com/google/deepvariant/issues/839:41,testability,log,log,41,"Hello @ayeshbond! Could you please add a log statement with the exact error that you're seeing? . Additionally, you can always disable the creation of the `vcf_stats_repot` in case it's blocking you from running DeepVariant.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/839
https://github.com/google/deepvariant/issues/839:70,usability,error,error,70,"Hello @ayeshbond! Could you please add a log statement with the exact error that you're seeing? . Additionally, you can always disable the creation of the `vcf_stats_repot` in case it's blocking you from running DeepVariant.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/839
https://github.com/google/deepvariant/issues/839:955,availability,error,error,955,"Hello @lucasbrambrink,. Thank you very much for the response. I am attaching the log file herewith. THe command I used was: . singularity run -B /usr/lib/locale/:/usr/lib/locale/ deepvariant_1.6.1.sif /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=""{rest_path}/tools/DeepVariant/ref_genomes/GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta"" --reads=""{rest_path}/MiniMap_SAM_BAM/11741-KA-0004.sorted.bam"" --output_vcf=""{rest_path}/deepvar_calls/11741-KA-0004_output.vcf.gz"" --output_gvcf=""{rest_path}/deepvar_calls/11741-KA-0004_output.g.vcf.gz"" --intermediate_results_dir ""{rest_path}/deepvar_calls/intermediate_results_dir/0004"" --num_shards=32 &> deepvar_0004.log. [deepvar_0004.log](https://github.com/user-attachments/files/16730424/deepvar_0004.log). Also, could you let me know how I can disable it the `vcf_stat_report`?` I tried to look for it but to no luck. It doesn't necessarily affect the variant calling through. Just gives an error/failure due to this last step. Thank you very much once again, and please let me know if I can get you any more information.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/839
https://github.com/google/deepvariant/issues/839:961,availability,failur,failure,961,"Hello @lucasbrambrink,. Thank you very much for the response. I am attaching the log file herewith. THe command I used was: . singularity run -B /usr/lib/locale/:/usr/lib/locale/ deepvariant_1.6.1.sif /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=""{rest_path}/tools/DeepVariant/ref_genomes/GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta"" --reads=""{rest_path}/MiniMap_SAM_BAM/11741-KA-0004.sorted.bam"" --output_vcf=""{rest_path}/deepvar_calls/11741-KA-0004_output.vcf.gz"" --output_gvcf=""{rest_path}/deepvar_calls/11741-KA-0004_output.g.vcf.gz"" --intermediate_results_dir ""{rest_path}/deepvar_calls/intermediate_results_dir/0004"" --num_shards=32 &> deepvar_0004.log. [deepvar_0004.log](https://github.com/user-attachments/files/16730424/deepvar_0004.log). Also, could you let me know how I can disable it the `vcf_stat_report`?` I tried to look for it but to no luck. It doesn't necessarily affect the variant calling through. Just gives an error/failure due to this last step. Thank you very much once again, and please let me know if I can get you any more information.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/839
https://github.com/google/deepvariant/issues/839:81,deployability,log,log,81,"Hello @lucasbrambrink,. Thank you very much for the response. I am attaching the log file herewith. THe command I used was: . singularity run -B /usr/lib/locale/:/usr/lib/locale/ deepvariant_1.6.1.sif /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=""{rest_path}/tools/DeepVariant/ref_genomes/GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta"" --reads=""{rest_path}/MiniMap_SAM_BAM/11741-KA-0004.sorted.bam"" --output_vcf=""{rest_path}/deepvar_calls/11741-KA-0004_output.vcf.gz"" --output_gvcf=""{rest_path}/deepvar_calls/11741-KA-0004_output.g.vcf.gz"" --intermediate_results_dir ""{rest_path}/deepvar_calls/intermediate_results_dir/0004"" --num_shards=32 &> deepvar_0004.log. [deepvar_0004.log](https://github.com/user-attachments/files/16730424/deepvar_0004.log). Also, could you let me know how I can disable it the `vcf_stat_report`?` I tried to look for it but to no luck. It doesn't necessarily affect the variant calling through. Just gives an error/failure due to this last step. Thank you very much once again, and please let me know if I can get you any more information.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/839
https://github.com/google/deepvariant/issues/839:676,deployability,log,log,676,"Hello @lucasbrambrink,. Thank you very much for the response. I am attaching the log file herewith. THe command I used was: . singularity run -B /usr/lib/locale/:/usr/lib/locale/ deepvariant_1.6.1.sif /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=""{rest_path}/tools/DeepVariant/ref_genomes/GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta"" --reads=""{rest_path}/MiniMap_SAM_BAM/11741-KA-0004.sorted.bam"" --output_vcf=""{rest_path}/deepvar_calls/11741-KA-0004_output.vcf.gz"" --output_gvcf=""{rest_path}/deepvar_calls/11741-KA-0004_output.g.vcf.gz"" --intermediate_results_dir ""{rest_path}/deepvar_calls/intermediate_results_dir/0004"" --num_shards=32 &> deepvar_0004.log. [deepvar_0004.log](https://github.com/user-attachments/files/16730424/deepvar_0004.log). Also, could you let me know how I can disable it the `vcf_stat_report`?` I tried to look for it but to no luck. It doesn't necessarily affect the variant calling through. Just gives an error/failure due to this last step. Thank you very much once again, and please let me know if I can get you any more information.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/839
https://github.com/google/deepvariant/issues/839:695,deployability,log,log,695,"Hello @lucasbrambrink,. Thank you very much for the response. I am attaching the log file herewith. THe command I used was: . singularity run -B /usr/lib/locale/:/usr/lib/locale/ deepvariant_1.6.1.sif /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=""{rest_path}/tools/DeepVariant/ref_genomes/GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta"" --reads=""{rest_path}/MiniMap_SAM_BAM/11741-KA-0004.sorted.bam"" --output_vcf=""{rest_path}/deepvar_calls/11741-KA-0004_output.vcf.gz"" --output_gvcf=""{rest_path}/deepvar_calls/11741-KA-0004_output.g.vcf.gz"" --intermediate_results_dir ""{rest_path}/deepvar_calls/intermediate_results_dir/0004"" --num_shards=32 &> deepvar_0004.log. [deepvar_0004.log](https://github.com/user-attachments/files/16730424/deepvar_0004.log). Also, could you let me know how I can disable it the `vcf_stat_report`?` I tried to look for it but to no luck. It doesn't necessarily affect the variant calling through. Just gives an error/failure due to this last step. Thank you very much once again, and please let me know if I can get you any more information.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/839
https://github.com/google/deepvariant/issues/839:764,deployability,log,log,764,"Hello @lucasbrambrink,. Thank you very much for the response. I am attaching the log file herewith. THe command I used was: . singularity run -B /usr/lib/locale/:/usr/lib/locale/ deepvariant_1.6.1.sif /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=""{rest_path}/tools/DeepVariant/ref_genomes/GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta"" --reads=""{rest_path}/MiniMap_SAM_BAM/11741-KA-0004.sorted.bam"" --output_vcf=""{rest_path}/deepvar_calls/11741-KA-0004_output.vcf.gz"" --output_gvcf=""{rest_path}/deepvar_calls/11741-KA-0004_output.g.vcf.gz"" --intermediate_results_dir ""{rest_path}/deepvar_calls/intermediate_results_dir/0004"" --num_shards=32 &> deepvar_0004.log. [deepvar_0004.log](https://github.com/user-attachments/files/16730424/deepvar_0004.log). Also, could you let me know how I can disable it the `vcf_stat_report`?` I tried to look for it but to no luck. It doesn't necessarily affect the variant calling through. Just gives an error/failure due to this last step. Thank you very much once again, and please let me know if I can get you any more information.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/839
https://github.com/google/deepvariant/issues/839:961,deployability,fail,failure,961,"Hello @lucasbrambrink,. Thank you very much for the response. I am attaching the log file herewith. THe command I used was: . singularity run -B /usr/lib/locale/:/usr/lib/locale/ deepvariant_1.6.1.sif /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=""{rest_path}/tools/DeepVariant/ref_genomes/GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta"" --reads=""{rest_path}/MiniMap_SAM_BAM/11741-KA-0004.sorted.bam"" --output_vcf=""{rest_path}/deepvar_calls/11741-KA-0004_output.vcf.gz"" --output_gvcf=""{rest_path}/deepvar_calls/11741-KA-0004_output.g.vcf.gz"" --intermediate_results_dir ""{rest_path}/deepvar_calls/intermediate_results_dir/0004"" --num_shards=32 &> deepvar_0004.log. [deepvar_0004.log](https://github.com/user-attachments/files/16730424/deepvar_0004.log). Also, could you let me know how I can disable it the `vcf_stat_report`?` I tried to look for it but to no luck. It doesn't necessarily affect the variant calling through. Just gives an error/failure due to this last step. Thank you very much once again, and please let me know if I can get you any more information.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/839
https://github.com/google/deepvariant/issues/839:955,performance,error,error,955,"Hello @lucasbrambrink,. Thank you very much for the response. I am attaching the log file herewith. THe command I used was: . singularity run -B /usr/lib/locale/:/usr/lib/locale/ deepvariant_1.6.1.sif /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=""{rest_path}/tools/DeepVariant/ref_genomes/GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta"" --reads=""{rest_path}/MiniMap_SAM_BAM/11741-KA-0004.sorted.bam"" --output_vcf=""{rest_path}/deepvar_calls/11741-KA-0004_output.vcf.gz"" --output_gvcf=""{rest_path}/deepvar_calls/11741-KA-0004_output.g.vcf.gz"" --intermediate_results_dir ""{rest_path}/deepvar_calls/intermediate_results_dir/0004"" --num_shards=32 &> deepvar_0004.log. [deepvar_0004.log](https://github.com/user-attachments/files/16730424/deepvar_0004.log). Also, could you let me know how I can disable it the `vcf_stat_report`?` I tried to look for it but to no luck. It doesn't necessarily affect the variant calling through. Just gives an error/failure due to this last step. Thank you very much once again, and please let me know if I can get you any more information.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/839
https://github.com/google/deepvariant/issues/839:961,performance,failur,failure,961,"Hello @lucasbrambrink,. Thank you very much for the response. I am attaching the log file herewith. THe command I used was: . singularity run -B /usr/lib/locale/:/usr/lib/locale/ deepvariant_1.6.1.sif /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=""{rest_path}/tools/DeepVariant/ref_genomes/GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta"" --reads=""{rest_path}/MiniMap_SAM_BAM/11741-KA-0004.sorted.bam"" --output_vcf=""{rest_path}/deepvar_calls/11741-KA-0004_output.vcf.gz"" --output_gvcf=""{rest_path}/deepvar_calls/11741-KA-0004_output.g.vcf.gz"" --intermediate_results_dir ""{rest_path}/deepvar_calls/intermediate_results_dir/0004"" --num_shards=32 &> deepvar_0004.log. [deepvar_0004.log](https://github.com/user-attachments/files/16730424/deepvar_0004.log). Also, could you let me know how I can disable it the `vcf_stat_report`?` I tried to look for it but to no luck. It doesn't necessarily affect the variant calling through. Just gives an error/failure due to this last step. Thank you very much once again, and please let me know if I can get you any more information.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/839
https://github.com/google/deepvariant/issues/839:885,reliability,doe,doesn,885,"Hello @lucasbrambrink,. Thank you very much for the response. I am attaching the log file herewith. THe command I used was: . singularity run -B /usr/lib/locale/:/usr/lib/locale/ deepvariant_1.6.1.sif /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=""{rest_path}/tools/DeepVariant/ref_genomes/GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta"" --reads=""{rest_path}/MiniMap_SAM_BAM/11741-KA-0004.sorted.bam"" --output_vcf=""{rest_path}/deepvar_calls/11741-KA-0004_output.vcf.gz"" --output_gvcf=""{rest_path}/deepvar_calls/11741-KA-0004_output.g.vcf.gz"" --intermediate_results_dir ""{rest_path}/deepvar_calls/intermediate_results_dir/0004"" --num_shards=32 &> deepvar_0004.log. [deepvar_0004.log](https://github.com/user-attachments/files/16730424/deepvar_0004.log). Also, could you let me know how I can disable it the `vcf_stat_report`?` I tried to look for it but to no luck. It doesn't necessarily affect the variant calling through. Just gives an error/failure due to this last step. Thank you very much once again, and please let me know if I can get you any more information.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/839
https://github.com/google/deepvariant/issues/839:961,reliability,fail,failure,961,"Hello @lucasbrambrink,. Thank you very much for the response. I am attaching the log file herewith. THe command I used was: . singularity run -B /usr/lib/locale/:/usr/lib/locale/ deepvariant_1.6.1.sif /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=""{rest_path}/tools/DeepVariant/ref_genomes/GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta"" --reads=""{rest_path}/MiniMap_SAM_BAM/11741-KA-0004.sorted.bam"" --output_vcf=""{rest_path}/deepvar_calls/11741-KA-0004_output.vcf.gz"" --output_gvcf=""{rest_path}/deepvar_calls/11741-KA-0004_output.g.vcf.gz"" --intermediate_results_dir ""{rest_path}/deepvar_calls/intermediate_results_dir/0004"" --num_shards=32 &> deepvar_0004.log. [deepvar_0004.log](https://github.com/user-attachments/files/16730424/deepvar_0004.log). Also, could you let me know how I can disable it the `vcf_stat_report`?` I tried to look for it but to no luck. It doesn't necessarily affect the variant calling through. Just gives an error/failure due to this last step. Thank you very much once again, and please let me know if I can get you any more information.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/839
https://github.com/google/deepvariant/issues/839:81,safety,log,log,81,"Hello @lucasbrambrink,. Thank you very much for the response. I am attaching the log file herewith. THe command I used was: . singularity run -B /usr/lib/locale/:/usr/lib/locale/ deepvariant_1.6.1.sif /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=""{rest_path}/tools/DeepVariant/ref_genomes/GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta"" --reads=""{rest_path}/MiniMap_SAM_BAM/11741-KA-0004.sorted.bam"" --output_vcf=""{rest_path}/deepvar_calls/11741-KA-0004_output.vcf.gz"" --output_gvcf=""{rest_path}/deepvar_calls/11741-KA-0004_output.g.vcf.gz"" --intermediate_results_dir ""{rest_path}/deepvar_calls/intermediate_results_dir/0004"" --num_shards=32 &> deepvar_0004.log. [deepvar_0004.log](https://github.com/user-attachments/files/16730424/deepvar_0004.log). Also, could you let me know how I can disable it the `vcf_stat_report`?` I tried to look for it but to no luck. It doesn't necessarily affect the variant calling through. Just gives an error/failure due to this last step. Thank you very much once again, and please let me know if I can get you any more information.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/839
https://github.com/google/deepvariant/issues/839:676,safety,log,log,676,"Hello @lucasbrambrink,. Thank you very much for the response. I am attaching the log file herewith. THe command I used was: . singularity run -B /usr/lib/locale/:/usr/lib/locale/ deepvariant_1.6.1.sif /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=""{rest_path}/tools/DeepVariant/ref_genomes/GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta"" --reads=""{rest_path}/MiniMap_SAM_BAM/11741-KA-0004.sorted.bam"" --output_vcf=""{rest_path}/deepvar_calls/11741-KA-0004_output.vcf.gz"" --output_gvcf=""{rest_path}/deepvar_calls/11741-KA-0004_output.g.vcf.gz"" --intermediate_results_dir ""{rest_path}/deepvar_calls/intermediate_results_dir/0004"" --num_shards=32 &> deepvar_0004.log. [deepvar_0004.log](https://github.com/user-attachments/files/16730424/deepvar_0004.log). Also, could you let me know how I can disable it the `vcf_stat_report`?` I tried to look for it but to no luck. It doesn't necessarily affect the variant calling through. Just gives an error/failure due to this last step. Thank you very much once again, and please let me know if I can get you any more information.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/839
https://github.com/google/deepvariant/issues/839:695,safety,log,log,695,"Hello @lucasbrambrink,. Thank you very much for the response. I am attaching the log file herewith. THe command I used was: . singularity run -B /usr/lib/locale/:/usr/lib/locale/ deepvariant_1.6.1.sif /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=""{rest_path}/tools/DeepVariant/ref_genomes/GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta"" --reads=""{rest_path}/MiniMap_SAM_BAM/11741-KA-0004.sorted.bam"" --output_vcf=""{rest_path}/deepvar_calls/11741-KA-0004_output.vcf.gz"" --output_gvcf=""{rest_path}/deepvar_calls/11741-KA-0004_output.g.vcf.gz"" --intermediate_results_dir ""{rest_path}/deepvar_calls/intermediate_results_dir/0004"" --num_shards=32 &> deepvar_0004.log. [deepvar_0004.log](https://github.com/user-attachments/files/16730424/deepvar_0004.log). Also, could you let me know how I can disable it the `vcf_stat_report`?` I tried to look for it but to no luck. It doesn't necessarily affect the variant calling through. Just gives an error/failure due to this last step. Thank you very much once again, and please let me know if I can get you any more information.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/839
https://github.com/google/deepvariant/issues/839:764,safety,log,log,764,"Hello @lucasbrambrink,. Thank you very much for the response. I am attaching the log file herewith. THe command I used was: . singularity run -B /usr/lib/locale/:/usr/lib/locale/ deepvariant_1.6.1.sif /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=""{rest_path}/tools/DeepVariant/ref_genomes/GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta"" --reads=""{rest_path}/MiniMap_SAM_BAM/11741-KA-0004.sorted.bam"" --output_vcf=""{rest_path}/deepvar_calls/11741-KA-0004_output.vcf.gz"" --output_gvcf=""{rest_path}/deepvar_calls/11741-KA-0004_output.g.vcf.gz"" --intermediate_results_dir ""{rest_path}/deepvar_calls/intermediate_results_dir/0004"" --num_shards=32 &> deepvar_0004.log. [deepvar_0004.log](https://github.com/user-attachments/files/16730424/deepvar_0004.log). Also, could you let me know how I can disable it the `vcf_stat_report`?` I tried to look for it but to no luck. It doesn't necessarily affect the variant calling through. Just gives an error/failure due to this last step. Thank you very much once again, and please let me know if I can get you any more information.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/839
https://github.com/google/deepvariant/issues/839:955,safety,error,error,955,"Hello @lucasbrambrink,. Thank you very much for the response. I am attaching the log file herewith. THe command I used was: . singularity run -B /usr/lib/locale/:/usr/lib/locale/ deepvariant_1.6.1.sif /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=""{rest_path}/tools/DeepVariant/ref_genomes/GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta"" --reads=""{rest_path}/MiniMap_SAM_BAM/11741-KA-0004.sorted.bam"" --output_vcf=""{rest_path}/deepvar_calls/11741-KA-0004_output.vcf.gz"" --output_gvcf=""{rest_path}/deepvar_calls/11741-KA-0004_output.g.vcf.gz"" --intermediate_results_dir ""{rest_path}/deepvar_calls/intermediate_results_dir/0004"" --num_shards=32 &> deepvar_0004.log. [deepvar_0004.log](https://github.com/user-attachments/files/16730424/deepvar_0004.log). Also, could you let me know how I can disable it the `vcf_stat_report`?` I tried to look for it but to no luck. It doesn't necessarily affect the variant calling through. Just gives an error/failure due to this last step. Thank you very much once again, and please let me know if I can get you any more information.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/839
https://github.com/google/deepvariant/issues/839:81,security,log,log,81,"Hello @lucasbrambrink,. Thank you very much for the response. I am attaching the log file herewith. THe command I used was: . singularity run -B /usr/lib/locale/:/usr/lib/locale/ deepvariant_1.6.1.sif /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=""{rest_path}/tools/DeepVariant/ref_genomes/GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta"" --reads=""{rest_path}/MiniMap_SAM_BAM/11741-KA-0004.sorted.bam"" --output_vcf=""{rest_path}/deepvar_calls/11741-KA-0004_output.vcf.gz"" --output_gvcf=""{rest_path}/deepvar_calls/11741-KA-0004_output.g.vcf.gz"" --intermediate_results_dir ""{rest_path}/deepvar_calls/intermediate_results_dir/0004"" --num_shards=32 &> deepvar_0004.log. [deepvar_0004.log](https://github.com/user-attachments/files/16730424/deepvar_0004.log). Also, could you let me know how I can disable it the `vcf_stat_report`?` I tried to look for it but to no luck. It doesn't necessarily affect the variant calling through. Just gives an error/failure due to this last step. Thank you very much once again, and please let me know if I can get you any more information.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/839
https://github.com/google/deepvariant/issues/839:676,security,log,log,676,"Hello @lucasbrambrink,. Thank you very much for the response. I am attaching the log file herewith. THe command I used was: . singularity run -B /usr/lib/locale/:/usr/lib/locale/ deepvariant_1.6.1.sif /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=""{rest_path}/tools/DeepVariant/ref_genomes/GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta"" --reads=""{rest_path}/MiniMap_SAM_BAM/11741-KA-0004.sorted.bam"" --output_vcf=""{rest_path}/deepvar_calls/11741-KA-0004_output.vcf.gz"" --output_gvcf=""{rest_path}/deepvar_calls/11741-KA-0004_output.g.vcf.gz"" --intermediate_results_dir ""{rest_path}/deepvar_calls/intermediate_results_dir/0004"" --num_shards=32 &> deepvar_0004.log. [deepvar_0004.log](https://github.com/user-attachments/files/16730424/deepvar_0004.log). Also, could you let me know how I can disable it the `vcf_stat_report`?` I tried to look for it but to no luck. It doesn't necessarily affect the variant calling through. Just gives an error/failure due to this last step. Thank you very much once again, and please let me know if I can get you any more information.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/839
https://github.com/google/deepvariant/issues/839:695,security,log,log,695,"Hello @lucasbrambrink,. Thank you very much for the response. I am attaching the log file herewith. THe command I used was: . singularity run -B /usr/lib/locale/:/usr/lib/locale/ deepvariant_1.6.1.sif /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=""{rest_path}/tools/DeepVariant/ref_genomes/GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta"" --reads=""{rest_path}/MiniMap_SAM_BAM/11741-KA-0004.sorted.bam"" --output_vcf=""{rest_path}/deepvar_calls/11741-KA-0004_output.vcf.gz"" --output_gvcf=""{rest_path}/deepvar_calls/11741-KA-0004_output.g.vcf.gz"" --intermediate_results_dir ""{rest_path}/deepvar_calls/intermediate_results_dir/0004"" --num_shards=32 &> deepvar_0004.log. [deepvar_0004.log](https://github.com/user-attachments/files/16730424/deepvar_0004.log). Also, could you let me know how I can disable it the `vcf_stat_report`?` I tried to look for it but to no luck. It doesn't necessarily affect the variant calling through. Just gives an error/failure due to this last step. Thank you very much once again, and please let me know if I can get you any more information.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/839
https://github.com/google/deepvariant/issues/839:764,security,log,log,764,"Hello @lucasbrambrink,. Thank you very much for the response. I am attaching the log file herewith. THe command I used was: . singularity run -B /usr/lib/locale/:/usr/lib/locale/ deepvariant_1.6.1.sif /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=""{rest_path}/tools/DeepVariant/ref_genomes/GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta"" --reads=""{rest_path}/MiniMap_SAM_BAM/11741-KA-0004.sorted.bam"" --output_vcf=""{rest_path}/deepvar_calls/11741-KA-0004_output.vcf.gz"" --output_gvcf=""{rest_path}/deepvar_calls/11741-KA-0004_output.g.vcf.gz"" --intermediate_results_dir ""{rest_path}/deepvar_calls/intermediate_results_dir/0004"" --num_shards=32 &> deepvar_0004.log. [deepvar_0004.log](https://github.com/user-attachments/files/16730424/deepvar_0004.log). Also, could you let me know how I can disable it the `vcf_stat_report`?` I tried to look for it but to no luck. It doesn't necessarily affect the variant calling through. Just gives an error/failure due to this last step. Thank you very much once again, and please let me know if I can get you any more information.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/839
https://github.com/google/deepvariant/issues/839:81,testability,log,log,81,"Hello @lucasbrambrink,. Thank you very much for the response. I am attaching the log file herewith. THe command I used was: . singularity run -B /usr/lib/locale/:/usr/lib/locale/ deepvariant_1.6.1.sif /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=""{rest_path}/tools/DeepVariant/ref_genomes/GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta"" --reads=""{rest_path}/MiniMap_SAM_BAM/11741-KA-0004.sorted.bam"" --output_vcf=""{rest_path}/deepvar_calls/11741-KA-0004_output.vcf.gz"" --output_gvcf=""{rest_path}/deepvar_calls/11741-KA-0004_output.g.vcf.gz"" --intermediate_results_dir ""{rest_path}/deepvar_calls/intermediate_results_dir/0004"" --num_shards=32 &> deepvar_0004.log. [deepvar_0004.log](https://github.com/user-attachments/files/16730424/deepvar_0004.log). Also, could you let me know how I can disable it the `vcf_stat_report`?` I tried to look for it but to no luck. It doesn't necessarily affect the variant calling through. Just gives an error/failure due to this last step. Thank you very much once again, and please let me know if I can get you any more information.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/839
https://github.com/google/deepvariant/issues/839:676,testability,log,log,676,"Hello @lucasbrambrink,. Thank you very much for the response. I am attaching the log file herewith. THe command I used was: . singularity run -B /usr/lib/locale/:/usr/lib/locale/ deepvariant_1.6.1.sif /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=""{rest_path}/tools/DeepVariant/ref_genomes/GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta"" --reads=""{rest_path}/MiniMap_SAM_BAM/11741-KA-0004.sorted.bam"" --output_vcf=""{rest_path}/deepvar_calls/11741-KA-0004_output.vcf.gz"" --output_gvcf=""{rest_path}/deepvar_calls/11741-KA-0004_output.g.vcf.gz"" --intermediate_results_dir ""{rest_path}/deepvar_calls/intermediate_results_dir/0004"" --num_shards=32 &> deepvar_0004.log. [deepvar_0004.log](https://github.com/user-attachments/files/16730424/deepvar_0004.log). Also, could you let me know how I can disable it the `vcf_stat_report`?` I tried to look for it but to no luck. It doesn't necessarily affect the variant calling through. Just gives an error/failure due to this last step. Thank you very much once again, and please let me know if I can get you any more information.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/839
https://github.com/google/deepvariant/issues/839:695,testability,log,log,695,"Hello @lucasbrambrink,. Thank you very much for the response. I am attaching the log file herewith. THe command I used was: . singularity run -B /usr/lib/locale/:/usr/lib/locale/ deepvariant_1.6.1.sif /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=""{rest_path}/tools/DeepVariant/ref_genomes/GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta"" --reads=""{rest_path}/MiniMap_SAM_BAM/11741-KA-0004.sorted.bam"" --output_vcf=""{rest_path}/deepvar_calls/11741-KA-0004_output.vcf.gz"" --output_gvcf=""{rest_path}/deepvar_calls/11741-KA-0004_output.g.vcf.gz"" --intermediate_results_dir ""{rest_path}/deepvar_calls/intermediate_results_dir/0004"" --num_shards=32 &> deepvar_0004.log. [deepvar_0004.log](https://github.com/user-attachments/files/16730424/deepvar_0004.log). Also, could you let me know how I can disable it the `vcf_stat_report`?` I tried to look for it but to no luck. It doesn't necessarily affect the variant calling through. Just gives an error/failure due to this last step. Thank you very much once again, and please let me know if I can get you any more information.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/839
https://github.com/google/deepvariant/issues/839:764,testability,log,log,764,"Hello @lucasbrambrink,. Thank you very much for the response. I am attaching the log file herewith. THe command I used was: . singularity run -B /usr/lib/locale/:/usr/lib/locale/ deepvariant_1.6.1.sif /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=""{rest_path}/tools/DeepVariant/ref_genomes/GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta"" --reads=""{rest_path}/MiniMap_SAM_BAM/11741-KA-0004.sorted.bam"" --output_vcf=""{rest_path}/deepvar_calls/11741-KA-0004_output.vcf.gz"" --output_gvcf=""{rest_path}/deepvar_calls/11741-KA-0004_output.g.vcf.gz"" --intermediate_results_dir ""{rest_path}/deepvar_calls/intermediate_results_dir/0004"" --num_shards=32 &> deepvar_0004.log. [deepvar_0004.log](https://github.com/user-attachments/files/16730424/deepvar_0004.log). Also, could you let me know how I can disable it the `vcf_stat_report`?` I tried to look for it but to no luck. It doesn't necessarily affect the variant calling through. Just gives an error/failure due to this last step. Thank you very much once again, and please let me know if I can get you any more information.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/839
https://github.com/google/deepvariant/issues/839:104,usability,command,command,104,"Hello @lucasbrambrink,. Thank you very much for the response. I am attaching the log file herewith. THe command I used was: . singularity run -B /usr/lib/locale/:/usr/lib/locale/ deepvariant_1.6.1.sif /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=""{rest_path}/tools/DeepVariant/ref_genomes/GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta"" --reads=""{rest_path}/MiniMap_SAM_BAM/11741-KA-0004.sorted.bam"" --output_vcf=""{rest_path}/deepvar_calls/11741-KA-0004_output.vcf.gz"" --output_gvcf=""{rest_path}/deepvar_calls/11741-KA-0004_output.g.vcf.gz"" --intermediate_results_dir ""{rest_path}/deepvar_calls/intermediate_results_dir/0004"" --num_shards=32 &> deepvar_0004.log. [deepvar_0004.log](https://github.com/user-attachments/files/16730424/deepvar_0004.log). Also, could you let me know how I can disable it the `vcf_stat_report`?` I tried to look for it but to no luck. It doesn't necessarily affect the variant calling through. Just gives an error/failure due to this last step. Thank you very much once again, and please let me know if I can get you any more information.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/839
https://github.com/google/deepvariant/issues/839:274,usability,tool,tools,274,"Hello @lucasbrambrink,. Thank you very much for the response. I am attaching the log file herewith. THe command I used was: . singularity run -B /usr/lib/locale/:/usr/lib/locale/ deepvariant_1.6.1.sif /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=""{rest_path}/tools/DeepVariant/ref_genomes/GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta"" --reads=""{rest_path}/MiniMap_SAM_BAM/11741-KA-0004.sorted.bam"" --output_vcf=""{rest_path}/deepvar_calls/11741-KA-0004_output.vcf.gz"" --output_gvcf=""{rest_path}/deepvar_calls/11741-KA-0004_output.g.vcf.gz"" --intermediate_results_dir ""{rest_path}/deepvar_calls/intermediate_results_dir/0004"" --num_shards=32 &> deepvar_0004.log. [deepvar_0004.log](https://github.com/user-attachments/files/16730424/deepvar_0004.log). Also, could you let me know how I can disable it the `vcf_stat_report`?` I tried to look for it but to no luck. It doesn't necessarily affect the variant calling through. Just gives an error/failure due to this last step. Thank you very much once again, and please let me know if I can get you any more information.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/839
https://github.com/google/deepvariant/issues/839:719,usability,user,user-attachments,719,"Hello @lucasbrambrink,. Thank you very much for the response. I am attaching the log file herewith. THe command I used was: . singularity run -B /usr/lib/locale/:/usr/lib/locale/ deepvariant_1.6.1.sif /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=""{rest_path}/tools/DeepVariant/ref_genomes/GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta"" --reads=""{rest_path}/MiniMap_SAM_BAM/11741-KA-0004.sorted.bam"" --output_vcf=""{rest_path}/deepvar_calls/11741-KA-0004_output.vcf.gz"" --output_gvcf=""{rest_path}/deepvar_calls/11741-KA-0004_output.g.vcf.gz"" --intermediate_results_dir ""{rest_path}/deepvar_calls/intermediate_results_dir/0004"" --num_shards=32 &> deepvar_0004.log. [deepvar_0004.log](https://github.com/user-attachments/files/16730424/deepvar_0004.log). Also, could you let me know how I can disable it the `vcf_stat_report`?` I tried to look for it but to no luck. It doesn't necessarily affect the variant calling through. Just gives an error/failure due to this last step. Thank you very much once again, and please let me know if I can get you any more information.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/839
https://github.com/google/deepvariant/issues/839:955,usability,error,error,955,"Hello @lucasbrambrink,. Thank you very much for the response. I am attaching the log file herewith. THe command I used was: . singularity run -B /usr/lib/locale/:/usr/lib/locale/ deepvariant_1.6.1.sif /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=""{rest_path}/tools/DeepVariant/ref_genomes/GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta"" --reads=""{rest_path}/MiniMap_SAM_BAM/11741-KA-0004.sorted.bam"" --output_vcf=""{rest_path}/deepvar_calls/11741-KA-0004_output.vcf.gz"" --output_gvcf=""{rest_path}/deepvar_calls/11741-KA-0004_output.g.vcf.gz"" --intermediate_results_dir ""{rest_path}/deepvar_calls/intermediate_results_dir/0004"" --num_shards=32 &> deepvar_0004.log. [deepvar_0004.log](https://github.com/user-attachments/files/16730424/deepvar_0004.log). Also, could you let me know how I can disable it the `vcf_stat_report`?` I tried to look for it but to no luck. It doesn't necessarily affect the variant calling through. Just gives an error/failure due to this last step. Thank you very much once again, and please let me know if I can get you any more information.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/839
https://github.com/google/deepvariant/issues/840:64,availability,cluster,clusters,64,"@helizabeth1103 ,. I do not have expertise in running things on clusters, I have never used cluster compute resources so I am not sure how helpful I can be to help you determine cluster issues. However, the command looks right to me independently. If the paths are mounted properly, it should be able to train a model without any problem.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/840
https://github.com/google/deepvariant/issues/840:92,availability,cluster,cluster,92,"@helizabeth1103 ,. I do not have expertise in running things on clusters, I have never used cluster compute resources so I am not sure how helpful I can be to help you determine cluster issues. However, the command looks right to me independently. If the paths are mounted properly, it should be able to train a model without any problem.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/840
https://github.com/google/deepvariant/issues/840:178,availability,cluster,cluster,178,"@helizabeth1103 ,. I do not have expertise in running things on clusters, I have never used cluster compute resources so I am not sure how helpful I can be to help you determine cluster issues. However, the command looks right to me independently. If the paths are mounted properly, it should be able to train a model without any problem.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/840
https://github.com/google/deepvariant/issues/840:64,deployability,cluster,clusters,64,"@helizabeth1103 ,. I do not have expertise in running things on clusters, I have never used cluster compute resources so I am not sure how helpful I can be to help you determine cluster issues. However, the command looks right to me independently. If the paths are mounted properly, it should be able to train a model without any problem.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/840
https://github.com/google/deepvariant/issues/840:92,deployability,cluster,cluster,92,"@helizabeth1103 ,. I do not have expertise in running things on clusters, I have never used cluster compute resources so I am not sure how helpful I can be to help you determine cluster issues. However, the command looks right to me independently. If the paths are mounted properly, it should be able to train a model without any problem.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/840
https://github.com/google/deepvariant/issues/840:108,deployability,resourc,resources,108,"@helizabeth1103 ,. I do not have expertise in running things on clusters, I have never used cluster compute resources so I am not sure how helpful I can be to help you determine cluster issues. However, the command looks right to me independently. If the paths are mounted properly, it should be able to train a model without any problem.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/840
https://github.com/google/deepvariant/issues/840:178,deployability,cluster,cluster,178,"@helizabeth1103 ,. I do not have expertise in running things on clusters, I have never used cluster compute resources so I am not sure how helpful I can be to help you determine cluster issues. However, the command looks right to me independently. If the paths are mounted properly, it should be able to train a model without any problem.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/840
https://github.com/google/deepvariant/issues/840:108,energy efficiency,resourc,resources,108,"@helizabeth1103 ,. I do not have expertise in running things on clusters, I have never used cluster compute resources so I am not sure how helpful I can be to help you determine cluster issues. However, the command looks right to me independently. If the paths are mounted properly, it should be able to train a model without any problem.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/840
https://github.com/google/deepvariant/issues/840:312,energy efficiency,model,model,312,"@helizabeth1103 ,. I do not have expertise in running things on clusters, I have never used cluster compute resources so I am not sure how helpful I can be to help you determine cluster issues. However, the command looks right to me independently. If the paths are mounted properly, it should be able to train a model without any problem.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/840
https://github.com/google/deepvariant/issues/840:100,performance,compute resourc,compute resources,100,"@helizabeth1103 ,. I do not have expertise in running things on clusters, I have never used cluster compute resources so I am not sure how helpful I can be to help you determine cluster issues. However, the command looks right to me independently. If the paths are mounted properly, it should be able to train a model without any problem.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/840
https://github.com/google/deepvariant/issues/840:108,safety,resourc,resources,108,"@helizabeth1103 ,. I do not have expertise in running things on clusters, I have never used cluster compute resources so I am not sure how helpful I can be to help you determine cluster issues. However, the command looks right to me independently. If the paths are mounted properly, it should be able to train a model without any problem.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/840
https://github.com/google/deepvariant/issues/840:312,security,model,model,312,"@helizabeth1103 ,. I do not have expertise in running things on clusters, I have never used cluster compute resources so I am not sure how helpful I can be to help you determine cluster issues. However, the command looks right to me independently. If the paths are mounted properly, it should be able to train a model without any problem.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/840
https://github.com/google/deepvariant/issues/840:108,testability,resourc,resources,108,"@helizabeth1103 ,. I do not have expertise in running things on clusters, I have never used cluster compute resources so I am not sure how helpful I can be to help you determine cluster issues. However, the command looks right to me independently. If the paths are mounted properly, it should be able to train a model without any problem.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/840
https://github.com/google/deepvariant/issues/840:139,usability,help,helpful,139,"@helizabeth1103 ,. I do not have expertise in running things on clusters, I have never used cluster compute resources so I am not sure how helpful I can be to help you determine cluster issues. However, the command looks right to me independently. If the paths are mounted properly, it should be able to train a model without any problem.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/840
https://github.com/google/deepvariant/issues/840:159,usability,help,help,159,"@helizabeth1103 ,. I do not have expertise in running things on clusters, I have never used cluster compute resources so I am not sure how helpful I can be to help you determine cluster issues. However, the command looks right to me independently. If the paths are mounted properly, it should be able to train a model without any problem.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/840
https://github.com/google/deepvariant/issues/840:207,usability,command,command,207,"@helizabeth1103 ,. I do not have expertise in running things on clusters, I have never used cluster compute resources so I am not sure how helpful I can be to help you determine cluster issues. However, the command looks right to me independently. If the paths are mounted properly, it should be able to train a model without any problem.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/840
https://github.com/google/deepvariant/issues/840:49,deployability,version,version,49,"Thank you, it turned out to be an issue with the version of apptainer I was using, but I still very much appreciate you taking a look for me! . Do you have any recommendations for how to determine good starting points for parameters in model training? . Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/840
https://github.com/google/deepvariant/issues/840:236,energy efficiency,model,model,236,"Thank you, it turned out to be an issue with the version of apptainer I was using, but I still very much appreciate you taking a look for me! . Do you have any recommendations for how to determine good starting points for parameters in model training? . Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/840
https://github.com/google/deepvariant/issues/840:49,integrability,version,version,49,"Thank you, it turned out to be an issue with the version of apptainer I was using, but I still very much appreciate you taking a look for me! . Do you have any recommendations for how to determine good starting points for parameters in model training? . Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/840
https://github.com/google/deepvariant/issues/840:49,modifiability,version,version,49,"Thank you, it turned out to be an issue with the version of apptainer I was using, but I still very much appreciate you taking a look for me! . Do you have any recommendations for how to determine good starting points for parameters in model training? . Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/840
https://github.com/google/deepvariant/issues/840:222,modifiability,paramet,parameters,222,"Thank you, it turned out to be an issue with the version of apptainer I was using, but I still very much appreciate you taking a look for me! . Do you have any recommendations for how to determine good starting points for parameters in model training? . Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/840
https://github.com/google/deepvariant/issues/840:236,security,model,model,236,"Thank you, it turned out to be an issue with the version of apptainer I was using, but I still very much appreciate you taking a look for me! . Do you have any recommendations for how to determine good starting points for parameters in model training? . Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/840
https://github.com/google/deepvariant/issues/840:89,deployability,resourc,resources,89,"@helizabeth1103 , . Usually I start with having a fixed batch size that utilizes maximum resources and do a grid search on learning_rate. Once you have done that, you can play around with weight_decay to see if you want more generalizability from the model. If you have a way to do hyperparameter sweep, that would be the best way to go. . Please feel free to open the issue again if you are facing similar issues with training.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/840
https://github.com/google/deepvariant/issues/840:89,energy efficiency,resourc,resources,89,"@helizabeth1103 , . Usually I start with having a fixed batch size that utilizes maximum resources and do a grid search on learning_rate. Once you have done that, you can play around with weight_decay to see if you want more generalizability from the model. If you have a way to do hyperparameter sweep, that would be the best way to go. . Please feel free to open the issue again if you are facing similar issues with training.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/840
https://github.com/google/deepvariant/issues/840:251,energy efficiency,model,model,251,"@helizabeth1103 , . Usually I start with having a fixed batch size that utilizes maximum resources and do a grid search on learning_rate. Once you have done that, you can play around with weight_decay to see if you want more generalizability from the model. If you have a way to do hyperparameter sweep, that would be the best way to go. . Please feel free to open the issue again if you are facing similar issues with training.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/840
https://github.com/google/deepvariant/issues/840:56,integrability,batch,batch,56,"@helizabeth1103 , . Usually I start with having a fixed batch size that utilizes maximum resources and do a grid search on learning_rate. Once you have done that, you can play around with weight_decay to see if you want more generalizability from the model. If you have a way to do hyperparameter sweep, that would be the best way to go. . Please feel free to open the issue again if you are facing similar issues with training.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/840
https://github.com/google/deepvariant/issues/840:56,performance,batch,batch,56,"@helizabeth1103 , . Usually I start with having a fixed batch size that utilizes maximum resources and do a grid search on learning_rate. Once you have done that, you can play around with weight_decay to see if you want more generalizability from the model. If you have a way to do hyperparameter sweep, that would be the best way to go. . Please feel free to open the issue again if you are facing similar issues with training.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/840
https://github.com/google/deepvariant/issues/840:89,performance,resourc,resources,89,"@helizabeth1103 , . Usually I start with having a fixed batch size that utilizes maximum resources and do a grid search on learning_rate. Once you have done that, you can play around with weight_decay to see if you want more generalizability from the model. If you have a way to do hyperparameter sweep, that would be the best way to go. . Please feel free to open the issue again if you are facing similar issues with training.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/840
https://github.com/google/deepvariant/issues/840:89,safety,resourc,resources,89,"@helizabeth1103 , . Usually I start with having a fixed batch size that utilizes maximum resources and do a grid search on learning_rate. Once you have done that, you can play around with weight_decay to see if you want more generalizability from the model. If you have a way to do hyperparameter sweep, that would be the best way to go. . Please feel free to open the issue again if you are facing similar issues with training.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/840
https://github.com/google/deepvariant/issues/840:251,security,model,model,251,"@helizabeth1103 , . Usually I start with having a fixed batch size that utilizes maximum resources and do a grid search on learning_rate. Once you have done that, you can play around with weight_decay to see if you want more generalizability from the model. If you have a way to do hyperparameter sweep, that would be the best way to go. . Please feel free to open the issue again if you are facing similar issues with training.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/840
https://github.com/google/deepvariant/issues/840:89,testability,resourc,resources,89,"@helizabeth1103 , . Usually I start with having a fixed batch size that utilizes maximum resources and do a grid search on learning_rate. Once you have done that, you can play around with weight_decay to see if you want more generalizability from the model. If you have a way to do hyperparameter sweep, that would be the best way to go. . Please feel free to open the issue again if you are facing similar issues with training.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/840
https://github.com/google/deepvariant/issues/841:188,deployability,API,API,188,"hi @gambalab ,. Our current DeepVariant training code is only optimized to train on GPUs externally and will not work on TPUs. As you can see the TPU initialization is not achieved as the API is not present in the current code:. ```bash. tensorflow.python.framework.errors_impl.InvalidArgumentError: No OpKernel was registered to support Op 'ConfigureDistributedTPU' used by {{node ConfigureDistributedTPU}} with these attrs: [compilation_failure_closes_chips=false, tpu_cancellation_closes_chips=2, tpu_embedding_config="""", embedding_config="""", enable_whole_mesh_compilations=false, is_global_init=false]. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/841
https://github.com/google/deepvariant/issues/841:20,energy efficiency,current,current,20,"hi @gambalab ,. Our current DeepVariant training code is only optimized to train on GPUs externally and will not work on TPUs. As you can see the TPU initialization is not achieved as the API is not present in the current code:. ```bash. tensorflow.python.framework.errors_impl.InvalidArgumentError: No OpKernel was registered to support Op 'ConfigureDistributedTPU' used by {{node ConfigureDistributedTPU}} with these attrs: [compilation_failure_closes_chips=false, tpu_cancellation_closes_chips=2, tpu_embedding_config="""", embedding_config="""", enable_whole_mesh_compilations=false, is_global_init=false]. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/841
https://github.com/google/deepvariant/issues/841:62,energy efficiency,optim,optimized,62,"hi @gambalab ,. Our current DeepVariant training code is only optimized to train on GPUs externally and will not work on TPUs. As you can see the TPU initialization is not achieved as the API is not present in the current code:. ```bash. tensorflow.python.framework.errors_impl.InvalidArgumentError: No OpKernel was registered to support Op 'ConfigureDistributedTPU' used by {{node ConfigureDistributedTPU}} with these attrs: [compilation_failure_closes_chips=false, tpu_cancellation_closes_chips=2, tpu_embedding_config="""", embedding_config="""", enable_whole_mesh_compilations=false, is_global_init=false]. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/841
https://github.com/google/deepvariant/issues/841:84,energy efficiency,GPU,GPUs,84,"hi @gambalab ,. Our current DeepVariant training code is only optimized to train on GPUs externally and will not work on TPUs. As you can see the TPU initialization is not achieved as the API is not present in the current code:. ```bash. tensorflow.python.framework.errors_impl.InvalidArgumentError: No OpKernel was registered to support Op 'ConfigureDistributedTPU' used by {{node ConfigureDistributedTPU}} with these attrs: [compilation_failure_closes_chips=false, tpu_cancellation_closes_chips=2, tpu_embedding_config="""", embedding_config="""", enable_whole_mesh_compilations=false, is_global_init=false]. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/841
https://github.com/google/deepvariant/issues/841:214,energy efficiency,current,current,214,"hi @gambalab ,. Our current DeepVariant training code is only optimized to train on GPUs externally and will not work on TPUs. As you can see the TPU initialization is not achieved as the API is not present in the current code:. ```bash. tensorflow.python.framework.errors_impl.InvalidArgumentError: No OpKernel was registered to support Op 'ConfigureDistributedTPU' used by {{node ConfigureDistributedTPU}} with these attrs: [compilation_failure_closes_chips=false, tpu_cancellation_closes_chips=2, tpu_embedding_config="""", embedding_config="""", enable_whole_mesh_compilations=false, is_global_init=false]. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/841
https://github.com/google/deepvariant/issues/841:188,integrability,API,API,188,"hi @gambalab ,. Our current DeepVariant training code is only optimized to train on GPUs externally and will not work on TPUs. As you can see the TPU initialization is not achieved as the API is not present in the current code:. ```bash. tensorflow.python.framework.errors_impl.InvalidArgumentError: No OpKernel was registered to support Op 'ConfigureDistributedTPU' used by {{node ConfigureDistributedTPU}} with these attrs: [compilation_failure_closes_chips=false, tpu_cancellation_closes_chips=2, tpu_embedding_config="""", embedding_config="""", enable_whole_mesh_compilations=false, is_global_init=false]. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/841
https://github.com/google/deepvariant/issues/841:342,integrability,Configur,ConfigureDistributedTPU,342,"hi @gambalab ,. Our current DeepVariant training code is only optimized to train on GPUs externally and will not work on TPUs. As you can see the TPU initialization is not achieved as the API is not present in the current code:. ```bash. tensorflow.python.framework.errors_impl.InvalidArgumentError: No OpKernel was registered to support Op 'ConfigureDistributedTPU' used by {{node ConfigureDistributedTPU}} with these attrs: [compilation_failure_closes_chips=false, tpu_cancellation_closes_chips=2, tpu_embedding_config="""", embedding_config="""", enable_whole_mesh_compilations=false, is_global_init=false]. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/841
https://github.com/google/deepvariant/issues/841:382,integrability,Configur,ConfigureDistributedTPU,382,"hi @gambalab ,. Our current DeepVariant training code is only optimized to train on GPUs externally and will not work on TPUs. As you can see the TPU initialization is not achieved as the API is not present in the current code:. ```bash. tensorflow.python.framework.errors_impl.InvalidArgumentError: No OpKernel was registered to support Op 'ConfigureDistributedTPU' used by {{node ConfigureDistributedTPU}} with these attrs: [compilation_failure_closes_chips=false, tpu_cancellation_closes_chips=2, tpu_embedding_config="""", embedding_config="""", enable_whole_mesh_compilations=false, is_global_init=false]. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/841
https://github.com/google/deepvariant/issues/841:188,interoperability,API,API,188,"hi @gambalab ,. Our current DeepVariant training code is only optimized to train on GPUs externally and will not work on TPUs. As you can see the TPU initialization is not achieved as the API is not present in the current code:. ```bash. tensorflow.python.framework.errors_impl.InvalidArgumentError: No OpKernel was registered to support Op 'ConfigureDistributedTPU' used by {{node ConfigureDistributedTPU}} with these attrs: [compilation_failure_closes_chips=false, tpu_cancellation_closes_chips=2, tpu_embedding_config="""", embedding_config="""", enable_whole_mesh_compilations=false, is_global_init=false]. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/841
https://github.com/google/deepvariant/issues/841:342,modifiability,Configur,ConfigureDistributedTPU,342,"hi @gambalab ,. Our current DeepVariant training code is only optimized to train on GPUs externally and will not work on TPUs. As you can see the TPU initialization is not achieved as the API is not present in the current code:. ```bash. tensorflow.python.framework.errors_impl.InvalidArgumentError: No OpKernel was registered to support Op 'ConfigureDistributedTPU' used by {{node ConfigureDistributedTPU}} with these attrs: [compilation_failure_closes_chips=false, tpu_cancellation_closes_chips=2, tpu_embedding_config="""", embedding_config="""", enable_whole_mesh_compilations=false, is_global_init=false]. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/841
https://github.com/google/deepvariant/issues/841:382,modifiability,Configur,ConfigureDistributedTPU,382,"hi @gambalab ,. Our current DeepVariant training code is only optimized to train on GPUs externally and will not work on TPUs. As you can see the TPU initialization is not achieved as the API is not present in the current code:. ```bash. tensorflow.python.framework.errors_impl.InvalidArgumentError: No OpKernel was registered to support Op 'ConfigureDistributedTPU' used by {{node ConfigureDistributedTPU}} with these attrs: [compilation_failure_closes_chips=false, tpu_cancellation_closes_chips=2, tpu_embedding_config="""", embedding_config="""", enable_whole_mesh_compilations=false, is_global_init=false]. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/841
https://github.com/google/deepvariant/issues/841:62,performance,optimiz,optimized,62,"hi @gambalab ,. Our current DeepVariant training code is only optimized to train on GPUs externally and will not work on TPUs. As you can see the TPU initialization is not achieved as the API is not present in the current code:. ```bash. tensorflow.python.framework.errors_impl.InvalidArgumentError: No OpKernel was registered to support Op 'ConfigureDistributedTPU' used by {{node ConfigureDistributedTPU}} with these attrs: [compilation_failure_closes_chips=false, tpu_cancellation_closes_chips=2, tpu_embedding_config="""", embedding_config="""", enable_whole_mesh_compilations=false, is_global_init=false]. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/841
https://github.com/google/deepvariant/issues/841:84,performance,GPU,GPUs,84,"hi @gambalab ,. Our current DeepVariant training code is only optimized to train on GPUs externally and will not work on TPUs. As you can see the TPU initialization is not achieved as the API is not present in the current code:. ```bash. tensorflow.python.framework.errors_impl.InvalidArgumentError: No OpKernel was registered to support Op 'ConfigureDistributedTPU' used by {{node ConfigureDistributedTPU}} with these attrs: [compilation_failure_closes_chips=false, tpu_cancellation_closes_chips=2, tpu_embedding_config="""", embedding_config="""", enable_whole_mesh_compilations=false, is_global_init=false]. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/841
https://github.com/google/deepvariant/issues/841:342,security,Configur,ConfigureDistributedTPU,342,"hi @gambalab ,. Our current DeepVariant training code is only optimized to train on GPUs externally and will not work on TPUs. As you can see the TPU initialization is not achieved as the API is not present in the current code:. ```bash. tensorflow.python.framework.errors_impl.InvalidArgumentError: No OpKernel was registered to support Op 'ConfigureDistributedTPU' used by {{node ConfigureDistributedTPU}} with these attrs: [compilation_failure_closes_chips=false, tpu_cancellation_closes_chips=2, tpu_embedding_config="""", embedding_config="""", enable_whole_mesh_compilations=false, is_global_init=false]. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/841
https://github.com/google/deepvariant/issues/841:382,security,Configur,ConfigureDistributedTPU,382,"hi @gambalab ,. Our current DeepVariant training code is only optimized to train on GPUs externally and will not work on TPUs. As you can see the TPU initialization is not achieved as the API is not present in the current code:. ```bash. tensorflow.python.framework.errors_impl.InvalidArgumentError: No OpKernel was registered to support Op 'ConfigureDistributedTPU' used by {{node ConfigureDistributedTPU}} with these attrs: [compilation_failure_closes_chips=false, tpu_cancellation_closes_chips=2, tpu_embedding_config="""", embedding_config="""", enable_whole_mesh_compilations=false, is_global_init=false]. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/841
https://github.com/google/deepvariant/issues/841:330,usability,support,support,330,"hi @gambalab ,. Our current DeepVariant training code is only optimized to train on GPUs externally and will not work on TPUs. As you can see the TPU initialization is not achieved as the API is not present in the current code:. ```bash. tensorflow.python.framework.errors_impl.InvalidArgumentError: No OpKernel was registered to support Op 'ConfigureDistributedTPU' used by {{node ConfigureDistributedTPU}} with these attrs: [compilation_failure_closes_chips=false, tpu_cancellation_closes_chips=2, tpu_embedding_config="""", embedding_config="""", enable_whole_mesh_compilations=false, is_global_init=false]. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/841
https://github.com/google/deepvariant/issues/842:99,integrability,abstract,abstract,99,"@zyxNo1 ,. You can find the data here: https://www.biorxiv.org/content/10.1101/2020.12.11.422022v1.abstract",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/842
https://github.com/google/deepvariant/issues/842:99,modifiability,abstract,abstract,99,"@zyxNo1 ,. You can find the data here: https://www.biorxiv.org/content/10.1101/2020.12.11.422022v1.abstract",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/842
https://github.com/google/deepvariant/issues/842:63,performance,content,content,63,"@zyxNo1 ,. You can find the data here: https://www.biorxiv.org/content/10.1101/2020.12.11.422022v1.abstract",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/842
https://github.com/google/deepvariant/issues/843:235,energy efficiency,frequenc,frequency,235,"Hi @Wasya-the-Wolf ,. When you say ""could not be called"", does that mean the variant is absent from the VCF or it's a REFCALL? The thresholds are set in the fist step of DeepVariant, which is `make_examples` and any candidates passing frequency thresholds are then run through the CNN for genotyping.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/843
https://github.com/google/deepvariant/issues/843:58,reliability,doe,does,58,"Hi @Wasya-the-Wolf ,. When you say ""could not be called"", does that mean the variant is absent from the VCF or it's a REFCALL? The thresholds are set in the fist step of DeepVariant, which is `make_examples` and any candidates passing frequency thresholds are then run through the CNN for genotyping.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/843
https://github.com/google/deepvariant/issues/843:243,energy efficiency,frequenc,frequency,243,"> Hi @Wasya-the-Wolf ,. > . > When you say ""could not be called"", does that mean the variant is absent from the VCF or it's a REFCALL? The thresholds are set in the fist step of DeepVariant, which is `make_examples` and any candidates passing frequency thresholds are then run through the CNN for genotyping. Thank you very much for providing useful information. May I ask what parameters do I need to set in make_example to adjust the VAF threshold?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/843
https://github.com/google/deepvariant/issues/843:378,modifiability,paramet,parameters,378,"> Hi @Wasya-the-Wolf ,. > . > When you say ""could not be called"", does that mean the variant is absent from the VCF or it's a REFCALL? The thresholds are set in the fist step of DeepVariant, which is `make_examples` and any candidates passing frequency thresholds are then run through the CNN for genotyping. Thank you very much for providing useful information. May I ask what parameters do I need to set in make_example to adjust the VAF threshold?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/843
https://github.com/google/deepvariant/issues/843:66,reliability,doe,does,66,"> Hi @Wasya-the-Wolf ,. > . > When you say ""could not be called"", does that mean the variant is absent from the VCF or it's a REFCALL? The thresholds are set in the fist step of DeepVariant, which is `make_examples` and any candidates passing frequency thresholds are then run through the CNN for genotyping. Thank you very much for providing useful information. May I ask what parameters do I need to set in make_example to adjust the VAF threshold?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/843
https://github.com/google/deepvariant/issues/843:30,modifiability,paramet,parameters,30,"@Wasya-the-Wolf , the default parameters should be able to call variants with high sensitivity. Can you please explain this part of your question:. > When you say ""could not be called"", does that mean the variant is absent from the VCF or it's a REFCALL?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/843
https://github.com/google/deepvariant/issues/843:186,reliability,doe,does,186,"@Wasya-the-Wolf , the default parameters should be able to call variants with high sensitivity. Can you please explain this part of your question:. > When you say ""could not be called"", does that mean the variant is absent from the VCF or it's a REFCALL?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/843
https://github.com/google/deepvariant/issues/843:32,modifiability,paramet,parameters,32,"> @Wasya-the-Wolf , the default parameters should be able to call variants with high sensitivity. Can you please explain this part of your question:. > . > > When you say ""could not be called"", does that mean the variant is absent from the VCF or it's a REFCALL? Yes, it is absent from the VCF, not REFCALL. I have checked the raw vcf files, and it turned out that these variants did not appear in my output.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/843
https://github.com/google/deepvariant/issues/843:194,reliability,doe,does,194,"> @Wasya-the-Wolf , the default parameters should be able to call variants with high sensitivity. Can you please explain this part of your question:. > . > > When you say ""could not be called"", does that mean the variant is absent from the VCF or it's a REFCALL? Yes, it is absent from the VCF, not REFCALL. I have checked the raw vcf files, and it turned out that these variants did not appear in my output.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/843
https://github.com/google/deepvariant/issues/843:423,modifiability,paramet,parameters,423,"> @Wasya-the-Wolf ,. > . > I'd suggest using:. > . > ```shell. > --make_examples_extra_args ""vsc_min_fraction_indels=0.10,vsc_min_fraction_snps=0.10"". > ```. > . > And set it to your desired fraction. Although by default it low for `WES` which means those variants should appear in the output. But you can put a small value and see if you can rescue some of these variants. Thank you very much for your help! Before adding parameters, I have a small question: What does the `vsc_min_fraction` parameter do?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/843
https://github.com/google/deepvariant/issues/843:493,modifiability,paramet,parameter,493,"> @Wasya-the-Wolf ,. > . > I'd suggest using:. > . > ```shell. > --make_examples_extra_args ""vsc_min_fraction_indels=0.10,vsc_min_fraction_snps=0.10"". > ```. > . > And set it to your desired fraction. Although by default it low for `WES` which means those variants should appear in the output. But you can put a small value and see if you can rescue some of these variants. Thank you very much for your help! Before adding parameters, I have a small question: What does the `vsc_min_fraction` parameter do?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/843
https://github.com/google/deepvariant/issues/843:465,reliability,doe,does,465,"> @Wasya-the-Wolf ,. > . > I'd suggest using:. > . > ```shell. > --make_examples_extra_args ""vsc_min_fraction_indels=0.10,vsc_min_fraction_snps=0.10"". > ```. > . > And set it to your desired fraction. Although by default it low for `WES` which means those variants should appear in the output. But you can put a small value and see if you can rescue some of these variants. Thank you very much for your help! Before adding parameters, I have a small question: What does the `vsc_min_fraction` parameter do?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/843
https://github.com/google/deepvariant/issues/843:403,usability,help,help,403,"> @Wasya-the-Wolf ,. > . > I'd suggest using:. > . > ```shell. > --make_examples_extra_args ""vsc_min_fraction_indels=0.10,vsc_min_fraction_snps=0.10"". > ```. > . > And set it to your desired fraction. Although by default it low for `WES` which means those variants should appear in the output. But you can put a small value and see if you can rescue some of these variants. Thank you very much for your help! Before adding parameters, I have a small question: What does the `vsc_min_fraction` parameter do?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/843
https://github.com/google/deepvariant/issues/843:357,modifiability,paramet,parameters,357,```bash. time docker run -it \. google/deepvariant:1.6.1 \. /opt/deepvariant/bin/make_examples --helpfull | grep 'vsc_min_fraction_snps' -A 5. ```. Shows:. ```bash. --vsc_min_fraction_snps: SNP alleles occurring at least this fraction of all. counts in our AlleleCount will be advanced as candidates. (default: '0.12'). ```. You can look at the full set of parameters by removing grep. Please also consider seeing [how deepvariant works](https://github.com/google/deepvariant?tab=readme-ov-file#how-deepvariant-works) and the DeepVariant manuscript: https://www.nature.com/articles/nbt.4235 for more details.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/843
https://github.com/google/deepvariant/issues/843:9,performance,time,time,9,```bash. time docker run -it \. google/deepvariant:1.6.1 \. /opt/deepvariant/bin/make_examples --helpfull | grep 'vsc_min_fraction_snps' -A 5. ```. Shows:. ```bash. --vsc_min_fraction_snps: SNP alleles occurring at least this fraction of all. counts in our AlleleCount will be advanced as candidates. (default: '0.12'). ```. You can look at the full set of parameters by removing grep. Please also consider seeing [how deepvariant works](https://github.com/google/deepvariant?tab=readme-ov-file#how-deepvariant-works) and the DeepVariant manuscript: https://www.nature.com/articles/nbt.4235 for more details.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/843
https://github.com/google/deepvariant/issues/843:97,usability,help,helpfull,97,```bash. time docker run -it \. google/deepvariant:1.6.1 \. /opt/deepvariant/bin/make_examples --helpfull | grep 'vsc_min_fraction_snps' -A 5. ```. Shows:. ```bash. --vsc_min_fraction_snps: SNP alleles occurring at least this fraction of all. counts in our AlleleCount will be advanced as candidates. (default: '0.12'). ```. You can look at the full set of parameters by removing grep. Please also consider seeing [how deepvariant works](https://github.com/google/deepvariant?tab=readme-ov-file#how-deepvariant-works) and the DeepVariant manuscript: https://www.nature.com/articles/nbt.4235 for more details.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/843
https://github.com/google/deepvariant/issues/843:395,modifiability,paramet,parameters,395,> ```shell. > time docker run -it \. > google/deepvariant:1.6.1 \. > /opt/deepvariant/bin/make_examples --helpfull | grep 'vsc_min_fraction_snps' -A 5. > ```. > . > Shows:. > . > ```shell. > --vsc_min_fraction_snps: SNP alleles occurring at least this fraction of all. > counts in our AlleleCount will be advanced as candidates. > (default: '0.12'). > ```. > . > You can look at the full set of parameters by removing grep. Please also consider seeing [how deepvariant works](https://github.com/google/deepvariant?tab=readme-ov-file#how-deepvariant-works) and the DeepVariant manuscript: https://www.nature.com/articles/nbt.4235 for more details. ok，thank you！,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/843
https://github.com/google/deepvariant/issues/843:14,performance,time,time,14,> ```shell. > time docker run -it \. > google/deepvariant:1.6.1 \. > /opt/deepvariant/bin/make_examples --helpfull | grep 'vsc_min_fraction_snps' -A 5. > ```. > . > Shows:. > . > ```shell. > --vsc_min_fraction_snps: SNP alleles occurring at least this fraction of all. > counts in our AlleleCount will be advanced as candidates. > (default: '0.12'). > ```. > . > You can look at the full set of parameters by removing grep. Please also consider seeing [how deepvariant works](https://github.com/google/deepvariant?tab=readme-ov-file#how-deepvariant-works) and the DeepVariant manuscript: https://www.nature.com/articles/nbt.4235 for more details. ok，thank you！,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/843
https://github.com/google/deepvariant/issues/843:106,usability,help,helpfull,106,> ```shell. > time docker run -it \. > google/deepvariant:1.6.1 \. > /opt/deepvariant/bin/make_examples --helpfull | grep 'vsc_min_fraction_snps' -A 5. > ```. > . > Shows:. > . > ```shell. > --vsc_min_fraction_snps: SNP alleles occurring at least this fraction of all. > counts in our AlleleCount will be advanced as candidates. > (default: '0.12'). > ```. > . > You can look at the full set of parameters by removing grep. Please also consider seeing [how deepvariant works](https://github.com/google/deepvariant?tab=readme-ov-file#how-deepvariant-works) and the DeepVariant manuscript: https://www.nature.com/articles/nbt.4235 for more details. ok，thank you！,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/843
https://github.com/google/deepvariant/issues/843:129,interoperability,specif,specific-variant-in-my-data,129,I also recommend you reading this: https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md#why-does-deepvariant-not-call-a-specific-variant-in-my-data. I will close this issue for now. Please reopen if you have further questions.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/843
https://github.com/google/deepvariant/issues/843:101,reliability,doe,does-deepvariant-not-call-a-specific-variant-in-my-data,101,I also recommend you reading this: https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md#why-does-deepvariant-not-call-a-specific-variant-in-my-data. I will close this issue for now. Please reopen if you have further questions.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/843
https://github.com/google/deepvariant/issues/843:165,usability,close,close,165,I also recommend you reading this: https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md#why-does-deepvariant-not-call-a-specific-variant-in-my-data. I will close this issue for now. Please reopen if you have further questions.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/843
https://github.com/google/deepvariant/issues/844:62,deployability,updat,update,62,"@s-batalov, I will take a look soon on how difficult it is to update the CUDA version on our end. However, it's difficult to rebuild the 1.6.1 version with an updated CUDA version, it may have to be the next release where we update the CUDA version.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/844
https://github.com/google/deepvariant/issues/844:78,deployability,version,version,78,"@s-batalov, I will take a look soon on how difficult it is to update the CUDA version on our end. However, it's difficult to rebuild the 1.6.1 version with an updated CUDA version, it may have to be the next release where we update the CUDA version.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/844
https://github.com/google/deepvariant/issues/844:143,deployability,version,version,143,"@s-batalov, I will take a look soon on how difficult it is to update the CUDA version on our end. However, it's difficult to rebuild the 1.6.1 version with an updated CUDA version, it may have to be the next release where we update the CUDA version.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/844
https://github.com/google/deepvariant/issues/844:159,deployability,updat,updated,159,"@s-batalov, I will take a look soon on how difficult it is to update the CUDA version on our end. However, it's difficult to rebuild the 1.6.1 version with an updated CUDA version, it may have to be the next release where we update the CUDA version.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/844
https://github.com/google/deepvariant/issues/844:172,deployability,version,version,172,"@s-batalov, I will take a look soon on how difficult it is to update the CUDA version on our end. However, it's difficult to rebuild the 1.6.1 version with an updated CUDA version, it may have to be the next release where we update the CUDA version.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/844
https://github.com/google/deepvariant/issues/844:208,deployability,releas,release,208,"@s-batalov, I will take a look soon on how difficult it is to update the CUDA version on our end. However, it's difficult to rebuild the 1.6.1 version with an updated CUDA version, it may have to be the next release where we update the CUDA version.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/844
https://github.com/google/deepvariant/issues/844:225,deployability,updat,update,225,"@s-batalov, I will take a look soon on how difficult it is to update the CUDA version on our end. However, it's difficult to rebuild the 1.6.1 version with an updated CUDA version, it may have to be the next release where we update the CUDA version.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/844
https://github.com/google/deepvariant/issues/844:241,deployability,version,version,241,"@s-batalov, I will take a look soon on how difficult it is to update the CUDA version on our end. However, it's difficult to rebuild the 1.6.1 version with an updated CUDA version, it may have to be the next release where we update the CUDA version.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/844
https://github.com/google/deepvariant/issues/844:78,integrability,version,version,78,"@s-batalov, I will take a look soon on how difficult it is to update the CUDA version on our end. However, it's difficult to rebuild the 1.6.1 version with an updated CUDA version, it may have to be the next release where we update the CUDA version.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/844
https://github.com/google/deepvariant/issues/844:143,integrability,version,version,143,"@s-batalov, I will take a look soon on how difficult it is to update the CUDA version on our end. However, it's difficult to rebuild the 1.6.1 version with an updated CUDA version, it may have to be the next release where we update the CUDA version.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/844
https://github.com/google/deepvariant/issues/844:172,integrability,version,version,172,"@s-batalov, I will take a look soon on how difficult it is to update the CUDA version on our end. However, it's difficult to rebuild the 1.6.1 version with an updated CUDA version, it may have to be the next release where we update the CUDA version.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/844
https://github.com/google/deepvariant/issues/844:241,integrability,version,version,241,"@s-batalov, I will take a look soon on how difficult it is to update the CUDA version on our end. However, it's difficult to rebuild the 1.6.1 version with an updated CUDA version, it may have to be the next release where we update the CUDA version.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/844
https://github.com/google/deepvariant/issues/844:78,modifiability,version,version,78,"@s-batalov, I will take a look soon on how difficult it is to update the CUDA version on our end. However, it's difficult to rebuild the 1.6.1 version with an updated CUDA version, it may have to be the next release where we update the CUDA version.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/844
https://github.com/google/deepvariant/issues/844:143,modifiability,version,version,143,"@s-batalov, I will take a look soon on how difficult it is to update the CUDA version on our end. However, it's difficult to rebuild the 1.6.1 version with an updated CUDA version, it may have to be the next release where we update the CUDA version.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/844
https://github.com/google/deepvariant/issues/844:172,modifiability,version,version,172,"@s-batalov, I will take a look soon on how difficult it is to update the CUDA version on our end. However, it's difficult to rebuild the 1.6.1 version with an updated CUDA version, it may have to be the next release where we update the CUDA version.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/844
https://github.com/google/deepvariant/issues/844:241,modifiability,version,version,241,"@s-batalov, I will take a look soon on how difficult it is to update the CUDA version on our end. However, it's difficult to rebuild the 1.6.1 version with an updated CUDA version, it may have to be the next release where we update the CUDA version.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/844
https://github.com/google/deepvariant/issues/844:62,safety,updat,update,62,"@s-batalov, I will take a look soon on how difficult it is to update the CUDA version on our end. However, it's difficult to rebuild the 1.6.1 version with an updated CUDA version, it may have to be the next release where we update the CUDA version.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/844
https://github.com/google/deepvariant/issues/844:159,safety,updat,updated,159,"@s-batalov, I will take a look soon on how difficult it is to update the CUDA version on our end. However, it's difficult to rebuild the 1.6.1 version with an updated CUDA version, it may have to be the next release where we update the CUDA version.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/844
https://github.com/google/deepvariant/issues/844:225,safety,updat,update,225,"@s-batalov, I will take a look soon on how difficult it is to update the CUDA version on our end. However, it's difficult to rebuild the 1.6.1 version with an updated CUDA version, it may have to be the next release where we update the CUDA version.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/844
https://github.com/google/deepvariant/issues/844:62,security,updat,update,62,"@s-batalov, I will take a look soon on how difficult it is to update the CUDA version on our end. However, it's difficult to rebuild the 1.6.1 version with an updated CUDA version, it may have to be the next release where we update the CUDA version.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/844
https://github.com/google/deepvariant/issues/844:159,security,updat,updated,159,"@s-batalov, I will take a look soon on how difficult it is to update the CUDA version on our end. However, it's difficult to rebuild the 1.6.1 version with an updated CUDA version, it may have to be the next release where we update the CUDA version.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/844
https://github.com/google/deepvariant/issues/844:225,security,updat,update,225,"@s-batalov, I will take a look soon on how difficult it is to update the CUDA version on our end. However, it's difficult to rebuild the 1.6.1 version with an updated CUDA version, it may have to be the next release where we update the CUDA version.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/844
https://github.com/google/deepvariant/issues/844:35,deployability,updat,updated,35,"Hi @s-batalov ,. internally, we've updated to CUDA 11.8 already. Like @kishwarshafin mentioned, this change will show up in the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/844
https://github.com/google/deepvariant/issues/844:133,deployability,releas,release,133,"Hi @s-batalov ,. internally, we've updated to CUDA 11.8 already. Like @kishwarshafin mentioned, this change will show up in the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/844
https://github.com/google/deepvariant/issues/844:35,safety,updat,updated,35,"Hi @s-batalov ,. internally, we've updated to CUDA 11.8 already. Like @kishwarshafin mentioned, this change will show up in the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/844
https://github.com/google/deepvariant/issues/844:35,security,updat,updated,35,"Hi @s-batalov ,. internally, we've updated to CUDA 11.8 already. Like @kishwarshafin mentioned, this change will show up in the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/844
https://github.com/google/deepvariant/issues/844:86,deployability,releas,release,86,"So, as @pichuan said, this is non-actionable for now and will have to wait until next release. So I'll close this bug.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/844
https://github.com/google/deepvariant/issues/844:103,usability,close,close,103,"So, as @pichuan said, this is non-actionable for now and will have to wait until next release. So I'll close this bug.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/844
https://github.com/google/deepvariant/issues/844:298,availability,error,error,298,"Does this fix the mis-linked `libnvinfer_plugin.so.7` dlerror? . Seems like both `libnvinfer_plugin.so.7` and `libnvinfer_plugin.so.8` are in `LD_LIBRARY_PATH`, but the binary `run_deepvariant` is linked against `libnvinfer_plugin.so.7`, which complains about missing `libcublas.so.12`. Here's the error I'm getting:. ```stdout. 2024-10-07 09:10:29.222934: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2024-10-07 09:11:35.925528: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2024-10-07 09:11:35.925571: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. ```. Here's where I found `libnvinfer.so.7`:. ```stdout. [walid@a100: 1.6.1]$ singularity exec --nv deepvariant_1.6.1-gpu.sif ls /usr/local/nvidia/lib. libnvinfer.so.7 libnvinfer.so.8 libnvinfer_plugin.so.7 libnvinfer_plugin.so.8. ```. Here's my `ldd` call to see what it's linked to:. ```stdout. [walid@a100: 1.6.1]$ singularity exec --nv deepvariant_1.6.1-gpu.sif ldd /usr/local/nvidia/lib/libnvinfer_plugin.so.7. 	linux-vdso.so.1 (0x0000155555524000). 	libpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x0000155553038000). 	libdl.so.2 => /lib/x86_64-linux-gnu/libdl.so.2 (0x0000155553032000). 	librt.so.1 => /lib/x86_64-linux-gnu/librt.so.1 (0x0000155553028000). 	libcublas.so.12 => not found. 	libcublasLt.so.12 => ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/844
https://github.com/google/deepvariant/issues/844:557,availability,operat,operations,557,"Does this fix the mis-linked `libnvinfer_plugin.so.7` dlerror? . Seems like both `libnvinfer_plugin.so.7` and `libnvinfer_plugin.so.8` are in `LD_LIBRARY_PATH`, but the binary `run_deepvariant` is linked against `libnvinfer_plugin.so.7`, which complains about missing `libcublas.so.12`. Here's the error I'm getting:. ```stdout. 2024-10-07 09:10:29.222934: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2024-10-07 09:11:35.925528: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2024-10-07 09:11:35.925571: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. ```. Here's where I found `libnvinfer.so.7`:. ```stdout. [walid@a100: 1.6.1]$ singularity exec --nv deepvariant_1.6.1-gpu.sif ls /usr/local/nvidia/lib. libnvinfer.so.7 libnvinfer.so.8 libnvinfer_plugin.so.7 libnvinfer_plugin.so.8. ```. Here's my `ldd` call to see what it's linked to:. ```stdout. [walid@a100: 1.6.1]$ singularity exec --nv deepvariant_1.6.1-gpu.sif ldd /usr/local/nvidia/lib/libnvinfer_plugin.so.7. 	linux-vdso.so.1 (0x0000155555524000). 	libpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x0000155553038000). 	libdl.so.2 => /lib/x86_64-linux-gnu/libdl.so.2 (0x0000155553032000). 	librt.so.1 => /lib/x86_64-linux-gnu/librt.so.1 (0x0000155553028000). 	libcublas.so.12 => not found. 	libcublasLt.so.12 => ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/844
https://github.com/google/deepvariant/issues/844:603,availability,operat,operations,603,"Does this fix the mis-linked `libnvinfer_plugin.so.7` dlerror? . Seems like both `libnvinfer_plugin.so.7` and `libnvinfer_plugin.so.8` are in `LD_LIBRARY_PATH`, but the binary `run_deepvariant` is linked against `libnvinfer_plugin.so.7`, which complains about missing `libcublas.so.12`. Here's the error I'm getting:. ```stdout. 2024-10-07 09:10:29.222934: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2024-10-07 09:11:35.925528: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2024-10-07 09:11:35.925571: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. ```. Here's where I found `libnvinfer.so.7`:. ```stdout. [walid@a100: 1.6.1]$ singularity exec --nv deepvariant_1.6.1-gpu.sif ls /usr/local/nvidia/lib. libnvinfer.so.7 libnvinfer.so.8 libnvinfer_plugin.so.7 libnvinfer_plugin.so.8. ```. Here's my `ldd` call to see what it's linked to:. ```stdout. [walid@a100: 1.6.1]$ singularity exec --nv deepvariant_1.6.1-gpu.sif ldd /usr/local/nvidia/lib/libnvinfer_plugin.so.7. 	linux-vdso.so.1 (0x0000155555524000). 	libpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x0000155553038000). 	libdl.so.2 => /lib/x86_64-linux-gnu/libdl.so.2 (0x0000155553032000). 	librt.so.1 => /lib/x86_64-linux-gnu/librt.so.1 (0x0000155553028000). 	libcublas.so.12 => not found. 	libcublasLt.so.12 => ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/844
https://github.com/google/deepvariant/issues/844:1252,deployability,instal,installed,1252,"ut missing `libcublas.so.12`. Here's the error I'm getting:. ```stdout. 2024-10-07 09:10:29.222934: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2024-10-07 09:11:35.925528: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2024-10-07 09:11:35.925571: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. ```. Here's where I found `libnvinfer.so.7`:. ```stdout. [walid@a100: 1.6.1]$ singularity exec --nv deepvariant_1.6.1-gpu.sif ls /usr/local/nvidia/lib. libnvinfer.so.7 libnvinfer.so.8 libnvinfer_plugin.so.7 libnvinfer_plugin.so.8. ```. Here's my `ldd` call to see what it's linked to:. ```stdout. [walid@a100: 1.6.1]$ singularity exec --nv deepvariant_1.6.1-gpu.sif ldd /usr/local/nvidia/lib/libnvinfer_plugin.so.7. 	linux-vdso.so.1 (0x0000155555524000). 	libpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x0000155553038000). 	libdl.so.2 => /lib/x86_64-linux-gnu/libdl.so.2 (0x0000155553032000). 	librt.so.1 => /lib/x86_64-linux-gnu/librt.so.1 (0x0000155553028000). 	libcublas.so.12 => not found. 	libcublasLt.so.12 => not found. 	libcudnn.so.8 => /lib/x86_64-linux-gnu/libcudnn.so.8 (0x0000155552dfc000). 	libnvinfer.so.8 => /usr/local/nvidia/lib/libnvinfer.so.8 (0x000015554454d000). 	libstdc++.so.6 => /lib/x86_64-linux-gnu/libstdc++.so.6 (0x000015554436b000). 	libm.so.6 =",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/844
https://github.com/google/deepvariant/issues/844:2548,deployability,contain,container,2548,"to:. ```stdout. [walid@a100: 1.6.1]$ singularity exec --nv deepvariant_1.6.1-gpu.sif ldd /usr/local/nvidia/lib/libnvinfer_plugin.so.7. 	linux-vdso.so.1 (0x0000155555524000). 	libpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x0000155553038000). 	libdl.so.2 => /lib/x86_64-linux-gnu/libdl.so.2 (0x0000155553032000). 	librt.so.1 => /lib/x86_64-linux-gnu/librt.so.1 (0x0000155553028000). 	libcublas.so.12 => not found. 	libcublasLt.so.12 => not found. 	libcudnn.so.8 => /lib/x86_64-linux-gnu/libcudnn.so.8 (0x0000155552dfc000). 	libnvinfer.so.8 => /usr/local/nvidia/lib/libnvinfer.so.8 (0x000015554454d000). 	libstdc++.so.6 => /lib/x86_64-linux-gnu/libstdc++.so.6 (0x000015554436b000). 	libm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x000015554421c000). 	libgcc_s.so.1 => /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x0000155544201000). 	libc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x000015554400f000). 	/lib64/ld-linux-x86-64.so.2 (0x0000155555526000). ```. When I grep for `libcublas` in the container:. ```stdout. [walid@a100: 1.6.1]$ singularity exec --nv deepvariant_1.6.1-gpu.sif /usr/bin/grep -r -i 'libcublas' /. /.singularity.d/env/10-docker2singularity.sh:export NV_LIBCUBLAS_PACKAGE_NAME=""${NV_LIBCUBLAS_PACKAGE_NAME:-""libcublas-11-3""}"". /.singularity.d/env/10-docker2singularity.sh:export NV_LIBCUBLAS_VERSION=""${NV_LIBCUBLAS_VERSION:-""11.5.1.109-1""}"". /.singularity.d/env/10-docker2singularity.sh:export NV_LIBCUBLAS_PACKAGE=""${NV_LIBCUBLAS_PACKAGE:-""libcublas-11-3=11.5.1.109-1""}"". /.singularity.d/env/10-docker2singularity.sh:export NV_LIBCUBLAS_DEV_VERSION=""${NV_LIBCUBLAS_DEV_VERSION:-""11.5.1.109-1""}"". /.singularity.d/env/10-docker2singularity.sh:export NV_LIBCUBLAS_DEV_PACKAGE_NAME=""${NV_LIBCUBLAS_DEV_PACKAGE_NAME:-""libcublas-dev-11-3""}"". /.singularity.d/env/10-docker2singularity.sh:export NV_LIBCUBLAS_DEV_PACKAGE=""${NV_LIBCUBLAS_DEV_PACKAGE:-""libcublas-dev-11-3=11.5.1.109-1""}"". *** bunch more omitted output. I just wanted to show above versions ***. ```. I tried to pull an",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/844
https://github.com/google/deepvariant/issues/844:3516,deployability,version,versions,3516,"86_64-linux-gnu/libdl.so.2 (0x0000155553032000). 	librt.so.1 => /lib/x86_64-linux-gnu/librt.so.1 (0x0000155553028000). 	libcublas.so.12 => not found. 	libcublasLt.so.12 => not found. 	libcudnn.so.8 => /lib/x86_64-linux-gnu/libcudnn.so.8 (0x0000155552dfc000). 	libnvinfer.so.8 => /usr/local/nvidia/lib/libnvinfer.so.8 (0x000015554454d000). 	libstdc++.so.6 => /lib/x86_64-linux-gnu/libstdc++.so.6 (0x000015554436b000). 	libm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x000015554421c000). 	libgcc_s.so.1 => /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x0000155544201000). 	libc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x000015554400f000). 	/lib64/ld-linux-x86-64.so.2 (0x0000155555526000). ```. When I grep for `libcublas` in the container:. ```stdout. [walid@a100: 1.6.1]$ singularity exec --nv deepvariant_1.6.1-gpu.sif /usr/bin/grep -r -i 'libcublas' /. /.singularity.d/env/10-docker2singularity.sh:export NV_LIBCUBLAS_PACKAGE_NAME=""${NV_LIBCUBLAS_PACKAGE_NAME:-""libcublas-11-3""}"". /.singularity.d/env/10-docker2singularity.sh:export NV_LIBCUBLAS_VERSION=""${NV_LIBCUBLAS_VERSION:-""11.5.1.109-1""}"". /.singularity.d/env/10-docker2singularity.sh:export NV_LIBCUBLAS_PACKAGE=""${NV_LIBCUBLAS_PACKAGE:-""libcublas-11-3=11.5.1.109-1""}"". /.singularity.d/env/10-docker2singularity.sh:export NV_LIBCUBLAS_DEV_VERSION=""${NV_LIBCUBLAS_DEV_VERSION:-""11.5.1.109-1""}"". /.singularity.d/env/10-docker2singularity.sh:export NV_LIBCUBLAS_DEV_PACKAGE_NAME=""${NV_LIBCUBLAS_DEV_PACKAGE_NAME:-""libcublas-dev-11-3""}"". /.singularity.d/env/10-docker2singularity.sh:export NV_LIBCUBLAS_DEV_PACKAGE=""${NV_LIBCUBLAS_DEV_PACKAGE:-""libcublas-dev-11-3=11.5.1.109-1""}"". *** bunch more omitted output. I just wanted to show above versions ***. ```. I tried to pull and bind a local install of cublas, but I did not want to make my dependencies too complicated. . TL;DR: please test if `libcublas.so.12` is properly linked. Thank you! Thank you all for looking into this. Please let me know if I can test or be of help. :smile:. Cheers,. Walid",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/844
https://github.com/google/deepvariant/issues/844:3568,deployability,instal,install,3568,"86_64-linux-gnu/libdl.so.2 (0x0000155553032000). 	librt.so.1 => /lib/x86_64-linux-gnu/librt.so.1 (0x0000155553028000). 	libcublas.so.12 => not found. 	libcublasLt.so.12 => not found. 	libcudnn.so.8 => /lib/x86_64-linux-gnu/libcudnn.so.8 (0x0000155552dfc000). 	libnvinfer.so.8 => /usr/local/nvidia/lib/libnvinfer.so.8 (0x000015554454d000). 	libstdc++.so.6 => /lib/x86_64-linux-gnu/libstdc++.so.6 (0x000015554436b000). 	libm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x000015554421c000). 	libgcc_s.so.1 => /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x0000155544201000). 	libc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x000015554400f000). 	/lib64/ld-linux-x86-64.so.2 (0x0000155555526000). ```. When I grep for `libcublas` in the container:. ```stdout. [walid@a100: 1.6.1]$ singularity exec --nv deepvariant_1.6.1-gpu.sif /usr/bin/grep -r -i 'libcublas' /. /.singularity.d/env/10-docker2singularity.sh:export NV_LIBCUBLAS_PACKAGE_NAME=""${NV_LIBCUBLAS_PACKAGE_NAME:-""libcublas-11-3""}"". /.singularity.d/env/10-docker2singularity.sh:export NV_LIBCUBLAS_VERSION=""${NV_LIBCUBLAS_VERSION:-""11.5.1.109-1""}"". /.singularity.d/env/10-docker2singularity.sh:export NV_LIBCUBLAS_PACKAGE=""${NV_LIBCUBLAS_PACKAGE:-""libcublas-11-3=11.5.1.109-1""}"". /.singularity.d/env/10-docker2singularity.sh:export NV_LIBCUBLAS_DEV_VERSION=""${NV_LIBCUBLAS_DEV_VERSION:-""11.5.1.109-1""}"". /.singularity.d/env/10-docker2singularity.sh:export NV_LIBCUBLAS_DEV_PACKAGE_NAME=""${NV_LIBCUBLAS_DEV_PACKAGE_NAME:-""libcublas-dev-11-3""}"". /.singularity.d/env/10-docker2singularity.sh:export NV_LIBCUBLAS_DEV_PACKAGE=""${NV_LIBCUBLAS_DEV_PACKAGE:-""libcublas-dev-11-3=11.5.1.109-1""}"". *** bunch more omitted output. I just wanted to show above versions ***. ```. I tried to pull and bind a local install of cublas, but I did not want to make my dependencies too complicated. . TL;DR: please test if `libcublas.so.12` is properly linked. Thank you! Thank you all for looking into this. Please let me know if I can test or be of help. :smile:. Cheers,. Walid",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/844
https://github.com/google/deepvariant/issues/844:3617,deployability,depend,dependencies,3617,"86_64-linux-gnu/libdl.so.2 (0x0000155553032000). 	librt.so.1 => /lib/x86_64-linux-gnu/librt.so.1 (0x0000155553028000). 	libcublas.so.12 => not found. 	libcublasLt.so.12 => not found. 	libcudnn.so.8 => /lib/x86_64-linux-gnu/libcudnn.so.8 (0x0000155552dfc000). 	libnvinfer.so.8 => /usr/local/nvidia/lib/libnvinfer.so.8 (0x000015554454d000). 	libstdc++.so.6 => /lib/x86_64-linux-gnu/libstdc++.so.6 (0x000015554436b000). 	libm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x000015554421c000). 	libgcc_s.so.1 => /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x0000155544201000). 	libc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x000015554400f000). 	/lib64/ld-linux-x86-64.so.2 (0x0000155555526000). ```. When I grep for `libcublas` in the container:. ```stdout. [walid@a100: 1.6.1]$ singularity exec --nv deepvariant_1.6.1-gpu.sif /usr/bin/grep -r -i 'libcublas' /. /.singularity.d/env/10-docker2singularity.sh:export NV_LIBCUBLAS_PACKAGE_NAME=""${NV_LIBCUBLAS_PACKAGE_NAME:-""libcublas-11-3""}"". /.singularity.d/env/10-docker2singularity.sh:export NV_LIBCUBLAS_VERSION=""${NV_LIBCUBLAS_VERSION:-""11.5.1.109-1""}"". /.singularity.d/env/10-docker2singularity.sh:export NV_LIBCUBLAS_PACKAGE=""${NV_LIBCUBLAS_PACKAGE:-""libcublas-11-3=11.5.1.109-1""}"". /.singularity.d/env/10-docker2singularity.sh:export NV_LIBCUBLAS_DEV_VERSION=""${NV_LIBCUBLAS_DEV_VERSION:-""11.5.1.109-1""}"". /.singularity.d/env/10-docker2singularity.sh:export NV_LIBCUBLAS_DEV_PACKAGE_NAME=""${NV_LIBCUBLAS_DEV_PACKAGE_NAME:-""libcublas-dev-11-3""}"". /.singularity.d/env/10-docker2singularity.sh:export NV_LIBCUBLAS_DEV_PACKAGE=""${NV_LIBCUBLAS_DEV_PACKAGE:-""libcublas-dev-11-3=11.5.1.109-1""}"". *** bunch more omitted output. I just wanted to show above versions ***. ```. I tried to pull and bind a local install of cublas, but I did not want to make my dependencies too complicated. . TL;DR: please test if `libcublas.so.12` is properly linked. Thank you! Thank you all for looking into this. Please let me know if I can test or be of help. :smile:. Cheers,. Walid",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/844
https://github.com/google/deepvariant/issues/844:370,energy efficiency,core,core,370,"Does this fix the mis-linked `libnvinfer_plugin.so.7` dlerror? . Seems like both `libnvinfer_plugin.so.7` and `libnvinfer_plugin.so.8` are in `LD_LIBRARY_PATH`, but the binary `run_deepvariant` is linked against `libnvinfer_plugin.so.7`, which complains about missing `libcublas.so.12`. Here's the error I'm getting:. ```stdout. 2024-10-07 09:10:29.222934: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2024-10-07 09:11:35.925528: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2024-10-07 09:11:35.925571: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. ```. Here's where I found `libnvinfer.so.7`:. ```stdout. [walid@a100: 1.6.1]$ singularity exec --nv deepvariant_1.6.1-gpu.sif ls /usr/local/nvidia/lib. libnvinfer.so.7 libnvinfer.so.8 libnvinfer_plugin.so.7 libnvinfer_plugin.so.8. ```. Here's my `ldd` call to see what it's linked to:. ```stdout. [walid@a100: 1.6.1]$ singularity exec --nv deepvariant_1.6.1-gpu.sif ldd /usr/local/nvidia/lib/libnvinfer_plugin.so.7. 	linux-vdso.so.1 (0x0000155555524000). 	libpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x0000155553038000). 	libdl.so.2 => /lib/x86_64-linux-gnu/libdl.so.2 (0x0000155553032000). 	librt.so.1 => /lib/x86_64-linux-gnu/librt.so.1 (0x0000155553028000). 	libcublas.so.12 => not found. 	libcublasLt.so.12 => ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/844
https://github.com/google/deepvariant/issues/844:436,energy efficiency,optim,optimized,436,"Does this fix the mis-linked `libnvinfer_plugin.so.7` dlerror? . Seems like both `libnvinfer_plugin.so.7` and `libnvinfer_plugin.so.8` are in `LD_LIBRARY_PATH`, but the binary `run_deepvariant` is linked against `libnvinfer_plugin.so.7`, which complains about missing `libcublas.so.12`. Here's the error I'm getting:. ```stdout. 2024-10-07 09:10:29.222934: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2024-10-07 09:11:35.925528: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2024-10-07 09:11:35.925571: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. ```. Here's where I found `libnvinfer.so.7`:. ```stdout. [walid@a100: 1.6.1]$ singularity exec --nv deepvariant_1.6.1-gpu.sif ls /usr/local/nvidia/lib. libnvinfer.so.7 libnvinfer.so.8 libnvinfer_plugin.so.7 libnvinfer_plugin.so.8. ```. Here's my `ldd` call to see what it's linked to:. ```stdout. [walid@a100: 1.6.1]$ singularity exec --nv deepvariant_1.6.1-gpu.sif ldd /usr/local/nvidia/lib/libnvinfer_plugin.so.7. 	linux-vdso.so.1 (0x0000155555524000). 	libpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x0000155553038000). 	libdl.so.2 => /lib/x86_64-linux-gnu/libdl.so.2 (0x0000155553032000). 	librt.so.1 => /lib/x86_64-linux-gnu/librt.so.1 (0x0000155553028000). 	libcublas.so.12 => not found. 	libcublasLt.so.12 => ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/844
https://github.com/google/deepvariant/issues/844:516,energy efficiency,CPU,CPU,516,"Does this fix the mis-linked `libnvinfer_plugin.so.7` dlerror? . Seems like both `libnvinfer_plugin.so.7` and `libnvinfer_plugin.so.8` are in `LD_LIBRARY_PATH`, but the binary `run_deepvariant` is linked against `libnvinfer_plugin.so.7`, which complains about missing `libcublas.so.12`. Here's the error I'm getting:. ```stdout. 2024-10-07 09:10:29.222934: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2024-10-07 09:11:35.925528: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2024-10-07 09:11:35.925571: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. ```. Here's where I found `libnvinfer.so.7`:. ```stdout. [walid@a100: 1.6.1]$ singularity exec --nv deepvariant_1.6.1-gpu.sif ls /usr/local/nvidia/lib. libnvinfer.so.7 libnvinfer.so.8 libnvinfer_plugin.so.7 libnvinfer_plugin.so.8. ```. Here's my `ldd` call to see what it's linked to:. ```stdout. [walid@a100: 1.6.1]$ singularity exec --nv deepvariant_1.6.1-gpu.sif ldd /usr/local/nvidia/lib/libnvinfer_plugin.so.7. 	linux-vdso.so.1 (0x0000155555524000). 	libpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x0000155553038000). 	libdl.so.2 => /lib/x86_64-linux-gnu/libdl.so.2 (0x0000155553032000). 	librt.so.1 => /lib/x86_64-linux-gnu/librt.so.1 (0x0000155553028000). 	libcublas.so.12 => not found. 	libcublasLt.so.12 => ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/844
https://github.com/google/deepvariant/issues/844:786,energy efficiency,load,load,786,"Does this fix the mis-linked `libnvinfer_plugin.so.7` dlerror? . Seems like both `libnvinfer_plugin.so.7` and `libnvinfer_plugin.so.8` are in `LD_LIBRARY_PATH`, but the binary `run_deepvariant` is linked against `libnvinfer_plugin.so.7`, which complains about missing `libcublas.so.12`. Here's the error I'm getting:. ```stdout. 2024-10-07 09:10:29.222934: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2024-10-07 09:11:35.925528: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2024-10-07 09:11:35.925571: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. ```. Here's where I found `libnvinfer.so.7`:. ```stdout. [walid@a100: 1.6.1]$ singularity exec --nv deepvariant_1.6.1-gpu.sif ls /usr/local/nvidia/lib. libnvinfer.so.7 libnvinfer.so.8 libnvinfer_plugin.so.7 libnvinfer_plugin.so.8. ```. Here's my `ldd` call to see what it's linked to:. ```stdout. [walid@a100: 1.6.1]$ singularity exec --nv deepvariant_1.6.1-gpu.sif ldd /usr/local/nvidia/lib/libnvinfer_plugin.so.7. 	linux-vdso.so.1 (0x0000155555524000). 	libpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x0000155553038000). 	libdl.so.2 => /lib/x86_64-linux-gnu/libdl.so.2 (0x0000155553032000). 	librt.so.1 => /lib/x86_64-linux-gnu/librt.so.1 (0x0000155553028000). 	libcublas.so.12 => not found. 	libcublasLt.so.12 => ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/844
https://github.com/google/deepvariant/issues/844:1174,energy efficiency,GPU,GPU,1174,"`run_deepvariant` is linked against `libnvinfer_plugin.so.7`, which complains about missing `libcublas.so.12`. Here's the error I'm getting:. ```stdout. 2024-10-07 09:10:29.222934: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2024-10-07 09:11:35.925528: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2024-10-07 09:11:35.925571: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. ```. Here's where I found `libnvinfer.so.7`:. ```stdout. [walid@a100: 1.6.1]$ singularity exec --nv deepvariant_1.6.1-gpu.sif ls /usr/local/nvidia/lib. libnvinfer.so.7 libnvinfer.so.8 libnvinfer_plugin.so.7 libnvinfer_plugin.so.8. ```. Here's my `ldd` call to see what it's linked to:. ```stdout. [walid@a100: 1.6.1]$ singularity exec --nv deepvariant_1.6.1-gpu.sif ldd /usr/local/nvidia/lib/libnvinfer_plugin.so.7. 	linux-vdso.so.1 (0x0000155555524000). 	libpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x0000155553038000). 	libdl.so.2 => /lib/x86_64-linux-gnu/libdl.so.2 (0x0000155553032000). 	librt.so.1 => /lib/x86_64-linux-gnu/librt.so.1 (0x0000155553028000). 	libcublas.so.12 => not found. 	libcublasLt.so.12 => not found. 	libcudnn.so.8 => /lib/x86_64-linux-gnu/libcudnn.so.8 (0x0000155552dfc000). 	libnvinfer.so.8 => /usr/local/nvidia/lib/libnvinfer.so.8 (0x000015554454d000). 	libstdc+",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/844
https://github.com/google/deepvariant/issues/844:1390,energy efficiency,gpu,gpu,1390,"ure_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2024-10-07 09:11:35.925528: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2024-10-07 09:11:35.925571: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. ```. Here's where I found `libnvinfer.so.7`:. ```stdout. [walid@a100: 1.6.1]$ singularity exec --nv deepvariant_1.6.1-gpu.sif ls /usr/local/nvidia/lib. libnvinfer.so.7 libnvinfer.so.8 libnvinfer_plugin.so.7 libnvinfer_plugin.so.8. ```. Here's my `ldd` call to see what it's linked to:. ```stdout. [walid@a100: 1.6.1]$ singularity exec --nv deepvariant_1.6.1-gpu.sif ldd /usr/local/nvidia/lib/libnvinfer_plugin.so.7. 	linux-vdso.so.1 (0x0000155555524000). 	libpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x0000155553038000). 	libdl.so.2 => /lib/x86_64-linux-gnu/libdl.so.2 (0x0000155553032000). 	librt.so.1 => /lib/x86_64-linux-gnu/librt.so.1 (0x0000155553028000). 	libcublas.so.12 => not found. 	libcublasLt.so.12 => not found. 	libcudnn.so.8 => /lib/x86_64-linux-gnu/libcudnn.so.8 (0x0000155552dfc000). 	libnvinfer.so.8 => /usr/local/nvidia/lib/libnvinfer.so.8 (0x000015554454d000). 	libstdc++.so.6 => /lib/x86_64-linux-gnu/libstdc++.so.6 (0x000015554436b000). 	libm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x000015554421c000). 	libgcc_s.so.1 => /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x0000155544201000). 	li",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/844
https://github.com/google/deepvariant/issues/844:1630,energy efficiency,gpu,gpu,1630,"w with the appropriate compiler flags. 2024-10-07 09:11:35.925528: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2024-10-07 09:11:35.925571: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. ```. Here's where I found `libnvinfer.so.7`:. ```stdout. [walid@a100: 1.6.1]$ singularity exec --nv deepvariant_1.6.1-gpu.sif ls /usr/local/nvidia/lib. libnvinfer.so.7 libnvinfer.so.8 libnvinfer_plugin.so.7 libnvinfer_plugin.so.8. ```. Here's my `ldd` call to see what it's linked to:. ```stdout. [walid@a100: 1.6.1]$ singularity exec --nv deepvariant_1.6.1-gpu.sif ldd /usr/local/nvidia/lib/libnvinfer_plugin.so.7. 	linux-vdso.so.1 (0x0000155555524000). 	libpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x0000155553038000). 	libdl.so.2 => /lib/x86_64-linux-gnu/libdl.so.2 (0x0000155553032000). 	librt.so.1 => /lib/x86_64-linux-gnu/librt.so.1 (0x0000155553028000). 	libcublas.so.12 => not found. 	libcublasLt.so.12 => not found. 	libcudnn.so.8 => /lib/x86_64-linux-gnu/libcudnn.so.8 (0x0000155552dfc000). 	libnvinfer.so.8 => /usr/local/nvidia/lib/libnvinfer.so.8 (0x000015554454d000). 	libstdc++.so.6 => /lib/x86_64-linux-gnu/libstdc++.so.6 (0x000015554436b000). 	libm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x000015554421c000). 	libgcc_s.so.1 => /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x0000155544201000). 	libc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x000015554400f000). 	/lib64/ld-linux-x86-64.so.2 (0x0000155555526000). ```. When I grep for `libcublas` in the container:. ```stdout. [walid@a100: 1.6.1]$ singularity exec --nv deepvariant_1.6.1-",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/844
https://github.com/google/deepvariant/issues/844:2632,energy efficiency,gpu,gpu,2632,"sif ldd /usr/local/nvidia/lib/libnvinfer_plugin.so.7. 	linux-vdso.so.1 (0x0000155555524000). 	libpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x0000155553038000). 	libdl.so.2 => /lib/x86_64-linux-gnu/libdl.so.2 (0x0000155553032000). 	librt.so.1 => /lib/x86_64-linux-gnu/librt.so.1 (0x0000155553028000). 	libcublas.so.12 => not found. 	libcublasLt.so.12 => not found. 	libcudnn.so.8 => /lib/x86_64-linux-gnu/libcudnn.so.8 (0x0000155552dfc000). 	libnvinfer.so.8 => /usr/local/nvidia/lib/libnvinfer.so.8 (0x000015554454d000). 	libstdc++.so.6 => /lib/x86_64-linux-gnu/libstdc++.so.6 (0x000015554436b000). 	libm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x000015554421c000). 	libgcc_s.so.1 => /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x0000155544201000). 	libc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x000015554400f000). 	/lib64/ld-linux-x86-64.so.2 (0x0000155555526000). ```. When I grep for `libcublas` in the container:. ```stdout. [walid@a100: 1.6.1]$ singularity exec --nv deepvariant_1.6.1-gpu.sif /usr/bin/grep -r -i 'libcublas' /. /.singularity.d/env/10-docker2singularity.sh:export NV_LIBCUBLAS_PACKAGE_NAME=""${NV_LIBCUBLAS_PACKAGE_NAME:-""libcublas-11-3""}"". /.singularity.d/env/10-docker2singularity.sh:export NV_LIBCUBLAS_VERSION=""${NV_LIBCUBLAS_VERSION:-""11.5.1.109-1""}"". /.singularity.d/env/10-docker2singularity.sh:export NV_LIBCUBLAS_PACKAGE=""${NV_LIBCUBLAS_PACKAGE:-""libcublas-11-3=11.5.1.109-1""}"". /.singularity.d/env/10-docker2singularity.sh:export NV_LIBCUBLAS_DEV_VERSION=""${NV_LIBCUBLAS_DEV_VERSION:-""11.5.1.109-1""}"". /.singularity.d/env/10-docker2singularity.sh:export NV_LIBCUBLAS_DEV_PACKAGE_NAME=""${NV_LIBCUBLAS_DEV_PACKAGE_NAME:-""libcublas-dev-11-3""}"". /.singularity.d/env/10-docker2singularity.sh:export NV_LIBCUBLAS_DEV_PACKAGE=""${NV_LIBCUBLAS_DEV_PACKAGE:-""libcublas-dev-11-3=11.5.1.109-1""}"". *** bunch more omitted output. I just wanted to show above versions ***. ```. I tried to pull and bind a local install of cublas, but I did not want to make my dependencies too ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/844
https://github.com/google/deepvariant/issues/844:3516,integrability,version,versions,3516,"86_64-linux-gnu/libdl.so.2 (0x0000155553032000). 	librt.so.1 => /lib/x86_64-linux-gnu/librt.so.1 (0x0000155553028000). 	libcublas.so.12 => not found. 	libcublasLt.so.12 => not found. 	libcudnn.so.8 => /lib/x86_64-linux-gnu/libcudnn.so.8 (0x0000155552dfc000). 	libnvinfer.so.8 => /usr/local/nvidia/lib/libnvinfer.so.8 (0x000015554454d000). 	libstdc++.so.6 => /lib/x86_64-linux-gnu/libstdc++.so.6 (0x000015554436b000). 	libm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x000015554421c000). 	libgcc_s.so.1 => /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x0000155544201000). 	libc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x000015554400f000). 	/lib64/ld-linux-x86-64.so.2 (0x0000155555526000). ```. When I grep for `libcublas` in the container:. ```stdout. [walid@a100: 1.6.1]$ singularity exec --nv deepvariant_1.6.1-gpu.sif /usr/bin/grep -r -i 'libcublas' /. /.singularity.d/env/10-docker2singularity.sh:export NV_LIBCUBLAS_PACKAGE_NAME=""${NV_LIBCUBLAS_PACKAGE_NAME:-""libcublas-11-3""}"". /.singularity.d/env/10-docker2singularity.sh:export NV_LIBCUBLAS_VERSION=""${NV_LIBCUBLAS_VERSION:-""11.5.1.109-1""}"". /.singularity.d/env/10-docker2singularity.sh:export NV_LIBCUBLAS_PACKAGE=""${NV_LIBCUBLAS_PACKAGE:-""libcublas-11-3=11.5.1.109-1""}"". /.singularity.d/env/10-docker2singularity.sh:export NV_LIBCUBLAS_DEV_VERSION=""${NV_LIBCUBLAS_DEV_VERSION:-""11.5.1.109-1""}"". /.singularity.d/env/10-docker2singularity.sh:export NV_LIBCUBLAS_DEV_PACKAGE_NAME=""${NV_LIBCUBLAS_DEV_PACKAGE_NAME:-""libcublas-dev-11-3""}"". /.singularity.d/env/10-docker2singularity.sh:export NV_LIBCUBLAS_DEV_PACKAGE=""${NV_LIBCUBLAS_DEV_PACKAGE:-""libcublas-dev-11-3=11.5.1.109-1""}"". *** bunch more omitted output. I just wanted to show above versions ***. ```. I tried to pull and bind a local install of cublas, but I did not want to make my dependencies too complicated. . TL;DR: please test if `libcublas.so.12` is properly linked. Thank you! Thank you all for looking into this. Please let me know if I can test or be of help. :smile:. Cheers,. Walid",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/844
https://github.com/google/deepvariant/issues/844:3617,integrability,depend,dependencies,3617,"86_64-linux-gnu/libdl.so.2 (0x0000155553032000). 	librt.so.1 => /lib/x86_64-linux-gnu/librt.so.1 (0x0000155553028000). 	libcublas.so.12 => not found. 	libcublasLt.so.12 => not found. 	libcudnn.so.8 => /lib/x86_64-linux-gnu/libcudnn.so.8 (0x0000155552dfc000). 	libnvinfer.so.8 => /usr/local/nvidia/lib/libnvinfer.so.8 (0x000015554454d000). 	libstdc++.so.6 => /lib/x86_64-linux-gnu/libstdc++.so.6 (0x000015554436b000). 	libm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x000015554421c000). 	libgcc_s.so.1 => /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x0000155544201000). 	libc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x000015554400f000). 	/lib64/ld-linux-x86-64.so.2 (0x0000155555526000). ```. When I grep for `libcublas` in the container:. ```stdout. [walid@a100: 1.6.1]$ singularity exec --nv deepvariant_1.6.1-gpu.sif /usr/bin/grep -r -i 'libcublas' /. /.singularity.d/env/10-docker2singularity.sh:export NV_LIBCUBLAS_PACKAGE_NAME=""${NV_LIBCUBLAS_PACKAGE_NAME:-""libcublas-11-3""}"". /.singularity.d/env/10-docker2singularity.sh:export NV_LIBCUBLAS_VERSION=""${NV_LIBCUBLAS_VERSION:-""11.5.1.109-1""}"". /.singularity.d/env/10-docker2singularity.sh:export NV_LIBCUBLAS_PACKAGE=""${NV_LIBCUBLAS_PACKAGE:-""libcublas-11-3=11.5.1.109-1""}"". /.singularity.d/env/10-docker2singularity.sh:export NV_LIBCUBLAS_DEV_VERSION=""${NV_LIBCUBLAS_DEV_VERSION:-""11.5.1.109-1""}"". /.singularity.d/env/10-docker2singularity.sh:export NV_LIBCUBLAS_DEV_PACKAGE_NAME=""${NV_LIBCUBLAS_DEV_PACKAGE_NAME:-""libcublas-dev-11-3""}"". /.singularity.d/env/10-docker2singularity.sh:export NV_LIBCUBLAS_DEV_PACKAGE=""${NV_LIBCUBLAS_DEV_PACKAGE:-""libcublas-dev-11-3=11.5.1.109-1""}"". *** bunch more omitted output. I just wanted to show above versions ***. ```. I tried to pull and bind a local install of cublas, but I did not want to make my dependencies too complicated. . TL;DR: please test if `libcublas.so.12` is properly linked. Thank you! Thank you all for looking into this. Please let me know if I can test or be of help. :smile:. Cheers,. Walid",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/844
https://github.com/google/deepvariant/issues/844:375,interoperability,platform,platform,375,"Does this fix the mis-linked `libnvinfer_plugin.so.7` dlerror? . Seems like both `libnvinfer_plugin.so.7` and `libnvinfer_plugin.so.8` are in `LD_LIBRARY_PATH`, but the binary `run_deepvariant` is linked against `libnvinfer_plugin.so.7`, which complains about missing `libcublas.so.12`. Here's the error I'm getting:. ```stdout. 2024-10-07 09:10:29.222934: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2024-10-07 09:11:35.925528: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2024-10-07 09:11:35.925571: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. ```. Here's where I found `libnvinfer.so.7`:. ```stdout. [walid@a100: 1.6.1]$ singularity exec --nv deepvariant_1.6.1-gpu.sif ls /usr/local/nvidia/lib. libnvinfer.so.7 libnvinfer.so.8 libnvinfer_plugin.so.7 libnvinfer_plugin.so.8. ```. Here's my `ldd` call to see what it's linked to:. ```stdout. [walid@a100: 1.6.1]$ singularity exec --nv deepvariant_1.6.1-gpu.sif ldd /usr/local/nvidia/lib/libnvinfer_plugin.so.7. 	linux-vdso.so.1 (0x0000155555524000). 	libpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x0000155553038000). 	libdl.so.2 => /lib/x86_64-linux-gnu/libdl.so.2 (0x0000155553032000). 	librt.so.1 => /lib/x86_64-linux-gnu/librt.so.1 (0x0000155553028000). 	libcublas.so.12 => not found. 	libcublasLt.so.12 => ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/844
https://github.com/google/deepvariant/issues/844:741,interoperability,platform,platform,741,"Does this fix the mis-linked `libnvinfer_plugin.so.7` dlerror? . Seems like both `libnvinfer_plugin.so.7` and `libnvinfer_plugin.so.8` are in `LD_LIBRARY_PATH`, but the binary `run_deepvariant` is linked against `libnvinfer_plugin.so.7`, which complains about missing `libcublas.so.12`. Here's the error I'm getting:. ```stdout. 2024-10-07 09:10:29.222934: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2024-10-07 09:11:35.925528: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2024-10-07 09:11:35.925571: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. ```. Here's where I found `libnvinfer.so.7`:. ```stdout. [walid@a100: 1.6.1]$ singularity exec --nv deepvariant_1.6.1-gpu.sif ls /usr/local/nvidia/lib. libnvinfer.so.7 libnvinfer.so.8 libnvinfer_plugin.so.7 libnvinfer_plugin.so.8. ```. Here's my `ldd` call to see what it's linked to:. ```stdout. [walid@a100: 1.6.1]$ singularity exec --nv deepvariant_1.6.1-gpu.sif ldd /usr/local/nvidia/lib/libnvinfer_plugin.so.7. 	linux-vdso.so.1 (0x0000155555524000). 	libpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x0000155553038000). 	libdl.so.2 => /lib/x86_64-linux-gnu/libdl.so.2 (0x0000155553032000). 	librt.so.1 => /lib/x86_64-linux-gnu/librt.so.1 (0x0000155553028000). 	libcublas.so.12 => not found. 	libcublasLt.so.12 => ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/844
https://github.com/google/deepvariant/issues/844:871,interoperability,share,shared,871,"Does this fix the mis-linked `libnvinfer_plugin.so.7` dlerror? . Seems like both `libnvinfer_plugin.so.7` and `libnvinfer_plugin.so.8` are in `LD_LIBRARY_PATH`, but the binary `run_deepvariant` is linked against `libnvinfer_plugin.so.7`, which complains about missing `libcublas.so.12`. Here's the error I'm getting:. ```stdout. 2024-10-07 09:10:29.222934: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2024-10-07 09:11:35.925528: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2024-10-07 09:11:35.925571: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. ```. Here's where I found `libnvinfer.so.7`:. ```stdout. [walid@a100: 1.6.1]$ singularity exec --nv deepvariant_1.6.1-gpu.sif ls /usr/local/nvidia/lib. libnvinfer.so.7 libnvinfer.so.8 libnvinfer_plugin.so.7 libnvinfer_plugin.so.8. ```. Here's my `ldd` call to see what it's linked to:. ```stdout. [walid@a100: 1.6.1]$ singularity exec --nv deepvariant_1.6.1-gpu.sif ldd /usr/local/nvidia/lib/libnvinfer_plugin.so.7. 	linux-vdso.so.1 (0x0000155555524000). 	libpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x0000155553038000). 	libdl.so.2 => /lib/x86_64-linux-gnu/libdl.so.2 (0x0000155553032000). 	librt.so.1 => /lib/x86_64-linux-gnu/librt.so.1 (0x0000155553028000). 	libcublas.so.12 => not found. 	libcublasLt.so.12 => ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/844
https://github.com/google/deepvariant/issues/844:3555,interoperability,bind,bind,3555,"86_64-linux-gnu/libdl.so.2 (0x0000155553032000). 	librt.so.1 => /lib/x86_64-linux-gnu/librt.so.1 (0x0000155553028000). 	libcublas.so.12 => not found. 	libcublasLt.so.12 => not found. 	libcudnn.so.8 => /lib/x86_64-linux-gnu/libcudnn.so.8 (0x0000155552dfc000). 	libnvinfer.so.8 => /usr/local/nvidia/lib/libnvinfer.so.8 (0x000015554454d000). 	libstdc++.so.6 => /lib/x86_64-linux-gnu/libstdc++.so.6 (0x000015554436b000). 	libm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x000015554421c000). 	libgcc_s.so.1 => /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x0000155544201000). 	libc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x000015554400f000). 	/lib64/ld-linux-x86-64.so.2 (0x0000155555526000). ```. When I grep for `libcublas` in the container:. ```stdout. [walid@a100: 1.6.1]$ singularity exec --nv deepvariant_1.6.1-gpu.sif /usr/bin/grep -r -i 'libcublas' /. /.singularity.d/env/10-docker2singularity.sh:export NV_LIBCUBLAS_PACKAGE_NAME=""${NV_LIBCUBLAS_PACKAGE_NAME:-""libcublas-11-3""}"". /.singularity.d/env/10-docker2singularity.sh:export NV_LIBCUBLAS_VERSION=""${NV_LIBCUBLAS_VERSION:-""11.5.1.109-1""}"". /.singularity.d/env/10-docker2singularity.sh:export NV_LIBCUBLAS_PACKAGE=""${NV_LIBCUBLAS_PACKAGE:-""libcublas-11-3=11.5.1.109-1""}"". /.singularity.d/env/10-docker2singularity.sh:export NV_LIBCUBLAS_DEV_VERSION=""${NV_LIBCUBLAS_DEV_VERSION:-""11.5.1.109-1""}"". /.singularity.d/env/10-docker2singularity.sh:export NV_LIBCUBLAS_DEV_PACKAGE_NAME=""${NV_LIBCUBLAS_DEV_PACKAGE_NAME:-""libcublas-dev-11-3""}"". /.singularity.d/env/10-docker2singularity.sh:export NV_LIBCUBLAS_DEV_PACKAGE=""${NV_LIBCUBLAS_DEV_PACKAGE:-""libcublas-dev-11-3=11.5.1.109-1""}"". *** bunch more omitted output. I just wanted to show above versions ***. ```. I tried to pull and bind a local install of cublas, but I did not want to make my dependencies too complicated. . TL;DR: please test if `libcublas.so.12` is properly linked. Thank you! Thank you all for looking into this. Please let me know if I can test or be of help. :smile:. Cheers,. Walid",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/844
https://github.com/google/deepvariant/issues/844:3516,modifiability,version,versions,3516,"86_64-linux-gnu/libdl.so.2 (0x0000155553032000). 	librt.so.1 => /lib/x86_64-linux-gnu/librt.so.1 (0x0000155553028000). 	libcublas.so.12 => not found. 	libcublasLt.so.12 => not found. 	libcudnn.so.8 => /lib/x86_64-linux-gnu/libcudnn.so.8 (0x0000155552dfc000). 	libnvinfer.so.8 => /usr/local/nvidia/lib/libnvinfer.so.8 (0x000015554454d000). 	libstdc++.so.6 => /lib/x86_64-linux-gnu/libstdc++.so.6 (0x000015554436b000). 	libm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x000015554421c000). 	libgcc_s.so.1 => /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x0000155544201000). 	libc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x000015554400f000). 	/lib64/ld-linux-x86-64.so.2 (0x0000155555526000). ```. When I grep for `libcublas` in the container:. ```stdout. [walid@a100: 1.6.1]$ singularity exec --nv deepvariant_1.6.1-gpu.sif /usr/bin/grep -r -i 'libcublas' /. /.singularity.d/env/10-docker2singularity.sh:export NV_LIBCUBLAS_PACKAGE_NAME=""${NV_LIBCUBLAS_PACKAGE_NAME:-""libcublas-11-3""}"". /.singularity.d/env/10-docker2singularity.sh:export NV_LIBCUBLAS_VERSION=""${NV_LIBCUBLAS_VERSION:-""11.5.1.109-1""}"". /.singularity.d/env/10-docker2singularity.sh:export NV_LIBCUBLAS_PACKAGE=""${NV_LIBCUBLAS_PACKAGE:-""libcublas-11-3=11.5.1.109-1""}"". /.singularity.d/env/10-docker2singularity.sh:export NV_LIBCUBLAS_DEV_VERSION=""${NV_LIBCUBLAS_DEV_VERSION:-""11.5.1.109-1""}"". /.singularity.d/env/10-docker2singularity.sh:export NV_LIBCUBLAS_DEV_PACKAGE_NAME=""${NV_LIBCUBLAS_DEV_PACKAGE_NAME:-""libcublas-dev-11-3""}"". /.singularity.d/env/10-docker2singularity.sh:export NV_LIBCUBLAS_DEV_PACKAGE=""${NV_LIBCUBLAS_DEV_PACKAGE:-""libcublas-dev-11-3=11.5.1.109-1""}"". *** bunch more omitted output. I just wanted to show above versions ***. ```. I tried to pull and bind a local install of cublas, but I did not want to make my dependencies too complicated. . TL;DR: please test if `libcublas.so.12` is properly linked. Thank you! Thank you all for looking into this. Please let me know if I can test or be of help. :smile:. Cheers,. Walid",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/844
https://github.com/google/deepvariant/issues/844:3555,modifiability,bind,bind,3555,"86_64-linux-gnu/libdl.so.2 (0x0000155553032000). 	librt.so.1 => /lib/x86_64-linux-gnu/librt.so.1 (0x0000155553028000). 	libcublas.so.12 => not found. 	libcublasLt.so.12 => not found. 	libcudnn.so.8 => /lib/x86_64-linux-gnu/libcudnn.so.8 (0x0000155552dfc000). 	libnvinfer.so.8 => /usr/local/nvidia/lib/libnvinfer.so.8 (0x000015554454d000). 	libstdc++.so.6 => /lib/x86_64-linux-gnu/libstdc++.so.6 (0x000015554436b000). 	libm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x000015554421c000). 	libgcc_s.so.1 => /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x0000155544201000). 	libc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x000015554400f000). 	/lib64/ld-linux-x86-64.so.2 (0x0000155555526000). ```. When I grep for `libcublas` in the container:. ```stdout. [walid@a100: 1.6.1]$ singularity exec --nv deepvariant_1.6.1-gpu.sif /usr/bin/grep -r -i 'libcublas' /. /.singularity.d/env/10-docker2singularity.sh:export NV_LIBCUBLAS_PACKAGE_NAME=""${NV_LIBCUBLAS_PACKAGE_NAME:-""libcublas-11-3""}"". /.singularity.d/env/10-docker2singularity.sh:export NV_LIBCUBLAS_VERSION=""${NV_LIBCUBLAS_VERSION:-""11.5.1.109-1""}"". /.singularity.d/env/10-docker2singularity.sh:export NV_LIBCUBLAS_PACKAGE=""${NV_LIBCUBLAS_PACKAGE:-""libcublas-11-3=11.5.1.109-1""}"". /.singularity.d/env/10-docker2singularity.sh:export NV_LIBCUBLAS_DEV_VERSION=""${NV_LIBCUBLAS_DEV_VERSION:-""11.5.1.109-1""}"". /.singularity.d/env/10-docker2singularity.sh:export NV_LIBCUBLAS_DEV_PACKAGE_NAME=""${NV_LIBCUBLAS_DEV_PACKAGE_NAME:-""libcublas-dev-11-3""}"". /.singularity.d/env/10-docker2singularity.sh:export NV_LIBCUBLAS_DEV_PACKAGE=""${NV_LIBCUBLAS_DEV_PACKAGE:-""libcublas-dev-11-3=11.5.1.109-1""}"". *** bunch more omitted output. I just wanted to show above versions ***. ```. I tried to pull and bind a local install of cublas, but I did not want to make my dependencies too complicated. . TL;DR: please test if `libcublas.so.12` is properly linked. Thank you! Thank you all for looking into this. Please let me know if I can test or be of help. :smile:. Cheers,. Walid",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/844
https://github.com/google/deepvariant/issues/844:3617,modifiability,depend,dependencies,3617,"86_64-linux-gnu/libdl.so.2 (0x0000155553032000). 	librt.so.1 => /lib/x86_64-linux-gnu/librt.so.1 (0x0000155553028000). 	libcublas.so.12 => not found. 	libcublasLt.so.12 => not found. 	libcudnn.so.8 => /lib/x86_64-linux-gnu/libcudnn.so.8 (0x0000155552dfc000). 	libnvinfer.so.8 => /usr/local/nvidia/lib/libnvinfer.so.8 (0x000015554454d000). 	libstdc++.so.6 => /lib/x86_64-linux-gnu/libstdc++.so.6 (0x000015554436b000). 	libm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x000015554421c000). 	libgcc_s.so.1 => /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x0000155544201000). 	libc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x000015554400f000). 	/lib64/ld-linux-x86-64.so.2 (0x0000155555526000). ```. When I grep for `libcublas` in the container:. ```stdout. [walid@a100: 1.6.1]$ singularity exec --nv deepvariant_1.6.1-gpu.sif /usr/bin/grep -r -i 'libcublas' /. /.singularity.d/env/10-docker2singularity.sh:export NV_LIBCUBLAS_PACKAGE_NAME=""${NV_LIBCUBLAS_PACKAGE_NAME:-""libcublas-11-3""}"". /.singularity.d/env/10-docker2singularity.sh:export NV_LIBCUBLAS_VERSION=""${NV_LIBCUBLAS_VERSION:-""11.5.1.109-1""}"". /.singularity.d/env/10-docker2singularity.sh:export NV_LIBCUBLAS_PACKAGE=""${NV_LIBCUBLAS_PACKAGE:-""libcublas-11-3=11.5.1.109-1""}"". /.singularity.d/env/10-docker2singularity.sh:export NV_LIBCUBLAS_DEV_VERSION=""${NV_LIBCUBLAS_DEV_VERSION:-""11.5.1.109-1""}"". /.singularity.d/env/10-docker2singularity.sh:export NV_LIBCUBLAS_DEV_PACKAGE_NAME=""${NV_LIBCUBLAS_DEV_PACKAGE_NAME:-""libcublas-dev-11-3""}"". /.singularity.d/env/10-docker2singularity.sh:export NV_LIBCUBLAS_DEV_PACKAGE=""${NV_LIBCUBLAS_DEV_PACKAGE:-""libcublas-dev-11-3=11.5.1.109-1""}"". *** bunch more omitted output. I just wanted to show above versions ***. ```. I tried to pull and bind a local install of cublas, but I did not want to make my dependencies too complicated. . TL;DR: please test if `libcublas.so.12` is properly linked. Thank you! Thank you all for looking into this. Please let me know if I can test or be of help. :smile:. Cheers,. Walid",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/844
https://github.com/google/deepvariant/issues/844:298,performance,error,error,298,"Does this fix the mis-linked `libnvinfer_plugin.so.7` dlerror? . Seems like both `libnvinfer_plugin.so.7` and `libnvinfer_plugin.so.8` are in `LD_LIBRARY_PATH`, but the binary `run_deepvariant` is linked against `libnvinfer_plugin.so.7`, which complains about missing `libcublas.so.12`. Here's the error I'm getting:. ```stdout. 2024-10-07 09:10:29.222934: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2024-10-07 09:11:35.925528: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2024-10-07 09:11:35.925571: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. ```. Here's where I found `libnvinfer.so.7`:. ```stdout. [walid@a100: 1.6.1]$ singularity exec --nv deepvariant_1.6.1-gpu.sif ls /usr/local/nvidia/lib. libnvinfer.so.7 libnvinfer.so.8 libnvinfer_plugin.so.7 libnvinfer_plugin.so.8. ```. Here's my `ldd` call to see what it's linked to:. ```stdout. [walid@a100: 1.6.1]$ singularity exec --nv deepvariant_1.6.1-gpu.sif ldd /usr/local/nvidia/lib/libnvinfer_plugin.so.7. 	linux-vdso.so.1 (0x0000155555524000). 	libpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x0000155553038000). 	libdl.so.2 => /lib/x86_64-linux-gnu/libdl.so.2 (0x0000155553032000). 	librt.so.1 => /lib/x86_64-linux-gnu/librt.so.1 (0x0000155553028000). 	libcublas.so.12 => not found. 	libcublasLt.so.12 => ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/844
https://github.com/google/deepvariant/issues/844:436,performance,optimiz,optimized,436,"Does this fix the mis-linked `libnvinfer_plugin.so.7` dlerror? . Seems like both `libnvinfer_plugin.so.7` and `libnvinfer_plugin.so.8` are in `LD_LIBRARY_PATH`, but the binary `run_deepvariant` is linked against `libnvinfer_plugin.so.7`, which complains about missing `libcublas.so.12`. Here's the error I'm getting:. ```stdout. 2024-10-07 09:10:29.222934: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2024-10-07 09:11:35.925528: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2024-10-07 09:11:35.925571: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. ```. Here's where I found `libnvinfer.so.7`:. ```stdout. [walid@a100: 1.6.1]$ singularity exec --nv deepvariant_1.6.1-gpu.sif ls /usr/local/nvidia/lib. libnvinfer.so.7 libnvinfer.so.8 libnvinfer_plugin.so.7 libnvinfer_plugin.so.8. ```. Here's my `ldd` call to see what it's linked to:. ```stdout. [walid@a100: 1.6.1]$ singularity exec --nv deepvariant_1.6.1-gpu.sif ldd /usr/local/nvidia/lib/libnvinfer_plugin.so.7. 	linux-vdso.so.1 (0x0000155555524000). 	libpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x0000155553038000). 	libdl.so.2 => /lib/x86_64-linux-gnu/libdl.so.2 (0x0000155553032000). 	librt.so.1 => /lib/x86_64-linux-gnu/librt.so.1 (0x0000155553028000). 	libcublas.so.12 => not found. 	libcublasLt.so.12 => ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/844
https://github.com/google/deepvariant/issues/844:470,performance,Network,Network,470,"Does this fix the mis-linked `libnvinfer_plugin.so.7` dlerror? . Seems like both `libnvinfer_plugin.so.7` and `libnvinfer_plugin.so.8` are in `LD_LIBRARY_PATH`, but the binary `run_deepvariant` is linked against `libnvinfer_plugin.so.7`, which complains about missing `libcublas.so.12`. Here's the error I'm getting:. ```stdout. 2024-10-07 09:10:29.222934: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2024-10-07 09:11:35.925528: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2024-10-07 09:11:35.925571: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. ```. Here's where I found `libnvinfer.so.7`:. ```stdout. [walid@a100: 1.6.1]$ singularity exec --nv deepvariant_1.6.1-gpu.sif ls /usr/local/nvidia/lib. libnvinfer.so.7 libnvinfer.so.8 libnvinfer_plugin.so.7 libnvinfer_plugin.so.8. ```. Here's my `ldd` call to see what it's linked to:. ```stdout. [walid@a100: 1.6.1]$ singularity exec --nv deepvariant_1.6.1-gpu.sif ldd /usr/local/nvidia/lib/libnvinfer_plugin.so.7. 	linux-vdso.so.1 (0x0000155555524000). 	libpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x0000155553038000). 	libdl.so.2 => /lib/x86_64-linux-gnu/libdl.so.2 (0x0000155553032000). 	librt.so.1 => /lib/x86_64-linux-gnu/librt.so.1 (0x0000155553028000). 	libcublas.so.12 => not found. 	libcublasLt.so.12 => ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/844
https://github.com/google/deepvariant/issues/844:516,performance,CPU,CPU,516,"Does this fix the mis-linked `libnvinfer_plugin.so.7` dlerror? . Seems like both `libnvinfer_plugin.so.7` and `libnvinfer_plugin.so.8` are in `LD_LIBRARY_PATH`, but the binary `run_deepvariant` is linked against `libnvinfer_plugin.so.7`, which complains about missing `libcublas.so.12`. Here's the error I'm getting:. ```stdout. 2024-10-07 09:10:29.222934: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2024-10-07 09:11:35.925528: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2024-10-07 09:11:35.925571: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. ```. Here's where I found `libnvinfer.so.7`:. ```stdout. [walid@a100: 1.6.1]$ singularity exec --nv deepvariant_1.6.1-gpu.sif ls /usr/local/nvidia/lib. libnvinfer.so.7 libnvinfer.so.8 libnvinfer_plugin.so.7 libnvinfer_plugin.so.8. ```. Here's my `ldd` call to see what it's linked to:. ```stdout. [walid@a100: 1.6.1]$ singularity exec --nv deepvariant_1.6.1-gpu.sif ldd /usr/local/nvidia/lib/libnvinfer_plugin.so.7. 	linux-vdso.so.1 (0x0000155555524000). 	libpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x0000155553038000). 	libdl.so.2 => /lib/x86_64-linux-gnu/libdl.so.2 (0x0000155553032000). 	librt.so.1 => /lib/x86_64-linux-gnu/librt.so.1 (0x0000155553028000). 	libcublas.so.12 => not found. 	libcublasLt.so.12 => ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/844
https://github.com/google/deepvariant/issues/844:536,performance,perform,performance-critical,536,"Does this fix the mis-linked `libnvinfer_plugin.so.7` dlerror? . Seems like both `libnvinfer_plugin.so.7` and `libnvinfer_plugin.so.8` are in `LD_LIBRARY_PATH`, but the binary `run_deepvariant` is linked against `libnvinfer_plugin.so.7`, which complains about missing `libcublas.so.12`. Here's the error I'm getting:. ```stdout. 2024-10-07 09:10:29.222934: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2024-10-07 09:11:35.925528: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2024-10-07 09:11:35.925571: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. ```. Here's where I found `libnvinfer.so.7`:. ```stdout. [walid@a100: 1.6.1]$ singularity exec --nv deepvariant_1.6.1-gpu.sif ls /usr/local/nvidia/lib. libnvinfer.so.7 libnvinfer.so.8 libnvinfer_plugin.so.7 libnvinfer_plugin.so.8. ```. Here's my `ldd` call to see what it's linked to:. ```stdout. [walid@a100: 1.6.1]$ singularity exec --nv deepvariant_1.6.1-gpu.sif ldd /usr/local/nvidia/lib/libnvinfer_plugin.so.7. 	linux-vdso.so.1 (0x0000155555524000). 	libpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x0000155553038000). 	libdl.so.2 => /lib/x86_64-linux-gnu/libdl.so.2 (0x0000155553032000). 	librt.so.1 => /lib/x86_64-linux-gnu/librt.so.1 (0x0000155553028000). 	libcublas.so.12 => not found. 	libcublasLt.so.12 => ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/844
https://github.com/google/deepvariant/issues/844:786,performance,load,load,786,"Does this fix the mis-linked `libnvinfer_plugin.so.7` dlerror? . Seems like both `libnvinfer_plugin.so.7` and `libnvinfer_plugin.so.8` are in `LD_LIBRARY_PATH`, but the binary `run_deepvariant` is linked against `libnvinfer_plugin.so.7`, which complains about missing `libcublas.so.12`. Here's the error I'm getting:. ```stdout. 2024-10-07 09:10:29.222934: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2024-10-07 09:11:35.925528: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2024-10-07 09:11:35.925571: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. ```. Here's where I found `libnvinfer.so.7`:. ```stdout. [walid@a100: 1.6.1]$ singularity exec --nv deepvariant_1.6.1-gpu.sif ls /usr/local/nvidia/lib. libnvinfer.so.7 libnvinfer.so.8 libnvinfer_plugin.so.7 libnvinfer_plugin.so.8. ```. Here's my `ldd` call to see what it's linked to:. ```stdout. [walid@a100: 1.6.1]$ singularity exec --nv deepvariant_1.6.1-gpu.sif ldd /usr/local/nvidia/lib/libnvinfer_plugin.so.7. 	linux-vdso.so.1 (0x0000155555524000). 	libpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x0000155553038000). 	libdl.so.2 => /lib/x86_64-linux-gnu/libdl.so.2 (0x0000155553032000). 	librt.so.1 => /lib/x86_64-linux-gnu/librt.so.1 (0x0000155553028000). 	libcublas.so.12 => not found. 	libcublasLt.so.12 => ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/844
https://github.com/google/deepvariant/issues/844:1174,performance,GPU,GPU,1174,"`run_deepvariant` is linked against `libnvinfer_plugin.so.7`, which complains about missing `libcublas.so.12`. Here's the error I'm getting:. ```stdout. 2024-10-07 09:10:29.222934: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2024-10-07 09:11:35.925528: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2024-10-07 09:11:35.925571: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. ```. Here's where I found `libnvinfer.so.7`:. ```stdout. [walid@a100: 1.6.1]$ singularity exec --nv deepvariant_1.6.1-gpu.sif ls /usr/local/nvidia/lib. libnvinfer.so.7 libnvinfer.so.8 libnvinfer_plugin.so.7 libnvinfer_plugin.so.8. ```. Here's my `ldd` call to see what it's linked to:. ```stdout. [walid@a100: 1.6.1]$ singularity exec --nv deepvariant_1.6.1-gpu.sif ldd /usr/local/nvidia/lib/libnvinfer_plugin.so.7. 	linux-vdso.so.1 (0x0000155555524000). 	libpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x0000155553038000). 	libdl.so.2 => /lib/x86_64-linux-gnu/libdl.so.2 (0x0000155553032000). 	librt.so.1 => /lib/x86_64-linux-gnu/librt.so.1 (0x0000155553028000). 	libcublas.so.12 => not found. 	libcublasLt.so.12 => not found. 	libcudnn.so.8 => /lib/x86_64-linux-gnu/libcudnn.so.8 (0x0000155552dfc000). 	libnvinfer.so.8 => /usr/local/nvidia/lib/libnvinfer.so.8 (0x000015554454d000). 	libstdc+",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/844
https://github.com/google/deepvariant/issues/844:1390,performance,gpu,gpu,1390,"ure_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2024-10-07 09:11:35.925528: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2024-10-07 09:11:35.925571: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. ```. Here's where I found `libnvinfer.so.7`:. ```stdout. [walid@a100: 1.6.1]$ singularity exec --nv deepvariant_1.6.1-gpu.sif ls /usr/local/nvidia/lib. libnvinfer.so.7 libnvinfer.so.8 libnvinfer_plugin.so.7 libnvinfer_plugin.so.8. ```. Here's my `ldd` call to see what it's linked to:. ```stdout. [walid@a100: 1.6.1]$ singularity exec --nv deepvariant_1.6.1-gpu.sif ldd /usr/local/nvidia/lib/libnvinfer_plugin.so.7. 	linux-vdso.so.1 (0x0000155555524000). 	libpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x0000155553038000). 	libdl.so.2 => /lib/x86_64-linux-gnu/libdl.so.2 (0x0000155553032000). 	librt.so.1 => /lib/x86_64-linux-gnu/librt.so.1 (0x0000155553028000). 	libcublas.so.12 => not found. 	libcublasLt.so.12 => not found. 	libcudnn.so.8 => /lib/x86_64-linux-gnu/libcudnn.so.8 (0x0000155552dfc000). 	libnvinfer.so.8 => /usr/local/nvidia/lib/libnvinfer.so.8 (0x000015554454d000). 	libstdc++.so.6 => /lib/x86_64-linux-gnu/libstdc++.so.6 (0x000015554436b000). 	libm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x000015554421c000). 	libgcc_s.so.1 => /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x0000155544201000). 	li",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/844
https://github.com/google/deepvariant/issues/844:1630,performance,gpu,gpu,1630,"w with the appropriate compiler flags. 2024-10-07 09:11:35.925528: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2024-10-07 09:11:35.925571: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. ```. Here's where I found `libnvinfer.so.7`:. ```stdout. [walid@a100: 1.6.1]$ singularity exec --nv deepvariant_1.6.1-gpu.sif ls /usr/local/nvidia/lib. libnvinfer.so.7 libnvinfer.so.8 libnvinfer_plugin.so.7 libnvinfer_plugin.so.8. ```. Here's my `ldd` call to see what it's linked to:. ```stdout. [walid@a100: 1.6.1]$ singularity exec --nv deepvariant_1.6.1-gpu.sif ldd /usr/local/nvidia/lib/libnvinfer_plugin.so.7. 	linux-vdso.so.1 (0x0000155555524000). 	libpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x0000155553038000). 	libdl.so.2 => /lib/x86_64-linux-gnu/libdl.so.2 (0x0000155553032000). 	librt.so.1 => /lib/x86_64-linux-gnu/librt.so.1 (0x0000155553028000). 	libcublas.so.12 => not found. 	libcublasLt.so.12 => not found. 	libcudnn.so.8 => /lib/x86_64-linux-gnu/libcudnn.so.8 (0x0000155552dfc000). 	libnvinfer.so.8 => /usr/local/nvidia/lib/libnvinfer.so.8 (0x000015554454d000). 	libstdc++.so.6 => /lib/x86_64-linux-gnu/libstdc++.so.6 (0x000015554436b000). 	libm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x000015554421c000). 	libgcc_s.so.1 => /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x0000155544201000). 	libc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x000015554400f000). 	/lib64/ld-linux-x86-64.so.2 (0x0000155555526000). ```. When I grep for `libcublas` in the container:. ```stdout. [walid@a100: 1.6.1]$ singularity exec --nv deepvariant_1.6.1-",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/844
https://github.com/google/deepvariant/issues/844:2632,performance,gpu,gpu,2632,"sif ldd /usr/local/nvidia/lib/libnvinfer_plugin.so.7. 	linux-vdso.so.1 (0x0000155555524000). 	libpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x0000155553038000). 	libdl.so.2 => /lib/x86_64-linux-gnu/libdl.so.2 (0x0000155553032000). 	librt.so.1 => /lib/x86_64-linux-gnu/librt.so.1 (0x0000155553028000). 	libcublas.so.12 => not found. 	libcublasLt.so.12 => not found. 	libcudnn.so.8 => /lib/x86_64-linux-gnu/libcudnn.so.8 (0x0000155552dfc000). 	libnvinfer.so.8 => /usr/local/nvidia/lib/libnvinfer.so.8 (0x000015554454d000). 	libstdc++.so.6 => /lib/x86_64-linux-gnu/libstdc++.so.6 (0x000015554436b000). 	libm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x000015554421c000). 	libgcc_s.so.1 => /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x0000155544201000). 	libc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x000015554400f000). 	/lib64/ld-linux-x86-64.so.2 (0x0000155555526000). ```. When I grep for `libcublas` in the container:. ```stdout. [walid@a100: 1.6.1]$ singularity exec --nv deepvariant_1.6.1-gpu.sif /usr/bin/grep -r -i 'libcublas' /. /.singularity.d/env/10-docker2singularity.sh:export NV_LIBCUBLAS_PACKAGE_NAME=""${NV_LIBCUBLAS_PACKAGE_NAME:-""libcublas-11-3""}"". /.singularity.d/env/10-docker2singularity.sh:export NV_LIBCUBLAS_VERSION=""${NV_LIBCUBLAS_VERSION:-""11.5.1.109-1""}"". /.singularity.d/env/10-docker2singularity.sh:export NV_LIBCUBLAS_PACKAGE=""${NV_LIBCUBLAS_PACKAGE:-""libcublas-11-3=11.5.1.109-1""}"". /.singularity.d/env/10-docker2singularity.sh:export NV_LIBCUBLAS_DEV_VERSION=""${NV_LIBCUBLAS_DEV_VERSION:-""11.5.1.109-1""}"". /.singularity.d/env/10-docker2singularity.sh:export NV_LIBCUBLAS_DEV_PACKAGE_NAME=""${NV_LIBCUBLAS_DEV_PACKAGE_NAME:-""libcublas-dev-11-3""}"". /.singularity.d/env/10-docker2singularity.sh:export NV_LIBCUBLAS_DEV_PACKAGE=""${NV_LIBCUBLAS_DEV_PACKAGE:-""libcublas-dev-11-3=11.5.1.109-1""}"". *** bunch more omitted output. I just wanted to show above versions ***. ```. I tried to pull and bind a local install of cublas, but I did not want to make my dependencies too ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/844
https://github.com/google/deepvariant/issues/844:0,reliability,Doe,Does,0,"Does this fix the mis-linked `libnvinfer_plugin.so.7` dlerror? . Seems like both `libnvinfer_plugin.so.7` and `libnvinfer_plugin.so.8` are in `LD_LIBRARY_PATH`, but the binary `run_deepvariant` is linked against `libnvinfer_plugin.so.7`, which complains about missing `libcublas.so.12`. Here's the error I'm getting:. ```stdout. 2024-10-07 09:10:29.222934: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2024-10-07 09:11:35.925528: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2024-10-07 09:11:35.925571: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. ```. Here's where I found `libnvinfer.so.7`:. ```stdout. [walid@a100: 1.6.1]$ singularity exec --nv deepvariant_1.6.1-gpu.sif ls /usr/local/nvidia/lib. libnvinfer.so.7 libnvinfer.so.8 libnvinfer_plugin.so.7 libnvinfer_plugin.so.8. ```. Here's my `ldd` call to see what it's linked to:. ```stdout. [walid@a100: 1.6.1]$ singularity exec --nv deepvariant_1.6.1-gpu.sif ldd /usr/local/nvidia/lib/libnvinfer_plugin.so.7. 	linux-vdso.so.1 (0x0000155555524000). 	libpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x0000155553038000). 	libdl.so.2 => /lib/x86_64-linux-gnu/libdl.so.2 (0x0000155553032000). 	librt.so.1 => /lib/x86_64-linux-gnu/librt.so.1 (0x0000155553028000). 	libcublas.so.12 => not found. 	libcublasLt.so.12 => ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/844
https://github.com/google/deepvariant/issues/844:244,safety,compl,complains,244,"Does this fix the mis-linked `libnvinfer_plugin.so.7` dlerror? . Seems like both `libnvinfer_plugin.so.7` and `libnvinfer_plugin.so.8` are in `LD_LIBRARY_PATH`, but the binary `run_deepvariant` is linked against `libnvinfer_plugin.so.7`, which complains about missing `libcublas.so.12`. Here's the error I'm getting:. ```stdout. 2024-10-07 09:10:29.222934: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2024-10-07 09:11:35.925528: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2024-10-07 09:11:35.925571: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. ```. Here's where I found `libnvinfer.so.7`:. ```stdout. [walid@a100: 1.6.1]$ singularity exec --nv deepvariant_1.6.1-gpu.sif ls /usr/local/nvidia/lib. libnvinfer.so.7 libnvinfer.so.8 libnvinfer_plugin.so.7 libnvinfer_plugin.so.8. ```. Here's my `ldd` call to see what it's linked to:. ```stdout. [walid@a100: 1.6.1]$ singularity exec --nv deepvariant_1.6.1-gpu.sif ldd /usr/local/nvidia/lib/libnvinfer_plugin.so.7. 	linux-vdso.so.1 (0x0000155555524000). 	libpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x0000155553038000). 	libdl.so.2 => /lib/x86_64-linux-gnu/libdl.so.2 (0x0000155553032000). 	librt.so.1 => /lib/x86_64-linux-gnu/librt.so.1 (0x0000155553028000). 	libcublas.so.12 => not found. 	libcublasLt.so.12 => ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/844
https://github.com/google/deepvariant/issues/844:298,safety,error,error,298,"Does this fix the mis-linked `libnvinfer_plugin.so.7` dlerror? . Seems like both `libnvinfer_plugin.so.7` and `libnvinfer_plugin.so.8` are in `LD_LIBRARY_PATH`, but the binary `run_deepvariant` is linked against `libnvinfer_plugin.so.7`, which complains about missing `libcublas.so.12`. Here's the error I'm getting:. ```stdout. 2024-10-07 09:10:29.222934: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2024-10-07 09:11:35.925528: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2024-10-07 09:11:35.925571: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. ```. Here's where I found `libnvinfer.so.7`:. ```stdout. [walid@a100: 1.6.1]$ singularity exec --nv deepvariant_1.6.1-gpu.sif ls /usr/local/nvidia/lib. libnvinfer.so.7 libnvinfer.so.8 libnvinfer_plugin.so.7 libnvinfer_plugin.so.8. ```. Here's my `ldd` call to see what it's linked to:. ```stdout. [walid@a100: 1.6.1]$ singularity exec --nv deepvariant_1.6.1-gpu.sif ldd /usr/local/nvidia/lib/libnvinfer_plugin.so.7. 	linux-vdso.so.1 (0x0000155555524000). 	libpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x0000155553038000). 	libdl.so.2 => /lib/x86_64-linux-gnu/libdl.so.2 (0x0000155553032000). 	librt.so.1 => /lib/x86_64-linux-gnu/librt.so.1 (0x0000155553028000). 	libcublas.so.12 => not found. 	libcublasLt.so.12 => ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/844
https://github.com/google/deepvariant/issues/844:3617,safety,depend,dependencies,3617,"86_64-linux-gnu/libdl.so.2 (0x0000155553032000). 	librt.so.1 => /lib/x86_64-linux-gnu/librt.so.1 (0x0000155553028000). 	libcublas.so.12 => not found. 	libcublasLt.so.12 => not found. 	libcudnn.so.8 => /lib/x86_64-linux-gnu/libcudnn.so.8 (0x0000155552dfc000). 	libnvinfer.so.8 => /usr/local/nvidia/lib/libnvinfer.so.8 (0x000015554454d000). 	libstdc++.so.6 => /lib/x86_64-linux-gnu/libstdc++.so.6 (0x000015554436b000). 	libm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x000015554421c000). 	libgcc_s.so.1 => /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x0000155544201000). 	libc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x000015554400f000). 	/lib64/ld-linux-x86-64.so.2 (0x0000155555526000). ```. When I grep for `libcublas` in the container:. ```stdout. [walid@a100: 1.6.1]$ singularity exec --nv deepvariant_1.6.1-gpu.sif /usr/bin/grep -r -i 'libcublas' /. /.singularity.d/env/10-docker2singularity.sh:export NV_LIBCUBLAS_PACKAGE_NAME=""${NV_LIBCUBLAS_PACKAGE_NAME:-""libcublas-11-3""}"". /.singularity.d/env/10-docker2singularity.sh:export NV_LIBCUBLAS_VERSION=""${NV_LIBCUBLAS_VERSION:-""11.5.1.109-1""}"". /.singularity.d/env/10-docker2singularity.sh:export NV_LIBCUBLAS_PACKAGE=""${NV_LIBCUBLAS_PACKAGE:-""libcublas-11-3=11.5.1.109-1""}"". /.singularity.d/env/10-docker2singularity.sh:export NV_LIBCUBLAS_DEV_VERSION=""${NV_LIBCUBLAS_DEV_VERSION:-""11.5.1.109-1""}"". /.singularity.d/env/10-docker2singularity.sh:export NV_LIBCUBLAS_DEV_PACKAGE_NAME=""${NV_LIBCUBLAS_DEV_PACKAGE_NAME:-""libcublas-dev-11-3""}"". /.singularity.d/env/10-docker2singularity.sh:export NV_LIBCUBLAS_DEV_PACKAGE=""${NV_LIBCUBLAS_DEV_PACKAGE:-""libcublas-dev-11-3=11.5.1.109-1""}"". *** bunch more omitted output. I just wanted to show above versions ***. ```. I tried to pull and bind a local install of cublas, but I did not want to make my dependencies too complicated. . TL;DR: please test if `libcublas.so.12` is properly linked. Thank you! Thank you all for looking into this. Please let me know if I can test or be of help. :smile:. Cheers,. Walid",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/844
https://github.com/google/deepvariant/issues/844:3634,safety,compl,complicated,3634,"86_64-linux-gnu/libdl.so.2 (0x0000155553032000). 	librt.so.1 => /lib/x86_64-linux-gnu/librt.so.1 (0x0000155553028000). 	libcublas.so.12 => not found. 	libcublasLt.so.12 => not found. 	libcudnn.so.8 => /lib/x86_64-linux-gnu/libcudnn.so.8 (0x0000155552dfc000). 	libnvinfer.so.8 => /usr/local/nvidia/lib/libnvinfer.so.8 (0x000015554454d000). 	libstdc++.so.6 => /lib/x86_64-linux-gnu/libstdc++.so.6 (0x000015554436b000). 	libm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x000015554421c000). 	libgcc_s.so.1 => /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x0000155544201000). 	libc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x000015554400f000). 	/lib64/ld-linux-x86-64.so.2 (0x0000155555526000). ```. When I grep for `libcublas` in the container:. ```stdout. [walid@a100: 1.6.1]$ singularity exec --nv deepvariant_1.6.1-gpu.sif /usr/bin/grep -r -i 'libcublas' /. /.singularity.d/env/10-docker2singularity.sh:export NV_LIBCUBLAS_PACKAGE_NAME=""${NV_LIBCUBLAS_PACKAGE_NAME:-""libcublas-11-3""}"". /.singularity.d/env/10-docker2singularity.sh:export NV_LIBCUBLAS_VERSION=""${NV_LIBCUBLAS_VERSION:-""11.5.1.109-1""}"". /.singularity.d/env/10-docker2singularity.sh:export NV_LIBCUBLAS_PACKAGE=""${NV_LIBCUBLAS_PACKAGE:-""libcublas-11-3=11.5.1.109-1""}"". /.singularity.d/env/10-docker2singularity.sh:export NV_LIBCUBLAS_DEV_VERSION=""${NV_LIBCUBLAS_DEV_VERSION:-""11.5.1.109-1""}"". /.singularity.d/env/10-docker2singularity.sh:export NV_LIBCUBLAS_DEV_PACKAGE_NAME=""${NV_LIBCUBLAS_DEV_PACKAGE_NAME:-""libcublas-dev-11-3""}"". /.singularity.d/env/10-docker2singularity.sh:export NV_LIBCUBLAS_DEV_PACKAGE=""${NV_LIBCUBLAS_DEV_PACKAGE:-""libcublas-dev-11-3=11.5.1.109-1""}"". *** bunch more omitted output. I just wanted to show above versions ***. ```. I tried to pull and bind a local install of cublas, but I did not want to make my dependencies too complicated. . TL;DR: please test if `libcublas.so.12` is properly linked. Thank you! Thank you all for looking into this. Please let me know if I can test or be of help. :smile:. Cheers,. Walid",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/844
https://github.com/google/deepvariant/issues/844:3663,safety,test,test,3663,"86_64-linux-gnu/libdl.so.2 (0x0000155553032000). 	librt.so.1 => /lib/x86_64-linux-gnu/librt.so.1 (0x0000155553028000). 	libcublas.so.12 => not found. 	libcublasLt.so.12 => not found. 	libcudnn.so.8 => /lib/x86_64-linux-gnu/libcudnn.so.8 (0x0000155552dfc000). 	libnvinfer.so.8 => /usr/local/nvidia/lib/libnvinfer.so.8 (0x000015554454d000). 	libstdc++.so.6 => /lib/x86_64-linux-gnu/libstdc++.so.6 (0x000015554436b000). 	libm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x000015554421c000). 	libgcc_s.so.1 => /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x0000155544201000). 	libc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x000015554400f000). 	/lib64/ld-linux-x86-64.so.2 (0x0000155555526000). ```. When I grep for `libcublas` in the container:. ```stdout. [walid@a100: 1.6.1]$ singularity exec --nv deepvariant_1.6.1-gpu.sif /usr/bin/grep -r -i 'libcublas' /. /.singularity.d/env/10-docker2singularity.sh:export NV_LIBCUBLAS_PACKAGE_NAME=""${NV_LIBCUBLAS_PACKAGE_NAME:-""libcublas-11-3""}"". /.singularity.d/env/10-docker2singularity.sh:export NV_LIBCUBLAS_VERSION=""${NV_LIBCUBLAS_VERSION:-""11.5.1.109-1""}"". /.singularity.d/env/10-docker2singularity.sh:export NV_LIBCUBLAS_PACKAGE=""${NV_LIBCUBLAS_PACKAGE:-""libcublas-11-3=11.5.1.109-1""}"". /.singularity.d/env/10-docker2singularity.sh:export NV_LIBCUBLAS_DEV_VERSION=""${NV_LIBCUBLAS_DEV_VERSION:-""11.5.1.109-1""}"". /.singularity.d/env/10-docker2singularity.sh:export NV_LIBCUBLAS_DEV_PACKAGE_NAME=""${NV_LIBCUBLAS_DEV_PACKAGE_NAME:-""libcublas-dev-11-3""}"". /.singularity.d/env/10-docker2singularity.sh:export NV_LIBCUBLAS_DEV_PACKAGE=""${NV_LIBCUBLAS_DEV_PACKAGE:-""libcublas-dev-11-3=11.5.1.109-1""}"". *** bunch more omitted output. I just wanted to show above versions ***. ```. I tried to pull and bind a local install of cublas, but I did not want to make my dependencies too complicated. . TL;DR: please test if `libcublas.so.12` is properly linked. Thank you! Thank you all for looking into this. Please let me know if I can test or be of help. :smile:. Cheers,. Walid",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/844
https://github.com/google/deepvariant/issues/844:3785,safety,test,test,3785,"86_64-linux-gnu/libdl.so.2 (0x0000155553032000). 	librt.so.1 => /lib/x86_64-linux-gnu/librt.so.1 (0x0000155553028000). 	libcublas.so.12 => not found. 	libcublasLt.so.12 => not found. 	libcudnn.so.8 => /lib/x86_64-linux-gnu/libcudnn.so.8 (0x0000155552dfc000). 	libnvinfer.so.8 => /usr/local/nvidia/lib/libnvinfer.so.8 (0x000015554454d000). 	libstdc++.so.6 => /lib/x86_64-linux-gnu/libstdc++.so.6 (0x000015554436b000). 	libm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x000015554421c000). 	libgcc_s.so.1 => /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x0000155544201000). 	libc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x000015554400f000). 	/lib64/ld-linux-x86-64.so.2 (0x0000155555526000). ```. When I grep for `libcublas` in the container:. ```stdout. [walid@a100: 1.6.1]$ singularity exec --nv deepvariant_1.6.1-gpu.sif /usr/bin/grep -r -i 'libcublas' /. /.singularity.d/env/10-docker2singularity.sh:export NV_LIBCUBLAS_PACKAGE_NAME=""${NV_LIBCUBLAS_PACKAGE_NAME:-""libcublas-11-3""}"". /.singularity.d/env/10-docker2singularity.sh:export NV_LIBCUBLAS_VERSION=""${NV_LIBCUBLAS_VERSION:-""11.5.1.109-1""}"". /.singularity.d/env/10-docker2singularity.sh:export NV_LIBCUBLAS_PACKAGE=""${NV_LIBCUBLAS_PACKAGE:-""libcublas-11-3=11.5.1.109-1""}"". /.singularity.d/env/10-docker2singularity.sh:export NV_LIBCUBLAS_DEV_VERSION=""${NV_LIBCUBLAS_DEV_VERSION:-""11.5.1.109-1""}"". /.singularity.d/env/10-docker2singularity.sh:export NV_LIBCUBLAS_DEV_PACKAGE_NAME=""${NV_LIBCUBLAS_DEV_PACKAGE_NAME:-""libcublas-dev-11-3""}"". /.singularity.d/env/10-docker2singularity.sh:export NV_LIBCUBLAS_DEV_PACKAGE=""${NV_LIBCUBLAS_DEV_PACKAGE:-""libcublas-dev-11-3=11.5.1.109-1""}"". *** bunch more omitted output. I just wanted to show above versions ***. ```. I tried to pull and bind a local install of cublas, but I did not want to make my dependencies too complicated. . TL;DR: please test if `libcublas.so.12` is properly linked. Thank you! Thank you all for looking into this. Please let me know if I can test or be of help. :smile:. Cheers,. Walid",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/844
https://github.com/google/deepvariant/issues/844:244,security,compl,complains,244,"Does this fix the mis-linked `libnvinfer_plugin.so.7` dlerror? . Seems like both `libnvinfer_plugin.so.7` and `libnvinfer_plugin.so.8` are in `LD_LIBRARY_PATH`, but the binary `run_deepvariant` is linked against `libnvinfer_plugin.so.7`, which complains about missing `libcublas.so.12`. Here's the error I'm getting:. ```stdout. 2024-10-07 09:10:29.222934: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2024-10-07 09:11:35.925528: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2024-10-07 09:11:35.925571: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. ```. Here's where I found `libnvinfer.so.7`:. ```stdout. [walid@a100: 1.6.1]$ singularity exec --nv deepvariant_1.6.1-gpu.sif ls /usr/local/nvidia/lib. libnvinfer.so.7 libnvinfer.so.8 libnvinfer_plugin.so.7 libnvinfer_plugin.so.8. ```. Here's my `ldd` call to see what it's linked to:. ```stdout. [walid@a100: 1.6.1]$ singularity exec --nv deepvariant_1.6.1-gpu.sif ldd /usr/local/nvidia/lib/libnvinfer_plugin.so.7. 	linux-vdso.so.1 (0x0000155555524000). 	libpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x0000155553038000). 	libdl.so.2 => /lib/x86_64-linux-gnu/libdl.so.2 (0x0000155553032000). 	librt.so.1 => /lib/x86_64-linux-gnu/librt.so.1 (0x0000155553028000). 	libcublas.so.12 => not found. 	libcublasLt.so.12 => ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/844
https://github.com/google/deepvariant/issues/844:470,security,Network,Network,470,"Does this fix the mis-linked `libnvinfer_plugin.so.7` dlerror? . Seems like both `libnvinfer_plugin.so.7` and `libnvinfer_plugin.so.8` are in `LD_LIBRARY_PATH`, but the binary `run_deepvariant` is linked against `libnvinfer_plugin.so.7`, which complains about missing `libcublas.so.12`. Here's the error I'm getting:. ```stdout. 2024-10-07 09:10:29.222934: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2024-10-07 09:11:35.925528: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2024-10-07 09:11:35.925571: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. ```. Here's where I found `libnvinfer.so.7`:. ```stdout. [walid@a100: 1.6.1]$ singularity exec --nv deepvariant_1.6.1-gpu.sif ls /usr/local/nvidia/lib. libnvinfer.so.7 libnvinfer.so.8 libnvinfer_plugin.so.7 libnvinfer_plugin.so.8. ```. Here's my `ldd` call to see what it's linked to:. ```stdout. [walid@a100: 1.6.1]$ singularity exec --nv deepvariant_1.6.1-gpu.sif ldd /usr/local/nvidia/lib/libnvinfer_plugin.so.7. 	linux-vdso.so.1 (0x0000155555524000). 	libpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x0000155553038000). 	libdl.so.2 => /lib/x86_64-linux-gnu/libdl.so.2 (0x0000155553032000). 	librt.so.1 => /lib/x86_64-linux-gnu/librt.so.1 (0x0000155553028000). 	libcublas.so.12 => not found. 	libcublasLt.so.12 => ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/844
https://github.com/google/deepvariant/issues/844:3634,security,compl,complicated,3634,"86_64-linux-gnu/libdl.so.2 (0x0000155553032000). 	librt.so.1 => /lib/x86_64-linux-gnu/librt.so.1 (0x0000155553028000). 	libcublas.so.12 => not found. 	libcublasLt.so.12 => not found. 	libcudnn.so.8 => /lib/x86_64-linux-gnu/libcudnn.so.8 (0x0000155552dfc000). 	libnvinfer.so.8 => /usr/local/nvidia/lib/libnvinfer.so.8 (0x000015554454d000). 	libstdc++.so.6 => /lib/x86_64-linux-gnu/libstdc++.so.6 (0x000015554436b000). 	libm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x000015554421c000). 	libgcc_s.so.1 => /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x0000155544201000). 	libc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x000015554400f000). 	/lib64/ld-linux-x86-64.so.2 (0x0000155555526000). ```. When I grep for `libcublas` in the container:. ```stdout. [walid@a100: 1.6.1]$ singularity exec --nv deepvariant_1.6.1-gpu.sif /usr/bin/grep -r -i 'libcublas' /. /.singularity.d/env/10-docker2singularity.sh:export NV_LIBCUBLAS_PACKAGE_NAME=""${NV_LIBCUBLAS_PACKAGE_NAME:-""libcublas-11-3""}"". /.singularity.d/env/10-docker2singularity.sh:export NV_LIBCUBLAS_VERSION=""${NV_LIBCUBLAS_VERSION:-""11.5.1.109-1""}"". /.singularity.d/env/10-docker2singularity.sh:export NV_LIBCUBLAS_PACKAGE=""${NV_LIBCUBLAS_PACKAGE:-""libcublas-11-3=11.5.1.109-1""}"". /.singularity.d/env/10-docker2singularity.sh:export NV_LIBCUBLAS_DEV_VERSION=""${NV_LIBCUBLAS_DEV_VERSION:-""11.5.1.109-1""}"". /.singularity.d/env/10-docker2singularity.sh:export NV_LIBCUBLAS_DEV_PACKAGE_NAME=""${NV_LIBCUBLAS_DEV_PACKAGE_NAME:-""libcublas-dev-11-3""}"". /.singularity.d/env/10-docker2singularity.sh:export NV_LIBCUBLAS_DEV_PACKAGE=""${NV_LIBCUBLAS_DEV_PACKAGE:-""libcublas-dev-11-3=11.5.1.109-1""}"". *** bunch more omitted output. I just wanted to show above versions ***. ```. I tried to pull and bind a local install of cublas, but I did not want to make my dependencies too complicated. . TL;DR: please test if `libcublas.so.12` is properly linked. Thank you! Thank you all for looking into this. Please let me know if I can test or be of help. :smile:. Cheers,. Walid",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/844
https://github.com/google/deepvariant/issues/844:3617,testability,depend,dependencies,3617,"86_64-linux-gnu/libdl.so.2 (0x0000155553032000). 	librt.so.1 => /lib/x86_64-linux-gnu/librt.so.1 (0x0000155553028000). 	libcublas.so.12 => not found. 	libcublasLt.so.12 => not found. 	libcudnn.so.8 => /lib/x86_64-linux-gnu/libcudnn.so.8 (0x0000155552dfc000). 	libnvinfer.so.8 => /usr/local/nvidia/lib/libnvinfer.so.8 (0x000015554454d000). 	libstdc++.so.6 => /lib/x86_64-linux-gnu/libstdc++.so.6 (0x000015554436b000). 	libm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x000015554421c000). 	libgcc_s.so.1 => /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x0000155544201000). 	libc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x000015554400f000). 	/lib64/ld-linux-x86-64.so.2 (0x0000155555526000). ```. When I grep for `libcublas` in the container:. ```stdout. [walid@a100: 1.6.1]$ singularity exec --nv deepvariant_1.6.1-gpu.sif /usr/bin/grep -r -i 'libcublas' /. /.singularity.d/env/10-docker2singularity.sh:export NV_LIBCUBLAS_PACKAGE_NAME=""${NV_LIBCUBLAS_PACKAGE_NAME:-""libcublas-11-3""}"". /.singularity.d/env/10-docker2singularity.sh:export NV_LIBCUBLAS_VERSION=""${NV_LIBCUBLAS_VERSION:-""11.5.1.109-1""}"". /.singularity.d/env/10-docker2singularity.sh:export NV_LIBCUBLAS_PACKAGE=""${NV_LIBCUBLAS_PACKAGE:-""libcublas-11-3=11.5.1.109-1""}"". /.singularity.d/env/10-docker2singularity.sh:export NV_LIBCUBLAS_DEV_VERSION=""${NV_LIBCUBLAS_DEV_VERSION:-""11.5.1.109-1""}"". /.singularity.d/env/10-docker2singularity.sh:export NV_LIBCUBLAS_DEV_PACKAGE_NAME=""${NV_LIBCUBLAS_DEV_PACKAGE_NAME:-""libcublas-dev-11-3""}"". /.singularity.d/env/10-docker2singularity.sh:export NV_LIBCUBLAS_DEV_PACKAGE=""${NV_LIBCUBLAS_DEV_PACKAGE:-""libcublas-dev-11-3=11.5.1.109-1""}"". *** bunch more omitted output. I just wanted to show above versions ***. ```. I tried to pull and bind a local install of cublas, but I did not want to make my dependencies too complicated. . TL;DR: please test if `libcublas.so.12` is properly linked. Thank you! Thank you all for looking into this. Please let me know if I can test or be of help. :smile:. Cheers,. Walid",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/844
https://github.com/google/deepvariant/issues/844:3663,testability,test,test,3663,"86_64-linux-gnu/libdl.so.2 (0x0000155553032000). 	librt.so.1 => /lib/x86_64-linux-gnu/librt.so.1 (0x0000155553028000). 	libcublas.so.12 => not found. 	libcublasLt.so.12 => not found. 	libcudnn.so.8 => /lib/x86_64-linux-gnu/libcudnn.so.8 (0x0000155552dfc000). 	libnvinfer.so.8 => /usr/local/nvidia/lib/libnvinfer.so.8 (0x000015554454d000). 	libstdc++.so.6 => /lib/x86_64-linux-gnu/libstdc++.so.6 (0x000015554436b000). 	libm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x000015554421c000). 	libgcc_s.so.1 => /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x0000155544201000). 	libc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x000015554400f000). 	/lib64/ld-linux-x86-64.so.2 (0x0000155555526000). ```. When I grep for `libcublas` in the container:. ```stdout. [walid@a100: 1.6.1]$ singularity exec --nv deepvariant_1.6.1-gpu.sif /usr/bin/grep -r -i 'libcublas' /. /.singularity.d/env/10-docker2singularity.sh:export NV_LIBCUBLAS_PACKAGE_NAME=""${NV_LIBCUBLAS_PACKAGE_NAME:-""libcublas-11-3""}"". /.singularity.d/env/10-docker2singularity.sh:export NV_LIBCUBLAS_VERSION=""${NV_LIBCUBLAS_VERSION:-""11.5.1.109-1""}"". /.singularity.d/env/10-docker2singularity.sh:export NV_LIBCUBLAS_PACKAGE=""${NV_LIBCUBLAS_PACKAGE:-""libcublas-11-3=11.5.1.109-1""}"". /.singularity.d/env/10-docker2singularity.sh:export NV_LIBCUBLAS_DEV_VERSION=""${NV_LIBCUBLAS_DEV_VERSION:-""11.5.1.109-1""}"". /.singularity.d/env/10-docker2singularity.sh:export NV_LIBCUBLAS_DEV_PACKAGE_NAME=""${NV_LIBCUBLAS_DEV_PACKAGE_NAME:-""libcublas-dev-11-3""}"". /.singularity.d/env/10-docker2singularity.sh:export NV_LIBCUBLAS_DEV_PACKAGE=""${NV_LIBCUBLAS_DEV_PACKAGE:-""libcublas-dev-11-3=11.5.1.109-1""}"". *** bunch more omitted output. I just wanted to show above versions ***. ```. I tried to pull and bind a local install of cublas, but I did not want to make my dependencies too complicated. . TL;DR: please test if `libcublas.so.12` is properly linked. Thank you! Thank you all for looking into this. Please let me know if I can test or be of help. :smile:. Cheers,. Walid",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/844
https://github.com/google/deepvariant/issues/844:3785,testability,test,test,3785,"86_64-linux-gnu/libdl.so.2 (0x0000155553032000). 	librt.so.1 => /lib/x86_64-linux-gnu/librt.so.1 (0x0000155553028000). 	libcublas.so.12 => not found. 	libcublasLt.so.12 => not found. 	libcudnn.so.8 => /lib/x86_64-linux-gnu/libcudnn.so.8 (0x0000155552dfc000). 	libnvinfer.so.8 => /usr/local/nvidia/lib/libnvinfer.so.8 (0x000015554454d000). 	libstdc++.so.6 => /lib/x86_64-linux-gnu/libstdc++.so.6 (0x000015554436b000). 	libm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x000015554421c000). 	libgcc_s.so.1 => /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x0000155544201000). 	libc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x000015554400f000). 	/lib64/ld-linux-x86-64.so.2 (0x0000155555526000). ```. When I grep for `libcublas` in the container:. ```stdout. [walid@a100: 1.6.1]$ singularity exec --nv deepvariant_1.6.1-gpu.sif /usr/bin/grep -r -i 'libcublas' /. /.singularity.d/env/10-docker2singularity.sh:export NV_LIBCUBLAS_PACKAGE_NAME=""${NV_LIBCUBLAS_PACKAGE_NAME:-""libcublas-11-3""}"". /.singularity.d/env/10-docker2singularity.sh:export NV_LIBCUBLAS_VERSION=""${NV_LIBCUBLAS_VERSION:-""11.5.1.109-1""}"". /.singularity.d/env/10-docker2singularity.sh:export NV_LIBCUBLAS_PACKAGE=""${NV_LIBCUBLAS_PACKAGE:-""libcublas-11-3=11.5.1.109-1""}"". /.singularity.d/env/10-docker2singularity.sh:export NV_LIBCUBLAS_DEV_VERSION=""${NV_LIBCUBLAS_DEV_VERSION:-""11.5.1.109-1""}"". /.singularity.d/env/10-docker2singularity.sh:export NV_LIBCUBLAS_DEV_PACKAGE_NAME=""${NV_LIBCUBLAS_DEV_PACKAGE_NAME:-""libcublas-dev-11-3""}"". /.singularity.d/env/10-docker2singularity.sh:export NV_LIBCUBLAS_DEV_PACKAGE=""${NV_LIBCUBLAS_DEV_PACKAGE:-""libcublas-dev-11-3=11.5.1.109-1""}"". *** bunch more omitted output. I just wanted to show above versions ***. ```. I tried to pull and bind a local install of cublas, but I did not want to make my dependencies too complicated. . TL;DR: please test if `libcublas.so.12` is properly linked. Thank you! Thank you all for looking into this. Please let me know if I can test or be of help. :smile:. Cheers,. Walid",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/844
https://github.com/google/deepvariant/issues/844:298,usability,error,error,298,"Does this fix the mis-linked `libnvinfer_plugin.so.7` dlerror? . Seems like both `libnvinfer_plugin.so.7` and `libnvinfer_plugin.so.8` are in `LD_LIBRARY_PATH`, but the binary `run_deepvariant` is linked against `libnvinfer_plugin.so.7`, which complains about missing `libcublas.so.12`. Here's the error I'm getting:. ```stdout. 2024-10-07 09:10:29.222934: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2024-10-07 09:11:35.925528: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2024-10-07 09:11:35.925571: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. ```. Here's where I found `libnvinfer.so.7`:. ```stdout. [walid@a100: 1.6.1]$ singularity exec --nv deepvariant_1.6.1-gpu.sif ls /usr/local/nvidia/lib. libnvinfer.so.7 libnvinfer.so.8 libnvinfer_plugin.so.7 libnvinfer_plugin.so.8. ```. Here's my `ldd` call to see what it's linked to:. ```stdout. [walid@a100: 1.6.1]$ singularity exec --nv deepvariant_1.6.1-gpu.sif ldd /usr/local/nvidia/lib/libnvinfer_plugin.so.7. 	linux-vdso.so.1 (0x0000155555524000). 	libpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x0000155553038000). 	libdl.so.2 => /lib/x86_64-linux-gnu/libdl.so.2 (0x0000155553032000). 	librt.so.1 => /lib/x86_64-linux-gnu/librt.so.1 (0x0000155553028000). 	libcublas.so.12 => not found. 	libcublasLt.so.12 => ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/844
https://github.com/google/deepvariant/issues/844:536,usability,perform,performance-critical,536,"Does this fix the mis-linked `libnvinfer_plugin.so.7` dlerror? . Seems like both `libnvinfer_plugin.so.7` and `libnvinfer_plugin.so.8` are in `LD_LIBRARY_PATH`, but the binary `run_deepvariant` is linked against `libnvinfer_plugin.so.7`, which complains about missing `libcublas.so.12`. Here's the error I'm getting:. ```stdout. 2024-10-07 09:10:29.222934: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2024-10-07 09:11:35.925528: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2024-10-07 09:11:35.925571: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. ```. Here's where I found `libnvinfer.so.7`:. ```stdout. [walid@a100: 1.6.1]$ singularity exec --nv deepvariant_1.6.1-gpu.sif ls /usr/local/nvidia/lib. libnvinfer.so.7 libnvinfer.so.8 libnvinfer_plugin.so.7 libnvinfer_plugin.so.8. ```. Here's my `ldd` call to see what it's linked to:. ```stdout. [walid@a100: 1.6.1]$ singularity exec --nv deepvariant_1.6.1-gpu.sif ldd /usr/local/nvidia/lib/libnvinfer_plugin.so.7. 	linux-vdso.so.1 (0x0000155555524000). 	libpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x0000155553038000). 	libdl.so.2 => /lib/x86_64-linux-gnu/libdl.so.2 (0x0000155553032000). 	librt.so.1 => /lib/x86_64-linux-gnu/librt.so.1 (0x0000155553028000). 	libcublas.so.12 => not found. 	libcublasLt.so.12 => ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/844
https://github.com/google/deepvariant/issues/844:3799,usability,help,help,3799,"86_64-linux-gnu/libdl.so.2 (0x0000155553032000). 	librt.so.1 => /lib/x86_64-linux-gnu/librt.so.1 (0x0000155553028000). 	libcublas.so.12 => not found. 	libcublasLt.so.12 => not found. 	libcudnn.so.8 => /lib/x86_64-linux-gnu/libcudnn.so.8 (0x0000155552dfc000). 	libnvinfer.so.8 => /usr/local/nvidia/lib/libnvinfer.so.8 (0x000015554454d000). 	libstdc++.so.6 => /lib/x86_64-linux-gnu/libstdc++.so.6 (0x000015554436b000). 	libm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x000015554421c000). 	libgcc_s.so.1 => /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x0000155544201000). 	libc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x000015554400f000). 	/lib64/ld-linux-x86-64.so.2 (0x0000155555526000). ```. When I grep for `libcublas` in the container:. ```stdout. [walid@a100: 1.6.1]$ singularity exec --nv deepvariant_1.6.1-gpu.sif /usr/bin/grep -r -i 'libcublas' /. /.singularity.d/env/10-docker2singularity.sh:export NV_LIBCUBLAS_PACKAGE_NAME=""${NV_LIBCUBLAS_PACKAGE_NAME:-""libcublas-11-3""}"". /.singularity.d/env/10-docker2singularity.sh:export NV_LIBCUBLAS_VERSION=""${NV_LIBCUBLAS_VERSION:-""11.5.1.109-1""}"". /.singularity.d/env/10-docker2singularity.sh:export NV_LIBCUBLAS_PACKAGE=""${NV_LIBCUBLAS_PACKAGE:-""libcublas-11-3=11.5.1.109-1""}"". /.singularity.d/env/10-docker2singularity.sh:export NV_LIBCUBLAS_DEV_VERSION=""${NV_LIBCUBLAS_DEV_VERSION:-""11.5.1.109-1""}"". /.singularity.d/env/10-docker2singularity.sh:export NV_LIBCUBLAS_DEV_PACKAGE_NAME=""${NV_LIBCUBLAS_DEV_PACKAGE_NAME:-""libcublas-dev-11-3""}"". /.singularity.d/env/10-docker2singularity.sh:export NV_LIBCUBLAS_DEV_PACKAGE=""${NV_LIBCUBLAS_DEV_PACKAGE:-""libcublas-dev-11-3=11.5.1.109-1""}"". *** bunch more omitted output. I just wanted to show above versions ***. ```. I tried to pull and bind a local install of cublas, but I did not want to make my dependencies too complicated. . TL;DR: please test if `libcublas.so.12` is properly linked. Thank you! Thank you all for looking into this. Please let me know if I can test or be of help. :smile:. Cheers,. Walid",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/844
https://github.com/google/deepvariant/issues/845:49,deployability,releas,released,49,"@esraaelmligy ,. The RNAseq model is trained and released for DeepVariant 1.4.0, please see [this documentation](https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-rnaseq-case-study.md#running-deepvariant-rna-seq-on-a-cpu-only-machine) on how to run it. Please change `deepvariant:latest` to `deepvariant:1.4.0`.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/845
https://github.com/google/deepvariant/issues/845:28,energy efficiency,model,model,28,"@esraaelmligy ,. The RNAseq model is trained and released for DeepVariant 1.4.0, please see [this documentation](https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-rnaseq-case-study.md#running-deepvariant-rna-seq-on-a-cpu-only-machine) on how to run it. Please change `deepvariant:latest` to `deepvariant:1.4.0`.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/845
https://github.com/google/deepvariant/issues/845:234,energy efficiency,cpu,cpu-only-machine,234,"@esraaelmligy ,. The RNAseq model is trained and released for DeepVariant 1.4.0, please see [this documentation](https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-rnaseq-case-study.md#running-deepvariant-rna-seq-on-a-cpu-only-machine) on how to run it. Please change `deepvariant:latest` to `deepvariant:1.4.0`.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/845
https://github.com/google/deepvariant/issues/845:234,performance,cpu,cpu-only-machine,234,"@esraaelmligy ,. The RNAseq model is trained and released for DeepVariant 1.4.0, please see [this documentation](https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-rnaseq-case-study.md#running-deepvariant-rna-seq-on-a-cpu-only-machine) on how to run it. Please change `deepvariant:latest` to `deepvariant:1.4.0`.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/845
https://github.com/google/deepvariant/issues/845:28,security,model,model,28,"@esraaelmligy ,. The RNAseq model is trained and released for DeepVariant 1.4.0, please see [this documentation](https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-rnaseq-case-study.md#running-deepvariant-rna-seq-on-a-cpu-only-machine) on how to run it. Please change `deepvariant:latest` to `deepvariant:1.4.0`.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/845
https://github.com/google/deepvariant/issues/845:98,usability,document,documentation,98,"@esraaelmligy ,. The RNAseq model is trained and released for DeepVariant 1.4.0, please see [this documentation](https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-rnaseq-case-study.md#running-deepvariant-rna-seq-on-a-cpu-only-machine) on how to run it. Please change `deepvariant:latest` to `deepvariant:1.4.0`.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/845
https://github.com/google/deepvariant/issues/845:51,deployability,version,version,51,"@kishwarshafin ,. Thank you so much, I changed the version and the run finished well. I just have to ask whether using a non-updated version will dramatically affect the results, and if you are planning to do any updates on the RNAseq model soon so it could be suitable for later versions?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/845
https://github.com/google/deepvariant/issues/845:125,deployability,updat,updated,125,"@kishwarshafin ,. Thank you so much, I changed the version and the run finished well. I just have to ask whether using a non-updated version will dramatically affect the results, and if you are planning to do any updates on the RNAseq model soon so it could be suitable for later versions?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/845
https://github.com/google/deepvariant/issues/845:133,deployability,version,version,133,"@kishwarshafin ,. Thank you so much, I changed the version and the run finished well. I just have to ask whether using a non-updated version will dramatically affect the results, and if you are planning to do any updates on the RNAseq model soon so it could be suitable for later versions?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/845
https://github.com/google/deepvariant/issues/845:213,deployability,updat,updates,213,"@kishwarshafin ,. Thank you so much, I changed the version and the run finished well. I just have to ask whether using a non-updated version will dramatically affect the results, and if you are planning to do any updates on the RNAseq model soon so it could be suitable for later versions?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/845
https://github.com/google/deepvariant/issues/845:280,deployability,version,versions,280,"@kishwarshafin ,. Thank you so much, I changed the version and the run finished well. I just have to ask whether using a non-updated version will dramatically affect the results, and if you are planning to do any updates on the RNAseq model soon so it could be suitable for later versions?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/845
https://github.com/google/deepvariant/issues/845:235,energy efficiency,model,model,235,"@kishwarshafin ,. Thank you so much, I changed the version and the run finished well. I just have to ask whether using a non-updated version will dramatically affect the results, and if you are planning to do any updates on the RNAseq model soon so it could be suitable for later versions?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/845
https://github.com/google/deepvariant/issues/845:51,integrability,version,version,51,"@kishwarshafin ,. Thank you so much, I changed the version and the run finished well. I just have to ask whether using a non-updated version will dramatically affect the results, and if you are planning to do any updates on the RNAseq model soon so it could be suitable for later versions?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/845
https://github.com/google/deepvariant/issues/845:133,integrability,version,version,133,"@kishwarshafin ,. Thank you so much, I changed the version and the run finished well. I just have to ask whether using a non-updated version will dramatically affect the results, and if you are planning to do any updates on the RNAseq model soon so it could be suitable for later versions?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/845
https://github.com/google/deepvariant/issues/845:280,integrability,version,versions,280,"@kishwarshafin ,. Thank you so much, I changed the version and the run finished well. I just have to ask whether using a non-updated version will dramatically affect the results, and if you are planning to do any updates on the RNAseq model soon so it could be suitable for later versions?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/845
https://github.com/google/deepvariant/issues/845:51,modifiability,version,version,51,"@kishwarshafin ,. Thank you so much, I changed the version and the run finished well. I just have to ask whether using a non-updated version will dramatically affect the results, and if you are planning to do any updates on the RNAseq model soon so it could be suitable for later versions?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/845
https://github.com/google/deepvariant/issues/845:133,modifiability,version,version,133,"@kishwarshafin ,. Thank you so much, I changed the version and the run finished well. I just have to ask whether using a non-updated version will dramatically affect the results, and if you are planning to do any updates on the RNAseq model soon so it could be suitable for later versions?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/845
https://github.com/google/deepvariant/issues/845:280,modifiability,version,versions,280,"@kishwarshafin ,. Thank you so much, I changed the version and the run finished well. I just have to ask whether using a non-updated version will dramatically affect the results, and if you are planning to do any updates on the RNAseq model soon so it could be suitable for later versions?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/845
https://github.com/google/deepvariant/issues/845:125,safety,updat,updated,125,"@kishwarshafin ,. Thank you so much, I changed the version and the run finished well. I just have to ask whether using a non-updated version will dramatically affect the results, and if you are planning to do any updates on the RNAseq model soon so it could be suitable for later versions?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/845
https://github.com/google/deepvariant/issues/845:213,safety,updat,updates,213,"@kishwarshafin ,. Thank you so much, I changed the version and the run finished well. I just have to ask whether using a non-updated version will dramatically affect the results, and if you are planning to do any updates on the RNAseq model soon so it could be suitable for later versions?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/845
https://github.com/google/deepvariant/issues/845:125,security,updat,updated,125,"@kishwarshafin ,. Thank you so much, I changed the version and the run finished well. I just have to ask whether using a non-updated version will dramatically affect the results, and if you are planning to do any updates on the RNAseq model soon so it could be suitable for later versions?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/845
https://github.com/google/deepvariant/issues/845:213,security,updat,updates,213,"@kishwarshafin ,. Thank you so much, I changed the version and the run finished well. I just have to ask whether using a non-updated version will dramatically affect the results, and if you are planning to do any updates on the RNAseq model soon so it could be suitable for later versions?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/845
https://github.com/google/deepvariant/issues/845:235,security,model,model,235,"@kishwarshafin ,. Thank you so much, I changed the version and the run finished well. I just have to ask whether using a non-updated version will dramatically affect the results, and if you are planning to do any updates on the RNAseq model soon so it could be suitable for later versions?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/845
https://github.com/google/deepvariant/issues/845:194,testability,plan,planning,194,"@kishwarshafin ,. Thank you so much, I changed the version and the run finished well. I just have to ask whether using a non-updated version will dramatically affect the results, and if you are planning to do any updates on the RNAseq model soon so it could be suitable for later versions?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/845
https://github.com/google/deepvariant/issues/845:25,deployability,updat,updated,25,"@esraaelmligy ,. The non updated version will not affect the results much. Any updates on RNASeq model is currently unplanned but I can give you an update closer to the next release date. Ideally, yes, we would want to train a new model. I just haven't assessed how much time and effort is required there vs other priorities. I have taken a note and will update you when we make a decision. . Will close this bug for now. Thank you for confirming that it worked.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/845
https://github.com/google/deepvariant/issues/845:33,deployability,version,version,33,"@esraaelmligy ,. The non updated version will not affect the results much. Any updates on RNASeq model is currently unplanned but I can give you an update closer to the next release date. Ideally, yes, we would want to train a new model. I just haven't assessed how much time and effort is required there vs other priorities. I have taken a note and will update you when we make a decision. . Will close this bug for now. Thank you for confirming that it worked.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/845
https://github.com/google/deepvariant/issues/845:79,deployability,updat,updates,79,"@esraaelmligy ,. The non updated version will not affect the results much. Any updates on RNASeq model is currently unplanned but I can give you an update closer to the next release date. Ideally, yes, we would want to train a new model. I just haven't assessed how much time and effort is required there vs other priorities. I have taken a note and will update you when we make a decision. . Will close this bug for now. Thank you for confirming that it worked.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/845
https://github.com/google/deepvariant/issues/845:148,deployability,updat,update,148,"@esraaelmligy ,. The non updated version will not affect the results much. Any updates on RNASeq model is currently unplanned but I can give you an update closer to the next release date. Ideally, yes, we would want to train a new model. I just haven't assessed how much time and effort is required there vs other priorities. I have taken a note and will update you when we make a decision. . Will close this bug for now. Thank you for confirming that it worked.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/845
https://github.com/google/deepvariant/issues/845:174,deployability,releas,release,174,"@esraaelmligy ,. The non updated version will not affect the results much. Any updates on RNASeq model is currently unplanned but I can give you an update closer to the next release date. Ideally, yes, we would want to train a new model. I just haven't assessed how much time and effort is required there vs other priorities. I have taken a note and will update you when we make a decision. . Will close this bug for now. Thank you for confirming that it worked.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/845
https://github.com/google/deepvariant/issues/845:355,deployability,updat,update,355,"@esraaelmligy ,. The non updated version will not affect the results much. Any updates on RNASeq model is currently unplanned but I can give you an update closer to the next release date. Ideally, yes, we would want to train a new model. I just haven't assessed how much time and effort is required there vs other priorities. I have taken a note and will update you when we make a decision. . Will close this bug for now. Thank you for confirming that it worked.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/845
https://github.com/google/deepvariant/issues/845:97,energy efficiency,model,model,97,"@esraaelmligy ,. The non updated version will not affect the results much. Any updates on RNASeq model is currently unplanned but I can give you an update closer to the next release date. Ideally, yes, we would want to train a new model. I just haven't assessed how much time and effort is required there vs other priorities. I have taken a note and will update you when we make a decision. . Will close this bug for now. Thank you for confirming that it worked.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/845
https://github.com/google/deepvariant/issues/845:106,energy efficiency,current,currently,106,"@esraaelmligy ,. The non updated version will not affect the results much. Any updates on RNASeq model is currently unplanned but I can give you an update closer to the next release date. Ideally, yes, we would want to train a new model. I just haven't assessed how much time and effort is required there vs other priorities. I have taken a note and will update you when we make a decision. . Will close this bug for now. Thank you for confirming that it worked.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/845
https://github.com/google/deepvariant/issues/845:231,energy efficiency,model,model,231,"@esraaelmligy ,. The non updated version will not affect the results much. Any updates on RNASeq model is currently unplanned but I can give you an update closer to the next release date. Ideally, yes, we would want to train a new model. I just haven't assessed how much time and effort is required there vs other priorities. I have taken a note and will update you when we make a decision. . Will close this bug for now. Thank you for confirming that it worked.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/845
https://github.com/google/deepvariant/issues/845:33,integrability,version,version,33,"@esraaelmligy ,. The non updated version will not affect the results much. Any updates on RNASeq model is currently unplanned but I can give you an update closer to the next release date. Ideally, yes, we would want to train a new model. I just haven't assessed how much time and effort is required there vs other priorities. I have taken a note and will update you when we make a decision. . Will close this bug for now. Thank you for confirming that it worked.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/845
https://github.com/google/deepvariant/issues/845:33,modifiability,version,version,33,"@esraaelmligy ,. The non updated version will not affect the results much. Any updates on RNASeq model is currently unplanned but I can give you an update closer to the next release date. Ideally, yes, we would want to train a new model. I just haven't assessed how much time and effort is required there vs other priorities. I have taken a note and will update you when we make a decision. . Will close this bug for now. Thank you for confirming that it worked.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/845
https://github.com/google/deepvariant/issues/845:271,performance,time,time,271,"@esraaelmligy ,. The non updated version will not affect the results much. Any updates on RNASeq model is currently unplanned but I can give you an update closer to the next release date. Ideally, yes, we would want to train a new model. I just haven't assessed how much time and effort is required there vs other priorities. I have taken a note and will update you when we make a decision. . Will close this bug for now. Thank you for confirming that it worked.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/845
https://github.com/google/deepvariant/issues/845:25,safety,updat,updated,25,"@esraaelmligy ,. The non updated version will not affect the results much. Any updates on RNASeq model is currently unplanned but I can give you an update closer to the next release date. Ideally, yes, we would want to train a new model. I just haven't assessed how much time and effort is required there vs other priorities. I have taken a note and will update you when we make a decision. . Will close this bug for now. Thank you for confirming that it worked.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/845
https://github.com/google/deepvariant/issues/845:79,safety,updat,updates,79,"@esraaelmligy ,. The non updated version will not affect the results much. Any updates on RNASeq model is currently unplanned but I can give you an update closer to the next release date. Ideally, yes, we would want to train a new model. I just haven't assessed how much time and effort is required there vs other priorities. I have taken a note and will update you when we make a decision. . Will close this bug for now. Thank you for confirming that it worked.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/845
https://github.com/google/deepvariant/issues/845:148,safety,updat,update,148,"@esraaelmligy ,. The non updated version will not affect the results much. Any updates on RNASeq model is currently unplanned but I can give you an update closer to the next release date. Ideally, yes, we would want to train a new model. I just haven't assessed how much time and effort is required there vs other priorities. I have taken a note and will update you when we make a decision. . Will close this bug for now. Thank you for confirming that it worked.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/845
https://github.com/google/deepvariant/issues/845:355,safety,updat,update,355,"@esraaelmligy ,. The non updated version will not affect the results much. Any updates on RNASeq model is currently unplanned but I can give you an update closer to the next release date. Ideally, yes, we would want to train a new model. I just haven't assessed how much time and effort is required there vs other priorities. I have taken a note and will update you when we make a decision. . Will close this bug for now. Thank you for confirming that it worked.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/845
https://github.com/google/deepvariant/issues/845:25,security,updat,updated,25,"@esraaelmligy ,. The non updated version will not affect the results much. Any updates on RNASeq model is currently unplanned but I can give you an update closer to the next release date. Ideally, yes, we would want to train a new model. I just haven't assessed how much time and effort is required there vs other priorities. I have taken a note and will update you when we make a decision. . Will close this bug for now. Thank you for confirming that it worked.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/845
https://github.com/google/deepvariant/issues/845:79,security,updat,updates,79,"@esraaelmligy ,. The non updated version will not affect the results much. Any updates on RNASeq model is currently unplanned but I can give you an update closer to the next release date. Ideally, yes, we would want to train a new model. I just haven't assessed how much time and effort is required there vs other priorities. I have taken a note and will update you when we make a decision. . Will close this bug for now. Thank you for confirming that it worked.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/845
https://github.com/google/deepvariant/issues/845:97,security,model,model,97,"@esraaelmligy ,. The non updated version will not affect the results much. Any updates on RNASeq model is currently unplanned but I can give you an update closer to the next release date. Ideally, yes, we would want to train a new model. I just haven't assessed how much time and effort is required there vs other priorities. I have taken a note and will update you when we make a decision. . Will close this bug for now. Thank you for confirming that it worked.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/845
https://github.com/google/deepvariant/issues/845:148,security,updat,update,148,"@esraaelmligy ,. The non updated version will not affect the results much. Any updates on RNASeq model is currently unplanned but I can give you an update closer to the next release date. Ideally, yes, we would want to train a new model. I just haven't assessed how much time and effort is required there vs other priorities. I have taken a note and will update you when we make a decision. . Will close this bug for now. Thank you for confirming that it worked.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/845
https://github.com/google/deepvariant/issues/845:231,security,model,model,231,"@esraaelmligy ,. The non updated version will not affect the results much. Any updates on RNASeq model is currently unplanned but I can give you an update closer to the next release date. Ideally, yes, we would want to train a new model. I just haven't assessed how much time and effort is required there vs other priorities. I have taken a note and will update you when we make a decision. . Will close this bug for now. Thank you for confirming that it worked.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/845
https://github.com/google/deepvariant/issues/845:253,security,assess,assessed,253,"@esraaelmligy ,. The non updated version will not affect the results much. Any updates on RNASeq model is currently unplanned but I can give you an update closer to the next release date. Ideally, yes, we would want to train a new model. I just haven't assessed how much time and effort is required there vs other priorities. I have taken a note and will update you when we make a decision. . Will close this bug for now. Thank you for confirming that it worked.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/845
https://github.com/google/deepvariant/issues/845:355,security,updat,update,355,"@esraaelmligy ,. The non updated version will not affect the results much. Any updates on RNASeq model is currently unplanned but I can give you an update closer to the next release date. Ideally, yes, we would want to train a new model. I just haven't assessed how much time and effort is required there vs other priorities. I have taken a note and will update you when we make a decision. . Will close this bug for now. Thank you for confirming that it worked.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/845
https://github.com/google/deepvariant/issues/845:155,usability,close,closer,155,"@esraaelmligy ,. The non updated version will not affect the results much. Any updates on RNASeq model is currently unplanned but I can give you an update closer to the next release date. Ideally, yes, we would want to train a new model. I just haven't assessed how much time and effort is required there vs other priorities. I have taken a note and will update you when we make a decision. . Will close this bug for now. Thank you for confirming that it worked.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/845
https://github.com/google/deepvariant/issues/845:398,usability,close,close,398,"@esraaelmligy ,. The non updated version will not affect the results much. Any updates on RNASeq model is currently unplanned but I can give you an update closer to the next release date. Ideally, yes, we would want to train a new model. I just haven't assessed how much time and effort is required there vs other priorities. I have taken a note and will update you when we make a decision. . Will close this bug for now. Thank you for confirming that it worked.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/845
https://github.com/google/deepvariant/issues/845:436,usability,confirm,confirming,436,"@esraaelmligy ,. The non updated version will not affect the results much. Any updates on RNASeq model is currently unplanned but I can give you an update closer to the next release date. Ideally, yes, we would want to train a new model. I just haven't assessed how much time and effort is required there vs other priorities. I have taken a note and will update you when we make a decision. . Will close this bug for now. Thank you for confirming that it worked.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/845
https://github.com/google/deepvariant/issues/846:1169,deployability,scale,scaled,1169,"@pioneer-pi please see [how deepvariant works](https://github.com/google/deepvariant?tab=readme-ov-file#how-deepvariant-works) to see that the CNN provides a probability vector which we use to determine genotype likelihoods. 1. QUAL is the minimum probability of the genotype. Please see [here](https://github.com/google/deepvariant/blob/bf9ed7e6de97cf6c8381694cb996317a740625ad/deepvariant/postprocess_variants.py#L467) for details. 2. GQ and qual are synonymous one is just rounded. Please see how [this method](https://github.com/google/deepvariant/blob/bf9ed7e6de97cf6c8381694cb996317a740625ad/deepvariant/postprocess_variants.py#L446) determines both QUAL and GQ. 3. RefCall is called if the QUAL is below a set threshold which is 3. See [here](https://github.com/google/deepvariant/blob/bf9ed7e6de97cf6c8381694cb996317a740625ad/deepvariant/postprocess_variants.py#L387) for details. 4. PL is the genotype likelihoods. See [code](https://github.com/google/deepvariant/blob/bf9ed7e6de97cf6c8381694cb996317a740625ad/third_party/nucleus/io/vcf_conversion.cc#L1038) for details. Also described in header:. ```. ##FORMAT=<ID=PL,Number=G,Type=Integer,Description=""Phred-scaled genotype likelihoods rounded to the closest integer"">. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/846
https://github.com/google/deepvariant/issues/846:1169,energy efficiency,scale,scaled,1169,"@pioneer-pi please see [how deepvariant works](https://github.com/google/deepvariant?tab=readme-ov-file#how-deepvariant-works) to see that the CNN provides a probability vector which we use to determine genotype likelihoods. 1. QUAL is the minimum probability of the genotype. Please see [here](https://github.com/google/deepvariant/blob/bf9ed7e6de97cf6c8381694cb996317a740625ad/deepvariant/postprocess_variants.py#L467) for details. 2. GQ and qual are synonymous one is just rounded. Please see how [this method](https://github.com/google/deepvariant/blob/bf9ed7e6de97cf6c8381694cb996317a740625ad/deepvariant/postprocess_variants.py#L446) determines both QUAL and GQ. 3. RefCall is called if the QUAL is below a set threshold which is 3. See [here](https://github.com/google/deepvariant/blob/bf9ed7e6de97cf6c8381694cb996317a740625ad/deepvariant/postprocess_variants.py#L387) for details. 4. PL is the genotype likelihoods. See [code](https://github.com/google/deepvariant/blob/bf9ed7e6de97cf6c8381694cb996317a740625ad/third_party/nucleus/io/vcf_conversion.cc#L1038) for details. Also described in header:. ```. ##FORMAT=<ID=PL,Number=G,Type=Integer,Description=""Phred-scaled genotype likelihoods rounded to the closest integer"">. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/846
https://github.com/google/deepvariant/issues/846:1114,interoperability,FORMAT,FORMAT,1114,"@pioneer-pi please see [how deepvariant works](https://github.com/google/deepvariant?tab=readme-ov-file#how-deepvariant-works) to see that the CNN provides a probability vector which we use to determine genotype likelihoods. 1. QUAL is the minimum probability of the genotype. Please see [here](https://github.com/google/deepvariant/blob/bf9ed7e6de97cf6c8381694cb996317a740625ad/deepvariant/postprocess_variants.py#L467) for details. 2. GQ and qual are synonymous one is just rounded. Please see how [this method](https://github.com/google/deepvariant/blob/bf9ed7e6de97cf6c8381694cb996317a740625ad/deepvariant/postprocess_variants.py#L446) determines both QUAL and GQ. 3. RefCall is called if the QUAL is below a set threshold which is 3. See [here](https://github.com/google/deepvariant/blob/bf9ed7e6de97cf6c8381694cb996317a740625ad/deepvariant/postprocess_variants.py#L387) for details. 4. PL is the genotype likelihoods. See [code](https://github.com/google/deepvariant/blob/bf9ed7e6de97cf6c8381694cb996317a740625ad/third_party/nucleus/io/vcf_conversion.cc#L1038) for details. Also described in header:. ```. ##FORMAT=<ID=PL,Number=G,Type=Integer,Description=""Phred-scaled genotype likelihoods rounded to the closest integer"">. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/846
https://github.com/google/deepvariant/issues/846:1169,modifiability,scal,scaled,1169,"@pioneer-pi please see [how deepvariant works](https://github.com/google/deepvariant?tab=readme-ov-file#how-deepvariant-works) to see that the CNN provides a probability vector which we use to determine genotype likelihoods. 1. QUAL is the minimum probability of the genotype. Please see [here](https://github.com/google/deepvariant/blob/bf9ed7e6de97cf6c8381694cb996317a740625ad/deepvariant/postprocess_variants.py#L467) for details. 2. GQ and qual are synonymous one is just rounded. Please see how [this method](https://github.com/google/deepvariant/blob/bf9ed7e6de97cf6c8381694cb996317a740625ad/deepvariant/postprocess_variants.py#L446) determines both QUAL and GQ. 3. RefCall is called if the QUAL is below a set threshold which is 3. See [here](https://github.com/google/deepvariant/blob/bf9ed7e6de97cf6c8381694cb996317a740625ad/deepvariant/postprocess_variants.py#L387) for details. 4. PL is the genotype likelihoods. See [code](https://github.com/google/deepvariant/blob/bf9ed7e6de97cf6c8381694cb996317a740625ad/third_party/nucleus/io/vcf_conversion.cc#L1038) for details. Also described in header:. ```. ##FORMAT=<ID=PL,Number=G,Type=Integer,Description=""Phred-scaled genotype likelihoods rounded to the closest integer"">. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/846
https://github.com/google/deepvariant/issues/846:1169,performance,scale,scaled,1169,"@pioneer-pi please see [how deepvariant works](https://github.com/google/deepvariant?tab=readme-ov-file#how-deepvariant-works) to see that the CNN provides a probability vector which we use to determine genotype likelihoods. 1. QUAL is the minimum probability of the genotype. Please see [here](https://github.com/google/deepvariant/blob/bf9ed7e6de97cf6c8381694cb996317a740625ad/deepvariant/postprocess_variants.py#L467) for details. 2. GQ and qual are synonymous one is just rounded. Please see how [this method](https://github.com/google/deepvariant/blob/bf9ed7e6de97cf6c8381694cb996317a740625ad/deepvariant/postprocess_variants.py#L446) determines both QUAL and GQ. 3. RefCall is called if the QUAL is below a set threshold which is 3. See [here](https://github.com/google/deepvariant/blob/bf9ed7e6de97cf6c8381694cb996317a740625ad/deepvariant/postprocess_variants.py#L387) for details. 4. PL is the genotype likelihoods. See [code](https://github.com/google/deepvariant/blob/bf9ed7e6de97cf6c8381694cb996317a740625ad/third_party/nucleus/io/vcf_conversion.cc#L1038) for details. Also described in header:. ```. ##FORMAT=<ID=PL,Number=G,Type=Integer,Description=""Phred-scaled genotype likelihoods rounded to the closest integer"">. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/846
https://github.com/google/deepvariant/issues/846:240,usability,minim,minimum,240,"@pioneer-pi please see [how deepvariant works](https://github.com/google/deepvariant?tab=readme-ov-file#how-deepvariant-works) to see that the CNN provides a probability vector which we use to determine genotype likelihoods. 1. QUAL is the minimum probability of the genotype. Please see [here](https://github.com/google/deepvariant/blob/bf9ed7e6de97cf6c8381694cb996317a740625ad/deepvariant/postprocess_variants.py#L467) for details. 2. GQ and qual are synonymous one is just rounded. Please see how [this method](https://github.com/google/deepvariant/blob/bf9ed7e6de97cf6c8381694cb996317a740625ad/deepvariant/postprocess_variants.py#L446) determines both QUAL and GQ. 3. RefCall is called if the QUAL is below a set threshold which is 3. See [here](https://github.com/google/deepvariant/blob/bf9ed7e6de97cf6c8381694cb996317a740625ad/deepvariant/postprocess_variants.py#L387) for details. 4. PL is the genotype likelihoods. See [code](https://github.com/google/deepvariant/blob/bf9ed7e6de97cf6c8381694cb996317a740625ad/third_party/nucleus/io/vcf_conversion.cc#L1038) for details. Also described in header:. ```. ##FORMAT=<ID=PL,Number=G,Type=Integer,Description=""Phred-scaled genotype likelihoods rounded to the closest integer"">. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/846
https://github.com/google/deepvariant/issues/846:1212,usability,close,closest,1212,"@pioneer-pi please see [how deepvariant works](https://github.com/google/deepvariant?tab=readme-ov-file#how-deepvariant-works) to see that the CNN provides a probability vector which we use to determine genotype likelihoods. 1. QUAL is the minimum probability of the genotype. Please see [here](https://github.com/google/deepvariant/blob/bf9ed7e6de97cf6c8381694cb996317a740625ad/deepvariant/postprocess_variants.py#L467) for details. 2. GQ and qual are synonymous one is just rounded. Please see how [this method](https://github.com/google/deepvariant/blob/bf9ed7e6de97cf6c8381694cb996317a740625ad/deepvariant/postprocess_variants.py#L446) determines both QUAL and GQ. 3. RefCall is called if the QUAL is below a set threshold which is 3. See [here](https://github.com/google/deepvariant/blob/bf9ed7e6de97cf6c8381694cb996317a740625ad/deepvariant/postprocess_variants.py#L387) for details. 4. PL is the genotype likelihoods. See [code](https://github.com/google/deepvariant/blob/bf9ed7e6de97cf6c8381694cb996317a740625ad/third_party/nucleus/io/vcf_conversion.cc#L1038) for details. Also described in header:. ```. ##FORMAT=<ID=PL,Number=G,Type=Integer,Description=""Phred-scaled genotype likelihoods rounded to the closest integer"">. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/846
https://github.com/google/deepvariant/issues/847:25,deployability,log,log,25,"Hi @Han-Cao ,. from your log, I'm surprised that your make_examples step was making that many examples. Before it crashed, one shard already made 20480581 examples. And it seems like you have 40 shards. So, that would mean 20480581*40 = 819,223,240 examples has already been made. That is a surprising amount of examples! Is there anything unusual about your sample? Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/847
https://github.com/google/deepvariant/issues/847:25,safety,log,log,25,"Hi @Han-Cao ,. from your log, I'm surprised that your make_examples step was making that many examples. Before it crashed, one shard already made 20480581 examples. And it seems like you have 40 shards. So, that would mean 20480581*40 = 819,223,240 examples has already been made. That is a surprising amount of examples! Is there anything unusual about your sample? Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/847
https://github.com/google/deepvariant/issues/847:25,security,log,log,25,"Hi @Han-Cao ,. from your log, I'm surprised that your make_examples step was making that many examples. Before it crashed, one shard already made 20480581 examples. And it seems like you have 40 shards. So, that would mean 20480581*40 = 819,223,240 examples has already been made. That is a surprising amount of examples! Is there anything unusual about your sample? Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/847
https://github.com/google/deepvariant/issues/847:25,testability,log,log,25,"Hi @Han-Cao ,. from your log, I'm surprised that your make_examples step was making that many examples. Before it crashed, one shard already made 20480581 examples. And it seems like you have 40 shards. So, that would mean 20480581*40 = 819,223,240 examples has already been made. That is a surprising amount of examples! Is there anything unusual about your sample? Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/847
https://github.com/google/deepvariant/issues/847:54,availability,down,downloaded,54,"Hi @pichuan,. The sample is HG00438 from 1000G, which downloaded from. ```. /vol1/fastq/ERR398/008/ERR3988768/ERR3988768_1.fastq.gz. /vol1/fastq/ERR398/008/ERR3988768/ERR3988768_2.fastq.gz. ```. I tried to reproduce this issue with an 1MB bam file: [HG00438.chr1_1000000_2000000.bam.txt](https://github.com/user-attachments/files/16170832/HG00438.chr1_1000000_2000000.bam.txt) (.txt is only for uploading to github). Deepvariant found 5115 examples on it. Do you think this data is normal? If that 1MB region looks OK, then I guess this issue might be caused by some complex region in the graph, which cannot be well surjected to the bam file. I need some time to carefully check the data, and maybe is more likely an issue with the pangenome graph or `vg`. Many thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/847
https://github.com/google/deepvariant/issues/847:656,performance,time,time,656,"Hi @pichuan,. The sample is HG00438 from 1000G, which downloaded from. ```. /vol1/fastq/ERR398/008/ERR3988768/ERR3988768_1.fastq.gz. /vol1/fastq/ERR398/008/ERR3988768/ERR3988768_2.fastq.gz. ```. I tried to reproduce this issue with an 1MB bam file: [HG00438.chr1_1000000_2000000.bam.txt](https://github.com/user-attachments/files/16170832/HG00438.chr1_1000000_2000000.bam.txt) (.txt is only for uploading to github). Deepvariant found 5115 examples on it. Do you think this data is normal? If that 1MB region looks OK, then I guess this issue might be caused by some complex region in the graph, which cannot be well surjected to the bam file. I need some time to carefully check the data, and maybe is more likely an issue with the pangenome graph or `vg`. Many thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/847
https://github.com/google/deepvariant/issues/847:567,safety,compl,complex,567,"Hi @pichuan,. The sample is HG00438 from 1000G, which downloaded from. ```. /vol1/fastq/ERR398/008/ERR3988768/ERR3988768_1.fastq.gz. /vol1/fastq/ERR398/008/ERR3988768/ERR3988768_2.fastq.gz. ```. I tried to reproduce this issue with an 1MB bam file: [HG00438.chr1_1000000_2000000.bam.txt](https://github.com/user-attachments/files/16170832/HG00438.chr1_1000000_2000000.bam.txt) (.txt is only for uploading to github). Deepvariant found 5115 examples on it. Do you think this data is normal? If that 1MB region looks OK, then I guess this issue might be caused by some complex region in the graph, which cannot be well surjected to the bam file. I need some time to carefully check the data, and maybe is more likely an issue with the pangenome graph or `vg`. Many thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/847
https://github.com/google/deepvariant/issues/847:567,security,compl,complex,567,"Hi @pichuan,. The sample is HG00438 from 1000G, which downloaded from. ```. /vol1/fastq/ERR398/008/ERR3988768/ERR3988768_1.fastq.gz. /vol1/fastq/ERR398/008/ERR3988768/ERR3988768_2.fastq.gz. ```. I tried to reproduce this issue with an 1MB bam file: [HG00438.chr1_1000000_2000000.bam.txt](https://github.com/user-attachments/files/16170832/HG00438.chr1_1000000_2000000.bam.txt) (.txt is only for uploading to github). Deepvariant found 5115 examples on it. Do you think this data is normal? If that 1MB region looks OK, then I guess this issue might be caused by some complex region in the graph, which cannot be well surjected to the bam file. I need some time to carefully check the data, and maybe is more likely an issue with the pangenome graph or `vg`. Many thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/847
https://github.com/google/deepvariant/issues/847:307,usability,user,user-attachments,307,"Hi @pichuan,. The sample is HG00438 from 1000G, which downloaded from. ```. /vol1/fastq/ERR398/008/ERR3988768/ERR3988768_1.fastq.gz. /vol1/fastq/ERR398/008/ERR3988768/ERR3988768_2.fastq.gz. ```. I tried to reproduce this issue with an 1MB bam file: [HG00438.chr1_1000000_2000000.bam.txt](https://github.com/user-attachments/files/16170832/HG00438.chr1_1000000_2000000.bam.txt) (.txt is only for uploading to github). Deepvariant found 5115 examples on it. Do you think this data is normal? If that 1MB region looks OK, then I guess this issue might be caused by some complex region in the graph, which cannot be well surjected to the bam file. I need some time to carefully check the data, and maybe is more likely an issue with the pangenome graph or `vg`. Many thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/847
https://github.com/google/deepvariant/issues/847:43,interoperability,share,shared,43,"Hi @Han-Cao , I looked at the BAM file you shared. From my IGV, they certainly do not look like they're mapped correctly to me. ![image](https://github.com/user-attachments/assets/4727d465-cc96-4ca0-a138-63027abee6b6). Here is a more zoomed-in view:. ![image](https://github.com/user-attachments/assets/4363f21f-cb96-4434-8d04-37e8bc8afb2d). Please check how you map the files. I think the issue was before DeepVariant. Thank you! I'll close this issue now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/847
https://github.com/google/deepvariant/issues/847:156,usability,user,user-attachments,156,"Hi @Han-Cao , I looked at the BAM file you shared. From my IGV, they certainly do not look like they're mapped correctly to me. ![image](https://github.com/user-attachments/assets/4727d465-cc96-4ca0-a138-63027abee6b6). Here is a more zoomed-in view:. ![image](https://github.com/user-attachments/assets/4363f21f-cb96-4434-8d04-37e8bc8afb2d). Please check how you map the files. I think the issue was before DeepVariant. Thank you! I'll close this issue now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/847
https://github.com/google/deepvariant/issues/847:279,usability,user,user-attachments,279,"Hi @Han-Cao , I looked at the BAM file you shared. From my IGV, they certainly do not look like they're mapped correctly to me. ![image](https://github.com/user-attachments/assets/4727d465-cc96-4ca0-a138-63027abee6b6). Here is a more zoomed-in view:. ![image](https://github.com/user-attachments/assets/4363f21f-cb96-4434-8d04-37e8bc8afb2d). Please check how you map the files. I think the issue was before DeepVariant. Thank you! I'll close this issue now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/847
https://github.com/google/deepvariant/issues/847:436,usability,close,close,436,"Hi @Han-Cao , I looked at the BAM file you shared. From my IGV, they certainly do not look like they're mapped correctly to me. ![image](https://github.com/user-attachments/assets/4727d465-cc96-4ca0-a138-63027abee6b6). Here is a more zoomed-in view:. ![image](https://github.com/user-attachments/assets/4363f21f-cb96-4434-8d04-37e8bc8afb2d). Please check how you map the files. I think the issue was before DeepVariant. Thank you! I'll close this issue now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/847
https://github.com/google/deepvariant/issues/847:139,usability,user,user-attachments,139,"Hi @pichuan ,. That is because I surjected it to CHM13 not GRCh38. If you switch to CHM13, it is well aligned. ![image](https://github.com/user-attachments/assets/1d35b184-70e3-4d3e-b073-22cb5d849377).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/847
https://github.com/google/deepvariant/issues/847:1370,availability,slo,slow,1370,"@Han-Cao , I ran on your BAM. My command is something like:. ```bash. docker run \. -v /home/pichuan/wgs-case-study/input/data:/input \. -v /home/pichuan/wgs-case-study/output:/output \. google/deepvariant:1.6.1 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/chm13v2.0_noY.fa.gz \. --reads=/input/HG00438.chr1_1000000_2000000.bam \. --output_vcf=/output/deepvariant.output.vcf.gz \. --output_gvcf=/output/deepvariant.output.g.vcf.gz \. --num_shards 64 \. --logging_dir=/output/logs \. --intermediate_results_dir /output/intermediate_results_dir \. --make_examples_extra_args ""min_mapping_quality=1,keep_legacy_allele_counter_behavior=true,normalize_reads=true"" \. --report_title test-githubissue \. --regions chr1:1000000-2000000 \. --runtime_report. ```. Then, I checked how many variants there are:. ```. $ zcat deepvariant.output.vcf.gz | grep -v '^#' | wc -l. 4748. ```. A few thoughts and observations:. 1. Looking at IGV, it looks reasonable. (@AndrewCarroll also took a look and agreed). 2. The number of variants in this region didn't seem too unreasonable. . 3. Your BAM is higher coverage, which could also be why more candidates have been made. 4. I have not use CHM13 a lot myself. It is possible that CHM13 has regions that can take DeepVariant make_examples longer time to complete. If you notice specific regions being really slow (which you can use https://github.com/google/deepvariant/blob/r1.6.1/docs/runtime-by-region.md to check), please feel free to report those regions and we can take a closer look from there! 5. For the question of whether you're running Giraffe correctly, I don't think I'm able to help answer that question. I'd encourage you to run your Giraffe+DV setting through something where you have truths for, so you can check hap.py output and see if that's expected. And, to save your time, I recommend running DeepVariant on just one chr (e.g., chr20) to check. Hope this helps!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/847
https://github.com/google/deepvariant/issues/847:506,deployability,log,logs,506,"@Han-Cao , I ran on your BAM. My command is something like:. ```bash. docker run \. -v /home/pichuan/wgs-case-study/input/data:/input \. -v /home/pichuan/wgs-case-study/output:/output \. google/deepvariant:1.6.1 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/chm13v2.0_noY.fa.gz \. --reads=/input/HG00438.chr1_1000000_2000000.bam \. --output_vcf=/output/deepvariant.output.vcf.gz \. --output_gvcf=/output/deepvariant.output.g.vcf.gz \. --num_shards 64 \. --logging_dir=/output/logs \. --intermediate_results_dir /output/intermediate_results_dir \. --make_examples_extra_args ""min_mapping_quality=1,keep_legacy_allele_counter_behavior=true,normalize_reads=true"" \. --report_title test-githubissue \. --regions chr1:1000000-2000000 \. --runtime_report. ```. Then, I checked how many variants there are:. ```. $ zcat deepvariant.output.vcf.gz | grep -v '^#' | wc -l. 4748. ```. A few thoughts and observations:. 1. Looking at IGV, it looks reasonable. (@AndrewCarroll also took a look and agreed). 2. The number of variants in this region didn't seem too unreasonable. . 3. Your BAM is higher coverage, which could also be why more candidates have been made. 4. I have not use CHM13 a lot myself. It is possible that CHM13 has regions that can take DeepVariant make_examples longer time to complete. If you notice specific regions being really slow (which you can use https://github.com/google/deepvariant/blob/r1.6.1/docs/runtime-by-region.md to check), please feel free to report those regions and we can take a closer look from there! 5. For the question of whether you're running Giraffe correctly, I don't think I'm able to help answer that question. I'd encourage you to run your Giraffe+DV setting through something where you have truths for, so you can check hap.py output and see if that's expected. And, to save your time, I recommend running DeepVariant on just one chr (e.g., chr20) to check. Hope this helps!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/847
https://github.com/google/deepvariant/issues/847:923,deployability,observ,observations,923,"@Han-Cao , I ran on your BAM. My command is something like:. ```bash. docker run \. -v /home/pichuan/wgs-case-study/input/data:/input \. -v /home/pichuan/wgs-case-study/output:/output \. google/deepvariant:1.6.1 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/chm13v2.0_noY.fa.gz \. --reads=/input/HG00438.chr1_1000000_2000000.bam \. --output_vcf=/output/deepvariant.output.vcf.gz \. --output_gvcf=/output/deepvariant.output.g.vcf.gz \. --num_shards 64 \. --logging_dir=/output/logs \. --intermediate_results_dir /output/intermediate_results_dir \. --make_examples_extra_args ""min_mapping_quality=1,keep_legacy_allele_counter_behavior=true,normalize_reads=true"" \. --report_title test-githubissue \. --regions chr1:1000000-2000000 \. --runtime_report. ```. Then, I checked how many variants there are:. ```. $ zcat deepvariant.output.vcf.gz | grep -v '^#' | wc -l. 4748. ```. A few thoughts and observations:. 1. Looking at IGV, it looks reasonable. (@AndrewCarroll also took a look and agreed). 2. The number of variants in this region didn't seem too unreasonable. . 3. Your BAM is higher coverage, which could also be why more candidates have been made. 4. I have not use CHM13 a lot myself. It is possible that CHM13 has regions that can take DeepVariant make_examples longer time to complete. If you notice specific regions being really slow (which you can use https://github.com/google/deepvariant/blob/r1.6.1/docs/runtime-by-region.md to check), please feel free to report those regions and we can take a closer look from there! 5. For the question of whether you're running Giraffe correctly, I don't think I'm able to help answer that question. I'd encourage you to run your Giraffe+DV setting through something where you have truths for, so you can check hap.py output and see if that's expected. And, to save your time, I recommend running DeepVariant on just one chr (e.g., chr20) to check. Hope this helps!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/847
https://github.com/google/deepvariant/issues/847:1340,interoperability,specif,specific,1340,"@Han-Cao , I ran on your BAM. My command is something like:. ```bash. docker run \. -v /home/pichuan/wgs-case-study/input/data:/input \. -v /home/pichuan/wgs-case-study/output:/output \. google/deepvariant:1.6.1 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/chm13v2.0_noY.fa.gz \. --reads=/input/HG00438.chr1_1000000_2000000.bam \. --output_vcf=/output/deepvariant.output.vcf.gz \. --output_gvcf=/output/deepvariant.output.g.vcf.gz \. --num_shards 64 \. --logging_dir=/output/logs \. --intermediate_results_dir /output/intermediate_results_dir \. --make_examples_extra_args ""min_mapping_quality=1,keep_legacy_allele_counter_behavior=true,normalize_reads=true"" \. --report_title test-githubissue \. --regions chr1:1000000-2000000 \. --runtime_report. ```. Then, I checked how many variants there are:. ```. $ zcat deepvariant.output.vcf.gz | grep -v '^#' | wc -l. 4748. ```. A few thoughts and observations:. 1. Looking at IGV, it looks reasonable. (@AndrewCarroll also took a look and agreed). 2. The number of variants in this region didn't seem too unreasonable. . 3. Your BAM is higher coverage, which could also be why more candidates have been made. 4. I have not use CHM13 a lot myself. It is possible that CHM13 has regions that can take DeepVariant make_examples longer time to complete. If you notice specific regions being really slow (which you can use https://github.com/google/deepvariant/blob/r1.6.1/docs/runtime-by-region.md to check), please feel free to report those regions and we can take a closer look from there! 5. For the question of whether you're running Giraffe correctly, I don't think I'm able to help answer that question. I'd encourage you to run your Giraffe+DV setting through something where you have truths for, so you can check hap.py output and see if that's expected. And, to save your time, I recommend running DeepVariant on just one chr (e.g., chr20) to check. Hope this helps!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/847
https://github.com/google/deepvariant/issues/847:1308,performance,time,time,1308,"@Han-Cao , I ran on your BAM. My command is something like:. ```bash. docker run \. -v /home/pichuan/wgs-case-study/input/data:/input \. -v /home/pichuan/wgs-case-study/output:/output \. google/deepvariant:1.6.1 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/chm13v2.0_noY.fa.gz \. --reads=/input/HG00438.chr1_1000000_2000000.bam \. --output_vcf=/output/deepvariant.output.vcf.gz \. --output_gvcf=/output/deepvariant.output.g.vcf.gz \. --num_shards 64 \. --logging_dir=/output/logs \. --intermediate_results_dir /output/intermediate_results_dir \. --make_examples_extra_args ""min_mapping_quality=1,keep_legacy_allele_counter_behavior=true,normalize_reads=true"" \. --report_title test-githubissue \. --regions chr1:1000000-2000000 \. --runtime_report. ```. Then, I checked how many variants there are:. ```. $ zcat deepvariant.output.vcf.gz | grep -v '^#' | wc -l. 4748. ```. A few thoughts and observations:. 1. Looking at IGV, it looks reasonable. (@AndrewCarroll also took a look and agreed). 2. The number of variants in this region didn't seem too unreasonable. . 3. Your BAM is higher coverage, which could also be why more candidates have been made. 4. I have not use CHM13 a lot myself. It is possible that CHM13 has regions that can take DeepVariant make_examples longer time to complete. If you notice specific regions being really slow (which you can use https://github.com/google/deepvariant/blob/r1.6.1/docs/runtime-by-region.md to check), please feel free to report those regions and we can take a closer look from there! 5. For the question of whether you're running Giraffe correctly, I don't think I'm able to help answer that question. I'd encourage you to run your Giraffe+DV setting through something where you have truths for, so you can check hap.py output and see if that's expected. And, to save your time, I recommend running DeepVariant on just one chr (e.g., chr20) to check. Hope this helps!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/847
https://github.com/google/deepvariant/issues/847:1853,performance,time,time,1853,"@Han-Cao , I ran on your BAM. My command is something like:. ```bash. docker run \. -v /home/pichuan/wgs-case-study/input/data:/input \. -v /home/pichuan/wgs-case-study/output:/output \. google/deepvariant:1.6.1 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/chm13v2.0_noY.fa.gz \. --reads=/input/HG00438.chr1_1000000_2000000.bam \. --output_vcf=/output/deepvariant.output.vcf.gz \. --output_gvcf=/output/deepvariant.output.g.vcf.gz \. --num_shards 64 \. --logging_dir=/output/logs \. --intermediate_results_dir /output/intermediate_results_dir \. --make_examples_extra_args ""min_mapping_quality=1,keep_legacy_allele_counter_behavior=true,normalize_reads=true"" \. --report_title test-githubissue \. --regions chr1:1000000-2000000 \. --runtime_report. ```. Then, I checked how many variants there are:. ```. $ zcat deepvariant.output.vcf.gz | grep -v '^#' | wc -l. 4748. ```. A few thoughts and observations:. 1. Looking at IGV, it looks reasonable. (@AndrewCarroll also took a look and agreed). 2. The number of variants in this region didn't seem too unreasonable. . 3. Your BAM is higher coverage, which could also be why more candidates have been made. 4. I have not use CHM13 a lot myself. It is possible that CHM13 has regions that can take DeepVariant make_examples longer time to complete. If you notice specific regions being really slow (which you can use https://github.com/google/deepvariant/blob/r1.6.1/docs/runtime-by-region.md to check), please feel free to report those regions and we can take a closer look from there! 5. For the question of whether you're running Giraffe correctly, I don't think I'm able to help answer that question. I'd encourage you to run your Giraffe+DV setting through something where you have truths for, so you can check hap.py output and see if that's expected. And, to save your time, I recommend running DeepVariant on just one chr (e.g., chr20) to check. Hope this helps!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/847
https://github.com/google/deepvariant/issues/847:1370,reliability,slo,slow,1370,"@Han-Cao , I ran on your BAM. My command is something like:. ```bash. docker run \. -v /home/pichuan/wgs-case-study/input/data:/input \. -v /home/pichuan/wgs-case-study/output:/output \. google/deepvariant:1.6.1 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/chm13v2.0_noY.fa.gz \. --reads=/input/HG00438.chr1_1000000_2000000.bam \. --output_vcf=/output/deepvariant.output.vcf.gz \. --output_gvcf=/output/deepvariant.output.g.vcf.gz \. --num_shards 64 \. --logging_dir=/output/logs \. --intermediate_results_dir /output/intermediate_results_dir \. --make_examples_extra_args ""min_mapping_quality=1,keep_legacy_allele_counter_behavior=true,normalize_reads=true"" \. --report_title test-githubissue \. --regions chr1:1000000-2000000 \. --runtime_report. ```. Then, I checked how many variants there are:. ```. $ zcat deepvariant.output.vcf.gz | grep -v '^#' | wc -l. 4748. ```. A few thoughts and observations:. 1. Looking at IGV, it looks reasonable. (@AndrewCarroll also took a look and agreed). 2. The number of variants in this region didn't seem too unreasonable. . 3. Your BAM is higher coverage, which could also be why more candidates have been made. 4. I have not use CHM13 a lot myself. It is possible that CHM13 has regions that can take DeepVariant make_examples longer time to complete. If you notice specific regions being really slow (which you can use https://github.com/google/deepvariant/blob/r1.6.1/docs/runtime-by-region.md to check), please feel free to report those regions and we can take a closer look from there! 5. For the question of whether you're running Giraffe correctly, I don't think I'm able to help answer that question. I'd encourage you to run your Giraffe+DV setting through something where you have truths for, so you can check hap.py output and see if that's expected. And, to save your time, I recommend running DeepVariant on just one chr (e.g., chr20) to check. Hope this helps!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/847
https://github.com/google/deepvariant/issues/847:116,safety,input,input,116,"@Han-Cao , I ran on your BAM. My command is something like:. ```bash. docker run \. -v /home/pichuan/wgs-case-study/input/data:/input \. -v /home/pichuan/wgs-case-study/output:/output \. google/deepvariant:1.6.1 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/chm13v2.0_noY.fa.gz \. --reads=/input/HG00438.chr1_1000000_2000000.bam \. --output_vcf=/output/deepvariant.output.vcf.gz \. --output_gvcf=/output/deepvariant.output.g.vcf.gz \. --num_shards 64 \. --logging_dir=/output/logs \. --intermediate_results_dir /output/intermediate_results_dir \. --make_examples_extra_args ""min_mapping_quality=1,keep_legacy_allele_counter_behavior=true,normalize_reads=true"" \. --report_title test-githubissue \. --regions chr1:1000000-2000000 \. --runtime_report. ```. Then, I checked how many variants there are:. ```. $ zcat deepvariant.output.vcf.gz | grep -v '^#' | wc -l. 4748. ```. A few thoughts and observations:. 1. Looking at IGV, it looks reasonable. (@AndrewCarroll also took a look and agreed). 2. The number of variants in this region didn't seem too unreasonable. . 3. Your BAM is higher coverage, which could also be why more candidates have been made. 4. I have not use CHM13 a lot myself. It is possible that CHM13 has regions that can take DeepVariant make_examples longer time to complete. If you notice specific regions being really slow (which you can use https://github.com/google/deepvariant/blob/r1.6.1/docs/runtime-by-region.md to check), please feel free to report those regions and we can take a closer look from there! 5. For the question of whether you're running Giraffe correctly, I don't think I'm able to help answer that question. I'd encourage you to run your Giraffe+DV setting through something where you have truths for, so you can check hap.py output and see if that's expected. And, to save your time, I recommend running DeepVariant on just one chr (e.g., chr20) to check. Hope this helps!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/847
https://github.com/google/deepvariant/issues/847:128,safety,input,input,128,"@Han-Cao , I ran on your BAM. My command is something like:. ```bash. docker run \. -v /home/pichuan/wgs-case-study/input/data:/input \. -v /home/pichuan/wgs-case-study/output:/output \. google/deepvariant:1.6.1 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/chm13v2.0_noY.fa.gz \. --reads=/input/HG00438.chr1_1000000_2000000.bam \. --output_vcf=/output/deepvariant.output.vcf.gz \. --output_gvcf=/output/deepvariant.output.g.vcf.gz \. --num_shards 64 \. --logging_dir=/output/logs \. --intermediate_results_dir /output/intermediate_results_dir \. --make_examples_extra_args ""min_mapping_quality=1,keep_legacy_allele_counter_behavior=true,normalize_reads=true"" \. --report_title test-githubissue \. --regions chr1:1000000-2000000 \. --runtime_report. ```. Then, I checked how many variants there are:. ```. $ zcat deepvariant.output.vcf.gz | grep -v '^#' | wc -l. 4748. ```. A few thoughts and observations:. 1. Looking at IGV, it looks reasonable. (@AndrewCarroll also took a look and agreed). 2. The number of variants in this region didn't seem too unreasonable. . 3. Your BAM is higher coverage, which could also be why more candidates have been made. 4. I have not use CHM13 a lot myself. It is possible that CHM13 has regions that can take DeepVariant make_examples longer time to complete. If you notice specific regions being really slow (which you can use https://github.com/google/deepvariant/blob/r1.6.1/docs/runtime-by-region.md to check), please feel free to report those regions and we can take a closer look from there! 5. For the question of whether you're running Giraffe correctly, I don't think I'm able to help answer that question. I'd encourage you to run your Giraffe+DV setting through something where you have truths for, so you can check hap.py output and see if that's expected. And, to save your time, I recommend running DeepVariant on just one chr (e.g., chr20) to check. Hope this helps!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/847
https://github.com/google/deepvariant/issues/847:282,safety,input,input,282,"@Han-Cao , I ran on your BAM. My command is something like:. ```bash. docker run \. -v /home/pichuan/wgs-case-study/input/data:/input \. -v /home/pichuan/wgs-case-study/output:/output \. google/deepvariant:1.6.1 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/chm13v2.0_noY.fa.gz \. --reads=/input/HG00438.chr1_1000000_2000000.bam \. --output_vcf=/output/deepvariant.output.vcf.gz \. --output_gvcf=/output/deepvariant.output.g.vcf.gz \. --num_shards 64 \. --logging_dir=/output/logs \. --intermediate_results_dir /output/intermediate_results_dir \. --make_examples_extra_args ""min_mapping_quality=1,keep_legacy_allele_counter_behavior=true,normalize_reads=true"" \. --report_title test-githubissue \. --regions chr1:1000000-2000000 \. --runtime_report. ```. Then, I checked how many variants there are:. ```. $ zcat deepvariant.output.vcf.gz | grep -v '^#' | wc -l. 4748. ```. A few thoughts and observations:. 1. Looking at IGV, it looks reasonable. (@AndrewCarroll also took a look and agreed). 2. The number of variants in this region didn't seem too unreasonable. . 3. Your BAM is higher coverage, which could also be why more candidates have been made. 4. I have not use CHM13 a lot myself. It is possible that CHM13 has regions that can take DeepVariant make_examples longer time to complete. If you notice specific regions being really slow (which you can use https://github.com/google/deepvariant/blob/r1.6.1/docs/runtime-by-region.md to check), please feel free to report those regions and we can take a closer look from there! 5. For the question of whether you're running Giraffe correctly, I don't think I'm able to help answer that question. I'd encourage you to run your Giraffe+DV setting through something where you have truths for, so you can check hap.py output and see if that's expected. And, to save your time, I recommend running DeepVariant on just one chr (e.g., chr20) to check. Hope this helps!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/847
https://github.com/google/deepvariant/issues/847:320,safety,input,input,320,"@Han-Cao , I ran on your BAM. My command is something like:. ```bash. docker run \. -v /home/pichuan/wgs-case-study/input/data:/input \. -v /home/pichuan/wgs-case-study/output:/output \. google/deepvariant:1.6.1 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/chm13v2.0_noY.fa.gz \. --reads=/input/HG00438.chr1_1000000_2000000.bam \. --output_vcf=/output/deepvariant.output.vcf.gz \. --output_gvcf=/output/deepvariant.output.g.vcf.gz \. --num_shards 64 \. --logging_dir=/output/logs \. --intermediate_results_dir /output/intermediate_results_dir \. --make_examples_extra_args ""min_mapping_quality=1,keep_legacy_allele_counter_behavior=true,normalize_reads=true"" \. --report_title test-githubissue \. --regions chr1:1000000-2000000 \. --runtime_report. ```. Then, I checked how many variants there are:. ```. $ zcat deepvariant.output.vcf.gz | grep -v '^#' | wc -l. 4748. ```. A few thoughts and observations:. 1. Looking at IGV, it looks reasonable. (@AndrewCarroll also took a look and agreed). 2. The number of variants in this region didn't seem too unreasonable. . 3. Your BAM is higher coverage, which could also be why more candidates have been made. 4. I have not use CHM13 a lot myself. It is possible that CHM13 has regions that can take DeepVariant make_examples longer time to complete. If you notice specific regions being really slow (which you can use https://github.com/google/deepvariant/blob/r1.6.1/docs/runtime-by-region.md to check), please feel free to report those regions and we can take a closer look from there! 5. For the question of whether you're running Giraffe correctly, I don't think I'm able to help answer that question. I'd encourage you to run your Giraffe+DV setting through something where you have truths for, so you can check hap.py output and see if that's expected. And, to save your time, I recommend running DeepVariant on just one chr (e.g., chr20) to check. Hope this helps!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/847
https://github.com/google/deepvariant/issues/847:506,safety,log,logs,506,"@Han-Cao , I ran on your BAM. My command is something like:. ```bash. docker run \. -v /home/pichuan/wgs-case-study/input/data:/input \. -v /home/pichuan/wgs-case-study/output:/output \. google/deepvariant:1.6.1 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/chm13v2.0_noY.fa.gz \. --reads=/input/HG00438.chr1_1000000_2000000.bam \. --output_vcf=/output/deepvariant.output.vcf.gz \. --output_gvcf=/output/deepvariant.output.g.vcf.gz \. --num_shards 64 \. --logging_dir=/output/logs \. --intermediate_results_dir /output/intermediate_results_dir \. --make_examples_extra_args ""min_mapping_quality=1,keep_legacy_allele_counter_behavior=true,normalize_reads=true"" \. --report_title test-githubissue \. --regions chr1:1000000-2000000 \. --runtime_report. ```. Then, I checked how many variants there are:. ```. $ zcat deepvariant.output.vcf.gz | grep -v '^#' | wc -l. 4748. ```. A few thoughts and observations:. 1. Looking at IGV, it looks reasonable. (@AndrewCarroll also took a look and agreed). 2. The number of variants in this region didn't seem too unreasonable. . 3. Your BAM is higher coverage, which could also be why more candidates have been made. 4. I have not use CHM13 a lot myself. It is possible that CHM13 has regions that can take DeepVariant make_examples longer time to complete. If you notice specific regions being really slow (which you can use https://github.com/google/deepvariant/blob/r1.6.1/docs/runtime-by-region.md to check), please feel free to report those regions and we can take a closer look from there! 5. For the question of whether you're running Giraffe correctly, I don't think I'm able to help answer that question. I'd encourage you to run your Giraffe+DV setting through something where you have truths for, so you can check hap.py output and see if that's expected. And, to save your time, I recommend running DeepVariant on just one chr (e.g., chr20) to check. Hope this helps!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/847
https://github.com/google/deepvariant/issues/847:708,safety,test,test-githubissue,708,"@Han-Cao , I ran on your BAM. My command is something like:. ```bash. docker run \. -v /home/pichuan/wgs-case-study/input/data:/input \. -v /home/pichuan/wgs-case-study/output:/output \. google/deepvariant:1.6.1 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/chm13v2.0_noY.fa.gz \. --reads=/input/HG00438.chr1_1000000_2000000.bam \. --output_vcf=/output/deepvariant.output.vcf.gz \. --output_gvcf=/output/deepvariant.output.g.vcf.gz \. --num_shards 64 \. --logging_dir=/output/logs \. --intermediate_results_dir /output/intermediate_results_dir \. --make_examples_extra_args ""min_mapping_quality=1,keep_legacy_allele_counter_behavior=true,normalize_reads=true"" \. --report_title test-githubissue \. --regions chr1:1000000-2000000 \. --runtime_report. ```. Then, I checked how many variants there are:. ```. $ zcat deepvariant.output.vcf.gz | grep -v '^#' | wc -l. 4748. ```. A few thoughts and observations:. 1. Looking at IGV, it looks reasonable. (@AndrewCarroll also took a look and agreed). 2. The number of variants in this region didn't seem too unreasonable. . 3. Your BAM is higher coverage, which could also be why more candidates have been made. 4. I have not use CHM13 a lot myself. It is possible that CHM13 has regions that can take DeepVariant make_examples longer time to complete. If you notice specific regions being really slow (which you can use https://github.com/google/deepvariant/blob/r1.6.1/docs/runtime-by-region.md to check), please feel free to report those regions and we can take a closer look from there! 5. For the question of whether you're running Giraffe correctly, I don't think I'm able to help answer that question. I'd encourage you to run your Giraffe+DV setting through something where you have truths for, so you can check hap.py output and see if that's expected. And, to save your time, I recommend running DeepVariant on just one chr (e.g., chr20) to check. Hope this helps!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/847
https://github.com/google/deepvariant/issues/847:1316,safety,compl,complete,1316,"@Han-Cao , I ran on your BAM. My command is something like:. ```bash. docker run \. -v /home/pichuan/wgs-case-study/input/data:/input \. -v /home/pichuan/wgs-case-study/output:/output \. google/deepvariant:1.6.1 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/chm13v2.0_noY.fa.gz \. --reads=/input/HG00438.chr1_1000000_2000000.bam \. --output_vcf=/output/deepvariant.output.vcf.gz \. --output_gvcf=/output/deepvariant.output.g.vcf.gz \. --num_shards 64 \. --logging_dir=/output/logs \. --intermediate_results_dir /output/intermediate_results_dir \. --make_examples_extra_args ""min_mapping_quality=1,keep_legacy_allele_counter_behavior=true,normalize_reads=true"" \. --report_title test-githubissue \. --regions chr1:1000000-2000000 \. --runtime_report. ```. Then, I checked how many variants there are:. ```. $ zcat deepvariant.output.vcf.gz | grep -v '^#' | wc -l. 4748. ```. A few thoughts and observations:. 1. Looking at IGV, it looks reasonable. (@AndrewCarroll also took a look and agreed). 2. The number of variants in this region didn't seem too unreasonable. . 3. Your BAM is higher coverage, which could also be why more candidates have been made. 4. I have not use CHM13 a lot myself. It is possible that CHM13 has regions that can take DeepVariant make_examples longer time to complete. If you notice specific regions being really slow (which you can use https://github.com/google/deepvariant/blob/r1.6.1/docs/runtime-by-region.md to check), please feel free to report those regions and we can take a closer look from there! 5. For the question of whether you're running Giraffe correctly, I don't think I'm able to help answer that question. I'd encourage you to run your Giraffe+DV setting through something where you have truths for, so you can check hap.py output and see if that's expected. And, to save your time, I recommend running DeepVariant on just one chr (e.g., chr20) to check. Hope this helps!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/847
https://github.com/google/deepvariant/issues/847:506,security,log,logs,506,"@Han-Cao , I ran on your BAM. My command is something like:. ```bash. docker run \. -v /home/pichuan/wgs-case-study/input/data:/input \. -v /home/pichuan/wgs-case-study/output:/output \. google/deepvariant:1.6.1 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/chm13v2.0_noY.fa.gz \. --reads=/input/HG00438.chr1_1000000_2000000.bam \. --output_vcf=/output/deepvariant.output.vcf.gz \. --output_gvcf=/output/deepvariant.output.g.vcf.gz \. --num_shards 64 \. --logging_dir=/output/logs \. --intermediate_results_dir /output/intermediate_results_dir \. --make_examples_extra_args ""min_mapping_quality=1,keep_legacy_allele_counter_behavior=true,normalize_reads=true"" \. --report_title test-githubissue \. --regions chr1:1000000-2000000 \. --runtime_report. ```. Then, I checked how many variants there are:. ```. $ zcat deepvariant.output.vcf.gz | grep -v '^#' | wc -l. 4748. ```. A few thoughts and observations:. 1. Looking at IGV, it looks reasonable. (@AndrewCarroll also took a look and agreed). 2. The number of variants in this region didn't seem too unreasonable. . 3. Your BAM is higher coverage, which could also be why more candidates have been made. 4. I have not use CHM13 a lot myself. It is possible that CHM13 has regions that can take DeepVariant make_examples longer time to complete. If you notice specific regions being really slow (which you can use https://github.com/google/deepvariant/blob/r1.6.1/docs/runtime-by-region.md to check), please feel free to report those regions and we can take a closer look from there! 5. For the question of whether you're running Giraffe correctly, I don't think I'm able to help answer that question. I'd encourage you to run your Giraffe+DV setting through something where you have truths for, so you can check hap.py output and see if that's expected. And, to save your time, I recommend running DeepVariant on just one chr (e.g., chr20) to check. Hope this helps!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/847
https://github.com/google/deepvariant/issues/847:1316,security,compl,complete,1316,"@Han-Cao , I ran on your BAM. My command is something like:. ```bash. docker run \. -v /home/pichuan/wgs-case-study/input/data:/input \. -v /home/pichuan/wgs-case-study/output:/output \. google/deepvariant:1.6.1 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/chm13v2.0_noY.fa.gz \. --reads=/input/HG00438.chr1_1000000_2000000.bam \. --output_vcf=/output/deepvariant.output.vcf.gz \. --output_gvcf=/output/deepvariant.output.g.vcf.gz \. --num_shards 64 \. --logging_dir=/output/logs \. --intermediate_results_dir /output/intermediate_results_dir \. --make_examples_extra_args ""min_mapping_quality=1,keep_legacy_allele_counter_behavior=true,normalize_reads=true"" \. --report_title test-githubissue \. --regions chr1:1000000-2000000 \. --runtime_report. ```. Then, I checked how many variants there are:. ```. $ zcat deepvariant.output.vcf.gz | grep -v '^#' | wc -l. 4748. ```. A few thoughts and observations:. 1. Looking at IGV, it looks reasonable. (@AndrewCarroll also took a look and agreed). 2. The number of variants in this region didn't seem too unreasonable. . 3. Your BAM is higher coverage, which could also be why more candidates have been made. 4. I have not use CHM13 a lot myself. It is possible that CHM13 has regions that can take DeepVariant make_examples longer time to complete. If you notice specific regions being really slow (which you can use https://github.com/google/deepvariant/blob/r1.6.1/docs/runtime-by-region.md to check), please feel free to report those regions and we can take a closer look from there! 5. For the question of whether you're running Giraffe correctly, I don't think I'm able to help answer that question. I'd encourage you to run your Giraffe+DV setting through something where you have truths for, so you can check hap.py output and see if that's expected. And, to save your time, I recommend running DeepVariant on just one chr (e.g., chr20) to check. Hope this helps!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/847
https://github.com/google/deepvariant/issues/847:506,testability,log,logs,506,"@Han-Cao , I ran on your BAM. My command is something like:. ```bash. docker run \. -v /home/pichuan/wgs-case-study/input/data:/input \. -v /home/pichuan/wgs-case-study/output:/output \. google/deepvariant:1.6.1 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/chm13v2.0_noY.fa.gz \. --reads=/input/HG00438.chr1_1000000_2000000.bam \. --output_vcf=/output/deepvariant.output.vcf.gz \. --output_gvcf=/output/deepvariant.output.g.vcf.gz \. --num_shards 64 \. --logging_dir=/output/logs \. --intermediate_results_dir /output/intermediate_results_dir \. --make_examples_extra_args ""min_mapping_quality=1,keep_legacy_allele_counter_behavior=true,normalize_reads=true"" \. --report_title test-githubissue \. --regions chr1:1000000-2000000 \. --runtime_report. ```. Then, I checked how many variants there are:. ```. $ zcat deepvariant.output.vcf.gz | grep -v '^#' | wc -l. 4748. ```. A few thoughts and observations:. 1. Looking at IGV, it looks reasonable. (@AndrewCarroll also took a look and agreed). 2. The number of variants in this region didn't seem too unreasonable. . 3. Your BAM is higher coverage, which could also be why more candidates have been made. 4. I have not use CHM13 a lot myself. It is possible that CHM13 has regions that can take DeepVariant make_examples longer time to complete. If you notice specific regions being really slow (which you can use https://github.com/google/deepvariant/blob/r1.6.1/docs/runtime-by-region.md to check), please feel free to report those regions and we can take a closer look from there! 5. For the question of whether you're running Giraffe correctly, I don't think I'm able to help answer that question. I'd encourage you to run your Giraffe+DV setting through something where you have truths for, so you can check hap.py output and see if that's expected. And, to save your time, I recommend running DeepVariant on just one chr (e.g., chr20) to check. Hope this helps!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/847
https://github.com/google/deepvariant/issues/847:708,testability,test,test-githubissue,708,"@Han-Cao , I ran on your BAM. My command is something like:. ```bash. docker run \. -v /home/pichuan/wgs-case-study/input/data:/input \. -v /home/pichuan/wgs-case-study/output:/output \. google/deepvariant:1.6.1 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/chm13v2.0_noY.fa.gz \. --reads=/input/HG00438.chr1_1000000_2000000.bam \. --output_vcf=/output/deepvariant.output.vcf.gz \. --output_gvcf=/output/deepvariant.output.g.vcf.gz \. --num_shards 64 \. --logging_dir=/output/logs \. --intermediate_results_dir /output/intermediate_results_dir \. --make_examples_extra_args ""min_mapping_quality=1,keep_legacy_allele_counter_behavior=true,normalize_reads=true"" \. --report_title test-githubissue \. --regions chr1:1000000-2000000 \. --runtime_report. ```. Then, I checked how many variants there are:. ```. $ zcat deepvariant.output.vcf.gz | grep -v '^#' | wc -l. 4748. ```. A few thoughts and observations:. 1. Looking at IGV, it looks reasonable. (@AndrewCarroll also took a look and agreed). 2. The number of variants in this region didn't seem too unreasonable. . 3. Your BAM is higher coverage, which could also be why more candidates have been made. 4. I have not use CHM13 a lot myself. It is possible that CHM13 has regions that can take DeepVariant make_examples longer time to complete. If you notice specific regions being really slow (which you can use https://github.com/google/deepvariant/blob/r1.6.1/docs/runtime-by-region.md to check), please feel free to report those regions and we can take a closer look from there! 5. For the question of whether you're running Giraffe correctly, I don't think I'm able to help answer that question. I'd encourage you to run your Giraffe+DV setting through something where you have truths for, so you can check hap.py output and see if that's expected. And, to save your time, I recommend running DeepVariant on just one chr (e.g., chr20) to check. Hope this helps!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/847
https://github.com/google/deepvariant/issues/847:923,testability,observ,observations,923,"@Han-Cao , I ran on your BAM. My command is something like:. ```bash. docker run \. -v /home/pichuan/wgs-case-study/input/data:/input \. -v /home/pichuan/wgs-case-study/output:/output \. google/deepvariant:1.6.1 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/chm13v2.0_noY.fa.gz \. --reads=/input/HG00438.chr1_1000000_2000000.bam \. --output_vcf=/output/deepvariant.output.vcf.gz \. --output_gvcf=/output/deepvariant.output.g.vcf.gz \. --num_shards 64 \. --logging_dir=/output/logs \. --intermediate_results_dir /output/intermediate_results_dir \. --make_examples_extra_args ""min_mapping_quality=1,keep_legacy_allele_counter_behavior=true,normalize_reads=true"" \. --report_title test-githubissue \. --regions chr1:1000000-2000000 \. --runtime_report. ```. Then, I checked how many variants there are:. ```. $ zcat deepvariant.output.vcf.gz | grep -v '^#' | wc -l. 4748. ```. A few thoughts and observations:. 1. Looking at IGV, it looks reasonable. (@AndrewCarroll also took a look and agreed). 2. The number of variants in this region didn't seem too unreasonable. . 3. Your BAM is higher coverage, which could also be why more candidates have been made. 4. I have not use CHM13 a lot myself. It is possible that CHM13 has regions that can take DeepVariant make_examples longer time to complete. If you notice specific regions being really slow (which you can use https://github.com/google/deepvariant/blob/r1.6.1/docs/runtime-by-region.md to check), please feel free to report those regions and we can take a closer look from there! 5. For the question of whether you're running Giraffe correctly, I don't think I'm able to help answer that question. I'd encourage you to run your Giraffe+DV setting through something where you have truths for, so you can check hap.py output and see if that's expected. And, to save your time, I recommend running DeepVariant on just one chr (e.g., chr20) to check. Hope this helps!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/847
https://github.com/google/deepvariant/issues/847:1119,testability,coverag,coverage,1119,"@Han-Cao , I ran on your BAM. My command is something like:. ```bash. docker run \. -v /home/pichuan/wgs-case-study/input/data:/input \. -v /home/pichuan/wgs-case-study/output:/output \. google/deepvariant:1.6.1 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/chm13v2.0_noY.fa.gz \. --reads=/input/HG00438.chr1_1000000_2000000.bam \. --output_vcf=/output/deepvariant.output.vcf.gz \. --output_gvcf=/output/deepvariant.output.g.vcf.gz \. --num_shards 64 \. --logging_dir=/output/logs \. --intermediate_results_dir /output/intermediate_results_dir \. --make_examples_extra_args ""min_mapping_quality=1,keep_legacy_allele_counter_behavior=true,normalize_reads=true"" \. --report_title test-githubissue \. --regions chr1:1000000-2000000 \. --runtime_report. ```. Then, I checked how many variants there are:. ```. $ zcat deepvariant.output.vcf.gz | grep -v '^#' | wc -l. 4748. ```. A few thoughts and observations:. 1. Looking at IGV, it looks reasonable. (@AndrewCarroll also took a look and agreed). 2. The number of variants in this region didn't seem too unreasonable. . 3. Your BAM is higher coverage, which could also be why more candidates have been made. 4. I have not use CHM13 a lot myself. It is possible that CHM13 has regions that can take DeepVariant make_examples longer time to complete. If you notice specific regions being really slow (which you can use https://github.com/google/deepvariant/blob/r1.6.1/docs/runtime-by-region.md to check), please feel free to report those regions and we can take a closer look from there! 5. For the question of whether you're running Giraffe correctly, I don't think I'm able to help answer that question. I'd encourage you to run your Giraffe+DV setting through something where you have truths for, so you can check hap.py output and see if that's expected. And, to save your time, I recommend running DeepVariant on just one chr (e.g., chr20) to check. Hope this helps!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/847
https://github.com/google/deepvariant/issues/847:33,usability,command,command,33,"@Han-Cao , I ran on your BAM. My command is something like:. ```bash. docker run \. -v /home/pichuan/wgs-case-study/input/data:/input \. -v /home/pichuan/wgs-case-study/output:/output \. google/deepvariant:1.6.1 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/chm13v2.0_noY.fa.gz \. --reads=/input/HG00438.chr1_1000000_2000000.bam \. --output_vcf=/output/deepvariant.output.vcf.gz \. --output_gvcf=/output/deepvariant.output.g.vcf.gz \. --num_shards 64 \. --logging_dir=/output/logs \. --intermediate_results_dir /output/intermediate_results_dir \. --make_examples_extra_args ""min_mapping_quality=1,keep_legacy_allele_counter_behavior=true,normalize_reads=true"" \. --report_title test-githubissue \. --regions chr1:1000000-2000000 \. --runtime_report. ```. Then, I checked how many variants there are:. ```. $ zcat deepvariant.output.vcf.gz | grep -v '^#' | wc -l. 4748. ```. A few thoughts and observations:. 1. Looking at IGV, it looks reasonable. (@AndrewCarroll also took a look and agreed). 2. The number of variants in this region didn't seem too unreasonable. . 3. Your BAM is higher coverage, which could also be why more candidates have been made. 4. I have not use CHM13 a lot myself. It is possible that CHM13 has regions that can take DeepVariant make_examples longer time to complete. If you notice specific regions being really slow (which you can use https://github.com/google/deepvariant/blob/r1.6.1/docs/runtime-by-region.md to check), please feel free to report those regions and we can take a closer look from there! 5. For the question of whether you're running Giraffe correctly, I don't think I'm able to help answer that question. I'd encourage you to run your Giraffe+DV setting through something where you have truths for, so you can check hap.py output and see if that's expected. And, to save your time, I recommend running DeepVariant on just one chr (e.g., chr20) to check. Hope this helps!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/847
https://github.com/google/deepvariant/issues/847:116,usability,input,input,116,"@Han-Cao , I ran on your BAM. My command is something like:. ```bash. docker run \. -v /home/pichuan/wgs-case-study/input/data:/input \. -v /home/pichuan/wgs-case-study/output:/output \. google/deepvariant:1.6.1 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/chm13v2.0_noY.fa.gz \. --reads=/input/HG00438.chr1_1000000_2000000.bam \. --output_vcf=/output/deepvariant.output.vcf.gz \. --output_gvcf=/output/deepvariant.output.g.vcf.gz \. --num_shards 64 \. --logging_dir=/output/logs \. --intermediate_results_dir /output/intermediate_results_dir \. --make_examples_extra_args ""min_mapping_quality=1,keep_legacy_allele_counter_behavior=true,normalize_reads=true"" \. --report_title test-githubissue \. --regions chr1:1000000-2000000 \. --runtime_report. ```. Then, I checked how many variants there are:. ```. $ zcat deepvariant.output.vcf.gz | grep -v '^#' | wc -l. 4748. ```. A few thoughts and observations:. 1. Looking at IGV, it looks reasonable. (@AndrewCarroll also took a look and agreed). 2. The number of variants in this region didn't seem too unreasonable. . 3. Your BAM is higher coverage, which could also be why more candidates have been made. 4. I have not use CHM13 a lot myself. It is possible that CHM13 has regions that can take DeepVariant make_examples longer time to complete. If you notice specific regions being really slow (which you can use https://github.com/google/deepvariant/blob/r1.6.1/docs/runtime-by-region.md to check), please feel free to report those regions and we can take a closer look from there! 5. For the question of whether you're running Giraffe correctly, I don't think I'm able to help answer that question. I'd encourage you to run your Giraffe+DV setting through something where you have truths for, so you can check hap.py output and see if that's expected. And, to save your time, I recommend running DeepVariant on just one chr (e.g., chr20) to check. Hope this helps!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/847
https://github.com/google/deepvariant/issues/847:128,usability,input,input,128,"@Han-Cao , I ran on your BAM. My command is something like:. ```bash. docker run \. -v /home/pichuan/wgs-case-study/input/data:/input \. -v /home/pichuan/wgs-case-study/output:/output \. google/deepvariant:1.6.1 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/chm13v2.0_noY.fa.gz \. --reads=/input/HG00438.chr1_1000000_2000000.bam \. --output_vcf=/output/deepvariant.output.vcf.gz \. --output_gvcf=/output/deepvariant.output.g.vcf.gz \. --num_shards 64 \. --logging_dir=/output/logs \. --intermediate_results_dir /output/intermediate_results_dir \. --make_examples_extra_args ""min_mapping_quality=1,keep_legacy_allele_counter_behavior=true,normalize_reads=true"" \. --report_title test-githubissue \. --regions chr1:1000000-2000000 \. --runtime_report. ```. Then, I checked how many variants there are:. ```. $ zcat deepvariant.output.vcf.gz | grep -v '^#' | wc -l. 4748. ```. A few thoughts and observations:. 1. Looking at IGV, it looks reasonable. (@AndrewCarroll also took a look and agreed). 2. The number of variants in this region didn't seem too unreasonable. . 3. Your BAM is higher coverage, which could also be why more candidates have been made. 4. I have not use CHM13 a lot myself. It is possible that CHM13 has regions that can take DeepVariant make_examples longer time to complete. If you notice specific regions being really slow (which you can use https://github.com/google/deepvariant/blob/r1.6.1/docs/runtime-by-region.md to check), please feel free to report those regions and we can take a closer look from there! 5. For the question of whether you're running Giraffe correctly, I don't think I'm able to help answer that question. I'd encourage you to run your Giraffe+DV setting through something where you have truths for, so you can check hap.py output and see if that's expected. And, to save your time, I recommend running DeepVariant on just one chr (e.g., chr20) to check. Hope this helps!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/847
https://github.com/google/deepvariant/issues/847:282,usability,input,input,282,"@Han-Cao , I ran on your BAM. My command is something like:. ```bash. docker run \. -v /home/pichuan/wgs-case-study/input/data:/input \. -v /home/pichuan/wgs-case-study/output:/output \. google/deepvariant:1.6.1 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/chm13v2.0_noY.fa.gz \. --reads=/input/HG00438.chr1_1000000_2000000.bam \. --output_vcf=/output/deepvariant.output.vcf.gz \. --output_gvcf=/output/deepvariant.output.g.vcf.gz \. --num_shards 64 \. --logging_dir=/output/logs \. --intermediate_results_dir /output/intermediate_results_dir \. --make_examples_extra_args ""min_mapping_quality=1,keep_legacy_allele_counter_behavior=true,normalize_reads=true"" \. --report_title test-githubissue \. --regions chr1:1000000-2000000 \. --runtime_report. ```. Then, I checked how many variants there are:. ```. $ zcat deepvariant.output.vcf.gz | grep -v '^#' | wc -l. 4748. ```. A few thoughts and observations:. 1. Looking at IGV, it looks reasonable. (@AndrewCarroll also took a look and agreed). 2. The number of variants in this region didn't seem too unreasonable. . 3. Your BAM is higher coverage, which could also be why more candidates have been made. 4. I have not use CHM13 a lot myself. It is possible that CHM13 has regions that can take DeepVariant make_examples longer time to complete. If you notice specific regions being really slow (which you can use https://github.com/google/deepvariant/blob/r1.6.1/docs/runtime-by-region.md to check), please feel free to report those regions and we can take a closer look from there! 5. For the question of whether you're running Giraffe correctly, I don't think I'm able to help answer that question. I'd encourage you to run your Giraffe+DV setting through something where you have truths for, so you can check hap.py output and see if that's expected. And, to save your time, I recommend running DeepVariant on just one chr (e.g., chr20) to check. Hope this helps!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/847
https://github.com/google/deepvariant/issues/847:320,usability,input,input,320,"@Han-Cao , I ran on your BAM. My command is something like:. ```bash. docker run \. -v /home/pichuan/wgs-case-study/input/data:/input \. -v /home/pichuan/wgs-case-study/output:/output \. google/deepvariant:1.6.1 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/chm13v2.0_noY.fa.gz \. --reads=/input/HG00438.chr1_1000000_2000000.bam \. --output_vcf=/output/deepvariant.output.vcf.gz \. --output_gvcf=/output/deepvariant.output.g.vcf.gz \. --num_shards 64 \. --logging_dir=/output/logs \. --intermediate_results_dir /output/intermediate_results_dir \. --make_examples_extra_args ""min_mapping_quality=1,keep_legacy_allele_counter_behavior=true,normalize_reads=true"" \. --report_title test-githubissue \. --regions chr1:1000000-2000000 \. --runtime_report. ```. Then, I checked how many variants there are:. ```. $ zcat deepvariant.output.vcf.gz | grep -v '^#' | wc -l. 4748. ```. A few thoughts and observations:. 1. Looking at IGV, it looks reasonable. (@AndrewCarroll also took a look and agreed). 2. The number of variants in this region didn't seem too unreasonable. . 3. Your BAM is higher coverage, which could also be why more candidates have been made. 4. I have not use CHM13 a lot myself. It is possible that CHM13 has regions that can take DeepVariant make_examples longer time to complete. If you notice specific regions being really slow (which you can use https://github.com/google/deepvariant/blob/r1.6.1/docs/runtime-by-region.md to check), please feel free to report those regions and we can take a closer look from there! 5. For the question of whether you're running Giraffe correctly, I don't think I'm able to help answer that question. I'd encourage you to run your Giraffe+DV setting through something where you have truths for, so you can check hap.py output and see if that's expected. And, to save your time, I recommend running DeepVariant on just one chr (e.g., chr20) to check. Hope this helps!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/847
https://github.com/google/deepvariant/issues/847:1540,usability,close,closer,1540,"@Han-Cao , I ran on your BAM. My command is something like:. ```bash. docker run \. -v /home/pichuan/wgs-case-study/input/data:/input \. -v /home/pichuan/wgs-case-study/output:/output \. google/deepvariant:1.6.1 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/chm13v2.0_noY.fa.gz \. --reads=/input/HG00438.chr1_1000000_2000000.bam \. --output_vcf=/output/deepvariant.output.vcf.gz \. --output_gvcf=/output/deepvariant.output.g.vcf.gz \. --num_shards 64 \. --logging_dir=/output/logs \. --intermediate_results_dir /output/intermediate_results_dir \. --make_examples_extra_args ""min_mapping_quality=1,keep_legacy_allele_counter_behavior=true,normalize_reads=true"" \. --report_title test-githubissue \. --regions chr1:1000000-2000000 \. --runtime_report. ```. Then, I checked how many variants there are:. ```. $ zcat deepvariant.output.vcf.gz | grep -v '^#' | wc -l. 4748. ```. A few thoughts and observations:. 1. Looking at IGV, it looks reasonable. (@AndrewCarroll also took a look and agreed). 2. The number of variants in this region didn't seem too unreasonable. . 3. Your BAM is higher coverage, which could also be why more candidates have been made. 4. I have not use CHM13 a lot myself. It is possible that CHM13 has regions that can take DeepVariant make_examples longer time to complete. If you notice specific regions being really slow (which you can use https://github.com/google/deepvariant/blob/r1.6.1/docs/runtime-by-region.md to check), please feel free to report those regions and we can take a closer look from there! 5. For the question of whether you're running Giraffe correctly, I don't think I'm able to help answer that question. I'd encourage you to run your Giraffe+DV setting through something where you have truths for, so you can check hap.py output and see if that's expected. And, to save your time, I recommend running DeepVariant on just one chr (e.g., chr20) to check. Hope this helps!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/847
https://github.com/google/deepvariant/issues/847:1655,usability,help,help,1655,"@Han-Cao , I ran on your BAM. My command is something like:. ```bash. docker run \. -v /home/pichuan/wgs-case-study/input/data:/input \. -v /home/pichuan/wgs-case-study/output:/output \. google/deepvariant:1.6.1 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/chm13v2.0_noY.fa.gz \. --reads=/input/HG00438.chr1_1000000_2000000.bam \. --output_vcf=/output/deepvariant.output.vcf.gz \. --output_gvcf=/output/deepvariant.output.g.vcf.gz \. --num_shards 64 \. --logging_dir=/output/logs \. --intermediate_results_dir /output/intermediate_results_dir \. --make_examples_extra_args ""min_mapping_quality=1,keep_legacy_allele_counter_behavior=true,normalize_reads=true"" \. --report_title test-githubissue \. --regions chr1:1000000-2000000 \. --runtime_report. ```. Then, I checked how many variants there are:. ```. $ zcat deepvariant.output.vcf.gz | grep -v '^#' | wc -l. 4748. ```. A few thoughts and observations:. 1. Looking at IGV, it looks reasonable. (@AndrewCarroll also took a look and agreed). 2. The number of variants in this region didn't seem too unreasonable. . 3. Your BAM is higher coverage, which could also be why more candidates have been made. 4. I have not use CHM13 a lot myself. It is possible that CHM13 has regions that can take DeepVariant make_examples longer time to complete. If you notice specific regions being really slow (which you can use https://github.com/google/deepvariant/blob/r1.6.1/docs/runtime-by-region.md to check), please feel free to report those regions and we can take a closer look from there! 5. For the question of whether you're running Giraffe correctly, I don't think I'm able to help answer that question. I'd encourage you to run your Giraffe+DV setting through something where you have truths for, so you can check hap.py output and see if that's expected. And, to save your time, I recommend running DeepVariant on just one chr (e.g., chr20) to check. Hope this helps!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/847
https://github.com/google/deepvariant/issues/847:1941,usability,help,helps,1941,"@Han-Cao , I ran on your BAM. My command is something like:. ```bash. docker run \. -v /home/pichuan/wgs-case-study/input/data:/input \. -v /home/pichuan/wgs-case-study/output:/output \. google/deepvariant:1.6.1 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/chm13v2.0_noY.fa.gz \. --reads=/input/HG00438.chr1_1000000_2000000.bam \. --output_vcf=/output/deepvariant.output.vcf.gz \. --output_gvcf=/output/deepvariant.output.g.vcf.gz \. --num_shards 64 \. --logging_dir=/output/logs \. --intermediate_results_dir /output/intermediate_results_dir \. --make_examples_extra_args ""min_mapping_quality=1,keep_legacy_allele_counter_behavior=true,normalize_reads=true"" \. --report_title test-githubissue \. --regions chr1:1000000-2000000 \. --runtime_report. ```. Then, I checked how many variants there are:. ```. $ zcat deepvariant.output.vcf.gz | grep -v '^#' | wc -l. 4748. ```. A few thoughts and observations:. 1. Looking at IGV, it looks reasonable. (@AndrewCarroll also took a look and agreed). 2. The number of variants in this region didn't seem too unreasonable. . 3. Your BAM is higher coverage, which could also be why more candidates have been made. 4. I have not use CHM13 a lot myself. It is possible that CHM13 has regions that can take DeepVariant make_examples longer time to complete. If you notice specific regions being really slow (which you can use https://github.com/google/deepvariant/blob/r1.6.1/docs/runtime-by-region.md to check), please feel free to report those regions and we can take a closer look from there! 5. For the question of whether you're running Giraffe correctly, I don't think I'm able to help answer that question. I'd encourage you to run your Giraffe+DV setting through something where you have truths for, so you can check hap.py output and see if that's expected. And, to save your time, I recommend running DeepVariant on just one chr (e.g., chr20) to check. Hope this helps!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/847
https://github.com/google/deepvariant/issues/847:58,reliability,doe,does,58,"Hi @pichuan ,. I did some analysis on the bam file and it does have some issue. e.g., no reads can be found in chr2:145Mb-240Mb. After re-aligning the same data to a GRCh38-based graph, deepvariant works well now. The report is attached: [HG00438.hap8.visual_report.html.txt](https://github.com/user-attachments/files/16378354/HG00438.hap8.visual_report.html.txt). So, it seems the issue is due to the CHM13-based graph not deepvariant. Thank you so much!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/847
https://github.com/google/deepvariant/issues/847:295,usability,user,user-attachments,295,"Hi @pichuan ,. I did some analysis on the bam file and it does have some issue. e.g., no reads can be found in chr2:145Mb-240Mb. After re-aligning the same data to a GRCh38-based graph, deepvariant works well now. The report is attached: [HG00438.hap8.visual_report.html.txt](https://github.com/user-attachments/files/16378354/HG00438.hap8.visual_report.html.txt). So, it seems the issue is due to the CHM13-based graph not deepvariant. Thank you so much!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/847
https://github.com/google/deepvariant/issues/847:15,deployability,updat,update,15,Thanks for the update. Good to know that it works.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/847
https://github.com/google/deepvariant/issues/847:15,safety,updat,update,15,Thanks for the update. Good to know that it works.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/847
https://github.com/google/deepvariant/issues/847:15,security,updat,update,15,Thanks for the update. Good to know that it works.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/847
https://github.com/google/deepvariant/issues/848:314,usability,close,close,314,"Hi @DayTimeMouse . Please see https://github.com/google/deepsomatic/issues/16#issuecomment-2214935507 , which points to https://github.com/google/deepsomatic/issues/3. I kept your other question on DeepSomatic open: https://github.com/google/deepsomatic/issues/17 (which I also gave the same answer above. So I'll close this issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/848
https://github.com/google/deepvariant/issues/849:1023,availability,down,down,1023,"Hi @karoliinas ,. From your log, it seems like the DeepVariant model has made a prediction with unexpected numerical value. From your log, I'm unable to tell why this has occurred. In this command:. `podman run -it --rm --security-opt=label=disable --hooks-dir=/usr/share/containers/oci/hooks.d/ --gpus 1 -v /data:/data --device nvidia.com/gpu=all google/deepvariant:1.6.1-gpu /opt/deepvariant/bin/postprocess_variants --ref ""/data/references/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz"" --infile ""/data/variants/sample1.intermediate/call_variants_output.tfrecord.gz"" --outfile ""/data/variants/sample1.vcf.gz"" --cpus ""19"" --gvcf_outfile ""/data/variants/sample1.g.vcf.gz"" --nonvariant_site_tfrecord_path ""/data/variants/sample1.intermediate/gvcf.tfrecord@19.gz""`. If you can share the `sample1.intermediate/call_variants_output.tfrecord.gz` (and optionally`sample1.intermediate/gvcf.tfrecord@19.gz` files) with me, I can able to look into the records and see which example has this issue. (Or, if you can narrow this down to a small BAM file, and if you can share that BAM file, that works too). Please email to pichuan@google.com if you can share. If you can't share the files, we can think about what we can do here to help identify which example caused the issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/849
https://github.com/google/deepvariant/issues/849:28,deployability,log,log,28,"Hi @karoliinas ,. From your log, it seems like the DeepVariant model has made a prediction with unexpected numerical value. From your log, I'm unable to tell why this has occurred. In this command:. `podman run -it --rm --security-opt=label=disable --hooks-dir=/usr/share/containers/oci/hooks.d/ --gpus 1 -v /data:/data --device nvidia.com/gpu=all google/deepvariant:1.6.1-gpu /opt/deepvariant/bin/postprocess_variants --ref ""/data/references/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz"" --infile ""/data/variants/sample1.intermediate/call_variants_output.tfrecord.gz"" --outfile ""/data/variants/sample1.vcf.gz"" --cpus ""19"" --gvcf_outfile ""/data/variants/sample1.g.vcf.gz"" --nonvariant_site_tfrecord_path ""/data/variants/sample1.intermediate/gvcf.tfrecord@19.gz""`. If you can share the `sample1.intermediate/call_variants_output.tfrecord.gz` (and optionally`sample1.intermediate/gvcf.tfrecord@19.gz` files) with me, I can able to look into the records and see which example has this issue. (Or, if you can narrow this down to a small BAM file, and if you can share that BAM file, that works too). Please email to pichuan@google.com if you can share. If you can't share the files, we can think about what we can do here to help identify which example caused the issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/849
https://github.com/google/deepvariant/issues/849:134,deployability,log,log,134,"Hi @karoliinas ,. From your log, it seems like the DeepVariant model has made a prediction with unexpected numerical value. From your log, I'm unable to tell why this has occurred. In this command:. `podman run -it --rm --security-opt=label=disable --hooks-dir=/usr/share/containers/oci/hooks.d/ --gpus 1 -v /data:/data --device nvidia.com/gpu=all google/deepvariant:1.6.1-gpu /opt/deepvariant/bin/postprocess_variants --ref ""/data/references/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz"" --infile ""/data/variants/sample1.intermediate/call_variants_output.tfrecord.gz"" --outfile ""/data/variants/sample1.vcf.gz"" --cpus ""19"" --gvcf_outfile ""/data/variants/sample1.g.vcf.gz"" --nonvariant_site_tfrecord_path ""/data/variants/sample1.intermediate/gvcf.tfrecord@19.gz""`. If you can share the `sample1.intermediate/call_variants_output.tfrecord.gz` (and optionally`sample1.intermediate/gvcf.tfrecord@19.gz` files) with me, I can able to look into the records and see which example has this issue. (Or, if you can narrow this down to a small BAM file, and if you can share that BAM file, that works too). Please email to pichuan@google.com if you can share. If you can't share the files, we can think about what we can do here to help identify which example caused the issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/849
https://github.com/google/deepvariant/issues/849:272,deployability,contain,containers,272,"Hi @karoliinas ,. From your log, it seems like the DeepVariant model has made a prediction with unexpected numerical value. From your log, I'm unable to tell why this has occurred. In this command:. `podman run -it --rm --security-opt=label=disable --hooks-dir=/usr/share/containers/oci/hooks.d/ --gpus 1 -v /data:/data --device nvidia.com/gpu=all google/deepvariant:1.6.1-gpu /opt/deepvariant/bin/postprocess_variants --ref ""/data/references/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz"" --infile ""/data/variants/sample1.intermediate/call_variants_output.tfrecord.gz"" --outfile ""/data/variants/sample1.vcf.gz"" --cpus ""19"" --gvcf_outfile ""/data/variants/sample1.g.vcf.gz"" --nonvariant_site_tfrecord_path ""/data/variants/sample1.intermediate/gvcf.tfrecord@19.gz""`. If you can share the `sample1.intermediate/call_variants_output.tfrecord.gz` (and optionally`sample1.intermediate/gvcf.tfrecord@19.gz` files) with me, I can able to look into the records and see which example has this issue. (Or, if you can narrow this down to a small BAM file, and if you can share that BAM file, that works too). Please email to pichuan@google.com if you can share. If you can't share the files, we can think about what we can do here to help identify which example caused the issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/849
https://github.com/google/deepvariant/issues/849:63,energy efficiency,model,model,63,"Hi @karoliinas ,. From your log, it seems like the DeepVariant model has made a prediction with unexpected numerical value. From your log, I'm unable to tell why this has occurred. In this command:. `podman run -it --rm --security-opt=label=disable --hooks-dir=/usr/share/containers/oci/hooks.d/ --gpus 1 -v /data:/data --device nvidia.com/gpu=all google/deepvariant:1.6.1-gpu /opt/deepvariant/bin/postprocess_variants --ref ""/data/references/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz"" --infile ""/data/variants/sample1.intermediate/call_variants_output.tfrecord.gz"" --outfile ""/data/variants/sample1.vcf.gz"" --cpus ""19"" --gvcf_outfile ""/data/variants/sample1.g.vcf.gz"" --nonvariant_site_tfrecord_path ""/data/variants/sample1.intermediate/gvcf.tfrecord@19.gz""`. If you can share the `sample1.intermediate/call_variants_output.tfrecord.gz` (and optionally`sample1.intermediate/gvcf.tfrecord@19.gz` files) with me, I can able to look into the records and see which example has this issue. (Or, if you can narrow this down to a small BAM file, and if you can share that BAM file, that works too). Please email to pichuan@google.com if you can share. If you can't share the files, we can think about what we can do here to help identify which example caused the issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/849
https://github.com/google/deepvariant/issues/849:80,energy efficiency,predict,prediction,80,"Hi @karoliinas ,. From your log, it seems like the DeepVariant model has made a prediction with unexpected numerical value. From your log, I'm unable to tell why this has occurred. In this command:. `podman run -it --rm --security-opt=label=disable --hooks-dir=/usr/share/containers/oci/hooks.d/ --gpus 1 -v /data:/data --device nvidia.com/gpu=all google/deepvariant:1.6.1-gpu /opt/deepvariant/bin/postprocess_variants --ref ""/data/references/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz"" --infile ""/data/variants/sample1.intermediate/call_variants_output.tfrecord.gz"" --outfile ""/data/variants/sample1.vcf.gz"" --cpus ""19"" --gvcf_outfile ""/data/variants/sample1.g.vcf.gz"" --nonvariant_site_tfrecord_path ""/data/variants/sample1.intermediate/gvcf.tfrecord@19.gz""`. If you can share the `sample1.intermediate/call_variants_output.tfrecord.gz` (and optionally`sample1.intermediate/gvcf.tfrecord@19.gz` files) with me, I can able to look into the records and see which example has this issue. (Or, if you can narrow this down to a small BAM file, and if you can share that BAM file, that works too). Please email to pichuan@google.com if you can share. If you can't share the files, we can think about what we can do here to help identify which example caused the issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/849
https://github.com/google/deepvariant/issues/849:298,energy efficiency,gpu,gpus,298,"Hi @karoliinas ,. From your log, it seems like the DeepVariant model has made a prediction with unexpected numerical value. From your log, I'm unable to tell why this has occurred. In this command:. `podman run -it --rm --security-opt=label=disable --hooks-dir=/usr/share/containers/oci/hooks.d/ --gpus 1 -v /data:/data --device nvidia.com/gpu=all google/deepvariant:1.6.1-gpu /opt/deepvariant/bin/postprocess_variants --ref ""/data/references/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz"" --infile ""/data/variants/sample1.intermediate/call_variants_output.tfrecord.gz"" --outfile ""/data/variants/sample1.vcf.gz"" --cpus ""19"" --gvcf_outfile ""/data/variants/sample1.g.vcf.gz"" --nonvariant_site_tfrecord_path ""/data/variants/sample1.intermediate/gvcf.tfrecord@19.gz""`. If you can share the `sample1.intermediate/call_variants_output.tfrecord.gz` (and optionally`sample1.intermediate/gvcf.tfrecord@19.gz` files) with me, I can able to look into the records and see which example has this issue. (Or, if you can narrow this down to a small BAM file, and if you can share that BAM file, that works too). Please email to pichuan@google.com if you can share. If you can't share the files, we can think about what we can do here to help identify which example caused the issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/849
https://github.com/google/deepvariant/issues/849:340,energy efficiency,gpu,gpu,340,"Hi @karoliinas ,. From your log, it seems like the DeepVariant model has made a prediction with unexpected numerical value. From your log, I'm unable to tell why this has occurred. In this command:. `podman run -it --rm --security-opt=label=disable --hooks-dir=/usr/share/containers/oci/hooks.d/ --gpus 1 -v /data:/data --device nvidia.com/gpu=all google/deepvariant:1.6.1-gpu /opt/deepvariant/bin/postprocess_variants --ref ""/data/references/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz"" --infile ""/data/variants/sample1.intermediate/call_variants_output.tfrecord.gz"" --outfile ""/data/variants/sample1.vcf.gz"" --cpus ""19"" --gvcf_outfile ""/data/variants/sample1.g.vcf.gz"" --nonvariant_site_tfrecord_path ""/data/variants/sample1.intermediate/gvcf.tfrecord@19.gz""`. If you can share the `sample1.intermediate/call_variants_output.tfrecord.gz` (and optionally`sample1.intermediate/gvcf.tfrecord@19.gz` files) with me, I can able to look into the records and see which example has this issue. (Or, if you can narrow this down to a small BAM file, and if you can share that BAM file, that works too). Please email to pichuan@google.com if you can share. If you can't share the files, we can think about what we can do here to help identify which example caused the issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/849
https://github.com/google/deepvariant/issues/849:373,energy efficiency,gpu,gpu,373,"Hi @karoliinas ,. From your log, it seems like the DeepVariant model has made a prediction with unexpected numerical value. From your log, I'm unable to tell why this has occurred. In this command:. `podman run -it --rm --security-opt=label=disable --hooks-dir=/usr/share/containers/oci/hooks.d/ --gpus 1 -v /data:/data --device nvidia.com/gpu=all google/deepvariant:1.6.1-gpu /opt/deepvariant/bin/postprocess_variants --ref ""/data/references/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz"" --infile ""/data/variants/sample1.intermediate/call_variants_output.tfrecord.gz"" --outfile ""/data/variants/sample1.vcf.gz"" --cpus ""19"" --gvcf_outfile ""/data/variants/sample1.g.vcf.gz"" --nonvariant_site_tfrecord_path ""/data/variants/sample1.intermediate/gvcf.tfrecord@19.gz""`. If you can share the `sample1.intermediate/call_variants_output.tfrecord.gz` (and optionally`sample1.intermediate/gvcf.tfrecord@19.gz` files) with me, I can able to look into the records and see which example has this issue. (Or, if you can narrow this down to a small BAM file, and if you can share that BAM file, that works too). Please email to pichuan@google.com if you can share. If you can't share the files, we can think about what we can do here to help identify which example caused the issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/849
https://github.com/google/deepvariant/issues/849:619,energy efficiency,cpu,cpus,619,"Hi @karoliinas ,. From your log, it seems like the DeepVariant model has made a prediction with unexpected numerical value. From your log, I'm unable to tell why this has occurred. In this command:. `podman run -it --rm --security-opt=label=disable --hooks-dir=/usr/share/containers/oci/hooks.d/ --gpus 1 -v /data:/data --device nvidia.com/gpu=all google/deepvariant:1.6.1-gpu /opt/deepvariant/bin/postprocess_variants --ref ""/data/references/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz"" --infile ""/data/variants/sample1.intermediate/call_variants_output.tfrecord.gz"" --outfile ""/data/variants/sample1.vcf.gz"" --cpus ""19"" --gvcf_outfile ""/data/variants/sample1.g.vcf.gz"" --nonvariant_site_tfrecord_path ""/data/variants/sample1.intermediate/gvcf.tfrecord@19.gz""`. If you can share the `sample1.intermediate/call_variants_output.tfrecord.gz` (and optionally`sample1.intermediate/gvcf.tfrecord@19.gz` files) with me, I can able to look into the records and see which example has this issue. (Or, if you can narrow this down to a small BAM file, and if you can share that BAM file, that works too). Please email to pichuan@google.com if you can share. If you can't share the files, we can think about what we can do here to help identify which example caused the issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/849
https://github.com/google/deepvariant/issues/849:266,interoperability,share,share,266,"Hi @karoliinas ,. From your log, it seems like the DeepVariant model has made a prediction with unexpected numerical value. From your log, I'm unable to tell why this has occurred. In this command:. `podman run -it --rm --security-opt=label=disable --hooks-dir=/usr/share/containers/oci/hooks.d/ --gpus 1 -v /data:/data --device nvidia.com/gpu=all google/deepvariant:1.6.1-gpu /opt/deepvariant/bin/postprocess_variants --ref ""/data/references/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz"" --infile ""/data/variants/sample1.intermediate/call_variants_output.tfrecord.gz"" --outfile ""/data/variants/sample1.vcf.gz"" --cpus ""19"" --gvcf_outfile ""/data/variants/sample1.g.vcf.gz"" --nonvariant_site_tfrecord_path ""/data/variants/sample1.intermediate/gvcf.tfrecord@19.gz""`. If you can share the `sample1.intermediate/call_variants_output.tfrecord.gz` (and optionally`sample1.intermediate/gvcf.tfrecord@19.gz` files) with me, I can able to look into the records and see which example has this issue. (Or, if you can narrow this down to a small BAM file, and if you can share that BAM file, that works too). Please email to pichuan@google.com if you can share. If you can't share the files, we can think about what we can do here to help identify which example caused the issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/849
https://github.com/google/deepvariant/issues/849:781,interoperability,share,share,781,"Hi @karoliinas ,. From your log, it seems like the DeepVariant model has made a prediction with unexpected numerical value. From your log, I'm unable to tell why this has occurred. In this command:. `podman run -it --rm --security-opt=label=disable --hooks-dir=/usr/share/containers/oci/hooks.d/ --gpus 1 -v /data:/data --device nvidia.com/gpu=all google/deepvariant:1.6.1-gpu /opt/deepvariant/bin/postprocess_variants --ref ""/data/references/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz"" --infile ""/data/variants/sample1.intermediate/call_variants_output.tfrecord.gz"" --outfile ""/data/variants/sample1.vcf.gz"" --cpus ""19"" --gvcf_outfile ""/data/variants/sample1.g.vcf.gz"" --nonvariant_site_tfrecord_path ""/data/variants/sample1.intermediate/gvcf.tfrecord@19.gz""`. If you can share the `sample1.intermediate/call_variants_output.tfrecord.gz` (and optionally`sample1.intermediate/gvcf.tfrecord@19.gz` files) with me, I can able to look into the records and see which example has this issue. (Or, if you can narrow this down to a small BAM file, and if you can share that BAM file, that works too). Please email to pichuan@google.com if you can share. If you can't share the files, we can think about what we can do here to help identify which example caused the issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/849
https://github.com/google/deepvariant/issues/849:1064,interoperability,share,share,1064,"Hi @karoliinas ,. From your log, it seems like the DeepVariant model has made a prediction with unexpected numerical value. From your log, I'm unable to tell why this has occurred. In this command:. `podman run -it --rm --security-opt=label=disable --hooks-dir=/usr/share/containers/oci/hooks.d/ --gpus 1 -v /data:/data --device nvidia.com/gpu=all google/deepvariant:1.6.1-gpu /opt/deepvariant/bin/postprocess_variants --ref ""/data/references/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz"" --infile ""/data/variants/sample1.intermediate/call_variants_output.tfrecord.gz"" --outfile ""/data/variants/sample1.vcf.gz"" --cpus ""19"" --gvcf_outfile ""/data/variants/sample1.g.vcf.gz"" --nonvariant_site_tfrecord_path ""/data/variants/sample1.intermediate/gvcf.tfrecord@19.gz""`. If you can share the `sample1.intermediate/call_variants_output.tfrecord.gz` (and optionally`sample1.intermediate/gvcf.tfrecord@19.gz` files) with me, I can able to look into the records and see which example has this issue. (Or, if you can narrow this down to a small BAM file, and if you can share that BAM file, that works too). Please email to pichuan@google.com if you can share. If you can't share the files, we can think about what we can do here to help identify which example caused the issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/849
https://github.com/google/deepvariant/issues/849:1148,interoperability,share,share,1148,"Hi @karoliinas ,. From your log, it seems like the DeepVariant model has made a prediction with unexpected numerical value. From your log, I'm unable to tell why this has occurred. In this command:. `podman run -it --rm --security-opt=label=disable --hooks-dir=/usr/share/containers/oci/hooks.d/ --gpus 1 -v /data:/data --device nvidia.com/gpu=all google/deepvariant:1.6.1-gpu /opt/deepvariant/bin/postprocess_variants --ref ""/data/references/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz"" --infile ""/data/variants/sample1.intermediate/call_variants_output.tfrecord.gz"" --outfile ""/data/variants/sample1.vcf.gz"" --cpus ""19"" --gvcf_outfile ""/data/variants/sample1.g.vcf.gz"" --nonvariant_site_tfrecord_path ""/data/variants/sample1.intermediate/gvcf.tfrecord@19.gz""`. If you can share the `sample1.intermediate/call_variants_output.tfrecord.gz` (and optionally`sample1.intermediate/gvcf.tfrecord@19.gz` files) with me, I can able to look into the records and see which example has this issue. (Or, if you can narrow this down to a small BAM file, and if you can share that BAM file, that works too). Please email to pichuan@google.com if you can share. If you can't share the files, we can think about what we can do here to help identify which example caused the issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/849
https://github.com/google/deepvariant/issues/849:1168,interoperability,share,share,1168,"Hi @karoliinas ,. From your log, it seems like the DeepVariant model has made a prediction with unexpected numerical value. From your log, I'm unable to tell why this has occurred. In this command:. `podman run -it --rm --security-opt=label=disable --hooks-dir=/usr/share/containers/oci/hooks.d/ --gpus 1 -v /data:/data --device nvidia.com/gpu=all google/deepvariant:1.6.1-gpu /opt/deepvariant/bin/postprocess_variants --ref ""/data/references/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz"" --infile ""/data/variants/sample1.intermediate/call_variants_output.tfrecord.gz"" --outfile ""/data/variants/sample1.vcf.gz"" --cpus ""19"" --gvcf_outfile ""/data/variants/sample1.g.vcf.gz"" --nonvariant_site_tfrecord_path ""/data/variants/sample1.intermediate/gvcf.tfrecord@19.gz""`. If you can share the `sample1.intermediate/call_variants_output.tfrecord.gz` (and optionally`sample1.intermediate/gvcf.tfrecord@19.gz` files) with me, I can able to look into the records and see which example has this issue. (Or, if you can narrow this down to a small BAM file, and if you can share that BAM file, that works too). Please email to pichuan@google.com if you can share. If you can't share the files, we can think about what we can do here to help identify which example caused the issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/849
https://github.com/google/deepvariant/issues/849:528,modifiability,interm,intermediate,528,"Hi @karoliinas ,. From your log, it seems like the DeepVariant model has made a prediction with unexpected numerical value. From your log, I'm unable to tell why this has occurred. In this command:. `podman run -it --rm --security-opt=label=disable --hooks-dir=/usr/share/containers/oci/hooks.d/ --gpus 1 -v /data:/data --device nvidia.com/gpu=all google/deepvariant:1.6.1-gpu /opt/deepvariant/bin/postprocess_variants --ref ""/data/references/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz"" --infile ""/data/variants/sample1.intermediate/call_variants_output.tfrecord.gz"" --outfile ""/data/variants/sample1.vcf.gz"" --cpus ""19"" --gvcf_outfile ""/data/variants/sample1.g.vcf.gz"" --nonvariant_site_tfrecord_path ""/data/variants/sample1.intermediate/gvcf.tfrecord@19.gz""`. If you can share the `sample1.intermediate/call_variants_output.tfrecord.gz` (and optionally`sample1.intermediate/gvcf.tfrecord@19.gz` files) with me, I can able to look into the records and see which example has this issue. (Or, if you can narrow this down to a small BAM file, and if you can share that BAM file, that works too). Please email to pichuan@google.com if you can share. If you can't share the files, we can think about what we can do here to help identify which example caused the issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/849
https://github.com/google/deepvariant/issues/849:734,modifiability,interm,intermediate,734,"Hi @karoliinas ,. From your log, it seems like the DeepVariant model has made a prediction with unexpected numerical value. From your log, I'm unable to tell why this has occurred. In this command:. `podman run -it --rm --security-opt=label=disable --hooks-dir=/usr/share/containers/oci/hooks.d/ --gpus 1 -v /data:/data --device nvidia.com/gpu=all google/deepvariant:1.6.1-gpu /opt/deepvariant/bin/postprocess_variants --ref ""/data/references/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz"" --infile ""/data/variants/sample1.intermediate/call_variants_output.tfrecord.gz"" --outfile ""/data/variants/sample1.vcf.gz"" --cpus ""19"" --gvcf_outfile ""/data/variants/sample1.g.vcf.gz"" --nonvariant_site_tfrecord_path ""/data/variants/sample1.intermediate/gvcf.tfrecord@19.gz""`. If you can share the `sample1.intermediate/call_variants_output.tfrecord.gz` (and optionally`sample1.intermediate/gvcf.tfrecord@19.gz` files) with me, I can able to look into the records and see which example has this issue. (Or, if you can narrow this down to a small BAM file, and if you can share that BAM file, that works too). Please email to pichuan@google.com if you can share. If you can't share the files, we can think about what we can do here to help identify which example caused the issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/849
https://github.com/google/deepvariant/issues/849:800,modifiability,interm,intermediate,800,"Hi @karoliinas ,. From your log, it seems like the DeepVariant model has made a prediction with unexpected numerical value. From your log, I'm unable to tell why this has occurred. In this command:. `podman run -it --rm --security-opt=label=disable --hooks-dir=/usr/share/containers/oci/hooks.d/ --gpus 1 -v /data:/data --device nvidia.com/gpu=all google/deepvariant:1.6.1-gpu /opt/deepvariant/bin/postprocess_variants --ref ""/data/references/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz"" --infile ""/data/variants/sample1.intermediate/call_variants_output.tfrecord.gz"" --outfile ""/data/variants/sample1.vcf.gz"" --cpus ""19"" --gvcf_outfile ""/data/variants/sample1.g.vcf.gz"" --nonvariant_site_tfrecord_path ""/data/variants/sample1.intermediate/gvcf.tfrecord@19.gz""`. If you can share the `sample1.intermediate/call_variants_output.tfrecord.gz` (and optionally`sample1.intermediate/gvcf.tfrecord@19.gz` files) with me, I can able to look into the records and see which example has this issue. (Or, if you can narrow this down to a small BAM file, and if you can share that BAM file, that works too). Please email to pichuan@google.com if you can share. If you can't share the files, we can think about what we can do here to help identify which example caused the issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/849
https://github.com/google/deepvariant/issues/849:871,modifiability,interm,intermediate,871,"Hi @karoliinas ,. From your log, it seems like the DeepVariant model has made a prediction with unexpected numerical value. From your log, I'm unable to tell why this has occurred. In this command:. `podman run -it --rm --security-opt=label=disable --hooks-dir=/usr/share/containers/oci/hooks.d/ --gpus 1 -v /data:/data --device nvidia.com/gpu=all google/deepvariant:1.6.1-gpu /opt/deepvariant/bin/postprocess_variants --ref ""/data/references/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz"" --infile ""/data/variants/sample1.intermediate/call_variants_output.tfrecord.gz"" --outfile ""/data/variants/sample1.vcf.gz"" --cpus ""19"" --gvcf_outfile ""/data/variants/sample1.g.vcf.gz"" --nonvariant_site_tfrecord_path ""/data/variants/sample1.intermediate/gvcf.tfrecord@19.gz""`. If you can share the `sample1.intermediate/call_variants_output.tfrecord.gz` (and optionally`sample1.intermediate/gvcf.tfrecord@19.gz` files) with me, I can able to look into the records and see which example has this issue. (Or, if you can narrow this down to a small BAM file, and if you can share that BAM file, that works too). Please email to pichuan@google.com if you can share. If you can't share the files, we can think about what we can do here to help identify which example caused the issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/849
https://github.com/google/deepvariant/issues/849:298,performance,gpu,gpus,298,"Hi @karoliinas ,. From your log, it seems like the DeepVariant model has made a prediction with unexpected numerical value. From your log, I'm unable to tell why this has occurred. In this command:. `podman run -it --rm --security-opt=label=disable --hooks-dir=/usr/share/containers/oci/hooks.d/ --gpus 1 -v /data:/data --device nvidia.com/gpu=all google/deepvariant:1.6.1-gpu /opt/deepvariant/bin/postprocess_variants --ref ""/data/references/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz"" --infile ""/data/variants/sample1.intermediate/call_variants_output.tfrecord.gz"" --outfile ""/data/variants/sample1.vcf.gz"" --cpus ""19"" --gvcf_outfile ""/data/variants/sample1.g.vcf.gz"" --nonvariant_site_tfrecord_path ""/data/variants/sample1.intermediate/gvcf.tfrecord@19.gz""`. If you can share the `sample1.intermediate/call_variants_output.tfrecord.gz` (and optionally`sample1.intermediate/gvcf.tfrecord@19.gz` files) with me, I can able to look into the records and see which example has this issue. (Or, if you can narrow this down to a small BAM file, and if you can share that BAM file, that works too). Please email to pichuan@google.com if you can share. If you can't share the files, we can think about what we can do here to help identify which example caused the issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/849
https://github.com/google/deepvariant/issues/849:340,performance,gpu,gpu,340,"Hi @karoliinas ,. From your log, it seems like the DeepVariant model has made a prediction with unexpected numerical value. From your log, I'm unable to tell why this has occurred. In this command:. `podman run -it --rm --security-opt=label=disable --hooks-dir=/usr/share/containers/oci/hooks.d/ --gpus 1 -v /data:/data --device nvidia.com/gpu=all google/deepvariant:1.6.1-gpu /opt/deepvariant/bin/postprocess_variants --ref ""/data/references/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz"" --infile ""/data/variants/sample1.intermediate/call_variants_output.tfrecord.gz"" --outfile ""/data/variants/sample1.vcf.gz"" --cpus ""19"" --gvcf_outfile ""/data/variants/sample1.g.vcf.gz"" --nonvariant_site_tfrecord_path ""/data/variants/sample1.intermediate/gvcf.tfrecord@19.gz""`. If you can share the `sample1.intermediate/call_variants_output.tfrecord.gz` (and optionally`sample1.intermediate/gvcf.tfrecord@19.gz` files) with me, I can able to look into the records and see which example has this issue. (Or, if you can narrow this down to a small BAM file, and if you can share that BAM file, that works too). Please email to pichuan@google.com if you can share. If you can't share the files, we can think about what we can do here to help identify which example caused the issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/849
https://github.com/google/deepvariant/issues/849:373,performance,gpu,gpu,373,"Hi @karoliinas ,. From your log, it seems like the DeepVariant model has made a prediction with unexpected numerical value. From your log, I'm unable to tell why this has occurred. In this command:. `podman run -it --rm --security-opt=label=disable --hooks-dir=/usr/share/containers/oci/hooks.d/ --gpus 1 -v /data:/data --device nvidia.com/gpu=all google/deepvariant:1.6.1-gpu /opt/deepvariant/bin/postprocess_variants --ref ""/data/references/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz"" --infile ""/data/variants/sample1.intermediate/call_variants_output.tfrecord.gz"" --outfile ""/data/variants/sample1.vcf.gz"" --cpus ""19"" --gvcf_outfile ""/data/variants/sample1.g.vcf.gz"" --nonvariant_site_tfrecord_path ""/data/variants/sample1.intermediate/gvcf.tfrecord@19.gz""`. If you can share the `sample1.intermediate/call_variants_output.tfrecord.gz` (and optionally`sample1.intermediate/gvcf.tfrecord@19.gz` files) with me, I can able to look into the records and see which example has this issue. (Or, if you can narrow this down to a small BAM file, and if you can share that BAM file, that works too). Please email to pichuan@google.com if you can share. If you can't share the files, we can think about what we can do here to help identify which example caused the issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/849
https://github.com/google/deepvariant/issues/849:619,performance,cpu,cpus,619,"Hi @karoliinas ,. From your log, it seems like the DeepVariant model has made a prediction with unexpected numerical value. From your log, I'm unable to tell why this has occurred. In this command:. `podman run -it --rm --security-opt=label=disable --hooks-dir=/usr/share/containers/oci/hooks.d/ --gpus 1 -v /data:/data --device nvidia.com/gpu=all google/deepvariant:1.6.1-gpu /opt/deepvariant/bin/postprocess_variants --ref ""/data/references/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz"" --infile ""/data/variants/sample1.intermediate/call_variants_output.tfrecord.gz"" --outfile ""/data/variants/sample1.vcf.gz"" --cpus ""19"" --gvcf_outfile ""/data/variants/sample1.g.vcf.gz"" --nonvariant_site_tfrecord_path ""/data/variants/sample1.intermediate/gvcf.tfrecord@19.gz""`. If you can share the `sample1.intermediate/call_variants_output.tfrecord.gz` (and optionally`sample1.intermediate/gvcf.tfrecord@19.gz` files) with me, I can able to look into the records and see which example has this issue. (Or, if you can narrow this down to a small BAM file, and if you can share that BAM file, that works too). Please email to pichuan@google.com if you can share. If you can't share the files, we can think about what we can do here to help identify which example caused the issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/849
https://github.com/google/deepvariant/issues/849:28,safety,log,log,28,"Hi @karoliinas ,. From your log, it seems like the DeepVariant model has made a prediction with unexpected numerical value. From your log, I'm unable to tell why this has occurred. In this command:. `podman run -it --rm --security-opt=label=disable --hooks-dir=/usr/share/containers/oci/hooks.d/ --gpus 1 -v /data:/data --device nvidia.com/gpu=all google/deepvariant:1.6.1-gpu /opt/deepvariant/bin/postprocess_variants --ref ""/data/references/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz"" --infile ""/data/variants/sample1.intermediate/call_variants_output.tfrecord.gz"" --outfile ""/data/variants/sample1.vcf.gz"" --cpus ""19"" --gvcf_outfile ""/data/variants/sample1.g.vcf.gz"" --nonvariant_site_tfrecord_path ""/data/variants/sample1.intermediate/gvcf.tfrecord@19.gz""`. If you can share the `sample1.intermediate/call_variants_output.tfrecord.gz` (and optionally`sample1.intermediate/gvcf.tfrecord@19.gz` files) with me, I can able to look into the records and see which example has this issue. (Or, if you can narrow this down to a small BAM file, and if you can share that BAM file, that works too). Please email to pichuan@google.com if you can share. If you can't share the files, we can think about what we can do here to help identify which example caused the issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/849
https://github.com/google/deepvariant/issues/849:80,safety,predict,prediction,80,"Hi @karoliinas ,. From your log, it seems like the DeepVariant model has made a prediction with unexpected numerical value. From your log, I'm unable to tell why this has occurred. In this command:. `podman run -it --rm --security-opt=label=disable --hooks-dir=/usr/share/containers/oci/hooks.d/ --gpus 1 -v /data:/data --device nvidia.com/gpu=all google/deepvariant:1.6.1-gpu /opt/deepvariant/bin/postprocess_variants --ref ""/data/references/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz"" --infile ""/data/variants/sample1.intermediate/call_variants_output.tfrecord.gz"" --outfile ""/data/variants/sample1.vcf.gz"" --cpus ""19"" --gvcf_outfile ""/data/variants/sample1.g.vcf.gz"" --nonvariant_site_tfrecord_path ""/data/variants/sample1.intermediate/gvcf.tfrecord@19.gz""`. If you can share the `sample1.intermediate/call_variants_output.tfrecord.gz` (and optionally`sample1.intermediate/gvcf.tfrecord@19.gz` files) with me, I can able to look into the records and see which example has this issue. (Or, if you can narrow this down to a small BAM file, and if you can share that BAM file, that works too). Please email to pichuan@google.com if you can share. If you can't share the files, we can think about what we can do here to help identify which example caused the issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/849
https://github.com/google/deepvariant/issues/849:134,safety,log,log,134,"Hi @karoliinas ,. From your log, it seems like the DeepVariant model has made a prediction with unexpected numerical value. From your log, I'm unable to tell why this has occurred. In this command:. `podman run -it --rm --security-opt=label=disable --hooks-dir=/usr/share/containers/oci/hooks.d/ --gpus 1 -v /data:/data --device nvidia.com/gpu=all google/deepvariant:1.6.1-gpu /opt/deepvariant/bin/postprocess_variants --ref ""/data/references/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz"" --infile ""/data/variants/sample1.intermediate/call_variants_output.tfrecord.gz"" --outfile ""/data/variants/sample1.vcf.gz"" --cpus ""19"" --gvcf_outfile ""/data/variants/sample1.g.vcf.gz"" --nonvariant_site_tfrecord_path ""/data/variants/sample1.intermediate/gvcf.tfrecord@19.gz""`. If you can share the `sample1.intermediate/call_variants_output.tfrecord.gz` (and optionally`sample1.intermediate/gvcf.tfrecord@19.gz` files) with me, I can able to look into the records and see which example has this issue. (Or, if you can narrow this down to a small BAM file, and if you can share that BAM file, that works too). Please email to pichuan@google.com if you can share. If you can't share the files, we can think about what we can do here to help identify which example caused the issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/849
https://github.com/google/deepvariant/issues/849:28,security,log,log,28,"Hi @karoliinas ,. From your log, it seems like the DeepVariant model has made a prediction with unexpected numerical value. From your log, I'm unable to tell why this has occurred. In this command:. `podman run -it --rm --security-opt=label=disable --hooks-dir=/usr/share/containers/oci/hooks.d/ --gpus 1 -v /data:/data --device nvidia.com/gpu=all google/deepvariant:1.6.1-gpu /opt/deepvariant/bin/postprocess_variants --ref ""/data/references/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz"" --infile ""/data/variants/sample1.intermediate/call_variants_output.tfrecord.gz"" --outfile ""/data/variants/sample1.vcf.gz"" --cpus ""19"" --gvcf_outfile ""/data/variants/sample1.g.vcf.gz"" --nonvariant_site_tfrecord_path ""/data/variants/sample1.intermediate/gvcf.tfrecord@19.gz""`. If you can share the `sample1.intermediate/call_variants_output.tfrecord.gz` (and optionally`sample1.intermediate/gvcf.tfrecord@19.gz` files) with me, I can able to look into the records and see which example has this issue. (Or, if you can narrow this down to a small BAM file, and if you can share that BAM file, that works too). Please email to pichuan@google.com if you can share. If you can't share the files, we can think about what we can do here to help identify which example caused the issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/849
https://github.com/google/deepvariant/issues/849:63,security,model,model,63,"Hi @karoliinas ,. From your log, it seems like the DeepVariant model has made a prediction with unexpected numerical value. From your log, I'm unable to tell why this has occurred. In this command:. `podman run -it --rm --security-opt=label=disable --hooks-dir=/usr/share/containers/oci/hooks.d/ --gpus 1 -v /data:/data --device nvidia.com/gpu=all google/deepvariant:1.6.1-gpu /opt/deepvariant/bin/postprocess_variants --ref ""/data/references/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz"" --infile ""/data/variants/sample1.intermediate/call_variants_output.tfrecord.gz"" --outfile ""/data/variants/sample1.vcf.gz"" --cpus ""19"" --gvcf_outfile ""/data/variants/sample1.g.vcf.gz"" --nonvariant_site_tfrecord_path ""/data/variants/sample1.intermediate/gvcf.tfrecord@19.gz""`. If you can share the `sample1.intermediate/call_variants_output.tfrecord.gz` (and optionally`sample1.intermediate/gvcf.tfrecord@19.gz` files) with me, I can able to look into the records and see which example has this issue. (Or, if you can narrow this down to a small BAM file, and if you can share that BAM file, that works too). Please email to pichuan@google.com if you can share. If you can't share the files, we can think about what we can do here to help identify which example caused the issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/849
https://github.com/google/deepvariant/issues/849:134,security,log,log,134,"Hi @karoliinas ,. From your log, it seems like the DeepVariant model has made a prediction with unexpected numerical value. From your log, I'm unable to tell why this has occurred. In this command:. `podman run -it --rm --security-opt=label=disable --hooks-dir=/usr/share/containers/oci/hooks.d/ --gpus 1 -v /data:/data --device nvidia.com/gpu=all google/deepvariant:1.6.1-gpu /opt/deepvariant/bin/postprocess_variants --ref ""/data/references/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz"" --infile ""/data/variants/sample1.intermediate/call_variants_output.tfrecord.gz"" --outfile ""/data/variants/sample1.vcf.gz"" --cpus ""19"" --gvcf_outfile ""/data/variants/sample1.g.vcf.gz"" --nonvariant_site_tfrecord_path ""/data/variants/sample1.intermediate/gvcf.tfrecord@19.gz""`. If you can share the `sample1.intermediate/call_variants_output.tfrecord.gz` (and optionally`sample1.intermediate/gvcf.tfrecord@19.gz` files) with me, I can able to look into the records and see which example has this issue. (Or, if you can narrow this down to a small BAM file, and if you can share that BAM file, that works too). Please email to pichuan@google.com if you can share. If you can't share the files, we can think about what we can do here to help identify which example caused the issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/849
https://github.com/google/deepvariant/issues/849:222,security,secur,security-opt,222,"Hi @karoliinas ,. From your log, it seems like the DeepVariant model has made a prediction with unexpected numerical value. From your log, I'm unable to tell why this has occurred. In this command:. `podman run -it --rm --security-opt=label=disable --hooks-dir=/usr/share/containers/oci/hooks.d/ --gpus 1 -v /data:/data --device nvidia.com/gpu=all google/deepvariant:1.6.1-gpu /opt/deepvariant/bin/postprocess_variants --ref ""/data/references/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz"" --infile ""/data/variants/sample1.intermediate/call_variants_output.tfrecord.gz"" --outfile ""/data/variants/sample1.vcf.gz"" --cpus ""19"" --gvcf_outfile ""/data/variants/sample1.g.vcf.gz"" --nonvariant_site_tfrecord_path ""/data/variants/sample1.intermediate/gvcf.tfrecord@19.gz""`. If you can share the `sample1.intermediate/call_variants_output.tfrecord.gz` (and optionally`sample1.intermediate/gvcf.tfrecord@19.gz` files) with me, I can able to look into the records and see which example has this issue. (Or, if you can narrow this down to a small BAM file, and if you can share that BAM file, that works too). Please email to pichuan@google.com if you can share. If you can't share the files, we can think about what we can do here to help identify which example caused the issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/849
https://github.com/google/deepvariant/issues/849:1232,security,ident,identify,1232,"Hi @karoliinas ,. From your log, it seems like the DeepVariant model has made a prediction with unexpected numerical value. From your log, I'm unable to tell why this has occurred. In this command:. `podman run -it --rm --security-opt=label=disable --hooks-dir=/usr/share/containers/oci/hooks.d/ --gpus 1 -v /data:/data --device nvidia.com/gpu=all google/deepvariant:1.6.1-gpu /opt/deepvariant/bin/postprocess_variants --ref ""/data/references/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz"" --infile ""/data/variants/sample1.intermediate/call_variants_output.tfrecord.gz"" --outfile ""/data/variants/sample1.vcf.gz"" --cpus ""19"" --gvcf_outfile ""/data/variants/sample1.g.vcf.gz"" --nonvariant_site_tfrecord_path ""/data/variants/sample1.intermediate/gvcf.tfrecord@19.gz""`. If you can share the `sample1.intermediate/call_variants_output.tfrecord.gz` (and optionally`sample1.intermediate/gvcf.tfrecord@19.gz` files) with me, I can able to look into the records and see which example has this issue. (Or, if you can narrow this down to a small BAM file, and if you can share that BAM file, that works too). Please email to pichuan@google.com if you can share. If you can't share the files, we can think about what we can do here to help identify which example caused the issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/849
https://github.com/google/deepvariant/issues/849:28,testability,log,log,28,"Hi @karoliinas ,. From your log, it seems like the DeepVariant model has made a prediction with unexpected numerical value. From your log, I'm unable to tell why this has occurred. In this command:. `podman run -it --rm --security-opt=label=disable --hooks-dir=/usr/share/containers/oci/hooks.d/ --gpus 1 -v /data:/data --device nvidia.com/gpu=all google/deepvariant:1.6.1-gpu /opt/deepvariant/bin/postprocess_variants --ref ""/data/references/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz"" --infile ""/data/variants/sample1.intermediate/call_variants_output.tfrecord.gz"" --outfile ""/data/variants/sample1.vcf.gz"" --cpus ""19"" --gvcf_outfile ""/data/variants/sample1.g.vcf.gz"" --nonvariant_site_tfrecord_path ""/data/variants/sample1.intermediate/gvcf.tfrecord@19.gz""`. If you can share the `sample1.intermediate/call_variants_output.tfrecord.gz` (and optionally`sample1.intermediate/gvcf.tfrecord@19.gz` files) with me, I can able to look into the records and see which example has this issue. (Or, if you can narrow this down to a small BAM file, and if you can share that BAM file, that works too). Please email to pichuan@google.com if you can share. If you can't share the files, we can think about what we can do here to help identify which example caused the issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/849
https://github.com/google/deepvariant/issues/849:134,testability,log,log,134,"Hi @karoliinas ,. From your log, it seems like the DeepVariant model has made a prediction with unexpected numerical value. From your log, I'm unable to tell why this has occurred. In this command:. `podman run -it --rm --security-opt=label=disable --hooks-dir=/usr/share/containers/oci/hooks.d/ --gpus 1 -v /data:/data --device nvidia.com/gpu=all google/deepvariant:1.6.1-gpu /opt/deepvariant/bin/postprocess_variants --ref ""/data/references/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz"" --infile ""/data/variants/sample1.intermediate/call_variants_output.tfrecord.gz"" --outfile ""/data/variants/sample1.vcf.gz"" --cpus ""19"" --gvcf_outfile ""/data/variants/sample1.g.vcf.gz"" --nonvariant_site_tfrecord_path ""/data/variants/sample1.intermediate/gvcf.tfrecord@19.gz""`. If you can share the `sample1.intermediate/call_variants_output.tfrecord.gz` (and optionally`sample1.intermediate/gvcf.tfrecord@19.gz` files) with me, I can able to look into the records and see which example has this issue. (Or, if you can narrow this down to a small BAM file, and if you can share that BAM file, that works too). Please email to pichuan@google.com if you can share. If you can't share the files, we can think about what we can do here to help identify which example caused the issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/849
https://github.com/google/deepvariant/issues/849:251,testability,hook,hooks-dir,251,"Hi @karoliinas ,. From your log, it seems like the DeepVariant model has made a prediction with unexpected numerical value. From your log, I'm unable to tell why this has occurred. In this command:. `podman run -it --rm --security-opt=label=disable --hooks-dir=/usr/share/containers/oci/hooks.d/ --gpus 1 -v /data:/data --device nvidia.com/gpu=all google/deepvariant:1.6.1-gpu /opt/deepvariant/bin/postprocess_variants --ref ""/data/references/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz"" --infile ""/data/variants/sample1.intermediate/call_variants_output.tfrecord.gz"" --outfile ""/data/variants/sample1.vcf.gz"" --cpus ""19"" --gvcf_outfile ""/data/variants/sample1.g.vcf.gz"" --nonvariant_site_tfrecord_path ""/data/variants/sample1.intermediate/gvcf.tfrecord@19.gz""`. If you can share the `sample1.intermediate/call_variants_output.tfrecord.gz` (and optionally`sample1.intermediate/gvcf.tfrecord@19.gz` files) with me, I can able to look into the records and see which example has this issue. (Or, if you can narrow this down to a small BAM file, and if you can share that BAM file, that works too). Please email to pichuan@google.com if you can share. If you can't share the files, we can think about what we can do here to help identify which example caused the issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/849
https://github.com/google/deepvariant/issues/849:287,testability,hook,hooks,287,"Hi @karoliinas ,. From your log, it seems like the DeepVariant model has made a prediction with unexpected numerical value. From your log, I'm unable to tell why this has occurred. In this command:. `podman run -it --rm --security-opt=label=disable --hooks-dir=/usr/share/containers/oci/hooks.d/ --gpus 1 -v /data:/data --device nvidia.com/gpu=all google/deepvariant:1.6.1-gpu /opt/deepvariant/bin/postprocess_variants --ref ""/data/references/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz"" --infile ""/data/variants/sample1.intermediate/call_variants_output.tfrecord.gz"" --outfile ""/data/variants/sample1.vcf.gz"" --cpus ""19"" --gvcf_outfile ""/data/variants/sample1.g.vcf.gz"" --nonvariant_site_tfrecord_path ""/data/variants/sample1.intermediate/gvcf.tfrecord@19.gz""`. If you can share the `sample1.intermediate/call_variants_output.tfrecord.gz` (and optionally`sample1.intermediate/gvcf.tfrecord@19.gz` files) with me, I can able to look into the records and see which example has this issue. (Or, if you can narrow this down to a small BAM file, and if you can share that BAM file, that works too). Please email to pichuan@google.com if you can share. If you can't share the files, we can think about what we can do here to help identify which example caused the issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/849
https://github.com/google/deepvariant/issues/849:189,usability,command,command,189,"Hi @karoliinas ,. From your log, it seems like the DeepVariant model has made a prediction with unexpected numerical value. From your log, I'm unable to tell why this has occurred. In this command:. `podman run -it --rm --security-opt=label=disable --hooks-dir=/usr/share/containers/oci/hooks.d/ --gpus 1 -v /data:/data --device nvidia.com/gpu=all google/deepvariant:1.6.1-gpu /opt/deepvariant/bin/postprocess_variants --ref ""/data/references/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz"" --infile ""/data/variants/sample1.intermediate/call_variants_output.tfrecord.gz"" --outfile ""/data/variants/sample1.vcf.gz"" --cpus ""19"" --gvcf_outfile ""/data/variants/sample1.g.vcf.gz"" --nonvariant_site_tfrecord_path ""/data/variants/sample1.intermediate/gvcf.tfrecord@19.gz""`. If you can share the `sample1.intermediate/call_variants_output.tfrecord.gz` (and optionally`sample1.intermediate/gvcf.tfrecord@19.gz` files) with me, I can able to look into the records and see which example has this issue. (Or, if you can narrow this down to a small BAM file, and if you can share that BAM file, that works too). Please email to pichuan@google.com if you can share. If you can't share the files, we can think about what we can do here to help identify which example caused the issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/849
https://github.com/google/deepvariant/issues/849:1227,usability,help,help,1227,"Hi @karoliinas ,. From your log, it seems like the DeepVariant model has made a prediction with unexpected numerical value. From your log, I'm unable to tell why this has occurred. In this command:. `podman run -it --rm --security-opt=label=disable --hooks-dir=/usr/share/containers/oci/hooks.d/ --gpus 1 -v /data:/data --device nvidia.com/gpu=all google/deepvariant:1.6.1-gpu /opt/deepvariant/bin/postprocess_variants --ref ""/data/references/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz"" --infile ""/data/variants/sample1.intermediate/call_variants_output.tfrecord.gz"" --outfile ""/data/variants/sample1.vcf.gz"" --cpus ""19"" --gvcf_outfile ""/data/variants/sample1.g.vcf.gz"" --nonvariant_site_tfrecord_path ""/data/variants/sample1.intermediate/gvcf.tfrecord@19.gz""`. If you can share the `sample1.intermediate/call_variants_output.tfrecord.gz` (and optionally`sample1.intermediate/gvcf.tfrecord@19.gz` files) with me, I can able to look into the records and see which example has this issue. (Or, if you can narrow this down to a small BAM file, and if you can share that BAM file, that works too). Please email to pichuan@google.com if you can share. If you can't share the files, we can think about what we can do here to help identify which example caused the issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/849
https://github.com/google/deepvariant/issues/849:764,availability,fault,faulty,764,"Hi @pichuan, thank you for getting back so quickly! I'm working on patient data, so unfortunately it's not something I can share. . About the files you requested, I now see that there is no file called: `sample1.intermediate/call_variants_output.tfrecord.gz`, instead there's a number of files:. `call_variants_output-[00000-00015]-of-00016.tfrecord.gz`. also instead of `gvcf.tfrecord@19.gz`. there are:. `gvcf.tfrecord-[00000-00018]-of-00019.gz`. That's probably why the above command for postprocess_variants doesn't work, right? I am using 19 threads. I copied the command from `--dry_run=true`. So my question is, how to pass multiple arguments to` --infile` and `--nonvariant_site_tfrecord_path`? Thank you so much, I'm very happy if it turns I merely had a faulty command! .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/849
https://github.com/google/deepvariant/issues/849:764,energy efficiency,fault,faulty,764,"Hi @pichuan, thank you for getting back so quickly! I'm working on patient data, so unfortunately it's not something I can share. . About the files you requested, I now see that there is no file called: `sample1.intermediate/call_variants_output.tfrecord.gz`, instead there's a number of files:. `call_variants_output-[00000-00015]-of-00016.tfrecord.gz`. also instead of `gvcf.tfrecord@19.gz`. there are:. `gvcf.tfrecord-[00000-00018]-of-00019.gz`. That's probably why the above command for postprocess_variants doesn't work, right? I am using 19 threads. I copied the command from `--dry_run=true`. So my question is, how to pass multiple arguments to` --infile` and `--nonvariant_site_tfrecord_path`? Thank you so much, I'm very happy if it turns I merely had a faulty command! .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/849
https://github.com/google/deepvariant/issues/849:123,interoperability,share,share,123,"Hi @pichuan, thank you for getting back so quickly! I'm working on patient data, so unfortunately it's not something I can share. . About the files you requested, I now see that there is no file called: `sample1.intermediate/call_variants_output.tfrecord.gz`, instead there's a number of files:. `call_variants_output-[00000-00015]-of-00016.tfrecord.gz`. also instead of `gvcf.tfrecord@19.gz`. there are:. `gvcf.tfrecord-[00000-00018]-of-00019.gz`. That's probably why the above command for postprocess_variants doesn't work, right? I am using 19 threads. I copied the command from `--dry_run=true`. So my question is, how to pass multiple arguments to` --infile` and `--nonvariant_site_tfrecord_path`? Thank you so much, I'm very happy if it turns I merely had a faulty command! .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/849
https://github.com/google/deepvariant/issues/849:212,modifiability,interm,intermediate,212,"Hi @pichuan, thank you for getting back so quickly! I'm working on patient data, so unfortunately it's not something I can share. . About the files you requested, I now see that there is no file called: `sample1.intermediate/call_variants_output.tfrecord.gz`, instead there's a number of files:. `call_variants_output-[00000-00015]-of-00016.tfrecord.gz`. also instead of `gvcf.tfrecord@19.gz`. there are:. `gvcf.tfrecord-[00000-00018]-of-00019.gz`. That's probably why the above command for postprocess_variants doesn't work, right? I am using 19 threads. I copied the command from `--dry_run=true`. So my question is, how to pass multiple arguments to` --infile` and `--nonvariant_site_tfrecord_path`? Thank you so much, I'm very happy if it turns I merely had a faulty command! .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/849
https://github.com/google/deepvariant/issues/849:764,performance,fault,faulty,764,"Hi @pichuan, thank you for getting back so quickly! I'm working on patient data, so unfortunately it's not something I can share. . About the files you requested, I now see that there is no file called: `sample1.intermediate/call_variants_output.tfrecord.gz`, instead there's a number of files:. `call_variants_output-[00000-00015]-of-00016.tfrecord.gz`. also instead of `gvcf.tfrecord@19.gz`. there are:. `gvcf.tfrecord-[00000-00018]-of-00019.gz`. That's probably why the above command for postprocess_variants doesn't work, right? I am using 19 threads. I copied the command from `--dry_run=true`. So my question is, how to pass multiple arguments to` --infile` and `--nonvariant_site_tfrecord_path`? Thank you so much, I'm very happy if it turns I merely had a faulty command! .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/849
https://github.com/google/deepvariant/issues/849:512,reliability,doe,doesn,512,"Hi @pichuan, thank you for getting back so quickly! I'm working on patient data, so unfortunately it's not something I can share. . About the files you requested, I now see that there is no file called: `sample1.intermediate/call_variants_output.tfrecord.gz`, instead there's a number of files:. `call_variants_output-[00000-00015]-of-00016.tfrecord.gz`. also instead of `gvcf.tfrecord@19.gz`. there are:. `gvcf.tfrecord-[00000-00018]-of-00019.gz`. That's probably why the above command for postprocess_variants doesn't work, right? I am using 19 threads. I copied the command from `--dry_run=true`. So my question is, how to pass multiple arguments to` --infile` and `--nonvariant_site_tfrecord_path`? Thank you so much, I'm very happy if it turns I merely had a faulty command! .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/849
https://github.com/google/deepvariant/issues/849:764,reliability,fault,faulty,764,"Hi @pichuan, thank you for getting back so quickly! I'm working on patient data, so unfortunately it's not something I can share. . About the files you requested, I now see that there is no file called: `sample1.intermediate/call_variants_output.tfrecord.gz`, instead there's a number of files:. `call_variants_output-[00000-00015]-of-00016.tfrecord.gz`. also instead of `gvcf.tfrecord@19.gz`. there are:. `gvcf.tfrecord-[00000-00018]-of-00019.gz`. That's probably why the above command for postprocess_variants doesn't work, right? I am using 19 threads. I copied the command from `--dry_run=true`. So my question is, how to pass multiple arguments to` --infile` and `--nonvariant_site_tfrecord_path`? Thank you so much, I'm very happy if it turns I merely had a faulty command! .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/849
https://github.com/google/deepvariant/issues/849:764,safety,fault,faulty,764,"Hi @pichuan, thank you for getting back so quickly! I'm working on patient data, so unfortunately it's not something I can share. . About the files you requested, I now see that there is no file called: `sample1.intermediate/call_variants_output.tfrecord.gz`, instead there's a number of files:. `call_variants_output-[00000-00015]-of-00016.tfrecord.gz`. also instead of `gvcf.tfrecord@19.gz`. there are:. `gvcf.tfrecord-[00000-00018]-of-00019.gz`. That's probably why the above command for postprocess_variants doesn't work, right? I am using 19 threads. I copied the command from `--dry_run=true`. So my question is, how to pass multiple arguments to` --infile` and `--nonvariant_site_tfrecord_path`? Thank you so much, I'm very happy if it turns I merely had a faulty command! .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/849
https://github.com/google/deepvariant/issues/849:479,usability,command,command,479,"Hi @pichuan, thank you for getting back so quickly! I'm working on patient data, so unfortunately it's not something I can share. . About the files you requested, I now see that there is no file called: `sample1.intermediate/call_variants_output.tfrecord.gz`, instead there's a number of files:. `call_variants_output-[00000-00015]-of-00016.tfrecord.gz`. also instead of `gvcf.tfrecord@19.gz`. there are:. `gvcf.tfrecord-[00000-00018]-of-00019.gz`. That's probably why the above command for postprocess_variants doesn't work, right? I am using 19 threads. I copied the command from `--dry_run=true`. So my question is, how to pass multiple arguments to` --infile` and `--nonvariant_site_tfrecord_path`? Thank you so much, I'm very happy if it turns I merely had a faulty command! .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/849
https://github.com/google/deepvariant/issues/849:569,usability,command,command,569,"Hi @pichuan, thank you for getting back so quickly! I'm working on patient data, so unfortunately it's not something I can share. . About the files you requested, I now see that there is no file called: `sample1.intermediate/call_variants_output.tfrecord.gz`, instead there's a number of files:. `call_variants_output-[00000-00015]-of-00016.tfrecord.gz`. also instead of `gvcf.tfrecord@19.gz`. there are:. `gvcf.tfrecord-[00000-00018]-of-00019.gz`. That's probably why the above command for postprocess_variants doesn't work, right? I am using 19 threads. I copied the command from `--dry_run=true`. So my question is, how to pass multiple arguments to` --infile` and `--nonvariant_site_tfrecord_path`? Thank you so much, I'm very happy if it turns I merely had a faulty command! .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/849
https://github.com/google/deepvariant/issues/849:771,usability,command,command,771,"Hi @pichuan, thank you for getting back so quickly! I'm working on patient data, so unfortunately it's not something I can share. . About the files you requested, I now see that there is no file called: `sample1.intermediate/call_variants_output.tfrecord.gz`, instead there's a number of files:. `call_variants_output-[00000-00015]-of-00016.tfrecord.gz`. also instead of `gvcf.tfrecord@19.gz`. there are:. `gvcf.tfrecord-[00000-00018]-of-00019.gz`. That's probably why the above command for postprocess_variants doesn't work, right? I am using 19 threads. I copied the command from `--dry_run=true`. So my question is, how to pass multiple arguments to` --infile` and `--nonvariant_site_tfrecord_path`? Thank you so much, I'm very happy if it turns I merely had a faulty command! .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/849
https://github.com/google/deepvariant/issues/849:101,deployability,version,version,101,"Oh, and since we're here, I mentioned problems with the GPU. It seems that DeepVariant is using cuda version 11.3.1, but the nvidia driver (version 555.42.02) on the server is using cuda version 12.5. ```. nvidia-smi. +-----------------------------------------------------------------------------------------+. | NVIDIA-SMI 555.42.02 Driver Version: 555.42.02 CUDA Version: 12.5 |. |-----------------------------------------+------------------------+----------------------+. | GPU Name Persistence-M | Bus-Id Disp.A | Volatile Uncorr. ECC |. | Fan Temp Perf Pwr:Usage/Cap | Memory-Usage | GPU-Util Compute M. |. | | | MIG M. |. |=========================================+========================+======================|. | 0 Tesla V100-PCIE-16GB Off | 00000000:00:07.0 Off | 0 |. | N/A 28C P0 35W / 250W | 1MiB / 16384MiB | 0% Default |. | | | N/A |. +-----------------------------------------+------------------------+----------------------+. . +-----------------------------------------------------------------------------------------+. | Processes: |. | GPU GI CI PID Type Process name GPU Memory |. | ID ID Usage |. |=========================================================================================|. | No running processes found |. +-----------------------------------------------------------------------------------------+. ```. Should I update the driver, cuda or both? Also, installed on the server is cuda version 12.3, so I'm feeling a bit confused as to where the 12.5 comes from. ```. nvcc --version. nvcc: NVIDIA (R) Cuda compiler driver. Copyright (c) 2005-2023 NVIDIA Corporation. Built on Wed_Nov_22_10:17:15_PST_2023. Cuda compilation tools, release 12.3, V12.3.107. Build cuda_12.3.r12.3/compiler.33567101_0. ```. It's a right mess! Sometimes call_variants uses gpu and at other times it stalls. That's why I'm now running the commands separately, I have a lot of outputs from make_examples. Our IT support say they're happy the gpu works part of the time :) It's adding a l",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/849
https://github.com/google/deepvariant/issues/849:140,deployability,version,version,140,"Oh, and since we're here, I mentioned problems with the GPU. It seems that DeepVariant is using cuda version 11.3.1, but the nvidia driver (version 555.42.02) on the server is using cuda version 12.5. ```. nvidia-smi. +-----------------------------------------------------------------------------------------+. | NVIDIA-SMI 555.42.02 Driver Version: 555.42.02 CUDA Version: 12.5 |. |-----------------------------------------+------------------------+----------------------+. | GPU Name Persistence-M | Bus-Id Disp.A | Volatile Uncorr. ECC |. | Fan Temp Perf Pwr:Usage/Cap | Memory-Usage | GPU-Util Compute M. |. | | | MIG M. |. |=========================================+========================+======================|. | 0 Tesla V100-PCIE-16GB Off | 00000000:00:07.0 Off | 0 |. | N/A 28C P0 35W / 250W | 1MiB / 16384MiB | 0% Default |. | | | N/A |. +-----------------------------------------+------------------------+----------------------+. . +-----------------------------------------------------------------------------------------+. | Processes: |. | GPU GI CI PID Type Process name GPU Memory |. | ID ID Usage |. |=========================================================================================|. | No running processes found |. +-----------------------------------------------------------------------------------------+. ```. Should I update the driver, cuda or both? Also, installed on the server is cuda version 12.3, so I'm feeling a bit confused as to where the 12.5 comes from. ```. nvcc --version. nvcc: NVIDIA (R) Cuda compiler driver. Copyright (c) 2005-2023 NVIDIA Corporation. Built on Wed_Nov_22_10:17:15_PST_2023. Cuda compilation tools, release 12.3, V12.3.107. Build cuda_12.3.r12.3/compiler.33567101_0. ```. It's a right mess! Sometimes call_variants uses gpu and at other times it stalls. That's why I'm now running the commands separately, I have a lot of outputs from make_examples. Our IT support say they're happy the gpu works part of the time :) It's adding a l",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/849
https://github.com/google/deepvariant/issues/849:187,deployability,version,version,187,"Oh, and since we're here, I mentioned problems with the GPU. It seems that DeepVariant is using cuda version 11.3.1, but the nvidia driver (version 555.42.02) on the server is using cuda version 12.5. ```. nvidia-smi. +-----------------------------------------------------------------------------------------+. | NVIDIA-SMI 555.42.02 Driver Version: 555.42.02 CUDA Version: 12.5 |. |-----------------------------------------+------------------------+----------------------+. | GPU Name Persistence-M | Bus-Id Disp.A | Volatile Uncorr. ECC |. | Fan Temp Perf Pwr:Usage/Cap | Memory-Usage | GPU-Util Compute M. |. | | | MIG M. |. |=========================================+========================+======================|. | 0 Tesla V100-PCIE-16GB Off | 00000000:00:07.0 Off | 0 |. | N/A 28C P0 35W / 250W | 1MiB / 16384MiB | 0% Default |. | | | N/A |. +-----------------------------------------+------------------------+----------------------+. . +-----------------------------------------------------------------------------------------+. | Processes: |. | GPU GI CI PID Type Process name GPU Memory |. | ID ID Usage |. |=========================================================================================|. | No running processes found |. +-----------------------------------------------------------------------------------------+. ```. Should I update the driver, cuda or both? Also, installed on the server is cuda version 12.3, so I'm feeling a bit confused as to where the 12.5 comes from. ```. nvcc --version. nvcc: NVIDIA (R) Cuda compiler driver. Copyright (c) 2005-2023 NVIDIA Corporation. Built on Wed_Nov_22_10:17:15_PST_2023. Cuda compilation tools, release 12.3, V12.3.107. Build cuda_12.3.r12.3/compiler.33567101_0. ```. It's a right mess! Sometimes call_variants uses gpu and at other times it stalls. That's why I'm now running the commands separately, I have a lot of outputs from make_examples. Our IT support say they're happy the gpu works part of the time :) It's adding a l",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/849
https://github.com/google/deepvariant/issues/849:341,deployability,Version,Version,341,"Oh, and since we're here, I mentioned problems with the GPU. It seems that DeepVariant is using cuda version 11.3.1, but the nvidia driver (version 555.42.02) on the server is using cuda version 12.5. ```. nvidia-smi. +-----------------------------------------------------------------------------------------+. | NVIDIA-SMI 555.42.02 Driver Version: 555.42.02 CUDA Version: 12.5 |. |-----------------------------------------+------------------------+----------------------+. | GPU Name Persistence-M | Bus-Id Disp.A | Volatile Uncorr. ECC |. | Fan Temp Perf Pwr:Usage/Cap | Memory-Usage | GPU-Util Compute M. |. | | | MIG M. |. |=========================================+========================+======================|. | 0 Tesla V100-PCIE-16GB Off | 00000000:00:07.0 Off | 0 |. | N/A 28C P0 35W / 250W | 1MiB / 16384MiB | 0% Default |. | | | N/A |. +-----------------------------------------+------------------------+----------------------+. . +-----------------------------------------------------------------------------------------+. | Processes: |. | GPU GI CI PID Type Process name GPU Memory |. | ID ID Usage |. |=========================================================================================|. | No running processes found |. +-----------------------------------------------------------------------------------------+. ```. Should I update the driver, cuda or both? Also, installed on the server is cuda version 12.3, so I'm feeling a bit confused as to where the 12.5 comes from. ```. nvcc --version. nvcc: NVIDIA (R) Cuda compiler driver. Copyright (c) 2005-2023 NVIDIA Corporation. Built on Wed_Nov_22_10:17:15_PST_2023. Cuda compilation tools, release 12.3, V12.3.107. Build cuda_12.3.r12.3/compiler.33567101_0. ```. It's a right mess! Sometimes call_variants uses gpu and at other times it stalls. That's why I'm now running the commands separately, I have a lot of outputs from make_examples. Our IT support say they're happy the gpu works part of the time :) It's adding a l",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/849
https://github.com/google/deepvariant/issues/849:365,deployability,Version,Version,365,"Oh, and since we're here, I mentioned problems with the GPU. It seems that DeepVariant is using cuda version 11.3.1, but the nvidia driver (version 555.42.02) on the server is using cuda version 12.5. ```. nvidia-smi. +-----------------------------------------------------------------------------------------+. | NVIDIA-SMI 555.42.02 Driver Version: 555.42.02 CUDA Version: 12.5 |. |-----------------------------------------+------------------------+----------------------+. | GPU Name Persistence-M | Bus-Id Disp.A | Volatile Uncorr. ECC |. | Fan Temp Perf Pwr:Usage/Cap | Memory-Usage | GPU-Util Compute M. |. | | | MIG M. |. |=========================================+========================+======================|. | 0 Tesla V100-PCIE-16GB Off | 00000000:00:07.0 Off | 0 |. | N/A 28C P0 35W / 250W | 1MiB / 16384MiB | 0% Default |. | | | N/A |. +-----------------------------------------+------------------------+----------------------+. . +-----------------------------------------------------------------------------------------+. | Processes: |. | GPU GI CI PID Type Process name GPU Memory |. | ID ID Usage |. |=========================================================================================|. | No running processes found |. +-----------------------------------------------------------------------------------------+. ```. Should I update the driver, cuda or both? Also, installed on the server is cuda version 12.3, so I'm feeling a bit confused as to where the 12.5 comes from. ```. nvcc --version. nvcc: NVIDIA (R) Cuda compiler driver. Copyright (c) 2005-2023 NVIDIA Corporation. Built on Wed_Nov_22_10:17:15_PST_2023. Cuda compilation tools, release 12.3, V12.3.107. Build cuda_12.3.r12.3/compiler.33567101_0. ```. It's a right mess! Sometimes call_variants uses gpu and at other times it stalls. That's why I'm now running the commands separately, I have a lot of outputs from make_examples. Our IT support say they're happy the gpu works part of the time :) It's adding a l",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/849
https://github.com/google/deepvariant/issues/849:1352,deployability,updat,update,1352,".42.02) on the server is using cuda version 12.5. ```. nvidia-smi. +-----------------------------------------------------------------------------------------+. | NVIDIA-SMI 555.42.02 Driver Version: 555.42.02 CUDA Version: 12.5 |. |-----------------------------------------+------------------------+----------------------+. | GPU Name Persistence-M | Bus-Id Disp.A | Volatile Uncorr. ECC |. | Fan Temp Perf Pwr:Usage/Cap | Memory-Usage | GPU-Util Compute M. |. | | | MIG M. |. |=========================================+========================+======================|. | 0 Tesla V100-PCIE-16GB Off | 00000000:00:07.0 Off | 0 |. | N/A 28C P0 35W / 250W | 1MiB / 16384MiB | 0% Default |. | | | N/A |. +-----------------------------------------+------------------------+----------------------+. . +-----------------------------------------------------------------------------------------+. | Processes: |. | GPU GI CI PID Type Process name GPU Memory |. | ID ID Usage |. |=========================================================================================|. | No running processes found |. +-----------------------------------------------------------------------------------------+. ```. Should I update the driver, cuda or both? Also, installed on the server is cuda version 12.3, so I'm feeling a bit confused as to where the 12.5 comes from. ```. nvcc --version. nvcc: NVIDIA (R) Cuda compiler driver. Copyright (c) 2005-2023 NVIDIA Corporation. Built on Wed_Nov_22_10:17:15_PST_2023. Cuda compilation tools, release 12.3, V12.3.107. Build cuda_12.3.r12.3/compiler.33567101_0. ```. It's a right mess! Sometimes call_variants uses gpu and at other times it stalls. That's why I'm now running the commands separately, I have a lot of outputs from make_examples. Our IT support say they're happy the gpu works part of the time :) It's adding a lot of extra work, and I'm trying to come up with a solution. Since it's completely unrelated to the topic, perhaps I should create a new issue instead?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/849
https://github.com/google/deepvariant/issues/849:1391,deployability,instal,installed,1391,".42.02) on the server is using cuda version 12.5. ```. nvidia-smi. +-----------------------------------------------------------------------------------------+. | NVIDIA-SMI 555.42.02 Driver Version: 555.42.02 CUDA Version: 12.5 |. |-----------------------------------------+------------------------+----------------------+. | GPU Name Persistence-M | Bus-Id Disp.A | Volatile Uncorr. ECC |. | Fan Temp Perf Pwr:Usage/Cap | Memory-Usage | GPU-Util Compute M. |. | | | MIG M. |. |=========================================+========================+======================|. | 0 Tesla V100-PCIE-16GB Off | 00000000:00:07.0 Off | 0 |. | N/A 28C P0 35W / 250W | 1MiB / 16384MiB | 0% Default |. | | | N/A |. +-----------------------------------------+------------------------+----------------------+. . +-----------------------------------------------------------------------------------------+. | Processes: |. | GPU GI CI PID Type Process name GPU Memory |. | ID ID Usage |. |=========================================================================================|. | No running processes found |. +-----------------------------------------------------------------------------------------+. ```. Should I update the driver, cuda or both? Also, installed on the server is cuda version 12.3, so I'm feeling a bit confused as to where the 12.5 comes from. ```. nvcc --version. nvcc: NVIDIA (R) Cuda compiler driver. Copyright (c) 2005-2023 NVIDIA Corporation. Built on Wed_Nov_22_10:17:15_PST_2023. Cuda compilation tools, release 12.3, V12.3.107. Build cuda_12.3.r12.3/compiler.33567101_0. ```. It's a right mess! Sometimes call_variants uses gpu and at other times it stalls. That's why I'm now running the commands separately, I have a lot of outputs from make_examples. Our IT support say they're happy the gpu works part of the time :) It's adding a lot of extra work, and I'm trying to come up with a solution. Since it's completely unrelated to the topic, perhaps I should create a new issue instead?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/849
https://github.com/google/deepvariant/issues/849:1423,deployability,version,version,1423,".42.02) on the server is using cuda version 12.5. ```. nvidia-smi. +-----------------------------------------------------------------------------------------+. | NVIDIA-SMI 555.42.02 Driver Version: 555.42.02 CUDA Version: 12.5 |. |-----------------------------------------+------------------------+----------------------+. | GPU Name Persistence-M | Bus-Id Disp.A | Volatile Uncorr. ECC |. | Fan Temp Perf Pwr:Usage/Cap | Memory-Usage | GPU-Util Compute M. |. | | | MIG M. |. |=========================================+========================+======================|. | 0 Tesla V100-PCIE-16GB Off | 00000000:00:07.0 Off | 0 |. | N/A 28C P0 35W / 250W | 1MiB / 16384MiB | 0% Default |. | | | N/A |. +-----------------------------------------+------------------------+----------------------+. . +-----------------------------------------------------------------------------------------+. | Processes: |. | GPU GI CI PID Type Process name GPU Memory |. | ID ID Usage |. |=========================================================================================|. | No running processes found |. +-----------------------------------------------------------------------------------------+. ```. Should I update the driver, cuda or both? Also, installed on the server is cuda version 12.3, so I'm feeling a bit confused as to where the 12.5 comes from. ```. nvcc --version. nvcc: NVIDIA (R) Cuda compiler driver. Copyright (c) 2005-2023 NVIDIA Corporation. Built on Wed_Nov_22_10:17:15_PST_2023. Cuda compilation tools, release 12.3, V12.3.107. Build cuda_12.3.r12.3/compiler.33567101_0. ```. It's a right mess! Sometimes call_variants uses gpu and at other times it stalls. That's why I'm now running the commands separately, I have a lot of outputs from make_examples. Our IT support say they're happy the gpu works part of the time :) It's adding a lot of extra work, and I'm trying to come up with a solution. Since it's completely unrelated to the topic, perhaps I should create a new issue instead?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/849
https://github.com/google/deepvariant/issues/849:1512,deployability,version,version,1512,".42.02) on the server is using cuda version 12.5. ```. nvidia-smi. +-----------------------------------------------------------------------------------------+. | NVIDIA-SMI 555.42.02 Driver Version: 555.42.02 CUDA Version: 12.5 |. |-----------------------------------------+------------------------+----------------------+. | GPU Name Persistence-M | Bus-Id Disp.A | Volatile Uncorr. ECC |. | Fan Temp Perf Pwr:Usage/Cap | Memory-Usage | GPU-Util Compute M. |. | | | MIG M. |. |=========================================+========================+======================|. | 0 Tesla V100-PCIE-16GB Off | 00000000:00:07.0 Off | 0 |. | N/A 28C P0 35W / 250W | 1MiB / 16384MiB | 0% Default |. | | | N/A |. +-----------------------------------------+------------------------+----------------------+. . +-----------------------------------------------------------------------------------------+. | Processes: |. | GPU GI CI PID Type Process name GPU Memory |. | ID ID Usage |. |=========================================================================================|. | No running processes found |. +-----------------------------------------------------------------------------------------+. ```. Should I update the driver, cuda or both? Also, installed on the server is cuda version 12.3, so I'm feeling a bit confused as to where the 12.5 comes from. ```. nvcc --version. nvcc: NVIDIA (R) Cuda compiler driver. Copyright (c) 2005-2023 NVIDIA Corporation. Built on Wed_Nov_22_10:17:15_PST_2023. Cuda compilation tools, release 12.3, V12.3.107. Build cuda_12.3.r12.3/compiler.33567101_0. ```. It's a right mess! Sometimes call_variants uses gpu and at other times it stalls. That's why I'm now running the commands separately, I have a lot of outputs from make_examples. Our IT support say they're happy the gpu works part of the time :) It's adding a lot of extra work, and I'm trying to come up with a solution. Since it's completely unrelated to the topic, perhaps I should create a new issue instead?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/849
https://github.com/google/deepvariant/issues/849:1667,deployability,releas,release,1667,".42.02) on the server is using cuda version 12.5. ```. nvidia-smi. +-----------------------------------------------------------------------------------------+. | NVIDIA-SMI 555.42.02 Driver Version: 555.42.02 CUDA Version: 12.5 |. |-----------------------------------------+------------------------+----------------------+. | GPU Name Persistence-M | Bus-Id Disp.A | Volatile Uncorr. ECC |. | Fan Temp Perf Pwr:Usage/Cap | Memory-Usage | GPU-Util Compute M. |. | | | MIG M. |. |=========================================+========================+======================|. | 0 Tesla V100-PCIE-16GB Off | 00000000:00:07.0 Off | 0 |. | N/A 28C P0 35W / 250W | 1MiB / 16384MiB | 0% Default |. | | | N/A |. +-----------------------------------------+------------------------+----------------------+. . +-----------------------------------------------------------------------------------------+. | Processes: |. | GPU GI CI PID Type Process name GPU Memory |. | ID ID Usage |. |=========================================================================================|. | No running processes found |. +-----------------------------------------------------------------------------------------+. ```. Should I update the driver, cuda or both? Also, installed on the server is cuda version 12.3, so I'm feeling a bit confused as to where the 12.5 comes from. ```. nvcc --version. nvcc: NVIDIA (R) Cuda compiler driver. Copyright (c) 2005-2023 NVIDIA Corporation. Built on Wed_Nov_22_10:17:15_PST_2023. Cuda compilation tools, release 12.3, V12.3.107. Build cuda_12.3.r12.3/compiler.33567101_0. ```. It's a right mess! Sometimes call_variants uses gpu and at other times it stalls. That's why I'm now running the commands separately, I have a lot of outputs from make_examples. Our IT support say they're happy the gpu works part of the time :) It's adding a lot of extra work, and I'm trying to come up with a solution. Since it's completely unrelated to the topic, perhaps I should create a new issue instead?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/849
https://github.com/google/deepvariant/issues/849:1692,deployability,Build,Build,1692,".42.02) on the server is using cuda version 12.5. ```. nvidia-smi. +-----------------------------------------------------------------------------------------+. | NVIDIA-SMI 555.42.02 Driver Version: 555.42.02 CUDA Version: 12.5 |. |-----------------------------------------+------------------------+----------------------+. | GPU Name Persistence-M | Bus-Id Disp.A | Volatile Uncorr. ECC |. | Fan Temp Perf Pwr:Usage/Cap | Memory-Usage | GPU-Util Compute M. |. | | | MIG M. |. |=========================================+========================+======================|. | 0 Tesla V100-PCIE-16GB Off | 00000000:00:07.0 Off | 0 |. | N/A 28C P0 35W / 250W | 1MiB / 16384MiB | 0% Default |. | | | N/A |. +-----------------------------------------+------------------------+----------------------+. . +-----------------------------------------------------------------------------------------+. | Processes: |. | GPU GI CI PID Type Process name GPU Memory |. | ID ID Usage |. |=========================================================================================|. | No running processes found |. +-----------------------------------------------------------------------------------------+. ```. Should I update the driver, cuda or both? Also, installed on the server is cuda version 12.3, so I'm feeling a bit confused as to where the 12.5 comes from. ```. nvcc --version. nvcc: NVIDIA (R) Cuda compiler driver. Copyright (c) 2005-2023 NVIDIA Corporation. Built on Wed_Nov_22_10:17:15_PST_2023. Cuda compilation tools, release 12.3, V12.3.107. Build cuda_12.3.r12.3/compiler.33567101_0. ```. It's a right mess! Sometimes call_variants uses gpu and at other times it stalls. That's why I'm now running the commands separately, I have a lot of outputs from make_examples. Our IT support say they're happy the gpu works part of the time :) It's adding a lot of extra work, and I'm trying to come up with a solution. Since it's completely unrelated to the topic, perhaps I should create a new issue instead?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/849
https://github.com/google/deepvariant/issues/849:56,energy efficiency,GPU,GPU,56,"Oh, and since we're here, I mentioned problems with the GPU. It seems that DeepVariant is using cuda version 11.3.1, but the nvidia driver (version 555.42.02) on the server is using cuda version 12.5. ```. nvidia-smi. +-----------------------------------------------------------------------------------------+. | NVIDIA-SMI 555.42.02 Driver Version: 555.42.02 CUDA Version: 12.5 |. |-----------------------------------------+------------------------+----------------------+. | GPU Name Persistence-M | Bus-Id Disp.A | Volatile Uncorr. ECC |. | Fan Temp Perf Pwr:Usage/Cap | Memory-Usage | GPU-Util Compute M. |. | | | MIG M. |. |=========================================+========================+======================|. | 0 Tesla V100-PCIE-16GB Off | 00000000:00:07.0 Off | 0 |. | N/A 28C P0 35W / 250W | 1MiB / 16384MiB | 0% Default |. | | | N/A |. +-----------------------------------------+------------------------+----------------------+. . +-----------------------------------------------------------------------------------------+. | Processes: |. | GPU GI CI PID Type Process name GPU Memory |. | ID ID Usage |. |=========================================================================================|. | No running processes found |. +-----------------------------------------------------------------------------------------+. ```. Should I update the driver, cuda or both? Also, installed on the server is cuda version 12.3, so I'm feeling a bit confused as to where the 12.5 comes from. ```. nvcc --version. nvcc: NVIDIA (R) Cuda compiler driver. Copyright (c) 2005-2023 NVIDIA Corporation. Built on Wed_Nov_22_10:17:15_PST_2023. Cuda compilation tools, release 12.3, V12.3.107. Build cuda_12.3.r12.3/compiler.33567101_0. ```. It's a right mess! Sometimes call_variants uses gpu and at other times it stalls. That's why I'm now running the commands separately, I have a lot of outputs from make_examples. Our IT support say they're happy the gpu works part of the time :) It's adding a l",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/849
https://github.com/google/deepvariant/issues/849:477,energy efficiency,GPU,GPU,477,"Oh, and since we're here, I mentioned problems with the GPU. It seems that DeepVariant is using cuda version 11.3.1, but the nvidia driver (version 555.42.02) on the server is using cuda version 12.5. ```. nvidia-smi. +-----------------------------------------------------------------------------------------+. | NVIDIA-SMI 555.42.02 Driver Version: 555.42.02 CUDA Version: 12.5 |. |-----------------------------------------+------------------------+----------------------+. | GPU Name Persistence-M | Bus-Id Disp.A | Volatile Uncorr. ECC |. | Fan Temp Perf Pwr:Usage/Cap | Memory-Usage | GPU-Util Compute M. |. | | | MIG M. |. |=========================================+========================+======================|. | 0 Tesla V100-PCIE-16GB Off | 00000000:00:07.0 Off | 0 |. | N/A 28C P0 35W / 250W | 1MiB / 16384MiB | 0% Default |. | | | N/A |. +-----------------------------------------+------------------------+----------------------+. . +-----------------------------------------------------------------------------------------+. | Processes: |. | GPU GI CI PID Type Process name GPU Memory |. | ID ID Usage |. |=========================================================================================|. | No running processes found |. +-----------------------------------------------------------------------------------------+. ```. Should I update the driver, cuda or both? Also, installed on the server is cuda version 12.3, so I'm feeling a bit confused as to where the 12.5 comes from. ```. nvcc --version. nvcc: NVIDIA (R) Cuda compiler driver. Copyright (c) 2005-2023 NVIDIA Corporation. Built on Wed_Nov_22_10:17:15_PST_2023. Cuda compilation tools, release 12.3, V12.3.107. Build cuda_12.3.r12.3/compiler.33567101_0. ```. It's a right mess! Sometimes call_variants uses gpu and at other times it stalls. That's why I'm now running the commands separately, I have a lot of outputs from make_examples. Our IT support say they're happy the gpu works part of the time :) It's adding a l",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/849
https://github.com/google/deepvariant/issues/849:589,energy efficiency,GPU,GPU-Util,589,"Oh, and since we're here, I mentioned problems with the GPU. It seems that DeepVariant is using cuda version 11.3.1, but the nvidia driver (version 555.42.02) on the server is using cuda version 12.5. ```. nvidia-smi. +-----------------------------------------------------------------------------------------+. | NVIDIA-SMI 555.42.02 Driver Version: 555.42.02 CUDA Version: 12.5 |. |-----------------------------------------+------------------------+----------------------+. | GPU Name Persistence-M | Bus-Id Disp.A | Volatile Uncorr. ECC |. | Fan Temp Perf Pwr:Usage/Cap | Memory-Usage | GPU-Util Compute M. |. | | | MIG M. |. |=========================================+========================+======================|. | 0 Tesla V100-PCIE-16GB Off | 00000000:00:07.0 Off | 0 |. | N/A 28C P0 35W / 250W | 1MiB / 16384MiB | 0% Default |. | | | N/A |. +-----------------------------------------+------------------------+----------------------+. . +-----------------------------------------------------------------------------------------+. | Processes: |. | GPU GI CI PID Type Process name GPU Memory |. | ID ID Usage |. |=========================================================================================|. | No running processes found |. +-----------------------------------------------------------------------------------------+. ```. Should I update the driver, cuda or both? Also, installed on the server is cuda version 12.3, so I'm feeling a bit confused as to where the 12.5 comes from. ```. nvcc --version. nvcc: NVIDIA (R) Cuda compiler driver. Copyright (c) 2005-2023 NVIDIA Corporation. Built on Wed_Nov_22_10:17:15_PST_2023. Cuda compilation tools, release 12.3, V12.3.107. Build cuda_12.3.r12.3/compiler.33567101_0. ```. It's a right mess! Sometimes call_variants uses gpu and at other times it stalls. That's why I'm now running the commands separately, I have a lot of outputs from make_examples. Our IT support say they're happy the gpu works part of the time :) It's adding a l",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/849
https://github.com/google/deepvariant/issues/849:1057,energy efficiency,GPU,GPU,1057,". It seems that DeepVariant is using cuda version 11.3.1, but the nvidia driver (version 555.42.02) on the server is using cuda version 12.5. ```. nvidia-smi. +-----------------------------------------------------------------------------------------+. | NVIDIA-SMI 555.42.02 Driver Version: 555.42.02 CUDA Version: 12.5 |. |-----------------------------------------+------------------------+----------------------+. | GPU Name Persistence-M | Bus-Id Disp.A | Volatile Uncorr. ECC |. | Fan Temp Perf Pwr:Usage/Cap | Memory-Usage | GPU-Util Compute M. |. | | | MIG M. |. |=========================================+========================+======================|. | 0 Tesla V100-PCIE-16GB Off | 00000000:00:07.0 Off | 0 |. | N/A 28C P0 35W / 250W | 1MiB / 16384MiB | 0% Default |. | | | N/A |. +-----------------------------------------+------------------------+----------------------+. . +-----------------------------------------------------------------------------------------+. | Processes: |. | GPU GI CI PID Type Process name GPU Memory |. | ID ID Usage |. |=========================================================================================|. | No running processes found |. +-----------------------------------------------------------------------------------------+. ```. Should I update the driver, cuda or both? Also, installed on the server is cuda version 12.3, so I'm feeling a bit confused as to where the 12.5 comes from. ```. nvcc --version. nvcc: NVIDIA (R) Cuda compiler driver. Copyright (c) 2005-2023 NVIDIA Corporation. Built on Wed_Nov_22_10:17:15_PST_2023. Cuda compilation tools, release 12.3, V12.3.107. Build cuda_12.3.r12.3/compiler.33567101_0. ```. It's a right mess! Sometimes call_variants uses gpu and at other times it stalls. That's why I'm now running the commands separately, I have a lot of outputs from make_examples. Our IT support say they're happy the gpu works part of the time :) It's adding a lot of extra work, and I'm trying to come up with a solution",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/849
https://github.com/google/deepvariant/issues/849:1089,energy efficiency,GPU,GPU,1089,"sing cuda version 11.3.1, but the nvidia driver (version 555.42.02) on the server is using cuda version 12.5. ```. nvidia-smi. +-----------------------------------------------------------------------------------------+. | NVIDIA-SMI 555.42.02 Driver Version: 555.42.02 CUDA Version: 12.5 |. |-----------------------------------------+------------------------+----------------------+. | GPU Name Persistence-M | Bus-Id Disp.A | Volatile Uncorr. ECC |. | Fan Temp Perf Pwr:Usage/Cap | Memory-Usage | GPU-Util Compute M. |. | | | MIG M. |. |=========================================+========================+======================|. | 0 Tesla V100-PCIE-16GB Off | 00000000:00:07.0 Off | 0 |. | N/A 28C P0 35W / 250W | 1MiB / 16384MiB | 0% Default |. | | | N/A |. +-----------------------------------------+------------------------+----------------------+. . +-----------------------------------------------------------------------------------------+. | Processes: |. | GPU GI CI PID Type Process name GPU Memory |. | ID ID Usage |. |=========================================================================================|. | No running processes found |. +-----------------------------------------------------------------------------------------+. ```. Should I update the driver, cuda or both? Also, installed on the server is cuda version 12.3, so I'm feeling a bit confused as to where the 12.5 comes from. ```. nvcc --version. nvcc: NVIDIA (R) Cuda compiler driver. Copyright (c) 2005-2023 NVIDIA Corporation. Built on Wed_Nov_22_10:17:15_PST_2023. Cuda compilation tools, release 12.3, V12.3.107. Build cuda_12.3.r12.3/compiler.33567101_0. ```. It's a right mess! Sometimes call_variants uses gpu and at other times it stalls. That's why I'm now running the commands separately, I have a lot of outputs from make_examples. Our IT support say they're happy the gpu works part of the time :) It's adding a lot of extra work, and I'm trying to come up with a solution. Since it's completely unrelate",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/849
https://github.com/google/deepvariant/issues/849:1788,energy efficiency,gpu,gpu,1788,".42.02) on the server is using cuda version 12.5. ```. nvidia-smi. +-----------------------------------------------------------------------------------------+. | NVIDIA-SMI 555.42.02 Driver Version: 555.42.02 CUDA Version: 12.5 |. |-----------------------------------------+------------------------+----------------------+. | GPU Name Persistence-M | Bus-Id Disp.A | Volatile Uncorr. ECC |. | Fan Temp Perf Pwr:Usage/Cap | Memory-Usage | GPU-Util Compute M. |. | | | MIG M. |. |=========================================+========================+======================|. | 0 Tesla V100-PCIE-16GB Off | 00000000:00:07.0 Off | 0 |. | N/A 28C P0 35W / 250W | 1MiB / 16384MiB | 0% Default |. | | | N/A |. +-----------------------------------------+------------------------+----------------------+. . +-----------------------------------------------------------------------------------------+. | Processes: |. | GPU GI CI PID Type Process name GPU Memory |. | ID ID Usage |. |=========================================================================================|. | No running processes found |. +-----------------------------------------------------------------------------------------+. ```. Should I update the driver, cuda or both? Also, installed on the server is cuda version 12.3, so I'm feeling a bit confused as to where the 12.5 comes from. ```. nvcc --version. nvcc: NVIDIA (R) Cuda compiler driver. Copyright (c) 2005-2023 NVIDIA Corporation. Built on Wed_Nov_22_10:17:15_PST_2023. Cuda compilation tools, release 12.3, V12.3.107. Build cuda_12.3.r12.3/compiler.33567101_0. ```. It's a right mess! Sometimes call_variants uses gpu and at other times it stalls. That's why I'm now running the commands separately, I have a lot of outputs from make_examples. Our IT support say they're happy the gpu works part of the time :) It's adding a lot of extra work, and I'm trying to come up with a solution. Since it's completely unrelated to the topic, perhaps I should create a new issue instead?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/849
https://github.com/google/deepvariant/issues/849:1955,energy efficiency,gpu,gpu,1955,".42.02) on the server is using cuda version 12.5. ```. nvidia-smi. +-----------------------------------------------------------------------------------------+. | NVIDIA-SMI 555.42.02 Driver Version: 555.42.02 CUDA Version: 12.5 |. |-----------------------------------------+------------------------+----------------------+. | GPU Name Persistence-M | Bus-Id Disp.A | Volatile Uncorr. ECC |. | Fan Temp Perf Pwr:Usage/Cap | Memory-Usage | GPU-Util Compute M. |. | | | MIG M. |. |=========================================+========================+======================|. | 0 Tesla V100-PCIE-16GB Off | 00000000:00:07.0 Off | 0 |. | N/A 28C P0 35W / 250W | 1MiB / 16384MiB | 0% Default |. | | | N/A |. +-----------------------------------------+------------------------+----------------------+. . +-----------------------------------------------------------------------------------------+. | Processes: |. | GPU GI CI PID Type Process name GPU Memory |. | ID ID Usage |. |=========================================================================================|. | No running processes found |. +-----------------------------------------------------------------------------------------+. ```. Should I update the driver, cuda or both? Also, installed on the server is cuda version 12.3, so I'm feeling a bit confused as to where the 12.5 comes from. ```. nvcc --version. nvcc: NVIDIA (R) Cuda compiler driver. Copyright (c) 2005-2023 NVIDIA Corporation. Built on Wed_Nov_22_10:17:15_PST_2023. Cuda compilation tools, release 12.3, V12.3.107. Build cuda_12.3.r12.3/compiler.33567101_0. ```. It's a right mess! Sometimes call_variants uses gpu and at other times it stalls. That's why I'm now running the commands separately, I have a lot of outputs from make_examples. Our IT support say they're happy the gpu works part of the time :) It's adding a lot of extra work, and I'm trying to come up with a solution. Since it's completely unrelated to the topic, perhaps I should create a new issue instead?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/849
https://github.com/google/deepvariant/issues/849:101,integrability,version,version,101,"Oh, and since we're here, I mentioned problems with the GPU. It seems that DeepVariant is using cuda version 11.3.1, but the nvidia driver (version 555.42.02) on the server is using cuda version 12.5. ```. nvidia-smi. +-----------------------------------------------------------------------------------------+. | NVIDIA-SMI 555.42.02 Driver Version: 555.42.02 CUDA Version: 12.5 |. |-----------------------------------------+------------------------+----------------------+. | GPU Name Persistence-M | Bus-Id Disp.A | Volatile Uncorr. ECC |. | Fan Temp Perf Pwr:Usage/Cap | Memory-Usage | GPU-Util Compute M. |. | | | MIG M. |. |=========================================+========================+======================|. | 0 Tesla V100-PCIE-16GB Off | 00000000:00:07.0 Off | 0 |. | N/A 28C P0 35W / 250W | 1MiB / 16384MiB | 0% Default |. | | | N/A |. +-----------------------------------------+------------------------+----------------------+. . +-----------------------------------------------------------------------------------------+. | Processes: |. | GPU GI CI PID Type Process name GPU Memory |. | ID ID Usage |. |=========================================================================================|. | No running processes found |. +-----------------------------------------------------------------------------------------+. ```. Should I update the driver, cuda or both? Also, installed on the server is cuda version 12.3, so I'm feeling a bit confused as to where the 12.5 comes from. ```. nvcc --version. nvcc: NVIDIA (R) Cuda compiler driver. Copyright (c) 2005-2023 NVIDIA Corporation. Built on Wed_Nov_22_10:17:15_PST_2023. Cuda compilation tools, release 12.3, V12.3.107. Build cuda_12.3.r12.3/compiler.33567101_0. ```. It's a right mess! Sometimes call_variants uses gpu and at other times it stalls. That's why I'm now running the commands separately, I have a lot of outputs from make_examples. Our IT support say they're happy the gpu works part of the time :) It's adding a l",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/849
https://github.com/google/deepvariant/issues/849:140,integrability,version,version,140,"Oh, and since we're here, I mentioned problems with the GPU. It seems that DeepVariant is using cuda version 11.3.1, but the nvidia driver (version 555.42.02) on the server is using cuda version 12.5. ```. nvidia-smi. +-----------------------------------------------------------------------------------------+. | NVIDIA-SMI 555.42.02 Driver Version: 555.42.02 CUDA Version: 12.5 |. |-----------------------------------------+------------------------+----------------------+. | GPU Name Persistence-M | Bus-Id Disp.A | Volatile Uncorr. ECC |. | Fan Temp Perf Pwr:Usage/Cap | Memory-Usage | GPU-Util Compute M. |. | | | MIG M. |. |=========================================+========================+======================|. | 0 Tesla V100-PCIE-16GB Off | 00000000:00:07.0 Off | 0 |. | N/A 28C P0 35W / 250W | 1MiB / 16384MiB | 0% Default |. | | | N/A |. +-----------------------------------------+------------------------+----------------------+. . +-----------------------------------------------------------------------------------------+. | Processes: |. | GPU GI CI PID Type Process name GPU Memory |. | ID ID Usage |. |=========================================================================================|. | No running processes found |. +-----------------------------------------------------------------------------------------+. ```. Should I update the driver, cuda or both? Also, installed on the server is cuda version 12.3, so I'm feeling a bit confused as to where the 12.5 comes from. ```. nvcc --version. nvcc: NVIDIA (R) Cuda compiler driver. Copyright (c) 2005-2023 NVIDIA Corporation. Built on Wed_Nov_22_10:17:15_PST_2023. Cuda compilation tools, release 12.3, V12.3.107. Build cuda_12.3.r12.3/compiler.33567101_0. ```. It's a right mess! Sometimes call_variants uses gpu and at other times it stalls. That's why I'm now running the commands separately, I have a lot of outputs from make_examples. Our IT support say they're happy the gpu works part of the time :) It's adding a l",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/849
https://github.com/google/deepvariant/issues/849:187,integrability,version,version,187,"Oh, and since we're here, I mentioned problems with the GPU. It seems that DeepVariant is using cuda version 11.3.1, but the nvidia driver (version 555.42.02) on the server is using cuda version 12.5. ```. nvidia-smi. +-----------------------------------------------------------------------------------------+. | NVIDIA-SMI 555.42.02 Driver Version: 555.42.02 CUDA Version: 12.5 |. |-----------------------------------------+------------------------+----------------------+. | GPU Name Persistence-M | Bus-Id Disp.A | Volatile Uncorr. ECC |. | Fan Temp Perf Pwr:Usage/Cap | Memory-Usage | GPU-Util Compute M. |. | | | MIG M. |. |=========================================+========================+======================|. | 0 Tesla V100-PCIE-16GB Off | 00000000:00:07.0 Off | 0 |. | N/A 28C P0 35W / 250W | 1MiB / 16384MiB | 0% Default |. | | | N/A |. +-----------------------------------------+------------------------+----------------------+. . +-----------------------------------------------------------------------------------------+. | Processes: |. | GPU GI CI PID Type Process name GPU Memory |. | ID ID Usage |. |=========================================================================================|. | No running processes found |. +-----------------------------------------------------------------------------------------+. ```. Should I update the driver, cuda or both? Also, installed on the server is cuda version 12.3, so I'm feeling a bit confused as to where the 12.5 comes from. ```. nvcc --version. nvcc: NVIDIA (R) Cuda compiler driver. Copyright (c) 2005-2023 NVIDIA Corporation. Built on Wed_Nov_22_10:17:15_PST_2023. Cuda compilation tools, release 12.3, V12.3.107. Build cuda_12.3.r12.3/compiler.33567101_0. ```. It's a right mess! Sometimes call_variants uses gpu and at other times it stalls. That's why I'm now running the commands separately, I have a lot of outputs from make_examples. Our IT support say they're happy the gpu works part of the time :) It's adding a l",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/849
https://github.com/google/deepvariant/issues/849:341,integrability,Version,Version,341,"Oh, and since we're here, I mentioned problems with the GPU. It seems that DeepVariant is using cuda version 11.3.1, but the nvidia driver (version 555.42.02) on the server is using cuda version 12.5. ```. nvidia-smi. +-----------------------------------------------------------------------------------------+. | NVIDIA-SMI 555.42.02 Driver Version: 555.42.02 CUDA Version: 12.5 |. |-----------------------------------------+------------------------+----------------------+. | GPU Name Persistence-M | Bus-Id Disp.A | Volatile Uncorr. ECC |. | Fan Temp Perf Pwr:Usage/Cap | Memory-Usage | GPU-Util Compute M. |. | | | MIG M. |. |=========================================+========================+======================|. | 0 Tesla V100-PCIE-16GB Off | 00000000:00:07.0 Off | 0 |. | N/A 28C P0 35W / 250W | 1MiB / 16384MiB | 0% Default |. | | | N/A |. +-----------------------------------------+------------------------+----------------------+. . +-----------------------------------------------------------------------------------------+. | Processes: |. | GPU GI CI PID Type Process name GPU Memory |. | ID ID Usage |. |=========================================================================================|. | No running processes found |. +-----------------------------------------------------------------------------------------+. ```. Should I update the driver, cuda or both? Also, installed on the server is cuda version 12.3, so I'm feeling a bit confused as to where the 12.5 comes from. ```. nvcc --version. nvcc: NVIDIA (R) Cuda compiler driver. Copyright (c) 2005-2023 NVIDIA Corporation. Built on Wed_Nov_22_10:17:15_PST_2023. Cuda compilation tools, release 12.3, V12.3.107. Build cuda_12.3.r12.3/compiler.33567101_0. ```. It's a right mess! Sometimes call_variants uses gpu and at other times it stalls. That's why I'm now running the commands separately, I have a lot of outputs from make_examples. Our IT support say they're happy the gpu works part of the time :) It's adding a l",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/849
https://github.com/google/deepvariant/issues/849:365,integrability,Version,Version,365,"Oh, and since we're here, I mentioned problems with the GPU. It seems that DeepVariant is using cuda version 11.3.1, but the nvidia driver (version 555.42.02) on the server is using cuda version 12.5. ```. nvidia-smi. +-----------------------------------------------------------------------------------------+. | NVIDIA-SMI 555.42.02 Driver Version: 555.42.02 CUDA Version: 12.5 |. |-----------------------------------------+------------------------+----------------------+. | GPU Name Persistence-M | Bus-Id Disp.A | Volatile Uncorr. ECC |. | Fan Temp Perf Pwr:Usage/Cap | Memory-Usage | GPU-Util Compute M. |. | | | MIG M. |. |=========================================+========================+======================|. | 0 Tesla V100-PCIE-16GB Off | 00000000:00:07.0 Off | 0 |. | N/A 28C P0 35W / 250W | 1MiB / 16384MiB | 0% Default |. | | | N/A |. +-----------------------------------------+------------------------+----------------------+. . +-----------------------------------------------------------------------------------------+. | Processes: |. | GPU GI CI PID Type Process name GPU Memory |. | ID ID Usage |. |=========================================================================================|. | No running processes found |. +-----------------------------------------------------------------------------------------+. ```. Should I update the driver, cuda or both? Also, installed on the server is cuda version 12.3, so I'm feeling a bit confused as to where the 12.5 comes from. ```. nvcc --version. nvcc: NVIDIA (R) Cuda compiler driver. Copyright (c) 2005-2023 NVIDIA Corporation. Built on Wed_Nov_22_10:17:15_PST_2023. Cuda compilation tools, release 12.3, V12.3.107. Build cuda_12.3.r12.3/compiler.33567101_0. ```. It's a right mess! Sometimes call_variants uses gpu and at other times it stalls. That's why I'm now running the commands separately, I have a lot of outputs from make_examples. Our IT support say they're happy the gpu works part of the time :) It's adding a l",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/849
https://github.com/google/deepvariant/issues/849:1423,integrability,version,version,1423,".42.02) on the server is using cuda version 12.5. ```. nvidia-smi. +-----------------------------------------------------------------------------------------+. | NVIDIA-SMI 555.42.02 Driver Version: 555.42.02 CUDA Version: 12.5 |. |-----------------------------------------+------------------------+----------------------+. | GPU Name Persistence-M | Bus-Id Disp.A | Volatile Uncorr. ECC |. | Fan Temp Perf Pwr:Usage/Cap | Memory-Usage | GPU-Util Compute M. |. | | | MIG M. |. |=========================================+========================+======================|. | 0 Tesla V100-PCIE-16GB Off | 00000000:00:07.0 Off | 0 |. | N/A 28C P0 35W / 250W | 1MiB / 16384MiB | 0% Default |. | | | N/A |. +-----------------------------------------+------------------------+----------------------+. . +-----------------------------------------------------------------------------------------+. | Processes: |. | GPU GI CI PID Type Process name GPU Memory |. | ID ID Usage |. |=========================================================================================|. | No running processes found |. +-----------------------------------------------------------------------------------------+. ```. Should I update the driver, cuda or both? Also, installed on the server is cuda version 12.3, so I'm feeling a bit confused as to where the 12.5 comes from. ```. nvcc --version. nvcc: NVIDIA (R) Cuda compiler driver. Copyright (c) 2005-2023 NVIDIA Corporation. Built on Wed_Nov_22_10:17:15_PST_2023. Cuda compilation tools, release 12.3, V12.3.107. Build cuda_12.3.r12.3/compiler.33567101_0. ```. It's a right mess! Sometimes call_variants uses gpu and at other times it stalls. That's why I'm now running the commands separately, I have a lot of outputs from make_examples. Our IT support say they're happy the gpu works part of the time :) It's adding a lot of extra work, and I'm trying to come up with a solution. Since it's completely unrelated to the topic, perhaps I should create a new issue instead?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/849
https://github.com/google/deepvariant/issues/849:1512,integrability,version,version,1512,".42.02) on the server is using cuda version 12.5. ```. nvidia-smi. +-----------------------------------------------------------------------------------------+. | NVIDIA-SMI 555.42.02 Driver Version: 555.42.02 CUDA Version: 12.5 |. |-----------------------------------------+------------------------+----------------------+. | GPU Name Persistence-M | Bus-Id Disp.A | Volatile Uncorr. ECC |. | Fan Temp Perf Pwr:Usage/Cap | Memory-Usage | GPU-Util Compute M. |. | | | MIG M. |. |=========================================+========================+======================|. | 0 Tesla V100-PCIE-16GB Off | 00000000:00:07.0 Off | 0 |. | N/A 28C P0 35W / 250W | 1MiB / 16384MiB | 0% Default |. | | | N/A |. +-----------------------------------------+------------------------+----------------------+. . +-----------------------------------------------------------------------------------------+. | Processes: |. | GPU GI CI PID Type Process name GPU Memory |. | ID ID Usage |. |=========================================================================================|. | No running processes found |. +-----------------------------------------------------------------------------------------+. ```. Should I update the driver, cuda or both? Also, installed on the server is cuda version 12.3, so I'm feeling a bit confused as to where the 12.5 comes from. ```. nvcc --version. nvcc: NVIDIA (R) Cuda compiler driver. Copyright (c) 2005-2023 NVIDIA Corporation. Built on Wed_Nov_22_10:17:15_PST_2023. Cuda compilation tools, release 12.3, V12.3.107. Build cuda_12.3.r12.3/compiler.33567101_0. ```. It's a right mess! Sometimes call_variants uses gpu and at other times it stalls. That's why I'm now running the commands separately, I have a lot of outputs from make_examples. Our IT support say they're happy the gpu works part of the time :) It's adding a lot of extra work, and I'm trying to come up with a solution. Since it's completely unrelated to the topic, perhaps I should create a new issue instead?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/849
https://github.com/google/deepvariant/issues/849:2100,integrability,topic,topic,2100,".42.02) on the server is using cuda version 12.5. ```. nvidia-smi. +-----------------------------------------------------------------------------------------+. | NVIDIA-SMI 555.42.02 Driver Version: 555.42.02 CUDA Version: 12.5 |. |-----------------------------------------+------------------------+----------------------+. | GPU Name Persistence-M | Bus-Id Disp.A | Volatile Uncorr. ECC |. | Fan Temp Perf Pwr:Usage/Cap | Memory-Usage | GPU-Util Compute M. |. | | | MIG M. |. |=========================================+========================+======================|. | 0 Tesla V100-PCIE-16GB Off | 00000000:00:07.0 Off | 0 |. | N/A 28C P0 35W / 250W | 1MiB / 16384MiB | 0% Default |. | | | N/A |. +-----------------------------------------+------------------------+----------------------+. . +-----------------------------------------------------------------------------------------+. | Processes: |. | GPU GI CI PID Type Process name GPU Memory |. | ID ID Usage |. |=========================================================================================|. | No running processes found |. +-----------------------------------------------------------------------------------------+. ```. Should I update the driver, cuda or both? Also, installed on the server is cuda version 12.3, so I'm feeling a bit confused as to where the 12.5 comes from. ```. nvcc --version. nvcc: NVIDIA (R) Cuda compiler driver. Copyright (c) 2005-2023 NVIDIA Corporation. Built on Wed_Nov_22_10:17:15_PST_2023. Cuda compilation tools, release 12.3, V12.3.107. Build cuda_12.3.r12.3/compiler.33567101_0. ```. It's a right mess! Sometimes call_variants uses gpu and at other times it stalls. That's why I'm now running the commands separately, I have a lot of outputs from make_examples. Our IT support say they're happy the gpu works part of the time :) It's adding a lot of extra work, and I'm trying to come up with a solution. Since it's completely unrelated to the topic, perhaps I should create a new issue instead?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/849
https://github.com/google/deepvariant/issues/849:101,modifiability,version,version,101,"Oh, and since we're here, I mentioned problems with the GPU. It seems that DeepVariant is using cuda version 11.3.1, but the nvidia driver (version 555.42.02) on the server is using cuda version 12.5. ```. nvidia-smi. +-----------------------------------------------------------------------------------------+. | NVIDIA-SMI 555.42.02 Driver Version: 555.42.02 CUDA Version: 12.5 |. |-----------------------------------------+------------------------+----------------------+. | GPU Name Persistence-M | Bus-Id Disp.A | Volatile Uncorr. ECC |. | Fan Temp Perf Pwr:Usage/Cap | Memory-Usage | GPU-Util Compute M. |. | | | MIG M. |. |=========================================+========================+======================|. | 0 Tesla V100-PCIE-16GB Off | 00000000:00:07.0 Off | 0 |. | N/A 28C P0 35W / 250W | 1MiB / 16384MiB | 0% Default |. | | | N/A |. +-----------------------------------------+------------------------+----------------------+. . +-----------------------------------------------------------------------------------------+. | Processes: |. | GPU GI CI PID Type Process name GPU Memory |. | ID ID Usage |. |=========================================================================================|. | No running processes found |. +-----------------------------------------------------------------------------------------+. ```. Should I update the driver, cuda or both? Also, installed on the server is cuda version 12.3, so I'm feeling a bit confused as to where the 12.5 comes from. ```. nvcc --version. nvcc: NVIDIA (R) Cuda compiler driver. Copyright (c) 2005-2023 NVIDIA Corporation. Built on Wed_Nov_22_10:17:15_PST_2023. Cuda compilation tools, release 12.3, V12.3.107. Build cuda_12.3.r12.3/compiler.33567101_0. ```. It's a right mess! Sometimes call_variants uses gpu and at other times it stalls. That's why I'm now running the commands separately, I have a lot of outputs from make_examples. Our IT support say they're happy the gpu works part of the time :) It's adding a l",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/849
https://github.com/google/deepvariant/issues/849:140,modifiability,version,version,140,"Oh, and since we're here, I mentioned problems with the GPU. It seems that DeepVariant is using cuda version 11.3.1, but the nvidia driver (version 555.42.02) on the server is using cuda version 12.5. ```. nvidia-smi. +-----------------------------------------------------------------------------------------+. | NVIDIA-SMI 555.42.02 Driver Version: 555.42.02 CUDA Version: 12.5 |. |-----------------------------------------+------------------------+----------------------+. | GPU Name Persistence-M | Bus-Id Disp.A | Volatile Uncorr. ECC |. | Fan Temp Perf Pwr:Usage/Cap | Memory-Usage | GPU-Util Compute M. |. | | | MIG M. |. |=========================================+========================+======================|. | 0 Tesla V100-PCIE-16GB Off | 00000000:00:07.0 Off | 0 |. | N/A 28C P0 35W / 250W | 1MiB / 16384MiB | 0% Default |. | | | N/A |. +-----------------------------------------+------------------------+----------------------+. . +-----------------------------------------------------------------------------------------+. | Processes: |. | GPU GI CI PID Type Process name GPU Memory |. | ID ID Usage |. |=========================================================================================|. | No running processes found |. +-----------------------------------------------------------------------------------------+. ```. Should I update the driver, cuda or both? Also, installed on the server is cuda version 12.3, so I'm feeling a bit confused as to where the 12.5 comes from. ```. nvcc --version. nvcc: NVIDIA (R) Cuda compiler driver. Copyright (c) 2005-2023 NVIDIA Corporation. Built on Wed_Nov_22_10:17:15_PST_2023. Cuda compilation tools, release 12.3, V12.3.107. Build cuda_12.3.r12.3/compiler.33567101_0. ```. It's a right mess! Sometimes call_variants uses gpu and at other times it stalls. That's why I'm now running the commands separately, I have a lot of outputs from make_examples. Our IT support say they're happy the gpu works part of the time :) It's adding a l",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/849
https://github.com/google/deepvariant/issues/849:187,modifiability,version,version,187,"Oh, and since we're here, I mentioned problems with the GPU. It seems that DeepVariant is using cuda version 11.3.1, but the nvidia driver (version 555.42.02) on the server is using cuda version 12.5. ```. nvidia-smi. +-----------------------------------------------------------------------------------------+. | NVIDIA-SMI 555.42.02 Driver Version: 555.42.02 CUDA Version: 12.5 |. |-----------------------------------------+------------------------+----------------------+. | GPU Name Persistence-M | Bus-Id Disp.A | Volatile Uncorr. ECC |. | Fan Temp Perf Pwr:Usage/Cap | Memory-Usage | GPU-Util Compute M. |. | | | MIG M. |. |=========================================+========================+======================|. | 0 Tesla V100-PCIE-16GB Off | 00000000:00:07.0 Off | 0 |. | N/A 28C P0 35W / 250W | 1MiB / 16384MiB | 0% Default |. | | | N/A |. +-----------------------------------------+------------------------+----------------------+. . +-----------------------------------------------------------------------------------------+. | Processes: |. | GPU GI CI PID Type Process name GPU Memory |. | ID ID Usage |. |=========================================================================================|. | No running processes found |. +-----------------------------------------------------------------------------------------+. ```. Should I update the driver, cuda or both? Also, installed on the server is cuda version 12.3, so I'm feeling a bit confused as to where the 12.5 comes from. ```. nvcc --version. nvcc: NVIDIA (R) Cuda compiler driver. Copyright (c) 2005-2023 NVIDIA Corporation. Built on Wed_Nov_22_10:17:15_PST_2023. Cuda compilation tools, release 12.3, V12.3.107. Build cuda_12.3.r12.3/compiler.33567101_0. ```. It's a right mess! Sometimes call_variants uses gpu and at other times it stalls. That's why I'm now running the commands separately, I have a lot of outputs from make_examples. Our IT support say they're happy the gpu works part of the time :) It's adding a l",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/849
https://github.com/google/deepvariant/issues/849:341,modifiability,Version,Version,341,"Oh, and since we're here, I mentioned problems with the GPU. It seems that DeepVariant is using cuda version 11.3.1, but the nvidia driver (version 555.42.02) on the server is using cuda version 12.5. ```. nvidia-smi. +-----------------------------------------------------------------------------------------+. | NVIDIA-SMI 555.42.02 Driver Version: 555.42.02 CUDA Version: 12.5 |. |-----------------------------------------+------------------------+----------------------+. | GPU Name Persistence-M | Bus-Id Disp.A | Volatile Uncorr. ECC |. | Fan Temp Perf Pwr:Usage/Cap | Memory-Usage | GPU-Util Compute M. |. | | | MIG M. |. |=========================================+========================+======================|. | 0 Tesla V100-PCIE-16GB Off | 00000000:00:07.0 Off | 0 |. | N/A 28C P0 35W / 250W | 1MiB / 16384MiB | 0% Default |. | | | N/A |. +-----------------------------------------+------------------------+----------------------+. . +-----------------------------------------------------------------------------------------+. | Processes: |. | GPU GI CI PID Type Process name GPU Memory |. | ID ID Usage |. |=========================================================================================|. | No running processes found |. +-----------------------------------------------------------------------------------------+. ```. Should I update the driver, cuda or both? Also, installed on the server is cuda version 12.3, so I'm feeling a bit confused as to where the 12.5 comes from. ```. nvcc --version. nvcc: NVIDIA (R) Cuda compiler driver. Copyright (c) 2005-2023 NVIDIA Corporation. Built on Wed_Nov_22_10:17:15_PST_2023. Cuda compilation tools, release 12.3, V12.3.107. Build cuda_12.3.r12.3/compiler.33567101_0. ```. It's a right mess! Sometimes call_variants uses gpu and at other times it stalls. That's why I'm now running the commands separately, I have a lot of outputs from make_examples. Our IT support say they're happy the gpu works part of the time :) It's adding a l",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/849
https://github.com/google/deepvariant/issues/849:365,modifiability,Version,Version,365,"Oh, and since we're here, I mentioned problems with the GPU. It seems that DeepVariant is using cuda version 11.3.1, but the nvidia driver (version 555.42.02) on the server is using cuda version 12.5. ```. nvidia-smi. +-----------------------------------------------------------------------------------------+. | NVIDIA-SMI 555.42.02 Driver Version: 555.42.02 CUDA Version: 12.5 |. |-----------------------------------------+------------------------+----------------------+. | GPU Name Persistence-M | Bus-Id Disp.A | Volatile Uncorr. ECC |. | Fan Temp Perf Pwr:Usage/Cap | Memory-Usage | GPU-Util Compute M. |. | | | MIG M. |. |=========================================+========================+======================|. | 0 Tesla V100-PCIE-16GB Off | 00000000:00:07.0 Off | 0 |. | N/A 28C P0 35W / 250W | 1MiB / 16384MiB | 0% Default |. | | | N/A |. +-----------------------------------------+------------------------+----------------------+. . +-----------------------------------------------------------------------------------------+. | Processes: |. | GPU GI CI PID Type Process name GPU Memory |. | ID ID Usage |. |=========================================================================================|. | No running processes found |. +-----------------------------------------------------------------------------------------+. ```. Should I update the driver, cuda or both? Also, installed on the server is cuda version 12.3, so I'm feeling a bit confused as to where the 12.5 comes from. ```. nvcc --version. nvcc: NVIDIA (R) Cuda compiler driver. Copyright (c) 2005-2023 NVIDIA Corporation. Built on Wed_Nov_22_10:17:15_PST_2023. Cuda compilation tools, release 12.3, V12.3.107. Build cuda_12.3.r12.3/compiler.33567101_0. ```. It's a right mess! Sometimes call_variants uses gpu and at other times it stalls. That's why I'm now running the commands separately, I have a lot of outputs from make_examples. Our IT support say they're happy the gpu works part of the time :) It's adding a l",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/849
https://github.com/google/deepvariant/issues/849:1423,modifiability,version,version,1423,".42.02) on the server is using cuda version 12.5. ```. nvidia-smi. +-----------------------------------------------------------------------------------------+. | NVIDIA-SMI 555.42.02 Driver Version: 555.42.02 CUDA Version: 12.5 |. |-----------------------------------------+------------------------+----------------------+. | GPU Name Persistence-M | Bus-Id Disp.A | Volatile Uncorr. ECC |. | Fan Temp Perf Pwr:Usage/Cap | Memory-Usage | GPU-Util Compute M. |. | | | MIG M. |. |=========================================+========================+======================|. | 0 Tesla V100-PCIE-16GB Off | 00000000:00:07.0 Off | 0 |. | N/A 28C P0 35W / 250W | 1MiB / 16384MiB | 0% Default |. | | | N/A |. +-----------------------------------------+------------------------+----------------------+. . +-----------------------------------------------------------------------------------------+. | Processes: |. | GPU GI CI PID Type Process name GPU Memory |. | ID ID Usage |. |=========================================================================================|. | No running processes found |. +-----------------------------------------------------------------------------------------+. ```. Should I update the driver, cuda or both? Also, installed on the server is cuda version 12.3, so I'm feeling a bit confused as to where the 12.5 comes from. ```. nvcc --version. nvcc: NVIDIA (R) Cuda compiler driver. Copyright (c) 2005-2023 NVIDIA Corporation. Built on Wed_Nov_22_10:17:15_PST_2023. Cuda compilation tools, release 12.3, V12.3.107. Build cuda_12.3.r12.3/compiler.33567101_0. ```. It's a right mess! Sometimes call_variants uses gpu and at other times it stalls. That's why I'm now running the commands separately, I have a lot of outputs from make_examples. Our IT support say they're happy the gpu works part of the time :) It's adding a lot of extra work, and I'm trying to come up with a solution. Since it's completely unrelated to the topic, perhaps I should create a new issue instead?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/849
https://github.com/google/deepvariant/issues/849:1512,modifiability,version,version,1512,".42.02) on the server is using cuda version 12.5. ```. nvidia-smi. +-----------------------------------------------------------------------------------------+. | NVIDIA-SMI 555.42.02 Driver Version: 555.42.02 CUDA Version: 12.5 |. |-----------------------------------------+------------------------+----------------------+. | GPU Name Persistence-M | Bus-Id Disp.A | Volatile Uncorr. ECC |. | Fan Temp Perf Pwr:Usage/Cap | Memory-Usage | GPU-Util Compute M. |. | | | MIG M. |. |=========================================+========================+======================|. | 0 Tesla V100-PCIE-16GB Off | 00000000:00:07.0 Off | 0 |. | N/A 28C P0 35W / 250W | 1MiB / 16384MiB | 0% Default |. | | | N/A |. +-----------------------------------------+------------------------+----------------------+. . +-----------------------------------------------------------------------------------------+. | Processes: |. | GPU GI CI PID Type Process name GPU Memory |. | ID ID Usage |. |=========================================================================================|. | No running processes found |. +-----------------------------------------------------------------------------------------+. ```. Should I update the driver, cuda or both? Also, installed on the server is cuda version 12.3, so I'm feeling a bit confused as to where the 12.5 comes from. ```. nvcc --version. nvcc: NVIDIA (R) Cuda compiler driver. Copyright (c) 2005-2023 NVIDIA Corporation. Built on Wed_Nov_22_10:17:15_PST_2023. Cuda compilation tools, release 12.3, V12.3.107. Build cuda_12.3.r12.3/compiler.33567101_0. ```. It's a right mess! Sometimes call_variants uses gpu and at other times it stalls. That's why I'm now running the commands separately, I have a lot of outputs from make_examples. Our IT support say they're happy the gpu works part of the time :) It's adding a lot of extra work, and I'm trying to come up with a solution. Since it's completely unrelated to the topic, perhaps I should create a new issue instead?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/849
https://github.com/google/deepvariant/issues/849:56,performance,GPU,GPU,56,"Oh, and since we're here, I mentioned problems with the GPU. It seems that DeepVariant is using cuda version 11.3.1, but the nvidia driver (version 555.42.02) on the server is using cuda version 12.5. ```. nvidia-smi. +-----------------------------------------------------------------------------------------+. | NVIDIA-SMI 555.42.02 Driver Version: 555.42.02 CUDA Version: 12.5 |. |-----------------------------------------+------------------------+----------------------+. | GPU Name Persistence-M | Bus-Id Disp.A | Volatile Uncorr. ECC |. | Fan Temp Perf Pwr:Usage/Cap | Memory-Usage | GPU-Util Compute M. |. | | | MIG M. |. |=========================================+========================+======================|. | 0 Tesla V100-PCIE-16GB Off | 00000000:00:07.0 Off | 0 |. | N/A 28C P0 35W / 250W | 1MiB / 16384MiB | 0% Default |. | | | N/A |. +-----------------------------------------+------------------------+----------------------+. . +-----------------------------------------------------------------------------------------+. | Processes: |. | GPU GI CI PID Type Process name GPU Memory |. | ID ID Usage |. |=========================================================================================|. | No running processes found |. +-----------------------------------------------------------------------------------------+. ```. Should I update the driver, cuda or both? Also, installed on the server is cuda version 12.3, so I'm feeling a bit confused as to where the 12.5 comes from. ```. nvcc --version. nvcc: NVIDIA (R) Cuda compiler driver. Copyright (c) 2005-2023 NVIDIA Corporation. Built on Wed_Nov_22_10:17:15_PST_2023. Cuda compilation tools, release 12.3, V12.3.107. Build cuda_12.3.r12.3/compiler.33567101_0. ```. It's a right mess! Sometimes call_variants uses gpu and at other times it stalls. That's why I'm now running the commands separately, I have a lot of outputs from make_examples. Our IT support say they're happy the gpu works part of the time :) It's adding a l",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/849
https://github.com/google/deepvariant/issues/849:477,performance,GPU,GPU,477,"Oh, and since we're here, I mentioned problems with the GPU. It seems that DeepVariant is using cuda version 11.3.1, but the nvidia driver (version 555.42.02) on the server is using cuda version 12.5. ```. nvidia-smi. +-----------------------------------------------------------------------------------------+. | NVIDIA-SMI 555.42.02 Driver Version: 555.42.02 CUDA Version: 12.5 |. |-----------------------------------------+------------------------+----------------------+. | GPU Name Persistence-M | Bus-Id Disp.A | Volatile Uncorr. ECC |. | Fan Temp Perf Pwr:Usage/Cap | Memory-Usage | GPU-Util Compute M. |. | | | MIG M. |. |=========================================+========================+======================|. | 0 Tesla V100-PCIE-16GB Off | 00000000:00:07.0 Off | 0 |. | N/A 28C P0 35W / 250W | 1MiB / 16384MiB | 0% Default |. | | | N/A |. +-----------------------------------------+------------------------+----------------------+. . +-----------------------------------------------------------------------------------------+. | Processes: |. | GPU GI CI PID Type Process name GPU Memory |. | ID ID Usage |. |=========================================================================================|. | No running processes found |. +-----------------------------------------------------------------------------------------+. ```. Should I update the driver, cuda or both? Also, installed on the server is cuda version 12.3, so I'm feeling a bit confused as to where the 12.5 comes from. ```. nvcc --version. nvcc: NVIDIA (R) Cuda compiler driver. Copyright (c) 2005-2023 NVIDIA Corporation. Built on Wed_Nov_22_10:17:15_PST_2023. Cuda compilation tools, release 12.3, V12.3.107. Build cuda_12.3.r12.3/compiler.33567101_0. ```. It's a right mess! Sometimes call_variants uses gpu and at other times it stalls. That's why I'm now running the commands separately, I have a lot of outputs from make_examples. Our IT support say they're happy the gpu works part of the time :) It's adding a l",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/849
https://github.com/google/deepvariant/issues/849:574,performance,Memor,Memory-Usage,574,"Oh, and since we're here, I mentioned problems with the GPU. It seems that DeepVariant is using cuda version 11.3.1, but the nvidia driver (version 555.42.02) on the server is using cuda version 12.5. ```. nvidia-smi. +-----------------------------------------------------------------------------------------+. | NVIDIA-SMI 555.42.02 Driver Version: 555.42.02 CUDA Version: 12.5 |. |-----------------------------------------+------------------------+----------------------+. | GPU Name Persistence-M | Bus-Id Disp.A | Volatile Uncorr. ECC |. | Fan Temp Perf Pwr:Usage/Cap | Memory-Usage | GPU-Util Compute M. |. | | | MIG M. |. |=========================================+========================+======================|. | 0 Tesla V100-PCIE-16GB Off | 00000000:00:07.0 Off | 0 |. | N/A 28C P0 35W / 250W | 1MiB / 16384MiB | 0% Default |. | | | N/A |. +-----------------------------------------+------------------------+----------------------+. . +-----------------------------------------------------------------------------------------+. | Processes: |. | GPU GI CI PID Type Process name GPU Memory |. | ID ID Usage |. |=========================================================================================|. | No running processes found |. +-----------------------------------------------------------------------------------------+. ```. Should I update the driver, cuda or both? Also, installed on the server is cuda version 12.3, so I'm feeling a bit confused as to where the 12.5 comes from. ```. nvcc --version. nvcc: NVIDIA (R) Cuda compiler driver. Copyright (c) 2005-2023 NVIDIA Corporation. Built on Wed_Nov_22_10:17:15_PST_2023. Cuda compilation tools, release 12.3, V12.3.107. Build cuda_12.3.r12.3/compiler.33567101_0. ```. It's a right mess! Sometimes call_variants uses gpu and at other times it stalls. That's why I'm now running the commands separately, I have a lot of outputs from make_examples. Our IT support say they're happy the gpu works part of the time :) It's adding a l",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/849
https://github.com/google/deepvariant/issues/849:589,performance,GPU,GPU-Util,589,"Oh, and since we're here, I mentioned problems with the GPU. It seems that DeepVariant is using cuda version 11.3.1, but the nvidia driver (version 555.42.02) on the server is using cuda version 12.5. ```. nvidia-smi. +-----------------------------------------------------------------------------------------+. | NVIDIA-SMI 555.42.02 Driver Version: 555.42.02 CUDA Version: 12.5 |. |-----------------------------------------+------------------------+----------------------+. | GPU Name Persistence-M | Bus-Id Disp.A | Volatile Uncorr. ECC |. | Fan Temp Perf Pwr:Usage/Cap | Memory-Usage | GPU-Util Compute M. |. | | | MIG M. |. |=========================================+========================+======================|. | 0 Tesla V100-PCIE-16GB Off | 00000000:00:07.0 Off | 0 |. | N/A 28C P0 35W / 250W | 1MiB / 16384MiB | 0% Default |. | | | N/A |. +-----------------------------------------+------------------------+----------------------+. . +-----------------------------------------------------------------------------------------+. | Processes: |. | GPU GI CI PID Type Process name GPU Memory |. | ID ID Usage |. |=========================================================================================|. | No running processes found |. +-----------------------------------------------------------------------------------------+. ```. Should I update the driver, cuda or both? Also, installed on the server is cuda version 12.3, so I'm feeling a bit confused as to where the 12.5 comes from. ```. nvcc --version. nvcc: NVIDIA (R) Cuda compiler driver. Copyright (c) 2005-2023 NVIDIA Corporation. Built on Wed_Nov_22_10:17:15_PST_2023. Cuda compilation tools, release 12.3, V12.3.107. Build cuda_12.3.r12.3/compiler.33567101_0. ```. It's a right mess! Sometimes call_variants uses gpu and at other times it stalls. That's why I'm now running the commands separately, I have a lot of outputs from make_examples. Our IT support say they're happy the gpu works part of the time :) It's adding a l",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/849
https://github.com/google/deepvariant/issues/849:1057,performance,GPU,GPU,1057,". It seems that DeepVariant is using cuda version 11.3.1, but the nvidia driver (version 555.42.02) on the server is using cuda version 12.5. ```. nvidia-smi. +-----------------------------------------------------------------------------------------+. | NVIDIA-SMI 555.42.02 Driver Version: 555.42.02 CUDA Version: 12.5 |. |-----------------------------------------+------------------------+----------------------+. | GPU Name Persistence-M | Bus-Id Disp.A | Volatile Uncorr. ECC |. | Fan Temp Perf Pwr:Usage/Cap | Memory-Usage | GPU-Util Compute M. |. | | | MIG M. |. |=========================================+========================+======================|. | 0 Tesla V100-PCIE-16GB Off | 00000000:00:07.0 Off | 0 |. | N/A 28C P0 35W / 250W | 1MiB / 16384MiB | 0% Default |. | | | N/A |. +-----------------------------------------+------------------------+----------------------+. . +-----------------------------------------------------------------------------------------+. | Processes: |. | GPU GI CI PID Type Process name GPU Memory |. | ID ID Usage |. |=========================================================================================|. | No running processes found |. +-----------------------------------------------------------------------------------------+. ```. Should I update the driver, cuda or both? Also, installed on the server is cuda version 12.3, so I'm feeling a bit confused as to where the 12.5 comes from. ```. nvcc --version. nvcc: NVIDIA (R) Cuda compiler driver. Copyright (c) 2005-2023 NVIDIA Corporation. Built on Wed_Nov_22_10:17:15_PST_2023. Cuda compilation tools, release 12.3, V12.3.107. Build cuda_12.3.r12.3/compiler.33567101_0. ```. It's a right mess! Sometimes call_variants uses gpu and at other times it stalls. That's why I'm now running the commands separately, I have a lot of outputs from make_examples. Our IT support say they're happy the gpu works part of the time :) It's adding a lot of extra work, and I'm trying to come up with a solution",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/849
https://github.com/google/deepvariant/issues/849:1089,performance,GPU,GPU,1089,"sing cuda version 11.3.1, but the nvidia driver (version 555.42.02) on the server is using cuda version 12.5. ```. nvidia-smi. +-----------------------------------------------------------------------------------------+. | NVIDIA-SMI 555.42.02 Driver Version: 555.42.02 CUDA Version: 12.5 |. |-----------------------------------------+------------------------+----------------------+. | GPU Name Persistence-M | Bus-Id Disp.A | Volatile Uncorr. ECC |. | Fan Temp Perf Pwr:Usage/Cap | Memory-Usage | GPU-Util Compute M. |. | | | MIG M. |. |=========================================+========================+======================|. | 0 Tesla V100-PCIE-16GB Off | 00000000:00:07.0 Off | 0 |. | N/A 28C P0 35W / 250W | 1MiB / 16384MiB | 0% Default |. | | | N/A |. +-----------------------------------------+------------------------+----------------------+. . +-----------------------------------------------------------------------------------------+. | Processes: |. | GPU GI CI PID Type Process name GPU Memory |. | ID ID Usage |. |=========================================================================================|. | No running processes found |. +-----------------------------------------------------------------------------------------+. ```. Should I update the driver, cuda or both? Also, installed on the server is cuda version 12.3, so I'm feeling a bit confused as to where the 12.5 comes from. ```. nvcc --version. nvcc: NVIDIA (R) Cuda compiler driver. Copyright (c) 2005-2023 NVIDIA Corporation. Built on Wed_Nov_22_10:17:15_PST_2023. Cuda compilation tools, release 12.3, V12.3.107. Build cuda_12.3.r12.3/compiler.33567101_0. ```. It's a right mess! Sometimes call_variants uses gpu and at other times it stalls. That's why I'm now running the commands separately, I have a lot of outputs from make_examples. Our IT support say they're happy the gpu works part of the time :) It's adding a lot of extra work, and I'm trying to come up with a solution. Since it's completely unrelate",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/849
https://github.com/google/deepvariant/issues/849:1093,performance,Memor,Memory,1093,"cuda version 11.3.1, but the nvidia driver (version 555.42.02) on the server is using cuda version 12.5. ```. nvidia-smi. +-----------------------------------------------------------------------------------------+. | NVIDIA-SMI 555.42.02 Driver Version: 555.42.02 CUDA Version: 12.5 |. |-----------------------------------------+------------------------+----------------------+. | GPU Name Persistence-M | Bus-Id Disp.A | Volatile Uncorr. ECC |. | Fan Temp Perf Pwr:Usage/Cap | Memory-Usage | GPU-Util Compute M. |. | | | MIG M. |. |=========================================+========================+======================|. | 0 Tesla V100-PCIE-16GB Off | 00000000:00:07.0 Off | 0 |. | N/A 28C P0 35W / 250W | 1MiB / 16384MiB | 0% Default |. | | | N/A |. +-----------------------------------------+------------------------+----------------------+. . +-----------------------------------------------------------------------------------------+. | Processes: |. | GPU GI CI PID Type Process name GPU Memory |. | ID ID Usage |. |=========================================================================================|. | No running processes found |. +-----------------------------------------------------------------------------------------+. ```. Should I update the driver, cuda or both? Also, installed on the server is cuda version 12.3, so I'm feeling a bit confused as to where the 12.5 comes from. ```. nvcc --version. nvcc: NVIDIA (R) Cuda compiler driver. Copyright (c) 2005-2023 NVIDIA Corporation. Built on Wed_Nov_22_10:17:15_PST_2023. Cuda compilation tools, release 12.3, V12.3.107. Build cuda_12.3.r12.3/compiler.33567101_0. ```. It's a right mess! Sometimes call_variants uses gpu and at other times it stalls. That's why I'm now running the commands separately, I have a lot of outputs from make_examples. Our IT support say they're happy the gpu works part of the time :) It's adding a lot of extra work, and I'm trying to come up with a solution. Since it's completely unrelated to ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/849
https://github.com/google/deepvariant/issues/849:1788,performance,gpu,gpu,1788,".42.02) on the server is using cuda version 12.5. ```. nvidia-smi. +-----------------------------------------------------------------------------------------+. | NVIDIA-SMI 555.42.02 Driver Version: 555.42.02 CUDA Version: 12.5 |. |-----------------------------------------+------------------------+----------------------+. | GPU Name Persistence-M | Bus-Id Disp.A | Volatile Uncorr. ECC |. | Fan Temp Perf Pwr:Usage/Cap | Memory-Usage | GPU-Util Compute M. |. | | | MIG M. |. |=========================================+========================+======================|. | 0 Tesla V100-PCIE-16GB Off | 00000000:00:07.0 Off | 0 |. | N/A 28C P0 35W / 250W | 1MiB / 16384MiB | 0% Default |. | | | N/A |. +-----------------------------------------+------------------------+----------------------+. . +-----------------------------------------------------------------------------------------+. | Processes: |. | GPU GI CI PID Type Process name GPU Memory |. | ID ID Usage |. |=========================================================================================|. | No running processes found |. +-----------------------------------------------------------------------------------------+. ```. Should I update the driver, cuda or both? Also, installed on the server is cuda version 12.3, so I'm feeling a bit confused as to where the 12.5 comes from. ```. nvcc --version. nvcc: NVIDIA (R) Cuda compiler driver. Copyright (c) 2005-2023 NVIDIA Corporation. Built on Wed_Nov_22_10:17:15_PST_2023. Cuda compilation tools, release 12.3, V12.3.107. Build cuda_12.3.r12.3/compiler.33567101_0. ```. It's a right mess! Sometimes call_variants uses gpu and at other times it stalls. That's why I'm now running the commands separately, I have a lot of outputs from make_examples. Our IT support say they're happy the gpu works part of the time :) It's adding a lot of extra work, and I'm trying to come up with a solution. Since it's completely unrelated to the topic, perhaps I should create a new issue instead?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/849
https://github.com/google/deepvariant/issues/849:1805,performance,time,times,1805,".42.02) on the server is using cuda version 12.5. ```. nvidia-smi. +-----------------------------------------------------------------------------------------+. | NVIDIA-SMI 555.42.02 Driver Version: 555.42.02 CUDA Version: 12.5 |. |-----------------------------------------+------------------------+----------------------+. | GPU Name Persistence-M | Bus-Id Disp.A | Volatile Uncorr. ECC |. | Fan Temp Perf Pwr:Usage/Cap | Memory-Usage | GPU-Util Compute M. |. | | | MIG M. |. |=========================================+========================+======================|. | 0 Tesla V100-PCIE-16GB Off | 00000000:00:07.0 Off | 0 |. | N/A 28C P0 35W / 250W | 1MiB / 16384MiB | 0% Default |. | | | N/A |. +-----------------------------------------+------------------------+----------------------+. . +-----------------------------------------------------------------------------------------+. | Processes: |. | GPU GI CI PID Type Process name GPU Memory |. | ID ID Usage |. |=========================================================================================|. | No running processes found |. +-----------------------------------------------------------------------------------------+. ```. Should I update the driver, cuda or both? Also, installed on the server is cuda version 12.3, so I'm feeling a bit confused as to where the 12.5 comes from. ```. nvcc --version. nvcc: NVIDIA (R) Cuda compiler driver. Copyright (c) 2005-2023 NVIDIA Corporation. Built on Wed_Nov_22_10:17:15_PST_2023. Cuda compilation tools, release 12.3, V12.3.107. Build cuda_12.3.r12.3/compiler.33567101_0. ```. It's a right mess! Sometimes call_variants uses gpu and at other times it stalls. That's why I'm now running the commands separately, I have a lot of outputs from make_examples. Our IT support say they're happy the gpu works part of the time :) It's adding a lot of extra work, and I'm trying to come up with a solution. Since it's completely unrelated to the topic, perhaps I should create a new issue instead?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/849
https://github.com/google/deepvariant/issues/849:1955,performance,gpu,gpu,1955,".42.02) on the server is using cuda version 12.5. ```. nvidia-smi. +-----------------------------------------------------------------------------------------+. | NVIDIA-SMI 555.42.02 Driver Version: 555.42.02 CUDA Version: 12.5 |. |-----------------------------------------+------------------------+----------------------+. | GPU Name Persistence-M | Bus-Id Disp.A | Volatile Uncorr. ECC |. | Fan Temp Perf Pwr:Usage/Cap | Memory-Usage | GPU-Util Compute M. |. | | | MIG M. |. |=========================================+========================+======================|. | 0 Tesla V100-PCIE-16GB Off | 00000000:00:07.0 Off | 0 |. | N/A 28C P0 35W / 250W | 1MiB / 16384MiB | 0% Default |. | | | N/A |. +-----------------------------------------+------------------------+----------------------+. . +-----------------------------------------------------------------------------------------+. | Processes: |. | GPU GI CI PID Type Process name GPU Memory |. | ID ID Usage |. |=========================================================================================|. | No running processes found |. +-----------------------------------------------------------------------------------------+. ```. Should I update the driver, cuda or both? Also, installed on the server is cuda version 12.3, so I'm feeling a bit confused as to where the 12.5 comes from. ```. nvcc --version. nvcc: NVIDIA (R) Cuda compiler driver. Copyright (c) 2005-2023 NVIDIA Corporation. Built on Wed_Nov_22_10:17:15_PST_2023. Cuda compilation tools, release 12.3, V12.3.107. Build cuda_12.3.r12.3/compiler.33567101_0. ```. It's a right mess! Sometimes call_variants uses gpu and at other times it stalls. That's why I'm now running the commands separately, I have a lot of outputs from make_examples. Our IT support say they're happy the gpu works part of the time :) It's adding a lot of extra work, and I'm trying to come up with a solution. Since it's completely unrelated to the topic, perhaps I should create a new issue instead?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/849
https://github.com/google/deepvariant/issues/849:1977,performance,time,time,1977,".42.02) on the server is using cuda version 12.5. ```. nvidia-smi. +-----------------------------------------------------------------------------------------+. | NVIDIA-SMI 555.42.02 Driver Version: 555.42.02 CUDA Version: 12.5 |. |-----------------------------------------+------------------------+----------------------+. | GPU Name Persistence-M | Bus-Id Disp.A | Volatile Uncorr. ECC |. | Fan Temp Perf Pwr:Usage/Cap | Memory-Usage | GPU-Util Compute M. |. | | | MIG M. |. |=========================================+========================+======================|. | 0 Tesla V100-PCIE-16GB Off | 00000000:00:07.0 Off | 0 |. | N/A 28C P0 35W / 250W | 1MiB / 16384MiB | 0% Default |. | | | N/A |. +-----------------------------------------+------------------------+----------------------+. . +-----------------------------------------------------------------------------------------+. | Processes: |. | GPU GI CI PID Type Process name GPU Memory |. | ID ID Usage |. |=========================================================================================|. | No running processes found |. +-----------------------------------------------------------------------------------------+. ```. Should I update the driver, cuda or both? Also, installed on the server is cuda version 12.3, so I'm feeling a bit confused as to where the 12.5 comes from. ```. nvcc --version. nvcc: NVIDIA (R) Cuda compiler driver. Copyright (c) 2005-2023 NVIDIA Corporation. Built on Wed_Nov_22_10:17:15_PST_2023. Cuda compilation tools, release 12.3, V12.3.107. Build cuda_12.3.r12.3/compiler.33567101_0. ```. It's a right mess! Sometimes call_variants uses gpu and at other times it stalls. That's why I'm now running the commands separately, I have a lot of outputs from make_examples. Our IT support say they're happy the gpu works part of the time :) It's adding a lot of extra work, and I'm trying to come up with a solution. Since it's completely unrelated to the topic, perhaps I should create a new issue instead?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/849
https://github.com/google/deepvariant/issues/849:1352,safety,updat,update,1352,".42.02) on the server is using cuda version 12.5. ```. nvidia-smi. +-----------------------------------------------------------------------------------------+. | NVIDIA-SMI 555.42.02 Driver Version: 555.42.02 CUDA Version: 12.5 |. |-----------------------------------------+------------------------+----------------------+. | GPU Name Persistence-M | Bus-Id Disp.A | Volatile Uncorr. ECC |. | Fan Temp Perf Pwr:Usage/Cap | Memory-Usage | GPU-Util Compute M. |. | | | MIG M. |. |=========================================+========================+======================|. | 0 Tesla V100-PCIE-16GB Off | 00000000:00:07.0 Off | 0 |. | N/A 28C P0 35W / 250W | 1MiB / 16384MiB | 0% Default |. | | | N/A |. +-----------------------------------------+------------------------+----------------------+. . +-----------------------------------------------------------------------------------------+. | Processes: |. | GPU GI CI PID Type Process name GPU Memory |. | ID ID Usage |. |=========================================================================================|. | No running processes found |. +-----------------------------------------------------------------------------------------+. ```. Should I update the driver, cuda or both? Also, installed on the server is cuda version 12.3, so I'm feeling a bit confused as to where the 12.5 comes from. ```. nvcc --version. nvcc: NVIDIA (R) Cuda compiler driver. Copyright (c) 2005-2023 NVIDIA Corporation. Built on Wed_Nov_22_10:17:15_PST_2023. Cuda compilation tools, release 12.3, V12.3.107. Build cuda_12.3.r12.3/compiler.33567101_0. ```. It's a right mess! Sometimes call_variants uses gpu and at other times it stalls. That's why I'm now running the commands separately, I have a lot of outputs from make_examples. Our IT support say they're happy the gpu works part of the time :) It's adding a lot of extra work, and I'm trying to come up with a solution. Since it's completely unrelated to the topic, perhaps I should create a new issue instead?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/849
https://github.com/google/deepvariant/issues/849:2072,safety,compl,completely,2072,".42.02) on the server is using cuda version 12.5. ```. nvidia-smi. +-----------------------------------------------------------------------------------------+. | NVIDIA-SMI 555.42.02 Driver Version: 555.42.02 CUDA Version: 12.5 |. |-----------------------------------------+------------------------+----------------------+. | GPU Name Persistence-M | Bus-Id Disp.A | Volatile Uncorr. ECC |. | Fan Temp Perf Pwr:Usage/Cap | Memory-Usage | GPU-Util Compute M. |. | | | MIG M. |. |=========================================+========================+======================|. | 0 Tesla V100-PCIE-16GB Off | 00000000:00:07.0 Off | 0 |. | N/A 28C P0 35W / 250W | 1MiB / 16384MiB | 0% Default |. | | | N/A |. +-----------------------------------------+------------------------+----------------------+. . +-----------------------------------------------------------------------------------------+. | Processes: |. | GPU GI CI PID Type Process name GPU Memory |. | ID ID Usage |. |=========================================================================================|. | No running processes found |. +-----------------------------------------------------------------------------------------+. ```. Should I update the driver, cuda or both? Also, installed on the server is cuda version 12.3, so I'm feeling a bit confused as to where the 12.5 comes from. ```. nvcc --version. nvcc: NVIDIA (R) Cuda compiler driver. Copyright (c) 2005-2023 NVIDIA Corporation. Built on Wed_Nov_22_10:17:15_PST_2023. Cuda compilation tools, release 12.3, V12.3.107. Build cuda_12.3.r12.3/compiler.33567101_0. ```. It's a right mess! Sometimes call_variants uses gpu and at other times it stalls. That's why I'm now running the commands separately, I have a lot of outputs from make_examples. Our IT support say they're happy the gpu works part of the time :) It's adding a lot of extra work, and I'm trying to come up with a solution. Since it's completely unrelated to the topic, perhaps I should create a new issue instead?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/849
https://github.com/google/deepvariant/issues/849:1352,security,updat,update,1352,".42.02) on the server is using cuda version 12.5. ```. nvidia-smi. +-----------------------------------------------------------------------------------------+. | NVIDIA-SMI 555.42.02 Driver Version: 555.42.02 CUDA Version: 12.5 |. |-----------------------------------------+------------------------+----------------------+. | GPU Name Persistence-M | Bus-Id Disp.A | Volatile Uncorr. ECC |. | Fan Temp Perf Pwr:Usage/Cap | Memory-Usage | GPU-Util Compute M. |. | | | MIG M. |. |=========================================+========================+======================|. | 0 Tesla V100-PCIE-16GB Off | 00000000:00:07.0 Off | 0 |. | N/A 28C P0 35W / 250W | 1MiB / 16384MiB | 0% Default |. | | | N/A |. +-----------------------------------------+------------------------+----------------------+. . +-----------------------------------------------------------------------------------------+. | Processes: |. | GPU GI CI PID Type Process name GPU Memory |. | ID ID Usage |. |=========================================================================================|. | No running processes found |. +-----------------------------------------------------------------------------------------+. ```. Should I update the driver, cuda or both? Also, installed on the server is cuda version 12.3, so I'm feeling a bit confused as to where the 12.5 comes from. ```. nvcc --version. nvcc: NVIDIA (R) Cuda compiler driver. Copyright (c) 2005-2023 NVIDIA Corporation. Built on Wed_Nov_22_10:17:15_PST_2023. Cuda compilation tools, release 12.3, V12.3.107. Build cuda_12.3.r12.3/compiler.33567101_0. ```. It's a right mess! Sometimes call_variants uses gpu and at other times it stalls. That's why I'm now running the commands separately, I have a lot of outputs from make_examples. Our IT support say they're happy the gpu works part of the time :) It's adding a lot of extra work, and I'm trying to come up with a solution. Since it's completely unrelated to the topic, perhaps I should create a new issue instead?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/849
https://github.com/google/deepvariant/issues/849:2072,security,compl,completely,2072,".42.02) on the server is using cuda version 12.5. ```. nvidia-smi. +-----------------------------------------------------------------------------------------+. | NVIDIA-SMI 555.42.02 Driver Version: 555.42.02 CUDA Version: 12.5 |. |-----------------------------------------+------------------------+----------------------+. | GPU Name Persistence-M | Bus-Id Disp.A | Volatile Uncorr. ECC |. | Fan Temp Perf Pwr:Usage/Cap | Memory-Usage | GPU-Util Compute M. |. | | | MIG M. |. |=========================================+========================+======================|. | 0 Tesla V100-PCIE-16GB Off | 00000000:00:07.0 Off | 0 |. | N/A 28C P0 35W / 250W | 1MiB / 16384MiB | 0% Default |. | | | N/A |. +-----------------------------------------+------------------------+----------------------+. . +-----------------------------------------------------------------------------------------+. | Processes: |. | GPU GI CI PID Type Process name GPU Memory |. | ID ID Usage |. |=========================================================================================|. | No running processes found |. +-----------------------------------------------------------------------------------------+. ```. Should I update the driver, cuda or both? Also, installed on the server is cuda version 12.3, so I'm feeling a bit confused as to where the 12.5 comes from. ```. nvcc --version. nvcc: NVIDIA (R) Cuda compiler driver. Copyright (c) 2005-2023 NVIDIA Corporation. Built on Wed_Nov_22_10:17:15_PST_2023. Cuda compilation tools, release 12.3, V12.3.107. Build cuda_12.3.r12.3/compiler.33567101_0. ```. It's a right mess! Sometimes call_variants uses gpu and at other times it stalls. That's why I'm now running the commands separately, I have a lot of outputs from make_examples. Our IT support say they're happy the gpu works part of the time :) It's adding a lot of extra work, and I'm trying to come up with a solution. Since it's completely unrelated to the topic, perhaps I should create a new issue instead?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/849
https://github.com/google/deepvariant/issues/849:574,usability,Memor,Memory-Usage,574,"Oh, and since we're here, I mentioned problems with the GPU. It seems that DeepVariant is using cuda version 11.3.1, but the nvidia driver (version 555.42.02) on the server is using cuda version 12.5. ```. nvidia-smi. +-----------------------------------------------------------------------------------------+. | NVIDIA-SMI 555.42.02 Driver Version: 555.42.02 CUDA Version: 12.5 |. |-----------------------------------------+------------------------+----------------------+. | GPU Name Persistence-M | Bus-Id Disp.A | Volatile Uncorr. ECC |. | Fan Temp Perf Pwr:Usage/Cap | Memory-Usage | GPU-Util Compute M. |. | | | MIG M. |. |=========================================+========================+======================|. | 0 Tesla V100-PCIE-16GB Off | 00000000:00:07.0 Off | 0 |. | N/A 28C P0 35W / 250W | 1MiB / 16384MiB | 0% Default |. | | | N/A |. +-----------------------------------------+------------------------+----------------------+. . +-----------------------------------------------------------------------------------------+. | Processes: |. | GPU GI CI PID Type Process name GPU Memory |. | ID ID Usage |. |=========================================================================================|. | No running processes found |. +-----------------------------------------------------------------------------------------+. ```. Should I update the driver, cuda or both? Also, installed on the server is cuda version 12.3, so I'm feeling a bit confused as to where the 12.5 comes from. ```. nvcc --version. nvcc: NVIDIA (R) Cuda compiler driver. Copyright (c) 2005-2023 NVIDIA Corporation. Built on Wed_Nov_22_10:17:15_PST_2023. Cuda compilation tools, release 12.3, V12.3.107. Build cuda_12.3.r12.3/compiler.33567101_0. ```. It's a right mess! Sometimes call_variants uses gpu and at other times it stalls. That's why I'm now running the commands separately, I have a lot of outputs from make_examples. Our IT support say they're happy the gpu works part of the time :) It's adding a l",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/849
https://github.com/google/deepvariant/issues/849:1093,usability,Memor,Memory,1093,"cuda version 11.3.1, but the nvidia driver (version 555.42.02) on the server is using cuda version 12.5. ```. nvidia-smi. +-----------------------------------------------------------------------------------------+. | NVIDIA-SMI 555.42.02 Driver Version: 555.42.02 CUDA Version: 12.5 |. |-----------------------------------------+------------------------+----------------------+. | GPU Name Persistence-M | Bus-Id Disp.A | Volatile Uncorr. ECC |. | Fan Temp Perf Pwr:Usage/Cap | Memory-Usage | GPU-Util Compute M. |. | | | MIG M. |. |=========================================+========================+======================|. | 0 Tesla V100-PCIE-16GB Off | 00000000:00:07.0 Off | 0 |. | N/A 28C P0 35W / 250W | 1MiB / 16384MiB | 0% Default |. | | | N/A |. +-----------------------------------------+------------------------+----------------------+. . +-----------------------------------------------------------------------------------------+. | Processes: |. | GPU GI CI PID Type Process name GPU Memory |. | ID ID Usage |. |=========================================================================================|. | No running processes found |. +-----------------------------------------------------------------------------------------+. ```. Should I update the driver, cuda or both? Also, installed on the server is cuda version 12.3, so I'm feeling a bit confused as to where the 12.5 comes from. ```. nvcc --version. nvcc: NVIDIA (R) Cuda compiler driver. Copyright (c) 2005-2023 NVIDIA Corporation. Built on Wed_Nov_22_10:17:15_PST_2023. Cuda compilation tools, release 12.3, V12.3.107. Build cuda_12.3.r12.3/compiler.33567101_0. ```. It's a right mess! Sometimes call_variants uses gpu and at other times it stalls. That's why I'm now running the commands separately, I have a lot of outputs from make_examples. Our IT support say they're happy the gpu works part of the time :) It's adding a lot of extra work, and I'm trying to come up with a solution. Since it's completely unrelated to ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/849
https://github.com/google/deepvariant/issues/849:1660,usability,tool,tools,1660,".42.02) on the server is using cuda version 12.5. ```. nvidia-smi. +-----------------------------------------------------------------------------------------+. | NVIDIA-SMI 555.42.02 Driver Version: 555.42.02 CUDA Version: 12.5 |. |-----------------------------------------+------------------------+----------------------+. | GPU Name Persistence-M | Bus-Id Disp.A | Volatile Uncorr. ECC |. | Fan Temp Perf Pwr:Usage/Cap | Memory-Usage | GPU-Util Compute M. |. | | | MIG M. |. |=========================================+========================+======================|. | 0 Tesla V100-PCIE-16GB Off | 00000000:00:07.0 Off | 0 |. | N/A 28C P0 35W / 250W | 1MiB / 16384MiB | 0% Default |. | | | N/A |. +-----------------------------------------+------------------------+----------------------+. . +-----------------------------------------------------------------------------------------+. | Processes: |. | GPU GI CI PID Type Process name GPU Memory |. | ID ID Usage |. |=========================================================================================|. | No running processes found |. +-----------------------------------------------------------------------------------------+. ```. Should I update the driver, cuda or both? Also, installed on the server is cuda version 12.3, so I'm feeling a bit confused as to where the 12.5 comes from. ```. nvcc --version. nvcc: NVIDIA (R) Cuda compiler driver. Copyright (c) 2005-2023 NVIDIA Corporation. Built on Wed_Nov_22_10:17:15_PST_2023. Cuda compilation tools, release 12.3, V12.3.107. Build cuda_12.3.r12.3/compiler.33567101_0. ```. It's a right mess! Sometimes call_variants uses gpu and at other times it stalls. That's why I'm now running the commands separately, I have a lot of outputs from make_examples. Our IT support say they're happy the gpu works part of the time :) It's adding a lot of extra work, and I'm trying to come up with a solution. Since it's completely unrelated to the topic, perhaps I should create a new issue instead?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/849
https://github.com/google/deepvariant/issues/849:1853,usability,command,commands,1853,".42.02) on the server is using cuda version 12.5. ```. nvidia-smi. +-----------------------------------------------------------------------------------------+. | NVIDIA-SMI 555.42.02 Driver Version: 555.42.02 CUDA Version: 12.5 |. |-----------------------------------------+------------------------+----------------------+. | GPU Name Persistence-M | Bus-Id Disp.A | Volatile Uncorr. ECC |. | Fan Temp Perf Pwr:Usage/Cap | Memory-Usage | GPU-Util Compute M. |. | | | MIG M. |. |=========================================+========================+======================|. | 0 Tesla V100-PCIE-16GB Off | 00000000:00:07.0 Off | 0 |. | N/A 28C P0 35W / 250W | 1MiB / 16384MiB | 0% Default |. | | | N/A |. +-----------------------------------------+------------------------+----------------------+. . +-----------------------------------------------------------------------------------------+. | Processes: |. | GPU GI CI PID Type Process name GPU Memory |. | ID ID Usage |. |=========================================================================================|. | No running processes found |. +-----------------------------------------------------------------------------------------+. ```. Should I update the driver, cuda or both? Also, installed on the server is cuda version 12.3, so I'm feeling a bit confused as to where the 12.5 comes from. ```. nvcc --version. nvcc: NVIDIA (R) Cuda compiler driver. Copyright (c) 2005-2023 NVIDIA Corporation. Built on Wed_Nov_22_10:17:15_PST_2023. Cuda compilation tools, release 12.3, V12.3.107. Build cuda_12.3.r12.3/compiler.33567101_0. ```. It's a right mess! Sometimes call_variants uses gpu and at other times it stalls. That's why I'm now running the commands separately, I have a lot of outputs from make_examples. Our IT support say they're happy the gpu works part of the time :) It's adding a lot of extra work, and I'm trying to come up with a solution. Since it's completely unrelated to the topic, perhaps I should create a new issue instead?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/849
https://github.com/google/deepvariant/issues/849:1925,usability,support,support,1925,".42.02) on the server is using cuda version 12.5. ```. nvidia-smi. +-----------------------------------------------------------------------------------------+. | NVIDIA-SMI 555.42.02 Driver Version: 555.42.02 CUDA Version: 12.5 |. |-----------------------------------------+------------------------+----------------------+. | GPU Name Persistence-M | Bus-Id Disp.A | Volatile Uncorr. ECC |. | Fan Temp Perf Pwr:Usage/Cap | Memory-Usage | GPU-Util Compute M. |. | | | MIG M. |. |=========================================+========================+======================|. | 0 Tesla V100-PCIE-16GB Off | 00000000:00:07.0 Off | 0 |. | N/A 28C P0 35W / 250W | 1MiB / 16384MiB | 0% Default |. | | | N/A |. +-----------------------------------------+------------------------+----------------------+. . +-----------------------------------------------------------------------------------------+. | Processes: |. | GPU GI CI PID Type Process name GPU Memory |. | ID ID Usage |. |=========================================================================================|. | No running processes found |. +-----------------------------------------------------------------------------------------+. ```. Should I update the driver, cuda or both? Also, installed on the server is cuda version 12.3, so I'm feeling a bit confused as to where the 12.5 comes from. ```. nvcc --version. nvcc: NVIDIA (R) Cuda compiler driver. Copyright (c) 2005-2023 NVIDIA Corporation. Built on Wed_Nov_22_10:17:15_PST_2023. Cuda compilation tools, release 12.3, V12.3.107. Build cuda_12.3.r12.3/compiler.33567101_0. ```. It's a right mess! Sometimes call_variants uses gpu and at other times it stalls. That's why I'm now running the commands separately, I have a lot of outputs from make_examples. Our IT support say they're happy the gpu works part of the time :) It's adding a lot of extra work, and I'm trying to come up with a solution. Since it's completely unrelated to the topic, perhaps I should create a new issue instead?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/849
https://github.com/google/deepvariant/issues/849:238,deployability,contain,contain,238,"Hi @karoliinas ,. The format `gvcf.tfrecord@19.gz` is referring to files gvcf.tfrecord-[00000-00018]-of-00019.gz. So I don't think your commands are wrong. I think your call_variants_output-[00000-00015]-of-00016.tfrecord.gz files likely contain prediction values that are unexpected. I'm out of office now. I'll give you some examples to debug the call_variants_output next week!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/849
https://github.com/google/deepvariant/issues/849:246,energy efficiency,predict,prediction,246,"Hi @karoliinas ,. The format `gvcf.tfrecord@19.gz` is referring to files gvcf.tfrecord-[00000-00018]-of-00019.gz. So I don't think your commands are wrong. I think your call_variants_output-[00000-00015]-of-00016.tfrecord.gz files likely contain prediction values that are unexpected. I'm out of office now. I'll give you some examples to debug the call_variants_output next week!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/849
https://github.com/google/deepvariant/issues/849:22,interoperability,format,format,22,"Hi @karoliinas ,. The format `gvcf.tfrecord@19.gz` is referring to files gvcf.tfrecord-[00000-00018]-of-00019.gz. So I don't think your commands are wrong. I think your call_variants_output-[00000-00015]-of-00016.tfrecord.gz files likely contain prediction values that are unexpected. I'm out of office now. I'll give you some examples to debug the call_variants_output next week!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/849
https://github.com/google/deepvariant/issues/849:246,safety,predict,prediction,246,"Hi @karoliinas ,. The format `gvcf.tfrecord@19.gz` is referring to files gvcf.tfrecord-[00000-00018]-of-00019.gz. So I don't think your commands are wrong. I think your call_variants_output-[00000-00015]-of-00016.tfrecord.gz files likely contain prediction values that are unexpected. I'm out of office now. I'll give you some examples to debug the call_variants_output next week!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/849
https://github.com/google/deepvariant/issues/849:136,usability,command,commands,136,"Hi @karoliinas ,. The format `gvcf.tfrecord@19.gz` is referring to files gvcf.tfrecord-[00000-00018]-of-00019.gz. So I don't think your commands are wrong. I think your call_variants_output-[00000-00015]-of-00016.tfrecord.gz files likely contain prediction values that are unexpected. I'm out of office now. I'll give you some examples to debug the call_variants_output next week!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/849
https://github.com/google/deepvariant/issues/849:169,interoperability,format,format,169,"Hi @pichuan, thanks for clearing this up! When you get the chance, please let me know what to look for in the call_variants -output. Also, I'm not sure I understand the format, using zcat I get many very short lines. I see AD, DP and VAF but not sure how to read variant positions / probabilities. . Many thanks! . -Karoliina",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/849
https://github.com/google/deepvariant/issues/849:154,testability,understand,understand,154,"Hi @pichuan, thanks for clearing this up! When you get the chance, please let me know what to look for in the call_variants -output. Also, I'm not sure I understand the format, using zcat I get many very short lines. I see AD, DP and VAF but not sure how to read variant positions / probabilities. . Many thanks! . -Karoliina",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/849
