id,quality_attribute,keyword,matched_word,match_idx,sentence,source,author,repo,version,wiki,url
https://github.com/root-project/root/pull/2406:191,security,control,control,191,[WIP] Add a wrapper around add_subdirectory.; This patch gives us control over the addition in subfolders in cmake. It is necessary ingredient for the root package manager where it needs. to control precisely how the build is confugured and possibly ignore. some of the non-requested components.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2406
https://github.com/root-project/root/pull/2406:66,testability,control,control,66,[WIP] Add a wrapper around add_subdirectory.; This patch gives us control over the addition in subfolders in cmake. It is necessary ingredient for the root package manager where it needs. to control precisely how the build is confugured and possibly ignore. some of the non-requested components.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2406
https://github.com/root-project/root/pull/2406:191,testability,control,control,191,[WIP] Add a wrapper around add_subdirectory.; This patch gives us control over the addition in subfolders in cmake. It is necessary ingredient for the root package manager where it needs. to control precisely how the build is confugured and possibly ignore. some of the non-requested components.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2406
https://github.com/root-project/root/pull/2407:36,availability,error,error,36,[cxxmodules] Do not emit relocation error when -fno-validate-pch is set; This patch already landed in llvm. Clang emits error when implicit modules was relocated from the. first build directory. However this was biting our usecase where we copy. the contents of build directory to another directory in order to. distribute.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2407
https://github.com/root-project/root/pull/2407:120,availability,error,error,120,[cxxmodules] Do not emit relocation error when -fno-validate-pch is set; This patch already landed in llvm. Clang emits error when implicit modules was relocated from the. first build directory. However this was biting our usecase where we copy. the contents of build directory to another directory in order to. distribute.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2407
https://github.com/root-project/root/pull/2407:78,deployability,patch,patch,78,[cxxmodules] Do not emit relocation error when -fno-validate-pch is set; This patch already landed in llvm. Clang emits error when implicit modules was relocated from the. first build directory. However this was biting our usecase where we copy. the contents of build directory to another directory in order to. distribute.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2407
https://github.com/root-project/root/pull/2407:140,deployability,modul,modules,140,[cxxmodules] Do not emit relocation error when -fno-validate-pch is set; This patch already landed in llvm. Clang emits error when implicit modules was relocated from the. first build directory. However this was biting our usecase where we copy. the contents of build directory to another directory in order to. distribute.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2407
https://github.com/root-project/root/pull/2407:178,deployability,build,build,178,[cxxmodules] Do not emit relocation error when -fno-validate-pch is set; This patch already landed in llvm. Clang emits error when implicit modules was relocated from the. first build directory. However this was biting our usecase where we copy. the contents of build directory to another directory in order to. distribute.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2407
https://github.com/root-project/root/pull/2407:262,deployability,build,build,262,[cxxmodules] Do not emit relocation error when -fno-validate-pch is set; This patch already landed in llvm. Clang emits error when implicit modules was relocated from the. first build directory. However this was biting our usecase where we copy. the contents of build directory to another directory in order to. distribute.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2407
https://github.com/root-project/root/pull/2407:312,interoperability,distribut,distribute,312,[cxxmodules] Do not emit relocation error when -fno-validate-pch is set; This patch already landed in llvm. Clang emits error when implicit modules was relocated from the. first build directory. However this was biting our usecase where we copy. the contents of build directory to another directory in order to. distribute.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2407
https://github.com/root-project/root/pull/2407:140,modifiability,modul,modules,140,[cxxmodules] Do not emit relocation error when -fno-validate-pch is set; This patch already landed in llvm. Clang emits error when implicit modules was relocated from the. first build directory. However this was biting our usecase where we copy. the contents of build directory to another directory in order to. distribute.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2407
https://github.com/root-project/root/pull/2407:36,performance,error,error,36,[cxxmodules] Do not emit relocation error when -fno-validate-pch is set; This patch already landed in llvm. Clang emits error when implicit modules was relocated from the. first build directory. However this was biting our usecase where we copy. the contents of build directory to another directory in order to. distribute.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2407
https://github.com/root-project/root/pull/2407:120,performance,error,error,120,[cxxmodules] Do not emit relocation error when -fno-validate-pch is set; This patch already landed in llvm. Clang emits error when implicit modules was relocated from the. first build directory. However this was biting our usecase where we copy. the contents of build directory to another directory in order to. distribute.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2407
https://github.com/root-project/root/pull/2407:250,performance,content,contents,250,[cxxmodules] Do not emit relocation error when -fno-validate-pch is set; This patch already landed in llvm. Clang emits error when implicit modules was relocated from the. first build directory. However this was biting our usecase where we copy. the contents of build directory to another directory in order to. distribute.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2407
https://github.com/root-project/root/pull/2407:36,safety,error,error,36,[cxxmodules] Do not emit relocation error when -fno-validate-pch is set; This patch already landed in llvm. Clang emits error when implicit modules was relocated from the. first build directory. However this was biting our usecase where we copy. the contents of build directory to another directory in order to. distribute.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2407
https://github.com/root-project/root/pull/2407:52,safety,valid,validate-pch,52,[cxxmodules] Do not emit relocation error when -fno-validate-pch is set; This patch already landed in llvm. Clang emits error when implicit modules was relocated from the. first build directory. However this was biting our usecase where we copy. the contents of build directory to another directory in order to. distribute.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2407
https://github.com/root-project/root/pull/2407:78,safety,patch,patch,78,[cxxmodules] Do not emit relocation error when -fno-validate-pch is set; This patch already landed in llvm. Clang emits error when implicit modules was relocated from the. first build directory. However this was biting our usecase where we copy. the contents of build directory to another directory in order to. distribute.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2407
https://github.com/root-project/root/pull/2407:120,safety,error,error,120,[cxxmodules] Do not emit relocation error when -fno-validate-pch is set; This patch already landed in llvm. Clang emits error when implicit modules was relocated from the. first build directory. However this was biting our usecase where we copy. the contents of build directory to another directory in order to. distribute.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2407
https://github.com/root-project/root/pull/2407:140,safety,modul,modules,140,[cxxmodules] Do not emit relocation error when -fno-validate-pch is set; This patch already landed in llvm. Clang emits error when implicit modules was relocated from the. first build directory. However this was biting our usecase where we copy. the contents of build directory to another directory in order to. distribute.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2407
https://github.com/root-project/root/pull/2407:52,security,validat,validate-pch,52,[cxxmodules] Do not emit relocation error when -fno-validate-pch is set; This patch already landed in llvm. Clang emits error when implicit modules was relocated from the. first build directory. However this was biting our usecase where we copy. the contents of build directory to another directory in order to. distribute.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2407
https://github.com/root-project/root/pull/2407:78,security,patch,patch,78,[cxxmodules] Do not emit relocation error when -fno-validate-pch is set; This patch already landed in llvm. Clang emits error when implicit modules was relocated from the. first build directory. However this was biting our usecase where we copy. the contents of build directory to another directory in order to. distribute.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2407
https://github.com/root-project/root/pull/2407:36,usability,error,error,36,[cxxmodules] Do not emit relocation error when -fno-validate-pch is set; This patch already landed in llvm. Clang emits error when implicit modules was relocated from the. first build directory. However this was biting our usecase where we copy. the contents of build directory to another directory in order to. distribute.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2407
https://github.com/root-project/root/pull/2407:120,usability,error,error,120,[cxxmodules] Do not emit relocation error when -fno-validate-pch is set; This patch already landed in llvm. Clang emits error when implicit modules was relocated from the. first build directory. However this was biting our usecase where we copy. the contents of build directory to another directory in order to. distribute.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2407
https://github.com/root-project/root/pull/2408:59,deployability,depend,dependency,59,[pm] Cleaning dead code and includes; Removing source code dependency on Graf3d for Core. Code is dead and look like not used anywhere.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2408
https://github.com/root-project/root/pull/2408:84,energy efficiency,Core,Core,84,[pm] Cleaning dead code and includes; Removing source code dependency on Graf3d for Core. Code is dead and look like not used anywhere.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2408
https://github.com/root-project/root/pull/2408:59,integrability,depend,dependency,59,[pm] Cleaning dead code and includes; Removing source code dependency on Graf3d for Core. Code is dead and look like not used anywhere.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2408
https://github.com/root-project/root/pull/2408:59,modifiability,depend,dependency,59,[pm] Cleaning dead code and includes; Removing source code dependency on Graf3d for Core. Code is dead and look like not used anywhere.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2408
https://github.com/root-project/root/pull/2408:59,safety,depend,dependency,59,[pm] Cleaning dead code and includes; Removing source code dependency on Graf3d for Core. Code is dead and look like not used anywhere.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2408
https://github.com/root-project/root/pull/2408:59,testability,depend,dependency,59,[pm] Cleaning dead code and includes; Removing source code dependency on Graf3d for Core. Code is dead and look like not used anywhere.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2408
https://github.com/root-project/root/pull/2409:1,deployability,Modul,Modulariz,1,[Modulariz] Adding missing dependency MathCore for FFTW library;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2409
https://github.com/root-project/root/pull/2409:27,deployability,depend,dependency,27,[Modulariz] Adding missing dependency MathCore for FFTW library;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2409
https://github.com/root-project/root/pull/2409:1,integrability,Modular,Modulariz,1,[Modulariz] Adding missing dependency MathCore for FFTW library;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2409
https://github.com/root-project/root/pull/2409:27,integrability,depend,dependency,27,[Modulariz] Adding missing dependency MathCore for FFTW library;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2409
https://github.com/root-project/root/pull/2409:1,modifiability,Modul,Modulariz,1,[Modulariz] Adding missing dependency MathCore for FFTW library;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2409
https://github.com/root-project/root/pull/2409:27,modifiability,depend,dependency,27,[Modulariz] Adding missing dependency MathCore for FFTW library;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2409
https://github.com/root-project/root/pull/2409:1,safety,Modul,Modulariz,1,[Modulariz] Adding missing dependency MathCore for FFTW library;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2409
https://github.com/root-project/root/pull/2409:27,safety,depend,dependency,27,[Modulariz] Adding missing dependency MathCore for FFTW library;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2409
https://github.com/root-project/root/pull/2409:1,testability,Modula,Modulariz,1,[Modulariz] Adding missing dependency MathCore for FFTW library;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2409
https://github.com/root-project/root/pull/2409:27,testability,depend,dependency,27,[Modulariz] Adding missing dependency MathCore for FFTW library;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2409
https://github.com/root-project/root/pull/2410:13,deployability,Updat,Update,13,[TMVA] Doc - Update TMVA License location;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2410
https://github.com/root-project/root/pull/2410:13,safety,Updat,Update,13,[TMVA] Doc - Update TMVA License location;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2410
https://github.com/root-project/root/pull/2410:13,security,Updat,Update,13,[TMVA] Doc - Update TMVA License location;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2410
https://github.com/root-project/root/pull/2411:13,energy efficiency,Reduc,Reduce,13,[cxxmodules] Reduce pcm size by not embedding all used files; This reduces pcm size by 5.5 Mbytes. ```. `--> du lib/*.pcm | awk -F ' ' '{sum += $1} END {print sum}'. 340380. `--> du lib/*.pcm | awk -F ' ' '{sum += $1} END {print sum}'. 334932. ```,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2411
https://github.com/root-project/root/pull/2411:67,energy efficiency,reduc,reduces,67,[cxxmodules] Reduce pcm size by not embedding all used files; This reduces pcm size by 5.5 Mbytes. ```. `--> du lib/*.pcm | awk -F ' ' '{sum += $1} END {print sum}'. 340380. `--> du lib/*.pcm | awk -F ' ' '{sum += $1} END {print sum}'. 334932. ```,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2411
https://github.com/root-project/root/pull/2412:21,availability,cluster,clusters,21,"[TTree] Create event clusters when TTree is flushed.; Several of us were surprised to discover that calling `TTree::FlushBaskets()` does not actually create a new event cluster in terms of the TTree metadata -- even though it effectively creates an event cluster in terms of the physical disk layout! There may be cases where the user has special knowledge of the file where using the internally-calculated auto-flush intervals result in poor choices; CMS's NanoAOD is one such case. However, when CMS switched to calling `FlushBaskets` explicitly and disabling `AutoFlush`, this triggered unexpected behavior in `RDataFrame` as the file did not appear to have any proper event clusters. This change causes the `FlushBaskets` method to create an explicit event cluster. As we used the old behavior in some ROOT unit tests, the prior behavior is now accessible through a flag passed to the method. Fixes: ROOT-9442",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2412
https://github.com/root-project/root/pull/2412:169,availability,cluster,cluster,169,"[TTree] Create event clusters when TTree is flushed.; Several of us were surprised to discover that calling `TTree::FlushBaskets()` does not actually create a new event cluster in terms of the TTree metadata -- even though it effectively creates an event cluster in terms of the physical disk layout! There may be cases where the user has special knowledge of the file where using the internally-calculated auto-flush intervals result in poor choices; CMS's NanoAOD is one such case. However, when CMS switched to calling `FlushBaskets` explicitly and disabling `AutoFlush`, this triggered unexpected behavior in `RDataFrame` as the file did not appear to have any proper event clusters. This change causes the `FlushBaskets` method to create an explicit event cluster. As we used the old behavior in some ROOT unit tests, the prior behavior is now accessible through a flag passed to the method. Fixes: ROOT-9442",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2412
https://github.com/root-project/root/pull/2412:255,availability,cluster,cluster,255,"[TTree] Create event clusters when TTree is flushed.; Several of us were surprised to discover that calling `TTree::FlushBaskets()` does not actually create a new event cluster in terms of the TTree metadata -- even though it effectively creates an event cluster in terms of the physical disk layout! There may be cases where the user has special knowledge of the file where using the internally-calculated auto-flush intervals result in poor choices; CMS's NanoAOD is one such case. However, when CMS switched to calling `FlushBaskets` explicitly and disabling `AutoFlush`, this triggered unexpected behavior in `RDataFrame` as the file did not appear to have any proper event clusters. This change causes the `FlushBaskets` method to create an explicit event cluster. As we used the old behavior in some ROOT unit tests, the prior behavior is now accessible through a flag passed to the method. Fixes: ROOT-9442",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2412
https://github.com/root-project/root/pull/2412:678,availability,cluster,clusters,678,"[TTree] Create event clusters when TTree is flushed.; Several of us were surprised to discover that calling `TTree::FlushBaskets()` does not actually create a new event cluster in terms of the TTree metadata -- even though it effectively creates an event cluster in terms of the physical disk layout! There may be cases where the user has special knowledge of the file where using the internally-calculated auto-flush intervals result in poor choices; CMS's NanoAOD is one such case. However, when CMS switched to calling `FlushBaskets` explicitly and disabling `AutoFlush`, this triggered unexpected behavior in `RDataFrame` as the file did not appear to have any proper event clusters. This change causes the `FlushBaskets` method to create an explicit event cluster. As we used the old behavior in some ROOT unit tests, the prior behavior is now accessible through a flag passed to the method. Fixes: ROOT-9442",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2412
https://github.com/root-project/root/pull/2412:761,availability,cluster,cluster,761,"[TTree] Create event clusters when TTree is flushed.; Several of us were surprised to discover that calling `TTree::FlushBaskets()` does not actually create a new event cluster in terms of the TTree metadata -- even though it effectively creates an event cluster in terms of the physical disk layout! There may be cases where the user has special knowledge of the file where using the internally-calculated auto-flush intervals result in poor choices; CMS's NanoAOD is one such case. However, when CMS switched to calling `FlushBaskets` explicitly and disabling `AutoFlush`, this triggered unexpected behavior in `RDataFrame` as the file did not appear to have any proper event clusters. This change causes the `FlushBaskets` method to create an explicit event cluster. As we used the old behavior in some ROOT unit tests, the prior behavior is now accessible through a flag passed to the method. Fixes: ROOT-9442",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2412
https://github.com/root-project/root/pull/2412:21,deployability,cluster,clusters,21,"[TTree] Create event clusters when TTree is flushed.; Several of us were surprised to discover that calling `TTree::FlushBaskets()` does not actually create a new event cluster in terms of the TTree metadata -- even though it effectively creates an event cluster in terms of the physical disk layout! There may be cases where the user has special knowledge of the file where using the internally-calculated auto-flush intervals result in poor choices; CMS's NanoAOD is one such case. However, when CMS switched to calling `FlushBaskets` explicitly and disabling `AutoFlush`, this triggered unexpected behavior in `RDataFrame` as the file did not appear to have any proper event clusters. This change causes the `FlushBaskets` method to create an explicit event cluster. As we used the old behavior in some ROOT unit tests, the prior behavior is now accessible through a flag passed to the method. Fixes: ROOT-9442",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2412
https://github.com/root-project/root/pull/2412:169,deployability,cluster,cluster,169,"[TTree] Create event clusters when TTree is flushed.; Several of us were surprised to discover that calling `TTree::FlushBaskets()` does not actually create a new event cluster in terms of the TTree metadata -- even though it effectively creates an event cluster in terms of the physical disk layout! There may be cases where the user has special knowledge of the file where using the internally-calculated auto-flush intervals result in poor choices; CMS's NanoAOD is one such case. However, when CMS switched to calling `FlushBaskets` explicitly and disabling `AutoFlush`, this triggered unexpected behavior in `RDataFrame` as the file did not appear to have any proper event clusters. This change causes the `FlushBaskets` method to create an explicit event cluster. As we used the old behavior in some ROOT unit tests, the prior behavior is now accessible through a flag passed to the method. Fixes: ROOT-9442",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2412
https://github.com/root-project/root/pull/2412:255,deployability,cluster,cluster,255,"[TTree] Create event clusters when TTree is flushed.; Several of us were surprised to discover that calling `TTree::FlushBaskets()` does not actually create a new event cluster in terms of the TTree metadata -- even though it effectively creates an event cluster in terms of the physical disk layout! There may be cases where the user has special knowledge of the file where using the internally-calculated auto-flush intervals result in poor choices; CMS's NanoAOD is one such case. However, when CMS switched to calling `FlushBaskets` explicitly and disabling `AutoFlush`, this triggered unexpected behavior in `RDataFrame` as the file did not appear to have any proper event clusters. This change causes the `FlushBaskets` method to create an explicit event cluster. As we used the old behavior in some ROOT unit tests, the prior behavior is now accessible through a flag passed to the method. Fixes: ROOT-9442",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2412
https://github.com/root-project/root/pull/2412:678,deployability,cluster,clusters,678,"[TTree] Create event clusters when TTree is flushed.; Several of us were surprised to discover that calling `TTree::FlushBaskets()` does not actually create a new event cluster in terms of the TTree metadata -- even though it effectively creates an event cluster in terms of the physical disk layout! There may be cases where the user has special knowledge of the file where using the internally-calculated auto-flush intervals result in poor choices; CMS's NanoAOD is one such case. However, when CMS switched to calling `FlushBaskets` explicitly and disabling `AutoFlush`, this triggered unexpected behavior in `RDataFrame` as the file did not appear to have any proper event clusters. This change causes the `FlushBaskets` method to create an explicit event cluster. As we used the old behavior in some ROOT unit tests, the prior behavior is now accessible through a flag passed to the method. Fixes: ROOT-9442",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2412
https://github.com/root-project/root/pull/2412:761,deployability,cluster,cluster,761,"[TTree] Create event clusters when TTree is flushed.; Several of us were surprised to discover that calling `TTree::FlushBaskets()` does not actually create a new event cluster in terms of the TTree metadata -- even though it effectively creates an event cluster in terms of the physical disk layout! There may be cases where the user has special knowledge of the file where using the internally-calculated auto-flush intervals result in poor choices; CMS's NanoAOD is one such case. However, when CMS switched to calling `FlushBaskets` explicitly and disabling `AutoFlush`, this triggered unexpected behavior in `RDataFrame` as the file did not appear to have any proper event clusters. This change causes the `FlushBaskets` method to create an explicit event cluster. As we used the old behavior in some ROOT unit tests, the prior behavior is now accessible through a flag passed to the method. Fixes: ROOT-9442",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2412
https://github.com/root-project/root/pull/2412:15,integrability,event,event,15,"[TTree] Create event clusters when TTree is flushed.; Several of us were surprised to discover that calling `TTree::FlushBaskets()` does not actually create a new event cluster in terms of the TTree metadata -- even though it effectively creates an event cluster in terms of the physical disk layout! There may be cases where the user has special knowledge of the file where using the internally-calculated auto-flush intervals result in poor choices; CMS's NanoAOD is one such case. However, when CMS switched to calling `FlushBaskets` explicitly and disabling `AutoFlush`, this triggered unexpected behavior in `RDataFrame` as the file did not appear to have any proper event clusters. This change causes the `FlushBaskets` method to create an explicit event cluster. As we used the old behavior in some ROOT unit tests, the prior behavior is now accessible through a flag passed to the method. Fixes: ROOT-9442",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2412
https://github.com/root-project/root/pull/2412:86,integrability,discover,discover,86,"[TTree] Create event clusters when TTree is flushed.; Several of us were surprised to discover that calling `TTree::FlushBaskets()` does not actually create a new event cluster in terms of the TTree metadata -- even though it effectively creates an event cluster in terms of the physical disk layout! There may be cases where the user has special knowledge of the file where using the internally-calculated auto-flush intervals result in poor choices; CMS's NanoAOD is one such case. However, when CMS switched to calling `FlushBaskets` explicitly and disabling `AutoFlush`, this triggered unexpected behavior in `RDataFrame` as the file did not appear to have any proper event clusters. This change causes the `FlushBaskets` method to create an explicit event cluster. As we used the old behavior in some ROOT unit tests, the prior behavior is now accessible through a flag passed to the method. Fixes: ROOT-9442",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2412
https://github.com/root-project/root/pull/2412:163,integrability,event,event,163,"[TTree] Create event clusters when TTree is flushed.; Several of us were surprised to discover that calling `TTree::FlushBaskets()` does not actually create a new event cluster in terms of the TTree metadata -- even though it effectively creates an event cluster in terms of the physical disk layout! There may be cases where the user has special knowledge of the file where using the internally-calculated auto-flush intervals result in poor choices; CMS's NanoAOD is one such case. However, when CMS switched to calling `FlushBaskets` explicitly and disabling `AutoFlush`, this triggered unexpected behavior in `RDataFrame` as the file did not appear to have any proper event clusters. This change causes the `FlushBaskets` method to create an explicit event cluster. As we used the old behavior in some ROOT unit tests, the prior behavior is now accessible through a flag passed to the method. Fixes: ROOT-9442",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2412
https://github.com/root-project/root/pull/2412:249,integrability,event,event,249,"[TTree] Create event clusters when TTree is flushed.; Several of us were surprised to discover that calling `TTree::FlushBaskets()` does not actually create a new event cluster in terms of the TTree metadata -- even though it effectively creates an event cluster in terms of the physical disk layout! There may be cases where the user has special knowledge of the file where using the internally-calculated auto-flush intervals result in poor choices; CMS's NanoAOD is one such case. However, when CMS switched to calling `FlushBaskets` explicitly and disabling `AutoFlush`, this triggered unexpected behavior in `RDataFrame` as the file did not appear to have any proper event clusters. This change causes the `FlushBaskets` method to create an explicit event cluster. As we used the old behavior in some ROOT unit tests, the prior behavior is now accessible through a flag passed to the method. Fixes: ROOT-9442",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2412
https://github.com/root-project/root/pull/2412:672,integrability,event,event,672,"[TTree] Create event clusters when TTree is flushed.; Several of us were surprised to discover that calling `TTree::FlushBaskets()` does not actually create a new event cluster in terms of the TTree metadata -- even though it effectively creates an event cluster in terms of the physical disk layout! There may be cases where the user has special knowledge of the file where using the internally-calculated auto-flush intervals result in poor choices; CMS's NanoAOD is one such case. However, when CMS switched to calling `FlushBaskets` explicitly and disabling `AutoFlush`, this triggered unexpected behavior in `RDataFrame` as the file did not appear to have any proper event clusters. This change causes the `FlushBaskets` method to create an explicit event cluster. As we used the old behavior in some ROOT unit tests, the prior behavior is now accessible through a flag passed to the method. Fixes: ROOT-9442",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2412
https://github.com/root-project/root/pull/2412:755,integrability,event,event,755,"[TTree] Create event clusters when TTree is flushed.; Several of us were surprised to discover that calling `TTree::FlushBaskets()` does not actually create a new event cluster in terms of the TTree metadata -- even though it effectively creates an event cluster in terms of the physical disk layout! There may be cases where the user has special knowledge of the file where using the internally-calculated auto-flush intervals result in poor choices; CMS's NanoAOD is one such case. However, when CMS switched to calling `FlushBaskets` explicitly and disabling `AutoFlush`, this triggered unexpected behavior in `RDataFrame` as the file did not appear to have any proper event clusters. This change causes the `FlushBaskets` method to create an explicit event cluster. As we used the old behavior in some ROOT unit tests, the prior behavior is now accessible through a flag passed to the method. Fixes: ROOT-9442",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2412
https://github.com/root-project/root/pull/2412:86,interoperability,discover,discover,86,"[TTree] Create event clusters when TTree is flushed.; Several of us were surprised to discover that calling `TTree::FlushBaskets()` does not actually create a new event cluster in terms of the TTree metadata -- even though it effectively creates an event cluster in terms of the physical disk layout! There may be cases where the user has special knowledge of the file where using the internally-calculated auto-flush intervals result in poor choices; CMS's NanoAOD is one such case. However, when CMS switched to calling `FlushBaskets` explicitly and disabling `AutoFlush`, this triggered unexpected behavior in `RDataFrame` as the file did not appear to have any proper event clusters. This change causes the `FlushBaskets` method to create an explicit event cluster. As we used the old behavior in some ROOT unit tests, the prior behavior is now accessible through a flag passed to the method. Fixes: ROOT-9442",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2412
https://github.com/root-project/root/pull/2412:288,performance,disk,disk,288,"[TTree] Create event clusters when TTree is flushed.; Several of us were surprised to discover that calling `TTree::FlushBaskets()` does not actually create a new event cluster in terms of the TTree metadata -- even though it effectively creates an event cluster in terms of the physical disk layout! There may be cases where the user has special knowledge of the file where using the internally-calculated auto-flush intervals result in poor choices; CMS's NanoAOD is one such case. However, when CMS switched to calling `FlushBaskets` explicitly and disabling `AutoFlush`, this triggered unexpected behavior in `RDataFrame` as the file did not appear to have any proper event clusters. This change causes the `FlushBaskets` method to create an explicit event cluster. As we used the old behavior in some ROOT unit tests, the prior behavior is now accessible through a flag passed to the method. Fixes: ROOT-9442",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2412
https://github.com/root-project/root/pull/2412:132,reliability,doe,does,132,"[TTree] Create event clusters when TTree is flushed.; Several of us were surprised to discover that calling `TTree::FlushBaskets()` does not actually create a new event cluster in terms of the TTree metadata -- even though it effectively creates an event cluster in terms of the physical disk layout! There may be cases where the user has special knowledge of the file where using the internally-calculated auto-flush intervals result in poor choices; CMS's NanoAOD is one such case. However, when CMS switched to calling `FlushBaskets` explicitly and disabling `AutoFlush`, this triggered unexpected behavior in `RDataFrame` as the file did not appear to have any proper event clusters. This change causes the `FlushBaskets` method to create an explicit event cluster. As we used the old behavior in some ROOT unit tests, the prior behavior is now accessible through a flag passed to the method. Fixes: ROOT-9442",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2412
https://github.com/root-project/root/pull/2412:816,safety,test,tests,816,"[TTree] Create event clusters when TTree is flushed.; Several of us were surprised to discover that calling `TTree::FlushBaskets()` does not actually create a new event cluster in terms of the TTree metadata -- even though it effectively creates an event cluster in terms of the physical disk layout! There may be cases where the user has special knowledge of the file where using the internally-calculated auto-flush intervals result in poor choices; CMS's NanoAOD is one such case. However, when CMS switched to calling `FlushBaskets` explicitly and disabling `AutoFlush`, this triggered unexpected behavior in `RDataFrame` as the file did not appear to have any proper event clusters. This change causes the `FlushBaskets` method to create an explicit event cluster. As we used the old behavior in some ROOT unit tests, the prior behavior is now accessible through a flag passed to the method. Fixes: ROOT-9442",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2412
https://github.com/root-project/root/pull/2412:849,security,access,accessible,849,"[TTree] Create event clusters when TTree is flushed.; Several of us were surprised to discover that calling `TTree::FlushBaskets()` does not actually create a new event cluster in terms of the TTree metadata -- even though it effectively creates an event cluster in terms of the physical disk layout! There may be cases where the user has special knowledge of the file where using the internally-calculated auto-flush intervals result in poor choices; CMS's NanoAOD is one such case. However, when CMS switched to calling `FlushBaskets` explicitly and disabling `AutoFlush`, this triggered unexpected behavior in `RDataFrame` as the file did not appear to have any proper event clusters. This change causes the `FlushBaskets` method to create an explicit event cluster. As we used the old behavior in some ROOT unit tests, the prior behavior is now accessible through a flag passed to the method. Fixes: ROOT-9442",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2412
https://github.com/root-project/root/pull/2412:811,testability,unit,unit,811,"[TTree] Create event clusters when TTree is flushed.; Several of us were surprised to discover that calling `TTree::FlushBaskets()` does not actually create a new event cluster in terms of the TTree metadata -- even though it effectively creates an event cluster in terms of the physical disk layout! There may be cases where the user has special knowledge of the file where using the internally-calculated auto-flush intervals result in poor choices; CMS's NanoAOD is one such case. However, when CMS switched to calling `FlushBaskets` explicitly and disabling `AutoFlush`, this triggered unexpected behavior in `RDataFrame` as the file did not appear to have any proper event clusters. This change causes the `FlushBaskets` method to create an explicit event cluster. As we used the old behavior in some ROOT unit tests, the prior behavior is now accessible through a flag passed to the method. Fixes: ROOT-9442",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2412
https://github.com/root-project/root/pull/2412:816,testability,test,tests,816,"[TTree] Create event clusters when TTree is flushed.; Several of us were surprised to discover that calling `TTree::FlushBaskets()` does not actually create a new event cluster in terms of the TTree metadata -- even though it effectively creates an event cluster in terms of the physical disk layout! There may be cases where the user has special knowledge of the file where using the internally-calculated auto-flush intervals result in poor choices; CMS's NanoAOD is one such case. However, when CMS switched to calling `FlushBaskets` explicitly and disabling `AutoFlush`, this triggered unexpected behavior in `RDataFrame` as the file did not appear to have any proper event clusters. This change causes the `FlushBaskets` method to create an explicit event cluster. As we used the old behavior in some ROOT unit tests, the prior behavior is now accessible through a flag passed to the method. Fixes: ROOT-9442",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2412
https://github.com/root-project/root/pull/2412:86,usability,discov,discover,86,"[TTree] Create event clusters when TTree is flushed.; Several of us were surprised to discover that calling `TTree::FlushBaskets()` does not actually create a new event cluster in terms of the TTree metadata -- even though it effectively creates an event cluster in terms of the physical disk layout! There may be cases where the user has special knowledge of the file where using the internally-calculated auto-flush intervals result in poor choices; CMS's NanoAOD is one such case. However, when CMS switched to calling `FlushBaskets` explicitly and disabling `AutoFlush`, this triggered unexpected behavior in `RDataFrame` as the file did not appear to have any proper event clusters. This change causes the `FlushBaskets` method to create an explicit event cluster. As we used the old behavior in some ROOT unit tests, the prior behavior is now accessible through a flag passed to the method. Fixes: ROOT-9442",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2412
https://github.com/root-project/root/pull/2412:226,usability,effectiv,effectively,226,"[TTree] Create event clusters when TTree is flushed.; Several of us were surprised to discover that calling `TTree::FlushBaskets()` does not actually create a new event cluster in terms of the TTree metadata -- even though it effectively creates an event cluster in terms of the physical disk layout! There may be cases where the user has special knowledge of the file where using the internally-calculated auto-flush intervals result in poor choices; CMS's NanoAOD is one such case. However, when CMS switched to calling `FlushBaskets` explicitly and disabling `AutoFlush`, this triggered unexpected behavior in `RDataFrame` as the file did not appear to have any proper event clusters. This change causes the `FlushBaskets` method to create an explicit event cluster. As we used the old behavior in some ROOT unit tests, the prior behavior is now accessible through a flag passed to the method. Fixes: ROOT-9442",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2412
https://github.com/root-project/root/pull/2412:330,usability,user,user,330,"[TTree] Create event clusters when TTree is flushed.; Several of us were surprised to discover that calling `TTree::FlushBaskets()` does not actually create a new event cluster in terms of the TTree metadata -- even though it effectively creates an event cluster in terms of the physical disk layout! There may be cases where the user has special knowledge of the file where using the internally-calculated auto-flush intervals result in poor choices; CMS's NanoAOD is one such case. However, when CMS switched to calling `FlushBaskets` explicitly and disabling `AutoFlush`, this triggered unexpected behavior in `RDataFrame` as the file did not appear to have any proper event clusters. This change causes the `FlushBaskets` method to create an explicit event cluster. As we used the old behavior in some ROOT unit tests, the prior behavior is now accessible through a flag passed to the method. Fixes: ROOT-9442",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2412
https://github.com/root-project/root/pull/2412:601,usability,behavi,behavior,601,"[TTree] Create event clusters when TTree is flushed.; Several of us were surprised to discover that calling `TTree::FlushBaskets()` does not actually create a new event cluster in terms of the TTree metadata -- even though it effectively creates an event cluster in terms of the physical disk layout! There may be cases where the user has special knowledge of the file where using the internally-calculated auto-flush intervals result in poor choices; CMS's NanoAOD is one such case. However, when CMS switched to calling `FlushBaskets` explicitly and disabling `AutoFlush`, this triggered unexpected behavior in `RDataFrame` as the file did not appear to have any proper event clusters. This change causes the `FlushBaskets` method to create an explicit event cluster. As we used the old behavior in some ROOT unit tests, the prior behavior is now accessible through a flag passed to the method. Fixes: ROOT-9442",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2412
https://github.com/root-project/root/pull/2412:789,usability,behavi,behavior,789,"[TTree] Create event clusters when TTree is flushed.; Several of us were surprised to discover that calling `TTree::FlushBaskets()` does not actually create a new event cluster in terms of the TTree metadata -- even though it effectively creates an event cluster in terms of the physical disk layout! There may be cases where the user has special knowledge of the file where using the internally-calculated auto-flush intervals result in poor choices; CMS's NanoAOD is one such case. However, when CMS switched to calling `FlushBaskets` explicitly and disabling `AutoFlush`, this triggered unexpected behavior in `RDataFrame` as the file did not appear to have any proper event clusters. This change causes the `FlushBaskets` method to create an explicit event cluster. As we used the old behavior in some ROOT unit tests, the prior behavior is now accessible through a flag passed to the method. Fixes: ROOT-9442",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2412
https://github.com/root-project/root/pull/2412:833,usability,behavi,behavior,833,"[TTree] Create event clusters when TTree is flushed.; Several of us were surprised to discover that calling `TTree::FlushBaskets()` does not actually create a new event cluster in terms of the TTree metadata -- even though it effectively creates an event cluster in terms of the physical disk layout! There may be cases where the user has special knowledge of the file where using the internally-calculated auto-flush intervals result in poor choices; CMS's NanoAOD is one such case. However, when CMS switched to calling `FlushBaskets` explicitly and disabling `AutoFlush`, this triggered unexpected behavior in `RDataFrame` as the file did not appear to have any proper event clusters. This change causes the `FlushBaskets` method to create an explicit event cluster. As we used the old behavior in some ROOT unit tests, the prior behavior is now accessible through a flag passed to the method. Fixes: ROOT-9442",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2412
https://github.com/root-project/root/pull/2413:25,availability,operat,operator,25,Extended return value of operator* to TGeoHMatrix. Added Multiply tak…; This promotes the result of operator* for matrices to TGeoHMatrix to be able to handle properly translation * rotation. Added Multiply(Left) methods taking const matrix reference.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2413
https://github.com/root-project/root/pull/2413:100,availability,operat,operator,100,Extended return value of operator* to TGeoHMatrix. Added Multiply tak…; This promotes the result of operator* for matrices to TGeoHMatrix to be able to handle properly translation * rotation. Added Multiply(Left) methods taking const matrix reference.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2413
https://github.com/root-project/root/pull/2413:168,integrability,translat,translation,168,Extended return value of operator* to TGeoHMatrix. Added Multiply tak…; This promotes the result of operator* for matrices to TGeoHMatrix to be able to handle properly translation * rotation. Added Multiply(Left) methods taking const matrix reference.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2413
https://github.com/root-project/root/pull/2413:168,interoperability,translat,translation,168,Extended return value of operator* to TGeoHMatrix. Added Multiply tak…; This promotes the result of operator* for matrices to TGeoHMatrix to be able to handle properly translation * rotation. Added Multiply(Left) methods taking const matrix reference.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2413
https://github.com/root-project/root/pull/2413:0,modifiability,Exten,Extended,0,Extended return value of operator* to TGeoHMatrix. Added Multiply tak…; This promotes the result of operator* for matrices to TGeoHMatrix to be able to handle properly translation * rotation. Added Multiply(Left) methods taking const matrix reference.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2413
https://github.com/root-project/root/pull/2413:182,security,rotat,rotation,182,Extended return value of operator* to TGeoHMatrix. Added Multiply tak…; This promotes the result of operator* for matrices to TGeoHMatrix to be able to handle properly translation * rotation. Added Multiply(Left) methods taking const matrix reference.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2413
https://github.com/root-project/root/pull/2414:25,availability,operat,operator,25,Extended return value of operator* to TGeoHMatrix. Added Multiply tak…; …ing const reference. (cherry picked from commit af55b3e542e647d7b91a23d0880f69021e02a882),MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2414
https://github.com/root-project/root/pull/2414:0,modifiability,Exten,Extended,0,Extended return value of operator* to TGeoHMatrix. Added Multiply tak…; …ing const reference. (cherry picked from commit af55b3e542e647d7b91a23d0880f69021e02a882),MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2414
https://github.com/root-project/root/pull/2415:25,availability,operat,operator,25,Extended return value of operator* to TGeoHMatrix. Added Multiply tak…; …ing const reference. (cherry picked from commit af55b3e542e647d7b91a23d0880f69021e02a882),MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2415
https://github.com/root-project/root/pull/2415:0,modifiability,Exten,Extended,0,Extended return value of operator* to TGeoHMatrix. Added Multiply tak…; …ing const reference. (cherry picked from commit af55b3e542e647d7b91a23d0880f69021e02a882),MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2415
https://github.com/root-project/root/pull/2416:23,safety,test,test,23,"Fix dataframe_snapshot test on Windows; On Windows, one cannot delete a file while used by a process, so use different file names and make sure they are deleted beforehand",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2416
https://github.com/root-project/root/pull/2416:23,testability,test,test,23,"Fix dataframe_snapshot test on Windows; On Windows, one cannot delete a file while used by a process, so use different file names and make sure they are deleted beforehand",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2416
https://github.com/root-project/root/pull/2417:56,deployability,fail,fails,56,Try to fix PyMVA unit tests; - 3 PyMVA Multiclass tests fails cause they are opening `./tmva_example_multiple_background.root` which doesn't contain required input TTree objects. - As a dirty workaround I've changed input file path to be `../../../runtutorials/tmva_example_multiple_background.root` to use same tree filled by `runtutorials` unit-tests.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2417
https://github.com/root-project/root/pull/2417:141,deployability,contain,contain,141,Try to fix PyMVA unit tests; - 3 PyMVA Multiclass tests fails cause they are opening `./tmva_example_multiple_background.root` which doesn't contain required input TTree objects. - As a dirty workaround I've changed input file path to be `../../../runtutorials/tmva_example_multiple_background.root` to use same tree filled by `runtutorials` unit-tests.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2417
https://github.com/root-project/root/pull/2417:56,reliability,fail,fails,56,Try to fix PyMVA unit tests; - 3 PyMVA Multiclass tests fails cause they are opening `./tmva_example_multiple_background.root` which doesn't contain required input TTree objects. - As a dirty workaround I've changed input file path to be `../../../runtutorials/tmva_example_multiple_background.root` to use same tree filled by `runtutorials` unit-tests.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2417
https://github.com/root-project/root/pull/2417:133,reliability,doe,doesn,133,Try to fix PyMVA unit tests; - 3 PyMVA Multiclass tests fails cause they are opening `./tmva_example_multiple_background.root` which doesn't contain required input TTree objects. - As a dirty workaround I've changed input file path to be `../../../runtutorials/tmva_example_multiple_background.root` to use same tree filled by `runtutorials` unit-tests.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2417
https://github.com/root-project/root/pull/2417:22,safety,test,tests,22,Try to fix PyMVA unit tests; - 3 PyMVA Multiclass tests fails cause they are opening `./tmva_example_multiple_background.root` which doesn't contain required input TTree objects. - As a dirty workaround I've changed input file path to be `../../../runtutorials/tmva_example_multiple_background.root` to use same tree filled by `runtutorials` unit-tests.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2417
https://github.com/root-project/root/pull/2417:50,safety,test,tests,50,Try to fix PyMVA unit tests; - 3 PyMVA Multiclass tests fails cause they are opening `./tmva_example_multiple_background.root` which doesn't contain required input TTree objects. - As a dirty workaround I've changed input file path to be `../../../runtutorials/tmva_example_multiple_background.root` to use same tree filled by `runtutorials` unit-tests.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2417
https://github.com/root-project/root/pull/2417:158,safety,input,input,158,Try to fix PyMVA unit tests; - 3 PyMVA Multiclass tests fails cause they are opening `./tmva_example_multiple_background.root` which doesn't contain required input TTree objects. - As a dirty workaround I've changed input file path to be `../../../runtutorials/tmva_example_multiple_background.root` to use same tree filled by `runtutorials` unit-tests.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2417
https://github.com/root-project/root/pull/2417:216,safety,input,input,216,Try to fix PyMVA unit tests; - 3 PyMVA Multiclass tests fails cause they are opening `./tmva_example_multiple_background.root` which doesn't contain required input TTree objects. - As a dirty workaround I've changed input file path to be `../../../runtutorials/tmva_example_multiple_background.root` to use same tree filled by `runtutorials` unit-tests.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2417
https://github.com/root-project/root/pull/2417:347,safety,test,tests,347,Try to fix PyMVA unit tests; - 3 PyMVA Multiclass tests fails cause they are opening `./tmva_example_multiple_background.root` which doesn't contain required input TTree objects. - As a dirty workaround I've changed input file path to be `../../../runtutorials/tmva_example_multiple_background.root` to use same tree filled by `runtutorials` unit-tests.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2417
https://github.com/root-project/root/pull/2417:17,testability,unit,unit,17,Try to fix PyMVA unit tests; - 3 PyMVA Multiclass tests fails cause they are opening `./tmva_example_multiple_background.root` which doesn't contain required input TTree objects. - As a dirty workaround I've changed input file path to be `../../../runtutorials/tmva_example_multiple_background.root` to use same tree filled by `runtutorials` unit-tests.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2417
https://github.com/root-project/root/pull/2417:22,testability,test,tests,22,Try to fix PyMVA unit tests; - 3 PyMVA Multiclass tests fails cause they are opening `./tmva_example_multiple_background.root` which doesn't contain required input TTree objects. - As a dirty workaround I've changed input file path to be `../../../runtutorials/tmva_example_multiple_background.root` to use same tree filled by `runtutorials` unit-tests.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2417
https://github.com/root-project/root/pull/2417:50,testability,test,tests,50,Try to fix PyMVA unit tests; - 3 PyMVA Multiclass tests fails cause they are opening `./tmva_example_multiple_background.root` which doesn't contain required input TTree objects. - As a dirty workaround I've changed input file path to be `../../../runtutorials/tmva_example_multiple_background.root` to use same tree filled by `runtutorials` unit-tests.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2417
https://github.com/root-project/root/pull/2417:342,testability,unit,unit-tests,342,Try to fix PyMVA unit tests; - 3 PyMVA Multiclass tests fails cause they are opening `./tmva_example_multiple_background.root` which doesn't contain required input TTree objects. - As a dirty workaround I've changed input file path to be `../../../runtutorials/tmva_example_multiple_background.root` to use same tree filled by `runtutorials` unit-tests.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2417
https://github.com/root-project/root/pull/2417:158,usability,input,input,158,Try to fix PyMVA unit tests; - 3 PyMVA Multiclass tests fails cause they are opening `./tmva_example_multiple_background.root` which doesn't contain required input TTree objects. - As a dirty workaround I've changed input file path to be `../../../runtutorials/tmva_example_multiple_background.root` to use same tree filled by `runtutorials` unit-tests.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2417
https://github.com/root-project/root/pull/2417:216,usability,input,input,216,Try to fix PyMVA unit tests; - 3 PyMVA Multiclass tests fails cause they are opening `./tmva_example_multiple_background.root` which doesn't contain required input TTree objects. - As a dirty workaround I've changed input file path to be `../../../runtutorials/tmva_example_multiple_background.root` to use same tree filled by `runtutorials` unit-tests.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2417
https://github.com/root-project/root/pull/2418:20,usability,behavi,behavior,20,Comment on windows' behavior for TSystem::Unlink;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2418
https://github.com/root-project/root/pull/2419:26,usability,behavi,behavior,26,[DOC] Comment on windows' behavior for TSystem::Unlink;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2419
https://github.com/root-project/root/pull/2420:4,performance,memor,memory,4,Fix memory leak in GetStreamInfoListImpl (working with make_unique backport); This is the same PR than #2361 but with a fixed backport of `std::make_unique` for the case `std::make_unique<T[]>(N)` and c++11.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2420
https://github.com/root-project/root/pull/2420:4,usability,memor,memory,4,Fix memory leak in GetStreamInfoListImpl (working with make_unique backport); This is the same PR than #2361 but with a fixed backport of `std::make_unique` for the case `std::make_unique<T[]>(N)` and c++11.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2420
https://github.com/root-project/root/pull/2421:211,deployability,patch,patch,211,[cxxmodules] Check correctly if the decl was cached.; Rootcling always decided to take the most expensive path and make a. lookup even in the cases where it already has found the declaration. it looks for. This patch speeds up visibly rootcling dictionary generation time. and reduces the pcm size from 340MB to 165MB on my machine. It also. reduces the rss memory usage from 350MB to 250MB for ROOT with. runtime_cxxmodules. Patch by Axel Naumann(@Axel-Naumann) and me!,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2421
https://github.com/root-project/root/pull/2421:426,deployability,Patch,Patch,426,[cxxmodules] Check correctly if the decl was cached.; Rootcling always decided to take the most expensive path and make a. lookup even in the cases where it already has found the declaration. it looks for. This patch speeds up visibly rootcling dictionary generation time. and reduces the pcm size from 340MB to 165MB on my machine. It also. reduces the rss memory usage from 350MB to 250MB for ROOT with. runtime_cxxmodules. Patch by Axel Naumann(@Axel-Naumann) and me!,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2421
https://github.com/root-project/root/pull/2421:277,energy efficiency,reduc,reduces,277,[cxxmodules] Check correctly if the decl was cached.; Rootcling always decided to take the most expensive path and make a. lookup even in the cases where it already has found the declaration. it looks for. This patch speeds up visibly rootcling dictionary generation time. and reduces the pcm size from 340MB to 165MB on my machine. It also. reduces the rss memory usage from 350MB to 250MB for ROOT with. runtime_cxxmodules. Patch by Axel Naumann(@Axel-Naumann) and me!,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2421
https://github.com/root-project/root/pull/2421:342,energy efficiency,reduc,reduces,342,[cxxmodules] Check correctly if the decl was cached.; Rootcling always decided to take the most expensive path and make a. lookup even in the cases where it already has found the declaration. it looks for. This patch speeds up visibly rootcling dictionary generation time. and reduces the pcm size from 340MB to 165MB on my machine. It also. reduces the rss memory usage from 350MB to 250MB for ROOT with. runtime_cxxmodules. Patch by Axel Naumann(@Axel-Naumann) and me!,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2421
https://github.com/root-project/root/pull/2421:45,performance,cach,cached,45,[cxxmodules] Check correctly if the decl was cached.; Rootcling always decided to take the most expensive path and make a. lookup even in the cases where it already has found the declaration. it looks for. This patch speeds up visibly rootcling dictionary generation time. and reduces the pcm size from 340MB to 165MB on my machine. It also. reduces the rss memory usage from 350MB to 250MB for ROOT with. runtime_cxxmodules. Patch by Axel Naumann(@Axel-Naumann) and me!,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2421
https://github.com/root-project/root/pull/2421:267,performance,time,time,267,[cxxmodules] Check correctly if the decl was cached.; Rootcling always decided to take the most expensive path and make a. lookup even in the cases where it already has found the declaration. it looks for. This patch speeds up visibly rootcling dictionary generation time. and reduces the pcm size from 340MB to 165MB on my machine. It also. reduces the rss memory usage from 350MB to 250MB for ROOT with. runtime_cxxmodules. Patch by Axel Naumann(@Axel-Naumann) and me!,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2421
https://github.com/root-project/root/pull/2421:358,performance,memor,memory,358,[cxxmodules] Check correctly if the decl was cached.; Rootcling always decided to take the most expensive path and make a. lookup even in the cases where it already has found the declaration. it looks for. This patch speeds up visibly rootcling dictionary generation time. and reduces the pcm size from 340MB to 165MB on my machine. It also. reduces the rss memory usage from 350MB to 250MB for ROOT with. runtime_cxxmodules. Patch by Axel Naumann(@Axel-Naumann) and me!,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2421
https://github.com/root-project/root/pull/2421:211,safety,patch,patch,211,[cxxmodules] Check correctly if the decl was cached.; Rootcling always decided to take the most expensive path and make a. lookup even in the cases where it already has found the declaration. it looks for. This patch speeds up visibly rootcling dictionary generation time. and reduces the pcm size from 340MB to 165MB on my machine. It also. reduces the rss memory usage from 350MB to 250MB for ROOT with. runtime_cxxmodules. Patch by Axel Naumann(@Axel-Naumann) and me!,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2421
https://github.com/root-project/root/pull/2421:426,safety,Patch,Patch,426,[cxxmodules] Check correctly if the decl was cached.; Rootcling always decided to take the most expensive path and make a. lookup even in the cases where it already has found the declaration. it looks for. This patch speeds up visibly rootcling dictionary generation time. and reduces the pcm size from 340MB to 165MB on my machine. It also. reduces the rss memory usage from 350MB to 250MB for ROOT with. runtime_cxxmodules. Patch by Axel Naumann(@Axel-Naumann) and me!,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2421
https://github.com/root-project/root/pull/2421:211,security,patch,patch,211,[cxxmodules] Check correctly if the decl was cached.; Rootcling always decided to take the most expensive path and make a. lookup even in the cases where it already has found the declaration. it looks for. This patch speeds up visibly rootcling dictionary generation time. and reduces the pcm size from 340MB to 165MB on my machine. It also. reduces the rss memory usage from 350MB to 250MB for ROOT with. runtime_cxxmodules. Patch by Axel Naumann(@Axel-Naumann) and me!,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2421
https://github.com/root-project/root/pull/2421:426,security,Patch,Patch,426,[cxxmodules] Check correctly if the decl was cached.; Rootcling always decided to take the most expensive path and make a. lookup even in the cases where it already has found the declaration. it looks for. This patch speeds up visibly rootcling dictionary generation time. and reduces the pcm size from 340MB to 165MB on my machine. It also. reduces the rss memory usage from 350MB to 250MB for ROOT with. runtime_cxxmodules. Patch by Axel Naumann(@Axel-Naumann) and me!,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2421
https://github.com/root-project/root/pull/2421:358,usability,memor,memory,358,[cxxmodules] Check correctly if the decl was cached.; Rootcling always decided to take the most expensive path and make a. lookup even in the cases where it already has found the declaration. it looks for. This patch speeds up visibly rootcling dictionary generation time. and reduces the pcm size from 340MB to 165MB on my machine. It also. reduces the rss memory usage from 350MB to 250MB for ROOT with. runtime_cxxmodules. Patch by Axel Naumann(@Axel-Naumann) and me!,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2421
https://github.com/root-project/root/pull/2422:129,deployability,updat,updated,129,"Fix memory leak in GetStreamInfoListImpl (using std::vector as buffer); Sorry for spamming PRs! #2420 Implements the fix with an updated backport of `std::make_unique`. However, a solution with `std::vector` is probably more feasible.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2422
https://github.com/root-project/root/pull/2422:63,integrability,buffer,buffer,63,"Fix memory leak in GetStreamInfoListImpl (using std::vector as buffer); Sorry for spamming PRs! #2420 Implements the fix with an updated backport of `std::make_unique`. However, a solution with `std::vector` is probably more feasible.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2422
https://github.com/root-project/root/pull/2422:4,performance,memor,memory,4,"Fix memory leak in GetStreamInfoListImpl (using std::vector as buffer); Sorry for spamming PRs! #2420 Implements the fix with an updated backport of `std::make_unique`. However, a solution with `std::vector` is probably more feasible.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2422
https://github.com/root-project/root/pull/2422:129,safety,updat,updated,129,"Fix memory leak in GetStreamInfoListImpl (using std::vector as buffer); Sorry for spamming PRs! #2420 Implements the fix with an updated backport of `std::make_unique`. However, a solution with `std::vector` is probably more feasible.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2422
https://github.com/root-project/root/pull/2422:129,security,updat,updated,129,"Fix memory leak in GetStreamInfoListImpl (using std::vector as buffer); Sorry for spamming PRs! #2420 Implements the fix with an updated backport of `std::make_unique`. However, a solution with `std::vector` is probably more feasible.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2422
https://github.com/root-project/root/pull/2422:4,usability,memor,memory,4,"Fix memory leak in GetStreamInfoListImpl (using std::vector as buffer); Sorry for spamming PRs! #2420 Implements the fix with an updated backport of `std::make_unique`. However, a solution with `std::vector` is probably more feasible.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2422
https://github.com/root-project/root/pull/2424:1,deployability,build,buildsystem,1,[buildsystem] Removing globing of headers and source files for Net lib;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2424
https://github.com/root-project/root/pull/2425:1,deployability,build,buildsystem,1,[buildsystem] Fixing path to source files for ROOTVecOps library;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2425
https://github.com/root-project/root/pull/2426:24,modifiability,variab,variable,24,[DF][NFC] Remove unused variable in test;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2426
https://github.com/root-project/root/pull/2426:36,safety,test,test,36,[DF][NFC] Remove unused variable in test;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2426
https://github.com/root-project/root/pull/2426:36,testability,test,test,36,[DF][NFC] Remove unused variable in test;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2426
https://github.com/root-project/root/pull/2427:279,reliability,Doe,Does,279,Add std::make_unique backport supporting std::make_unique<T[]>(N); Improved backport of `std::make_unique` supporting the `std::make_unique<T[]>(N)` case. Code is taken from the original proposal of the feature [here](https://isocpp.org/files/papers/N3656.txt). To be discussed: Does this violate any licence? @Axel-Naumann Do you know whether we can take the code?,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2427
https://github.com/root-project/root/pull/2427:226,security,iso,isocpp,226,Add std::make_unique backport supporting std::make_unique<T[]>(N); Improved backport of `std::make_unique` supporting the `std::make_unique<T[]>(N)` case. Code is taken from the original proposal of the feature [here](https://isocpp.org/files/papers/N3656.txt). To be discussed: Does this violate any licence? @Axel-Naumann Do you know whether we can take the code?,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2427
https://github.com/root-project/root/pull/2427:30,usability,support,supporting,30,Add std::make_unique backport supporting std::make_unique<T[]>(N); Improved backport of `std::make_unique` supporting the `std::make_unique<T[]>(N)` case. Code is taken from the original proposal of the feature [here](https://isocpp.org/files/papers/N3656.txt). To be discussed: Does this violate any licence? @Axel-Naumann Do you know whether we can take the code?,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2427
https://github.com/root-project/root/pull/2427:107,usability,support,supporting,107,Add std::make_unique backport supporting std::make_unique<T[]>(N); Improved backport of `std::make_unique` supporting the `std::make_unique<T[]>(N)` case. Code is taken from the original proposal of the feature [here](https://isocpp.org/files/papers/N3656.txt). To be discussed: Does this violate any licence? @Axel-Naumann Do you know whether we can take the code?,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2427
https://github.com/root-project/root/pull/2428:11,deployability,build,build,11,Testing PR build.;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2428
https://github.com/root-project/root/pull/2428:0,safety,Test,Testing,0,Testing PR build.;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2428
https://github.com/root-project/root/pull/2428:0,testability,Test,Testing,0,Testing PR build.;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2428
https://github.com/root-project/root/pull/2429:11,deployability,build,build,11,Testing PR build.;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2429
https://github.com/root-project/root/pull/2429:0,safety,Test,Testing,0,Testing PR build.;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2429
https://github.com/root-project/root/pull/2429:0,testability,Test,Testing,0,Testing PR build.;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2429
https://github.com/root-project/root/pull/2431:11,deployability,build,build,11,Testing PR build.;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2431
https://github.com/root-project/root/pull/2431:0,safety,Test,Testing,0,Testing PR build.;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2431
https://github.com/root-project/root/pull/2431:0,testability,Test,Testing,0,Testing PR build.;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2431
https://github.com/root-project/root/pull/2432:11,deployability,build,build,11,Testing PR build.;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2432
https://github.com/root-project/root/pull/2432:0,safety,Test,Testing,0,Testing PR build.;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2432
https://github.com/root-project/root/pull/2432:0,testability,Test,Testing,0,Testing PR build.;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2432
https://github.com/root-project/root/pull/2433:203,energy efficiency,alloc,allocator,203,"[VecOps] Fix RAdoptAllocator<bool>; This PR fixes ROOT-9453 (""Cannot instantiate RVec<bool>"") and some related issues. The different semantics of std::vector<bool> make memory adoption through a. custom allocator more complex -- namely, `RAdoptAllocator<bool>` must be. rebindable to `RAdoptAllocator<unsigned long>`, but if adopted memory is. really a buffer of bools, reinterpretation of the adopted buffer as a. different type is going to break things in horrible ways. As a workaround, `RAdoptAllocator<bool>` is specialized to be a simple. allocator that forwards calls to `std::allocator`, never adopts memory, and. can be rebound to any other allocator (as it never adopts memory it can. rebind the same way that `std::allocator` can). Note that this is not enough to support `RVec<bool>` columns in `RDataFrame`, for two main reasons:. * `TTreeReaderArray<bool>` is broken, see [ROOT-9570](https://sft.its.cern.ch/jira/browse/ROOT-9570). * `RVec<bool>` cannot be constructed from a `(bool *, std::size_t)` pair and cannot adopt memory (because `RAdoptAllocator<bool>` cannot adopt memory, because of the problems mentioned above) . So `RDataFrame` would still have to treat `RVec<bool>` differently:. * use a `TTreeReaderValue<std::vector<bool>>` to read the data from disk correctly. * copy the data into an `RVec<bool>`, without relying on memory adoption. These changes might be added by a future PR as they are outside of the scope of ROOT-9453 (""Cannot instantiate RVec<bool>""), _if_ this proposed solution to ROOT-9453 is accepted.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2433
https://github.com/root-project/root/pull/2433:545,energy efficiency,alloc,allocator,545,"[VecOps] Fix RAdoptAllocator<bool>; This PR fixes ROOT-9453 (""Cannot instantiate RVec<bool>"") and some related issues. The different semantics of std::vector<bool> make memory adoption through a. custom allocator more complex -- namely, `RAdoptAllocator<bool>` must be. rebindable to `RAdoptAllocator<unsigned long>`, but if adopted memory is. really a buffer of bools, reinterpretation of the adopted buffer as a. different type is going to break things in horrible ways. As a workaround, `RAdoptAllocator<bool>` is specialized to be a simple. allocator that forwards calls to `std::allocator`, never adopts memory, and. can be rebound to any other allocator (as it never adopts memory it can. rebind the same way that `std::allocator` can). Note that this is not enough to support `RVec<bool>` columns in `RDataFrame`, for two main reasons:. * `TTreeReaderArray<bool>` is broken, see [ROOT-9570](https://sft.its.cern.ch/jira/browse/ROOT-9570). * `RVec<bool>` cannot be constructed from a `(bool *, std::size_t)` pair and cannot adopt memory (because `RAdoptAllocator<bool>` cannot adopt memory, because of the problems mentioned above) . So `RDataFrame` would still have to treat `RVec<bool>` differently:. * use a `TTreeReaderValue<std::vector<bool>>` to read the data from disk correctly. * copy the data into an `RVec<bool>`, without relying on memory adoption. These changes might be added by a future PR as they are outside of the scope of ROOT-9453 (""Cannot instantiate RVec<bool>""), _if_ this proposed solution to ROOT-9453 is accepted.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2433
https://github.com/root-project/root/pull/2433:584,energy efficiency,alloc,allocator,584,"[VecOps] Fix RAdoptAllocator<bool>; This PR fixes ROOT-9453 (""Cannot instantiate RVec<bool>"") and some related issues. The different semantics of std::vector<bool> make memory adoption through a. custom allocator more complex -- namely, `RAdoptAllocator<bool>` must be. rebindable to `RAdoptAllocator<unsigned long>`, but if adopted memory is. really a buffer of bools, reinterpretation of the adopted buffer as a. different type is going to break things in horrible ways. As a workaround, `RAdoptAllocator<bool>` is specialized to be a simple. allocator that forwards calls to `std::allocator`, never adopts memory, and. can be rebound to any other allocator (as it never adopts memory it can. rebind the same way that `std::allocator` can). Note that this is not enough to support `RVec<bool>` columns in `RDataFrame`, for two main reasons:. * `TTreeReaderArray<bool>` is broken, see [ROOT-9570](https://sft.its.cern.ch/jira/browse/ROOT-9570). * `RVec<bool>` cannot be constructed from a `(bool *, std::size_t)` pair and cannot adopt memory (because `RAdoptAllocator<bool>` cannot adopt memory, because of the problems mentioned above) . So `RDataFrame` would still have to treat `RVec<bool>` differently:. * use a `TTreeReaderValue<std::vector<bool>>` to read the data from disk correctly. * copy the data into an `RVec<bool>`, without relying on memory adoption. These changes might be added by a future PR as they are outside of the scope of ROOT-9453 (""Cannot instantiate RVec<bool>""), _if_ this proposed solution to ROOT-9453 is accepted.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2433
https://github.com/root-project/root/pull/2433:650,energy efficiency,alloc,allocator,650,"[VecOps] Fix RAdoptAllocator<bool>; This PR fixes ROOT-9453 (""Cannot instantiate RVec<bool>"") and some related issues. The different semantics of std::vector<bool> make memory adoption through a. custom allocator more complex -- namely, `RAdoptAllocator<bool>` must be. rebindable to `RAdoptAllocator<unsigned long>`, but if adopted memory is. really a buffer of bools, reinterpretation of the adopted buffer as a. different type is going to break things in horrible ways. As a workaround, `RAdoptAllocator<bool>` is specialized to be a simple. allocator that forwards calls to `std::allocator`, never adopts memory, and. can be rebound to any other allocator (as it never adopts memory it can. rebind the same way that `std::allocator` can). Note that this is not enough to support `RVec<bool>` columns in `RDataFrame`, for two main reasons:. * `TTreeReaderArray<bool>` is broken, see [ROOT-9570](https://sft.its.cern.ch/jira/browse/ROOT-9570). * `RVec<bool>` cannot be constructed from a `(bool *, std::size_t)` pair and cannot adopt memory (because `RAdoptAllocator<bool>` cannot adopt memory, because of the problems mentioned above) . So `RDataFrame` would still have to treat `RVec<bool>` differently:. * use a `TTreeReaderValue<std::vector<bool>>` to read the data from disk correctly. * copy the data into an `RVec<bool>`, without relying on memory adoption. These changes might be added by a future PR as they are outside of the scope of ROOT-9453 (""Cannot instantiate RVec<bool>""), _if_ this proposed solution to ROOT-9453 is accepted.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2433
https://github.com/root-project/root/pull/2433:726,energy efficiency,alloc,allocator,726,"[VecOps] Fix RAdoptAllocator<bool>; This PR fixes ROOT-9453 (""Cannot instantiate RVec<bool>"") and some related issues. The different semantics of std::vector<bool> make memory adoption through a. custom allocator more complex -- namely, `RAdoptAllocator<bool>` must be. rebindable to `RAdoptAllocator<unsigned long>`, but if adopted memory is. really a buffer of bools, reinterpretation of the adopted buffer as a. different type is going to break things in horrible ways. As a workaround, `RAdoptAllocator<bool>` is specialized to be a simple. allocator that forwards calls to `std::allocator`, never adopts memory, and. can be rebound to any other allocator (as it never adopts memory it can. rebind the same way that `std::allocator` can). Note that this is not enough to support `RVec<bool>` columns in `RDataFrame`, for two main reasons:. * `TTreeReaderArray<bool>` is broken, see [ROOT-9570](https://sft.its.cern.ch/jira/browse/ROOT-9570). * `RVec<bool>` cannot be constructed from a `(bool *, std::size_t)` pair and cannot adopt memory (because `RAdoptAllocator<bool>` cannot adopt memory, because of the problems mentioned above) . So `RDataFrame` would still have to treat `RVec<bool>` differently:. * use a `TTreeReaderValue<std::vector<bool>>` to read the data from disk correctly. * copy the data into an `RVec<bool>`, without relying on memory adoption. These changes might be added by a future PR as they are outside of the scope of ROOT-9453 (""Cannot instantiate RVec<bool>""), _if_ this proposed solution to ROOT-9453 is accepted.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2433
https://github.com/root-project/root/pull/2433:353,integrability,buffer,buffer,353,"[VecOps] Fix RAdoptAllocator<bool>; This PR fixes ROOT-9453 (""Cannot instantiate RVec<bool>"") and some related issues. The different semantics of std::vector<bool> make memory adoption through a. custom allocator more complex -- namely, `RAdoptAllocator<bool>` must be. rebindable to `RAdoptAllocator<unsigned long>`, but if adopted memory is. really a buffer of bools, reinterpretation of the adopted buffer as a. different type is going to break things in horrible ways. As a workaround, `RAdoptAllocator<bool>` is specialized to be a simple. allocator that forwards calls to `std::allocator`, never adopts memory, and. can be rebound to any other allocator (as it never adopts memory it can. rebind the same way that `std::allocator` can). Note that this is not enough to support `RVec<bool>` columns in `RDataFrame`, for two main reasons:. * `TTreeReaderArray<bool>` is broken, see [ROOT-9570](https://sft.its.cern.ch/jira/browse/ROOT-9570). * `RVec<bool>` cannot be constructed from a `(bool *, std::size_t)` pair and cannot adopt memory (because `RAdoptAllocator<bool>` cannot adopt memory, because of the problems mentioned above) . So `RDataFrame` would still have to treat `RVec<bool>` differently:. * use a `TTreeReaderValue<std::vector<bool>>` to read the data from disk correctly. * copy the data into an `RVec<bool>`, without relying on memory adoption. These changes might be added by a future PR as they are outside of the scope of ROOT-9453 (""Cannot instantiate RVec<bool>""), _if_ this proposed solution to ROOT-9453 is accepted.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2433
https://github.com/root-project/root/pull/2433:402,integrability,buffer,buffer,402,"[VecOps] Fix RAdoptAllocator<bool>; This PR fixes ROOT-9453 (""Cannot instantiate RVec<bool>"") and some related issues. The different semantics of std::vector<bool> make memory adoption through a. custom allocator more complex -- namely, `RAdoptAllocator<bool>` must be. rebindable to `RAdoptAllocator<unsigned long>`, but if adopted memory is. really a buffer of bools, reinterpretation of the adopted buffer as a. different type is going to break things in horrible ways. As a workaround, `RAdoptAllocator<bool>` is specialized to be a simple. allocator that forwards calls to `std::allocator`, never adopts memory, and. can be rebound to any other allocator (as it never adopts memory it can. rebind the same way that `std::allocator` can). Note that this is not enough to support `RVec<bool>` columns in `RDataFrame`, for two main reasons:. * `TTreeReaderArray<bool>` is broken, see [ROOT-9570](https://sft.its.cern.ch/jira/browse/ROOT-9570). * `RVec<bool>` cannot be constructed from a `(bool *, std::size_t)` pair and cannot adopt memory (because `RAdoptAllocator<bool>` cannot adopt memory, because of the problems mentioned above) . So `RDataFrame` would still have to treat `RVec<bool>` differently:. * use a `TTreeReaderValue<std::vector<bool>>` to read the data from disk correctly. * copy the data into an `RVec<bool>`, without relying on memory adoption. These changes might be added by a future PR as they are outside of the scope of ROOT-9453 (""Cannot instantiate RVec<bool>""), _if_ this proposed solution to ROOT-9453 is accepted.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2433
https://github.com/root-project/root/pull/2433:133,interoperability,semant,semantics,133,"[VecOps] Fix RAdoptAllocator<bool>; This PR fixes ROOT-9453 (""Cannot instantiate RVec<bool>"") and some related issues. The different semantics of std::vector<bool> make memory adoption through a. custom allocator more complex -- namely, `RAdoptAllocator<bool>` must be. rebindable to `RAdoptAllocator<unsigned long>`, but if adopted memory is. really a buffer of bools, reinterpretation of the adopted buffer as a. different type is going to break things in horrible ways. As a workaround, `RAdoptAllocator<bool>` is specialized to be a simple. allocator that forwards calls to `std::allocator`, never adopts memory, and. can be rebound to any other allocator (as it never adopts memory it can. rebind the same way that `std::allocator` can). Note that this is not enough to support `RVec<bool>` columns in `RDataFrame`, for two main reasons:. * `TTreeReaderArray<bool>` is broken, see [ROOT-9570](https://sft.its.cern.ch/jira/browse/ROOT-9570). * `RVec<bool>` cannot be constructed from a `(bool *, std::size_t)` pair and cannot adopt memory (because `RAdoptAllocator<bool>` cannot adopt memory, because of the problems mentioned above) . So `RDataFrame` would still have to treat `RVec<bool>` differently:. * use a `TTreeReaderValue<std::vector<bool>>` to read the data from disk correctly. * copy the data into an `RVec<bool>`, without relying on memory adoption. These changes might be added by a future PR as they are outside of the scope of ROOT-9453 (""Cannot instantiate RVec<bool>""), _if_ this proposed solution to ROOT-9453 is accepted.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2433
https://github.com/root-project/root/pull/2433:169,performance,memor,memory,169,"[VecOps] Fix RAdoptAllocator<bool>; This PR fixes ROOT-9453 (""Cannot instantiate RVec<bool>"") and some related issues. The different semantics of std::vector<bool> make memory adoption through a. custom allocator more complex -- namely, `RAdoptAllocator<bool>` must be. rebindable to `RAdoptAllocator<unsigned long>`, but if adopted memory is. really a buffer of bools, reinterpretation of the adopted buffer as a. different type is going to break things in horrible ways. As a workaround, `RAdoptAllocator<bool>` is specialized to be a simple. allocator that forwards calls to `std::allocator`, never adopts memory, and. can be rebound to any other allocator (as it never adopts memory it can. rebind the same way that `std::allocator` can). Note that this is not enough to support `RVec<bool>` columns in `RDataFrame`, for two main reasons:. * `TTreeReaderArray<bool>` is broken, see [ROOT-9570](https://sft.its.cern.ch/jira/browse/ROOT-9570). * `RVec<bool>` cannot be constructed from a `(bool *, std::size_t)` pair and cannot adopt memory (because `RAdoptAllocator<bool>` cannot adopt memory, because of the problems mentioned above) . So `RDataFrame` would still have to treat `RVec<bool>` differently:. * use a `TTreeReaderValue<std::vector<bool>>` to read the data from disk correctly. * copy the data into an `RVec<bool>`, without relying on memory adoption. These changes might be added by a future PR as they are outside of the scope of ROOT-9453 (""Cannot instantiate RVec<bool>""), _if_ this proposed solution to ROOT-9453 is accepted.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2433
https://github.com/root-project/root/pull/2433:333,performance,memor,memory,333,"[VecOps] Fix RAdoptAllocator<bool>; This PR fixes ROOT-9453 (""Cannot instantiate RVec<bool>"") and some related issues. The different semantics of std::vector<bool> make memory adoption through a. custom allocator more complex -- namely, `RAdoptAllocator<bool>` must be. rebindable to `RAdoptAllocator<unsigned long>`, but if adopted memory is. really a buffer of bools, reinterpretation of the adopted buffer as a. different type is going to break things in horrible ways. As a workaround, `RAdoptAllocator<bool>` is specialized to be a simple. allocator that forwards calls to `std::allocator`, never adopts memory, and. can be rebound to any other allocator (as it never adopts memory it can. rebind the same way that `std::allocator` can). Note that this is not enough to support `RVec<bool>` columns in `RDataFrame`, for two main reasons:. * `TTreeReaderArray<bool>` is broken, see [ROOT-9570](https://sft.its.cern.ch/jira/browse/ROOT-9570). * `RVec<bool>` cannot be constructed from a `(bool *, std::size_t)` pair and cannot adopt memory (because `RAdoptAllocator<bool>` cannot adopt memory, because of the problems mentioned above) . So `RDataFrame` would still have to treat `RVec<bool>` differently:. * use a `TTreeReaderValue<std::vector<bool>>` to read the data from disk correctly. * copy the data into an `RVec<bool>`, without relying on memory adoption. These changes might be added by a future PR as they are outside of the scope of ROOT-9453 (""Cannot instantiate RVec<bool>""), _if_ this proposed solution to ROOT-9453 is accepted.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2433
https://github.com/root-project/root/pull/2433:609,performance,memor,memory,609,"[VecOps] Fix RAdoptAllocator<bool>; This PR fixes ROOT-9453 (""Cannot instantiate RVec<bool>"") and some related issues. The different semantics of std::vector<bool> make memory adoption through a. custom allocator more complex -- namely, `RAdoptAllocator<bool>` must be. rebindable to `RAdoptAllocator<unsigned long>`, but if adopted memory is. really a buffer of bools, reinterpretation of the adopted buffer as a. different type is going to break things in horrible ways. As a workaround, `RAdoptAllocator<bool>` is specialized to be a simple. allocator that forwards calls to `std::allocator`, never adopts memory, and. can be rebound to any other allocator (as it never adopts memory it can. rebind the same way that `std::allocator` can). Note that this is not enough to support `RVec<bool>` columns in `RDataFrame`, for two main reasons:. * `TTreeReaderArray<bool>` is broken, see [ROOT-9570](https://sft.its.cern.ch/jira/browse/ROOT-9570). * `RVec<bool>` cannot be constructed from a `(bool *, std::size_t)` pair and cannot adopt memory (because `RAdoptAllocator<bool>` cannot adopt memory, because of the problems mentioned above) . So `RDataFrame` would still have to treat `RVec<bool>` differently:. * use a `TTreeReaderValue<std::vector<bool>>` to read the data from disk correctly. * copy the data into an `RVec<bool>`, without relying on memory adoption. These changes might be added by a future PR as they are outside of the scope of ROOT-9453 (""Cannot instantiate RVec<bool>""), _if_ this proposed solution to ROOT-9453 is accepted.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2433
https://github.com/root-project/root/pull/2433:680,performance,memor,memory,680,"[VecOps] Fix RAdoptAllocator<bool>; This PR fixes ROOT-9453 (""Cannot instantiate RVec<bool>"") and some related issues. The different semantics of std::vector<bool> make memory adoption through a. custom allocator more complex -- namely, `RAdoptAllocator<bool>` must be. rebindable to `RAdoptAllocator<unsigned long>`, but if adopted memory is. really a buffer of bools, reinterpretation of the adopted buffer as a. different type is going to break things in horrible ways. As a workaround, `RAdoptAllocator<bool>` is specialized to be a simple. allocator that forwards calls to `std::allocator`, never adopts memory, and. can be rebound to any other allocator (as it never adopts memory it can. rebind the same way that `std::allocator` can). Note that this is not enough to support `RVec<bool>` columns in `RDataFrame`, for two main reasons:. * `TTreeReaderArray<bool>` is broken, see [ROOT-9570](https://sft.its.cern.ch/jira/browse/ROOT-9570). * `RVec<bool>` cannot be constructed from a `(bool *, std::size_t)` pair and cannot adopt memory (because `RAdoptAllocator<bool>` cannot adopt memory, because of the problems mentioned above) . So `RDataFrame` would still have to treat `RVec<bool>` differently:. * use a `TTreeReaderValue<std::vector<bool>>` to read the data from disk correctly. * copy the data into an `RVec<bool>`, without relying on memory adoption. These changes might be added by a future PR as they are outside of the scope of ROOT-9453 (""Cannot instantiate RVec<bool>""), _if_ this proposed solution to ROOT-9453 is accepted.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2433
https://github.com/root-project/root/pull/2433:1036,performance,memor,memory,1036,"[VecOps] Fix RAdoptAllocator<bool>; This PR fixes ROOT-9453 (""Cannot instantiate RVec<bool>"") and some related issues. The different semantics of std::vector<bool> make memory adoption through a. custom allocator more complex -- namely, `RAdoptAllocator<bool>` must be. rebindable to `RAdoptAllocator<unsigned long>`, but if adopted memory is. really a buffer of bools, reinterpretation of the adopted buffer as a. different type is going to break things in horrible ways. As a workaround, `RAdoptAllocator<bool>` is specialized to be a simple. allocator that forwards calls to `std::allocator`, never adopts memory, and. can be rebound to any other allocator (as it never adopts memory it can. rebind the same way that `std::allocator` can). Note that this is not enough to support `RVec<bool>` columns in `RDataFrame`, for two main reasons:. * `TTreeReaderArray<bool>` is broken, see [ROOT-9570](https://sft.its.cern.ch/jira/browse/ROOT-9570). * `RVec<bool>` cannot be constructed from a `(bool *, std::size_t)` pair and cannot adopt memory (because `RAdoptAllocator<bool>` cannot adopt memory, because of the problems mentioned above) . So `RDataFrame` would still have to treat `RVec<bool>` differently:. * use a `TTreeReaderValue<std::vector<bool>>` to read the data from disk correctly. * copy the data into an `RVec<bool>`, without relying on memory adoption. These changes might be added by a future PR as they are outside of the scope of ROOT-9453 (""Cannot instantiate RVec<bool>""), _if_ this proposed solution to ROOT-9453 is accepted.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2433
https://github.com/root-project/root/pull/2433:1089,performance,memor,memory,1089,"[VecOps] Fix RAdoptAllocator<bool>; This PR fixes ROOT-9453 (""Cannot instantiate RVec<bool>"") and some related issues. The different semantics of std::vector<bool> make memory adoption through a. custom allocator more complex -- namely, `RAdoptAllocator<bool>` must be. rebindable to `RAdoptAllocator<unsigned long>`, but if adopted memory is. really a buffer of bools, reinterpretation of the adopted buffer as a. different type is going to break things in horrible ways. As a workaround, `RAdoptAllocator<bool>` is specialized to be a simple. allocator that forwards calls to `std::allocator`, never adopts memory, and. can be rebound to any other allocator (as it never adopts memory it can. rebind the same way that `std::allocator` can). Note that this is not enough to support `RVec<bool>` columns in `RDataFrame`, for two main reasons:. * `TTreeReaderArray<bool>` is broken, see [ROOT-9570](https://sft.its.cern.ch/jira/browse/ROOT-9570). * `RVec<bool>` cannot be constructed from a `(bool *, std::size_t)` pair and cannot adopt memory (because `RAdoptAllocator<bool>` cannot adopt memory, because of the problems mentioned above) . So `RDataFrame` would still have to treat `RVec<bool>` differently:. * use a `TTreeReaderValue<std::vector<bool>>` to read the data from disk correctly. * copy the data into an `RVec<bool>`, without relying on memory adoption. These changes might be added by a future PR as they are outside of the scope of ROOT-9453 (""Cannot instantiate RVec<bool>""), _if_ this proposed solution to ROOT-9453 is accepted.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2433
https://github.com/root-project/root/pull/2433:1277,performance,disk,disk,1277,"[VecOps] Fix RAdoptAllocator<bool>; This PR fixes ROOT-9453 (""Cannot instantiate RVec<bool>"") and some related issues. The different semantics of std::vector<bool> make memory adoption through a. custom allocator more complex -- namely, `RAdoptAllocator<bool>` must be. rebindable to `RAdoptAllocator<unsigned long>`, but if adopted memory is. really a buffer of bools, reinterpretation of the adopted buffer as a. different type is going to break things in horrible ways. As a workaround, `RAdoptAllocator<bool>` is specialized to be a simple. allocator that forwards calls to `std::allocator`, never adopts memory, and. can be rebound to any other allocator (as it never adopts memory it can. rebind the same way that `std::allocator` can). Note that this is not enough to support `RVec<bool>` columns in `RDataFrame`, for two main reasons:. * `TTreeReaderArray<bool>` is broken, see [ROOT-9570](https://sft.its.cern.ch/jira/browse/ROOT-9570). * `RVec<bool>` cannot be constructed from a `(bool *, std::size_t)` pair and cannot adopt memory (because `RAdoptAllocator<bool>` cannot adopt memory, because of the problems mentioned above) . So `RDataFrame` would still have to treat `RVec<bool>` differently:. * use a `TTreeReaderValue<std::vector<bool>>` to read the data from disk correctly. * copy the data into an `RVec<bool>`, without relying on memory adoption. These changes might be added by a future PR as they are outside of the scope of ROOT-9453 (""Cannot instantiate RVec<bool>""), _if_ this proposed solution to ROOT-9453 is accepted.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2433
https://github.com/root-project/root/pull/2433:1350,performance,memor,memory,1350,"[VecOps] Fix RAdoptAllocator<bool>; This PR fixes ROOT-9453 (""Cannot instantiate RVec<bool>"") and some related issues. The different semantics of std::vector<bool> make memory adoption through a. custom allocator more complex -- namely, `RAdoptAllocator<bool>` must be. rebindable to `RAdoptAllocator<unsigned long>`, but if adopted memory is. really a buffer of bools, reinterpretation of the adopted buffer as a. different type is going to break things in horrible ways. As a workaround, `RAdoptAllocator<bool>` is specialized to be a simple. allocator that forwards calls to `std::allocator`, never adopts memory, and. can be rebound to any other allocator (as it never adopts memory it can. rebind the same way that `std::allocator` can). Note that this is not enough to support `RVec<bool>` columns in `RDataFrame`, for two main reasons:. * `TTreeReaderArray<bool>` is broken, see [ROOT-9570](https://sft.its.cern.ch/jira/browse/ROOT-9570). * `RVec<bool>` cannot be constructed from a `(bool *, std::size_t)` pair and cannot adopt memory (because `RAdoptAllocator<bool>` cannot adopt memory, because of the problems mentioned above) . So `RDataFrame` would still have to treat `RVec<bool>` differently:. * use a `TTreeReaderValue<std::vector<bool>>` to read the data from disk correctly. * copy the data into an `RVec<bool>`, without relying on memory adoption. These changes might be added by a future PR as they are outside of the scope of ROOT-9453 (""Cannot instantiate RVec<bool>""), _if_ this proposed solution to ROOT-9453 is accepted.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2433
https://github.com/root-project/root/pull/2433:218,safety,compl,complex,218,"[VecOps] Fix RAdoptAllocator<bool>; This PR fixes ROOT-9453 (""Cannot instantiate RVec<bool>"") and some related issues. The different semantics of std::vector<bool> make memory adoption through a. custom allocator more complex -- namely, `RAdoptAllocator<bool>` must be. rebindable to `RAdoptAllocator<unsigned long>`, but if adopted memory is. really a buffer of bools, reinterpretation of the adopted buffer as a. different type is going to break things in horrible ways. As a workaround, `RAdoptAllocator<bool>` is specialized to be a simple. allocator that forwards calls to `std::allocator`, never adopts memory, and. can be rebound to any other allocator (as it never adopts memory it can. rebind the same way that `std::allocator` can). Note that this is not enough to support `RVec<bool>` columns in `RDataFrame`, for two main reasons:. * `TTreeReaderArray<bool>` is broken, see [ROOT-9570](https://sft.its.cern.ch/jira/browse/ROOT-9570). * `RVec<bool>` cannot be constructed from a `(bool *, std::size_t)` pair and cannot adopt memory (because `RAdoptAllocator<bool>` cannot adopt memory, because of the problems mentioned above) . So `RDataFrame` would still have to treat `RVec<bool>` differently:. * use a `TTreeReaderValue<std::vector<bool>>` to read the data from disk correctly. * copy the data into an `RVec<bool>`, without relying on memory adoption. These changes might be added by a future PR as they are outside of the scope of ROOT-9453 (""Cannot instantiate RVec<bool>""), _if_ this proposed solution to ROOT-9453 is accepted.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2433
https://github.com/root-project/root/pull/2433:218,security,compl,complex,218,"[VecOps] Fix RAdoptAllocator<bool>; This PR fixes ROOT-9453 (""Cannot instantiate RVec<bool>"") and some related issues. The different semantics of std::vector<bool> make memory adoption through a. custom allocator more complex -- namely, `RAdoptAllocator<bool>` must be. rebindable to `RAdoptAllocator<unsigned long>`, but if adopted memory is. really a buffer of bools, reinterpretation of the adopted buffer as a. different type is going to break things in horrible ways. As a workaround, `RAdoptAllocator<bool>` is specialized to be a simple. allocator that forwards calls to `std::allocator`, never adopts memory, and. can be rebound to any other allocator (as it never adopts memory it can. rebind the same way that `std::allocator` can). Note that this is not enough to support `RVec<bool>` columns in `RDataFrame`, for two main reasons:. * `TTreeReaderArray<bool>` is broken, see [ROOT-9570](https://sft.its.cern.ch/jira/browse/ROOT-9570). * `RVec<bool>` cannot be constructed from a `(bool *, std::size_t)` pair and cannot adopt memory (because `RAdoptAllocator<bool>` cannot adopt memory, because of the problems mentioned above) . So `RDataFrame` would still have to treat `RVec<bool>` differently:. * use a `TTreeReaderValue<std::vector<bool>>` to read the data from disk correctly. * copy the data into an `RVec<bool>`, without relying on memory adoption. These changes might be added by a future PR as they are outside of the scope of ROOT-9453 (""Cannot instantiate RVec<bool>""), _if_ this proposed solution to ROOT-9453 is accepted.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2433
https://github.com/root-project/root/pull/2433:537,testability,simpl,simple,537,"[VecOps] Fix RAdoptAllocator<bool>; This PR fixes ROOT-9453 (""Cannot instantiate RVec<bool>"") and some related issues. The different semantics of std::vector<bool> make memory adoption through a. custom allocator more complex -- namely, `RAdoptAllocator<bool>` must be. rebindable to `RAdoptAllocator<unsigned long>`, but if adopted memory is. really a buffer of bools, reinterpretation of the adopted buffer as a. different type is going to break things in horrible ways. As a workaround, `RAdoptAllocator<bool>` is specialized to be a simple. allocator that forwards calls to `std::allocator`, never adopts memory, and. can be rebound to any other allocator (as it never adopts memory it can. rebind the same way that `std::allocator` can). Note that this is not enough to support `RVec<bool>` columns in `RDataFrame`, for two main reasons:. * `TTreeReaderArray<bool>` is broken, see [ROOT-9570](https://sft.its.cern.ch/jira/browse/ROOT-9570). * `RVec<bool>` cannot be constructed from a `(bool *, std::size_t)` pair and cannot adopt memory (because `RAdoptAllocator<bool>` cannot adopt memory, because of the problems mentioned above) . So `RDataFrame` would still have to treat `RVec<bool>` differently:. * use a `TTreeReaderValue<std::vector<bool>>` to read the data from disk correctly. * copy the data into an `RVec<bool>`, without relying on memory adoption. These changes might be added by a future PR as they are outside of the scope of ROOT-9453 (""Cannot instantiate RVec<bool>""), _if_ this proposed solution to ROOT-9453 is accepted.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2433
https://github.com/root-project/root/pull/2433:169,usability,memor,memory,169,"[VecOps] Fix RAdoptAllocator<bool>; This PR fixes ROOT-9453 (""Cannot instantiate RVec<bool>"") and some related issues. The different semantics of std::vector<bool> make memory adoption through a. custom allocator more complex -- namely, `RAdoptAllocator<bool>` must be. rebindable to `RAdoptAllocator<unsigned long>`, but if adopted memory is. really a buffer of bools, reinterpretation of the adopted buffer as a. different type is going to break things in horrible ways. As a workaround, `RAdoptAllocator<bool>` is specialized to be a simple. allocator that forwards calls to `std::allocator`, never adopts memory, and. can be rebound to any other allocator (as it never adopts memory it can. rebind the same way that `std::allocator` can). Note that this is not enough to support `RVec<bool>` columns in `RDataFrame`, for two main reasons:. * `TTreeReaderArray<bool>` is broken, see [ROOT-9570](https://sft.its.cern.ch/jira/browse/ROOT-9570). * `RVec<bool>` cannot be constructed from a `(bool *, std::size_t)` pair and cannot adopt memory (because `RAdoptAllocator<bool>` cannot adopt memory, because of the problems mentioned above) . So `RDataFrame` would still have to treat `RVec<bool>` differently:. * use a `TTreeReaderValue<std::vector<bool>>` to read the data from disk correctly. * copy the data into an `RVec<bool>`, without relying on memory adoption. These changes might be added by a future PR as they are outside of the scope of ROOT-9453 (""Cannot instantiate RVec<bool>""), _if_ this proposed solution to ROOT-9453 is accepted.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2433
https://github.com/root-project/root/pull/2433:196,usability,custom,custom,196,"[VecOps] Fix RAdoptAllocator<bool>; This PR fixes ROOT-9453 (""Cannot instantiate RVec<bool>"") and some related issues. The different semantics of std::vector<bool> make memory adoption through a. custom allocator more complex -- namely, `RAdoptAllocator<bool>` must be. rebindable to `RAdoptAllocator<unsigned long>`, but if adopted memory is. really a buffer of bools, reinterpretation of the adopted buffer as a. different type is going to break things in horrible ways. As a workaround, `RAdoptAllocator<bool>` is specialized to be a simple. allocator that forwards calls to `std::allocator`, never adopts memory, and. can be rebound to any other allocator (as it never adopts memory it can. rebind the same way that `std::allocator` can). Note that this is not enough to support `RVec<bool>` columns in `RDataFrame`, for two main reasons:. * `TTreeReaderArray<bool>` is broken, see [ROOT-9570](https://sft.its.cern.ch/jira/browse/ROOT-9570). * `RVec<bool>` cannot be constructed from a `(bool *, std::size_t)` pair and cannot adopt memory (because `RAdoptAllocator<bool>` cannot adopt memory, because of the problems mentioned above) . So `RDataFrame` would still have to treat `RVec<bool>` differently:. * use a `TTreeReaderValue<std::vector<bool>>` to read the data from disk correctly. * copy the data into an `RVec<bool>`, without relying on memory adoption. These changes might be added by a future PR as they are outside of the scope of ROOT-9453 (""Cannot instantiate RVec<bool>""), _if_ this proposed solution to ROOT-9453 is accepted.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2433
https://github.com/root-project/root/pull/2433:333,usability,memor,memory,333,"[VecOps] Fix RAdoptAllocator<bool>; This PR fixes ROOT-9453 (""Cannot instantiate RVec<bool>"") and some related issues. The different semantics of std::vector<bool> make memory adoption through a. custom allocator more complex -- namely, `RAdoptAllocator<bool>` must be. rebindable to `RAdoptAllocator<unsigned long>`, but if adopted memory is. really a buffer of bools, reinterpretation of the adopted buffer as a. different type is going to break things in horrible ways. As a workaround, `RAdoptAllocator<bool>` is specialized to be a simple. allocator that forwards calls to `std::allocator`, never adopts memory, and. can be rebound to any other allocator (as it never adopts memory it can. rebind the same way that `std::allocator` can). Note that this is not enough to support `RVec<bool>` columns in `RDataFrame`, for two main reasons:. * `TTreeReaderArray<bool>` is broken, see [ROOT-9570](https://sft.its.cern.ch/jira/browse/ROOT-9570). * `RVec<bool>` cannot be constructed from a `(bool *, std::size_t)` pair and cannot adopt memory (because `RAdoptAllocator<bool>` cannot adopt memory, because of the problems mentioned above) . So `RDataFrame` would still have to treat `RVec<bool>` differently:. * use a `TTreeReaderValue<std::vector<bool>>` to read the data from disk correctly. * copy the data into an `RVec<bool>`, without relying on memory adoption. These changes might be added by a future PR as they are outside of the scope of ROOT-9453 (""Cannot instantiate RVec<bool>""), _if_ this proposed solution to ROOT-9453 is accepted.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2433
https://github.com/root-project/root/pull/2433:537,usability,simpl,simple,537,"[VecOps] Fix RAdoptAllocator<bool>; This PR fixes ROOT-9453 (""Cannot instantiate RVec<bool>"") and some related issues. The different semantics of std::vector<bool> make memory adoption through a. custom allocator more complex -- namely, `RAdoptAllocator<bool>` must be. rebindable to `RAdoptAllocator<unsigned long>`, but if adopted memory is. really a buffer of bools, reinterpretation of the adopted buffer as a. different type is going to break things in horrible ways. As a workaround, `RAdoptAllocator<bool>` is specialized to be a simple. allocator that forwards calls to `std::allocator`, never adopts memory, and. can be rebound to any other allocator (as it never adopts memory it can. rebind the same way that `std::allocator` can). Note that this is not enough to support `RVec<bool>` columns in `RDataFrame`, for two main reasons:. * `TTreeReaderArray<bool>` is broken, see [ROOT-9570](https://sft.its.cern.ch/jira/browse/ROOT-9570). * `RVec<bool>` cannot be constructed from a `(bool *, std::size_t)` pair and cannot adopt memory (because `RAdoptAllocator<bool>` cannot adopt memory, because of the problems mentioned above) . So `RDataFrame` would still have to treat `RVec<bool>` differently:. * use a `TTreeReaderValue<std::vector<bool>>` to read the data from disk correctly. * copy the data into an `RVec<bool>`, without relying on memory adoption. These changes might be added by a future PR as they are outside of the scope of ROOT-9453 (""Cannot instantiate RVec<bool>""), _if_ this proposed solution to ROOT-9453 is accepted.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2433
https://github.com/root-project/root/pull/2433:609,usability,memor,memory,609,"[VecOps] Fix RAdoptAllocator<bool>; This PR fixes ROOT-9453 (""Cannot instantiate RVec<bool>"") and some related issues. The different semantics of std::vector<bool> make memory adoption through a. custom allocator more complex -- namely, `RAdoptAllocator<bool>` must be. rebindable to `RAdoptAllocator<unsigned long>`, but if adopted memory is. really a buffer of bools, reinterpretation of the adopted buffer as a. different type is going to break things in horrible ways. As a workaround, `RAdoptAllocator<bool>` is specialized to be a simple. allocator that forwards calls to `std::allocator`, never adopts memory, and. can be rebound to any other allocator (as it never adopts memory it can. rebind the same way that `std::allocator` can). Note that this is not enough to support `RVec<bool>` columns in `RDataFrame`, for two main reasons:. * `TTreeReaderArray<bool>` is broken, see [ROOT-9570](https://sft.its.cern.ch/jira/browse/ROOT-9570). * `RVec<bool>` cannot be constructed from a `(bool *, std::size_t)` pair and cannot adopt memory (because `RAdoptAllocator<bool>` cannot adopt memory, because of the problems mentioned above) . So `RDataFrame` would still have to treat `RVec<bool>` differently:. * use a `TTreeReaderValue<std::vector<bool>>` to read the data from disk correctly. * copy the data into an `RVec<bool>`, without relying on memory adoption. These changes might be added by a future PR as they are outside of the scope of ROOT-9453 (""Cannot instantiate RVec<bool>""), _if_ this proposed solution to ROOT-9453 is accepted.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2433
https://github.com/root-project/root/pull/2433:680,usability,memor,memory,680,"[VecOps] Fix RAdoptAllocator<bool>; This PR fixes ROOT-9453 (""Cannot instantiate RVec<bool>"") and some related issues. The different semantics of std::vector<bool> make memory adoption through a. custom allocator more complex -- namely, `RAdoptAllocator<bool>` must be. rebindable to `RAdoptAllocator<unsigned long>`, but if adopted memory is. really a buffer of bools, reinterpretation of the adopted buffer as a. different type is going to break things in horrible ways. As a workaround, `RAdoptAllocator<bool>` is specialized to be a simple. allocator that forwards calls to `std::allocator`, never adopts memory, and. can be rebound to any other allocator (as it never adopts memory it can. rebind the same way that `std::allocator` can). Note that this is not enough to support `RVec<bool>` columns in `RDataFrame`, for two main reasons:. * `TTreeReaderArray<bool>` is broken, see [ROOT-9570](https://sft.its.cern.ch/jira/browse/ROOT-9570). * `RVec<bool>` cannot be constructed from a `(bool *, std::size_t)` pair and cannot adopt memory (because `RAdoptAllocator<bool>` cannot adopt memory, because of the problems mentioned above) . So `RDataFrame` would still have to treat `RVec<bool>` differently:. * use a `TTreeReaderValue<std::vector<bool>>` to read the data from disk correctly. * copy the data into an `RVec<bool>`, without relying on memory adoption. These changes might be added by a future PR as they are outside of the scope of ROOT-9453 (""Cannot instantiate RVec<bool>""), _if_ this proposed solution to ROOT-9453 is accepted.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2433
https://github.com/root-project/root/pull/2433:775,usability,support,support,775,"[VecOps] Fix RAdoptAllocator<bool>; This PR fixes ROOT-9453 (""Cannot instantiate RVec<bool>"") and some related issues. The different semantics of std::vector<bool> make memory adoption through a. custom allocator more complex -- namely, `RAdoptAllocator<bool>` must be. rebindable to `RAdoptAllocator<unsigned long>`, but if adopted memory is. really a buffer of bools, reinterpretation of the adopted buffer as a. different type is going to break things in horrible ways. As a workaround, `RAdoptAllocator<bool>` is specialized to be a simple. allocator that forwards calls to `std::allocator`, never adopts memory, and. can be rebound to any other allocator (as it never adopts memory it can. rebind the same way that `std::allocator` can). Note that this is not enough to support `RVec<bool>` columns in `RDataFrame`, for two main reasons:. * `TTreeReaderArray<bool>` is broken, see [ROOT-9570](https://sft.its.cern.ch/jira/browse/ROOT-9570). * `RVec<bool>` cannot be constructed from a `(bool *, std::size_t)` pair and cannot adopt memory (because `RAdoptAllocator<bool>` cannot adopt memory, because of the problems mentioned above) . So `RDataFrame` would still have to treat `RVec<bool>` differently:. * use a `TTreeReaderValue<std::vector<bool>>` to read the data from disk correctly. * copy the data into an `RVec<bool>`, without relying on memory adoption. These changes might be added by a future PR as they are outside of the scope of ROOT-9453 (""Cannot instantiate RVec<bool>""), _if_ this proposed solution to ROOT-9453 is accepted.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2433
https://github.com/root-project/root/pull/2433:1036,usability,memor,memory,1036,"[VecOps] Fix RAdoptAllocator<bool>; This PR fixes ROOT-9453 (""Cannot instantiate RVec<bool>"") and some related issues. The different semantics of std::vector<bool> make memory adoption through a. custom allocator more complex -- namely, `RAdoptAllocator<bool>` must be. rebindable to `RAdoptAllocator<unsigned long>`, but if adopted memory is. really a buffer of bools, reinterpretation of the adopted buffer as a. different type is going to break things in horrible ways. As a workaround, `RAdoptAllocator<bool>` is specialized to be a simple. allocator that forwards calls to `std::allocator`, never adopts memory, and. can be rebound to any other allocator (as it never adopts memory it can. rebind the same way that `std::allocator` can). Note that this is not enough to support `RVec<bool>` columns in `RDataFrame`, for two main reasons:. * `TTreeReaderArray<bool>` is broken, see [ROOT-9570](https://sft.its.cern.ch/jira/browse/ROOT-9570). * `RVec<bool>` cannot be constructed from a `(bool *, std::size_t)` pair and cannot adopt memory (because `RAdoptAllocator<bool>` cannot adopt memory, because of the problems mentioned above) . So `RDataFrame` would still have to treat `RVec<bool>` differently:. * use a `TTreeReaderValue<std::vector<bool>>` to read the data from disk correctly. * copy the data into an `RVec<bool>`, without relying on memory adoption. These changes might be added by a future PR as they are outside of the scope of ROOT-9453 (""Cannot instantiate RVec<bool>""), _if_ this proposed solution to ROOT-9453 is accepted.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2433
https://github.com/root-project/root/pull/2433:1089,usability,memor,memory,1089,"[VecOps] Fix RAdoptAllocator<bool>; This PR fixes ROOT-9453 (""Cannot instantiate RVec<bool>"") and some related issues. The different semantics of std::vector<bool> make memory adoption through a. custom allocator more complex -- namely, `RAdoptAllocator<bool>` must be. rebindable to `RAdoptAllocator<unsigned long>`, but if adopted memory is. really a buffer of bools, reinterpretation of the adopted buffer as a. different type is going to break things in horrible ways. As a workaround, `RAdoptAllocator<bool>` is specialized to be a simple. allocator that forwards calls to `std::allocator`, never adopts memory, and. can be rebound to any other allocator (as it never adopts memory it can. rebind the same way that `std::allocator` can). Note that this is not enough to support `RVec<bool>` columns in `RDataFrame`, for two main reasons:. * `TTreeReaderArray<bool>` is broken, see [ROOT-9570](https://sft.its.cern.ch/jira/browse/ROOT-9570). * `RVec<bool>` cannot be constructed from a `(bool *, std::size_t)` pair and cannot adopt memory (because `RAdoptAllocator<bool>` cannot adopt memory, because of the problems mentioned above) . So `RDataFrame` would still have to treat `RVec<bool>` differently:. * use a `TTreeReaderValue<std::vector<bool>>` to read the data from disk correctly. * copy the data into an `RVec<bool>`, without relying on memory adoption. These changes might be added by a future PR as they are outside of the scope of ROOT-9453 (""Cannot instantiate RVec<bool>""), _if_ this proposed solution to ROOT-9453 is accepted.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2433
https://github.com/root-project/root/pull/2433:1350,usability,memor,memory,1350,"[VecOps] Fix RAdoptAllocator<bool>; This PR fixes ROOT-9453 (""Cannot instantiate RVec<bool>"") and some related issues. The different semantics of std::vector<bool> make memory adoption through a. custom allocator more complex -- namely, `RAdoptAllocator<bool>` must be. rebindable to `RAdoptAllocator<unsigned long>`, but if adopted memory is. really a buffer of bools, reinterpretation of the adopted buffer as a. different type is going to break things in horrible ways. As a workaround, `RAdoptAllocator<bool>` is specialized to be a simple. allocator that forwards calls to `std::allocator`, never adopts memory, and. can be rebound to any other allocator (as it never adopts memory it can. rebind the same way that `std::allocator` can). Note that this is not enough to support `RVec<bool>` columns in `RDataFrame`, for two main reasons:. * `TTreeReaderArray<bool>` is broken, see [ROOT-9570](https://sft.its.cern.ch/jira/browse/ROOT-9570). * `RVec<bool>` cannot be constructed from a `(bool *, std::size_t)` pair and cannot adopt memory (because `RAdoptAllocator<bool>` cannot adopt memory, because of the problems mentioned above) . So `RDataFrame` would still have to treat `RVec<bool>` differently:. * use a `TTreeReaderValue<std::vector<bool>>` to read the data from disk correctly. * copy the data into an `RVec<bool>`, without relying on memory adoption. These changes might be added by a future PR as they are outside of the scope of ROOT-9453 (""Cannot instantiate RVec<bool>""), _if_ this proposed solution to ROOT-9453 is accepted.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2433
https://github.com/root-project/root/pull/2434:83,deployability,modul,module-release,83,"[cxxmodule] Improve pcm size by caching LookupHelper; ```. Master. [yuka@yuka-arch module-release]$ du lib/*.pcm | grep -v rdict | awk -F ' ' '{sum += $1} END {print sum}'. 360608. Processing /home/yuka/CERN/ROOT/memory.C... cpu time = 0.291462 seconds. sys time = 0.064409 seconds. res memory = 345.816 Mbytes. vir memory = 573.508 Mbytes. With this patch. [yuka@yuka-arch module-release]$ du lib/*.pcm | grep -v rdict | awk -F ' ' '{sum += $1} END {print sum}'. 343336. Processing /home/yuka/CERN/ROOT/memory.C... cpu time = 0.268580 seconds. sys time = 0.082866 seconds. res memory = 329.051 Mbytes. vir memory = 553.289 Mbytes. ```. It improves res memeory by 5% compared with master, and by 8% with",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2434
https://github.com/root-project/root/pull/2434:351,deployability,patch,patch,351,"[cxxmodule] Improve pcm size by caching LookupHelper; ```. Master. [yuka@yuka-arch module-release]$ du lib/*.pcm | grep -v rdict | awk -F ' ' '{sum += $1} END {print sum}'. 360608. Processing /home/yuka/CERN/ROOT/memory.C... cpu time = 0.291462 seconds. sys time = 0.064409 seconds. res memory = 345.816 Mbytes. vir memory = 573.508 Mbytes. With this patch. [yuka@yuka-arch module-release]$ du lib/*.pcm | grep -v rdict | awk -F ' ' '{sum += $1} END {print sum}'. 343336. Processing /home/yuka/CERN/ROOT/memory.C... cpu time = 0.268580 seconds. sys time = 0.082866 seconds. res memory = 329.051 Mbytes. vir memory = 553.289 Mbytes. ```. It improves res memeory by 5% compared with master, and by 8% with",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2434
https://github.com/root-project/root/pull/2434:374,deployability,modul,module-release,374,"[cxxmodule] Improve pcm size by caching LookupHelper; ```. Master. [yuka@yuka-arch module-release]$ du lib/*.pcm | grep -v rdict | awk -F ' ' '{sum += $1} END {print sum}'. 360608. Processing /home/yuka/CERN/ROOT/memory.C... cpu time = 0.291462 seconds. sys time = 0.064409 seconds. res memory = 345.816 Mbytes. vir memory = 573.508 Mbytes. With this patch. [yuka@yuka-arch module-release]$ du lib/*.pcm | grep -v rdict | awk -F ' ' '{sum += $1} END {print sum}'. 343336. Processing /home/yuka/CERN/ROOT/memory.C... cpu time = 0.268580 seconds. sys time = 0.082866 seconds. res memory = 329.051 Mbytes. vir memory = 553.289 Mbytes. ```. It improves res memeory by 5% compared with master, and by 8% with",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2434
https://github.com/root-project/root/pull/2434:225,energy efficiency,cpu,cpu,225,"[cxxmodule] Improve pcm size by caching LookupHelper; ```. Master. [yuka@yuka-arch module-release]$ du lib/*.pcm | grep -v rdict | awk -F ' ' '{sum += $1} END {print sum}'. 360608. Processing /home/yuka/CERN/ROOT/memory.C... cpu time = 0.291462 seconds. sys time = 0.064409 seconds. res memory = 345.816 Mbytes. vir memory = 573.508 Mbytes. With this patch. [yuka@yuka-arch module-release]$ du lib/*.pcm | grep -v rdict | awk -F ' ' '{sum += $1} END {print sum}'. 343336. Processing /home/yuka/CERN/ROOT/memory.C... cpu time = 0.268580 seconds. sys time = 0.082866 seconds. res memory = 329.051 Mbytes. vir memory = 553.289 Mbytes. ```. It improves res memeory by 5% compared with master, and by 8% with",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2434
https://github.com/root-project/root/pull/2434:516,energy efficiency,cpu,cpu,516,"[cxxmodule] Improve pcm size by caching LookupHelper; ```. Master. [yuka@yuka-arch module-release]$ du lib/*.pcm | grep -v rdict | awk -F ' ' '{sum += $1} END {print sum}'. 360608. Processing /home/yuka/CERN/ROOT/memory.C... cpu time = 0.291462 seconds. sys time = 0.064409 seconds. res memory = 345.816 Mbytes. vir memory = 573.508 Mbytes. With this patch. [yuka@yuka-arch module-release]$ du lib/*.pcm | grep -v rdict | awk -F ' ' '{sum += $1} END {print sum}'. 343336. Processing /home/yuka/CERN/ROOT/memory.C... cpu time = 0.268580 seconds. sys time = 0.082866 seconds. res memory = 329.051 Mbytes. vir memory = 553.289 Mbytes. ```. It improves res memeory by 5% compared with master, and by 8% with",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2434
https://github.com/root-project/root/pull/2434:83,modifiability,modul,module-release,83,"[cxxmodule] Improve pcm size by caching LookupHelper; ```. Master. [yuka@yuka-arch module-release]$ du lib/*.pcm | grep -v rdict | awk -F ' ' '{sum += $1} END {print sum}'. 360608. Processing /home/yuka/CERN/ROOT/memory.C... cpu time = 0.291462 seconds. sys time = 0.064409 seconds. res memory = 345.816 Mbytes. vir memory = 573.508 Mbytes. With this patch. [yuka@yuka-arch module-release]$ du lib/*.pcm | grep -v rdict | awk -F ' ' '{sum += $1} END {print sum}'. 343336. Processing /home/yuka/CERN/ROOT/memory.C... cpu time = 0.268580 seconds. sys time = 0.082866 seconds. res memory = 329.051 Mbytes. vir memory = 553.289 Mbytes. ```. It improves res memeory by 5% compared with master, and by 8% with",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2434
https://github.com/root-project/root/pull/2434:374,modifiability,modul,module-release,374,"[cxxmodule] Improve pcm size by caching LookupHelper; ```. Master. [yuka@yuka-arch module-release]$ du lib/*.pcm | grep -v rdict | awk -F ' ' '{sum += $1} END {print sum}'. 360608. Processing /home/yuka/CERN/ROOT/memory.C... cpu time = 0.291462 seconds. sys time = 0.064409 seconds. res memory = 345.816 Mbytes. vir memory = 573.508 Mbytes. With this patch. [yuka@yuka-arch module-release]$ du lib/*.pcm | grep -v rdict | awk -F ' ' '{sum += $1} END {print sum}'. 343336. Processing /home/yuka/CERN/ROOT/memory.C... cpu time = 0.268580 seconds. sys time = 0.082866 seconds. res memory = 329.051 Mbytes. vir memory = 553.289 Mbytes. ```. It improves res memeory by 5% compared with master, and by 8% with",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2434
https://github.com/root-project/root/pull/2434:32,performance,cach,caching,32,"[cxxmodule] Improve pcm size by caching LookupHelper; ```. Master. [yuka@yuka-arch module-release]$ du lib/*.pcm | grep -v rdict | awk -F ' ' '{sum += $1} END {print sum}'. 360608. Processing /home/yuka/CERN/ROOT/memory.C... cpu time = 0.291462 seconds. sys time = 0.064409 seconds. res memory = 345.816 Mbytes. vir memory = 573.508 Mbytes. With this patch. [yuka@yuka-arch module-release]$ du lib/*.pcm | grep -v rdict | awk -F ' ' '{sum += $1} END {print sum}'. 343336. Processing /home/yuka/CERN/ROOT/memory.C... cpu time = 0.268580 seconds. sys time = 0.082866 seconds. res memory = 329.051 Mbytes. vir memory = 553.289 Mbytes. ```. It improves res memeory by 5% compared with master, and by 8% with",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2434
https://github.com/root-project/root/pull/2434:213,performance,memor,memory,213,"[cxxmodule] Improve pcm size by caching LookupHelper; ```. Master. [yuka@yuka-arch module-release]$ du lib/*.pcm | grep -v rdict | awk -F ' ' '{sum += $1} END {print sum}'. 360608. Processing /home/yuka/CERN/ROOT/memory.C... cpu time = 0.291462 seconds. sys time = 0.064409 seconds. res memory = 345.816 Mbytes. vir memory = 573.508 Mbytes. With this patch. [yuka@yuka-arch module-release]$ du lib/*.pcm | grep -v rdict | awk -F ' ' '{sum += $1} END {print sum}'. 343336. Processing /home/yuka/CERN/ROOT/memory.C... cpu time = 0.268580 seconds. sys time = 0.082866 seconds. res memory = 329.051 Mbytes. vir memory = 553.289 Mbytes. ```. It improves res memeory by 5% compared with master, and by 8% with",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2434
https://github.com/root-project/root/pull/2434:225,performance,cpu,cpu,225,"[cxxmodule] Improve pcm size by caching LookupHelper; ```. Master. [yuka@yuka-arch module-release]$ du lib/*.pcm | grep -v rdict | awk -F ' ' '{sum += $1} END {print sum}'. 360608. Processing /home/yuka/CERN/ROOT/memory.C... cpu time = 0.291462 seconds. sys time = 0.064409 seconds. res memory = 345.816 Mbytes. vir memory = 573.508 Mbytes. With this patch. [yuka@yuka-arch module-release]$ du lib/*.pcm | grep -v rdict | awk -F ' ' '{sum += $1} END {print sum}'. 343336. Processing /home/yuka/CERN/ROOT/memory.C... cpu time = 0.268580 seconds. sys time = 0.082866 seconds. res memory = 329.051 Mbytes. vir memory = 553.289 Mbytes. ```. It improves res memeory by 5% compared with master, and by 8% with",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2434
https://github.com/root-project/root/pull/2434:229,performance,time,time,229,"[cxxmodule] Improve pcm size by caching LookupHelper; ```. Master. [yuka@yuka-arch module-release]$ du lib/*.pcm | grep -v rdict | awk -F ' ' '{sum += $1} END {print sum}'. 360608. Processing /home/yuka/CERN/ROOT/memory.C... cpu time = 0.291462 seconds. sys time = 0.064409 seconds. res memory = 345.816 Mbytes. vir memory = 573.508 Mbytes. With this patch. [yuka@yuka-arch module-release]$ du lib/*.pcm | grep -v rdict | awk -F ' ' '{sum += $1} END {print sum}'. 343336. Processing /home/yuka/CERN/ROOT/memory.C... cpu time = 0.268580 seconds. sys time = 0.082866 seconds. res memory = 329.051 Mbytes. vir memory = 553.289 Mbytes. ```. It improves res memeory by 5% compared with master, and by 8% with",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2434
https://github.com/root-project/root/pull/2434:258,performance,time,time,258,"[cxxmodule] Improve pcm size by caching LookupHelper; ```. Master. [yuka@yuka-arch module-release]$ du lib/*.pcm | grep -v rdict | awk -F ' ' '{sum += $1} END {print sum}'. 360608. Processing /home/yuka/CERN/ROOT/memory.C... cpu time = 0.291462 seconds. sys time = 0.064409 seconds. res memory = 345.816 Mbytes. vir memory = 573.508 Mbytes. With this patch. [yuka@yuka-arch module-release]$ du lib/*.pcm | grep -v rdict | awk -F ' ' '{sum += $1} END {print sum}'. 343336. Processing /home/yuka/CERN/ROOT/memory.C... cpu time = 0.268580 seconds. sys time = 0.082866 seconds. res memory = 329.051 Mbytes. vir memory = 553.289 Mbytes. ```. It improves res memeory by 5% compared with master, and by 8% with",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2434
https://github.com/root-project/root/pull/2434:287,performance,memor,memory,287,"[cxxmodule] Improve pcm size by caching LookupHelper; ```. Master. [yuka@yuka-arch module-release]$ du lib/*.pcm | grep -v rdict | awk -F ' ' '{sum += $1} END {print sum}'. 360608. Processing /home/yuka/CERN/ROOT/memory.C... cpu time = 0.291462 seconds. sys time = 0.064409 seconds. res memory = 345.816 Mbytes. vir memory = 573.508 Mbytes. With this patch. [yuka@yuka-arch module-release]$ du lib/*.pcm | grep -v rdict | awk -F ' ' '{sum += $1} END {print sum}'. 343336. Processing /home/yuka/CERN/ROOT/memory.C... cpu time = 0.268580 seconds. sys time = 0.082866 seconds. res memory = 329.051 Mbytes. vir memory = 553.289 Mbytes. ```. It improves res memeory by 5% compared with master, and by 8% with",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2434
https://github.com/root-project/root/pull/2434:316,performance,memor,memory,316,"[cxxmodule] Improve pcm size by caching LookupHelper; ```. Master. [yuka@yuka-arch module-release]$ du lib/*.pcm | grep -v rdict | awk -F ' ' '{sum += $1} END {print sum}'. 360608. Processing /home/yuka/CERN/ROOT/memory.C... cpu time = 0.291462 seconds. sys time = 0.064409 seconds. res memory = 345.816 Mbytes. vir memory = 573.508 Mbytes. With this patch. [yuka@yuka-arch module-release]$ du lib/*.pcm | grep -v rdict | awk -F ' ' '{sum += $1} END {print sum}'. 343336. Processing /home/yuka/CERN/ROOT/memory.C... cpu time = 0.268580 seconds. sys time = 0.082866 seconds. res memory = 329.051 Mbytes. vir memory = 553.289 Mbytes. ```. It improves res memeory by 5% compared with master, and by 8% with",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2434
https://github.com/root-project/root/pull/2434:504,performance,memor,memory,504,"[cxxmodule] Improve pcm size by caching LookupHelper; ```. Master. [yuka@yuka-arch module-release]$ du lib/*.pcm | grep -v rdict | awk -F ' ' '{sum += $1} END {print sum}'. 360608. Processing /home/yuka/CERN/ROOT/memory.C... cpu time = 0.291462 seconds. sys time = 0.064409 seconds. res memory = 345.816 Mbytes. vir memory = 573.508 Mbytes. With this patch. [yuka@yuka-arch module-release]$ du lib/*.pcm | grep -v rdict | awk -F ' ' '{sum += $1} END {print sum}'. 343336. Processing /home/yuka/CERN/ROOT/memory.C... cpu time = 0.268580 seconds. sys time = 0.082866 seconds. res memory = 329.051 Mbytes. vir memory = 553.289 Mbytes. ```. It improves res memeory by 5% compared with master, and by 8% with",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2434
https://github.com/root-project/root/pull/2434:516,performance,cpu,cpu,516,"[cxxmodule] Improve pcm size by caching LookupHelper; ```. Master. [yuka@yuka-arch module-release]$ du lib/*.pcm | grep -v rdict | awk -F ' ' '{sum += $1} END {print sum}'. 360608. Processing /home/yuka/CERN/ROOT/memory.C... cpu time = 0.291462 seconds. sys time = 0.064409 seconds. res memory = 345.816 Mbytes. vir memory = 573.508 Mbytes. With this patch. [yuka@yuka-arch module-release]$ du lib/*.pcm | grep -v rdict | awk -F ' ' '{sum += $1} END {print sum}'. 343336. Processing /home/yuka/CERN/ROOT/memory.C... cpu time = 0.268580 seconds. sys time = 0.082866 seconds. res memory = 329.051 Mbytes. vir memory = 553.289 Mbytes. ```. It improves res memeory by 5% compared with master, and by 8% with",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2434
https://github.com/root-project/root/pull/2434:520,performance,time,time,520,"[cxxmodule] Improve pcm size by caching LookupHelper; ```. Master. [yuka@yuka-arch module-release]$ du lib/*.pcm | grep -v rdict | awk -F ' ' '{sum += $1} END {print sum}'. 360608. Processing /home/yuka/CERN/ROOT/memory.C... cpu time = 0.291462 seconds. sys time = 0.064409 seconds. res memory = 345.816 Mbytes. vir memory = 573.508 Mbytes. With this patch. [yuka@yuka-arch module-release]$ du lib/*.pcm | grep -v rdict | awk -F ' ' '{sum += $1} END {print sum}'. 343336. Processing /home/yuka/CERN/ROOT/memory.C... cpu time = 0.268580 seconds. sys time = 0.082866 seconds. res memory = 329.051 Mbytes. vir memory = 553.289 Mbytes. ```. It improves res memeory by 5% compared with master, and by 8% with",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2434
https://github.com/root-project/root/pull/2434:549,performance,time,time,549,"[cxxmodule] Improve pcm size by caching LookupHelper; ```. Master. [yuka@yuka-arch module-release]$ du lib/*.pcm | grep -v rdict | awk -F ' ' '{sum += $1} END {print sum}'. 360608. Processing /home/yuka/CERN/ROOT/memory.C... cpu time = 0.291462 seconds. sys time = 0.064409 seconds. res memory = 345.816 Mbytes. vir memory = 573.508 Mbytes. With this patch. [yuka@yuka-arch module-release]$ du lib/*.pcm | grep -v rdict | awk -F ' ' '{sum += $1} END {print sum}'. 343336. Processing /home/yuka/CERN/ROOT/memory.C... cpu time = 0.268580 seconds. sys time = 0.082866 seconds. res memory = 329.051 Mbytes. vir memory = 553.289 Mbytes. ```. It improves res memeory by 5% compared with master, and by 8% with",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2434
https://github.com/root-project/root/pull/2434:578,performance,memor,memory,578,"[cxxmodule] Improve pcm size by caching LookupHelper; ```. Master. [yuka@yuka-arch module-release]$ du lib/*.pcm | grep -v rdict | awk -F ' ' '{sum += $1} END {print sum}'. 360608. Processing /home/yuka/CERN/ROOT/memory.C... cpu time = 0.291462 seconds. sys time = 0.064409 seconds. res memory = 345.816 Mbytes. vir memory = 573.508 Mbytes. With this patch. [yuka@yuka-arch module-release]$ du lib/*.pcm | grep -v rdict | awk -F ' ' '{sum += $1} END {print sum}'. 343336. Processing /home/yuka/CERN/ROOT/memory.C... cpu time = 0.268580 seconds. sys time = 0.082866 seconds. res memory = 329.051 Mbytes. vir memory = 553.289 Mbytes. ```. It improves res memeory by 5% compared with master, and by 8% with",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2434
https://github.com/root-project/root/pull/2434:607,performance,memor,memory,607,"[cxxmodule] Improve pcm size by caching LookupHelper; ```. Master. [yuka@yuka-arch module-release]$ du lib/*.pcm | grep -v rdict | awk -F ' ' '{sum += $1} END {print sum}'. 360608. Processing /home/yuka/CERN/ROOT/memory.C... cpu time = 0.291462 seconds. sys time = 0.064409 seconds. res memory = 345.816 Mbytes. vir memory = 573.508 Mbytes. With this patch. [yuka@yuka-arch module-release]$ du lib/*.pcm | grep -v rdict | awk -F ' ' '{sum += $1} END {print sum}'. 343336. Processing /home/yuka/CERN/ROOT/memory.C... cpu time = 0.268580 seconds. sys time = 0.082866 seconds. res memory = 329.051 Mbytes. vir memory = 553.289 Mbytes. ```. It improves res memeory by 5% compared with master, and by 8% with",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2434
https://github.com/root-project/root/pull/2434:83,safety,modul,module-release,83,"[cxxmodule] Improve pcm size by caching LookupHelper; ```. Master. [yuka@yuka-arch module-release]$ du lib/*.pcm | grep -v rdict | awk -F ' ' '{sum += $1} END {print sum}'. 360608. Processing /home/yuka/CERN/ROOT/memory.C... cpu time = 0.291462 seconds. sys time = 0.064409 seconds. res memory = 345.816 Mbytes. vir memory = 573.508 Mbytes. With this patch. [yuka@yuka-arch module-release]$ du lib/*.pcm | grep -v rdict | awk -F ' ' '{sum += $1} END {print sum}'. 343336. Processing /home/yuka/CERN/ROOT/memory.C... cpu time = 0.268580 seconds. sys time = 0.082866 seconds. res memory = 329.051 Mbytes. vir memory = 553.289 Mbytes. ```. It improves res memeory by 5% compared with master, and by 8% with",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2434
https://github.com/root-project/root/pull/2434:351,safety,patch,patch,351,"[cxxmodule] Improve pcm size by caching LookupHelper; ```. Master. [yuka@yuka-arch module-release]$ du lib/*.pcm | grep -v rdict | awk -F ' ' '{sum += $1} END {print sum}'. 360608. Processing /home/yuka/CERN/ROOT/memory.C... cpu time = 0.291462 seconds. sys time = 0.064409 seconds. res memory = 345.816 Mbytes. vir memory = 573.508 Mbytes. With this patch. [yuka@yuka-arch module-release]$ du lib/*.pcm | grep -v rdict | awk -F ' ' '{sum += $1} END {print sum}'. 343336. Processing /home/yuka/CERN/ROOT/memory.C... cpu time = 0.268580 seconds. sys time = 0.082866 seconds. res memory = 329.051 Mbytes. vir memory = 553.289 Mbytes. ```. It improves res memeory by 5% compared with master, and by 8% with",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2434
https://github.com/root-project/root/pull/2434:374,safety,modul,module-release,374,"[cxxmodule] Improve pcm size by caching LookupHelper; ```. Master. [yuka@yuka-arch module-release]$ du lib/*.pcm | grep -v rdict | awk -F ' ' '{sum += $1} END {print sum}'. 360608. Processing /home/yuka/CERN/ROOT/memory.C... cpu time = 0.291462 seconds. sys time = 0.064409 seconds. res memory = 345.816 Mbytes. vir memory = 573.508 Mbytes. With this patch. [yuka@yuka-arch module-release]$ du lib/*.pcm | grep -v rdict | awk -F ' ' '{sum += $1} END {print sum}'. 343336. Processing /home/yuka/CERN/ROOT/memory.C... cpu time = 0.268580 seconds. sys time = 0.082866 seconds. res memory = 329.051 Mbytes. vir memory = 553.289 Mbytes. ```. It improves res memeory by 5% compared with master, and by 8% with",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2434
https://github.com/root-project/root/pull/2434:351,security,patch,patch,351,"[cxxmodule] Improve pcm size by caching LookupHelper; ```. Master. [yuka@yuka-arch module-release]$ du lib/*.pcm | grep -v rdict | awk -F ' ' '{sum += $1} END {print sum}'. 360608. Processing /home/yuka/CERN/ROOT/memory.C... cpu time = 0.291462 seconds. sys time = 0.064409 seconds. res memory = 345.816 Mbytes. vir memory = 573.508 Mbytes. With this patch. [yuka@yuka-arch module-release]$ du lib/*.pcm | grep -v rdict | awk -F ' ' '{sum += $1} END {print sum}'. 343336. Processing /home/yuka/CERN/ROOT/memory.C... cpu time = 0.268580 seconds. sys time = 0.082866 seconds. res memory = 329.051 Mbytes. vir memory = 553.289 Mbytes. ```. It improves res memeory by 5% compared with master, and by 8% with",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2434
https://github.com/root-project/root/pull/2434:213,usability,memor,memory,213,"[cxxmodule] Improve pcm size by caching LookupHelper; ```. Master. [yuka@yuka-arch module-release]$ du lib/*.pcm | grep -v rdict | awk -F ' ' '{sum += $1} END {print sum}'. 360608. Processing /home/yuka/CERN/ROOT/memory.C... cpu time = 0.291462 seconds. sys time = 0.064409 seconds. res memory = 345.816 Mbytes. vir memory = 573.508 Mbytes. With this patch. [yuka@yuka-arch module-release]$ du lib/*.pcm | grep -v rdict | awk -F ' ' '{sum += $1} END {print sum}'. 343336. Processing /home/yuka/CERN/ROOT/memory.C... cpu time = 0.268580 seconds. sys time = 0.082866 seconds. res memory = 329.051 Mbytes. vir memory = 553.289 Mbytes. ```. It improves res memeory by 5% compared with master, and by 8% with",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2434
https://github.com/root-project/root/pull/2434:287,usability,memor,memory,287,"[cxxmodule] Improve pcm size by caching LookupHelper; ```. Master. [yuka@yuka-arch module-release]$ du lib/*.pcm | grep -v rdict | awk -F ' ' '{sum += $1} END {print sum}'. 360608. Processing /home/yuka/CERN/ROOT/memory.C... cpu time = 0.291462 seconds. sys time = 0.064409 seconds. res memory = 345.816 Mbytes. vir memory = 573.508 Mbytes. With this patch. [yuka@yuka-arch module-release]$ du lib/*.pcm | grep -v rdict | awk -F ' ' '{sum += $1} END {print sum}'. 343336. Processing /home/yuka/CERN/ROOT/memory.C... cpu time = 0.268580 seconds. sys time = 0.082866 seconds. res memory = 329.051 Mbytes. vir memory = 553.289 Mbytes. ```. It improves res memeory by 5% compared with master, and by 8% with",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2434
https://github.com/root-project/root/pull/2434:316,usability,memor,memory,316,"[cxxmodule] Improve pcm size by caching LookupHelper; ```. Master. [yuka@yuka-arch module-release]$ du lib/*.pcm | grep -v rdict | awk -F ' ' '{sum += $1} END {print sum}'. 360608. Processing /home/yuka/CERN/ROOT/memory.C... cpu time = 0.291462 seconds. sys time = 0.064409 seconds. res memory = 345.816 Mbytes. vir memory = 573.508 Mbytes. With this patch. [yuka@yuka-arch module-release]$ du lib/*.pcm | grep -v rdict | awk -F ' ' '{sum += $1} END {print sum}'. 343336. Processing /home/yuka/CERN/ROOT/memory.C... cpu time = 0.268580 seconds. sys time = 0.082866 seconds. res memory = 329.051 Mbytes. vir memory = 553.289 Mbytes. ```. It improves res memeory by 5% compared with master, and by 8% with",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2434
https://github.com/root-project/root/pull/2434:504,usability,memor,memory,504,"[cxxmodule] Improve pcm size by caching LookupHelper; ```. Master. [yuka@yuka-arch module-release]$ du lib/*.pcm | grep -v rdict | awk -F ' ' '{sum += $1} END {print sum}'. 360608. Processing /home/yuka/CERN/ROOT/memory.C... cpu time = 0.291462 seconds. sys time = 0.064409 seconds. res memory = 345.816 Mbytes. vir memory = 573.508 Mbytes. With this patch. [yuka@yuka-arch module-release]$ du lib/*.pcm | grep -v rdict | awk -F ' ' '{sum += $1} END {print sum}'. 343336. Processing /home/yuka/CERN/ROOT/memory.C... cpu time = 0.268580 seconds. sys time = 0.082866 seconds. res memory = 329.051 Mbytes. vir memory = 553.289 Mbytes. ```. It improves res memeory by 5% compared with master, and by 8% with",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2434
https://github.com/root-project/root/pull/2434:578,usability,memor,memory,578,"[cxxmodule] Improve pcm size by caching LookupHelper; ```. Master. [yuka@yuka-arch module-release]$ du lib/*.pcm | grep -v rdict | awk -F ' ' '{sum += $1} END {print sum}'. 360608. Processing /home/yuka/CERN/ROOT/memory.C... cpu time = 0.291462 seconds. sys time = 0.064409 seconds. res memory = 345.816 Mbytes. vir memory = 573.508 Mbytes. With this patch. [yuka@yuka-arch module-release]$ du lib/*.pcm | grep -v rdict | awk -F ' ' '{sum += $1} END {print sum}'. 343336. Processing /home/yuka/CERN/ROOT/memory.C... cpu time = 0.268580 seconds. sys time = 0.082866 seconds. res memory = 329.051 Mbytes. vir memory = 553.289 Mbytes. ```. It improves res memeory by 5% compared with master, and by 8% with",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2434
https://github.com/root-project/root/pull/2434:607,usability,memor,memory,607,"[cxxmodule] Improve pcm size by caching LookupHelper; ```. Master. [yuka@yuka-arch module-release]$ du lib/*.pcm | grep -v rdict | awk -F ' ' '{sum += $1} END {print sum}'. 360608. Processing /home/yuka/CERN/ROOT/memory.C... cpu time = 0.291462 seconds. sys time = 0.064409 seconds. res memory = 345.816 Mbytes. vir memory = 573.508 Mbytes. With this patch. [yuka@yuka-arch module-release]$ du lib/*.pcm | grep -v rdict | awk -F ' ' '{sum += $1} END {print sum}'. 343336. Processing /home/yuka/CERN/ROOT/memory.C... cpu time = 0.268580 seconds. sys time = 0.082866 seconds. res memory = 329.051 Mbytes. vir memory = 553.289 Mbytes. ```. It improves res memeory by 5% compared with master, and by 8% with",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2434
https://github.com/root-project/root/pull/2435:38,deployability,modul,module,38,[cxxmodules] Do not exclude R related module from preloading; We were excluding these modules because it had conflict with local. variables such as PI.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2435
https://github.com/root-project/root/pull/2435:86,deployability,modul,modules,86,[cxxmodules] Do not exclude R related module from preloading; We were excluding these modules because it had conflict with local. variables such as PI.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2435
https://github.com/root-project/root/pull/2435:109,interoperability,conflict,conflict,109,[cxxmodules] Do not exclude R related module from preloading; We were excluding these modules because it had conflict with local. variables such as PI.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2435
https://github.com/root-project/root/pull/2435:38,modifiability,modul,module,38,[cxxmodules] Do not exclude R related module from preloading; We were excluding these modules because it had conflict with local. variables such as PI.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2435
https://github.com/root-project/root/pull/2435:86,modifiability,modul,modules,86,[cxxmodules] Do not exclude R related module from preloading; We were excluding these modules because it had conflict with local. variables such as PI.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2435
https://github.com/root-project/root/pull/2435:130,modifiability,variab,variables,130,[cxxmodules] Do not exclude R related module from preloading; We were excluding these modules because it had conflict with local. variables such as PI.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2435
https://github.com/root-project/root/pull/2435:38,safety,modul,module,38,[cxxmodules] Do not exclude R related module from preloading; We were excluding these modules because it had conflict with local. variables such as PI.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2435
https://github.com/root-project/root/pull/2435:86,safety,modul,modules,86,[cxxmodules] Do not exclude R related module from preloading; We were excluding these modules because it had conflict with local. variables such as PI.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2435
https://github.com/root-project/root/pull/2436:1054,availability,error,error,1054,Add missing libcrypto for undefined references; Missing linking to the libcrypto library causes undefined symbols:. ````. /usr/bin/ld: ../../lib/librpdutil.a(globus.cxx.o): in function `ROOT::GlbsToolCheckProxy(char**)':. /builddir/build/BUILD/root-6.14.02/net/rpdutils/src/globus.cxx:547: undefined reference to `PEM_read_X509'. /usr/bin/ld: /builddir/build/BUILD/root-6.14.02/net/rpdutils/src/globus.cxx:552: undefined reference to `X509_get_issuer_name'. /usr/bin/ld: /builddir/build/BUILD/root-6.14.02/net/rpdutils/src/globus.cxx:552: undefined reference to `X509_NAME_oneline'. /usr/bin/ld: ../../lib/librpdutil.a(globus.cxx.o): in function `ROOT::GlbsToolCheckCert(char**)':. /builddir/build/BUILD/root-6.14.02/net/rpdutils/src/globus.cxx:187: undefined reference to `PEM_read_X509'. /usr/bin/ld: /builddir/build/BUILD/root-6.14.02/net/rpdutils/src/globus.cxx:192: undefined reference to `X509_get_subject_name'. /usr/bin/ld: /builddir/build/BUILD/root-6.14.02/net/rpdutils/src/globus.cxx:192: undefined reference to `X509_NAME_oneline'. collect2: error: ld returned 1 exit status. ````.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2436
https://github.com/root-project/root/pull/2436:223,deployability,build,builddir,223,Add missing libcrypto for undefined references; Missing linking to the libcrypto library causes undefined symbols:. ````. /usr/bin/ld: ../../lib/librpdutil.a(globus.cxx.o): in function `ROOT::GlbsToolCheckProxy(char**)':. /builddir/build/BUILD/root-6.14.02/net/rpdutils/src/globus.cxx:547: undefined reference to `PEM_read_X509'. /usr/bin/ld: /builddir/build/BUILD/root-6.14.02/net/rpdutils/src/globus.cxx:552: undefined reference to `X509_get_issuer_name'. /usr/bin/ld: /builddir/build/BUILD/root-6.14.02/net/rpdutils/src/globus.cxx:552: undefined reference to `X509_NAME_oneline'. /usr/bin/ld: ../../lib/librpdutil.a(globus.cxx.o): in function `ROOT::GlbsToolCheckCert(char**)':. /builddir/build/BUILD/root-6.14.02/net/rpdutils/src/globus.cxx:187: undefined reference to `PEM_read_X509'. /usr/bin/ld: /builddir/build/BUILD/root-6.14.02/net/rpdutils/src/globus.cxx:192: undefined reference to `X509_get_subject_name'. /usr/bin/ld: /builddir/build/BUILD/root-6.14.02/net/rpdutils/src/globus.cxx:192: undefined reference to `X509_NAME_oneline'. collect2: error: ld returned 1 exit status. ````.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2436
https://github.com/root-project/root/pull/2436:232,deployability,build,build,232,Add missing libcrypto for undefined references; Missing linking to the libcrypto library causes undefined symbols:. ````. /usr/bin/ld: ../../lib/librpdutil.a(globus.cxx.o): in function `ROOT::GlbsToolCheckProxy(char**)':. /builddir/build/BUILD/root-6.14.02/net/rpdutils/src/globus.cxx:547: undefined reference to `PEM_read_X509'. /usr/bin/ld: /builddir/build/BUILD/root-6.14.02/net/rpdutils/src/globus.cxx:552: undefined reference to `X509_get_issuer_name'. /usr/bin/ld: /builddir/build/BUILD/root-6.14.02/net/rpdutils/src/globus.cxx:552: undefined reference to `X509_NAME_oneline'. /usr/bin/ld: ../../lib/librpdutil.a(globus.cxx.o): in function `ROOT::GlbsToolCheckCert(char**)':. /builddir/build/BUILD/root-6.14.02/net/rpdutils/src/globus.cxx:187: undefined reference to `PEM_read_X509'. /usr/bin/ld: /builddir/build/BUILD/root-6.14.02/net/rpdutils/src/globus.cxx:192: undefined reference to `X509_get_subject_name'. /usr/bin/ld: /builddir/build/BUILD/root-6.14.02/net/rpdutils/src/globus.cxx:192: undefined reference to `X509_NAME_oneline'. collect2: error: ld returned 1 exit status. ````.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2436
https://github.com/root-project/root/pull/2436:238,deployability,BUILD,BUILD,238,Add missing libcrypto for undefined references; Missing linking to the libcrypto library causes undefined symbols:. ````. /usr/bin/ld: ../../lib/librpdutil.a(globus.cxx.o): in function `ROOT::GlbsToolCheckProxy(char**)':. /builddir/build/BUILD/root-6.14.02/net/rpdutils/src/globus.cxx:547: undefined reference to `PEM_read_X509'. /usr/bin/ld: /builddir/build/BUILD/root-6.14.02/net/rpdutils/src/globus.cxx:552: undefined reference to `X509_get_issuer_name'. /usr/bin/ld: /builddir/build/BUILD/root-6.14.02/net/rpdutils/src/globus.cxx:552: undefined reference to `X509_NAME_oneline'. /usr/bin/ld: ../../lib/librpdutil.a(globus.cxx.o): in function `ROOT::GlbsToolCheckCert(char**)':. /builddir/build/BUILD/root-6.14.02/net/rpdutils/src/globus.cxx:187: undefined reference to `PEM_read_X509'. /usr/bin/ld: /builddir/build/BUILD/root-6.14.02/net/rpdutils/src/globus.cxx:192: undefined reference to `X509_get_subject_name'. /usr/bin/ld: /builddir/build/BUILD/root-6.14.02/net/rpdutils/src/globus.cxx:192: undefined reference to `X509_NAME_oneline'. collect2: error: ld returned 1 exit status. ````.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2436
https://github.com/root-project/root/pull/2436:344,deployability,build,builddir,344,Add missing libcrypto for undefined references; Missing linking to the libcrypto library causes undefined symbols:. ````. /usr/bin/ld: ../../lib/librpdutil.a(globus.cxx.o): in function `ROOT::GlbsToolCheckProxy(char**)':. /builddir/build/BUILD/root-6.14.02/net/rpdutils/src/globus.cxx:547: undefined reference to `PEM_read_X509'. /usr/bin/ld: /builddir/build/BUILD/root-6.14.02/net/rpdutils/src/globus.cxx:552: undefined reference to `X509_get_issuer_name'. /usr/bin/ld: /builddir/build/BUILD/root-6.14.02/net/rpdutils/src/globus.cxx:552: undefined reference to `X509_NAME_oneline'. /usr/bin/ld: ../../lib/librpdutil.a(globus.cxx.o): in function `ROOT::GlbsToolCheckCert(char**)':. /builddir/build/BUILD/root-6.14.02/net/rpdutils/src/globus.cxx:187: undefined reference to `PEM_read_X509'. /usr/bin/ld: /builddir/build/BUILD/root-6.14.02/net/rpdutils/src/globus.cxx:192: undefined reference to `X509_get_subject_name'. /usr/bin/ld: /builddir/build/BUILD/root-6.14.02/net/rpdutils/src/globus.cxx:192: undefined reference to `X509_NAME_oneline'. collect2: error: ld returned 1 exit status. ````.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2436
https://github.com/root-project/root/pull/2436:353,deployability,build,build,353,Add missing libcrypto for undefined references; Missing linking to the libcrypto library causes undefined symbols:. ````. /usr/bin/ld: ../../lib/librpdutil.a(globus.cxx.o): in function `ROOT::GlbsToolCheckProxy(char**)':. /builddir/build/BUILD/root-6.14.02/net/rpdutils/src/globus.cxx:547: undefined reference to `PEM_read_X509'. /usr/bin/ld: /builddir/build/BUILD/root-6.14.02/net/rpdutils/src/globus.cxx:552: undefined reference to `X509_get_issuer_name'. /usr/bin/ld: /builddir/build/BUILD/root-6.14.02/net/rpdutils/src/globus.cxx:552: undefined reference to `X509_NAME_oneline'. /usr/bin/ld: ../../lib/librpdutil.a(globus.cxx.o): in function `ROOT::GlbsToolCheckCert(char**)':. /builddir/build/BUILD/root-6.14.02/net/rpdutils/src/globus.cxx:187: undefined reference to `PEM_read_X509'. /usr/bin/ld: /builddir/build/BUILD/root-6.14.02/net/rpdutils/src/globus.cxx:192: undefined reference to `X509_get_subject_name'. /usr/bin/ld: /builddir/build/BUILD/root-6.14.02/net/rpdutils/src/globus.cxx:192: undefined reference to `X509_NAME_oneline'. collect2: error: ld returned 1 exit status. ````.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2436
https://github.com/root-project/root/pull/2436:359,deployability,BUILD,BUILD,359,Add missing libcrypto for undefined references; Missing linking to the libcrypto library causes undefined symbols:. ````. /usr/bin/ld: ../../lib/librpdutil.a(globus.cxx.o): in function `ROOT::GlbsToolCheckProxy(char**)':. /builddir/build/BUILD/root-6.14.02/net/rpdutils/src/globus.cxx:547: undefined reference to `PEM_read_X509'. /usr/bin/ld: /builddir/build/BUILD/root-6.14.02/net/rpdutils/src/globus.cxx:552: undefined reference to `X509_get_issuer_name'. /usr/bin/ld: /builddir/build/BUILD/root-6.14.02/net/rpdutils/src/globus.cxx:552: undefined reference to `X509_NAME_oneline'. /usr/bin/ld: ../../lib/librpdutil.a(globus.cxx.o): in function `ROOT::GlbsToolCheckCert(char**)':. /builddir/build/BUILD/root-6.14.02/net/rpdutils/src/globus.cxx:187: undefined reference to `PEM_read_X509'. /usr/bin/ld: /builddir/build/BUILD/root-6.14.02/net/rpdutils/src/globus.cxx:192: undefined reference to `X509_get_subject_name'. /usr/bin/ld: /builddir/build/BUILD/root-6.14.02/net/rpdutils/src/globus.cxx:192: undefined reference to `X509_NAME_oneline'. collect2: error: ld returned 1 exit status. ````.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2436
https://github.com/root-project/root/pull/2436:472,deployability,build,builddir,472,Add missing libcrypto for undefined references; Missing linking to the libcrypto library causes undefined symbols:. ````. /usr/bin/ld: ../../lib/librpdutil.a(globus.cxx.o): in function `ROOT::GlbsToolCheckProxy(char**)':. /builddir/build/BUILD/root-6.14.02/net/rpdutils/src/globus.cxx:547: undefined reference to `PEM_read_X509'. /usr/bin/ld: /builddir/build/BUILD/root-6.14.02/net/rpdutils/src/globus.cxx:552: undefined reference to `X509_get_issuer_name'. /usr/bin/ld: /builddir/build/BUILD/root-6.14.02/net/rpdutils/src/globus.cxx:552: undefined reference to `X509_NAME_oneline'. /usr/bin/ld: ../../lib/librpdutil.a(globus.cxx.o): in function `ROOT::GlbsToolCheckCert(char**)':. /builddir/build/BUILD/root-6.14.02/net/rpdutils/src/globus.cxx:187: undefined reference to `PEM_read_X509'. /usr/bin/ld: /builddir/build/BUILD/root-6.14.02/net/rpdutils/src/globus.cxx:192: undefined reference to `X509_get_subject_name'. /usr/bin/ld: /builddir/build/BUILD/root-6.14.02/net/rpdutils/src/globus.cxx:192: undefined reference to `X509_NAME_oneline'. collect2: error: ld returned 1 exit status. ````.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2436
https://github.com/root-project/root/pull/2436:481,deployability,build,build,481,Add missing libcrypto for undefined references; Missing linking to the libcrypto library causes undefined symbols:. ````. /usr/bin/ld: ../../lib/librpdutil.a(globus.cxx.o): in function `ROOT::GlbsToolCheckProxy(char**)':. /builddir/build/BUILD/root-6.14.02/net/rpdutils/src/globus.cxx:547: undefined reference to `PEM_read_X509'. /usr/bin/ld: /builddir/build/BUILD/root-6.14.02/net/rpdutils/src/globus.cxx:552: undefined reference to `X509_get_issuer_name'. /usr/bin/ld: /builddir/build/BUILD/root-6.14.02/net/rpdutils/src/globus.cxx:552: undefined reference to `X509_NAME_oneline'. /usr/bin/ld: ../../lib/librpdutil.a(globus.cxx.o): in function `ROOT::GlbsToolCheckCert(char**)':. /builddir/build/BUILD/root-6.14.02/net/rpdutils/src/globus.cxx:187: undefined reference to `PEM_read_X509'. /usr/bin/ld: /builddir/build/BUILD/root-6.14.02/net/rpdutils/src/globus.cxx:192: undefined reference to `X509_get_subject_name'. /usr/bin/ld: /builddir/build/BUILD/root-6.14.02/net/rpdutils/src/globus.cxx:192: undefined reference to `X509_NAME_oneline'. collect2: error: ld returned 1 exit status. ````.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2436
https://github.com/root-project/root/pull/2436:487,deployability,BUILD,BUILD,487,Add missing libcrypto for undefined references; Missing linking to the libcrypto library causes undefined symbols:. ````. /usr/bin/ld: ../../lib/librpdutil.a(globus.cxx.o): in function `ROOT::GlbsToolCheckProxy(char**)':. /builddir/build/BUILD/root-6.14.02/net/rpdutils/src/globus.cxx:547: undefined reference to `PEM_read_X509'. /usr/bin/ld: /builddir/build/BUILD/root-6.14.02/net/rpdutils/src/globus.cxx:552: undefined reference to `X509_get_issuer_name'. /usr/bin/ld: /builddir/build/BUILD/root-6.14.02/net/rpdutils/src/globus.cxx:552: undefined reference to `X509_NAME_oneline'. /usr/bin/ld: ../../lib/librpdutil.a(globus.cxx.o): in function `ROOT::GlbsToolCheckCert(char**)':. /builddir/build/BUILD/root-6.14.02/net/rpdutils/src/globus.cxx:187: undefined reference to `PEM_read_X509'. /usr/bin/ld: /builddir/build/BUILD/root-6.14.02/net/rpdutils/src/globus.cxx:192: undefined reference to `X509_get_subject_name'. /usr/bin/ld: /builddir/build/BUILD/root-6.14.02/net/rpdutils/src/globus.cxx:192: undefined reference to `X509_NAME_oneline'. collect2: error: ld returned 1 exit status. ````.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2436
https://github.com/root-project/root/pull/2436:683,deployability,build,builddir,683,Add missing libcrypto for undefined references; Missing linking to the libcrypto library causes undefined symbols:. ````. /usr/bin/ld: ../../lib/librpdutil.a(globus.cxx.o): in function `ROOT::GlbsToolCheckProxy(char**)':. /builddir/build/BUILD/root-6.14.02/net/rpdutils/src/globus.cxx:547: undefined reference to `PEM_read_X509'. /usr/bin/ld: /builddir/build/BUILD/root-6.14.02/net/rpdutils/src/globus.cxx:552: undefined reference to `X509_get_issuer_name'. /usr/bin/ld: /builddir/build/BUILD/root-6.14.02/net/rpdutils/src/globus.cxx:552: undefined reference to `X509_NAME_oneline'. /usr/bin/ld: ../../lib/librpdutil.a(globus.cxx.o): in function `ROOT::GlbsToolCheckCert(char**)':. /builddir/build/BUILD/root-6.14.02/net/rpdutils/src/globus.cxx:187: undefined reference to `PEM_read_X509'. /usr/bin/ld: /builddir/build/BUILD/root-6.14.02/net/rpdutils/src/globus.cxx:192: undefined reference to `X509_get_subject_name'. /usr/bin/ld: /builddir/build/BUILD/root-6.14.02/net/rpdutils/src/globus.cxx:192: undefined reference to `X509_NAME_oneline'. collect2: error: ld returned 1 exit status. ````.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2436
https://github.com/root-project/root/pull/2436:692,deployability,build,build,692,Add missing libcrypto for undefined references; Missing linking to the libcrypto library causes undefined symbols:. ````. /usr/bin/ld: ../../lib/librpdutil.a(globus.cxx.o): in function `ROOT::GlbsToolCheckProxy(char**)':. /builddir/build/BUILD/root-6.14.02/net/rpdutils/src/globus.cxx:547: undefined reference to `PEM_read_X509'. /usr/bin/ld: /builddir/build/BUILD/root-6.14.02/net/rpdutils/src/globus.cxx:552: undefined reference to `X509_get_issuer_name'. /usr/bin/ld: /builddir/build/BUILD/root-6.14.02/net/rpdutils/src/globus.cxx:552: undefined reference to `X509_NAME_oneline'. /usr/bin/ld: ../../lib/librpdutil.a(globus.cxx.o): in function `ROOT::GlbsToolCheckCert(char**)':. /builddir/build/BUILD/root-6.14.02/net/rpdutils/src/globus.cxx:187: undefined reference to `PEM_read_X509'. /usr/bin/ld: /builddir/build/BUILD/root-6.14.02/net/rpdutils/src/globus.cxx:192: undefined reference to `X509_get_subject_name'. /usr/bin/ld: /builddir/build/BUILD/root-6.14.02/net/rpdutils/src/globus.cxx:192: undefined reference to `X509_NAME_oneline'. collect2: error: ld returned 1 exit status. ````.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2436
https://github.com/root-project/root/pull/2436:698,deployability,BUILD,BUILD,698,Add missing libcrypto for undefined references; Missing linking to the libcrypto library causes undefined symbols:. ````. /usr/bin/ld: ../../lib/librpdutil.a(globus.cxx.o): in function `ROOT::GlbsToolCheckProxy(char**)':. /builddir/build/BUILD/root-6.14.02/net/rpdutils/src/globus.cxx:547: undefined reference to `PEM_read_X509'. /usr/bin/ld: /builddir/build/BUILD/root-6.14.02/net/rpdutils/src/globus.cxx:552: undefined reference to `X509_get_issuer_name'. /usr/bin/ld: /builddir/build/BUILD/root-6.14.02/net/rpdutils/src/globus.cxx:552: undefined reference to `X509_NAME_oneline'. /usr/bin/ld: ../../lib/librpdutil.a(globus.cxx.o): in function `ROOT::GlbsToolCheckCert(char**)':. /builddir/build/BUILD/root-6.14.02/net/rpdutils/src/globus.cxx:187: undefined reference to `PEM_read_X509'. /usr/bin/ld: /builddir/build/BUILD/root-6.14.02/net/rpdutils/src/globus.cxx:192: undefined reference to `X509_get_subject_name'. /usr/bin/ld: /builddir/build/BUILD/root-6.14.02/net/rpdutils/src/globus.cxx:192: undefined reference to `X509_NAME_oneline'. collect2: error: ld returned 1 exit status. ````.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2436
https://github.com/root-project/root/pull/2436:804,deployability,build,builddir,804,Add missing libcrypto for undefined references; Missing linking to the libcrypto library causes undefined symbols:. ````. /usr/bin/ld: ../../lib/librpdutil.a(globus.cxx.o): in function `ROOT::GlbsToolCheckProxy(char**)':. /builddir/build/BUILD/root-6.14.02/net/rpdutils/src/globus.cxx:547: undefined reference to `PEM_read_X509'. /usr/bin/ld: /builddir/build/BUILD/root-6.14.02/net/rpdutils/src/globus.cxx:552: undefined reference to `X509_get_issuer_name'. /usr/bin/ld: /builddir/build/BUILD/root-6.14.02/net/rpdutils/src/globus.cxx:552: undefined reference to `X509_NAME_oneline'. /usr/bin/ld: ../../lib/librpdutil.a(globus.cxx.o): in function `ROOT::GlbsToolCheckCert(char**)':. /builddir/build/BUILD/root-6.14.02/net/rpdutils/src/globus.cxx:187: undefined reference to `PEM_read_X509'. /usr/bin/ld: /builddir/build/BUILD/root-6.14.02/net/rpdutils/src/globus.cxx:192: undefined reference to `X509_get_subject_name'. /usr/bin/ld: /builddir/build/BUILD/root-6.14.02/net/rpdutils/src/globus.cxx:192: undefined reference to `X509_NAME_oneline'. collect2: error: ld returned 1 exit status. ````.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2436
https://github.com/root-project/root/pull/2436:813,deployability,build,build,813,Add missing libcrypto for undefined references; Missing linking to the libcrypto library causes undefined symbols:. ````. /usr/bin/ld: ../../lib/librpdutil.a(globus.cxx.o): in function `ROOT::GlbsToolCheckProxy(char**)':. /builddir/build/BUILD/root-6.14.02/net/rpdutils/src/globus.cxx:547: undefined reference to `PEM_read_X509'. /usr/bin/ld: /builddir/build/BUILD/root-6.14.02/net/rpdutils/src/globus.cxx:552: undefined reference to `X509_get_issuer_name'. /usr/bin/ld: /builddir/build/BUILD/root-6.14.02/net/rpdutils/src/globus.cxx:552: undefined reference to `X509_NAME_oneline'. /usr/bin/ld: ../../lib/librpdutil.a(globus.cxx.o): in function `ROOT::GlbsToolCheckCert(char**)':. /builddir/build/BUILD/root-6.14.02/net/rpdutils/src/globus.cxx:187: undefined reference to `PEM_read_X509'. /usr/bin/ld: /builddir/build/BUILD/root-6.14.02/net/rpdutils/src/globus.cxx:192: undefined reference to `X509_get_subject_name'. /usr/bin/ld: /builddir/build/BUILD/root-6.14.02/net/rpdutils/src/globus.cxx:192: undefined reference to `X509_NAME_oneline'. collect2: error: ld returned 1 exit status. ````.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2436
https://github.com/root-project/root/pull/2436:819,deployability,BUILD,BUILD,819,Add missing libcrypto for undefined references; Missing linking to the libcrypto library causes undefined symbols:. ````. /usr/bin/ld: ../../lib/librpdutil.a(globus.cxx.o): in function `ROOT::GlbsToolCheckProxy(char**)':. /builddir/build/BUILD/root-6.14.02/net/rpdutils/src/globus.cxx:547: undefined reference to `PEM_read_X509'. /usr/bin/ld: /builddir/build/BUILD/root-6.14.02/net/rpdutils/src/globus.cxx:552: undefined reference to `X509_get_issuer_name'. /usr/bin/ld: /builddir/build/BUILD/root-6.14.02/net/rpdutils/src/globus.cxx:552: undefined reference to `X509_NAME_oneline'. /usr/bin/ld: ../../lib/librpdutil.a(globus.cxx.o): in function `ROOT::GlbsToolCheckCert(char**)':. /builddir/build/BUILD/root-6.14.02/net/rpdutils/src/globus.cxx:187: undefined reference to `PEM_read_X509'. /usr/bin/ld: /builddir/build/BUILD/root-6.14.02/net/rpdutils/src/globus.cxx:192: undefined reference to `X509_get_subject_name'. /usr/bin/ld: /builddir/build/BUILD/root-6.14.02/net/rpdutils/src/globus.cxx:192: undefined reference to `X509_NAME_oneline'. collect2: error: ld returned 1 exit status. ````.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2436
https://github.com/root-project/root/pull/2436:933,deployability,build,builddir,933,Add missing libcrypto for undefined references; Missing linking to the libcrypto library causes undefined symbols:. ````. /usr/bin/ld: ../../lib/librpdutil.a(globus.cxx.o): in function `ROOT::GlbsToolCheckProxy(char**)':. /builddir/build/BUILD/root-6.14.02/net/rpdutils/src/globus.cxx:547: undefined reference to `PEM_read_X509'. /usr/bin/ld: /builddir/build/BUILD/root-6.14.02/net/rpdutils/src/globus.cxx:552: undefined reference to `X509_get_issuer_name'. /usr/bin/ld: /builddir/build/BUILD/root-6.14.02/net/rpdutils/src/globus.cxx:552: undefined reference to `X509_NAME_oneline'. /usr/bin/ld: ../../lib/librpdutil.a(globus.cxx.o): in function `ROOT::GlbsToolCheckCert(char**)':. /builddir/build/BUILD/root-6.14.02/net/rpdutils/src/globus.cxx:187: undefined reference to `PEM_read_X509'. /usr/bin/ld: /builddir/build/BUILD/root-6.14.02/net/rpdutils/src/globus.cxx:192: undefined reference to `X509_get_subject_name'. /usr/bin/ld: /builddir/build/BUILD/root-6.14.02/net/rpdutils/src/globus.cxx:192: undefined reference to `X509_NAME_oneline'. collect2: error: ld returned 1 exit status. ````.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2436
https://github.com/root-project/root/pull/2436:942,deployability,build,build,942,Add missing libcrypto for undefined references; Missing linking to the libcrypto library causes undefined symbols:. ````. /usr/bin/ld: ../../lib/librpdutil.a(globus.cxx.o): in function `ROOT::GlbsToolCheckProxy(char**)':. /builddir/build/BUILD/root-6.14.02/net/rpdutils/src/globus.cxx:547: undefined reference to `PEM_read_X509'. /usr/bin/ld: /builddir/build/BUILD/root-6.14.02/net/rpdutils/src/globus.cxx:552: undefined reference to `X509_get_issuer_name'. /usr/bin/ld: /builddir/build/BUILD/root-6.14.02/net/rpdutils/src/globus.cxx:552: undefined reference to `X509_NAME_oneline'. /usr/bin/ld: ../../lib/librpdutil.a(globus.cxx.o): in function `ROOT::GlbsToolCheckCert(char**)':. /builddir/build/BUILD/root-6.14.02/net/rpdutils/src/globus.cxx:187: undefined reference to `PEM_read_X509'. /usr/bin/ld: /builddir/build/BUILD/root-6.14.02/net/rpdutils/src/globus.cxx:192: undefined reference to `X509_get_subject_name'. /usr/bin/ld: /builddir/build/BUILD/root-6.14.02/net/rpdutils/src/globus.cxx:192: undefined reference to `X509_NAME_oneline'. collect2: error: ld returned 1 exit status. ````.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2436
https://github.com/root-project/root/pull/2436:948,deployability,BUILD,BUILD,948,Add missing libcrypto for undefined references; Missing linking to the libcrypto library causes undefined symbols:. ````. /usr/bin/ld: ../../lib/librpdutil.a(globus.cxx.o): in function `ROOT::GlbsToolCheckProxy(char**)':. /builddir/build/BUILD/root-6.14.02/net/rpdutils/src/globus.cxx:547: undefined reference to `PEM_read_X509'. /usr/bin/ld: /builddir/build/BUILD/root-6.14.02/net/rpdutils/src/globus.cxx:552: undefined reference to `X509_get_issuer_name'. /usr/bin/ld: /builddir/build/BUILD/root-6.14.02/net/rpdutils/src/globus.cxx:552: undefined reference to `X509_NAME_oneline'. /usr/bin/ld: ../../lib/librpdutil.a(globus.cxx.o): in function `ROOT::GlbsToolCheckCert(char**)':. /builddir/build/BUILD/root-6.14.02/net/rpdutils/src/globus.cxx:187: undefined reference to `PEM_read_X509'. /usr/bin/ld: /builddir/build/BUILD/root-6.14.02/net/rpdutils/src/globus.cxx:192: undefined reference to `X509_get_subject_name'. /usr/bin/ld: /builddir/build/BUILD/root-6.14.02/net/rpdutils/src/globus.cxx:192: undefined reference to `X509_NAME_oneline'. collect2: error: ld returned 1 exit status. ````.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2436
https://github.com/root-project/root/pull/2436:1054,performance,error,error,1054,Add missing libcrypto for undefined references; Missing linking to the libcrypto library causes undefined symbols:. ````. /usr/bin/ld: ../../lib/librpdutil.a(globus.cxx.o): in function `ROOT::GlbsToolCheckProxy(char**)':. /builddir/build/BUILD/root-6.14.02/net/rpdutils/src/globus.cxx:547: undefined reference to `PEM_read_X509'. /usr/bin/ld: /builddir/build/BUILD/root-6.14.02/net/rpdutils/src/globus.cxx:552: undefined reference to `X509_get_issuer_name'. /usr/bin/ld: /builddir/build/BUILD/root-6.14.02/net/rpdutils/src/globus.cxx:552: undefined reference to `X509_NAME_oneline'. /usr/bin/ld: ../../lib/librpdutil.a(globus.cxx.o): in function `ROOT::GlbsToolCheckCert(char**)':. /builddir/build/BUILD/root-6.14.02/net/rpdutils/src/globus.cxx:187: undefined reference to `PEM_read_X509'. /usr/bin/ld: /builddir/build/BUILD/root-6.14.02/net/rpdutils/src/globus.cxx:192: undefined reference to `X509_get_subject_name'. /usr/bin/ld: /builddir/build/BUILD/root-6.14.02/net/rpdutils/src/globus.cxx:192: undefined reference to `X509_NAME_oneline'. collect2: error: ld returned 1 exit status. ````.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2436
https://github.com/root-project/root/pull/2436:1054,safety,error,error,1054,Add missing libcrypto for undefined references; Missing linking to the libcrypto library causes undefined symbols:. ````. /usr/bin/ld: ../../lib/librpdutil.a(globus.cxx.o): in function `ROOT::GlbsToolCheckProxy(char**)':. /builddir/build/BUILD/root-6.14.02/net/rpdutils/src/globus.cxx:547: undefined reference to `PEM_read_X509'. /usr/bin/ld: /builddir/build/BUILD/root-6.14.02/net/rpdutils/src/globus.cxx:552: undefined reference to `X509_get_issuer_name'. /usr/bin/ld: /builddir/build/BUILD/root-6.14.02/net/rpdutils/src/globus.cxx:552: undefined reference to `X509_NAME_oneline'. /usr/bin/ld: ../../lib/librpdutil.a(globus.cxx.o): in function `ROOT::GlbsToolCheckCert(char**)':. /builddir/build/BUILD/root-6.14.02/net/rpdutils/src/globus.cxx:187: undefined reference to `PEM_read_X509'. /usr/bin/ld: /builddir/build/BUILD/root-6.14.02/net/rpdutils/src/globus.cxx:192: undefined reference to `X509_get_subject_name'. /usr/bin/ld: /builddir/build/BUILD/root-6.14.02/net/rpdutils/src/globus.cxx:192: undefined reference to `X509_NAME_oneline'. collect2: error: ld returned 1 exit status. ````.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2436
https://github.com/root-project/root/pull/2436:1054,usability,error,error,1054,Add missing libcrypto for undefined references; Missing linking to the libcrypto library causes undefined symbols:. ````. /usr/bin/ld: ../../lib/librpdutil.a(globus.cxx.o): in function `ROOT::GlbsToolCheckProxy(char**)':. /builddir/build/BUILD/root-6.14.02/net/rpdutils/src/globus.cxx:547: undefined reference to `PEM_read_X509'. /usr/bin/ld: /builddir/build/BUILD/root-6.14.02/net/rpdutils/src/globus.cxx:552: undefined reference to `X509_get_issuer_name'. /usr/bin/ld: /builddir/build/BUILD/root-6.14.02/net/rpdutils/src/globus.cxx:552: undefined reference to `X509_NAME_oneline'. /usr/bin/ld: ../../lib/librpdutil.a(globus.cxx.o): in function `ROOT::GlbsToolCheckCert(char**)':. /builddir/build/BUILD/root-6.14.02/net/rpdutils/src/globus.cxx:187: undefined reference to `PEM_read_X509'. /usr/bin/ld: /builddir/build/BUILD/root-6.14.02/net/rpdutils/src/globus.cxx:192: undefined reference to `X509_get_subject_name'. /usr/bin/ld: /builddir/build/BUILD/root-6.14.02/net/rpdutils/src/globus.cxx:192: undefined reference to `X509_NAME_oneline'. collect2: error: ld returned 1 exit status. ````.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2436
https://github.com/root-project/root/pull/2436:1080,usability,statu,status,1080,Add missing libcrypto for undefined references; Missing linking to the libcrypto library causes undefined symbols:. ````. /usr/bin/ld: ../../lib/librpdutil.a(globus.cxx.o): in function `ROOT::GlbsToolCheckProxy(char**)':. /builddir/build/BUILD/root-6.14.02/net/rpdutils/src/globus.cxx:547: undefined reference to `PEM_read_X509'. /usr/bin/ld: /builddir/build/BUILD/root-6.14.02/net/rpdutils/src/globus.cxx:552: undefined reference to `X509_get_issuer_name'. /usr/bin/ld: /builddir/build/BUILD/root-6.14.02/net/rpdutils/src/globus.cxx:552: undefined reference to `X509_NAME_oneline'. /usr/bin/ld: ../../lib/librpdutil.a(globus.cxx.o): in function `ROOT::GlbsToolCheckCert(char**)':. /builddir/build/BUILD/root-6.14.02/net/rpdutils/src/globus.cxx:187: undefined reference to `PEM_read_X509'. /usr/bin/ld: /builddir/build/BUILD/root-6.14.02/net/rpdutils/src/globus.cxx:192: undefined reference to `X509_get_subject_name'. /usr/bin/ld: /builddir/build/BUILD/root-6.14.02/net/rpdutils/src/globus.cxx:192: undefined reference to `X509_NAME_oneline'. collect2: error: ld returned 1 exit status. ````.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2436
https://github.com/root-project/root/pull/2437:343,deployability,log,logic,343,"[TTree] Avoid time-consuming part of FindLeaf if leaf name has no '.'; Just a proposal :). Feel free to close if not interesting. The `TString::Form` calls in `TTree::FindLeaf` are by far the most expensive calls in the function and show up as a significant part of the runtime of `TTreeReaderArray::CreateProxy`. If I'm not mistaken all that logic can be skipped in case `searchname` does not contain a dot, with significant speed-ups for such a `FindLeaf` call.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2437
https://github.com/root-project/root/pull/2437:394,deployability,contain,contain,394,"[TTree] Avoid time-consuming part of FindLeaf if leaf name has no '.'; Just a proposal :). Feel free to close if not interesting. The `TString::Form` calls in `TTree::FindLeaf` are by far the most expensive calls in the function and show up as a significant part of the runtime of `TTreeReaderArray::CreateProxy`. If I'm not mistaken all that logic can be skipped in case `searchname` does not contain a dot, with significant speed-ups for such a `FindLeaf` call.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2437
https://github.com/root-project/root/pull/2437:14,performance,time,time-consuming,14,"[TTree] Avoid time-consuming part of FindLeaf if leaf name has no '.'; Just a proposal :). Feel free to close if not interesting. The `TString::Form` calls in `TTree::FindLeaf` are by far the most expensive calls in the function and show up as a significant part of the runtime of `TTreeReaderArray::CreateProxy`. If I'm not mistaken all that logic can be skipped in case `searchname` does not contain a dot, with significant speed-ups for such a `FindLeaf` call.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2437
https://github.com/root-project/root/pull/2437:385,reliability,doe,does,385,"[TTree] Avoid time-consuming part of FindLeaf if leaf name has no '.'; Just a proposal :). Feel free to close if not interesting. The `TString::Form` calls in `TTree::FindLeaf` are by far the most expensive calls in the function and show up as a significant part of the runtime of `TTreeReaderArray::CreateProxy`. If I'm not mistaken all that logic can be skipped in case `searchname` does not contain a dot, with significant speed-ups for such a `FindLeaf` call.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2437
https://github.com/root-project/root/pull/2437:8,safety,Avoid,Avoid,8,"[TTree] Avoid time-consuming part of FindLeaf if leaf name has no '.'; Just a proposal :). Feel free to close if not interesting. The `TString::Form` calls in `TTree::FindLeaf` are by far the most expensive calls in the function and show up as a significant part of the runtime of `TTreeReaderArray::CreateProxy`. If I'm not mistaken all that logic can be skipped in case `searchname` does not contain a dot, with significant speed-ups for such a `FindLeaf` call.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2437
https://github.com/root-project/root/pull/2437:343,safety,log,logic,343,"[TTree] Avoid time-consuming part of FindLeaf if leaf name has no '.'; Just a proposal :). Feel free to close if not interesting. The `TString::Form` calls in `TTree::FindLeaf` are by far the most expensive calls in the function and show up as a significant part of the runtime of `TTreeReaderArray::CreateProxy`. If I'm not mistaken all that logic can be skipped in case `searchname` does not contain a dot, with significant speed-ups for such a `FindLeaf` call.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2437
https://github.com/root-project/root/pull/2437:246,security,sign,significant,246,"[TTree] Avoid time-consuming part of FindLeaf if leaf name has no '.'; Just a proposal :). Feel free to close if not interesting. The `TString::Form` calls in `TTree::FindLeaf` are by far the most expensive calls in the function and show up as a significant part of the runtime of `TTreeReaderArray::CreateProxy`. If I'm not mistaken all that logic can be skipped in case `searchname` does not contain a dot, with significant speed-ups for such a `FindLeaf` call.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2437
https://github.com/root-project/root/pull/2437:343,security,log,logic,343,"[TTree] Avoid time-consuming part of FindLeaf if leaf name has no '.'; Just a proposal :). Feel free to close if not interesting. The `TString::Form` calls in `TTree::FindLeaf` are by far the most expensive calls in the function and show up as a significant part of the runtime of `TTreeReaderArray::CreateProxy`. If I'm not mistaken all that logic can be skipped in case `searchname` does not contain a dot, with significant speed-ups for such a `FindLeaf` call.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2437
https://github.com/root-project/root/pull/2437:414,security,sign,significant,414,"[TTree] Avoid time-consuming part of FindLeaf if leaf name has no '.'; Just a proposal :). Feel free to close if not interesting. The `TString::Form` calls in `TTree::FindLeaf` are by far the most expensive calls in the function and show up as a significant part of the runtime of `TTreeReaderArray::CreateProxy`. If I'm not mistaken all that logic can be skipped in case `searchname` does not contain a dot, with significant speed-ups for such a `FindLeaf` call.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2437
https://github.com/root-project/root/pull/2437:343,testability,log,logic,343,"[TTree] Avoid time-consuming part of FindLeaf if leaf name has no '.'; Just a proposal :). Feel free to close if not interesting. The `TString::Form` calls in `TTree::FindLeaf` are by far the most expensive calls in the function and show up as a significant part of the runtime of `TTreeReaderArray::CreateProxy`. If I'm not mistaken all that logic can be skipped in case `searchname` does not contain a dot, with significant speed-ups for such a `FindLeaf` call.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2437
https://github.com/root-project/root/pull/2437:104,usability,close,close,104,"[TTree] Avoid time-consuming part of FindLeaf if leaf name has no '.'; Just a proposal :). Feel free to close if not interesting. The `TString::Form` calls in `TTree::FindLeaf` are by far the most expensive calls in the function and show up as a significant part of the runtime of `TTreeReaderArray::CreateProxy`. If I'm not mistaken all that logic can be skipped in case `searchname` does not contain a dot, with significant speed-ups for such a `FindLeaf` call.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2437
https://github.com/root-project/root/pull/2438:11,deployability,API,API,11,[TMVA] Add API Support for Adam Optimizer.; * Implement Adam Optimizer. * Add Unit tests and MethodDL tests. Reference Implementation: [Tensorflow Adam Optimizer](https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer),MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2438
https://github.com/root-project/root/pull/2438:32,energy efficiency,Optim,Optimizer,32,[TMVA] Add API Support for Adam Optimizer.; * Implement Adam Optimizer. * Add Unit tests and MethodDL tests. Reference Implementation: [Tensorflow Adam Optimizer](https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer),MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2438
https://github.com/root-project/root/pull/2438:61,energy efficiency,Optim,Optimizer,61,[TMVA] Add API Support for Adam Optimizer.; * Implement Adam Optimizer. * Add Unit tests and MethodDL tests. Reference Implementation: [Tensorflow Adam Optimizer](https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer),MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2438
https://github.com/root-project/root/pull/2438:152,energy efficiency,Optim,Optimizer,152,[TMVA] Add API Support for Adam Optimizer.; * Implement Adam Optimizer. * Add Unit tests and MethodDL tests. Reference Implementation: [Tensorflow Adam Optimizer](https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer),MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2438
https://github.com/root-project/root/pull/2438:11,integrability,API,API,11,[TMVA] Add API Support for Adam Optimizer.; * Implement Adam Optimizer. * Add Unit tests and MethodDL tests. Reference Implementation: [Tensorflow Adam Optimizer](https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer),MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2438
https://github.com/root-project/root/pull/2438:11,interoperability,API,API,11,[TMVA] Add API Support for Adam Optimizer.; * Implement Adam Optimizer. * Add Unit tests and MethodDL tests. Reference Implementation: [Tensorflow Adam Optimizer](https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer),MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2438
https://github.com/root-project/root/pull/2438:32,performance,Optimiz,Optimizer,32,[TMVA] Add API Support for Adam Optimizer.; * Implement Adam Optimizer. * Add Unit tests and MethodDL tests. Reference Implementation: [Tensorflow Adam Optimizer](https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer),MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2438
https://github.com/root-project/root/pull/2438:61,performance,Optimiz,Optimizer,61,[TMVA] Add API Support for Adam Optimizer.; * Implement Adam Optimizer. * Add Unit tests and MethodDL tests. Reference Implementation: [Tensorflow Adam Optimizer](https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer),MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2438
https://github.com/root-project/root/pull/2438:152,performance,Optimiz,Optimizer,152,[TMVA] Add API Support for Adam Optimizer.; * Implement Adam Optimizer. * Add Unit tests and MethodDL tests. Reference Implementation: [Tensorflow Adam Optimizer](https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer),MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2438
https://github.com/root-project/root/pull/2438:83,safety,test,tests,83,[TMVA] Add API Support for Adam Optimizer.; * Implement Adam Optimizer. * Add Unit tests and MethodDL tests. Reference Implementation: [Tensorflow Adam Optimizer](https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer),MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2438
https://github.com/root-project/root/pull/2438:102,safety,test,tests,102,[TMVA] Add API Support for Adam Optimizer.; * Implement Adam Optimizer. * Add Unit tests and MethodDL tests. Reference Implementation: [Tensorflow Adam Optimizer](https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer),MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2438
https://github.com/root-project/root/pull/2438:78,testability,Unit,Unit,78,[TMVA] Add API Support for Adam Optimizer.; * Implement Adam Optimizer. * Add Unit tests and MethodDL tests. Reference Implementation: [Tensorflow Adam Optimizer](https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer),MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2438
https://github.com/root-project/root/pull/2438:83,testability,test,tests,83,[TMVA] Add API Support for Adam Optimizer.; * Implement Adam Optimizer. * Add Unit tests and MethodDL tests. Reference Implementation: [Tensorflow Adam Optimizer](https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer),MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2438
https://github.com/root-project/root/pull/2438:102,testability,test,tests,102,[TMVA] Add API Support for Adam Optimizer.; * Implement Adam Optimizer. * Add Unit tests and MethodDL tests. Reference Implementation: [Tensorflow Adam Optimizer](https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer),MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2438
https://github.com/root-project/root/pull/2438:15,usability,Support,Support,15,[TMVA] Add API Support for Adam Optimizer.; * Implement Adam Optimizer. * Add Unit tests and MethodDL tests. Reference Implementation: [Tensorflow Adam Optimizer](https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer),MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2438
https://github.com/root-project/root/pull/2439:11,deployability,API,API,11,[TMVA] Add API Support for Adagrad Optimizer.; * Implement Adagrad Optimizer. * Add Unit tests and MethodDL tests. Reference Implementation: [Tensorflow Adagrad Optimizer](https://www.tensorflow.org/api_docs/python/tf/train/AdagradOptimizer),MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2439
https://github.com/root-project/root/pull/2439:35,energy efficiency,Optim,Optimizer,35,[TMVA] Add API Support for Adagrad Optimizer.; * Implement Adagrad Optimizer. * Add Unit tests and MethodDL tests. Reference Implementation: [Tensorflow Adagrad Optimizer](https://www.tensorflow.org/api_docs/python/tf/train/AdagradOptimizer),MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2439
https://github.com/root-project/root/pull/2439:67,energy efficiency,Optim,Optimizer,67,[TMVA] Add API Support for Adagrad Optimizer.; * Implement Adagrad Optimizer. * Add Unit tests and MethodDL tests. Reference Implementation: [Tensorflow Adagrad Optimizer](https://www.tensorflow.org/api_docs/python/tf/train/AdagradOptimizer),MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2439
https://github.com/root-project/root/pull/2439:161,energy efficiency,Optim,Optimizer,161,[TMVA] Add API Support for Adagrad Optimizer.; * Implement Adagrad Optimizer. * Add Unit tests and MethodDL tests. Reference Implementation: [Tensorflow Adagrad Optimizer](https://www.tensorflow.org/api_docs/python/tf/train/AdagradOptimizer),MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2439
https://github.com/root-project/root/pull/2439:11,integrability,API,API,11,[TMVA] Add API Support for Adagrad Optimizer.; * Implement Adagrad Optimizer. * Add Unit tests and MethodDL tests. Reference Implementation: [Tensorflow Adagrad Optimizer](https://www.tensorflow.org/api_docs/python/tf/train/AdagradOptimizer),MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2439
https://github.com/root-project/root/pull/2439:11,interoperability,API,API,11,[TMVA] Add API Support for Adagrad Optimizer.; * Implement Adagrad Optimizer. * Add Unit tests and MethodDL tests. Reference Implementation: [Tensorflow Adagrad Optimizer](https://www.tensorflow.org/api_docs/python/tf/train/AdagradOptimizer),MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2439
https://github.com/root-project/root/pull/2439:35,performance,Optimiz,Optimizer,35,[TMVA] Add API Support for Adagrad Optimizer.; * Implement Adagrad Optimizer. * Add Unit tests and MethodDL tests. Reference Implementation: [Tensorflow Adagrad Optimizer](https://www.tensorflow.org/api_docs/python/tf/train/AdagradOptimizer),MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2439
https://github.com/root-project/root/pull/2439:67,performance,Optimiz,Optimizer,67,[TMVA] Add API Support for Adagrad Optimizer.; * Implement Adagrad Optimizer. * Add Unit tests and MethodDL tests. Reference Implementation: [Tensorflow Adagrad Optimizer](https://www.tensorflow.org/api_docs/python/tf/train/AdagradOptimizer),MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2439
https://github.com/root-project/root/pull/2439:161,performance,Optimiz,Optimizer,161,[TMVA] Add API Support for Adagrad Optimizer.; * Implement Adagrad Optimizer. * Add Unit tests and MethodDL tests. Reference Implementation: [Tensorflow Adagrad Optimizer](https://www.tensorflow.org/api_docs/python/tf/train/AdagradOptimizer),MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2439
https://github.com/root-project/root/pull/2439:89,safety,test,tests,89,[TMVA] Add API Support for Adagrad Optimizer.; * Implement Adagrad Optimizer. * Add Unit tests and MethodDL tests. Reference Implementation: [Tensorflow Adagrad Optimizer](https://www.tensorflow.org/api_docs/python/tf/train/AdagradOptimizer),MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2439
https://github.com/root-project/root/pull/2439:108,safety,test,tests,108,[TMVA] Add API Support for Adagrad Optimizer.; * Implement Adagrad Optimizer. * Add Unit tests and MethodDL tests. Reference Implementation: [Tensorflow Adagrad Optimizer](https://www.tensorflow.org/api_docs/python/tf/train/AdagradOptimizer),MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2439
https://github.com/root-project/root/pull/2439:84,testability,Unit,Unit,84,[TMVA] Add API Support for Adagrad Optimizer.; * Implement Adagrad Optimizer. * Add Unit tests and MethodDL tests. Reference Implementation: [Tensorflow Adagrad Optimizer](https://www.tensorflow.org/api_docs/python/tf/train/AdagradOptimizer),MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2439
https://github.com/root-project/root/pull/2439:89,testability,test,tests,89,[TMVA] Add API Support for Adagrad Optimizer.; * Implement Adagrad Optimizer. * Add Unit tests and MethodDL tests. Reference Implementation: [Tensorflow Adagrad Optimizer](https://www.tensorflow.org/api_docs/python/tf/train/AdagradOptimizer),MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2439
https://github.com/root-project/root/pull/2439:108,testability,test,tests,108,[TMVA] Add API Support for Adagrad Optimizer.; * Implement Adagrad Optimizer. * Add Unit tests and MethodDL tests. Reference Implementation: [Tensorflow Adagrad Optimizer](https://www.tensorflow.org/api_docs/python/tf/train/AdagradOptimizer),MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2439
https://github.com/root-project/root/pull/2439:15,usability,Support,Support,15,[TMVA] Add API Support for Adagrad Optimizer.; * Implement Adagrad Optimizer. * Add Unit tests and MethodDL tests. Reference Implementation: [Tensorflow Adagrad Optimizer](https://www.tensorflow.org/api_docs/python/tf/train/AdagradOptimizer),MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2439
https://github.com/root-project/root/pull/2440:11,deployability,API,API,11,[TMVA] Add API Support for RMSProp Optimizer:; * Implement RMSProp Optimizer. * Add Unit tests and MethodDL tests. Reference Implementation: [Tensorflow RMSProp Optimizer](https://www.tensorflow.org/api_docs/python/tf/train/RMSPropOptimizer),MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2440
https://github.com/root-project/root/pull/2440:35,energy efficiency,Optim,Optimizer,35,[TMVA] Add API Support for RMSProp Optimizer:; * Implement RMSProp Optimizer. * Add Unit tests and MethodDL tests. Reference Implementation: [Tensorflow RMSProp Optimizer](https://www.tensorflow.org/api_docs/python/tf/train/RMSPropOptimizer),MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2440
https://github.com/root-project/root/pull/2440:67,energy efficiency,Optim,Optimizer,67,[TMVA] Add API Support for RMSProp Optimizer:; * Implement RMSProp Optimizer. * Add Unit tests and MethodDL tests. Reference Implementation: [Tensorflow RMSProp Optimizer](https://www.tensorflow.org/api_docs/python/tf/train/RMSPropOptimizer),MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2440
https://github.com/root-project/root/pull/2440:161,energy efficiency,Optim,Optimizer,161,[TMVA] Add API Support for RMSProp Optimizer:; * Implement RMSProp Optimizer. * Add Unit tests and MethodDL tests. Reference Implementation: [Tensorflow RMSProp Optimizer](https://www.tensorflow.org/api_docs/python/tf/train/RMSPropOptimizer),MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2440
https://github.com/root-project/root/pull/2440:11,integrability,API,API,11,[TMVA] Add API Support for RMSProp Optimizer:; * Implement RMSProp Optimizer. * Add Unit tests and MethodDL tests. Reference Implementation: [Tensorflow RMSProp Optimizer](https://www.tensorflow.org/api_docs/python/tf/train/RMSPropOptimizer),MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2440
https://github.com/root-project/root/pull/2440:11,interoperability,API,API,11,[TMVA] Add API Support for RMSProp Optimizer:; * Implement RMSProp Optimizer. * Add Unit tests and MethodDL tests. Reference Implementation: [Tensorflow RMSProp Optimizer](https://www.tensorflow.org/api_docs/python/tf/train/RMSPropOptimizer),MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2440
https://github.com/root-project/root/pull/2440:35,performance,Optimiz,Optimizer,35,[TMVA] Add API Support for RMSProp Optimizer:; * Implement RMSProp Optimizer. * Add Unit tests and MethodDL tests. Reference Implementation: [Tensorflow RMSProp Optimizer](https://www.tensorflow.org/api_docs/python/tf/train/RMSPropOptimizer),MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2440
https://github.com/root-project/root/pull/2440:67,performance,Optimiz,Optimizer,67,[TMVA] Add API Support for RMSProp Optimizer:; * Implement RMSProp Optimizer. * Add Unit tests and MethodDL tests. Reference Implementation: [Tensorflow RMSProp Optimizer](https://www.tensorflow.org/api_docs/python/tf/train/RMSPropOptimizer),MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2440
https://github.com/root-project/root/pull/2440:161,performance,Optimiz,Optimizer,161,[TMVA] Add API Support for RMSProp Optimizer:; * Implement RMSProp Optimizer. * Add Unit tests and MethodDL tests. Reference Implementation: [Tensorflow RMSProp Optimizer](https://www.tensorflow.org/api_docs/python/tf/train/RMSPropOptimizer),MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2440
https://github.com/root-project/root/pull/2440:89,safety,test,tests,89,[TMVA] Add API Support for RMSProp Optimizer:; * Implement RMSProp Optimizer. * Add Unit tests and MethodDL tests. Reference Implementation: [Tensorflow RMSProp Optimizer](https://www.tensorflow.org/api_docs/python/tf/train/RMSPropOptimizer),MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2440
https://github.com/root-project/root/pull/2440:108,safety,test,tests,108,[TMVA] Add API Support for RMSProp Optimizer:; * Implement RMSProp Optimizer. * Add Unit tests and MethodDL tests. Reference Implementation: [Tensorflow RMSProp Optimizer](https://www.tensorflow.org/api_docs/python/tf/train/RMSPropOptimizer),MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2440
https://github.com/root-project/root/pull/2440:84,testability,Unit,Unit,84,[TMVA] Add API Support for RMSProp Optimizer:; * Implement RMSProp Optimizer. * Add Unit tests and MethodDL tests. Reference Implementation: [Tensorflow RMSProp Optimizer](https://www.tensorflow.org/api_docs/python/tf/train/RMSPropOptimizer),MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2440
https://github.com/root-project/root/pull/2440:89,testability,test,tests,89,[TMVA] Add API Support for RMSProp Optimizer:; * Implement RMSProp Optimizer. * Add Unit tests and MethodDL tests. Reference Implementation: [Tensorflow RMSProp Optimizer](https://www.tensorflow.org/api_docs/python/tf/train/RMSPropOptimizer),MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2440
https://github.com/root-project/root/pull/2440:108,testability,test,tests,108,[TMVA] Add API Support for RMSProp Optimizer:; * Implement RMSProp Optimizer. * Add Unit tests and MethodDL tests. Reference Implementation: [Tensorflow RMSProp Optimizer](https://www.tensorflow.org/api_docs/python/tf/train/RMSPropOptimizer),MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2440
https://github.com/root-project/root/pull/2440:15,usability,Support,Support,15,[TMVA] Add API Support for RMSProp Optimizer:; * Implement RMSProp Optimizer. * Add Unit tests and MethodDL tests. Reference Implementation: [Tensorflow RMSProp Optimizer](https://www.tensorflow.org/api_docs/python/tf/train/RMSPropOptimizer),MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2440
https://github.com/root-project/root/pull/2441:11,deployability,API,API,11,[TMVA] Add API Support for Adadelta Optimizer.; * Implement Adagrad Optimizer. * Add Unit tests and MethodDL tests. Reference Implementation: [Tensorflow Adadelta Optimizer](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adadelta),MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2441
https://github.com/root-project/root/pull/2441:36,energy efficiency,Optim,Optimizer,36,[TMVA] Add API Support for Adadelta Optimizer.; * Implement Adagrad Optimizer. * Add Unit tests and MethodDL tests. Reference Implementation: [Tensorflow Adadelta Optimizer](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adadelta),MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2441
https://github.com/root-project/root/pull/2441:68,energy efficiency,Optim,Optimizer,68,[TMVA] Add API Support for Adadelta Optimizer.; * Implement Adagrad Optimizer. * Add Unit tests and MethodDL tests. Reference Implementation: [Tensorflow Adadelta Optimizer](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adadelta),MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2441
https://github.com/root-project/root/pull/2441:163,energy efficiency,Optim,Optimizer,163,[TMVA] Add API Support for Adadelta Optimizer.; * Implement Adagrad Optimizer. * Add Unit tests and MethodDL tests. Reference Implementation: [Tensorflow Adadelta Optimizer](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adadelta),MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2441
https://github.com/root-project/root/pull/2441:226,energy efficiency,optim,optimizers,226,[TMVA] Add API Support for Adadelta Optimizer.; * Implement Adagrad Optimizer. * Add Unit tests and MethodDL tests. Reference Implementation: [Tensorflow Adadelta Optimizer](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adadelta),MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2441
https://github.com/root-project/root/pull/2441:11,integrability,API,API,11,[TMVA] Add API Support for Adadelta Optimizer.; * Implement Adagrad Optimizer. * Add Unit tests and MethodDL tests. Reference Implementation: [Tensorflow Adadelta Optimizer](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adadelta),MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2441
https://github.com/root-project/root/pull/2441:11,interoperability,API,API,11,[TMVA] Add API Support for Adadelta Optimizer.; * Implement Adagrad Optimizer. * Add Unit tests and MethodDL tests. Reference Implementation: [Tensorflow Adadelta Optimizer](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adadelta),MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2441
https://github.com/root-project/root/pull/2441:36,performance,Optimiz,Optimizer,36,[TMVA] Add API Support for Adadelta Optimizer.; * Implement Adagrad Optimizer. * Add Unit tests and MethodDL tests. Reference Implementation: [Tensorflow Adadelta Optimizer](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adadelta),MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2441
https://github.com/root-project/root/pull/2441:68,performance,Optimiz,Optimizer,68,[TMVA] Add API Support for Adadelta Optimizer.; * Implement Adagrad Optimizer. * Add Unit tests and MethodDL tests. Reference Implementation: [Tensorflow Adadelta Optimizer](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adadelta),MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2441
https://github.com/root-project/root/pull/2441:163,performance,Optimiz,Optimizer,163,[TMVA] Add API Support for Adadelta Optimizer.; * Implement Adagrad Optimizer. * Add Unit tests and MethodDL tests. Reference Implementation: [Tensorflow Adadelta Optimizer](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adadelta),MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2441
https://github.com/root-project/root/pull/2441:226,performance,optimiz,optimizers,226,[TMVA] Add API Support for Adadelta Optimizer.; * Implement Adagrad Optimizer. * Add Unit tests and MethodDL tests. Reference Implementation: [Tensorflow Adadelta Optimizer](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adadelta),MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2441
https://github.com/root-project/root/pull/2441:90,safety,test,tests,90,[TMVA] Add API Support for Adadelta Optimizer.; * Implement Adagrad Optimizer. * Add Unit tests and MethodDL tests. Reference Implementation: [Tensorflow Adadelta Optimizer](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adadelta),MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2441
https://github.com/root-project/root/pull/2441:109,safety,test,tests,109,[TMVA] Add API Support for Adadelta Optimizer.; * Implement Adagrad Optimizer. * Add Unit tests and MethodDL tests. Reference Implementation: [Tensorflow Adadelta Optimizer](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adadelta),MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2441
https://github.com/root-project/root/pull/2441:85,testability,Unit,Unit,85,[TMVA] Add API Support for Adadelta Optimizer.; * Implement Adagrad Optimizer. * Add Unit tests and MethodDL tests. Reference Implementation: [Tensorflow Adadelta Optimizer](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adadelta),MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2441
https://github.com/root-project/root/pull/2441:90,testability,test,tests,90,[TMVA] Add API Support for Adadelta Optimizer.; * Implement Adagrad Optimizer. * Add Unit tests and MethodDL tests. Reference Implementation: [Tensorflow Adadelta Optimizer](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adadelta),MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2441
https://github.com/root-project/root/pull/2441:109,testability,test,tests,109,[TMVA] Add API Support for Adadelta Optimizer.; * Implement Adagrad Optimizer. * Add Unit tests and MethodDL tests. Reference Implementation: [Tensorflow Adadelta Optimizer](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adadelta),MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2441
https://github.com/root-project/root/pull/2441:15,usability,Support,Support,15,[TMVA] Add API Support for Adadelta Optimizer.; * Implement Adagrad Optimizer. * Add Unit tests and MethodDL tests. Reference Implementation: [Tensorflow Adadelta Optimizer](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adadelta),MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2441
https://github.com/root-project/root/pull/2442:32,usability,command,commands,32,Py2help v2; Added the necessary commands to generate root command line options and manual page.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2442
https://github.com/root-project/root/pull/2442:58,usability,command,command,58,Py2help v2; Added the necessary commands to generate root command line options and manual page.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2442
https://github.com/root-project/root/pull/2443:22,availability,redund,redundant,22,"[DF][ROOT-9463] Avoid redundant results in GetColumnNames; As reported in ROOT-9463, the result of GetColumnNames is redundant. A simple reproducer:. ```. TFile f(""f.root"", ""recreate"");. TTree t(""t"", ""t"");. int a;. t.Branch(""a"", &a);. a = 42;. t.Fill();. t.Write();. f.Close();. ROOT::RDataFrame df(""t"", ""f.root"");. for (auto x : df.GetColumnNames()). std::cout <<x <<std::endl;. ```. This redundancy is now not exposed to the user. **It would be nice** to have more tests on real use cases.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2443
https://github.com/root-project/root/pull/2443:117,availability,redund,redundant,117,"[DF][ROOT-9463] Avoid redundant results in GetColumnNames; As reported in ROOT-9463, the result of GetColumnNames is redundant. A simple reproducer:. ```. TFile f(""f.root"", ""recreate"");. TTree t(""t"", ""t"");. int a;. t.Branch(""a"", &a);. a = 42;. t.Fill();. t.Write();. f.Close();. ROOT::RDataFrame df(""t"", ""f.root"");. for (auto x : df.GetColumnNames()). std::cout <<x <<std::endl;. ```. This redundancy is now not exposed to the user. **It would be nice** to have more tests on real use cases.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2443
https://github.com/root-project/root/pull/2443:390,availability,redund,redundancy,390,"[DF][ROOT-9463] Avoid redundant results in GetColumnNames; As reported in ROOT-9463, the result of GetColumnNames is redundant. A simple reproducer:. ```. TFile f(""f.root"", ""recreate"");. TTree t(""t"", ""t"");. int a;. t.Branch(""a"", &a);. a = 42;. t.Fill();. t.Write();. f.Close();. ROOT::RDataFrame df(""t"", ""f.root"");. for (auto x : df.GetColumnNames()). std::cout <<x <<std::endl;. ```. This redundancy is now not exposed to the user. **It would be nice** to have more tests on real use cases.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2443
https://github.com/root-project/root/pull/2443:22,deployability,redundan,redundant,22,"[DF][ROOT-9463] Avoid redundant results in GetColumnNames; As reported in ROOT-9463, the result of GetColumnNames is redundant. A simple reproducer:. ```. TFile f(""f.root"", ""recreate"");. TTree t(""t"", ""t"");. int a;. t.Branch(""a"", &a);. a = 42;. t.Fill();. t.Write();. f.Close();. ROOT::RDataFrame df(""t"", ""f.root"");. for (auto x : df.GetColumnNames()). std::cout <<x <<std::endl;. ```. This redundancy is now not exposed to the user. **It would be nice** to have more tests on real use cases.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2443
https://github.com/root-project/root/pull/2443:117,deployability,redundan,redundant,117,"[DF][ROOT-9463] Avoid redundant results in GetColumnNames; As reported in ROOT-9463, the result of GetColumnNames is redundant. A simple reproducer:. ```. TFile f(""f.root"", ""recreate"");. TTree t(""t"", ""t"");. int a;. t.Branch(""a"", &a);. a = 42;. t.Fill();. t.Write();. f.Close();. ROOT::RDataFrame df(""t"", ""f.root"");. for (auto x : df.GetColumnNames()). std::cout <<x <<std::endl;. ```. This redundancy is now not exposed to the user. **It would be nice** to have more tests on real use cases.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2443
https://github.com/root-project/root/pull/2443:390,deployability,redundan,redundancy,390,"[DF][ROOT-9463] Avoid redundant results in GetColumnNames; As reported in ROOT-9463, the result of GetColumnNames is redundant. A simple reproducer:. ```. TFile f(""f.root"", ""recreate"");. TTree t(""t"", ""t"");. int a;. t.Branch(""a"", &a);. a = 42;. t.Fill();. t.Write();. f.Close();. ROOT::RDataFrame df(""t"", ""f.root"");. for (auto x : df.GetColumnNames()). std::cout <<x <<std::endl;. ```. This redundancy is now not exposed to the user. **It would be nice** to have more tests on real use cases.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2443
https://github.com/root-project/root/pull/2443:22,reliability,redundan,redundant,22,"[DF][ROOT-9463] Avoid redundant results in GetColumnNames; As reported in ROOT-9463, the result of GetColumnNames is redundant. A simple reproducer:. ```. TFile f(""f.root"", ""recreate"");. TTree t(""t"", ""t"");. int a;. t.Branch(""a"", &a);. a = 42;. t.Fill();. t.Write();. f.Close();. ROOT::RDataFrame df(""t"", ""f.root"");. for (auto x : df.GetColumnNames()). std::cout <<x <<std::endl;. ```. This redundancy is now not exposed to the user. **It would be nice** to have more tests on real use cases.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2443
https://github.com/root-project/root/pull/2443:117,reliability,redundan,redundant,117,"[DF][ROOT-9463] Avoid redundant results in GetColumnNames; As reported in ROOT-9463, the result of GetColumnNames is redundant. A simple reproducer:. ```. TFile f(""f.root"", ""recreate"");. TTree t(""t"", ""t"");. int a;. t.Branch(""a"", &a);. a = 42;. t.Fill();. t.Write();. f.Close();. ROOT::RDataFrame df(""t"", ""f.root"");. for (auto x : df.GetColumnNames()). std::cout <<x <<std::endl;. ```. This redundancy is now not exposed to the user. **It would be nice** to have more tests on real use cases.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2443
https://github.com/root-project/root/pull/2443:390,reliability,redundan,redundancy,390,"[DF][ROOT-9463] Avoid redundant results in GetColumnNames; As reported in ROOT-9463, the result of GetColumnNames is redundant. A simple reproducer:. ```. TFile f(""f.root"", ""recreate"");. TTree t(""t"", ""t"");. int a;. t.Branch(""a"", &a);. a = 42;. t.Fill();. t.Write();. f.Close();. ROOT::RDataFrame df(""t"", ""f.root"");. for (auto x : df.GetColumnNames()). std::cout <<x <<std::endl;. ```. This redundancy is now not exposed to the user. **It would be nice** to have more tests on real use cases.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2443
https://github.com/root-project/root/pull/2443:16,safety,Avoid,Avoid,16,"[DF][ROOT-9463] Avoid redundant results in GetColumnNames; As reported in ROOT-9463, the result of GetColumnNames is redundant. A simple reproducer:. ```. TFile f(""f.root"", ""recreate"");. TTree t(""t"", ""t"");. int a;. t.Branch(""a"", &a);. a = 42;. t.Fill();. t.Write();. f.Close();. ROOT::RDataFrame df(""t"", ""f.root"");. for (auto x : df.GetColumnNames()). std::cout <<x <<std::endl;. ```. This redundancy is now not exposed to the user. **It would be nice** to have more tests on real use cases.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2443
https://github.com/root-project/root/pull/2443:22,safety,redund,redundant,22,"[DF][ROOT-9463] Avoid redundant results in GetColumnNames; As reported in ROOT-9463, the result of GetColumnNames is redundant. A simple reproducer:. ```. TFile f(""f.root"", ""recreate"");. TTree t(""t"", ""t"");. int a;. t.Branch(""a"", &a);. a = 42;. t.Fill();. t.Write();. f.Close();. ROOT::RDataFrame df(""t"", ""f.root"");. for (auto x : df.GetColumnNames()). std::cout <<x <<std::endl;. ```. This redundancy is now not exposed to the user. **It would be nice** to have more tests on real use cases.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2443
https://github.com/root-project/root/pull/2443:117,safety,redund,redundant,117,"[DF][ROOT-9463] Avoid redundant results in GetColumnNames; As reported in ROOT-9463, the result of GetColumnNames is redundant. A simple reproducer:. ```. TFile f(""f.root"", ""recreate"");. TTree t(""t"", ""t"");. int a;. t.Branch(""a"", &a);. a = 42;. t.Fill();. t.Write();. f.Close();. ROOT::RDataFrame df(""t"", ""f.root"");. for (auto x : df.GetColumnNames()). std::cout <<x <<std::endl;. ```. This redundancy is now not exposed to the user. **It would be nice** to have more tests on real use cases.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2443
https://github.com/root-project/root/pull/2443:390,safety,redund,redundancy,390,"[DF][ROOT-9463] Avoid redundant results in GetColumnNames; As reported in ROOT-9463, the result of GetColumnNames is redundant. A simple reproducer:. ```. TFile f(""f.root"", ""recreate"");. TTree t(""t"", ""t"");. int a;. t.Branch(""a"", &a);. a = 42;. t.Fill();. t.Write();. f.Close();. ROOT::RDataFrame df(""t"", ""f.root"");. for (auto x : df.GetColumnNames()). std::cout <<x <<std::endl;. ```. This redundancy is now not exposed to the user. **It would be nice** to have more tests on real use cases.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2443
https://github.com/root-project/root/pull/2443:467,safety,test,tests,467,"[DF][ROOT-9463] Avoid redundant results in GetColumnNames; As reported in ROOT-9463, the result of GetColumnNames is redundant. A simple reproducer:. ```. TFile f(""f.root"", ""recreate"");. TTree t(""t"", ""t"");. int a;. t.Branch(""a"", &a);. a = 42;. t.Fill();. t.Write();. f.Close();. ROOT::RDataFrame df(""t"", ""f.root"");. for (auto x : df.GetColumnNames()). std::cout <<x <<std::endl;. ```. This redundancy is now not exposed to the user. **It would be nice** to have more tests on real use cases.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2443
https://github.com/root-project/root/pull/2443:412,security,expos,exposed,412,"[DF][ROOT-9463] Avoid redundant results in GetColumnNames; As reported in ROOT-9463, the result of GetColumnNames is redundant. A simple reproducer:. ```. TFile f(""f.root"", ""recreate"");. TTree t(""t"", ""t"");. int a;. t.Branch(""a"", &a);. a = 42;. t.Fill();. t.Write();. f.Close();. ROOT::RDataFrame df(""t"", ""f.root"");. for (auto x : df.GetColumnNames()). std::cout <<x <<std::endl;. ```. This redundancy is now not exposed to the user. **It would be nice** to have more tests on real use cases.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2443
https://github.com/root-project/root/pull/2443:130,testability,simpl,simple,130,"[DF][ROOT-9463] Avoid redundant results in GetColumnNames; As reported in ROOT-9463, the result of GetColumnNames is redundant. A simple reproducer:. ```. TFile f(""f.root"", ""recreate"");. TTree t(""t"", ""t"");. int a;. t.Branch(""a"", &a);. a = 42;. t.Fill();. t.Write();. f.Close();. ROOT::RDataFrame df(""t"", ""f.root"");. for (auto x : df.GetColumnNames()). std::cout <<x <<std::endl;. ```. This redundancy is now not exposed to the user. **It would be nice** to have more tests on real use cases.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2443
https://github.com/root-project/root/pull/2443:467,testability,test,tests,467,"[DF][ROOT-9463] Avoid redundant results in GetColumnNames; As reported in ROOT-9463, the result of GetColumnNames is redundant. A simple reproducer:. ```. TFile f(""f.root"", ""recreate"");. TTree t(""t"", ""t"");. int a;. t.Branch(""a"", &a);. a = 42;. t.Fill();. t.Write();. f.Close();. ROOT::RDataFrame df(""t"", ""f.root"");. for (auto x : df.GetColumnNames()). std::cout <<x <<std::endl;. ```. This redundancy is now not exposed to the user. **It would be nice** to have more tests on real use cases.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2443
https://github.com/root-project/root/pull/2443:130,usability,simpl,simple,130,"[DF][ROOT-9463] Avoid redundant results in GetColumnNames; As reported in ROOT-9463, the result of GetColumnNames is redundant. A simple reproducer:. ```. TFile f(""f.root"", ""recreate"");. TTree t(""t"", ""t"");. int a;. t.Branch(""a"", &a);. a = 42;. t.Fill();. t.Write();. f.Close();. ROOT::RDataFrame df(""t"", ""f.root"");. for (auto x : df.GetColumnNames()). std::cout <<x <<std::endl;. ```. This redundancy is now not exposed to the user. **It would be nice** to have more tests on real use cases.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2443
https://github.com/root-project/root/pull/2443:269,usability,Close,Close,269,"[DF][ROOT-9463] Avoid redundant results in GetColumnNames; As reported in ROOT-9463, the result of GetColumnNames is redundant. A simple reproducer:. ```. TFile f(""f.root"", ""recreate"");. TTree t(""t"", ""t"");. int a;. t.Branch(""a"", &a);. a = 42;. t.Fill();. t.Write();. f.Close();. ROOT::RDataFrame df(""t"", ""f.root"");. for (auto x : df.GetColumnNames()). std::cout <<x <<std::endl;. ```. This redundancy is now not exposed to the user. **It would be nice** to have more tests on real use cases.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2443
https://github.com/root-project/root/pull/2443:427,usability,user,user,427,"[DF][ROOT-9463] Avoid redundant results in GetColumnNames; As reported in ROOT-9463, the result of GetColumnNames is redundant. A simple reproducer:. ```. TFile f(""f.root"", ""recreate"");. TTree t(""t"", ""t"");. int a;. t.Branch(""a"", &a);. a = 42;. t.Fill();. t.Write();. f.Close();. ROOT::RDataFrame df(""t"", ""f.root"");. for (auto x : df.GetColumnNames()). std::cout <<x <<std::endl;. ```. This redundancy is now not exposed to the user. **It would be nice** to have more tests on real use cases.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2443
https://github.com/root-project/root/pull/2444:18,interoperability,format,formatting,18,[DF][NFC] Fix doc formatting;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2444
https://github.com/root-project/root/pull/2447:113,deployability,manag,management,113,"[DF] Move action ownership from RLoopManager to RResultPtr; This is a first step towards the change in ownership management. required by ROOT-9416, and unblocks development of features that depend on actions staying around even after their event-loop has completed, e.g. generalized merging mechanism, necessary for distributed execution.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2447
https://github.com/root-project/root/pull/2447:190,deployability,depend,depend,190,"[DF] Move action ownership from RLoopManager to RResultPtr; This is a first step towards the change in ownership management. required by ROOT-9416, and unblocks development of features that depend on actions staying around even after their event-loop has completed, e.g. generalized merging mechanism, necessary for distributed execution.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2447
https://github.com/root-project/root/pull/2447:113,energy efficiency,manag,management,113,"[DF] Move action ownership from RLoopManager to RResultPtr; This is a first step towards the change in ownership management. required by ROOT-9416, and unblocks development of features that depend on actions staying around even after their event-loop has completed, e.g. generalized merging mechanism, necessary for distributed execution.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2447
https://github.com/root-project/root/pull/2447:190,integrability,depend,depend,190,"[DF] Move action ownership from RLoopManager to RResultPtr; This is a first step towards the change in ownership management. required by ROOT-9416, and unblocks development of features that depend on actions staying around even after their event-loop has completed, e.g. generalized merging mechanism, necessary for distributed execution.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2447
https://github.com/root-project/root/pull/2447:240,integrability,event,event-loop,240,"[DF] Move action ownership from RLoopManager to RResultPtr; This is a first step towards the change in ownership management. required by ROOT-9416, and unblocks development of features that depend on actions staying around even after their event-loop has completed, e.g. generalized merging mechanism, necessary for distributed execution.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2447
https://github.com/root-project/root/pull/2447:316,interoperability,distribut,distributed,316,"[DF] Move action ownership from RLoopManager to RResultPtr; This is a first step towards the change in ownership management. required by ROOT-9416, and unblocks development of features that depend on actions staying around even after their event-loop has completed, e.g. generalized merging mechanism, necessary for distributed execution.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2447
https://github.com/root-project/root/pull/2447:190,modifiability,depend,depend,190,"[DF] Move action ownership from RLoopManager to RResultPtr; This is a first step towards the change in ownership management. required by ROOT-9416, and unblocks development of features that depend on actions staying around even after their event-loop has completed, e.g. generalized merging mechanism, necessary for distributed execution.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2447
https://github.com/root-project/root/pull/2447:113,safety,manag,management,113,"[DF] Move action ownership from RLoopManager to RResultPtr; This is a first step towards the change in ownership management. required by ROOT-9416, and unblocks development of features that depend on actions staying around even after their event-loop has completed, e.g. generalized merging mechanism, necessary for distributed execution.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2447
https://github.com/root-project/root/pull/2447:190,safety,depend,depend,190,"[DF] Move action ownership from RLoopManager to RResultPtr; This is a first step towards the change in ownership management. required by ROOT-9416, and unblocks development of features that depend on actions staying around even after their event-loop has completed, e.g. generalized merging mechanism, necessary for distributed execution.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2447
https://github.com/root-project/root/pull/2447:255,safety,compl,completed,255,"[DF] Move action ownership from RLoopManager to RResultPtr; This is a first step towards the change in ownership management. required by ROOT-9416, and unblocks development of features that depend on actions staying around even after their event-loop has completed, e.g. generalized merging mechanism, necessary for distributed execution.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2447
https://github.com/root-project/root/pull/2447:255,security,compl,completed,255,"[DF] Move action ownership from RLoopManager to RResultPtr; This is a first step towards the change in ownership management. required by ROOT-9416, and unblocks development of features that depend on actions staying around even after their event-loop has completed, e.g. generalized merging mechanism, necessary for distributed execution.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2447
https://github.com/root-project/root/pull/2447:190,testability,depend,depend,190,"[DF] Move action ownership from RLoopManager to RResultPtr; This is a first step towards the change in ownership management. required by ROOT-9416, and unblocks development of features that depend on actions staying around even after their event-loop has completed, e.g. generalized merging mechanism, necessary for distributed execution.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2447
https://github.com/root-project/root/pull/2448:125,interoperability,convers,conversion,125,"Reland ""[cxxmodules] Check correctly if the decl was cached.""; Instead of checking for -1 and ending in strange pointer-bool conversion, we use a data member to tell us if we have looked up the record decl already.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2448
https://github.com/root-project/root/pull/2448:53,performance,cach,cached,53,"Reland ""[cxxmodules] Check correctly if the decl was cached.""; Instead of checking for -1 and ending in strange pointer-bool conversion, we use a data member to tell us if we have looked up the record decl already.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2448
https://github.com/root-project/root/pull/2449:562,energy efficiency,current,current,562,"Unit tests for Pythonizations; Add a couple of unit tests for two pythonizations:. - Access a TTree branch as an attribute. - Make a TTree iterable. Both of them test different types of branches, which is especially relevant in the first test. The fact that the pythonization of `SetBranchAddress` is still not present in PyROOT experimental prevents the second test from testing more branch types (i.e. those that require a reference to a pointer); this is not an issue anyway since the code of the TTree-iterable pythonization is tested in its entirety by the current cases.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2449
https://github.com/root-project/root/pull/2449:37,integrability,coupl,couple,37,"Unit tests for Pythonizations; Add a couple of unit tests for two pythonizations:. - Access a TTree branch as an attribute. - Make a TTree iterable. Both of them test different types of branches, which is especially relevant in the first test. The fact that the pythonization of `SetBranchAddress` is still not present in PyROOT experimental prevents the second test from testing more branch types (i.e. those that require a reference to a pointer); this is not an issue anyway since the code of the TTree-iterable pythonization is tested in its entirety by the current cases.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2449
https://github.com/root-project/root/pull/2449:37,modifiability,coupl,couple,37,"Unit tests for Pythonizations; Add a couple of unit tests for two pythonizations:. - Access a TTree branch as an attribute. - Make a TTree iterable. Both of them test different types of branches, which is especially relevant in the first test. The fact that the pythonization of `SetBranchAddress` is still not present in PyROOT experimental prevents the second test from testing more branch types (i.e. those that require a reference to a pointer); this is not an issue anyway since the code of the TTree-iterable pythonization is tested in its entirety by the current cases.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2449
https://github.com/root-project/root/pull/2449:5,safety,test,tests,5,"Unit tests for Pythonizations; Add a couple of unit tests for two pythonizations:. - Access a TTree branch as an attribute. - Make a TTree iterable. Both of them test different types of branches, which is especially relevant in the first test. The fact that the pythonization of `SetBranchAddress` is still not present in PyROOT experimental prevents the second test from testing more branch types (i.e. those that require a reference to a pointer); this is not an issue anyway since the code of the TTree-iterable pythonization is tested in its entirety by the current cases.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2449
https://github.com/root-project/root/pull/2449:52,safety,test,tests,52,"Unit tests for Pythonizations; Add a couple of unit tests for two pythonizations:. - Access a TTree branch as an attribute. - Make a TTree iterable. Both of them test different types of branches, which is especially relevant in the first test. The fact that the pythonization of `SetBranchAddress` is still not present in PyROOT experimental prevents the second test from testing more branch types (i.e. those that require a reference to a pointer); this is not an issue anyway since the code of the TTree-iterable pythonization is tested in its entirety by the current cases.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2449
https://github.com/root-project/root/pull/2449:162,safety,test,test,162,"Unit tests for Pythonizations; Add a couple of unit tests for two pythonizations:. - Access a TTree branch as an attribute. - Make a TTree iterable. Both of them test different types of branches, which is especially relevant in the first test. The fact that the pythonization of `SetBranchAddress` is still not present in PyROOT experimental prevents the second test from testing more branch types (i.e. those that require a reference to a pointer); this is not an issue anyway since the code of the TTree-iterable pythonization is tested in its entirety by the current cases.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2449
https://github.com/root-project/root/pull/2449:238,safety,test,test,238,"Unit tests for Pythonizations; Add a couple of unit tests for two pythonizations:. - Access a TTree branch as an attribute. - Make a TTree iterable. Both of them test different types of branches, which is especially relevant in the first test. The fact that the pythonization of `SetBranchAddress` is still not present in PyROOT experimental prevents the second test from testing more branch types (i.e. those that require a reference to a pointer); this is not an issue anyway since the code of the TTree-iterable pythonization is tested in its entirety by the current cases.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2449
https://github.com/root-project/root/pull/2449:342,safety,prevent,prevents,342,"Unit tests for Pythonizations; Add a couple of unit tests for two pythonizations:. - Access a TTree branch as an attribute. - Make a TTree iterable. Both of them test different types of branches, which is especially relevant in the first test. The fact that the pythonization of `SetBranchAddress` is still not present in PyROOT experimental prevents the second test from testing more branch types (i.e. those that require a reference to a pointer); this is not an issue anyway since the code of the TTree-iterable pythonization is tested in its entirety by the current cases.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2449
https://github.com/root-project/root/pull/2449:362,safety,test,test,362,"Unit tests for Pythonizations; Add a couple of unit tests for two pythonizations:. - Access a TTree branch as an attribute. - Make a TTree iterable. Both of them test different types of branches, which is especially relevant in the first test. The fact that the pythonization of `SetBranchAddress` is still not present in PyROOT experimental prevents the second test from testing more branch types (i.e. those that require a reference to a pointer); this is not an issue anyway since the code of the TTree-iterable pythonization is tested in its entirety by the current cases.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2449
https://github.com/root-project/root/pull/2449:372,safety,test,testing,372,"Unit tests for Pythonizations; Add a couple of unit tests for two pythonizations:. - Access a TTree branch as an attribute. - Make a TTree iterable. Both of them test different types of branches, which is especially relevant in the first test. The fact that the pythonization of `SetBranchAddress` is still not present in PyROOT experimental prevents the second test from testing more branch types (i.e. those that require a reference to a pointer); this is not an issue anyway since the code of the TTree-iterable pythonization is tested in its entirety by the current cases.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2449
https://github.com/root-project/root/pull/2449:532,safety,test,tested,532,"Unit tests for Pythonizations; Add a couple of unit tests for two pythonizations:. - Access a TTree branch as an attribute. - Make a TTree iterable. Both of them test different types of branches, which is especially relevant in the first test. The fact that the pythonization of `SetBranchAddress` is still not present in PyROOT experimental prevents the second test from testing more branch types (i.e. those that require a reference to a pointer); this is not an issue anyway since the code of the TTree-iterable pythonization is tested in its entirety by the current cases.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2449
https://github.com/root-project/root/pull/2449:85,security,Access,Access,85,"Unit tests for Pythonizations; Add a couple of unit tests for two pythonizations:. - Access a TTree branch as an attribute. - Make a TTree iterable. Both of them test different types of branches, which is especially relevant in the first test. The fact that the pythonization of `SetBranchAddress` is still not present in PyROOT experimental prevents the second test from testing more branch types (i.e. those that require a reference to a pointer); this is not an issue anyway since the code of the TTree-iterable pythonization is tested in its entirety by the current cases.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2449
https://github.com/root-project/root/pull/2449:342,security,preven,prevents,342,"Unit tests for Pythonizations; Add a couple of unit tests for two pythonizations:. - Access a TTree branch as an attribute. - Make a TTree iterable. Both of them test different types of branches, which is especially relevant in the first test. The fact that the pythonization of `SetBranchAddress` is still not present in PyROOT experimental prevents the second test from testing more branch types (i.e. those that require a reference to a pointer); this is not an issue anyway since the code of the TTree-iterable pythonization is tested in its entirety by the current cases.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2449
https://github.com/root-project/root/pull/2449:0,testability,Unit,Unit,0,"Unit tests for Pythonizations; Add a couple of unit tests for two pythonizations:. - Access a TTree branch as an attribute. - Make a TTree iterable. Both of them test different types of branches, which is especially relevant in the first test. The fact that the pythonization of `SetBranchAddress` is still not present in PyROOT experimental prevents the second test from testing more branch types (i.e. those that require a reference to a pointer); this is not an issue anyway since the code of the TTree-iterable pythonization is tested in its entirety by the current cases.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2449
https://github.com/root-project/root/pull/2449:5,testability,test,tests,5,"Unit tests for Pythonizations; Add a couple of unit tests for two pythonizations:. - Access a TTree branch as an attribute. - Make a TTree iterable. Both of them test different types of branches, which is especially relevant in the first test. The fact that the pythonization of `SetBranchAddress` is still not present in PyROOT experimental prevents the second test from testing more branch types (i.e. those that require a reference to a pointer); this is not an issue anyway since the code of the TTree-iterable pythonization is tested in its entirety by the current cases.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2449
https://github.com/root-project/root/pull/2449:37,testability,coupl,couple,37,"Unit tests for Pythonizations; Add a couple of unit tests for two pythonizations:. - Access a TTree branch as an attribute. - Make a TTree iterable. Both of them test different types of branches, which is especially relevant in the first test. The fact that the pythonization of `SetBranchAddress` is still not present in PyROOT experimental prevents the second test from testing more branch types (i.e. those that require a reference to a pointer); this is not an issue anyway since the code of the TTree-iterable pythonization is tested in its entirety by the current cases.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2449
https://github.com/root-project/root/pull/2449:47,testability,unit,unit,47,"Unit tests for Pythonizations; Add a couple of unit tests for two pythonizations:. - Access a TTree branch as an attribute. - Make a TTree iterable. Both of them test different types of branches, which is especially relevant in the first test. The fact that the pythonization of `SetBranchAddress` is still not present in PyROOT experimental prevents the second test from testing more branch types (i.e. those that require a reference to a pointer); this is not an issue anyway since the code of the TTree-iterable pythonization is tested in its entirety by the current cases.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2449
https://github.com/root-project/root/pull/2449:52,testability,test,tests,52,"Unit tests for Pythonizations; Add a couple of unit tests for two pythonizations:. - Access a TTree branch as an attribute. - Make a TTree iterable. Both of them test different types of branches, which is especially relevant in the first test. The fact that the pythonization of `SetBranchAddress` is still not present in PyROOT experimental prevents the second test from testing more branch types (i.e. those that require a reference to a pointer); this is not an issue anyway since the code of the TTree-iterable pythonization is tested in its entirety by the current cases.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2449
https://github.com/root-project/root/pull/2449:162,testability,test,test,162,"Unit tests for Pythonizations; Add a couple of unit tests for two pythonizations:. - Access a TTree branch as an attribute. - Make a TTree iterable. Both of them test different types of branches, which is especially relevant in the first test. The fact that the pythonization of `SetBranchAddress` is still not present in PyROOT experimental prevents the second test from testing more branch types (i.e. those that require a reference to a pointer); this is not an issue anyway since the code of the TTree-iterable pythonization is tested in its entirety by the current cases.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2449
https://github.com/root-project/root/pull/2449:238,testability,test,test,238,"Unit tests for Pythonizations; Add a couple of unit tests for two pythonizations:. - Access a TTree branch as an attribute. - Make a TTree iterable. Both of them test different types of branches, which is especially relevant in the first test. The fact that the pythonization of `SetBranchAddress` is still not present in PyROOT experimental prevents the second test from testing more branch types (i.e. those that require a reference to a pointer); this is not an issue anyway since the code of the TTree-iterable pythonization is tested in its entirety by the current cases.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2449
https://github.com/root-project/root/pull/2449:362,testability,test,test,362,"Unit tests for Pythonizations; Add a couple of unit tests for two pythonizations:. - Access a TTree branch as an attribute. - Make a TTree iterable. Both of them test different types of branches, which is especially relevant in the first test. The fact that the pythonization of `SetBranchAddress` is still not present in PyROOT experimental prevents the second test from testing more branch types (i.e. those that require a reference to a pointer); this is not an issue anyway since the code of the TTree-iterable pythonization is tested in its entirety by the current cases.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2449
https://github.com/root-project/root/pull/2449:372,testability,test,testing,372,"Unit tests for Pythonizations; Add a couple of unit tests for two pythonizations:. - Access a TTree branch as an attribute. - Make a TTree iterable. Both of them test different types of branches, which is especially relevant in the first test. The fact that the pythonization of `SetBranchAddress` is still not present in PyROOT experimental prevents the second test from testing more branch types (i.e. those that require a reference to a pointer); this is not an issue anyway since the code of the TTree-iterable pythonization is tested in its entirety by the current cases.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2449
https://github.com/root-project/root/pull/2449:532,testability,test,tested,532,"Unit tests for Pythonizations; Add a couple of unit tests for two pythonizations:. - Access a TTree branch as an attribute. - Make a TTree iterable. Both of them test different types of branches, which is especially relevant in the first test. The fact that the pythonization of `SetBranchAddress` is still not present in PyROOT experimental prevents the second test from testing more branch types (i.e. those that require a reference to a pointer); this is not an issue anyway since the code of the TTree-iterable pythonization is tested in its entirety by the current cases.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2449
https://github.com/root-project/root/pull/2450:40,safety,avoid,avoid,40,[DF] Store RJittedAction on the heap to avoid use after delete; The use after delete might happen in case the corresponding RResultPtr. goes out of scope before the concrete action is jitted.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2450
https://github.com/root-project/root/pull/2451:0,availability,Consist,Consistency,0,Consistency for copy constructors and assignment.;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2451
https://github.com/root-project/root/pull/2451:0,usability,Consist,Consistency,0,Consistency for copy constructors and assignment.;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2451
https://github.com/root-project/root/pull/2452:0,testability,Simpl,Simplify,0,Simplify Code. NFC.;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2452
https://github.com/root-project/root/pull/2452:0,usability,Simpl,Simplify,0,Simplify Code. NFC.;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2452
https://github.com/root-project/root/pull/2454:77,energy efficiency,draw,draw,77,Avg ROC Curve and its Notebook; Added couple of functions to create and then draw the Avg ROC Curve. Also created a notebook showing how to use these functions.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2454
https://github.com/root-project/root/pull/2454:38,integrability,coupl,couple,38,Avg ROC Curve and its Notebook; Added couple of functions to create and then draw the Avg ROC Curve. Also created a notebook showing how to use these functions.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2454
https://github.com/root-project/root/pull/2454:38,modifiability,coupl,couple,38,Avg ROC Curve and its Notebook; Added couple of functions to create and then draw the Avg ROC Curve. Also created a notebook showing how to use these functions.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2454
https://github.com/root-project/root/pull/2454:38,testability,coupl,couple,38,Avg ROC Curve and its Notebook; Added couple of functions to create and then draw the Avg ROC Curve. Also created a notebook showing how to use these functions.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2454
https://github.com/root-project/root/pull/2455:34,deployability,infrastructur,infrastructure,34,Add change with no effect to test infrastructure (not meant for merging);,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2455
https://github.com/root-project/root/pull/2455:29,safety,test,test,29,Add change with no effect to test infrastructure (not meant for merging);,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2455
https://github.com/root-project/root/pull/2455:29,testability,test,test,29,Add change with no effect to test infrastructure (not meant for merging);,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2455
https://github.com/root-project/root/pull/2456:0,availability,Consist,Consistency,0,Consistency for copy constructors and assignment.; (cherry picked from commit 9999fb552c400538d0250c436c4983cefa34f216),MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2456
https://github.com/root-project/root/pull/2456:0,usability,Consist,Consistency,0,Consistency for copy constructors and assignment.; (cherry picked from commit 9999fb552c400538d0250c436c4983cefa34f216),MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2456
https://github.com/root-project/root/pull/2457:0,availability,Consist,Consistency,0,Consistency for copy constructors and assignment.; (cherry picked from commit 9999fb552c400538d0250c436c4983cefa34f216),MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2457
https://github.com/root-project/root/pull/2457:0,usability,Consist,Consistency,0,Consistency for copy constructors and assignment.; (cherry picked from commit 9999fb552c400538d0250c436c4983cefa34f216),MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2457
https://github.com/root-project/root/pull/2458:0,availability,Consist,Consistency,0,Consistency for copy constructors and assignment.; (cherry picked from commit 9999fb552c400538d0250c436c4983cefa34f216),MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2458
https://github.com/root-project/root/pull/2458:0,usability,Consist,Consistency,0,Consistency for copy constructors and assignment.; (cherry picked from commit 9999fb552c400538d0250c436c4983cefa34f216),MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2458
https://github.com/root-project/root/pull/2459:399,availability,Failur,Failures,399,[DF] Give nodes shared ownership of their parents; This solves ROOT-9416. - users now do not have to take care that the head node of the dataframe remains in scope. - these kind of constructs are now allowed in python and C++:. ```c++. auto df = RDataFrame(...).Filter(...);. ```. - these kind of constructs are now allowed in python:. ```c++. df = RDataFrame(...). df = df.Filter(...). ```. EDIT:. Failures in roottests are fixed by [PR 212](https://github.com/root-project/roottest/pull/212) in roottest.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2459
https://github.com/root-project/root/pull/2459:399,deployability,Fail,Failures,399,[DF] Give nodes shared ownership of their parents; This solves ROOT-9416. - users now do not have to take care that the head node of the dataframe remains in scope. - these kind of constructs are now allowed in python and C++:. ```c++. auto df = RDataFrame(...).Filter(...);. ```. - these kind of constructs are now allowed in python:. ```c++. df = RDataFrame(...). df = df.Filter(...). ```. EDIT:. Failures in roottests are fixed by [PR 212](https://github.com/root-project/roottest/pull/212) in roottest.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2459
https://github.com/root-project/root/pull/2459:262,integrability,Filter,Filter,262,[DF] Give nodes shared ownership of their parents; This solves ROOT-9416. - users now do not have to take care that the head node of the dataframe remains in scope. - these kind of constructs are now allowed in python and C++:. ```c++. auto df = RDataFrame(...).Filter(...);. ```. - these kind of constructs are now allowed in python:. ```c++. df = RDataFrame(...). df = df.Filter(...). ```. EDIT:. Failures in roottests are fixed by [PR 212](https://github.com/root-project/roottest/pull/212) in roottest.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2459
https://github.com/root-project/root/pull/2459:374,integrability,Filter,Filter,374,[DF] Give nodes shared ownership of their parents; This solves ROOT-9416. - users now do not have to take care that the head node of the dataframe remains in scope. - these kind of constructs are now allowed in python and C++:. ```c++. auto df = RDataFrame(...).Filter(...);. ```. - these kind of constructs are now allowed in python:. ```c++. df = RDataFrame(...). df = df.Filter(...). ```. EDIT:. Failures in roottests are fixed by [PR 212](https://github.com/root-project/roottest/pull/212) in roottest.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2459
https://github.com/root-project/root/pull/2459:16,interoperability,share,shared,16,[DF] Give nodes shared ownership of their parents; This solves ROOT-9416. - users now do not have to take care that the head node of the dataframe remains in scope. - these kind of constructs are now allowed in python and C++:. ```c++. auto df = RDataFrame(...).Filter(...);. ```. - these kind of constructs are now allowed in python:. ```c++. df = RDataFrame(...). df = df.Filter(...). ```. EDIT:. Failures in roottests are fixed by [PR 212](https://github.com/root-project/roottest/pull/212) in roottest.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2459
https://github.com/root-project/root/pull/2459:399,performance,Failur,Failures,399,[DF] Give nodes shared ownership of their parents; This solves ROOT-9416. - users now do not have to take care that the head node of the dataframe remains in scope. - these kind of constructs are now allowed in python and C++:. ```c++. auto df = RDataFrame(...).Filter(...);. ```. - these kind of constructs are now allowed in python:. ```c++. df = RDataFrame(...). df = df.Filter(...). ```. EDIT:. Failures in roottests are fixed by [PR 212](https://github.com/root-project/roottest/pull/212) in roottest.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2459
https://github.com/root-project/root/pull/2459:399,reliability,Fail,Failures,399,[DF] Give nodes shared ownership of their parents; This solves ROOT-9416. - users now do not have to take care that the head node of the dataframe remains in scope. - these kind of constructs are now allowed in python and C++:. ```c++. auto df = RDataFrame(...).Filter(...);. ```. - these kind of constructs are now allowed in python:. ```c++. df = RDataFrame(...). df = df.Filter(...). ```. EDIT:. Failures in roottests are fixed by [PR 212](https://github.com/root-project/roottest/pull/212) in roottest.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2459
https://github.com/root-project/root/pull/2459:76,usability,user,users,76,[DF] Give nodes shared ownership of their parents; This solves ROOT-9416. - users now do not have to take care that the head node of the dataframe remains in scope. - these kind of constructs are now allowed in python and C++:. ```c++. auto df = RDataFrame(...).Filter(...);. ```. - these kind of constructs are now allowed in python:. ```c++. df = RDataFrame(...). df = df.Filter(...). ```. EDIT:. Failures in roottests are fixed by [PR 212](https://github.com/root-project/roottest/pull/212) in roottest.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2459
https://github.com/root-project/root/pull/2460:250,availability,down,down,250,"[TMVA] Fix for TMVACrossValidationRegression tutorial crash; Method signature used for `DataLoader::PrepareTrainingAndTestSet` triggered the generation of classes ""Signal"" and ""Background"". Since the task was regression this caused confusion further down the processing pipeline. Code changed to use global selection cut instead of implicit signal/background cuts. Also includes additional improvements to variable names and documentation.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2460
https://github.com/root-project/root/pull/2460:270,deployability,pipelin,pipeline,270,"[TMVA] Fix for TMVACrossValidationRegression tutorial crash; Method signature used for `DataLoader::PrepareTrainingAndTestSet` triggered the generation of classes ""Signal"" and ""Background"". Since the task was regression this caused confusion further down the processing pipeline. Code changed to use global selection cut instead of implicit signal/background cuts. Also includes additional improvements to variable names and documentation.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2460
https://github.com/root-project/root/pull/2460:270,integrability,pipelin,pipeline,270,"[TMVA] Fix for TMVACrossValidationRegression tutorial crash; Method signature used for `DataLoader::PrepareTrainingAndTestSet` triggered the generation of classes ""Signal"" and ""Background"". Since the task was regression this caused confusion further down the processing pipeline. Code changed to use global selection cut instead of implicit signal/background cuts. Also includes additional improvements to variable names and documentation.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2460
https://github.com/root-project/root/pull/2460:406,modifiability,variab,variable,406,"[TMVA] Fix for TMVACrossValidationRegression tutorial crash; Method signature used for `DataLoader::PrepareTrainingAndTestSet` triggered the generation of classes ""Signal"" and ""Background"". Since the task was regression this caused confusion further down the processing pipeline. Code changed to use global selection cut instead of implicit signal/background cuts. Also includes additional improvements to variable names and documentation.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2460
https://github.com/root-project/root/pull/2460:68,security,sign,signature,68,"[TMVA] Fix for TMVACrossValidationRegression tutorial crash; Method signature used for `DataLoader::PrepareTrainingAndTestSet` triggered the generation of classes ""Signal"" and ""Background"". Since the task was regression this caused confusion further down the processing pipeline. Code changed to use global selection cut instead of implicit signal/background cuts. Also includes additional improvements to variable names and documentation.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2460
https://github.com/root-project/root/pull/2460:164,security,Sign,Signal,164,"[TMVA] Fix for TMVACrossValidationRegression tutorial crash; Method signature used for `DataLoader::PrepareTrainingAndTestSet` triggered the generation of classes ""Signal"" and ""Background"". Since the task was regression this caused confusion further down the processing pipeline. Code changed to use global selection cut instead of implicit signal/background cuts. Also includes additional improvements to variable names and documentation.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2460
https://github.com/root-project/root/pull/2460:341,security,sign,signal,341,"[TMVA] Fix for TMVACrossValidationRegression tutorial crash; Method signature used for `DataLoader::PrepareTrainingAndTestSet` triggered the generation of classes ""Signal"" and ""Background"". Since the task was regression this caused confusion further down the processing pipeline. Code changed to use global selection cut instead of implicit signal/background cuts. Also includes additional improvements to variable names and documentation.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2460
https://github.com/root-project/root/pull/2460:209,testability,regress,regression,209,"[TMVA] Fix for TMVACrossValidationRegression tutorial crash; Method signature used for `DataLoader::PrepareTrainingAndTestSet` triggered the generation of classes ""Signal"" and ""Background"". Since the task was regression this caused confusion further down the processing pipeline. Code changed to use global selection cut instead of implicit signal/background cuts. Also includes additional improvements to variable names and documentation.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2460
https://github.com/root-project/root/pull/2460:425,usability,document,documentation,425,"[TMVA] Fix for TMVACrossValidationRegression tutorial crash; Method signature used for `DataLoader::PrepareTrainingAndTestSet` triggered the generation of classes ""Signal"" and ""Background"". Since the task was regression this caused confusion further down the processing pipeline. Code changed to use global selection cut instead of implicit signal/background cuts. Also includes additional improvements to variable names and documentation.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2460
https://github.com/root-project/root/pull/2461:38,deployability,log,logic,38,"[DF][ROOT-9457] Move result-readiness logic to RAction; Before this commit, RResultPtr and RLoopManager shared the status (run or not run) of the RAction. Now only the action has knowledge of it, and exposes a method to retrieve the information, used only by the RResultPtr.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2461
https://github.com/root-project/root/pull/2461:104,interoperability,share,shared,104,"[DF][ROOT-9457] Move result-readiness logic to RAction; Before this commit, RResultPtr and RLoopManager shared the status (run or not run) of the RAction. Now only the action has knowledge of it, and exposes a method to retrieve the information, used only by the RResultPtr.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2461
https://github.com/root-project/root/pull/2461:38,safety,log,logic,38,"[DF][ROOT-9457] Move result-readiness logic to RAction; Before this commit, RResultPtr and RLoopManager shared the status (run or not run) of the RAction. Now only the action has knowledge of it, and exposes a method to retrieve the information, used only by the RResultPtr.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2461
https://github.com/root-project/root/pull/2461:38,security,log,logic,38,"[DF][ROOT-9457] Move result-readiness logic to RAction; Before this commit, RResultPtr and RLoopManager shared the status (run or not run) of the RAction. Now only the action has knowledge of it, and exposes a method to retrieve the information, used only by the RResultPtr.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2461
https://github.com/root-project/root/pull/2461:200,security,expos,exposes,200,"[DF][ROOT-9457] Move result-readiness logic to RAction; Before this commit, RResultPtr and RLoopManager shared the status (run or not run) of the RAction. Now only the action has knowledge of it, and exposes a method to retrieve the information, used only by the RResultPtr.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2461
https://github.com/root-project/root/pull/2461:38,testability,log,logic,38,"[DF][ROOT-9457] Move result-readiness logic to RAction; Before this commit, RResultPtr and RLoopManager shared the status (run or not run) of the RAction. Now only the action has knowledge of it, and exposes a method to retrieve the information, used only by the RResultPtr.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2461
https://github.com/root-project/root/pull/2461:115,usability,statu,status,115,"[DF][ROOT-9457] Move result-readiness logic to RAction; Before this commit, RResultPtr and RLoopManager shared the status (run or not run) of the RAction. Now only the action has knowledge of it, and exposes a method to retrieve the information, used only by the RResultPtr.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2461
https://github.com/root-project/root/pull/2462:0,deployability,Updat,Update,0,Update clad to v0.2; The relevant highlights are:. * Support better Windows (thanks to Bertrand Bellenot!);. * Disabled automatic discovery of system LLVM -- clad should only. search for LLVM at DCLAD_PATH_TO_LLVM_BUILD. On some platforms. (discovered by Oksana Shadura via rootbench) clad discovers the. system LLVM which is compatible in principle but this is not what. we want for ROOT. * Implemented -CLAD_BUILD_STATIC_ONLY -- this covers the ROOT usecase. where we do not need shared objects but link the libraries against. another shared object (libCling.so). This allows platforms which have. disabled LLVM_ENABLE_PLUGINS to still build clad and use it. Such. example is CYGWIN and Windows. See more at: https://github.com/vgvassilev/clad/releases/tag/v0.2,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2462
https://github.com/root-project/root/pull/2462:120,deployability,automat,automatic,120,Update clad to v0.2; The relevant highlights are:. * Support better Windows (thanks to Bertrand Bellenot!);. * Disabled automatic discovery of system LLVM -- clad should only. search for LLVM at DCLAD_PATH_TO_LLVM_BUILD. On some platforms. (discovered by Oksana Shadura via rootbench) clad discovers the. system LLVM which is compatible in principle but this is not what. we want for ROOT. * Implemented -CLAD_BUILD_STATIC_ONLY -- this covers the ROOT usecase. where we do not need shared objects but link the libraries against. another shared object (libCling.so). This allows platforms which have. disabled LLVM_ENABLE_PLUGINS to still build clad and use it. Such. example is CYGWIN and Windows. See more at: https://github.com/vgvassilev/clad/releases/tag/v0.2,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2462
https://github.com/root-project/root/pull/2462:638,deployability,build,build,638,Update clad to v0.2; The relevant highlights are:. * Support better Windows (thanks to Bertrand Bellenot!);. * Disabled automatic discovery of system LLVM -- clad should only. search for LLVM at DCLAD_PATH_TO_LLVM_BUILD. On some platforms. (discovered by Oksana Shadura via rootbench) clad discovers the. system LLVM which is compatible in principle but this is not what. we want for ROOT. * Implemented -CLAD_BUILD_STATIC_ONLY -- this covers the ROOT usecase. where we do not need shared objects but link the libraries against. another shared object (libCling.so). This allows platforms which have. disabled LLVM_ENABLE_PLUGINS to still build clad and use it. Such. example is CYGWIN and Windows. See more at: https://github.com/vgvassilev/clad/releases/tag/v0.2,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2462
https://github.com/root-project/root/pull/2462:746,deployability,releas,releases,746,Update clad to v0.2; The relevant highlights are:. * Support better Windows (thanks to Bertrand Bellenot!);. * Disabled automatic discovery of system LLVM -- clad should only. search for LLVM at DCLAD_PATH_TO_LLVM_BUILD. On some platforms. (discovered by Oksana Shadura via rootbench) clad discovers the. system LLVM which is compatible in principle but this is not what. we want for ROOT. * Implemented -CLAD_BUILD_STATIC_ONLY -- this covers the ROOT usecase. where we do not need shared objects but link the libraries against. another shared object (libCling.so). This allows platforms which have. disabled LLVM_ENABLE_PLUGINS to still build clad and use it. Such. example is CYGWIN and Windows. See more at: https://github.com/vgvassilev/clad/releases/tag/v0.2,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2462
https://github.com/root-project/root/pull/2462:130,integrability,discover,discovery,130,Update clad to v0.2; The relevant highlights are:. * Support better Windows (thanks to Bertrand Bellenot!);. * Disabled automatic discovery of system LLVM -- clad should only. search for LLVM at DCLAD_PATH_TO_LLVM_BUILD. On some platforms. (discovered by Oksana Shadura via rootbench) clad discovers the. system LLVM which is compatible in principle but this is not what. we want for ROOT. * Implemented -CLAD_BUILD_STATIC_ONLY -- this covers the ROOT usecase. where we do not need shared objects but link the libraries against. another shared object (libCling.so). This allows platforms which have. disabled LLVM_ENABLE_PLUGINS to still build clad and use it. Such. example is CYGWIN and Windows. See more at: https://github.com/vgvassilev/clad/releases/tag/v0.2,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2462
https://github.com/root-project/root/pull/2462:241,integrability,discover,discovered,241,Update clad to v0.2; The relevant highlights are:. * Support better Windows (thanks to Bertrand Bellenot!);. * Disabled automatic discovery of system LLVM -- clad should only. search for LLVM at DCLAD_PATH_TO_LLVM_BUILD. On some platforms. (discovered by Oksana Shadura via rootbench) clad discovers the. system LLVM which is compatible in principle but this is not what. we want for ROOT. * Implemented -CLAD_BUILD_STATIC_ONLY -- this covers the ROOT usecase. where we do not need shared objects but link the libraries against. another shared object (libCling.so). This allows platforms which have. disabled LLVM_ENABLE_PLUGINS to still build clad and use it. Such. example is CYGWIN and Windows. See more at: https://github.com/vgvassilev/clad/releases/tag/v0.2,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2462
https://github.com/root-project/root/pull/2462:290,integrability,discover,discovers,290,Update clad to v0.2; The relevant highlights are:. * Support better Windows (thanks to Bertrand Bellenot!);. * Disabled automatic discovery of system LLVM -- clad should only. search for LLVM at DCLAD_PATH_TO_LLVM_BUILD. On some platforms. (discovered by Oksana Shadura via rootbench) clad discovers the. system LLVM which is compatible in principle but this is not what. we want for ROOT. * Implemented -CLAD_BUILD_STATIC_ONLY -- this covers the ROOT usecase. where we do not need shared objects but link the libraries against. another shared object (libCling.so). This allows platforms which have. disabled LLVM_ENABLE_PLUGINS to still build clad and use it. Such. example is CYGWIN and Windows. See more at: https://github.com/vgvassilev/clad/releases/tag/v0.2,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2462
https://github.com/root-project/root/pull/2462:130,interoperability,discover,discovery,130,Update clad to v0.2; The relevant highlights are:. * Support better Windows (thanks to Bertrand Bellenot!);. * Disabled automatic discovery of system LLVM -- clad should only. search for LLVM at DCLAD_PATH_TO_LLVM_BUILD. On some platforms. (discovered by Oksana Shadura via rootbench) clad discovers the. system LLVM which is compatible in principle but this is not what. we want for ROOT. * Implemented -CLAD_BUILD_STATIC_ONLY -- this covers the ROOT usecase. where we do not need shared objects but link the libraries against. another shared object (libCling.so). This allows platforms which have. disabled LLVM_ENABLE_PLUGINS to still build clad and use it. Such. example is CYGWIN and Windows. See more at: https://github.com/vgvassilev/clad/releases/tag/v0.2,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2462
https://github.com/root-project/root/pull/2462:229,interoperability,platform,platforms,229,Update clad to v0.2; The relevant highlights are:. * Support better Windows (thanks to Bertrand Bellenot!);. * Disabled automatic discovery of system LLVM -- clad should only. search for LLVM at DCLAD_PATH_TO_LLVM_BUILD. On some platforms. (discovered by Oksana Shadura via rootbench) clad discovers the. system LLVM which is compatible in principle but this is not what. we want for ROOT. * Implemented -CLAD_BUILD_STATIC_ONLY -- this covers the ROOT usecase. where we do not need shared objects but link the libraries against. another shared object (libCling.so). This allows platforms which have. disabled LLVM_ENABLE_PLUGINS to still build clad and use it. Such. example is CYGWIN and Windows. See more at: https://github.com/vgvassilev/clad/releases/tag/v0.2,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2462
https://github.com/root-project/root/pull/2462:241,interoperability,discover,discovered,241,Update clad to v0.2; The relevant highlights are:. * Support better Windows (thanks to Bertrand Bellenot!);. * Disabled automatic discovery of system LLVM -- clad should only. search for LLVM at DCLAD_PATH_TO_LLVM_BUILD. On some platforms. (discovered by Oksana Shadura via rootbench) clad discovers the. system LLVM which is compatible in principle but this is not what. we want for ROOT. * Implemented -CLAD_BUILD_STATIC_ONLY -- this covers the ROOT usecase. where we do not need shared objects but link the libraries against. another shared object (libCling.so). This allows platforms which have. disabled LLVM_ENABLE_PLUGINS to still build clad and use it. Such. example is CYGWIN and Windows. See more at: https://github.com/vgvassilev/clad/releases/tag/v0.2,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2462
https://github.com/root-project/root/pull/2462:290,interoperability,discover,discovers,290,Update clad to v0.2; The relevant highlights are:. * Support better Windows (thanks to Bertrand Bellenot!);. * Disabled automatic discovery of system LLVM -- clad should only. search for LLVM at DCLAD_PATH_TO_LLVM_BUILD. On some platforms. (discovered by Oksana Shadura via rootbench) clad discovers the. system LLVM which is compatible in principle but this is not what. we want for ROOT. * Implemented -CLAD_BUILD_STATIC_ONLY -- this covers the ROOT usecase. where we do not need shared objects but link the libraries against. another shared object (libCling.so). This allows platforms which have. disabled LLVM_ENABLE_PLUGINS to still build clad and use it. Such. example is CYGWIN and Windows. See more at: https://github.com/vgvassilev/clad/releases/tag/v0.2,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2462
https://github.com/root-project/root/pull/2462:326,interoperability,compatib,compatible,326,Update clad to v0.2; The relevant highlights are:. * Support better Windows (thanks to Bertrand Bellenot!);. * Disabled automatic discovery of system LLVM -- clad should only. search for LLVM at DCLAD_PATH_TO_LLVM_BUILD. On some platforms. (discovered by Oksana Shadura via rootbench) clad discovers the. system LLVM which is compatible in principle but this is not what. we want for ROOT. * Implemented -CLAD_BUILD_STATIC_ONLY -- this covers the ROOT usecase. where we do not need shared objects but link the libraries against. another shared object (libCling.so). This allows platforms which have. disabled LLVM_ENABLE_PLUGINS to still build clad and use it. Such. example is CYGWIN and Windows. See more at: https://github.com/vgvassilev/clad/releases/tag/v0.2,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2462
https://github.com/root-project/root/pull/2462:482,interoperability,share,shared,482,Update clad to v0.2; The relevant highlights are:. * Support better Windows (thanks to Bertrand Bellenot!);. * Disabled automatic discovery of system LLVM -- clad should only. search for LLVM at DCLAD_PATH_TO_LLVM_BUILD. On some platforms. (discovered by Oksana Shadura via rootbench) clad discovers the. system LLVM which is compatible in principle but this is not what. we want for ROOT. * Implemented -CLAD_BUILD_STATIC_ONLY -- this covers the ROOT usecase. where we do not need shared objects but link the libraries against. another shared object (libCling.so). This allows platforms which have. disabled LLVM_ENABLE_PLUGINS to still build clad and use it. Such. example is CYGWIN and Windows. See more at: https://github.com/vgvassilev/clad/releases/tag/v0.2,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2462
https://github.com/root-project/root/pull/2462:537,interoperability,share,shared,537,Update clad to v0.2; The relevant highlights are:. * Support better Windows (thanks to Bertrand Bellenot!);. * Disabled automatic discovery of system LLVM -- clad should only. search for LLVM at DCLAD_PATH_TO_LLVM_BUILD. On some platforms. (discovered by Oksana Shadura via rootbench) clad discovers the. system LLVM which is compatible in principle but this is not what. we want for ROOT. * Implemented -CLAD_BUILD_STATIC_ONLY -- this covers the ROOT usecase. where we do not need shared objects but link the libraries against. another shared object (libCling.so). This allows platforms which have. disabled LLVM_ENABLE_PLUGINS to still build clad and use it. Such. example is CYGWIN and Windows. See more at: https://github.com/vgvassilev/clad/releases/tag/v0.2,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2462
https://github.com/root-project/root/pull/2462:578,interoperability,platform,platforms,578,Update clad to v0.2; The relevant highlights are:. * Support better Windows (thanks to Bertrand Bellenot!);. * Disabled automatic discovery of system LLVM -- clad should only. search for LLVM at DCLAD_PATH_TO_LLVM_BUILD. On some platforms. (discovered by Oksana Shadura via rootbench) clad discovers the. system LLVM which is compatible in principle but this is not what. we want for ROOT. * Implemented -CLAD_BUILD_STATIC_ONLY -- this covers the ROOT usecase. where we do not need shared objects but link the libraries against. another shared object (libCling.so). This allows platforms which have. disabled LLVM_ENABLE_PLUGINS to still build clad and use it. Such. example is CYGWIN and Windows. See more at: https://github.com/vgvassilev/clad/releases/tag/v0.2,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2462
https://github.com/root-project/root/pull/2462:0,safety,Updat,Update,0,Update clad to v0.2; The relevant highlights are:. * Support better Windows (thanks to Bertrand Bellenot!);. * Disabled automatic discovery of system LLVM -- clad should only. search for LLVM at DCLAD_PATH_TO_LLVM_BUILD. On some platforms. (discovered by Oksana Shadura via rootbench) clad discovers the. system LLVM which is compatible in principle but this is not what. we want for ROOT. * Implemented -CLAD_BUILD_STATIC_ONLY -- this covers the ROOT usecase. where we do not need shared objects but link the libraries against. another shared object (libCling.so). This allows platforms which have. disabled LLVM_ENABLE_PLUGINS to still build clad and use it. Such. example is CYGWIN and Windows. See more at: https://github.com/vgvassilev/clad/releases/tag/v0.2,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2462
https://github.com/root-project/root/pull/2462:0,security,Updat,Update,0,Update clad to v0.2; The relevant highlights are:. * Support better Windows (thanks to Bertrand Bellenot!);. * Disabled automatic discovery of system LLVM -- clad should only. search for LLVM at DCLAD_PATH_TO_LLVM_BUILD. On some platforms. (discovered by Oksana Shadura via rootbench) clad discovers the. system LLVM which is compatible in principle but this is not what. we want for ROOT. * Implemented -CLAD_BUILD_STATIC_ONLY -- this covers the ROOT usecase. where we do not need shared objects but link the libraries against. another shared object (libCling.so). This allows platforms which have. disabled LLVM_ENABLE_PLUGINS to still build clad and use it. Such. example is CYGWIN and Windows. See more at: https://github.com/vgvassilev/clad/releases/tag/v0.2,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2462
https://github.com/root-project/root/pull/2462:120,testability,automat,automatic,120,Update clad to v0.2; The relevant highlights are:. * Support better Windows (thanks to Bertrand Bellenot!);. * Disabled automatic discovery of system LLVM -- clad should only. search for LLVM at DCLAD_PATH_TO_LLVM_BUILD. On some platforms. (discovered by Oksana Shadura via rootbench) clad discovers the. system LLVM which is compatible in principle but this is not what. we want for ROOT. * Implemented -CLAD_BUILD_STATIC_ONLY -- this covers the ROOT usecase. where we do not need shared objects but link the libraries against. another shared object (libCling.so). This allows platforms which have. disabled LLVM_ENABLE_PLUGINS to still build clad and use it. Such. example is CYGWIN and Windows. See more at: https://github.com/vgvassilev/clad/releases/tag/v0.2,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2462
https://github.com/root-project/root/pull/2462:53,usability,Support,Support,53,Update clad to v0.2; The relevant highlights are:. * Support better Windows (thanks to Bertrand Bellenot!);. * Disabled automatic discovery of system LLVM -- clad should only. search for LLVM at DCLAD_PATH_TO_LLVM_BUILD. On some platforms. (discovered by Oksana Shadura via rootbench) clad discovers the. system LLVM which is compatible in principle but this is not what. we want for ROOT. * Implemented -CLAD_BUILD_STATIC_ONLY -- this covers the ROOT usecase. where we do not need shared objects but link the libraries against. another shared object (libCling.so). This allows platforms which have. disabled LLVM_ENABLE_PLUGINS to still build clad and use it. Such. example is CYGWIN and Windows. See more at: https://github.com/vgvassilev/clad/releases/tag/v0.2,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2462
https://github.com/root-project/root/pull/2462:130,usability,discov,discovery,130,Update clad to v0.2; The relevant highlights are:. * Support better Windows (thanks to Bertrand Bellenot!);. * Disabled automatic discovery of system LLVM -- clad should only. search for LLVM at DCLAD_PATH_TO_LLVM_BUILD. On some platforms. (discovered by Oksana Shadura via rootbench) clad discovers the. system LLVM which is compatible in principle but this is not what. we want for ROOT. * Implemented -CLAD_BUILD_STATIC_ONLY -- this covers the ROOT usecase. where we do not need shared objects but link the libraries against. another shared object (libCling.so). This allows platforms which have. disabled LLVM_ENABLE_PLUGINS to still build clad and use it. Such. example is CYGWIN and Windows. See more at: https://github.com/vgvassilev/clad/releases/tag/v0.2,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2462
https://github.com/root-project/root/pull/2462:241,usability,discov,discovered,241,Update clad to v0.2; The relevant highlights are:. * Support better Windows (thanks to Bertrand Bellenot!);. * Disabled automatic discovery of system LLVM -- clad should only. search for LLVM at DCLAD_PATH_TO_LLVM_BUILD. On some platforms. (discovered by Oksana Shadura via rootbench) clad discovers the. system LLVM which is compatible in principle but this is not what. we want for ROOT. * Implemented -CLAD_BUILD_STATIC_ONLY -- this covers the ROOT usecase. where we do not need shared objects but link the libraries against. another shared object (libCling.so). This allows platforms which have. disabled LLVM_ENABLE_PLUGINS to still build clad and use it. Such. example is CYGWIN and Windows. See more at: https://github.com/vgvassilev/clad/releases/tag/v0.2,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2462
https://github.com/root-project/root/pull/2462:290,usability,discov,discovers,290,Update clad to v0.2; The relevant highlights are:. * Support better Windows (thanks to Bertrand Bellenot!);. * Disabled automatic discovery of system LLVM -- clad should only. search for LLVM at DCLAD_PATH_TO_LLVM_BUILD. On some platforms. (discovered by Oksana Shadura via rootbench) clad discovers the. system LLVM which is compatible in principle but this is not what. we want for ROOT. * Implemented -CLAD_BUILD_STATIC_ONLY -- this covers the ROOT usecase. where we do not need shared objects but link the libraries against. another shared object (libCling.so). This allows platforms which have. disabled LLVM_ENABLE_PLUGINS to still build clad and use it. Such. example is CYGWIN and Windows. See more at: https://github.com/vgvassilev/clad/releases/tag/v0.2,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2462
https://github.com/root-project/root/pull/2463:549,availability,state,state,549,"Reland ""[cxxmodules] Check correctly if the decl was cached.""; Improve the commit by tighten condition to take an expensive branch. In BaseSelectionRule::Match, for normal CXXRecordDecl, nothing need to be worried as. you can just compare name or pointer and that's it. However when you have typedef, it means that you can't just compare name (it's different!) or pointer (it's also different). And here, ScopeSearch is not actually searching the argument. It's just renewing the fCXXRecordDecl from fName to keep it up with the current interpreter state. Also, rootcling needed to parse header to get the actual definition of typedef, because rootmap file given to the invocation was lacking the necessary information. For example it contained `typedef typedefB`, from this there is no way to know which RecordDecl is underneath. With modules, we don't use rootmap files. Thus we need to check if it's a typdef or not, and if it was a typedef,. make sure that it didn't take the same branch with the same interp. state (by checking (void*)-1) and it's not from modules. This improves cxxmodules res memory by 90MB. ```. before:. res memory = 345.816 Mbytes. after. res memory = 267.582 Mbytes. ```. Also, this fasten rootcling 2 times. ```. before:. User time (seconds): 2.56. after:. User time (seconds): 1.35. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2463
https://github.com/root-project/root/pull/2463:1014,availability,state,state,1014,"Reland ""[cxxmodules] Check correctly if the decl was cached.""; Improve the commit by tighten condition to take an expensive branch. In BaseSelectionRule::Match, for normal CXXRecordDecl, nothing need to be worried as. you can just compare name or pointer and that's it. However when you have typedef, it means that you can't just compare name (it's different!) or pointer (it's also different). And here, ScopeSearch is not actually searching the argument. It's just renewing the fCXXRecordDecl from fName to keep it up with the current interpreter state. Also, rootcling needed to parse header to get the actual definition of typedef, because rootmap file given to the invocation was lacking the necessary information. For example it contained `typedef typedefB`, from this there is no way to know which RecordDecl is underneath. With modules, we don't use rootmap files. Thus we need to check if it's a typdef or not, and if it was a typedef,. make sure that it didn't take the same branch with the same interp. state (by checking (void*)-1) and it's not from modules. This improves cxxmodules res memory by 90MB. ```. before:. res memory = 345.816 Mbytes. after. res memory = 267.582 Mbytes. ```. Also, this fasten rootcling 2 times. ```. before:. User time (seconds): 2.56. after:. User time (seconds): 1.35. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2463
https://github.com/root-project/root/pull/2463:735,deployability,contain,contained,735,"Reland ""[cxxmodules] Check correctly if the decl was cached.""; Improve the commit by tighten condition to take an expensive branch. In BaseSelectionRule::Match, for normal CXXRecordDecl, nothing need to be worried as. you can just compare name or pointer and that's it. However when you have typedef, it means that you can't just compare name (it's different!) or pointer (it's also different). And here, ScopeSearch is not actually searching the argument. It's just renewing the fCXXRecordDecl from fName to keep it up with the current interpreter state. Also, rootcling needed to parse header to get the actual definition of typedef, because rootmap file given to the invocation was lacking the necessary information. For example it contained `typedef typedefB`, from this there is no way to know which RecordDecl is underneath. With modules, we don't use rootmap files. Thus we need to check if it's a typdef or not, and if it was a typedef,. make sure that it didn't take the same branch with the same interp. state (by checking (void*)-1) and it's not from modules. This improves cxxmodules res memory by 90MB. ```. before:. res memory = 345.816 Mbytes. after. res memory = 267.582 Mbytes. ```. Also, this fasten rootcling 2 times. ```. before:. User time (seconds): 2.56. after:. User time (seconds): 1.35. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2463
https://github.com/root-project/root/pull/2463:836,deployability,modul,modules,836,"Reland ""[cxxmodules] Check correctly if the decl was cached.""; Improve the commit by tighten condition to take an expensive branch. In BaseSelectionRule::Match, for normal CXXRecordDecl, nothing need to be worried as. you can just compare name or pointer and that's it. However when you have typedef, it means that you can't just compare name (it's different!) or pointer (it's also different). And here, ScopeSearch is not actually searching the argument. It's just renewing the fCXXRecordDecl from fName to keep it up with the current interpreter state. Also, rootcling needed to parse header to get the actual definition of typedef, because rootmap file given to the invocation was lacking the necessary information. For example it contained `typedef typedefB`, from this there is no way to know which RecordDecl is underneath. With modules, we don't use rootmap files. Thus we need to check if it's a typdef or not, and if it was a typedef,. make sure that it didn't take the same branch with the same interp. state (by checking (void*)-1) and it's not from modules. This improves cxxmodules res memory by 90MB. ```. before:. res memory = 345.816 Mbytes. after. res memory = 267.582 Mbytes. ```. Also, this fasten rootcling 2 times. ```. before:. User time (seconds): 2.56. after:. User time (seconds): 1.35. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2463
https://github.com/root-project/root/pull/2463:1062,deployability,modul,modules,1062,"Reland ""[cxxmodules] Check correctly if the decl was cached.""; Improve the commit by tighten condition to take an expensive branch. In BaseSelectionRule::Match, for normal CXXRecordDecl, nothing need to be worried as. you can just compare name or pointer and that's it. However when you have typedef, it means that you can't just compare name (it's different!) or pointer (it's also different). And here, ScopeSearch is not actually searching the argument. It's just renewing the fCXXRecordDecl from fName to keep it up with the current interpreter state. Also, rootcling needed to parse header to get the actual definition of typedef, because rootmap file given to the invocation was lacking the necessary information. For example it contained `typedef typedefB`, from this there is no way to know which RecordDecl is underneath. With modules, we don't use rootmap files. Thus we need to check if it's a typdef or not, and if it was a typedef,. make sure that it didn't take the same branch with the same interp. state (by checking (void*)-1) and it's not from modules. This improves cxxmodules res memory by 90MB. ```. before:. res memory = 345.816 Mbytes. after. res memory = 267.582 Mbytes. ```. Also, this fasten rootcling 2 times. ```. before:. User time (seconds): 2.56. after:. User time (seconds): 1.35. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2463
https://github.com/root-project/root/pull/2463:529,energy efficiency,current,current,529,"Reland ""[cxxmodules] Check correctly if the decl was cached.""; Improve the commit by tighten condition to take an expensive branch. In BaseSelectionRule::Match, for normal CXXRecordDecl, nothing need to be worried as. you can just compare name or pointer and that's it. However when you have typedef, it means that you can't just compare name (it's different!) or pointer (it's also different). And here, ScopeSearch is not actually searching the argument. It's just renewing the fCXXRecordDecl from fName to keep it up with the current interpreter state. Also, rootcling needed to parse header to get the actual definition of typedef, because rootmap file given to the invocation was lacking the necessary information. For example it contained `typedef typedefB`, from this there is no way to know which RecordDecl is underneath. With modules, we don't use rootmap files. Thus we need to check if it's a typdef or not, and if it was a typedef,. make sure that it didn't take the same branch with the same interp. state (by checking (void*)-1) and it's not from modules. This improves cxxmodules res memory by 90MB. ```. before:. res memory = 345.816 Mbytes. after. res memory = 267.582 Mbytes. ```. Also, this fasten rootcling 2 times. ```. before:. User time (seconds): 2.56. after:. User time (seconds): 1.35. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2463
https://github.com/root-project/root/pull/2463:549,integrability,state,state,549,"Reland ""[cxxmodules] Check correctly if the decl was cached.""; Improve the commit by tighten condition to take an expensive branch. In BaseSelectionRule::Match, for normal CXXRecordDecl, nothing need to be worried as. you can just compare name or pointer and that's it. However when you have typedef, it means that you can't just compare name (it's different!) or pointer (it's also different). And here, ScopeSearch is not actually searching the argument. It's just renewing the fCXXRecordDecl from fName to keep it up with the current interpreter state. Also, rootcling needed to parse header to get the actual definition of typedef, because rootmap file given to the invocation was lacking the necessary information. For example it contained `typedef typedefB`, from this there is no way to know which RecordDecl is underneath. With modules, we don't use rootmap files. Thus we need to check if it's a typdef or not, and if it was a typedef,. make sure that it didn't take the same branch with the same interp. state (by checking (void*)-1) and it's not from modules. This improves cxxmodules res memory by 90MB. ```. before:. res memory = 345.816 Mbytes. after. res memory = 267.582 Mbytes. ```. Also, this fasten rootcling 2 times. ```. before:. User time (seconds): 2.56. after:. User time (seconds): 1.35. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2463
https://github.com/root-project/root/pull/2463:1014,integrability,state,state,1014,"Reland ""[cxxmodules] Check correctly if the decl was cached.""; Improve the commit by tighten condition to take an expensive branch. In BaseSelectionRule::Match, for normal CXXRecordDecl, nothing need to be worried as. you can just compare name or pointer and that's it. However when you have typedef, it means that you can't just compare name (it's different!) or pointer (it's also different). And here, ScopeSearch is not actually searching the argument. It's just renewing the fCXXRecordDecl from fName to keep it up with the current interpreter state. Also, rootcling needed to parse header to get the actual definition of typedef, because rootmap file given to the invocation was lacking the necessary information. For example it contained `typedef typedefB`, from this there is no way to know which RecordDecl is underneath. With modules, we don't use rootmap files. Thus we need to check if it's a typdef or not, and if it was a typedef,. make sure that it didn't take the same branch with the same interp. state (by checking (void*)-1) and it's not from modules. This improves cxxmodules res memory by 90MB. ```. before:. res memory = 345.816 Mbytes. after. res memory = 267.582 Mbytes. ```. Also, this fasten rootcling 2 times. ```. before:. User time (seconds): 2.56. after:. User time (seconds): 1.35. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2463
https://github.com/root-project/root/pull/2463:836,modifiability,modul,modules,836,"Reland ""[cxxmodules] Check correctly if the decl was cached.""; Improve the commit by tighten condition to take an expensive branch. In BaseSelectionRule::Match, for normal CXXRecordDecl, nothing need to be worried as. you can just compare name or pointer and that's it. However when you have typedef, it means that you can't just compare name (it's different!) or pointer (it's also different). And here, ScopeSearch is not actually searching the argument. It's just renewing the fCXXRecordDecl from fName to keep it up with the current interpreter state. Also, rootcling needed to parse header to get the actual definition of typedef, because rootmap file given to the invocation was lacking the necessary information. For example it contained `typedef typedefB`, from this there is no way to know which RecordDecl is underneath. With modules, we don't use rootmap files. Thus we need to check if it's a typdef or not, and if it was a typedef,. make sure that it didn't take the same branch with the same interp. state (by checking (void*)-1) and it's not from modules. This improves cxxmodules res memory by 90MB. ```. before:. res memory = 345.816 Mbytes. after. res memory = 267.582 Mbytes. ```. Also, this fasten rootcling 2 times. ```. before:. User time (seconds): 2.56. after:. User time (seconds): 1.35. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2463
https://github.com/root-project/root/pull/2463:1062,modifiability,modul,modules,1062,"Reland ""[cxxmodules] Check correctly if the decl was cached.""; Improve the commit by tighten condition to take an expensive branch. In BaseSelectionRule::Match, for normal CXXRecordDecl, nothing need to be worried as. you can just compare name or pointer and that's it. However when you have typedef, it means that you can't just compare name (it's different!) or pointer (it's also different). And here, ScopeSearch is not actually searching the argument. It's just renewing the fCXXRecordDecl from fName to keep it up with the current interpreter state. Also, rootcling needed to parse header to get the actual definition of typedef, because rootmap file given to the invocation was lacking the necessary information. For example it contained `typedef typedefB`, from this there is no way to know which RecordDecl is underneath. With modules, we don't use rootmap files. Thus we need to check if it's a typdef or not, and if it was a typedef,. make sure that it didn't take the same branch with the same interp. state (by checking (void*)-1) and it's not from modules. This improves cxxmodules res memory by 90MB. ```. before:. res memory = 345.816 Mbytes. after. res memory = 267.582 Mbytes. ```. Also, this fasten rootcling 2 times. ```. before:. User time (seconds): 2.56. after:. User time (seconds): 1.35. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2463
https://github.com/root-project/root/pull/2463:53,performance,cach,cached,53,"Reland ""[cxxmodules] Check correctly if the decl was cached.""; Improve the commit by tighten condition to take an expensive branch. In BaseSelectionRule::Match, for normal CXXRecordDecl, nothing need to be worried as. you can just compare name or pointer and that's it. However when you have typedef, it means that you can't just compare name (it's different!) or pointer (it's also different). And here, ScopeSearch is not actually searching the argument. It's just renewing the fCXXRecordDecl from fName to keep it up with the current interpreter state. Also, rootcling needed to parse header to get the actual definition of typedef, because rootmap file given to the invocation was lacking the necessary information. For example it contained `typedef typedefB`, from this there is no way to know which RecordDecl is underneath. With modules, we don't use rootmap files. Thus we need to check if it's a typdef or not, and if it was a typedef,. make sure that it didn't take the same branch with the same interp. state (by checking (void*)-1) and it's not from modules. This improves cxxmodules res memory by 90MB. ```. before:. res memory = 345.816 Mbytes. after. res memory = 267.582 Mbytes. ```. Also, this fasten rootcling 2 times. ```. before:. User time (seconds): 2.56. after:. User time (seconds): 1.35. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2463
https://github.com/root-project/root/pull/2463:1100,performance,memor,memory,1100,"Reland ""[cxxmodules] Check correctly if the decl was cached.""; Improve the commit by tighten condition to take an expensive branch. In BaseSelectionRule::Match, for normal CXXRecordDecl, nothing need to be worried as. you can just compare name or pointer and that's it. However when you have typedef, it means that you can't just compare name (it's different!) or pointer (it's also different). And here, ScopeSearch is not actually searching the argument. It's just renewing the fCXXRecordDecl from fName to keep it up with the current interpreter state. Also, rootcling needed to parse header to get the actual definition of typedef, because rootmap file given to the invocation was lacking the necessary information. For example it contained `typedef typedefB`, from this there is no way to know which RecordDecl is underneath. With modules, we don't use rootmap files. Thus we need to check if it's a typdef or not, and if it was a typedef,. make sure that it didn't take the same branch with the same interp. state (by checking (void*)-1) and it's not from modules. This improves cxxmodules res memory by 90MB. ```. before:. res memory = 345.816 Mbytes. after. res memory = 267.582 Mbytes. ```. Also, this fasten rootcling 2 times. ```. before:. User time (seconds): 2.56. after:. User time (seconds): 1.35. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2463
https://github.com/root-project/root/pull/2463:1134,performance,memor,memory,1134,"Reland ""[cxxmodules] Check correctly if the decl was cached.""; Improve the commit by tighten condition to take an expensive branch. In BaseSelectionRule::Match, for normal CXXRecordDecl, nothing need to be worried as. you can just compare name or pointer and that's it. However when you have typedef, it means that you can't just compare name (it's different!) or pointer (it's also different). And here, ScopeSearch is not actually searching the argument. It's just renewing the fCXXRecordDecl from fName to keep it up with the current interpreter state. Also, rootcling needed to parse header to get the actual definition of typedef, because rootmap file given to the invocation was lacking the necessary information. For example it contained `typedef typedefB`, from this there is no way to know which RecordDecl is underneath. With modules, we don't use rootmap files. Thus we need to check if it's a typdef or not, and if it was a typedef,. make sure that it didn't take the same branch with the same interp. state (by checking (void*)-1) and it's not from modules. This improves cxxmodules res memory by 90MB. ```. before:. res memory = 345.816 Mbytes. after. res memory = 267.582 Mbytes. ```. Also, this fasten rootcling 2 times. ```. before:. User time (seconds): 2.56. after:. User time (seconds): 1.35. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2463
https://github.com/root-project/root/pull/2463:1170,performance,memor,memory,1170,"Reland ""[cxxmodules] Check correctly if the decl was cached.""; Improve the commit by tighten condition to take an expensive branch. In BaseSelectionRule::Match, for normal CXXRecordDecl, nothing need to be worried as. you can just compare name or pointer and that's it. However when you have typedef, it means that you can't just compare name (it's different!) or pointer (it's also different). And here, ScopeSearch is not actually searching the argument. It's just renewing the fCXXRecordDecl from fName to keep it up with the current interpreter state. Also, rootcling needed to parse header to get the actual definition of typedef, because rootmap file given to the invocation was lacking the necessary information. For example it contained `typedef typedefB`, from this there is no way to know which RecordDecl is underneath. With modules, we don't use rootmap files. Thus we need to check if it's a typdef or not, and if it was a typedef,. make sure that it didn't take the same branch with the same interp. state (by checking (void*)-1) and it's not from modules. This improves cxxmodules res memory by 90MB. ```. before:. res memory = 345.816 Mbytes. after. res memory = 267.582 Mbytes. ```. Also, this fasten rootcling 2 times. ```. before:. User time (seconds): 2.56. after:. User time (seconds): 1.35. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2463
https://github.com/root-project/root/pull/2463:1230,performance,time,times,1230,"Reland ""[cxxmodules] Check correctly if the decl was cached.""; Improve the commit by tighten condition to take an expensive branch. In BaseSelectionRule::Match, for normal CXXRecordDecl, nothing need to be worried as. you can just compare name or pointer and that's it. However when you have typedef, it means that you can't just compare name (it's different!) or pointer (it's also different). And here, ScopeSearch is not actually searching the argument. It's just renewing the fCXXRecordDecl from fName to keep it up with the current interpreter state. Also, rootcling needed to parse header to get the actual definition of typedef, because rootmap file given to the invocation was lacking the necessary information. For example it contained `typedef typedefB`, from this there is no way to know which RecordDecl is underneath. With modules, we don't use rootmap files. Thus we need to check if it's a typdef or not, and if it was a typedef,. make sure that it didn't take the same branch with the same interp. state (by checking (void*)-1) and it's not from modules. This improves cxxmodules res memory by 90MB. ```. before:. res memory = 345.816 Mbytes. after. res memory = 267.582 Mbytes. ```. Also, this fasten rootcling 2 times. ```. before:. User time (seconds): 2.56. after:. User time (seconds): 1.35. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2463
https://github.com/root-project/root/pull/2463:1256,performance,time,time,1256,"Reland ""[cxxmodules] Check correctly if the decl was cached.""; Improve the commit by tighten condition to take an expensive branch. In BaseSelectionRule::Match, for normal CXXRecordDecl, nothing need to be worried as. you can just compare name or pointer and that's it. However when you have typedef, it means that you can't just compare name (it's different!) or pointer (it's also different). And here, ScopeSearch is not actually searching the argument. It's just renewing the fCXXRecordDecl from fName to keep it up with the current interpreter state. Also, rootcling needed to parse header to get the actual definition of typedef, because rootmap file given to the invocation was lacking the necessary information. For example it contained `typedef typedefB`, from this there is no way to know which RecordDecl is underneath. With modules, we don't use rootmap files. Thus we need to check if it's a typdef or not, and if it was a typedef,. make sure that it didn't take the same branch with the same interp. state (by checking (void*)-1) and it's not from modules. This improves cxxmodules res memory by 90MB. ```. before:. res memory = 345.816 Mbytes. after. res memory = 267.582 Mbytes. ```. Also, this fasten rootcling 2 times. ```. before:. User time (seconds): 2.56. after:. User time (seconds): 1.35. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2463
https://github.com/root-project/root/pull/2463:1291,performance,time,time,1291,"Reland ""[cxxmodules] Check correctly if the decl was cached.""; Improve the commit by tighten condition to take an expensive branch. In BaseSelectionRule::Match, for normal CXXRecordDecl, nothing need to be worried as. you can just compare name or pointer and that's it. However when you have typedef, it means that you can't just compare name (it's different!) or pointer (it's also different). And here, ScopeSearch is not actually searching the argument. It's just renewing the fCXXRecordDecl from fName to keep it up with the current interpreter state. Also, rootcling needed to parse header to get the actual definition of typedef, because rootmap file given to the invocation was lacking the necessary information. For example it contained `typedef typedefB`, from this there is no way to know which RecordDecl is underneath. With modules, we don't use rootmap files. Thus we need to check if it's a typdef or not, and if it was a typedef,. make sure that it didn't take the same branch with the same interp. state (by checking (void*)-1) and it's not from modules. This improves cxxmodules res memory by 90MB. ```. before:. res memory = 345.816 Mbytes. after. res memory = 267.582 Mbytes. ```. Also, this fasten rootcling 2 times. ```. before:. User time (seconds): 2.56. after:. User time (seconds): 1.35. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2463
https://github.com/root-project/root/pull/2463:836,safety,modul,modules,836,"Reland ""[cxxmodules] Check correctly if the decl was cached.""; Improve the commit by tighten condition to take an expensive branch. In BaseSelectionRule::Match, for normal CXXRecordDecl, nothing need to be worried as. you can just compare name or pointer and that's it. However when you have typedef, it means that you can't just compare name (it's different!) or pointer (it's also different). And here, ScopeSearch is not actually searching the argument. It's just renewing the fCXXRecordDecl from fName to keep it up with the current interpreter state. Also, rootcling needed to parse header to get the actual definition of typedef, because rootmap file given to the invocation was lacking the necessary information. For example it contained `typedef typedefB`, from this there is no way to know which RecordDecl is underneath. With modules, we don't use rootmap files. Thus we need to check if it's a typdef or not, and if it was a typedef,. make sure that it didn't take the same branch with the same interp. state (by checking (void*)-1) and it's not from modules. This improves cxxmodules res memory by 90MB. ```. before:. res memory = 345.816 Mbytes. after. res memory = 267.582 Mbytes. ```. Also, this fasten rootcling 2 times. ```. before:. User time (seconds): 2.56. after:. User time (seconds): 1.35. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2463
https://github.com/root-project/root/pull/2463:1062,safety,modul,modules,1062,"Reland ""[cxxmodules] Check correctly if the decl was cached.""; Improve the commit by tighten condition to take an expensive branch. In BaseSelectionRule::Match, for normal CXXRecordDecl, nothing need to be worried as. you can just compare name or pointer and that's it. However when you have typedef, it means that you can't just compare name (it's different!) or pointer (it's also different). And here, ScopeSearch is not actually searching the argument. It's just renewing the fCXXRecordDecl from fName to keep it up with the current interpreter state. Also, rootcling needed to parse header to get the actual definition of typedef, because rootmap file given to the invocation was lacking the necessary information. For example it contained `typedef typedefB`, from this there is no way to know which RecordDecl is underneath. With modules, we don't use rootmap files. Thus we need to check if it's a typdef or not, and if it was a typedef,. make sure that it didn't take the same branch with the same interp. state (by checking (void*)-1) and it's not from modules. This improves cxxmodules res memory by 90MB. ```. before:. res memory = 345.816 Mbytes. after. res memory = 267.582 Mbytes. ```. Also, this fasten rootcling 2 times. ```. before:. User time (seconds): 2.56. after:. User time (seconds): 1.35. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2463
https://github.com/root-project/root/pull/2463:1100,usability,memor,memory,1100,"Reland ""[cxxmodules] Check correctly if the decl was cached.""; Improve the commit by tighten condition to take an expensive branch. In BaseSelectionRule::Match, for normal CXXRecordDecl, nothing need to be worried as. you can just compare name or pointer and that's it. However when you have typedef, it means that you can't just compare name (it's different!) or pointer (it's also different). And here, ScopeSearch is not actually searching the argument. It's just renewing the fCXXRecordDecl from fName to keep it up with the current interpreter state. Also, rootcling needed to parse header to get the actual definition of typedef, because rootmap file given to the invocation was lacking the necessary information. For example it contained `typedef typedefB`, from this there is no way to know which RecordDecl is underneath. With modules, we don't use rootmap files. Thus we need to check if it's a typdef or not, and if it was a typedef,. make sure that it didn't take the same branch with the same interp. state (by checking (void*)-1) and it's not from modules. This improves cxxmodules res memory by 90MB. ```. before:. res memory = 345.816 Mbytes. after. res memory = 267.582 Mbytes. ```. Also, this fasten rootcling 2 times. ```. before:. User time (seconds): 2.56. after:. User time (seconds): 1.35. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2463
https://github.com/root-project/root/pull/2463:1134,usability,memor,memory,1134,"Reland ""[cxxmodules] Check correctly if the decl was cached.""; Improve the commit by tighten condition to take an expensive branch. In BaseSelectionRule::Match, for normal CXXRecordDecl, nothing need to be worried as. you can just compare name or pointer and that's it. However when you have typedef, it means that you can't just compare name (it's different!) or pointer (it's also different). And here, ScopeSearch is not actually searching the argument. It's just renewing the fCXXRecordDecl from fName to keep it up with the current interpreter state. Also, rootcling needed to parse header to get the actual definition of typedef, because rootmap file given to the invocation was lacking the necessary information. For example it contained `typedef typedefB`, from this there is no way to know which RecordDecl is underneath. With modules, we don't use rootmap files. Thus we need to check if it's a typdef or not, and if it was a typedef,. make sure that it didn't take the same branch with the same interp. state (by checking (void*)-1) and it's not from modules. This improves cxxmodules res memory by 90MB. ```. before:. res memory = 345.816 Mbytes. after. res memory = 267.582 Mbytes. ```. Also, this fasten rootcling 2 times. ```. before:. User time (seconds): 2.56. after:. User time (seconds): 1.35. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2463
https://github.com/root-project/root/pull/2463:1170,usability,memor,memory,1170,"Reland ""[cxxmodules] Check correctly if the decl was cached.""; Improve the commit by tighten condition to take an expensive branch. In BaseSelectionRule::Match, for normal CXXRecordDecl, nothing need to be worried as. you can just compare name or pointer and that's it. However when you have typedef, it means that you can't just compare name (it's different!) or pointer (it's also different). And here, ScopeSearch is not actually searching the argument. It's just renewing the fCXXRecordDecl from fName to keep it up with the current interpreter state. Also, rootcling needed to parse header to get the actual definition of typedef, because rootmap file given to the invocation was lacking the necessary information. For example it contained `typedef typedefB`, from this there is no way to know which RecordDecl is underneath. With modules, we don't use rootmap files. Thus we need to check if it's a typdef or not, and if it was a typedef,. make sure that it didn't take the same branch with the same interp. state (by checking (void*)-1) and it's not from modules. This improves cxxmodules res memory by 90MB. ```. before:. res memory = 345.816 Mbytes. after. res memory = 267.582 Mbytes. ```. Also, this fasten rootcling 2 times. ```. before:. User time (seconds): 2.56. after:. User time (seconds): 1.35. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2463
https://github.com/root-project/root/pull/2463:1251,usability,User,User,1251,"Reland ""[cxxmodules] Check correctly if the decl was cached.""; Improve the commit by tighten condition to take an expensive branch. In BaseSelectionRule::Match, for normal CXXRecordDecl, nothing need to be worried as. you can just compare name or pointer and that's it. However when you have typedef, it means that you can't just compare name (it's different!) or pointer (it's also different). And here, ScopeSearch is not actually searching the argument. It's just renewing the fCXXRecordDecl from fName to keep it up with the current interpreter state. Also, rootcling needed to parse header to get the actual definition of typedef, because rootmap file given to the invocation was lacking the necessary information. For example it contained `typedef typedefB`, from this there is no way to know which RecordDecl is underneath. With modules, we don't use rootmap files. Thus we need to check if it's a typdef or not, and if it was a typedef,. make sure that it didn't take the same branch with the same interp. state (by checking (void*)-1) and it's not from modules. This improves cxxmodules res memory by 90MB. ```. before:. res memory = 345.816 Mbytes. after. res memory = 267.582 Mbytes. ```. Also, this fasten rootcling 2 times. ```. before:. User time (seconds): 2.56. after:. User time (seconds): 1.35. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2463
https://github.com/root-project/root/pull/2463:1286,usability,User,User,1286,"Reland ""[cxxmodules] Check correctly if the decl was cached.""; Improve the commit by tighten condition to take an expensive branch. In BaseSelectionRule::Match, for normal CXXRecordDecl, nothing need to be worried as. you can just compare name or pointer and that's it. However when you have typedef, it means that you can't just compare name (it's different!) or pointer (it's also different). And here, ScopeSearch is not actually searching the argument. It's just renewing the fCXXRecordDecl from fName to keep it up with the current interpreter state. Also, rootcling needed to parse header to get the actual definition of typedef, because rootmap file given to the invocation was lacking the necessary information. For example it contained `typedef typedefB`, from this there is no way to know which RecordDecl is underneath. With modules, we don't use rootmap files. Thus we need to check if it's a typdef or not, and if it was a typedef,. make sure that it didn't take the same branch with the same interp. state (by checking (void*)-1) and it's not from modules. This improves cxxmodules res memory by 90MB. ```. before:. res memory = 345.816 Mbytes. after. res memory = 267.582 Mbytes. ```. Also, this fasten rootcling 2 times. ```. before:. User time (seconds): 2.56. after:. User time (seconds): 1.35. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2463
https://github.com/root-project/root/pull/2464:193,deployability,modul,module,193,[cxxmodules] Save cpu time and memory by caching createFileID; prepareForParsing is where Cling creates temporary FileID in order to. perform a lookup. However this gives extra AST contents to module(pcm). and resulted in enormous pcm size and a startup time overhead. We can. cache FileID when the given code is same and return already existing. FileID instead of creating exactly the same one. ```. master. cpu time = 0.291462 seconds. sys time = 0.064409 seconds. res memory = 345.816 Mbytes. vir memory = 573.508 Mbytes. patch. cpu time = 0.235828 seconds. sys time = 0.098327 seconds. res memory = 260.012 Mbytes. vir memory = 377.945 Mbytes. ```,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2464
https://github.com/root-project/root/pull/2464:525,deployability,patch,patch,525,[cxxmodules] Save cpu time and memory by caching createFileID; prepareForParsing is where Cling creates temporary FileID in order to. perform a lookup. However this gives extra AST contents to module(pcm). and resulted in enormous pcm size and a startup time overhead. We can. cache FileID when the given code is same and return already existing. FileID instead of creating exactly the same one. ```. master. cpu time = 0.291462 seconds. sys time = 0.064409 seconds. res memory = 345.816 Mbytes. vir memory = 573.508 Mbytes. patch. cpu time = 0.235828 seconds. sys time = 0.098327 seconds. res memory = 260.012 Mbytes. vir memory = 377.945 Mbytes. ```,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2464
https://github.com/root-project/root/pull/2464:18,energy efficiency,cpu,cpu,18,[cxxmodules] Save cpu time and memory by caching createFileID; prepareForParsing is where Cling creates temporary FileID in order to. perform a lookup. However this gives extra AST contents to module(pcm). and resulted in enormous pcm size and a startup time overhead. We can. cache FileID when the given code is same and return already existing. FileID instead of creating exactly the same one. ```. master. cpu time = 0.291462 seconds. sys time = 0.064409 seconds. res memory = 345.816 Mbytes. vir memory = 573.508 Mbytes. patch. cpu time = 0.235828 seconds. sys time = 0.098327 seconds. res memory = 260.012 Mbytes. vir memory = 377.945 Mbytes. ```,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2464
https://github.com/root-project/root/pull/2464:409,energy efficiency,cpu,cpu,409,[cxxmodules] Save cpu time and memory by caching createFileID; prepareForParsing is where Cling creates temporary FileID in order to. perform a lookup. However this gives extra AST contents to module(pcm). and resulted in enormous pcm size and a startup time overhead. We can. cache FileID when the given code is same and return already existing. FileID instead of creating exactly the same one. ```. master. cpu time = 0.291462 seconds. sys time = 0.064409 seconds. res memory = 345.816 Mbytes. vir memory = 573.508 Mbytes. patch. cpu time = 0.235828 seconds. sys time = 0.098327 seconds. res memory = 260.012 Mbytes. vir memory = 377.945 Mbytes. ```,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2464
https://github.com/root-project/root/pull/2464:532,energy efficiency,cpu,cpu,532,[cxxmodules] Save cpu time and memory by caching createFileID; prepareForParsing is where Cling creates temporary FileID in order to. perform a lookup. However this gives extra AST contents to module(pcm). and resulted in enormous pcm size and a startup time overhead. We can. cache FileID when the given code is same and return already existing. FileID instead of creating exactly the same one. ```. master. cpu time = 0.291462 seconds. sys time = 0.064409 seconds. res memory = 345.816 Mbytes. vir memory = 573.508 Mbytes. patch. cpu time = 0.235828 seconds. sys time = 0.098327 seconds. res memory = 260.012 Mbytes. vir memory = 377.945 Mbytes. ```,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2464
https://github.com/root-project/root/pull/2464:193,modifiability,modul,module,193,[cxxmodules] Save cpu time and memory by caching createFileID; prepareForParsing is where Cling creates temporary FileID in order to. perform a lookup. However this gives extra AST contents to module(pcm). and resulted in enormous pcm size and a startup time overhead. We can. cache FileID when the given code is same and return already existing. FileID instead of creating exactly the same one. ```. master. cpu time = 0.291462 seconds. sys time = 0.064409 seconds. res memory = 345.816 Mbytes. vir memory = 573.508 Mbytes. patch. cpu time = 0.235828 seconds. sys time = 0.098327 seconds. res memory = 260.012 Mbytes. vir memory = 377.945 Mbytes. ```,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2464
https://github.com/root-project/root/pull/2464:18,performance,cpu,cpu,18,[cxxmodules] Save cpu time and memory by caching createFileID; prepareForParsing is where Cling creates temporary FileID in order to. perform a lookup. However this gives extra AST contents to module(pcm). and resulted in enormous pcm size and a startup time overhead. We can. cache FileID when the given code is same and return already existing. FileID instead of creating exactly the same one. ```. master. cpu time = 0.291462 seconds. sys time = 0.064409 seconds. res memory = 345.816 Mbytes. vir memory = 573.508 Mbytes. patch. cpu time = 0.235828 seconds. sys time = 0.098327 seconds. res memory = 260.012 Mbytes. vir memory = 377.945 Mbytes. ```,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2464
https://github.com/root-project/root/pull/2464:22,performance,time,time,22,[cxxmodules] Save cpu time and memory by caching createFileID; prepareForParsing is where Cling creates temporary FileID in order to. perform a lookup. However this gives extra AST contents to module(pcm). and resulted in enormous pcm size and a startup time overhead. We can. cache FileID when the given code is same and return already existing. FileID instead of creating exactly the same one. ```. master. cpu time = 0.291462 seconds. sys time = 0.064409 seconds. res memory = 345.816 Mbytes. vir memory = 573.508 Mbytes. patch. cpu time = 0.235828 seconds. sys time = 0.098327 seconds. res memory = 260.012 Mbytes. vir memory = 377.945 Mbytes. ```,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2464
https://github.com/root-project/root/pull/2464:31,performance,memor,memory,31,[cxxmodules] Save cpu time and memory by caching createFileID; prepareForParsing is where Cling creates temporary FileID in order to. perform a lookup. However this gives extra AST contents to module(pcm). and resulted in enormous pcm size and a startup time overhead. We can. cache FileID when the given code is same and return already existing. FileID instead of creating exactly the same one. ```. master. cpu time = 0.291462 seconds. sys time = 0.064409 seconds. res memory = 345.816 Mbytes. vir memory = 573.508 Mbytes. patch. cpu time = 0.235828 seconds. sys time = 0.098327 seconds. res memory = 260.012 Mbytes. vir memory = 377.945 Mbytes. ```,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2464
https://github.com/root-project/root/pull/2464:41,performance,cach,caching,41,[cxxmodules] Save cpu time and memory by caching createFileID; prepareForParsing is where Cling creates temporary FileID in order to. perform a lookup. However this gives extra AST contents to module(pcm). and resulted in enormous pcm size and a startup time overhead. We can. cache FileID when the given code is same and return already existing. FileID instead of creating exactly the same one. ```. master. cpu time = 0.291462 seconds. sys time = 0.064409 seconds. res memory = 345.816 Mbytes. vir memory = 573.508 Mbytes. patch. cpu time = 0.235828 seconds. sys time = 0.098327 seconds. res memory = 260.012 Mbytes. vir memory = 377.945 Mbytes. ```,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2464
https://github.com/root-project/root/pull/2464:134,performance,perform,perform,134,[cxxmodules] Save cpu time and memory by caching createFileID; prepareForParsing is where Cling creates temporary FileID in order to. perform a lookup. However this gives extra AST contents to module(pcm). and resulted in enormous pcm size and a startup time overhead. We can. cache FileID when the given code is same and return already existing. FileID instead of creating exactly the same one. ```. master. cpu time = 0.291462 seconds. sys time = 0.064409 seconds. res memory = 345.816 Mbytes. vir memory = 573.508 Mbytes. patch. cpu time = 0.235828 seconds. sys time = 0.098327 seconds. res memory = 260.012 Mbytes. vir memory = 377.945 Mbytes. ```,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2464
https://github.com/root-project/root/pull/2464:181,performance,content,contents,181,[cxxmodules] Save cpu time and memory by caching createFileID; prepareForParsing is where Cling creates temporary FileID in order to. perform a lookup. However this gives extra AST contents to module(pcm). and resulted in enormous pcm size and a startup time overhead. We can. cache FileID when the given code is same and return already existing. FileID instead of creating exactly the same one. ```. master. cpu time = 0.291462 seconds. sys time = 0.064409 seconds. res memory = 345.816 Mbytes. vir memory = 573.508 Mbytes. patch. cpu time = 0.235828 seconds. sys time = 0.098327 seconds. res memory = 260.012 Mbytes. vir memory = 377.945 Mbytes. ```,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2464
https://github.com/root-project/root/pull/2464:254,performance,time,time,254,[cxxmodules] Save cpu time and memory by caching createFileID; prepareForParsing is where Cling creates temporary FileID in order to. perform a lookup. However this gives extra AST contents to module(pcm). and resulted in enormous pcm size and a startup time overhead. We can. cache FileID when the given code is same and return already existing. FileID instead of creating exactly the same one. ```. master. cpu time = 0.291462 seconds. sys time = 0.064409 seconds. res memory = 345.816 Mbytes. vir memory = 573.508 Mbytes. patch. cpu time = 0.235828 seconds. sys time = 0.098327 seconds. res memory = 260.012 Mbytes. vir memory = 377.945 Mbytes. ```,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2464
https://github.com/root-project/root/pull/2464:259,performance,overhead,overhead,259,[cxxmodules] Save cpu time and memory by caching createFileID; prepareForParsing is where Cling creates temporary FileID in order to. perform a lookup. However this gives extra AST contents to module(pcm). and resulted in enormous pcm size and a startup time overhead. We can. cache FileID when the given code is same and return already existing. FileID instead of creating exactly the same one. ```. master. cpu time = 0.291462 seconds. sys time = 0.064409 seconds. res memory = 345.816 Mbytes. vir memory = 573.508 Mbytes. patch. cpu time = 0.235828 seconds. sys time = 0.098327 seconds. res memory = 260.012 Mbytes. vir memory = 377.945 Mbytes. ```,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2464
https://github.com/root-project/root/pull/2464:277,performance,cach,cache,277,[cxxmodules] Save cpu time and memory by caching createFileID; prepareForParsing is where Cling creates temporary FileID in order to. perform a lookup. However this gives extra AST contents to module(pcm). and resulted in enormous pcm size and a startup time overhead. We can. cache FileID when the given code is same and return already existing. FileID instead of creating exactly the same one. ```. master. cpu time = 0.291462 seconds. sys time = 0.064409 seconds. res memory = 345.816 Mbytes. vir memory = 573.508 Mbytes. patch. cpu time = 0.235828 seconds. sys time = 0.098327 seconds. res memory = 260.012 Mbytes. vir memory = 377.945 Mbytes. ```,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2464
https://github.com/root-project/root/pull/2464:409,performance,cpu,cpu,409,[cxxmodules] Save cpu time and memory by caching createFileID; prepareForParsing is where Cling creates temporary FileID in order to. perform a lookup. However this gives extra AST contents to module(pcm). and resulted in enormous pcm size and a startup time overhead. We can. cache FileID when the given code is same and return already existing. FileID instead of creating exactly the same one. ```. master. cpu time = 0.291462 seconds. sys time = 0.064409 seconds. res memory = 345.816 Mbytes. vir memory = 573.508 Mbytes. patch. cpu time = 0.235828 seconds. sys time = 0.098327 seconds. res memory = 260.012 Mbytes. vir memory = 377.945 Mbytes. ```,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2464
https://github.com/root-project/root/pull/2464:413,performance,time,time,413,[cxxmodules] Save cpu time and memory by caching createFileID; prepareForParsing is where Cling creates temporary FileID in order to. perform a lookup. However this gives extra AST contents to module(pcm). and resulted in enormous pcm size and a startup time overhead. We can. cache FileID when the given code is same and return already existing. FileID instead of creating exactly the same one. ```. master. cpu time = 0.291462 seconds. sys time = 0.064409 seconds. res memory = 345.816 Mbytes. vir memory = 573.508 Mbytes. patch. cpu time = 0.235828 seconds. sys time = 0.098327 seconds. res memory = 260.012 Mbytes. vir memory = 377.945 Mbytes. ```,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2464
https://github.com/root-project/root/pull/2464:442,performance,time,time,442,[cxxmodules] Save cpu time and memory by caching createFileID; prepareForParsing is where Cling creates temporary FileID in order to. perform a lookup. However this gives extra AST contents to module(pcm). and resulted in enormous pcm size and a startup time overhead. We can. cache FileID when the given code is same and return already existing. FileID instead of creating exactly the same one. ```. master. cpu time = 0.291462 seconds. sys time = 0.064409 seconds. res memory = 345.816 Mbytes. vir memory = 573.508 Mbytes. patch. cpu time = 0.235828 seconds. sys time = 0.098327 seconds. res memory = 260.012 Mbytes. vir memory = 377.945 Mbytes. ```,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2464
https://github.com/root-project/root/pull/2464:471,performance,memor,memory,471,[cxxmodules] Save cpu time and memory by caching createFileID; prepareForParsing is where Cling creates temporary FileID in order to. perform a lookup. However this gives extra AST contents to module(pcm). and resulted in enormous pcm size and a startup time overhead. We can. cache FileID when the given code is same and return already existing. FileID instead of creating exactly the same one. ```. master. cpu time = 0.291462 seconds. sys time = 0.064409 seconds. res memory = 345.816 Mbytes. vir memory = 573.508 Mbytes. patch. cpu time = 0.235828 seconds. sys time = 0.098327 seconds. res memory = 260.012 Mbytes. vir memory = 377.945 Mbytes. ```,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2464
https://github.com/root-project/root/pull/2464:500,performance,memor,memory,500,[cxxmodules] Save cpu time and memory by caching createFileID; prepareForParsing is where Cling creates temporary FileID in order to. perform a lookup. However this gives extra AST contents to module(pcm). and resulted in enormous pcm size and a startup time overhead. We can. cache FileID when the given code is same and return already existing. FileID instead of creating exactly the same one. ```. master. cpu time = 0.291462 seconds. sys time = 0.064409 seconds. res memory = 345.816 Mbytes. vir memory = 573.508 Mbytes. patch. cpu time = 0.235828 seconds. sys time = 0.098327 seconds. res memory = 260.012 Mbytes. vir memory = 377.945 Mbytes. ```,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2464
https://github.com/root-project/root/pull/2464:532,performance,cpu,cpu,532,[cxxmodules] Save cpu time and memory by caching createFileID; prepareForParsing is where Cling creates temporary FileID in order to. perform a lookup. However this gives extra AST contents to module(pcm). and resulted in enormous pcm size and a startup time overhead. We can. cache FileID when the given code is same and return already existing. FileID instead of creating exactly the same one. ```. master. cpu time = 0.291462 seconds. sys time = 0.064409 seconds. res memory = 345.816 Mbytes. vir memory = 573.508 Mbytes. patch. cpu time = 0.235828 seconds. sys time = 0.098327 seconds. res memory = 260.012 Mbytes. vir memory = 377.945 Mbytes. ```,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2464
https://github.com/root-project/root/pull/2464:536,performance,time,time,536,[cxxmodules] Save cpu time and memory by caching createFileID; prepareForParsing is where Cling creates temporary FileID in order to. perform a lookup. However this gives extra AST contents to module(pcm). and resulted in enormous pcm size and a startup time overhead. We can. cache FileID when the given code is same and return already existing. FileID instead of creating exactly the same one. ```. master. cpu time = 0.291462 seconds. sys time = 0.064409 seconds. res memory = 345.816 Mbytes. vir memory = 573.508 Mbytes. patch. cpu time = 0.235828 seconds. sys time = 0.098327 seconds. res memory = 260.012 Mbytes. vir memory = 377.945 Mbytes. ```,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2464
https://github.com/root-project/root/pull/2464:565,performance,time,time,565,[cxxmodules] Save cpu time and memory by caching createFileID; prepareForParsing is where Cling creates temporary FileID in order to. perform a lookup. However this gives extra AST contents to module(pcm). and resulted in enormous pcm size and a startup time overhead. We can. cache FileID when the given code is same and return already existing. FileID instead of creating exactly the same one. ```. master. cpu time = 0.291462 seconds. sys time = 0.064409 seconds. res memory = 345.816 Mbytes. vir memory = 573.508 Mbytes. patch. cpu time = 0.235828 seconds. sys time = 0.098327 seconds. res memory = 260.012 Mbytes. vir memory = 377.945 Mbytes. ```,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2464
https://github.com/root-project/root/pull/2464:594,performance,memor,memory,594,[cxxmodules] Save cpu time and memory by caching createFileID; prepareForParsing is where Cling creates temporary FileID in order to. perform a lookup. However this gives extra AST contents to module(pcm). and resulted in enormous pcm size and a startup time overhead. We can. cache FileID when the given code is same and return already existing. FileID instead of creating exactly the same one. ```. master. cpu time = 0.291462 seconds. sys time = 0.064409 seconds. res memory = 345.816 Mbytes. vir memory = 573.508 Mbytes. patch. cpu time = 0.235828 seconds. sys time = 0.098327 seconds. res memory = 260.012 Mbytes. vir memory = 377.945 Mbytes. ```,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2464
https://github.com/root-project/root/pull/2464:623,performance,memor,memory,623,[cxxmodules] Save cpu time and memory by caching createFileID; prepareForParsing is where Cling creates temporary FileID in order to. perform a lookup. However this gives extra AST contents to module(pcm). and resulted in enormous pcm size and a startup time overhead. We can. cache FileID when the given code is same and return already existing. FileID instead of creating exactly the same one. ```. master. cpu time = 0.291462 seconds. sys time = 0.064409 seconds. res memory = 345.816 Mbytes. vir memory = 573.508 Mbytes. patch. cpu time = 0.235828 seconds. sys time = 0.098327 seconds. res memory = 260.012 Mbytes. vir memory = 377.945 Mbytes. ```,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2464
https://github.com/root-project/root/pull/2464:193,safety,modul,module,193,[cxxmodules] Save cpu time and memory by caching createFileID; prepareForParsing is where Cling creates temporary FileID in order to. perform a lookup. However this gives extra AST contents to module(pcm). and resulted in enormous pcm size and a startup time overhead. We can. cache FileID when the given code is same and return already existing. FileID instead of creating exactly the same one. ```. master. cpu time = 0.291462 seconds. sys time = 0.064409 seconds. res memory = 345.816 Mbytes. vir memory = 573.508 Mbytes. patch. cpu time = 0.235828 seconds. sys time = 0.098327 seconds. res memory = 260.012 Mbytes. vir memory = 377.945 Mbytes. ```,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2464
https://github.com/root-project/root/pull/2464:525,safety,patch,patch,525,[cxxmodules] Save cpu time and memory by caching createFileID; prepareForParsing is where Cling creates temporary FileID in order to. perform a lookup. However this gives extra AST contents to module(pcm). and resulted in enormous pcm size and a startup time overhead. We can. cache FileID when the given code is same and return already existing. FileID instead of creating exactly the same one. ```. master. cpu time = 0.291462 seconds. sys time = 0.064409 seconds. res memory = 345.816 Mbytes. vir memory = 573.508 Mbytes. patch. cpu time = 0.235828 seconds. sys time = 0.098327 seconds. res memory = 260.012 Mbytes. vir memory = 377.945 Mbytes. ```,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2464
https://github.com/root-project/root/pull/2464:525,security,patch,patch,525,[cxxmodules] Save cpu time and memory by caching createFileID; prepareForParsing is where Cling creates temporary FileID in order to. perform a lookup. However this gives extra AST contents to module(pcm). and resulted in enormous pcm size and a startup time overhead. We can. cache FileID when the given code is same and return already existing. FileID instead of creating exactly the same one. ```. master. cpu time = 0.291462 seconds. sys time = 0.064409 seconds. res memory = 345.816 Mbytes. vir memory = 573.508 Mbytes. patch. cpu time = 0.235828 seconds. sys time = 0.098327 seconds. res memory = 260.012 Mbytes. vir memory = 377.945 Mbytes. ```,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2464
https://github.com/root-project/root/pull/2464:31,usability,memor,memory,31,[cxxmodules] Save cpu time and memory by caching createFileID; prepareForParsing is where Cling creates temporary FileID in order to. perform a lookup. However this gives extra AST contents to module(pcm). and resulted in enormous pcm size and a startup time overhead. We can. cache FileID when the given code is same and return already existing. FileID instead of creating exactly the same one. ```. master. cpu time = 0.291462 seconds. sys time = 0.064409 seconds. res memory = 345.816 Mbytes. vir memory = 573.508 Mbytes. patch. cpu time = 0.235828 seconds. sys time = 0.098327 seconds. res memory = 260.012 Mbytes. vir memory = 377.945 Mbytes. ```,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2464
https://github.com/root-project/root/pull/2464:134,usability,perform,perform,134,[cxxmodules] Save cpu time and memory by caching createFileID; prepareForParsing is where Cling creates temporary FileID in order to. perform a lookup. However this gives extra AST contents to module(pcm). and resulted in enormous pcm size and a startup time overhead. We can. cache FileID when the given code is same and return already existing. FileID instead of creating exactly the same one. ```. master. cpu time = 0.291462 seconds. sys time = 0.064409 seconds. res memory = 345.816 Mbytes. vir memory = 573.508 Mbytes. patch. cpu time = 0.235828 seconds. sys time = 0.098327 seconds. res memory = 260.012 Mbytes. vir memory = 377.945 Mbytes. ```,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2464
https://github.com/root-project/root/pull/2464:471,usability,memor,memory,471,[cxxmodules] Save cpu time and memory by caching createFileID; prepareForParsing is where Cling creates temporary FileID in order to. perform a lookup. However this gives extra AST contents to module(pcm). and resulted in enormous pcm size and a startup time overhead. We can. cache FileID when the given code is same and return already existing. FileID instead of creating exactly the same one. ```. master. cpu time = 0.291462 seconds. sys time = 0.064409 seconds. res memory = 345.816 Mbytes. vir memory = 573.508 Mbytes. patch. cpu time = 0.235828 seconds. sys time = 0.098327 seconds. res memory = 260.012 Mbytes. vir memory = 377.945 Mbytes. ```,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2464
https://github.com/root-project/root/pull/2464:500,usability,memor,memory,500,[cxxmodules] Save cpu time and memory by caching createFileID; prepareForParsing is where Cling creates temporary FileID in order to. perform a lookup. However this gives extra AST contents to module(pcm). and resulted in enormous pcm size and a startup time overhead. We can. cache FileID when the given code is same and return already existing. FileID instead of creating exactly the same one. ```. master. cpu time = 0.291462 seconds. sys time = 0.064409 seconds. res memory = 345.816 Mbytes. vir memory = 573.508 Mbytes. patch. cpu time = 0.235828 seconds. sys time = 0.098327 seconds. res memory = 260.012 Mbytes. vir memory = 377.945 Mbytes. ```,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2464
https://github.com/root-project/root/pull/2464:594,usability,memor,memory,594,[cxxmodules] Save cpu time and memory by caching createFileID; prepareForParsing is where Cling creates temporary FileID in order to. perform a lookup. However this gives extra AST contents to module(pcm). and resulted in enormous pcm size and a startup time overhead. We can. cache FileID when the given code is same and return already existing. FileID instead of creating exactly the same one. ```. master. cpu time = 0.291462 seconds. sys time = 0.064409 seconds. res memory = 345.816 Mbytes. vir memory = 573.508 Mbytes. patch. cpu time = 0.235828 seconds. sys time = 0.098327 seconds. res memory = 260.012 Mbytes. vir memory = 377.945 Mbytes. ```,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2464
https://github.com/root-project/root/pull/2464:623,usability,memor,memory,623,[cxxmodules] Save cpu time and memory by caching createFileID; prepareForParsing is where Cling creates temporary FileID in order to. perform a lookup. However this gives extra AST contents to module(pcm). and resulted in enormous pcm size and a startup time overhead. We can. cache FileID when the given code is same and return already existing. FileID instead of creating exactly the same one. ```. master. cpu time = 0.291462 seconds. sys time = 0.064409 seconds. res memory = 345.816 Mbytes. vir memory = 573.508 Mbytes. patch. cpu time = 0.235828 seconds. sys time = 0.098327 seconds. res memory = 260.012 Mbytes. vir memory = 377.945 Mbytes. ```,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2464
https://github.com/root-project/root/pull/2465:109,availability,state,state,109,[TMVA] Update documentation pdf; This updates the checked in version of the TMVA User's guide to reflect the state of the tex sources.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2465
https://github.com/root-project/root/pull/2465:7,deployability,Updat,Update,7,[TMVA] Update documentation pdf; This updates the checked in version of the TMVA User's guide to reflect the state of the tex sources.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2465
https://github.com/root-project/root/pull/2465:38,deployability,updat,updates,38,[TMVA] Update documentation pdf; This updates the checked in version of the TMVA User's guide to reflect the state of the tex sources.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2465
https://github.com/root-project/root/pull/2465:61,deployability,version,version,61,[TMVA] Update documentation pdf; This updates the checked in version of the TMVA User's guide to reflect the state of the tex sources.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2465
https://github.com/root-project/root/pull/2465:61,integrability,version,version,61,[TMVA] Update documentation pdf; This updates the checked in version of the TMVA User's guide to reflect the state of the tex sources.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2465
https://github.com/root-project/root/pull/2465:109,integrability,state,state,109,[TMVA] Update documentation pdf; This updates the checked in version of the TMVA User's guide to reflect the state of the tex sources.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2465
https://github.com/root-project/root/pull/2465:61,modifiability,version,version,61,[TMVA] Update documentation pdf; This updates the checked in version of the TMVA User's guide to reflect the state of the tex sources.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2465
https://github.com/root-project/root/pull/2465:7,safety,Updat,Update,7,[TMVA] Update documentation pdf; This updates the checked in version of the TMVA User's guide to reflect the state of the tex sources.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2465
https://github.com/root-project/root/pull/2465:38,safety,updat,updates,38,[TMVA] Update documentation pdf; This updates the checked in version of the TMVA User's guide to reflect the state of the tex sources.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2465
https://github.com/root-project/root/pull/2465:7,security,Updat,Update,7,[TMVA] Update documentation pdf; This updates the checked in version of the TMVA User's guide to reflect the state of the tex sources.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2465
https://github.com/root-project/root/pull/2465:38,security,updat,updates,38,[TMVA] Update documentation pdf; This updates the checked in version of the TMVA User's guide to reflect the state of the tex sources.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2465
https://github.com/root-project/root/pull/2465:14,usability,document,documentation,14,[TMVA] Update documentation pdf; This updates the checked in version of the TMVA User's guide to reflect the state of the tex sources.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2465
https://github.com/root-project/root/pull/2465:81,usability,User,User,81,[TMVA] Update documentation pdf; This updates the checked in version of the TMVA User's guide to reflect the state of the tex sources.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2465
https://github.com/root-project/root/pull/2465:88,usability,guid,guide,88,[TMVA] Update documentation pdf; This updates the checked in version of the TMVA User's guide to reflect the state of the tex sources.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2465
https://github.com/root-project/root/pull/2466:4,deployability,build,building,4,Fix building of clad and related test on Windows;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2466
https://github.com/root-project/root/pull/2466:33,safety,test,test,33,Fix building of clad and related test on Windows;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2466
https://github.com/root-project/root/pull/2466:33,testability,test,test,33,Fix building of clad and related test on Windows;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2466
https://github.com/root-project/root/pull/2467:374,usability,clear,clear,374,"[Experimental PyROOT] Restructure generic pythonizations; Restructure generic pythonization with the following idea:. 1. We would like to have a `_className.py` per class, where all pythonizations are added. However, for the very generic ones, a `_generic.py` file is added. 2. Rename the implementation of the pretty printing (generic) pythonization so that it's much more clear what you have to expect in the file. 3. ~~Rename the C++ side of the pythonization from `GenericPythonization` to `AddPrettyPrintingPyz` so that the python side of the pythonization is much more readable. This allows us to see all added pythonizations directly in the pythonization `_className.py` files.~~ We agreed on keeping teh `GenericPyz.cxx` filename of the source, although we want to rename the function `PythonizeGeneric` in a more meaningful way to `AddPrettyPrintingPyz`. That keeps the structure of the sources (one file per class) and enables us to see all pythonization for a class on the python side in the `_class.py` files.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2467
https://github.com/root-project/root/pull/2468:38,integrability,interfac,interface,38,"[Experimental PyROOT] Add numpy array interface to STL vector and RVec; Reimplemented the numpy array interface for the experimental pyroot. I've pushed the setup mostly to python, which makes the code much nicer compared to the implementation in the ""standard"" pyroot. This PR sits on top of #2467 (and therefore blocked by #2449).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2468
https://github.com/root-project/root/pull/2468:102,integrability,interfac,interface,102,"[Experimental PyROOT] Add numpy array interface to STL vector and RVec; Reimplemented the numpy array interface for the experimental pyroot. I've pushed the setup mostly to python, which makes the code much nicer compared to the implementation in the ""standard"" pyroot. This PR sits on top of #2467 (and therefore blocked by #2449).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2468
https://github.com/root-project/root/pull/2468:38,interoperability,interfac,interface,38,"[Experimental PyROOT] Add numpy array interface to STL vector and RVec; Reimplemented the numpy array interface for the experimental pyroot. I've pushed the setup mostly to python, which makes the code much nicer compared to the implementation in the ""standard"" pyroot. This PR sits on top of #2467 (and therefore blocked by #2449).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2468
https://github.com/root-project/root/pull/2468:102,interoperability,interfac,interface,102,"[Experimental PyROOT] Add numpy array interface to STL vector and RVec; Reimplemented the numpy array interface for the experimental pyroot. I've pushed the setup mostly to python, which makes the code much nicer compared to the implementation in the ""standard"" pyroot. This PR sits on top of #2467 (and therefore blocked by #2449).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2468
https://github.com/root-project/root/pull/2468:252,interoperability,standard,standard,252,"[Experimental PyROOT] Add numpy array interface to STL vector and RVec; Reimplemented the numpy array interface for the experimental pyroot. I've pushed the setup mostly to python, which makes the code much nicer compared to the implementation in the ""standard"" pyroot. This PR sits on top of #2467 (and therefore blocked by #2449).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2468
https://github.com/root-project/root/pull/2468:38,modifiability,interfac,interface,38,"[Experimental PyROOT] Add numpy array interface to STL vector and RVec; Reimplemented the numpy array interface for the experimental pyroot. I've pushed the setup mostly to python, which makes the code much nicer compared to the implementation in the ""standard"" pyroot. This PR sits on top of #2467 (and therefore blocked by #2449).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2468
https://github.com/root-project/root/pull/2468:102,modifiability,interfac,interface,102,"[Experimental PyROOT] Add numpy array interface to STL vector and RVec; Reimplemented the numpy array interface for the experimental pyroot. I've pushed the setup mostly to python, which makes the code much nicer compared to the implementation in the ""standard"" pyroot. This PR sits on top of #2467 (and therefore blocked by #2449).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2468
https://github.com/root-project/root/pull/2469:45,security,ssl,ssl,45,Add https support in TGHtmlBrowser (requires ssl);,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2469
https://github.com/root-project/root/pull/2469:10,usability,support,support,10,Add https support in TGHtmlBrowser (requires ssl);,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2469
https://github.com/root-project/root/pull/2470:134,energy efficiency,draw,drawing,134,"[DF] LoopManager tracks run actions; The LoopManager now tracks the actions that have already run. This commit is needed by the Graph drawing PR. This can't be directly tested as the LM is not exposed. To test it I used the following code, not commited. RInterface has a Test() method that just call GetAllActions() on the RLoopManager. ```. TEST_P(RDFSimpleTests, Action). {. RDataFrame rd1(8);. EXPECT_EQ(rd1.Test().size(), 0);. auto a1 = rd1.StdDev<ULong64_t>(""tdfentry_"");. EXPECT_EQ(rd1.Test().size(), 1);. auto a2 = rd1.Mean<>(""tdfentry_"");. EXPECT_EQ(rd1.Test().size(), 2);. auto a3 = rd1.Count();. EXPECT_EQ(rd1.Test().size(), 3);. *a1;. EXPECT_EQ(rd1.Test().size(), 3);. {. auto a4 = rd1.StdDev<ULong64_t>(""tdfentry_"");. EXPECT_EQ(rd1.Test().size(), 4);. }. EXPECT_EQ(rd1.Test().size(), 3);. {. auto a4 = rd1.StdDev<ULong64_t>(""tdfentry_"");. EXPECT_EQ(rd1.Test().size(), 4);. *a4;. }. EXPECT_EQ(rd1.Test().size(), 3);. }. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2470
https://github.com/root-project/root/pull/2470:169,safety,test,tested,169,"[DF] LoopManager tracks run actions; The LoopManager now tracks the actions that have already run. This commit is needed by the Graph drawing PR. This can't be directly tested as the LM is not exposed. To test it I used the following code, not commited. RInterface has a Test() method that just call GetAllActions() on the RLoopManager. ```. TEST_P(RDFSimpleTests, Action). {. RDataFrame rd1(8);. EXPECT_EQ(rd1.Test().size(), 0);. auto a1 = rd1.StdDev<ULong64_t>(""tdfentry_"");. EXPECT_EQ(rd1.Test().size(), 1);. auto a2 = rd1.Mean<>(""tdfentry_"");. EXPECT_EQ(rd1.Test().size(), 2);. auto a3 = rd1.Count();. EXPECT_EQ(rd1.Test().size(), 3);. *a1;. EXPECT_EQ(rd1.Test().size(), 3);. {. auto a4 = rd1.StdDev<ULong64_t>(""tdfentry_"");. EXPECT_EQ(rd1.Test().size(), 4);. }. EXPECT_EQ(rd1.Test().size(), 3);. {. auto a4 = rd1.StdDev<ULong64_t>(""tdfentry_"");. EXPECT_EQ(rd1.Test().size(), 4);. *a4;. }. EXPECT_EQ(rd1.Test().size(), 3);. }. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2470
https://github.com/root-project/root/pull/2470:205,safety,test,test,205,"[DF] LoopManager tracks run actions; The LoopManager now tracks the actions that have already run. This commit is needed by the Graph drawing PR. This can't be directly tested as the LM is not exposed. To test it I used the following code, not commited. RInterface has a Test() method that just call GetAllActions() on the RLoopManager. ```. TEST_P(RDFSimpleTests, Action). {. RDataFrame rd1(8);. EXPECT_EQ(rd1.Test().size(), 0);. auto a1 = rd1.StdDev<ULong64_t>(""tdfentry_"");. EXPECT_EQ(rd1.Test().size(), 1);. auto a2 = rd1.Mean<>(""tdfentry_"");. EXPECT_EQ(rd1.Test().size(), 2);. auto a3 = rd1.Count();. EXPECT_EQ(rd1.Test().size(), 3);. *a1;. EXPECT_EQ(rd1.Test().size(), 3);. {. auto a4 = rd1.StdDev<ULong64_t>(""tdfentry_"");. EXPECT_EQ(rd1.Test().size(), 4);. }. EXPECT_EQ(rd1.Test().size(), 3);. {. auto a4 = rd1.StdDev<ULong64_t>(""tdfentry_"");. EXPECT_EQ(rd1.Test().size(), 4);. *a4;. }. EXPECT_EQ(rd1.Test().size(), 3);. }. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2470
https://github.com/root-project/root/pull/2470:271,safety,Test,Test,271,"[DF] LoopManager tracks run actions; The LoopManager now tracks the actions that have already run. This commit is needed by the Graph drawing PR. This can't be directly tested as the LM is not exposed. To test it I used the following code, not commited. RInterface has a Test() method that just call GetAllActions() on the RLoopManager. ```. TEST_P(RDFSimpleTests, Action). {. RDataFrame rd1(8);. EXPECT_EQ(rd1.Test().size(), 0);. auto a1 = rd1.StdDev<ULong64_t>(""tdfentry_"");. EXPECT_EQ(rd1.Test().size(), 1);. auto a2 = rd1.Mean<>(""tdfentry_"");. EXPECT_EQ(rd1.Test().size(), 2);. auto a3 = rd1.Count();. EXPECT_EQ(rd1.Test().size(), 3);. *a1;. EXPECT_EQ(rd1.Test().size(), 3);. {. auto a4 = rd1.StdDev<ULong64_t>(""tdfentry_"");. EXPECT_EQ(rd1.Test().size(), 4);. }. EXPECT_EQ(rd1.Test().size(), 3);. {. auto a4 = rd1.StdDev<ULong64_t>(""tdfentry_"");. EXPECT_EQ(rd1.Test().size(), 4);. *a4;. }. EXPECT_EQ(rd1.Test().size(), 3);. }. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2470
https://github.com/root-project/root/pull/2470:411,safety,Test,Test,411,"[DF] LoopManager tracks run actions; The LoopManager now tracks the actions that have already run. This commit is needed by the Graph drawing PR. This can't be directly tested as the LM is not exposed. To test it I used the following code, not commited. RInterface has a Test() method that just call GetAllActions() on the RLoopManager. ```. TEST_P(RDFSimpleTests, Action). {. RDataFrame rd1(8);. EXPECT_EQ(rd1.Test().size(), 0);. auto a1 = rd1.StdDev<ULong64_t>(""tdfentry_"");. EXPECT_EQ(rd1.Test().size(), 1);. auto a2 = rd1.Mean<>(""tdfentry_"");. EXPECT_EQ(rd1.Test().size(), 2);. auto a3 = rd1.Count();. EXPECT_EQ(rd1.Test().size(), 3);. *a1;. EXPECT_EQ(rd1.Test().size(), 3);. {. auto a4 = rd1.StdDev<ULong64_t>(""tdfentry_"");. EXPECT_EQ(rd1.Test().size(), 4);. }. EXPECT_EQ(rd1.Test().size(), 3);. {. auto a4 = rd1.StdDev<ULong64_t>(""tdfentry_"");. EXPECT_EQ(rd1.Test().size(), 4);. *a4;. }. EXPECT_EQ(rd1.Test().size(), 3);. }. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2470
https://github.com/root-project/root/pull/2470:492,safety,Test,Test,492,"[DF] LoopManager tracks run actions; The LoopManager now tracks the actions that have already run. This commit is needed by the Graph drawing PR. This can't be directly tested as the LM is not exposed. To test it I used the following code, not commited. RInterface has a Test() method that just call GetAllActions() on the RLoopManager. ```. TEST_P(RDFSimpleTests, Action). {. RDataFrame rd1(8);. EXPECT_EQ(rd1.Test().size(), 0);. auto a1 = rd1.StdDev<ULong64_t>(""tdfentry_"");. EXPECT_EQ(rd1.Test().size(), 1);. auto a2 = rd1.Mean<>(""tdfentry_"");. EXPECT_EQ(rd1.Test().size(), 2);. auto a3 = rd1.Count();. EXPECT_EQ(rd1.Test().size(), 3);. *a1;. EXPECT_EQ(rd1.Test().size(), 3);. {. auto a4 = rd1.StdDev<ULong64_t>(""tdfentry_"");. EXPECT_EQ(rd1.Test().size(), 4);. }. EXPECT_EQ(rd1.Test().size(), 3);. {. auto a4 = rd1.StdDev<ULong64_t>(""tdfentry_"");. EXPECT_EQ(rd1.Test().size(), 4);. *a4;. }. EXPECT_EQ(rd1.Test().size(), 3);. }. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2470
https://github.com/root-project/root/pull/2470:562,safety,Test,Test,562,"[DF] LoopManager tracks run actions; The LoopManager now tracks the actions that have already run. This commit is needed by the Graph drawing PR. This can't be directly tested as the LM is not exposed. To test it I used the following code, not commited. RInterface has a Test() method that just call GetAllActions() on the RLoopManager. ```. TEST_P(RDFSimpleTests, Action). {. RDataFrame rd1(8);. EXPECT_EQ(rd1.Test().size(), 0);. auto a1 = rd1.StdDev<ULong64_t>(""tdfentry_"");. EXPECT_EQ(rd1.Test().size(), 1);. auto a2 = rd1.Mean<>(""tdfentry_"");. EXPECT_EQ(rd1.Test().size(), 2);. auto a3 = rd1.Count();. EXPECT_EQ(rd1.Test().size(), 3);. *a1;. EXPECT_EQ(rd1.Test().size(), 3);. {. auto a4 = rd1.StdDev<ULong64_t>(""tdfentry_"");. EXPECT_EQ(rd1.Test().size(), 4);. }. EXPECT_EQ(rd1.Test().size(), 3);. {. auto a4 = rd1.StdDev<ULong64_t>(""tdfentry_"");. EXPECT_EQ(rd1.Test().size(), 4);. *a4;. }. EXPECT_EQ(rd1.Test().size(), 3);. }. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2470
https://github.com/root-project/root/pull/2470:620,safety,Test,Test,620,"[DF] LoopManager tracks run actions; The LoopManager now tracks the actions that have already run. This commit is needed by the Graph drawing PR. This can't be directly tested as the LM is not exposed. To test it I used the following code, not commited. RInterface has a Test() method that just call GetAllActions() on the RLoopManager. ```. TEST_P(RDFSimpleTests, Action). {. RDataFrame rd1(8);. EXPECT_EQ(rd1.Test().size(), 0);. auto a1 = rd1.StdDev<ULong64_t>(""tdfentry_"");. EXPECT_EQ(rd1.Test().size(), 1);. auto a2 = rd1.Mean<>(""tdfentry_"");. EXPECT_EQ(rd1.Test().size(), 2);. auto a3 = rd1.Count();. EXPECT_EQ(rd1.Test().size(), 3);. *a1;. EXPECT_EQ(rd1.Test().size(), 3);. {. auto a4 = rd1.StdDev<ULong64_t>(""tdfentry_"");. EXPECT_EQ(rd1.Test().size(), 4);. }. EXPECT_EQ(rd1.Test().size(), 3);. {. auto a4 = rd1.StdDev<ULong64_t>(""tdfentry_"");. EXPECT_EQ(rd1.Test().size(), 4);. *a4;. }. EXPECT_EQ(rd1.Test().size(), 3);. }. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2470
https://github.com/root-project/root/pull/2470:660,safety,Test,Test,660,"[DF] LoopManager tracks run actions; The LoopManager now tracks the actions that have already run. This commit is needed by the Graph drawing PR. This can't be directly tested as the LM is not exposed. To test it I used the following code, not commited. RInterface has a Test() method that just call GetAllActions() on the RLoopManager. ```. TEST_P(RDFSimpleTests, Action). {. RDataFrame rd1(8);. EXPECT_EQ(rd1.Test().size(), 0);. auto a1 = rd1.StdDev<ULong64_t>(""tdfentry_"");. EXPECT_EQ(rd1.Test().size(), 1);. auto a2 = rd1.Mean<>(""tdfentry_"");. EXPECT_EQ(rd1.Test().size(), 2);. auto a3 = rd1.Count();. EXPECT_EQ(rd1.Test().size(), 3);. *a1;. EXPECT_EQ(rd1.Test().size(), 3);. {. auto a4 = rd1.StdDev<ULong64_t>(""tdfentry_"");. EXPECT_EQ(rd1.Test().size(), 4);. }. EXPECT_EQ(rd1.Test().size(), 3);. {. auto a4 = rd1.StdDev<ULong64_t>(""tdfentry_"");. EXPECT_EQ(rd1.Test().size(), 4);. *a4;. }. EXPECT_EQ(rd1.Test().size(), 3);. }. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2470
https://github.com/root-project/root/pull/2470:744,safety,Test,Test,744,"[DF] LoopManager tracks run actions; The LoopManager now tracks the actions that have already run. This commit is needed by the Graph drawing PR. This can't be directly tested as the LM is not exposed. To test it I used the following code, not commited. RInterface has a Test() method that just call GetAllActions() on the RLoopManager. ```. TEST_P(RDFSimpleTests, Action). {. RDataFrame rd1(8);. EXPECT_EQ(rd1.Test().size(), 0);. auto a1 = rd1.StdDev<ULong64_t>(""tdfentry_"");. EXPECT_EQ(rd1.Test().size(), 1);. auto a2 = rd1.Mean<>(""tdfentry_"");. EXPECT_EQ(rd1.Test().size(), 2);. auto a3 = rd1.Count();. EXPECT_EQ(rd1.Test().size(), 3);. *a1;. EXPECT_EQ(rd1.Test().size(), 3);. {. auto a4 = rd1.StdDev<ULong64_t>(""tdfentry_"");. EXPECT_EQ(rd1.Test().size(), 4);. }. EXPECT_EQ(rd1.Test().size(), 3);. {. auto a4 = rd1.StdDev<ULong64_t>(""tdfentry_"");. EXPECT_EQ(rd1.Test().size(), 4);. *a4;. }. EXPECT_EQ(rd1.Test().size(), 3);. }. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2470
https://github.com/root-project/root/pull/2470:781,safety,Test,Test,781,"[DF] LoopManager tracks run actions; The LoopManager now tracks the actions that have already run. This commit is needed by the Graph drawing PR. This can't be directly tested as the LM is not exposed. To test it I used the following code, not commited. RInterface has a Test() method that just call GetAllActions() on the RLoopManager. ```. TEST_P(RDFSimpleTests, Action). {. RDataFrame rd1(8);. EXPECT_EQ(rd1.Test().size(), 0);. auto a1 = rd1.StdDev<ULong64_t>(""tdfentry_"");. EXPECT_EQ(rd1.Test().size(), 1);. auto a2 = rd1.Mean<>(""tdfentry_"");. EXPECT_EQ(rd1.Test().size(), 2);. auto a3 = rd1.Count();. EXPECT_EQ(rd1.Test().size(), 3);. *a1;. EXPECT_EQ(rd1.Test().size(), 3);. {. auto a4 = rd1.StdDev<ULong64_t>(""tdfentry_"");. EXPECT_EQ(rd1.Test().size(), 4);. }. EXPECT_EQ(rd1.Test().size(), 3);. {. auto a4 = rd1.StdDev<ULong64_t>(""tdfentry_"");. EXPECT_EQ(rd1.Test().size(), 4);. *a4;. }. EXPECT_EQ(rd1.Test().size(), 3);. }. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2470
https://github.com/root-project/root/pull/2470:865,safety,Test,Test,865,"[DF] LoopManager tracks run actions; The LoopManager now tracks the actions that have already run. This commit is needed by the Graph drawing PR. This can't be directly tested as the LM is not exposed. To test it I used the following code, not commited. RInterface has a Test() method that just call GetAllActions() on the RLoopManager. ```. TEST_P(RDFSimpleTests, Action). {. RDataFrame rd1(8);. EXPECT_EQ(rd1.Test().size(), 0);. auto a1 = rd1.StdDev<ULong64_t>(""tdfentry_"");. EXPECT_EQ(rd1.Test().size(), 1);. auto a2 = rd1.Mean<>(""tdfentry_"");. EXPECT_EQ(rd1.Test().size(), 2);. auto a3 = rd1.Count();. EXPECT_EQ(rd1.Test().size(), 3);. *a1;. EXPECT_EQ(rd1.Test().size(), 3);. {. auto a4 = rd1.StdDev<ULong64_t>(""tdfentry_"");. EXPECT_EQ(rd1.Test().size(), 4);. }. EXPECT_EQ(rd1.Test().size(), 3);. {. auto a4 = rd1.StdDev<ULong64_t>(""tdfentry_"");. EXPECT_EQ(rd1.Test().size(), 4);. *a4;. }. EXPECT_EQ(rd1.Test().size(), 3);. }. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2470
https://github.com/root-project/root/pull/2470:908,safety,Test,Test,908,"[DF] LoopManager tracks run actions; The LoopManager now tracks the actions that have already run. This commit is needed by the Graph drawing PR. This can't be directly tested as the LM is not exposed. To test it I used the following code, not commited. RInterface has a Test() method that just call GetAllActions() on the RLoopManager. ```. TEST_P(RDFSimpleTests, Action). {. RDataFrame rd1(8);. EXPECT_EQ(rd1.Test().size(), 0);. auto a1 = rd1.StdDev<ULong64_t>(""tdfentry_"");. EXPECT_EQ(rd1.Test().size(), 1);. auto a2 = rd1.Mean<>(""tdfentry_"");. EXPECT_EQ(rd1.Test().size(), 2);. auto a3 = rd1.Count();. EXPECT_EQ(rd1.Test().size(), 3);. *a1;. EXPECT_EQ(rd1.Test().size(), 3);. {. auto a4 = rd1.StdDev<ULong64_t>(""tdfentry_"");. EXPECT_EQ(rd1.Test().size(), 4);. }. EXPECT_EQ(rd1.Test().size(), 3);. {. auto a4 = rd1.StdDev<ULong64_t>(""tdfentry_"");. EXPECT_EQ(rd1.Test().size(), 4);. *a4;. }. EXPECT_EQ(rd1.Test().size(), 3);. }. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2470
https://github.com/root-project/root/pull/2470:193,security,expos,exposed,193,"[DF] LoopManager tracks run actions; The LoopManager now tracks the actions that have already run. This commit is needed by the Graph drawing PR. This can't be directly tested as the LM is not exposed. To test it I used the following code, not commited. RInterface has a Test() method that just call GetAllActions() on the RLoopManager. ```. TEST_P(RDFSimpleTests, Action). {. RDataFrame rd1(8);. EXPECT_EQ(rd1.Test().size(), 0);. auto a1 = rd1.StdDev<ULong64_t>(""tdfentry_"");. EXPECT_EQ(rd1.Test().size(), 1);. auto a2 = rd1.Mean<>(""tdfentry_"");. EXPECT_EQ(rd1.Test().size(), 2);. auto a3 = rd1.Count();. EXPECT_EQ(rd1.Test().size(), 3);. *a1;. EXPECT_EQ(rd1.Test().size(), 3);. {. auto a4 = rd1.StdDev<ULong64_t>(""tdfentry_"");. EXPECT_EQ(rd1.Test().size(), 4);. }. EXPECT_EQ(rd1.Test().size(), 3);. {. auto a4 = rd1.StdDev<ULong64_t>(""tdfentry_"");. EXPECT_EQ(rd1.Test().size(), 4);. *a4;. }. EXPECT_EQ(rd1.Test().size(), 3);. }. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2470
https://github.com/root-project/root/pull/2470:169,testability,test,tested,169,"[DF] LoopManager tracks run actions; The LoopManager now tracks the actions that have already run. This commit is needed by the Graph drawing PR. This can't be directly tested as the LM is not exposed. To test it I used the following code, not commited. RInterface has a Test() method that just call GetAllActions() on the RLoopManager. ```. TEST_P(RDFSimpleTests, Action). {. RDataFrame rd1(8);. EXPECT_EQ(rd1.Test().size(), 0);. auto a1 = rd1.StdDev<ULong64_t>(""tdfentry_"");. EXPECT_EQ(rd1.Test().size(), 1);. auto a2 = rd1.Mean<>(""tdfentry_"");. EXPECT_EQ(rd1.Test().size(), 2);. auto a3 = rd1.Count();. EXPECT_EQ(rd1.Test().size(), 3);. *a1;. EXPECT_EQ(rd1.Test().size(), 3);. {. auto a4 = rd1.StdDev<ULong64_t>(""tdfentry_"");. EXPECT_EQ(rd1.Test().size(), 4);. }. EXPECT_EQ(rd1.Test().size(), 3);. {. auto a4 = rd1.StdDev<ULong64_t>(""tdfentry_"");. EXPECT_EQ(rd1.Test().size(), 4);. *a4;. }. EXPECT_EQ(rd1.Test().size(), 3);. }. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2470
https://github.com/root-project/root/pull/2470:205,testability,test,test,205,"[DF] LoopManager tracks run actions; The LoopManager now tracks the actions that have already run. This commit is needed by the Graph drawing PR. This can't be directly tested as the LM is not exposed. To test it I used the following code, not commited. RInterface has a Test() method that just call GetAllActions() on the RLoopManager. ```. TEST_P(RDFSimpleTests, Action). {. RDataFrame rd1(8);. EXPECT_EQ(rd1.Test().size(), 0);. auto a1 = rd1.StdDev<ULong64_t>(""tdfentry_"");. EXPECT_EQ(rd1.Test().size(), 1);. auto a2 = rd1.Mean<>(""tdfentry_"");. EXPECT_EQ(rd1.Test().size(), 2);. auto a3 = rd1.Count();. EXPECT_EQ(rd1.Test().size(), 3);. *a1;. EXPECT_EQ(rd1.Test().size(), 3);. {. auto a4 = rd1.StdDev<ULong64_t>(""tdfentry_"");. EXPECT_EQ(rd1.Test().size(), 4);. }. EXPECT_EQ(rd1.Test().size(), 3);. {. auto a4 = rd1.StdDev<ULong64_t>(""tdfentry_"");. EXPECT_EQ(rd1.Test().size(), 4);. *a4;. }. EXPECT_EQ(rd1.Test().size(), 3);. }. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2470
https://github.com/root-project/root/pull/2470:271,testability,Test,Test,271,"[DF] LoopManager tracks run actions; The LoopManager now tracks the actions that have already run. This commit is needed by the Graph drawing PR. This can't be directly tested as the LM is not exposed. To test it I used the following code, not commited. RInterface has a Test() method that just call GetAllActions() on the RLoopManager. ```. TEST_P(RDFSimpleTests, Action). {. RDataFrame rd1(8);. EXPECT_EQ(rd1.Test().size(), 0);. auto a1 = rd1.StdDev<ULong64_t>(""tdfentry_"");. EXPECT_EQ(rd1.Test().size(), 1);. auto a2 = rd1.Mean<>(""tdfentry_"");. EXPECT_EQ(rd1.Test().size(), 2);. auto a3 = rd1.Count();. EXPECT_EQ(rd1.Test().size(), 3);. *a1;. EXPECT_EQ(rd1.Test().size(), 3);. {. auto a4 = rd1.StdDev<ULong64_t>(""tdfentry_"");. EXPECT_EQ(rd1.Test().size(), 4);. }. EXPECT_EQ(rd1.Test().size(), 3);. {. auto a4 = rd1.StdDev<ULong64_t>(""tdfentry_"");. EXPECT_EQ(rd1.Test().size(), 4);. *a4;. }. EXPECT_EQ(rd1.Test().size(), 3);. }. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2470
https://github.com/root-project/root/pull/2470:411,testability,Test,Test,411,"[DF] LoopManager tracks run actions; The LoopManager now tracks the actions that have already run. This commit is needed by the Graph drawing PR. This can't be directly tested as the LM is not exposed. To test it I used the following code, not commited. RInterface has a Test() method that just call GetAllActions() on the RLoopManager. ```. TEST_P(RDFSimpleTests, Action). {. RDataFrame rd1(8);. EXPECT_EQ(rd1.Test().size(), 0);. auto a1 = rd1.StdDev<ULong64_t>(""tdfentry_"");. EXPECT_EQ(rd1.Test().size(), 1);. auto a2 = rd1.Mean<>(""tdfentry_"");. EXPECT_EQ(rd1.Test().size(), 2);. auto a3 = rd1.Count();. EXPECT_EQ(rd1.Test().size(), 3);. *a1;. EXPECT_EQ(rd1.Test().size(), 3);. {. auto a4 = rd1.StdDev<ULong64_t>(""tdfentry_"");. EXPECT_EQ(rd1.Test().size(), 4);. }. EXPECT_EQ(rd1.Test().size(), 3);. {. auto a4 = rd1.StdDev<ULong64_t>(""tdfentry_"");. EXPECT_EQ(rd1.Test().size(), 4);. *a4;. }. EXPECT_EQ(rd1.Test().size(), 3);. }. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2470
https://github.com/root-project/root/pull/2470:492,testability,Test,Test,492,"[DF] LoopManager tracks run actions; The LoopManager now tracks the actions that have already run. This commit is needed by the Graph drawing PR. This can't be directly tested as the LM is not exposed. To test it I used the following code, not commited. RInterface has a Test() method that just call GetAllActions() on the RLoopManager. ```. TEST_P(RDFSimpleTests, Action). {. RDataFrame rd1(8);. EXPECT_EQ(rd1.Test().size(), 0);. auto a1 = rd1.StdDev<ULong64_t>(""tdfentry_"");. EXPECT_EQ(rd1.Test().size(), 1);. auto a2 = rd1.Mean<>(""tdfentry_"");. EXPECT_EQ(rd1.Test().size(), 2);. auto a3 = rd1.Count();. EXPECT_EQ(rd1.Test().size(), 3);. *a1;. EXPECT_EQ(rd1.Test().size(), 3);. {. auto a4 = rd1.StdDev<ULong64_t>(""tdfentry_"");. EXPECT_EQ(rd1.Test().size(), 4);. }. EXPECT_EQ(rd1.Test().size(), 3);. {. auto a4 = rd1.StdDev<ULong64_t>(""tdfentry_"");. EXPECT_EQ(rd1.Test().size(), 4);. *a4;. }. EXPECT_EQ(rd1.Test().size(), 3);. }. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2470
https://github.com/root-project/root/pull/2470:562,testability,Test,Test,562,"[DF] LoopManager tracks run actions; The LoopManager now tracks the actions that have already run. This commit is needed by the Graph drawing PR. This can't be directly tested as the LM is not exposed. To test it I used the following code, not commited. RInterface has a Test() method that just call GetAllActions() on the RLoopManager. ```. TEST_P(RDFSimpleTests, Action). {. RDataFrame rd1(8);. EXPECT_EQ(rd1.Test().size(), 0);. auto a1 = rd1.StdDev<ULong64_t>(""tdfentry_"");. EXPECT_EQ(rd1.Test().size(), 1);. auto a2 = rd1.Mean<>(""tdfentry_"");. EXPECT_EQ(rd1.Test().size(), 2);. auto a3 = rd1.Count();. EXPECT_EQ(rd1.Test().size(), 3);. *a1;. EXPECT_EQ(rd1.Test().size(), 3);. {. auto a4 = rd1.StdDev<ULong64_t>(""tdfentry_"");. EXPECT_EQ(rd1.Test().size(), 4);. }. EXPECT_EQ(rd1.Test().size(), 3);. {. auto a4 = rd1.StdDev<ULong64_t>(""tdfentry_"");. EXPECT_EQ(rd1.Test().size(), 4);. *a4;. }. EXPECT_EQ(rd1.Test().size(), 3);. }. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2470
https://github.com/root-project/root/pull/2470:620,testability,Test,Test,620,"[DF] LoopManager tracks run actions; The LoopManager now tracks the actions that have already run. This commit is needed by the Graph drawing PR. This can't be directly tested as the LM is not exposed. To test it I used the following code, not commited. RInterface has a Test() method that just call GetAllActions() on the RLoopManager. ```. TEST_P(RDFSimpleTests, Action). {. RDataFrame rd1(8);. EXPECT_EQ(rd1.Test().size(), 0);. auto a1 = rd1.StdDev<ULong64_t>(""tdfentry_"");. EXPECT_EQ(rd1.Test().size(), 1);. auto a2 = rd1.Mean<>(""tdfentry_"");. EXPECT_EQ(rd1.Test().size(), 2);. auto a3 = rd1.Count();. EXPECT_EQ(rd1.Test().size(), 3);. *a1;. EXPECT_EQ(rd1.Test().size(), 3);. {. auto a4 = rd1.StdDev<ULong64_t>(""tdfentry_"");. EXPECT_EQ(rd1.Test().size(), 4);. }. EXPECT_EQ(rd1.Test().size(), 3);. {. auto a4 = rd1.StdDev<ULong64_t>(""tdfentry_"");. EXPECT_EQ(rd1.Test().size(), 4);. *a4;. }. EXPECT_EQ(rd1.Test().size(), 3);. }. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2470
https://github.com/root-project/root/pull/2470:660,testability,Test,Test,660,"[DF] LoopManager tracks run actions; The LoopManager now tracks the actions that have already run. This commit is needed by the Graph drawing PR. This can't be directly tested as the LM is not exposed. To test it I used the following code, not commited. RInterface has a Test() method that just call GetAllActions() on the RLoopManager. ```. TEST_P(RDFSimpleTests, Action). {. RDataFrame rd1(8);. EXPECT_EQ(rd1.Test().size(), 0);. auto a1 = rd1.StdDev<ULong64_t>(""tdfentry_"");. EXPECT_EQ(rd1.Test().size(), 1);. auto a2 = rd1.Mean<>(""tdfentry_"");. EXPECT_EQ(rd1.Test().size(), 2);. auto a3 = rd1.Count();. EXPECT_EQ(rd1.Test().size(), 3);. *a1;. EXPECT_EQ(rd1.Test().size(), 3);. {. auto a4 = rd1.StdDev<ULong64_t>(""tdfentry_"");. EXPECT_EQ(rd1.Test().size(), 4);. }. EXPECT_EQ(rd1.Test().size(), 3);. {. auto a4 = rd1.StdDev<ULong64_t>(""tdfentry_"");. EXPECT_EQ(rd1.Test().size(), 4);. *a4;. }. EXPECT_EQ(rd1.Test().size(), 3);. }. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2470
https://github.com/root-project/root/pull/2470:744,testability,Test,Test,744,"[DF] LoopManager tracks run actions; The LoopManager now tracks the actions that have already run. This commit is needed by the Graph drawing PR. This can't be directly tested as the LM is not exposed. To test it I used the following code, not commited. RInterface has a Test() method that just call GetAllActions() on the RLoopManager. ```. TEST_P(RDFSimpleTests, Action). {. RDataFrame rd1(8);. EXPECT_EQ(rd1.Test().size(), 0);. auto a1 = rd1.StdDev<ULong64_t>(""tdfentry_"");. EXPECT_EQ(rd1.Test().size(), 1);. auto a2 = rd1.Mean<>(""tdfentry_"");. EXPECT_EQ(rd1.Test().size(), 2);. auto a3 = rd1.Count();. EXPECT_EQ(rd1.Test().size(), 3);. *a1;. EXPECT_EQ(rd1.Test().size(), 3);. {. auto a4 = rd1.StdDev<ULong64_t>(""tdfentry_"");. EXPECT_EQ(rd1.Test().size(), 4);. }. EXPECT_EQ(rd1.Test().size(), 3);. {. auto a4 = rd1.StdDev<ULong64_t>(""tdfentry_"");. EXPECT_EQ(rd1.Test().size(), 4);. *a4;. }. EXPECT_EQ(rd1.Test().size(), 3);. }. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2470
https://github.com/root-project/root/pull/2470:781,testability,Test,Test,781,"[DF] LoopManager tracks run actions; The LoopManager now tracks the actions that have already run. This commit is needed by the Graph drawing PR. This can't be directly tested as the LM is not exposed. To test it I used the following code, not commited. RInterface has a Test() method that just call GetAllActions() on the RLoopManager. ```. TEST_P(RDFSimpleTests, Action). {. RDataFrame rd1(8);. EXPECT_EQ(rd1.Test().size(), 0);. auto a1 = rd1.StdDev<ULong64_t>(""tdfentry_"");. EXPECT_EQ(rd1.Test().size(), 1);. auto a2 = rd1.Mean<>(""tdfentry_"");. EXPECT_EQ(rd1.Test().size(), 2);. auto a3 = rd1.Count();. EXPECT_EQ(rd1.Test().size(), 3);. *a1;. EXPECT_EQ(rd1.Test().size(), 3);. {. auto a4 = rd1.StdDev<ULong64_t>(""tdfentry_"");. EXPECT_EQ(rd1.Test().size(), 4);. }. EXPECT_EQ(rd1.Test().size(), 3);. {. auto a4 = rd1.StdDev<ULong64_t>(""tdfentry_"");. EXPECT_EQ(rd1.Test().size(), 4);. *a4;. }. EXPECT_EQ(rd1.Test().size(), 3);. }. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2470
https://github.com/root-project/root/pull/2470:865,testability,Test,Test,865,"[DF] LoopManager tracks run actions; The LoopManager now tracks the actions that have already run. This commit is needed by the Graph drawing PR. This can't be directly tested as the LM is not exposed. To test it I used the following code, not commited. RInterface has a Test() method that just call GetAllActions() on the RLoopManager. ```. TEST_P(RDFSimpleTests, Action). {. RDataFrame rd1(8);. EXPECT_EQ(rd1.Test().size(), 0);. auto a1 = rd1.StdDev<ULong64_t>(""tdfentry_"");. EXPECT_EQ(rd1.Test().size(), 1);. auto a2 = rd1.Mean<>(""tdfentry_"");. EXPECT_EQ(rd1.Test().size(), 2);. auto a3 = rd1.Count();. EXPECT_EQ(rd1.Test().size(), 3);. *a1;. EXPECT_EQ(rd1.Test().size(), 3);. {. auto a4 = rd1.StdDev<ULong64_t>(""tdfentry_"");. EXPECT_EQ(rd1.Test().size(), 4);. }. EXPECT_EQ(rd1.Test().size(), 3);. {. auto a4 = rd1.StdDev<ULong64_t>(""tdfentry_"");. EXPECT_EQ(rd1.Test().size(), 4);. *a4;. }. EXPECT_EQ(rd1.Test().size(), 3);. }. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2470
https://github.com/root-project/root/pull/2470:908,testability,Test,Test,908,"[DF] LoopManager tracks run actions; The LoopManager now tracks the actions that have already run. This commit is needed by the Graph drawing PR. This can't be directly tested as the LM is not exposed. To test it I used the following code, not commited. RInterface has a Test() method that just call GetAllActions() on the RLoopManager. ```. TEST_P(RDFSimpleTests, Action). {. RDataFrame rd1(8);. EXPECT_EQ(rd1.Test().size(), 0);. auto a1 = rd1.StdDev<ULong64_t>(""tdfentry_"");. EXPECT_EQ(rd1.Test().size(), 1);. auto a2 = rd1.Mean<>(""tdfentry_"");. EXPECT_EQ(rd1.Test().size(), 2);. auto a3 = rd1.Count();. EXPECT_EQ(rd1.Test().size(), 3);. *a1;. EXPECT_EQ(rd1.Test().size(), 3);. {. auto a4 = rd1.StdDev<ULong64_t>(""tdfentry_"");. EXPECT_EQ(rd1.Test().size(), 4);. }. EXPECT_EQ(rd1.Test().size(), 3);. {. auto a4 = rd1.StdDev<ULong64_t>(""tdfentry_"");. EXPECT_EQ(rd1.Test().size(), 4);. *a4;. }. EXPECT_EQ(rd1.Test().size(), 3);. }. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2470
https://github.com/root-project/root/pull/2472:232,deployability,depend,depend,232,"xml: fix gcc8 warning with strncpy arguments; At this place buffer was allocated using length of source string. gcc does not like when strncpy length argument directly derives from. source string length. Actually, buffer should not depend from source. length - here it was workaround for old problem, which is already fixed. Therefore just use constant buffer length",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2472
https://github.com/root-project/root/pull/2472:71,energy efficiency,alloc,allocated,71,"xml: fix gcc8 warning with strncpy arguments; At this place buffer was allocated using length of source string. gcc does not like when strncpy length argument directly derives from. source string length. Actually, buffer should not depend from source. length - here it was workaround for old problem, which is already fixed. Therefore just use constant buffer length",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2472
https://github.com/root-project/root/pull/2472:60,integrability,buffer,buffer,60,"xml: fix gcc8 warning with strncpy arguments; At this place buffer was allocated using length of source string. gcc does not like when strncpy length argument directly derives from. source string length. Actually, buffer should not depend from source. length - here it was workaround for old problem, which is already fixed. Therefore just use constant buffer length",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2472
https://github.com/root-project/root/pull/2472:214,integrability,buffer,buffer,214,"xml: fix gcc8 warning with strncpy arguments; At this place buffer was allocated using length of source string. gcc does not like when strncpy length argument directly derives from. source string length. Actually, buffer should not depend from source. length - here it was workaround for old problem, which is already fixed. Therefore just use constant buffer length",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2472
https://github.com/root-project/root/pull/2472:232,integrability,depend,depend,232,"xml: fix gcc8 warning with strncpy arguments; At this place buffer was allocated using length of source string. gcc does not like when strncpy length argument directly derives from. source string length. Actually, buffer should not depend from source. length - here it was workaround for old problem, which is already fixed. Therefore just use constant buffer length",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2472
https://github.com/root-project/root/pull/2472:353,integrability,buffer,buffer,353,"xml: fix gcc8 warning with strncpy arguments; At this place buffer was allocated using length of source string. gcc does not like when strncpy length argument directly derives from. source string length. Actually, buffer should not depend from source. length - here it was workaround for old problem, which is already fixed. Therefore just use constant buffer length",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2472
https://github.com/root-project/root/pull/2472:0,interoperability,xml,xml,0,"xml: fix gcc8 warning with strncpy arguments; At this place buffer was allocated using length of source string. gcc does not like when strncpy length argument directly derives from. source string length. Actually, buffer should not depend from source. length - here it was workaround for old problem, which is already fixed. Therefore just use constant buffer length",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2472
https://github.com/root-project/root/pull/2472:232,modifiability,depend,depend,232,"xml: fix gcc8 warning with strncpy arguments; At this place buffer was allocated using length of source string. gcc does not like when strncpy length argument directly derives from. source string length. Actually, buffer should not depend from source. length - here it was workaround for old problem, which is already fixed. Therefore just use constant buffer length",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2472
https://github.com/root-project/root/pull/2472:116,reliability,doe,does,116,"xml: fix gcc8 warning with strncpy arguments; At this place buffer was allocated using length of source string. gcc does not like when strncpy length argument directly derives from. source string length. Actually, buffer should not depend from source. length - here it was workaround for old problem, which is already fixed. Therefore just use constant buffer length",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2472
https://github.com/root-project/root/pull/2472:232,safety,depend,depend,232,"xml: fix gcc8 warning with strncpy arguments; At this place buffer was allocated using length of source string. gcc does not like when strncpy length argument directly derives from. source string length. Actually, buffer should not depend from source. length - here it was workaround for old problem, which is already fixed. Therefore just use constant buffer length",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2472
https://github.com/root-project/root/pull/2472:232,testability,depend,depend,232,"xml: fix gcc8 warning with strncpy arguments; At this place buffer was allocated using length of source string. gcc does not like when strncpy length argument directly derives from. source string length. Actually, buffer should not depend from source. length - here it was workaround for old problem, which is already fixed. Therefore just use constant buffer length",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2472
https://github.com/root-project/root/pull/2473:95,usability,help,helpers,95,"[Experimental PyROOT] Rename sources to follow convention of one file…; … per class, introduce helpers for python-side pythonizations. As discussed with @etejedor, we want to follow the rule to have only one source file per pythonized class. However, for python-side pythonization we need helpers (on the C++ side), which are now placed in the `Helpers.cxx` source file.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2473
https://github.com/root-project/root/pull/2473:289,usability,help,helpers,289,"[Experimental PyROOT] Rename sources to follow convention of one file…; … per class, introduce helpers for python-side pythonizations. As discussed with @etejedor, we want to follow the rule to have only one source file per pythonized class. However, for python-side pythonization we need helpers (on the C++ side), which are now placed in the `Helpers.cxx` source file.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2473
https://github.com/root-project/root/pull/2473:345,usability,Help,Helpers,345,"[Experimental PyROOT] Rename sources to follow convention of one file…; … per class, introduce helpers for python-side pythonizations. As discussed with @etejedor, we want to follow the rule to have only one source file per pythonized class. However, for python-side pythonization we need helpers (on the C++ side), which are now placed in the `Helpers.cxx` source file.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2473
https://github.com/root-project/root/pull/2474:78,energy efficiency,Core,Core,78,"Transparent migration of Sort and BinarySearch algorithms to MathBase header (Core); It will make ROOT I/O [RIO library] independent from MathCore, which can easily allow to separate ROOT minimal part (Core with interpreter + I/O).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2474
https://github.com/root-project/root/pull/2474:202,energy efficiency,Core,Core,202,"Transparent migration of Sort and BinarySearch algorithms to MathBase header (Core); It will make ROOT I/O [RIO library] independent from MathCore, which can easily allow to separate ROOT minimal part (Core with interpreter + I/O).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2474
https://github.com/root-project/root/pull/2474:103,performance,I/O,I/O,103,"Transparent migration of Sort and BinarySearch algorithms to MathBase header (Core); It will make ROOT I/O [RIO library] independent from MathCore, which can easily allow to separate ROOT minimal part (Core with interpreter + I/O).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2474
https://github.com/root-project/root/pull/2474:226,performance,I/O,I/O,226,"Transparent migration of Sort and BinarySearch algorithms to MathBase header (Core); It will make ROOT I/O [RIO library] independent from MathCore, which can easily allow to separate ROOT minimal part (Core with interpreter + I/O).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2474
https://github.com/root-project/root/pull/2474:188,usability,minim,minimal,188,"Transparent migration of Sort and BinarySearch algorithms to MathBase header (Core); It will make ROOT I/O [RIO library] independent from MathCore, which can easily allow to separate ROOT minimal part (Core with interpreter + I/O).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2474
https://github.com/root-project/root/pull/2475:191,energy efficiency,GPU,GPUs,191,"added cuda stream support to parallelize over events in the batch; Now using CUDA streams to support parallelization over events in the batch. This yields a 40% performance boost on high end GPUs (GTX 1080 Ti) and 10-15% on weaker models (Quadro 1000M). Additionally, temporary matrices used by `Im2Col` in the forward pass are maintained as a `ConvLayer` data field in order to minimize cuda memory allocations.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2475
https://github.com/root-project/root/pull/2475:231,energy efficiency,model,models,231,"added cuda stream support to parallelize over events in the batch; Now using CUDA streams to support parallelization over events in the batch. This yields a 40% performance boost on high end GPUs (GTX 1080 Ti) and 10-15% on weaker models (Quadro 1000M). Additionally, temporary matrices used by `Im2Col` in the forward pass are maintained as a `ConvLayer` data field in order to minimize cuda memory allocations.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2475
https://github.com/root-project/root/pull/2475:400,energy efficiency,alloc,allocations,400,"added cuda stream support to parallelize over events in the batch; Now using CUDA streams to support parallelization over events in the batch. This yields a 40% performance boost on high end GPUs (GTX 1080 Ti) and 10-15% on weaker models (Quadro 1000M). Additionally, temporary matrices used by `Im2Col` in the forward pass are maintained as a `ConvLayer` data field in order to minimize cuda memory allocations.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2475
https://github.com/root-project/root/pull/2475:46,integrability,event,events,46,"added cuda stream support to parallelize over events in the batch; Now using CUDA streams to support parallelization over events in the batch. This yields a 40% performance boost on high end GPUs (GTX 1080 Ti) and 10-15% on weaker models (Quadro 1000M). Additionally, temporary matrices used by `Im2Col` in the forward pass are maintained as a `ConvLayer` data field in order to minimize cuda memory allocations.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2475
https://github.com/root-project/root/pull/2475:60,integrability,batch,batch,60,"added cuda stream support to parallelize over events in the batch; Now using CUDA streams to support parallelization over events in the batch. This yields a 40% performance boost on high end GPUs (GTX 1080 Ti) and 10-15% on weaker models (Quadro 1000M). Additionally, temporary matrices used by `Im2Col` in the forward pass are maintained as a `ConvLayer` data field in order to minimize cuda memory allocations.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2475
https://github.com/root-project/root/pull/2475:122,integrability,event,events,122,"added cuda stream support to parallelize over events in the batch; Now using CUDA streams to support parallelization over events in the batch. This yields a 40% performance boost on high end GPUs (GTX 1080 Ti) and 10-15% on weaker models (Quadro 1000M). Additionally, temporary matrices used by `Im2Col` in the forward pass are maintained as a `ConvLayer` data field in order to minimize cuda memory allocations.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2475
https://github.com/root-project/root/pull/2475:136,integrability,batch,batch,136,"added cuda stream support to parallelize over events in the batch; Now using CUDA streams to support parallelization over events in the batch. This yields a 40% performance boost on high end GPUs (GTX 1080 Ti) and 10-15% on weaker models (Quadro 1000M). Additionally, temporary matrices used by `Im2Col` in the forward pass are maintained as a `ConvLayer` data field in order to minimize cuda memory allocations.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2475
https://github.com/root-project/root/pull/2475:328,modifiability,maintain,maintained,328,"added cuda stream support to parallelize over events in the batch; Now using CUDA streams to support parallelization over events in the batch. This yields a 40% performance boost on high end GPUs (GTX 1080 Ti) and 10-15% on weaker models (Quadro 1000M). Additionally, temporary matrices used by `Im2Col` in the forward pass are maintained as a `ConvLayer` data field in order to minimize cuda memory allocations.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2475
https://github.com/root-project/root/pull/2475:29,performance,parallel,parallelize,29,"added cuda stream support to parallelize over events in the batch; Now using CUDA streams to support parallelization over events in the batch. This yields a 40% performance boost on high end GPUs (GTX 1080 Ti) and 10-15% on weaker models (Quadro 1000M). Additionally, temporary matrices used by `Im2Col` in the forward pass are maintained as a `ConvLayer` data field in order to minimize cuda memory allocations.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2475
https://github.com/root-project/root/pull/2475:60,performance,batch,batch,60,"added cuda stream support to parallelize over events in the batch; Now using CUDA streams to support parallelization over events in the batch. This yields a 40% performance boost on high end GPUs (GTX 1080 Ti) and 10-15% on weaker models (Quadro 1000M). Additionally, temporary matrices used by `Im2Col` in the forward pass are maintained as a `ConvLayer` data field in order to minimize cuda memory allocations.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2475
https://github.com/root-project/root/pull/2475:101,performance,parallel,parallelization,101,"added cuda stream support to parallelize over events in the batch; Now using CUDA streams to support parallelization over events in the batch. This yields a 40% performance boost on high end GPUs (GTX 1080 Ti) and 10-15% on weaker models (Quadro 1000M). Additionally, temporary matrices used by `Im2Col` in the forward pass are maintained as a `ConvLayer` data field in order to minimize cuda memory allocations.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2475
https://github.com/root-project/root/pull/2475:136,performance,batch,batch,136,"added cuda stream support to parallelize over events in the batch; Now using CUDA streams to support parallelization over events in the batch. This yields a 40% performance boost on high end GPUs (GTX 1080 Ti) and 10-15% on weaker models (Quadro 1000M). Additionally, temporary matrices used by `Im2Col` in the forward pass are maintained as a `ConvLayer` data field in order to minimize cuda memory allocations.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2475
https://github.com/root-project/root/pull/2475:161,performance,perform,performance,161,"added cuda stream support to parallelize over events in the batch; Now using CUDA streams to support parallelization over events in the batch. This yields a 40% performance boost on high end GPUs (GTX 1080 Ti) and 10-15% on weaker models (Quadro 1000M). Additionally, temporary matrices used by `Im2Col` in the forward pass are maintained as a `ConvLayer` data field in order to minimize cuda memory allocations.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2475
https://github.com/root-project/root/pull/2475:191,performance,GPU,GPUs,191,"added cuda stream support to parallelize over events in the batch; Now using CUDA streams to support parallelization over events in the batch. This yields a 40% performance boost on high end GPUs (GTX 1080 Ti) and 10-15% on weaker models (Quadro 1000M). Additionally, temporary matrices used by `Im2Col` in the forward pass are maintained as a `ConvLayer` data field in order to minimize cuda memory allocations.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2475
https://github.com/root-project/root/pull/2475:393,performance,memor,memory,393,"added cuda stream support to parallelize over events in the batch; Now using CUDA streams to support parallelization over events in the batch. This yields a 40% performance boost on high end GPUs (GTX 1080 Ti) and 10-15% on weaker models (Quadro 1000M). Additionally, temporary matrices used by `Im2Col` in the forward pass are maintained as a `ConvLayer` data field in order to minimize cuda memory allocations.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2475
https://github.com/root-project/root/pull/2475:328,safety,maintain,maintained,328,"added cuda stream support to parallelize over events in the batch; Now using CUDA streams to support parallelization over events in the batch. This yields a 40% performance boost on high end GPUs (GTX 1080 Ti) and 10-15% on weaker models (Quadro 1000M). Additionally, temporary matrices used by `Im2Col` in the forward pass are maintained as a `ConvLayer` data field in order to minimize cuda memory allocations.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2475
https://github.com/root-project/root/pull/2475:231,security,model,models,231,"added cuda stream support to parallelize over events in the batch; Now using CUDA streams to support parallelization over events in the batch. This yields a 40% performance boost on high end GPUs (GTX 1080 Ti) and 10-15% on weaker models (Quadro 1000M). Additionally, temporary matrices used by `Im2Col` in the forward pass are maintained as a `ConvLayer` data field in order to minimize cuda memory allocations.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2475
https://github.com/root-project/root/pull/2475:18,usability,support,support,18,"added cuda stream support to parallelize over events in the batch; Now using CUDA streams to support parallelization over events in the batch. This yields a 40% performance boost on high end GPUs (GTX 1080 Ti) and 10-15% on weaker models (Quadro 1000M). Additionally, temporary matrices used by `Im2Col` in the forward pass are maintained as a `ConvLayer` data field in order to minimize cuda memory allocations.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2475
https://github.com/root-project/root/pull/2475:93,usability,support,support,93,"added cuda stream support to parallelize over events in the batch; Now using CUDA streams to support parallelization over events in the batch. This yields a 40% performance boost on high end GPUs (GTX 1080 Ti) and 10-15% on weaker models (Quadro 1000M). Additionally, temporary matrices used by `Im2Col` in the forward pass are maintained as a `ConvLayer` data field in order to minimize cuda memory allocations.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2475
https://github.com/root-project/root/pull/2475:161,usability,perform,performance,161,"added cuda stream support to parallelize over events in the batch; Now using CUDA streams to support parallelization over events in the batch. This yields a 40% performance boost on high end GPUs (GTX 1080 Ti) and 10-15% on weaker models (Quadro 1000M). Additionally, temporary matrices used by `Im2Col` in the forward pass are maintained as a `ConvLayer` data field in order to minimize cuda memory allocations.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2475
https://github.com/root-project/root/pull/2475:379,usability,minim,minimize,379,"added cuda stream support to parallelize over events in the batch; Now using CUDA streams to support parallelization over events in the batch. This yields a 40% performance boost on high end GPUs (GTX 1080 Ti) and 10-15% on weaker models (Quadro 1000M). Additionally, temporary matrices used by `Im2Col` in the forward pass are maintained as a `ConvLayer` data field in order to minimize cuda memory allocations.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2475
https://github.com/root-project/root/pull/2475:393,usability,memor,memory,393,"added cuda stream support to parallelize over events in the batch; Now using CUDA streams to support parallelization over events in the batch. This yields a 40% performance boost on high end GPUs (GTX 1080 Ti) and 10-15% on weaker models (Quadro 1000M). Additionally, temporary matrices used by `Im2Col` in the forward pass are maintained as a `ConvLayer` data field in order to minimize cuda memory allocations.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2475
https://github.com/root-project/root/pull/2476:398,availability,failur,failure,398,"[ROOT-9617] Add TTree::SetBranchAddress pythonization; Modify the behaviour of SetBranchAddress so that proxy references can be passed as arguments from the Python side, more precisely in cases where the C++ implementation of the method expects the address of a pointer. For example:. ```python. v = ROOT.std.vector('int')(). t.SetBranchAddress(""my_vector_branch"", v). ```. Pending items:. - Check failure in Python 3. - Test other overloads. - Any other case to support? E.g. SetBranchAddress of individual elements in a struct? - Use helper method `GetClass` that was factored out",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2476
https://github.com/root-project/root/pull/2476:398,deployability,fail,failure,398,"[ROOT-9617] Add TTree::SetBranchAddress pythonization; Modify the behaviour of SetBranchAddress so that proxy references can be passed as arguments from the Python side, more precisely in cases where the C++ implementation of the method expects the address of a pointer. For example:. ```python. v = ROOT.std.vector('int')(). t.SetBranchAddress(""my_vector_branch"", v). ```. Pending items:. - Check failure in Python 3. - Test other overloads. - Any other case to support? E.g. SetBranchAddress of individual elements in a struct? - Use helper method `GetClass` that was factored out",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2476
https://github.com/root-project/root/pull/2476:104,interoperability,prox,proxy,104,"[ROOT-9617] Add TTree::SetBranchAddress pythonization; Modify the behaviour of SetBranchAddress so that proxy references can be passed as arguments from the Python side, more precisely in cases where the C++ implementation of the method expects the address of a pointer. For example:. ```python. v = ROOT.std.vector('int')(). t.SetBranchAddress(""my_vector_branch"", v). ```. Pending items:. - Check failure in Python 3. - Test other overloads. - Any other case to support? E.g. SetBranchAddress of individual elements in a struct? - Use helper method `GetClass` that was factored out",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2476
https://github.com/root-project/root/pull/2476:398,performance,failur,failure,398,"[ROOT-9617] Add TTree::SetBranchAddress pythonization; Modify the behaviour of SetBranchAddress so that proxy references can be passed as arguments from the Python side, more precisely in cases where the C++ implementation of the method expects the address of a pointer. For example:. ```python. v = ROOT.std.vector('int')(). t.SetBranchAddress(""my_vector_branch"", v). ```. Pending items:. - Check failure in Python 3. - Test other overloads. - Any other case to support? E.g. SetBranchAddress of individual elements in a struct? - Use helper method `GetClass` that was factored out",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2476
https://github.com/root-project/root/pull/2476:398,reliability,fail,failure,398,"[ROOT-9617] Add TTree::SetBranchAddress pythonization; Modify the behaviour of SetBranchAddress so that proxy references can be passed as arguments from the Python side, more precisely in cases where the C++ implementation of the method expects the address of a pointer. For example:. ```python. v = ROOT.std.vector('int')(). t.SetBranchAddress(""my_vector_branch"", v). ```. Pending items:. - Check failure in Python 3. - Test other overloads. - Any other case to support? E.g. SetBranchAddress of individual elements in a struct? - Use helper method `GetClass` that was factored out",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2476
https://github.com/root-project/root/pull/2476:421,safety,Test,Test,421,"[ROOT-9617] Add TTree::SetBranchAddress pythonization; Modify the behaviour of SetBranchAddress so that proxy references can be passed as arguments from the Python side, more precisely in cases where the C++ implementation of the method expects the address of a pointer. For example:. ```python. v = ROOT.std.vector('int')(). t.SetBranchAddress(""my_vector_branch"", v). ```. Pending items:. - Check failure in Python 3. - Test other overloads. - Any other case to support? E.g. SetBranchAddress of individual elements in a struct? - Use helper method `GetClass` that was factored out",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2476
https://github.com/root-project/root/pull/2476:55,security,Modif,Modify,55,"[ROOT-9617] Add TTree::SetBranchAddress pythonization; Modify the behaviour of SetBranchAddress so that proxy references can be passed as arguments from the Python side, more precisely in cases where the C++ implementation of the method expects the address of a pointer. For example:. ```python. v = ROOT.std.vector('int')(). t.SetBranchAddress(""my_vector_branch"", v). ```. Pending items:. - Check failure in Python 3. - Test other overloads. - Any other case to support? E.g. SetBranchAddress of individual elements in a struct? - Use helper method `GetClass` that was factored out",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2476
https://github.com/root-project/root/pull/2476:421,testability,Test,Test,421,"[ROOT-9617] Add TTree::SetBranchAddress pythonization; Modify the behaviour of SetBranchAddress so that proxy references can be passed as arguments from the Python side, more precisely in cases where the C++ implementation of the method expects the address of a pointer. For example:. ```python. v = ROOT.std.vector('int')(). t.SetBranchAddress(""my_vector_branch"", v). ```. Pending items:. - Check failure in Python 3. - Test other overloads. - Any other case to support? E.g. SetBranchAddress of individual elements in a struct? - Use helper method `GetClass` that was factored out",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2476
https://github.com/root-project/root/pull/2476:66,usability,behavi,behaviour,66,"[ROOT-9617] Add TTree::SetBranchAddress pythonization; Modify the behaviour of SetBranchAddress so that proxy references can be passed as arguments from the Python side, more precisely in cases where the C++ implementation of the method expects the address of a pointer. For example:. ```python. v = ROOT.std.vector('int')(). t.SetBranchAddress(""my_vector_branch"", v). ```. Pending items:. - Check failure in Python 3. - Test other overloads. - Any other case to support? E.g. SetBranchAddress of individual elements in a struct? - Use helper method `GetClass` that was factored out",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2476
https://github.com/root-project/root/pull/2476:463,usability,support,support,463,"[ROOT-9617] Add TTree::SetBranchAddress pythonization; Modify the behaviour of SetBranchAddress so that proxy references can be passed as arguments from the Python side, more precisely in cases where the C++ implementation of the method expects the address of a pointer. For example:. ```python. v = ROOT.std.vector('int')(). t.SetBranchAddress(""my_vector_branch"", v). ```. Pending items:. - Check failure in Python 3. - Test other overloads. - Any other case to support? E.g. SetBranchAddress of individual elements in a struct? - Use helper method `GetClass` that was factored out",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2476
https://github.com/root-project/root/pull/2476:536,usability,help,helper,536,"[ROOT-9617] Add TTree::SetBranchAddress pythonization; Modify the behaviour of SetBranchAddress so that proxy references can be passed as arguments from the Python side, more precisely in cases where the C++ implementation of the method expects the address of a pointer. For example:. ```python. v = ROOT.std.vector('int')(). t.SetBranchAddress(""my_vector_branch"", v). ```. Pending items:. - Check failure in Python 3. - Test other overloads. - Any other case to support? E.g. SetBranchAddress of individual elements in a struct? - Use helper method `GetClass` that was factored out",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2476
https://github.com/root-project/root/pull/2477:561,deployability,Depend,Depends,561,"[VecOps] Add helpers to compute combinations of collections; Basic workflow is as follows (have a look at the commited tutorial):. **Combinations of two vectors:**. ```cpp. RVec<double> v1{1., 2., 3.};. RVec<double> v2{-4., -5.};. auto idx = Combinations(v1, v2);. auto c1 = Take(v1, idx[0]);. auto c2 = Take(v2, idx[1]);. auto v3 = c1 * c2;. ```. **Unique combinations of elements from a single vector:**. ```cpp. RVec<double> v1{1., 2., 3.};. auto idx = Combinations(v1, 2);. auto c1 = Take(v1, idx[0]);. auto c2 = Take(v1, idx[1]);. auto v2 = c1 * c2;. ```. Depends on #2351",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2477
https://github.com/root-project/root/pull/2477:561,integrability,Depend,Depends,561,"[VecOps] Add helpers to compute combinations of collections; Basic workflow is as follows (have a look at the commited tutorial):. **Combinations of two vectors:**. ```cpp. RVec<double> v1{1., 2., 3.};. RVec<double> v2{-4., -5.};. auto idx = Combinations(v1, v2);. auto c1 = Take(v1, idx[0]);. auto c2 = Take(v2, idx[1]);. auto v3 = c1 * c2;. ```. **Unique combinations of elements from a single vector:**. ```cpp. RVec<double> v1{1., 2., 3.};. auto idx = Combinations(v1, 2);. auto c1 = Take(v1, idx[0]);. auto c2 = Take(v1, idx[1]);. auto v2 = c1 * c2;. ```. Depends on #2351",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2477
https://github.com/root-project/root/pull/2477:561,modifiability,Depend,Depends,561,"[VecOps] Add helpers to compute combinations of collections; Basic workflow is as follows (have a look at the commited tutorial):. **Combinations of two vectors:**. ```cpp. RVec<double> v1{1., 2., 3.};. RVec<double> v2{-4., -5.};. auto idx = Combinations(v1, v2);. auto c1 = Take(v1, idx[0]);. auto c2 = Take(v2, idx[1]);. auto v3 = c1 * c2;. ```. **Unique combinations of elements from a single vector:**. ```cpp. RVec<double> v1{1., 2., 3.};. auto idx = Combinations(v1, 2);. auto c1 = Take(v1, idx[0]);. auto c2 = Take(v1, idx[1]);. auto v2 = c1 * c2;. ```. Depends on #2351",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2477
https://github.com/root-project/root/pull/2477:561,safety,Depend,Depends,561,"[VecOps] Add helpers to compute combinations of collections; Basic workflow is as follows (have a look at the commited tutorial):. **Combinations of two vectors:**. ```cpp. RVec<double> v1{1., 2., 3.};. RVec<double> v2{-4., -5.};. auto idx = Combinations(v1, v2);. auto c1 = Take(v1, idx[0]);. auto c2 = Take(v2, idx[1]);. auto v3 = c1 * c2;. ```. **Unique combinations of elements from a single vector:**. ```cpp. RVec<double> v1{1., 2., 3.};. auto idx = Combinations(v1, 2);. auto c1 = Take(v1, idx[0]);. auto c2 = Take(v1, idx[1]);. auto v2 = c1 * c2;. ```. Depends on #2351",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2477
https://github.com/root-project/root/pull/2477:561,testability,Depend,Depends,561,"[VecOps] Add helpers to compute combinations of collections; Basic workflow is as follows (have a look at the commited tutorial):. **Combinations of two vectors:**. ```cpp. RVec<double> v1{1., 2., 3.};. RVec<double> v2{-4., -5.};. auto idx = Combinations(v1, v2);. auto c1 = Take(v1, idx[0]);. auto c2 = Take(v2, idx[1]);. auto v3 = c1 * c2;. ```. **Unique combinations of elements from a single vector:**. ```cpp. RVec<double> v1{1., 2., 3.};. auto idx = Combinations(v1, 2);. auto c1 = Take(v1, idx[0]);. auto c2 = Take(v1, idx[1]);. auto v2 = c1 * c2;. ```. Depends on #2351",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2477
https://github.com/root-project/root/pull/2477:13,usability,help,helpers,13,"[VecOps] Add helpers to compute combinations of collections; Basic workflow is as follows (have a look at the commited tutorial):. **Combinations of two vectors:**. ```cpp. RVec<double> v1{1., 2., 3.};. RVec<double> v2{-4., -5.};. auto idx = Combinations(v1, v2);. auto c1 = Take(v1, idx[0]);. auto c2 = Take(v2, idx[1]);. auto v3 = c1 * c2;. ```. **Unique combinations of elements from a single vector:**. ```cpp. RVec<double> v1{1., 2., 3.};. auto idx = Combinations(v1, 2);. auto c1 = Take(v1, idx[0]);. auto c2 = Take(v1, idx[1]);. auto v2 = c1 * c2;. ```. Depends on #2351",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2477
https://github.com/root-project/root/pull/2477:67,usability,workflow,workflow,67,"[VecOps] Add helpers to compute combinations of collections; Basic workflow is as follows (have a look at the commited tutorial):. **Combinations of two vectors:**. ```cpp. RVec<double> v1{1., 2., 3.};. RVec<double> v2{-4., -5.};. auto idx = Combinations(v1, v2);. auto c1 = Take(v1, idx[0]);. auto c2 = Take(v2, idx[1]);. auto v3 = c1 * c2;. ```. **Unique combinations of elements from a single vector:**. ```cpp. RVec<double> v1{1., 2., 3.};. auto idx = Combinations(v1, 2);. auto c1 = Take(v1, idx[0]);. auto c2 = Take(v1, idx[1]);. auto v2 = c1 * c2;. ```. Depends on #2351",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2477
https://github.com/root-project/root/pull/2478:54,availability,error,error,54,"v6-14-00-patches: Fix typo in RArrowDS; This fixes an error when using RDataFrame with an RArrowDS with integer columns. Somehow it used to work with Long64_t, but not with Long32_t.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2478
https://github.com/root-project/root/pull/2478:9,deployability,patch,patches,9,"v6-14-00-patches: Fix typo in RArrowDS; This fixes an error when using RDataFrame with an RArrowDS with integer columns. Somehow it used to work with Long64_t, but not with Long32_t.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2478
https://github.com/root-project/root/pull/2478:54,performance,error,error,54,"v6-14-00-patches: Fix typo in RArrowDS; This fixes an error when using RDataFrame with an RArrowDS with integer columns. Somehow it used to work with Long64_t, but not with Long32_t.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2478
https://github.com/root-project/root/pull/2478:9,safety,patch,patches,9,"v6-14-00-patches: Fix typo in RArrowDS; This fixes an error when using RDataFrame with an RArrowDS with integer columns. Somehow it used to work with Long64_t, but not with Long32_t.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2478
https://github.com/root-project/root/pull/2478:54,safety,error,error,54,"v6-14-00-patches: Fix typo in RArrowDS; This fixes an error when using RDataFrame with an RArrowDS with integer columns. Somehow it used to work with Long64_t, but not with Long32_t.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2478
https://github.com/root-project/root/pull/2478:9,security,patch,patches,9,"v6-14-00-patches: Fix typo in RArrowDS; This fixes an error when using RDataFrame with an RArrowDS with integer columns. Somehow it used to work with Long64_t, but not with Long32_t.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2478
https://github.com/root-project/root/pull/2478:54,usability,error,error,54,"v6-14-00-patches: Fix typo in RArrowDS; This fixes an error when using RDataFrame with an RArrowDS with integer columns. Somehow it used to work with Long64_t, but not with Long32_t.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2478
https://github.com/root-project/root/pull/2479:55,availability,error,error,55,Use cstdint types rather than ROOT ones; This fixes an error when using RDataFrame with an RArrowDS with integer columns.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2479
https://github.com/root-project/root/pull/2479:55,performance,error,error,55,Use cstdint types rather than ROOT ones; This fixes an error when using RDataFrame with an RArrowDS with integer columns.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2479
https://github.com/root-project/root/pull/2479:55,safety,error,error,55,Use cstdint types rather than ROOT ones; This fixes an error when using RDataFrame with an RArrowDS with integer columns.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2479
https://github.com/root-project/root/pull/2479:55,usability,error,error,55,Use cstdint types rather than ROOT ones; This fixes an error when using RDataFrame with an RArrowDS with integer columns.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2479
https://github.com/root-project/root/pull/2480:0,availability,Restor,Restoring,0,Restoring correct CMake invocation of rootcling for ROOT_GENERATE_DIC…; …TIONARY() after '[cxxmodules] Move away from environment variable' commit. Fixing issue https://root-forum.cern.ch/t/root-generate-dictionary-failing-with-cmake-3-11-and-root-6-14-02/30281,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2480
https://github.com/root-project/root/pull/2480:215,deployability,fail,failing-with-cmake-,215,Restoring correct CMake invocation of rootcling for ROOT_GENERATE_DIC…; …TIONARY() after '[cxxmodules] Move away from environment variable' commit. Fixing issue https://root-forum.cern.ch/t/root-generate-dictionary-failing-with-cmake-3-11-and-root-6-14-02/30281,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2480
https://github.com/root-project/root/pull/2480:130,modifiability,variab,variable,130,Restoring correct CMake invocation of rootcling for ROOT_GENERATE_DIC…; …TIONARY() after '[cxxmodules] Move away from environment variable' commit. Fixing issue https://root-forum.cern.ch/t/root-generate-dictionary-failing-with-cmake-3-11-and-root-6-14-02/30281,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2480
https://github.com/root-project/root/pull/2480:0,reliability,Restor,Restoring,0,Restoring correct CMake invocation of rootcling for ROOT_GENERATE_DIC…; …TIONARY() after '[cxxmodules] Move away from environment variable' commit. Fixing issue https://root-forum.cern.ch/t/root-generate-dictionary-failing-with-cmake-3-11-and-root-6-14-02/30281,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2480
https://github.com/root-project/root/pull/2480:215,reliability,fail,failing-with-cmake-,215,Restoring correct CMake invocation of rootcling for ROOT_GENERATE_DIC…; …TIONARY() after '[cxxmodules] Move away from environment variable' commit. Fixing issue https://root-forum.cern.ch/t/root-generate-dictionary-failing-with-cmake-3-11-and-root-6-14-02/30281,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2480
https://github.com/root-project/root/pull/2481:0,availability,Restor,Restoring,0,Restoring correct CMake invocation of rootcling for ROOT_GENERATE_DIC…; …TIONARY() after '[cxxmodules] Move away from environment variable' commit. Fixing issue https://root-forum.cern.ch/t/root-generate-dictionary-failing-with-cmake-3-11-and-root-6-14-02/30281/4,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2481
https://github.com/root-project/root/pull/2481:215,deployability,fail,failing-with-cmake-,215,Restoring correct CMake invocation of rootcling for ROOT_GENERATE_DIC…; …TIONARY() after '[cxxmodules] Move away from environment variable' commit. Fixing issue https://root-forum.cern.ch/t/root-generate-dictionary-failing-with-cmake-3-11-and-root-6-14-02/30281/4,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2481
https://github.com/root-project/root/pull/2481:130,modifiability,variab,variable,130,Restoring correct CMake invocation of rootcling for ROOT_GENERATE_DIC…; …TIONARY() after '[cxxmodules] Move away from environment variable' commit. Fixing issue https://root-forum.cern.ch/t/root-generate-dictionary-failing-with-cmake-3-11-and-root-6-14-02/30281/4,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2481
https://github.com/root-project/root/pull/2481:0,reliability,Restor,Restoring,0,Restoring correct CMake invocation of rootcling for ROOT_GENERATE_DIC…; …TIONARY() after '[cxxmodules] Move away from environment variable' commit. Fixing issue https://root-forum.cern.ch/t/root-generate-dictionary-failing-with-cmake-3-11-and-root-6-14-02/30281/4,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2481
https://github.com/root-project/root/pull/2481:215,reliability,fail,failing-with-cmake-,215,Restoring correct CMake invocation of rootcling for ROOT_GENERATE_DIC…; …TIONARY() after '[cxxmodules] Move away from environment variable' commit. Fixing issue https://root-forum.cern.ch/t/root-generate-dictionary-failing-with-cmake-3-11-and-root-6-14-02/30281/4,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2481
https://github.com/root-project/root/pull/2482:264,deployability,stage,stage,264,"Additional fix for rule scheduling in TBranchElement. ; This is an additional fix for cms-sw/cmssw#22594. If one of the class associated with a TBranchElement has a base class and/or sub-object class that. has a rule that requires a cache (onfileObject) object to stage the original data *but* does not. have branch in the TTree that also neeed the same cache object, we need to associate a cache. object with the StreamerInfoAction sequence that needs it (associated with a higher level branch). and give it ownership of this cache object)",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2482
https://github.com/root-project/root/pull/2482:24,energy efficiency,schedul,scheduling,24,"Additional fix for rule scheduling in TBranchElement. ; This is an additional fix for cms-sw/cmssw#22594. If one of the class associated with a TBranchElement has a base class and/or sub-object class that. has a rule that requires a cache (onfileObject) object to stage the original data *but* does not. have branch in the TTree that also neeed the same cache object, we need to associate a cache. object with the StreamerInfoAction sequence that needs it (associated with a higher level branch). and give it ownership of this cache object)",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2482
https://github.com/root-project/root/pull/2482:183,integrability,sub,sub-object,183,"Additional fix for rule scheduling in TBranchElement. ; This is an additional fix for cms-sw/cmssw#22594. If one of the class associated with a TBranchElement has a base class and/or sub-object class that. has a rule that requires a cache (onfileObject) object to stage the original data *but* does not. have branch in the TTree that also neeed the same cache object, we need to associate a cache. object with the StreamerInfoAction sequence that needs it (associated with a higher level branch). and give it ownership of this cache object)",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2482
https://github.com/root-project/root/pull/2482:24,performance,schedul,scheduling,24,"Additional fix for rule scheduling in TBranchElement. ; This is an additional fix for cms-sw/cmssw#22594. If one of the class associated with a TBranchElement has a base class and/or sub-object class that. has a rule that requires a cache (onfileObject) object to stage the original data *but* does not. have branch in the TTree that also neeed the same cache object, we need to associate a cache. object with the StreamerInfoAction sequence that needs it (associated with a higher level branch). and give it ownership of this cache object)",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2482
https://github.com/root-project/root/pull/2482:233,performance,cach,cache,233,"Additional fix for rule scheduling in TBranchElement. ; This is an additional fix for cms-sw/cmssw#22594. If one of the class associated with a TBranchElement has a base class and/or sub-object class that. has a rule that requires a cache (onfileObject) object to stage the original data *but* does not. have branch in the TTree that also neeed the same cache object, we need to associate a cache. object with the StreamerInfoAction sequence that needs it (associated with a higher level branch). and give it ownership of this cache object)",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2482
https://github.com/root-project/root/pull/2482:354,performance,cach,cache,354,"Additional fix for rule scheduling in TBranchElement. ; This is an additional fix for cms-sw/cmssw#22594. If one of the class associated with a TBranchElement has a base class and/or sub-object class that. has a rule that requires a cache (onfileObject) object to stage the original data *but* does not. have branch in the TTree that also neeed the same cache object, we need to associate a cache. object with the StreamerInfoAction sequence that needs it (associated with a higher level branch). and give it ownership of this cache object)",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2482
https://github.com/root-project/root/pull/2482:391,performance,cach,cache,391,"Additional fix for rule scheduling in TBranchElement. ; This is an additional fix for cms-sw/cmssw#22594. If one of the class associated with a TBranchElement has a base class and/or sub-object class that. has a rule that requires a cache (onfileObject) object to stage the original data *but* does not. have branch in the TTree that also neeed the same cache object, we need to associate a cache. object with the StreamerInfoAction sequence that needs it (associated with a higher level branch). and give it ownership of this cache object)",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2482
https://github.com/root-project/root/pull/2482:527,performance,cach,cache,527,"Additional fix for rule scheduling in TBranchElement. ; This is an additional fix for cms-sw/cmssw#22594. If one of the class associated with a TBranchElement has a base class and/or sub-object class that. has a rule that requires a cache (onfileObject) object to stage the original data *but* does not. have branch in the TTree that also neeed the same cache object, we need to associate a cache. object with the StreamerInfoAction sequence that needs it (associated with a higher level branch). and give it ownership of this cache object)",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2482
https://github.com/root-project/root/pull/2482:294,reliability,doe,does,294,"Additional fix for rule scheduling in TBranchElement. ; This is an additional fix for cms-sw/cmssw#22594. If one of the class associated with a TBranchElement has a base class and/or sub-object class that. has a rule that requires a cache (onfileObject) object to stage the original data *but* does not. have branch in the TTree that also neeed the same cache object, we need to associate a cache. object with the StreamerInfoAction sequence that needs it (associated with a higher level branch). and give it ownership of this cache object)",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2482
https://github.com/root-project/root/pull/2483:51,availability,operat,operations,51,"Implement parsing cache for the LookupHelper.; The operations done by the LookupHelper are costly in both memory and. performance. Almost every operation requires memory allocation and parsing. of often non-trivial C++ code. Unfortunately, the LookupHelper is used very intensively by rootcling and. ROOT. The callers usually do not use any caching mechanisms and redo the. expensive operations over and over even though the answer is known to be. the same as before. For instance, building the dictionary of shows:. ```. MathCore:. Cached entries: 217. Total parse requests: 54051. Cache hits: 53834. TreePlayer:. Cached entries: 183. Total parse requests: 57697. Cache hits: 57514. ```. This patch introduces the first set of caching functionality. In. particular, each LookupHelper::find* function allocates a memory buffer. which is then stored in the clang::SourceManager. We hash the buffer. content and keep a mapping between a hash and FileID and next time we. encounter the same content we do not allocate a new FileID but reuse the. old one. We see decrease in memory footprint by 7% for non-cxxmodules ROOT. For cxxmodules we see significant reduction of the pcm sizes (by half). which translates into rss improvements:. ```. master before:. cpu time = 0.291462 seconds. sys time = 0.064409 seconds. res memory = 345.816 Mbytes. vir memory = 573.508 Mbytes. master after:. cpu time = 0.235828 seconds. sys time = 0.098327 seconds. res memory = 260.012 Mbytes. vir memory = 377.945 Mbytes. ```. Patch by Yuka Takahashi and me.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2483
https://github.com/root-project/root/pull/2483:144,availability,operat,operation,144,"Implement parsing cache for the LookupHelper.; The operations done by the LookupHelper are costly in both memory and. performance. Almost every operation requires memory allocation and parsing. of often non-trivial C++ code. Unfortunately, the LookupHelper is used very intensively by rootcling and. ROOT. The callers usually do not use any caching mechanisms and redo the. expensive operations over and over even though the answer is known to be. the same as before. For instance, building the dictionary of shows:. ```. MathCore:. Cached entries: 217. Total parse requests: 54051. Cache hits: 53834. TreePlayer:. Cached entries: 183. Total parse requests: 57697. Cache hits: 57514. ```. This patch introduces the first set of caching functionality. In. particular, each LookupHelper::find* function allocates a memory buffer. which is then stored in the clang::SourceManager. We hash the buffer. content and keep a mapping between a hash and FileID and next time we. encounter the same content we do not allocate a new FileID but reuse the. old one. We see decrease in memory footprint by 7% for non-cxxmodules ROOT. For cxxmodules we see significant reduction of the pcm sizes (by half). which translates into rss improvements:. ```. master before:. cpu time = 0.291462 seconds. sys time = 0.064409 seconds. res memory = 345.816 Mbytes. vir memory = 573.508 Mbytes. master after:. cpu time = 0.235828 seconds. sys time = 0.098327 seconds. res memory = 260.012 Mbytes. vir memory = 377.945 Mbytes. ```. Patch by Yuka Takahashi and me.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2483
https://github.com/root-project/root/pull/2483:384,availability,operat,operations,384,"Implement parsing cache for the LookupHelper.; The operations done by the LookupHelper are costly in both memory and. performance. Almost every operation requires memory allocation and parsing. of often non-trivial C++ code. Unfortunately, the LookupHelper is used very intensively by rootcling and. ROOT. The callers usually do not use any caching mechanisms and redo the. expensive operations over and over even though the answer is known to be. the same as before. For instance, building the dictionary of shows:. ```. MathCore:. Cached entries: 217. Total parse requests: 54051. Cache hits: 53834. TreePlayer:. Cached entries: 183. Total parse requests: 57697. Cache hits: 57514. ```. This patch introduces the first set of caching functionality. In. particular, each LookupHelper::find* function allocates a memory buffer. which is then stored in the clang::SourceManager. We hash the buffer. content and keep a mapping between a hash and FileID and next time we. encounter the same content we do not allocate a new FileID but reuse the. old one. We see decrease in memory footprint by 7% for non-cxxmodules ROOT. For cxxmodules we see significant reduction of the pcm sizes (by half). which translates into rss improvements:. ```. master before:. cpu time = 0.291462 seconds. sys time = 0.064409 seconds. res memory = 345.816 Mbytes. vir memory = 573.508 Mbytes. master after:. cpu time = 0.235828 seconds. sys time = 0.098327 seconds. res memory = 260.012 Mbytes. vir memory = 377.945 Mbytes. ```. Patch by Yuka Takahashi and me.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2483
https://github.com/root-project/root/pull/2483:482,deployability,build,building,482,"Implement parsing cache for the LookupHelper.; The operations done by the LookupHelper are costly in both memory and. performance. Almost every operation requires memory allocation and parsing. of often non-trivial C++ code. Unfortunately, the LookupHelper is used very intensively by rootcling and. ROOT. The callers usually do not use any caching mechanisms and redo the. expensive operations over and over even though the answer is known to be. the same as before. For instance, building the dictionary of shows:. ```. MathCore:. Cached entries: 217. Total parse requests: 54051. Cache hits: 53834. TreePlayer:. Cached entries: 183. Total parse requests: 57697. Cache hits: 57514. ```. This patch introduces the first set of caching functionality. In. particular, each LookupHelper::find* function allocates a memory buffer. which is then stored in the clang::SourceManager. We hash the buffer. content and keep a mapping between a hash and FileID and next time we. encounter the same content we do not allocate a new FileID but reuse the. old one. We see decrease in memory footprint by 7% for non-cxxmodules ROOT. For cxxmodules we see significant reduction of the pcm sizes (by half). which translates into rss improvements:. ```. master before:. cpu time = 0.291462 seconds. sys time = 0.064409 seconds. res memory = 345.816 Mbytes. vir memory = 573.508 Mbytes. master after:. cpu time = 0.235828 seconds. sys time = 0.098327 seconds. res memory = 260.012 Mbytes. vir memory = 377.945 Mbytes. ```. Patch by Yuka Takahashi and me.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2483
https://github.com/root-project/root/pull/2483:694,deployability,patch,patch,694,"Implement parsing cache for the LookupHelper.; The operations done by the LookupHelper are costly in both memory and. performance. Almost every operation requires memory allocation and parsing. of often non-trivial C++ code. Unfortunately, the LookupHelper is used very intensively by rootcling and. ROOT. The callers usually do not use any caching mechanisms and redo the. expensive operations over and over even though the answer is known to be. the same as before. For instance, building the dictionary of shows:. ```. MathCore:. Cached entries: 217. Total parse requests: 54051. Cache hits: 53834. TreePlayer:. Cached entries: 183. Total parse requests: 57697. Cache hits: 57514. ```. This patch introduces the first set of caching functionality. In. particular, each LookupHelper::find* function allocates a memory buffer. which is then stored in the clang::SourceManager. We hash the buffer. content and keep a mapping between a hash and FileID and next time we. encounter the same content we do not allocate a new FileID but reuse the. old one. We see decrease in memory footprint by 7% for non-cxxmodules ROOT. For cxxmodules we see significant reduction of the pcm sizes (by half). which translates into rss improvements:. ```. master before:. cpu time = 0.291462 seconds. sys time = 0.064409 seconds. res memory = 345.816 Mbytes. vir memory = 573.508 Mbytes. master after:. cpu time = 0.235828 seconds. sys time = 0.098327 seconds. res memory = 260.012 Mbytes. vir memory = 377.945 Mbytes. ```. Patch by Yuka Takahashi and me.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2483
https://github.com/root-project/root/pull/2483:1505,deployability,Patch,Patch,1505,"Implement parsing cache for the LookupHelper.; The operations done by the LookupHelper are costly in both memory and. performance. Almost every operation requires memory allocation and parsing. of often non-trivial C++ code. Unfortunately, the LookupHelper is used very intensively by rootcling and. ROOT. The callers usually do not use any caching mechanisms and redo the. expensive operations over and over even though the answer is known to be. the same as before. For instance, building the dictionary of shows:. ```. MathCore:. Cached entries: 217. Total parse requests: 54051. Cache hits: 53834. TreePlayer:. Cached entries: 183. Total parse requests: 57697. Cache hits: 57514. ```. This patch introduces the first set of caching functionality. In. particular, each LookupHelper::find* function allocates a memory buffer. which is then stored in the clang::SourceManager. We hash the buffer. content and keep a mapping between a hash and FileID and next time we. encounter the same content we do not allocate a new FileID but reuse the. old one. We see decrease in memory footprint by 7% for non-cxxmodules ROOT. For cxxmodules we see significant reduction of the pcm sizes (by half). which translates into rss improvements:. ```. master before:. cpu time = 0.291462 seconds. sys time = 0.064409 seconds. res memory = 345.816 Mbytes. vir memory = 573.508 Mbytes. master after:. cpu time = 0.235828 seconds. sys time = 0.098327 seconds. res memory = 260.012 Mbytes. vir memory = 377.945 Mbytes. ```. Patch by Yuka Takahashi and me.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2483
https://github.com/root-project/root/pull/2483:170,energy efficiency,alloc,allocation,170,"Implement parsing cache for the LookupHelper.; The operations done by the LookupHelper are costly in both memory and. performance. Almost every operation requires memory allocation and parsing. of often non-trivial C++ code. Unfortunately, the LookupHelper is used very intensively by rootcling and. ROOT. The callers usually do not use any caching mechanisms and redo the. expensive operations over and over even though the answer is known to be. the same as before. For instance, building the dictionary of shows:. ```. MathCore:. Cached entries: 217. Total parse requests: 54051. Cache hits: 53834. TreePlayer:. Cached entries: 183. Total parse requests: 57697. Cache hits: 57514. ```. This patch introduces the first set of caching functionality. In. particular, each LookupHelper::find* function allocates a memory buffer. which is then stored in the clang::SourceManager. We hash the buffer. content and keep a mapping between a hash and FileID and next time we. encounter the same content we do not allocate a new FileID but reuse the. old one. We see decrease in memory footprint by 7% for non-cxxmodules ROOT. For cxxmodules we see significant reduction of the pcm sizes (by half). which translates into rss improvements:. ```. master before:. cpu time = 0.291462 seconds. sys time = 0.064409 seconds. res memory = 345.816 Mbytes. vir memory = 573.508 Mbytes. master after:. cpu time = 0.235828 seconds. sys time = 0.098327 seconds. res memory = 260.012 Mbytes. vir memory = 377.945 Mbytes. ```. Patch by Yuka Takahashi and me.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2483
https://github.com/root-project/root/pull/2483:801,energy efficiency,alloc,allocates,801,"Implement parsing cache for the LookupHelper.; The operations done by the LookupHelper are costly in both memory and. performance. Almost every operation requires memory allocation and parsing. of often non-trivial C++ code. Unfortunately, the LookupHelper is used very intensively by rootcling and. ROOT. The callers usually do not use any caching mechanisms and redo the. expensive operations over and over even though the answer is known to be. the same as before. For instance, building the dictionary of shows:. ```. MathCore:. Cached entries: 217. Total parse requests: 54051. Cache hits: 53834. TreePlayer:. Cached entries: 183. Total parse requests: 57697. Cache hits: 57514. ```. This patch introduces the first set of caching functionality. In. particular, each LookupHelper::find* function allocates a memory buffer. which is then stored in the clang::SourceManager. We hash the buffer. content and keep a mapping between a hash and FileID and next time we. encounter the same content we do not allocate a new FileID but reuse the. old one. We see decrease in memory footprint by 7% for non-cxxmodules ROOT. For cxxmodules we see significant reduction of the pcm sizes (by half). which translates into rss improvements:. ```. master before:. cpu time = 0.291462 seconds. sys time = 0.064409 seconds. res memory = 345.816 Mbytes. vir memory = 573.508 Mbytes. master after:. cpu time = 0.235828 seconds. sys time = 0.098327 seconds. res memory = 260.012 Mbytes. vir memory = 377.945 Mbytes. ```. Patch by Yuka Takahashi and me.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2483
https://github.com/root-project/root/pull/2483:1006,energy efficiency,alloc,allocate,1006,"Implement parsing cache for the LookupHelper.; The operations done by the LookupHelper are costly in both memory and. performance. Almost every operation requires memory allocation and parsing. of often non-trivial C++ code. Unfortunately, the LookupHelper is used very intensively by rootcling and. ROOT. The callers usually do not use any caching mechanisms and redo the. expensive operations over and over even though the answer is known to be. the same as before. For instance, building the dictionary of shows:. ```. MathCore:. Cached entries: 217. Total parse requests: 54051. Cache hits: 53834. TreePlayer:. Cached entries: 183. Total parse requests: 57697. Cache hits: 57514. ```. This patch introduces the first set of caching functionality. In. particular, each LookupHelper::find* function allocates a memory buffer. which is then stored in the clang::SourceManager. We hash the buffer. content and keep a mapping between a hash and FileID and next time we. encounter the same content we do not allocate a new FileID but reuse the. old one. We see decrease in memory footprint by 7% for non-cxxmodules ROOT. For cxxmodules we see significant reduction of the pcm sizes (by half). which translates into rss improvements:. ```. master before:. cpu time = 0.291462 seconds. sys time = 0.064409 seconds. res memory = 345.816 Mbytes. vir memory = 573.508 Mbytes. master after:. cpu time = 0.235828 seconds. sys time = 0.098327 seconds. res memory = 260.012 Mbytes. vir memory = 377.945 Mbytes. ```. Patch by Yuka Takahashi and me.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2483
https://github.com/root-project/root/pull/2483:1153,energy efficiency,reduc,reduction,1153,"Implement parsing cache for the LookupHelper.; The operations done by the LookupHelper are costly in both memory and. performance. Almost every operation requires memory allocation and parsing. of often non-trivial C++ code. Unfortunately, the LookupHelper is used very intensively by rootcling and. ROOT. The callers usually do not use any caching mechanisms and redo the. expensive operations over and over even though the answer is known to be. the same as before. For instance, building the dictionary of shows:. ```. MathCore:. Cached entries: 217. Total parse requests: 54051. Cache hits: 53834. TreePlayer:. Cached entries: 183. Total parse requests: 57697. Cache hits: 57514. ```. This patch introduces the first set of caching functionality. In. particular, each LookupHelper::find* function allocates a memory buffer. which is then stored in the clang::SourceManager. We hash the buffer. content and keep a mapping between a hash and FileID and next time we. encounter the same content we do not allocate a new FileID but reuse the. old one. We see decrease in memory footprint by 7% for non-cxxmodules ROOT. For cxxmodules we see significant reduction of the pcm sizes (by half). which translates into rss improvements:. ```. master before:. cpu time = 0.291462 seconds. sys time = 0.064409 seconds. res memory = 345.816 Mbytes. vir memory = 573.508 Mbytes. master after:. cpu time = 0.235828 seconds. sys time = 0.098327 seconds. res memory = 260.012 Mbytes. vir memory = 377.945 Mbytes. ```. Patch by Yuka Takahashi and me.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2483
https://github.com/root-project/root/pull/2483:1253,energy efficiency,cpu,cpu,1253,"Implement parsing cache for the LookupHelper.; The operations done by the LookupHelper are costly in both memory and. performance. Almost every operation requires memory allocation and parsing. of often non-trivial C++ code. Unfortunately, the LookupHelper is used very intensively by rootcling and. ROOT. The callers usually do not use any caching mechanisms and redo the. expensive operations over and over even though the answer is known to be. the same as before. For instance, building the dictionary of shows:. ```. MathCore:. Cached entries: 217. Total parse requests: 54051. Cache hits: 53834. TreePlayer:. Cached entries: 183. Total parse requests: 57697. Cache hits: 57514. ```. This patch introduces the first set of caching functionality. In. particular, each LookupHelper::find* function allocates a memory buffer. which is then stored in the clang::SourceManager. We hash the buffer. content and keep a mapping between a hash and FileID and next time we. encounter the same content we do not allocate a new FileID but reuse the. old one. We see decrease in memory footprint by 7% for non-cxxmodules ROOT. For cxxmodules we see significant reduction of the pcm sizes (by half). which translates into rss improvements:. ```. master before:. cpu time = 0.291462 seconds. sys time = 0.064409 seconds. res memory = 345.816 Mbytes. vir memory = 573.508 Mbytes. master after:. cpu time = 0.235828 seconds. sys time = 0.098327 seconds. res memory = 260.012 Mbytes. vir memory = 377.945 Mbytes. ```. Patch by Yuka Takahashi and me.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2483
https://github.com/root-project/root/pull/2483:1384,energy efficiency,cpu,cpu,1384,"Implement parsing cache for the LookupHelper.; The operations done by the LookupHelper are costly in both memory and. performance. Almost every operation requires memory allocation and parsing. of often non-trivial C++ code. Unfortunately, the LookupHelper is used very intensively by rootcling and. ROOT. The callers usually do not use any caching mechanisms and redo the. expensive operations over and over even though the answer is known to be. the same as before. For instance, building the dictionary of shows:. ```. MathCore:. Cached entries: 217. Total parse requests: 54051. Cache hits: 53834. TreePlayer:. Cached entries: 183. Total parse requests: 57697. Cache hits: 57514. ```. This patch introduces the first set of caching functionality. In. particular, each LookupHelper::find* function allocates a memory buffer. which is then stored in the clang::SourceManager. We hash the buffer. content and keep a mapping between a hash and FileID and next time we. encounter the same content we do not allocate a new FileID but reuse the. old one. We see decrease in memory footprint by 7% for non-cxxmodules ROOT. For cxxmodules we see significant reduction of the pcm sizes (by half). which translates into rss improvements:. ```. master before:. cpu time = 0.291462 seconds. sys time = 0.064409 seconds. res memory = 345.816 Mbytes. vir memory = 573.508 Mbytes. master after:. cpu time = 0.235828 seconds. sys time = 0.098327 seconds. res memory = 260.012 Mbytes. vir memory = 377.945 Mbytes. ```. Patch by Yuka Takahashi and me.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2483
https://github.com/root-project/root/pull/2483:820,integrability,buffer,buffer,820,"Implement parsing cache for the LookupHelper.; The operations done by the LookupHelper are costly in both memory and. performance. Almost every operation requires memory allocation and parsing. of often non-trivial C++ code. Unfortunately, the LookupHelper is used very intensively by rootcling and. ROOT. The callers usually do not use any caching mechanisms and redo the. expensive operations over and over even though the answer is known to be. the same as before. For instance, building the dictionary of shows:. ```. MathCore:. Cached entries: 217. Total parse requests: 54051. Cache hits: 53834. TreePlayer:. Cached entries: 183. Total parse requests: 57697. Cache hits: 57514. ```. This patch introduces the first set of caching functionality. In. particular, each LookupHelper::find* function allocates a memory buffer. which is then stored in the clang::SourceManager. We hash the buffer. content and keep a mapping between a hash and FileID and next time we. encounter the same content we do not allocate a new FileID but reuse the. old one. We see decrease in memory footprint by 7% for non-cxxmodules ROOT. For cxxmodules we see significant reduction of the pcm sizes (by half). which translates into rss improvements:. ```. master before:. cpu time = 0.291462 seconds. sys time = 0.064409 seconds. res memory = 345.816 Mbytes. vir memory = 573.508 Mbytes. master after:. cpu time = 0.235828 seconds. sys time = 0.098327 seconds. res memory = 260.012 Mbytes. vir memory = 377.945 Mbytes. ```. Patch by Yuka Takahashi and me.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2483
https://github.com/root-project/root/pull/2483:890,integrability,buffer,buffer,890,"Implement parsing cache for the LookupHelper.; The operations done by the LookupHelper are costly in both memory and. performance. Almost every operation requires memory allocation and parsing. of often non-trivial C++ code. Unfortunately, the LookupHelper is used very intensively by rootcling and. ROOT. The callers usually do not use any caching mechanisms and redo the. expensive operations over and over even though the answer is known to be. the same as before. For instance, building the dictionary of shows:. ```. MathCore:. Cached entries: 217. Total parse requests: 54051. Cache hits: 53834. TreePlayer:. Cached entries: 183. Total parse requests: 57697. Cache hits: 57514. ```. This patch introduces the first set of caching functionality. In. particular, each LookupHelper::find* function allocates a memory buffer. which is then stored in the clang::SourceManager. We hash the buffer. content and keep a mapping between a hash and FileID and next time we. encounter the same content we do not allocate a new FileID but reuse the. old one. We see decrease in memory footprint by 7% for non-cxxmodules ROOT. For cxxmodules we see significant reduction of the pcm sizes (by half). which translates into rss improvements:. ```. master before:. cpu time = 0.291462 seconds. sys time = 0.064409 seconds. res memory = 345.816 Mbytes. vir memory = 573.508 Mbytes. master after:. cpu time = 0.235828 seconds. sys time = 0.098327 seconds. res memory = 260.012 Mbytes. vir memory = 377.945 Mbytes. ```. Patch by Yuka Takahashi and me.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2483
https://github.com/root-project/root/pull/2483:1197,integrability,translat,translates,1197,"Implement parsing cache for the LookupHelper.; The operations done by the LookupHelper are costly in both memory and. performance. Almost every operation requires memory allocation and parsing. of often non-trivial C++ code. Unfortunately, the LookupHelper is used very intensively by rootcling and. ROOT. The callers usually do not use any caching mechanisms and redo the. expensive operations over and over even though the answer is known to be. the same as before. For instance, building the dictionary of shows:. ```. MathCore:. Cached entries: 217. Total parse requests: 54051. Cache hits: 53834. TreePlayer:. Cached entries: 183. Total parse requests: 57697. Cache hits: 57514. ```. This patch introduces the first set of caching functionality. In. particular, each LookupHelper::find* function allocates a memory buffer. which is then stored in the clang::SourceManager. We hash the buffer. content and keep a mapping between a hash and FileID and next time we. encounter the same content we do not allocate a new FileID but reuse the. old one. We see decrease in memory footprint by 7% for non-cxxmodules ROOT. For cxxmodules we see significant reduction of the pcm sizes (by half). which translates into rss improvements:. ```. master before:. cpu time = 0.291462 seconds. sys time = 0.064409 seconds. res memory = 345.816 Mbytes. vir memory = 573.508 Mbytes. master after:. cpu time = 0.235828 seconds. sys time = 0.098327 seconds. res memory = 260.012 Mbytes. vir memory = 377.945 Mbytes. ```. Patch by Yuka Takahashi and me.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2483
https://github.com/root-project/root/pull/2483:1197,interoperability,translat,translates,1197,"Implement parsing cache for the LookupHelper.; The operations done by the LookupHelper are costly in both memory and. performance. Almost every operation requires memory allocation and parsing. of often non-trivial C++ code. Unfortunately, the LookupHelper is used very intensively by rootcling and. ROOT. The callers usually do not use any caching mechanisms and redo the. expensive operations over and over even though the answer is known to be. the same as before. For instance, building the dictionary of shows:. ```. MathCore:. Cached entries: 217. Total parse requests: 54051. Cache hits: 53834. TreePlayer:. Cached entries: 183. Total parse requests: 57697. Cache hits: 57514. ```. This patch introduces the first set of caching functionality. In. particular, each LookupHelper::find* function allocates a memory buffer. which is then stored in the clang::SourceManager. We hash the buffer. content and keep a mapping between a hash and FileID and next time we. encounter the same content we do not allocate a new FileID but reuse the. old one. We see decrease in memory footprint by 7% for non-cxxmodules ROOT. For cxxmodules we see significant reduction of the pcm sizes (by half). which translates into rss improvements:. ```. master before:. cpu time = 0.291462 seconds. sys time = 0.064409 seconds. res memory = 345.816 Mbytes. vir memory = 573.508 Mbytes. master after:. cpu time = 0.235828 seconds. sys time = 0.098327 seconds. res memory = 260.012 Mbytes. vir memory = 377.945 Mbytes. ```. Patch by Yuka Takahashi and me.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2483
https://github.com/root-project/root/pull/2483:1032,modifiability,reu,reuse,1032,"Implement parsing cache for the LookupHelper.; The operations done by the LookupHelper are costly in both memory and. performance. Almost every operation requires memory allocation and parsing. of often non-trivial C++ code. Unfortunately, the LookupHelper is used very intensively by rootcling and. ROOT. The callers usually do not use any caching mechanisms and redo the. expensive operations over and over even though the answer is known to be. the same as before. For instance, building the dictionary of shows:. ```. MathCore:. Cached entries: 217. Total parse requests: 54051. Cache hits: 53834. TreePlayer:. Cached entries: 183. Total parse requests: 57697. Cache hits: 57514. ```. This patch introduces the first set of caching functionality. In. particular, each LookupHelper::find* function allocates a memory buffer. which is then stored in the clang::SourceManager. We hash the buffer. content and keep a mapping between a hash and FileID and next time we. encounter the same content we do not allocate a new FileID but reuse the. old one. We see decrease in memory footprint by 7% for non-cxxmodules ROOT. For cxxmodules we see significant reduction of the pcm sizes (by half). which translates into rss improvements:. ```. master before:. cpu time = 0.291462 seconds. sys time = 0.064409 seconds. res memory = 345.816 Mbytes. vir memory = 573.508 Mbytes. master after:. cpu time = 0.235828 seconds. sys time = 0.098327 seconds. res memory = 260.012 Mbytes. vir memory = 377.945 Mbytes. ```. Patch by Yuka Takahashi and me.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2483
https://github.com/root-project/root/pull/2483:18,performance,cach,cache,18,"Implement parsing cache for the LookupHelper.; The operations done by the LookupHelper are costly in both memory and. performance. Almost every operation requires memory allocation and parsing. of often non-trivial C++ code. Unfortunately, the LookupHelper is used very intensively by rootcling and. ROOT. The callers usually do not use any caching mechanisms and redo the. expensive operations over and over even though the answer is known to be. the same as before. For instance, building the dictionary of shows:. ```. MathCore:. Cached entries: 217. Total parse requests: 54051. Cache hits: 53834. TreePlayer:. Cached entries: 183. Total parse requests: 57697. Cache hits: 57514. ```. This patch introduces the first set of caching functionality. In. particular, each LookupHelper::find* function allocates a memory buffer. which is then stored in the clang::SourceManager. We hash the buffer. content and keep a mapping between a hash and FileID and next time we. encounter the same content we do not allocate a new FileID but reuse the. old one. We see decrease in memory footprint by 7% for non-cxxmodules ROOT. For cxxmodules we see significant reduction of the pcm sizes (by half). which translates into rss improvements:. ```. master before:. cpu time = 0.291462 seconds. sys time = 0.064409 seconds. res memory = 345.816 Mbytes. vir memory = 573.508 Mbytes. master after:. cpu time = 0.235828 seconds. sys time = 0.098327 seconds. res memory = 260.012 Mbytes. vir memory = 377.945 Mbytes. ```. Patch by Yuka Takahashi and me.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2483
https://github.com/root-project/root/pull/2483:106,performance,memor,memory,106,"Implement parsing cache for the LookupHelper.; The operations done by the LookupHelper are costly in both memory and. performance. Almost every operation requires memory allocation and parsing. of often non-trivial C++ code. Unfortunately, the LookupHelper is used very intensively by rootcling and. ROOT. The callers usually do not use any caching mechanisms and redo the. expensive operations over and over even though the answer is known to be. the same as before. For instance, building the dictionary of shows:. ```. MathCore:. Cached entries: 217. Total parse requests: 54051. Cache hits: 53834. TreePlayer:. Cached entries: 183. Total parse requests: 57697. Cache hits: 57514. ```. This patch introduces the first set of caching functionality. In. particular, each LookupHelper::find* function allocates a memory buffer. which is then stored in the clang::SourceManager. We hash the buffer. content and keep a mapping between a hash and FileID and next time we. encounter the same content we do not allocate a new FileID but reuse the. old one. We see decrease in memory footprint by 7% for non-cxxmodules ROOT. For cxxmodules we see significant reduction of the pcm sizes (by half). which translates into rss improvements:. ```. master before:. cpu time = 0.291462 seconds. sys time = 0.064409 seconds. res memory = 345.816 Mbytes. vir memory = 573.508 Mbytes. master after:. cpu time = 0.235828 seconds. sys time = 0.098327 seconds. res memory = 260.012 Mbytes. vir memory = 377.945 Mbytes. ```. Patch by Yuka Takahashi and me.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2483
https://github.com/root-project/root/pull/2483:118,performance,perform,performance,118,"Implement parsing cache for the LookupHelper.; The operations done by the LookupHelper are costly in both memory and. performance. Almost every operation requires memory allocation and parsing. of often non-trivial C++ code. Unfortunately, the LookupHelper is used very intensively by rootcling and. ROOT. The callers usually do not use any caching mechanisms and redo the. expensive operations over and over even though the answer is known to be. the same as before. For instance, building the dictionary of shows:. ```. MathCore:. Cached entries: 217. Total parse requests: 54051. Cache hits: 53834. TreePlayer:. Cached entries: 183. Total parse requests: 57697. Cache hits: 57514. ```. This patch introduces the first set of caching functionality. In. particular, each LookupHelper::find* function allocates a memory buffer. which is then stored in the clang::SourceManager. We hash the buffer. content and keep a mapping between a hash and FileID and next time we. encounter the same content we do not allocate a new FileID but reuse the. old one. We see decrease in memory footprint by 7% for non-cxxmodules ROOT. For cxxmodules we see significant reduction of the pcm sizes (by half). which translates into rss improvements:. ```. master before:. cpu time = 0.291462 seconds. sys time = 0.064409 seconds. res memory = 345.816 Mbytes. vir memory = 573.508 Mbytes. master after:. cpu time = 0.235828 seconds. sys time = 0.098327 seconds. res memory = 260.012 Mbytes. vir memory = 377.945 Mbytes. ```. Patch by Yuka Takahashi and me.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2483
https://github.com/root-project/root/pull/2483:163,performance,memor,memory,163,"Implement parsing cache for the LookupHelper.; The operations done by the LookupHelper are costly in both memory and. performance. Almost every operation requires memory allocation and parsing. of often non-trivial C++ code. Unfortunately, the LookupHelper is used very intensively by rootcling and. ROOT. The callers usually do not use any caching mechanisms and redo the. expensive operations over and over even though the answer is known to be. the same as before. For instance, building the dictionary of shows:. ```. MathCore:. Cached entries: 217. Total parse requests: 54051. Cache hits: 53834. TreePlayer:. Cached entries: 183. Total parse requests: 57697. Cache hits: 57514. ```. This patch introduces the first set of caching functionality. In. particular, each LookupHelper::find* function allocates a memory buffer. which is then stored in the clang::SourceManager. We hash the buffer. content and keep a mapping between a hash and FileID and next time we. encounter the same content we do not allocate a new FileID but reuse the. old one. We see decrease in memory footprint by 7% for non-cxxmodules ROOT. For cxxmodules we see significant reduction of the pcm sizes (by half). which translates into rss improvements:. ```. master before:. cpu time = 0.291462 seconds. sys time = 0.064409 seconds. res memory = 345.816 Mbytes. vir memory = 573.508 Mbytes. master after:. cpu time = 0.235828 seconds. sys time = 0.098327 seconds. res memory = 260.012 Mbytes. vir memory = 377.945 Mbytes. ```. Patch by Yuka Takahashi and me.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2483
https://github.com/root-project/root/pull/2483:341,performance,cach,caching,341,"Implement parsing cache for the LookupHelper.; The operations done by the LookupHelper are costly in both memory and. performance. Almost every operation requires memory allocation and parsing. of often non-trivial C++ code. Unfortunately, the LookupHelper is used very intensively by rootcling and. ROOT. The callers usually do not use any caching mechanisms and redo the. expensive operations over and over even though the answer is known to be. the same as before. For instance, building the dictionary of shows:. ```. MathCore:. Cached entries: 217. Total parse requests: 54051. Cache hits: 53834. TreePlayer:. Cached entries: 183. Total parse requests: 57697. Cache hits: 57514. ```. This patch introduces the first set of caching functionality. In. particular, each LookupHelper::find* function allocates a memory buffer. which is then stored in the clang::SourceManager. We hash the buffer. content and keep a mapping between a hash and FileID and next time we. encounter the same content we do not allocate a new FileID but reuse the. old one. We see decrease in memory footprint by 7% for non-cxxmodules ROOT. For cxxmodules we see significant reduction of the pcm sizes (by half). which translates into rss improvements:. ```. master before:. cpu time = 0.291462 seconds. sys time = 0.064409 seconds. res memory = 345.816 Mbytes. vir memory = 573.508 Mbytes. master after:. cpu time = 0.235828 seconds. sys time = 0.098327 seconds. res memory = 260.012 Mbytes. vir memory = 377.945 Mbytes. ```. Patch by Yuka Takahashi and me.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2483
https://github.com/root-project/root/pull/2483:533,performance,Cach,Cached,533,"Implement parsing cache for the LookupHelper.; The operations done by the LookupHelper are costly in both memory and. performance. Almost every operation requires memory allocation and parsing. of often non-trivial C++ code. Unfortunately, the LookupHelper is used very intensively by rootcling and. ROOT. The callers usually do not use any caching mechanisms and redo the. expensive operations over and over even though the answer is known to be. the same as before. For instance, building the dictionary of shows:. ```. MathCore:. Cached entries: 217. Total parse requests: 54051. Cache hits: 53834. TreePlayer:. Cached entries: 183. Total parse requests: 57697. Cache hits: 57514. ```. This patch introduces the first set of caching functionality. In. particular, each LookupHelper::find* function allocates a memory buffer. which is then stored in the clang::SourceManager. We hash the buffer. content and keep a mapping between a hash and FileID and next time we. encounter the same content we do not allocate a new FileID but reuse the. old one. We see decrease in memory footprint by 7% for non-cxxmodules ROOT. For cxxmodules we see significant reduction of the pcm sizes (by half). which translates into rss improvements:. ```. master before:. cpu time = 0.291462 seconds. sys time = 0.064409 seconds. res memory = 345.816 Mbytes. vir memory = 573.508 Mbytes. master after:. cpu time = 0.235828 seconds. sys time = 0.098327 seconds. res memory = 260.012 Mbytes. vir memory = 377.945 Mbytes. ```. Patch by Yuka Takahashi and me.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2483
https://github.com/root-project/root/pull/2483:583,performance,Cach,Cache,583,"Implement parsing cache for the LookupHelper.; The operations done by the LookupHelper are costly in both memory and. performance. Almost every operation requires memory allocation and parsing. of often non-trivial C++ code. Unfortunately, the LookupHelper is used very intensively by rootcling and. ROOT. The callers usually do not use any caching mechanisms and redo the. expensive operations over and over even though the answer is known to be. the same as before. For instance, building the dictionary of shows:. ```. MathCore:. Cached entries: 217. Total parse requests: 54051. Cache hits: 53834. TreePlayer:. Cached entries: 183. Total parse requests: 57697. Cache hits: 57514. ```. This patch introduces the first set of caching functionality. In. particular, each LookupHelper::find* function allocates a memory buffer. which is then stored in the clang::SourceManager. We hash the buffer. content and keep a mapping between a hash and FileID and next time we. encounter the same content we do not allocate a new FileID but reuse the. old one. We see decrease in memory footprint by 7% for non-cxxmodules ROOT. For cxxmodules we see significant reduction of the pcm sizes (by half). which translates into rss improvements:. ```. master before:. cpu time = 0.291462 seconds. sys time = 0.064409 seconds. res memory = 345.816 Mbytes. vir memory = 573.508 Mbytes. master after:. cpu time = 0.235828 seconds. sys time = 0.098327 seconds. res memory = 260.012 Mbytes. vir memory = 377.945 Mbytes. ```. Patch by Yuka Takahashi and me.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2483
https://github.com/root-project/root/pull/2483:615,performance,Cach,Cached,615,"Implement parsing cache for the LookupHelper.; The operations done by the LookupHelper are costly in both memory and. performance. Almost every operation requires memory allocation and parsing. of often non-trivial C++ code. Unfortunately, the LookupHelper is used very intensively by rootcling and. ROOT. The callers usually do not use any caching mechanisms and redo the. expensive operations over and over even though the answer is known to be. the same as before. For instance, building the dictionary of shows:. ```. MathCore:. Cached entries: 217. Total parse requests: 54051. Cache hits: 53834. TreePlayer:. Cached entries: 183. Total parse requests: 57697. Cache hits: 57514. ```. This patch introduces the first set of caching functionality. In. particular, each LookupHelper::find* function allocates a memory buffer. which is then stored in the clang::SourceManager. We hash the buffer. content and keep a mapping between a hash and FileID and next time we. encounter the same content we do not allocate a new FileID but reuse the. old one. We see decrease in memory footprint by 7% for non-cxxmodules ROOT. For cxxmodules we see significant reduction of the pcm sizes (by half). which translates into rss improvements:. ```. master before:. cpu time = 0.291462 seconds. sys time = 0.064409 seconds. res memory = 345.816 Mbytes. vir memory = 573.508 Mbytes. master after:. cpu time = 0.235828 seconds. sys time = 0.098327 seconds. res memory = 260.012 Mbytes. vir memory = 377.945 Mbytes. ```. Patch by Yuka Takahashi and me.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2483
https://github.com/root-project/root/pull/2483:665,performance,Cach,Cache,665,"Implement parsing cache for the LookupHelper.; The operations done by the LookupHelper are costly in both memory and. performance. Almost every operation requires memory allocation and parsing. of often non-trivial C++ code. Unfortunately, the LookupHelper is used very intensively by rootcling and. ROOT. The callers usually do not use any caching mechanisms and redo the. expensive operations over and over even though the answer is known to be. the same as before. For instance, building the dictionary of shows:. ```. MathCore:. Cached entries: 217. Total parse requests: 54051. Cache hits: 53834. TreePlayer:. Cached entries: 183. Total parse requests: 57697. Cache hits: 57514. ```. This patch introduces the first set of caching functionality. In. particular, each LookupHelper::find* function allocates a memory buffer. which is then stored in the clang::SourceManager. We hash the buffer. content and keep a mapping between a hash and FileID and next time we. encounter the same content we do not allocate a new FileID but reuse the. old one. We see decrease in memory footprint by 7% for non-cxxmodules ROOT. For cxxmodules we see significant reduction of the pcm sizes (by half). which translates into rss improvements:. ```. master before:. cpu time = 0.291462 seconds. sys time = 0.064409 seconds. res memory = 345.816 Mbytes. vir memory = 573.508 Mbytes. master after:. cpu time = 0.235828 seconds. sys time = 0.098327 seconds. res memory = 260.012 Mbytes. vir memory = 377.945 Mbytes. ```. Patch by Yuka Takahashi and me.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2483
https://github.com/root-project/root/pull/2483:728,performance,cach,caching,728,"Implement parsing cache for the LookupHelper.; The operations done by the LookupHelper are costly in both memory and. performance. Almost every operation requires memory allocation and parsing. of often non-trivial C++ code. Unfortunately, the LookupHelper is used very intensively by rootcling and. ROOT. The callers usually do not use any caching mechanisms and redo the. expensive operations over and over even though the answer is known to be. the same as before. For instance, building the dictionary of shows:. ```. MathCore:. Cached entries: 217. Total parse requests: 54051. Cache hits: 53834. TreePlayer:. Cached entries: 183. Total parse requests: 57697. Cache hits: 57514. ```. This patch introduces the first set of caching functionality. In. particular, each LookupHelper::find* function allocates a memory buffer. which is then stored in the clang::SourceManager. We hash the buffer. content and keep a mapping between a hash and FileID and next time we. encounter the same content we do not allocate a new FileID but reuse the. old one. We see decrease in memory footprint by 7% for non-cxxmodules ROOT. For cxxmodules we see significant reduction of the pcm sizes (by half). which translates into rss improvements:. ```. master before:. cpu time = 0.291462 seconds. sys time = 0.064409 seconds. res memory = 345.816 Mbytes. vir memory = 573.508 Mbytes. master after:. cpu time = 0.235828 seconds. sys time = 0.098327 seconds. res memory = 260.012 Mbytes. vir memory = 377.945 Mbytes. ```. Patch by Yuka Takahashi and me.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2483
https://github.com/root-project/root/pull/2483:813,performance,memor,memory,813,"Implement parsing cache for the LookupHelper.; The operations done by the LookupHelper are costly in both memory and. performance. Almost every operation requires memory allocation and parsing. of often non-trivial C++ code. Unfortunately, the LookupHelper is used very intensively by rootcling and. ROOT. The callers usually do not use any caching mechanisms and redo the. expensive operations over and over even though the answer is known to be. the same as before. For instance, building the dictionary of shows:. ```. MathCore:. Cached entries: 217. Total parse requests: 54051. Cache hits: 53834. TreePlayer:. Cached entries: 183. Total parse requests: 57697. Cache hits: 57514. ```. This patch introduces the first set of caching functionality. In. particular, each LookupHelper::find* function allocates a memory buffer. which is then stored in the clang::SourceManager. We hash the buffer. content and keep a mapping between a hash and FileID and next time we. encounter the same content we do not allocate a new FileID but reuse the. old one. We see decrease in memory footprint by 7% for non-cxxmodules ROOT. For cxxmodules we see significant reduction of the pcm sizes (by half). which translates into rss improvements:. ```. master before:. cpu time = 0.291462 seconds. sys time = 0.064409 seconds. res memory = 345.816 Mbytes. vir memory = 573.508 Mbytes. master after:. cpu time = 0.235828 seconds. sys time = 0.098327 seconds. res memory = 260.012 Mbytes. vir memory = 377.945 Mbytes. ```. Patch by Yuka Takahashi and me.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2483
https://github.com/root-project/root/pull/2483:898,performance,content,content,898,"Implement parsing cache for the LookupHelper.; The operations done by the LookupHelper are costly in both memory and. performance. Almost every operation requires memory allocation and parsing. of often non-trivial C++ code. Unfortunately, the LookupHelper is used very intensively by rootcling and. ROOT. The callers usually do not use any caching mechanisms and redo the. expensive operations over and over even though the answer is known to be. the same as before. For instance, building the dictionary of shows:. ```. MathCore:. Cached entries: 217. Total parse requests: 54051. Cache hits: 53834. TreePlayer:. Cached entries: 183. Total parse requests: 57697. Cache hits: 57514. ```. This patch introduces the first set of caching functionality. In. particular, each LookupHelper::find* function allocates a memory buffer. which is then stored in the clang::SourceManager. We hash the buffer. content and keep a mapping between a hash and FileID and next time we. encounter the same content we do not allocate a new FileID but reuse the. old one. We see decrease in memory footprint by 7% for non-cxxmodules ROOT. For cxxmodules we see significant reduction of the pcm sizes (by half). which translates into rss improvements:. ```. master before:. cpu time = 0.291462 seconds. sys time = 0.064409 seconds. res memory = 345.816 Mbytes. vir memory = 573.508 Mbytes. master after:. cpu time = 0.235828 seconds. sys time = 0.098327 seconds. res memory = 260.012 Mbytes. vir memory = 377.945 Mbytes. ```. Patch by Yuka Takahashi and me.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2483
https://github.com/root-project/root/pull/2483:960,performance,time,time,960,"Implement parsing cache for the LookupHelper.; The operations done by the LookupHelper are costly in both memory and. performance. Almost every operation requires memory allocation and parsing. of often non-trivial C++ code. Unfortunately, the LookupHelper is used very intensively by rootcling and. ROOT. The callers usually do not use any caching mechanisms and redo the. expensive operations over and over even though the answer is known to be. the same as before. For instance, building the dictionary of shows:. ```. MathCore:. Cached entries: 217. Total parse requests: 54051. Cache hits: 53834. TreePlayer:. Cached entries: 183. Total parse requests: 57697. Cache hits: 57514. ```. This patch introduces the first set of caching functionality. In. particular, each LookupHelper::find* function allocates a memory buffer. which is then stored in the clang::SourceManager. We hash the buffer. content and keep a mapping between a hash and FileID and next time we. encounter the same content we do not allocate a new FileID but reuse the. old one. We see decrease in memory footprint by 7% for non-cxxmodules ROOT. For cxxmodules we see significant reduction of the pcm sizes (by half). which translates into rss improvements:. ```. master before:. cpu time = 0.291462 seconds. sys time = 0.064409 seconds. res memory = 345.816 Mbytes. vir memory = 573.508 Mbytes. master after:. cpu time = 0.235828 seconds. sys time = 0.098327 seconds. res memory = 260.012 Mbytes. vir memory = 377.945 Mbytes. ```. Patch by Yuka Takahashi and me.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2483
https://github.com/root-project/root/pull/2483:988,performance,content,content,988,"Implement parsing cache for the LookupHelper.; The operations done by the LookupHelper are costly in both memory and. performance. Almost every operation requires memory allocation and parsing. of often non-trivial C++ code. Unfortunately, the LookupHelper is used very intensively by rootcling and. ROOT. The callers usually do not use any caching mechanisms and redo the. expensive operations over and over even though the answer is known to be. the same as before. For instance, building the dictionary of shows:. ```. MathCore:. Cached entries: 217. Total parse requests: 54051. Cache hits: 53834. TreePlayer:. Cached entries: 183. Total parse requests: 57697. Cache hits: 57514. ```. This patch introduces the first set of caching functionality. In. particular, each LookupHelper::find* function allocates a memory buffer. which is then stored in the clang::SourceManager. We hash the buffer. content and keep a mapping between a hash and FileID and next time we. encounter the same content we do not allocate a new FileID but reuse the. old one. We see decrease in memory footprint by 7% for non-cxxmodules ROOT. For cxxmodules we see significant reduction of the pcm sizes (by half). which translates into rss improvements:. ```. master before:. cpu time = 0.291462 seconds. sys time = 0.064409 seconds. res memory = 345.816 Mbytes. vir memory = 573.508 Mbytes. master after:. cpu time = 0.235828 seconds. sys time = 0.098327 seconds. res memory = 260.012 Mbytes. vir memory = 377.945 Mbytes. ```. Patch by Yuka Takahashi and me.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2483
https://github.com/root-project/root/pull/2483:1071,performance,memor,memory,1071,"Implement parsing cache for the LookupHelper.; The operations done by the LookupHelper are costly in both memory and. performance. Almost every operation requires memory allocation and parsing. of often non-trivial C++ code. Unfortunately, the LookupHelper is used very intensively by rootcling and. ROOT. The callers usually do not use any caching mechanisms and redo the. expensive operations over and over even though the answer is known to be. the same as before. For instance, building the dictionary of shows:. ```. MathCore:. Cached entries: 217. Total parse requests: 54051. Cache hits: 53834. TreePlayer:. Cached entries: 183. Total parse requests: 57697. Cache hits: 57514. ```. This patch introduces the first set of caching functionality. In. particular, each LookupHelper::find* function allocates a memory buffer. which is then stored in the clang::SourceManager. We hash the buffer. content and keep a mapping between a hash and FileID and next time we. encounter the same content we do not allocate a new FileID but reuse the. old one. We see decrease in memory footprint by 7% for non-cxxmodules ROOT. For cxxmodules we see significant reduction of the pcm sizes (by half). which translates into rss improvements:. ```. master before:. cpu time = 0.291462 seconds. sys time = 0.064409 seconds. res memory = 345.816 Mbytes. vir memory = 573.508 Mbytes. master after:. cpu time = 0.235828 seconds. sys time = 0.098327 seconds. res memory = 260.012 Mbytes. vir memory = 377.945 Mbytes. ```. Patch by Yuka Takahashi and me.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2483
https://github.com/root-project/root/pull/2483:1253,performance,cpu,cpu,1253,"Implement parsing cache for the LookupHelper.; The operations done by the LookupHelper are costly in both memory and. performance. Almost every operation requires memory allocation and parsing. of often non-trivial C++ code. Unfortunately, the LookupHelper is used very intensively by rootcling and. ROOT. The callers usually do not use any caching mechanisms and redo the. expensive operations over and over even though the answer is known to be. the same as before. For instance, building the dictionary of shows:. ```. MathCore:. Cached entries: 217. Total parse requests: 54051. Cache hits: 53834. TreePlayer:. Cached entries: 183. Total parse requests: 57697. Cache hits: 57514. ```. This patch introduces the first set of caching functionality. In. particular, each LookupHelper::find* function allocates a memory buffer. which is then stored in the clang::SourceManager. We hash the buffer. content and keep a mapping between a hash and FileID and next time we. encounter the same content we do not allocate a new FileID but reuse the. old one. We see decrease in memory footprint by 7% for non-cxxmodules ROOT. For cxxmodules we see significant reduction of the pcm sizes (by half). which translates into rss improvements:. ```. master before:. cpu time = 0.291462 seconds. sys time = 0.064409 seconds. res memory = 345.816 Mbytes. vir memory = 573.508 Mbytes. master after:. cpu time = 0.235828 seconds. sys time = 0.098327 seconds. res memory = 260.012 Mbytes. vir memory = 377.945 Mbytes. ```. Patch by Yuka Takahashi and me.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2483
https://github.com/root-project/root/pull/2483:1257,performance,time,time,1257,"Implement parsing cache for the LookupHelper.; The operations done by the LookupHelper are costly in both memory and. performance. Almost every operation requires memory allocation and parsing. of often non-trivial C++ code. Unfortunately, the LookupHelper is used very intensively by rootcling and. ROOT. The callers usually do not use any caching mechanisms and redo the. expensive operations over and over even though the answer is known to be. the same as before. For instance, building the dictionary of shows:. ```. MathCore:. Cached entries: 217. Total parse requests: 54051. Cache hits: 53834. TreePlayer:. Cached entries: 183. Total parse requests: 57697. Cache hits: 57514. ```. This patch introduces the first set of caching functionality. In. particular, each LookupHelper::find* function allocates a memory buffer. which is then stored in the clang::SourceManager. We hash the buffer. content and keep a mapping between a hash and FileID and next time we. encounter the same content we do not allocate a new FileID but reuse the. old one. We see decrease in memory footprint by 7% for non-cxxmodules ROOT. For cxxmodules we see significant reduction of the pcm sizes (by half). which translates into rss improvements:. ```. master before:. cpu time = 0.291462 seconds. sys time = 0.064409 seconds. res memory = 345.816 Mbytes. vir memory = 573.508 Mbytes. master after:. cpu time = 0.235828 seconds. sys time = 0.098327 seconds. res memory = 260.012 Mbytes. vir memory = 377.945 Mbytes. ```. Patch by Yuka Takahashi and me.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2483
https://github.com/root-project/root/pull/2483:1286,performance,time,time,1286,"Implement parsing cache for the LookupHelper.; The operations done by the LookupHelper are costly in both memory and. performance. Almost every operation requires memory allocation and parsing. of often non-trivial C++ code. Unfortunately, the LookupHelper is used very intensively by rootcling and. ROOT. The callers usually do not use any caching mechanisms and redo the. expensive operations over and over even though the answer is known to be. the same as before. For instance, building the dictionary of shows:. ```. MathCore:. Cached entries: 217. Total parse requests: 54051. Cache hits: 53834. TreePlayer:. Cached entries: 183. Total parse requests: 57697. Cache hits: 57514. ```. This patch introduces the first set of caching functionality. In. particular, each LookupHelper::find* function allocates a memory buffer. which is then stored in the clang::SourceManager. We hash the buffer. content and keep a mapping between a hash and FileID and next time we. encounter the same content we do not allocate a new FileID but reuse the. old one. We see decrease in memory footprint by 7% for non-cxxmodules ROOT. For cxxmodules we see significant reduction of the pcm sizes (by half). which translates into rss improvements:. ```. master before:. cpu time = 0.291462 seconds. sys time = 0.064409 seconds. res memory = 345.816 Mbytes. vir memory = 573.508 Mbytes. master after:. cpu time = 0.235828 seconds. sys time = 0.098327 seconds. res memory = 260.012 Mbytes. vir memory = 377.945 Mbytes. ```. Patch by Yuka Takahashi and me.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2483
https://github.com/root-project/root/pull/2483:1315,performance,memor,memory,1315,"Implement parsing cache for the LookupHelper.; The operations done by the LookupHelper are costly in both memory and. performance. Almost every operation requires memory allocation and parsing. of often non-trivial C++ code. Unfortunately, the LookupHelper is used very intensively by rootcling and. ROOT. The callers usually do not use any caching mechanisms and redo the. expensive operations over and over even though the answer is known to be. the same as before. For instance, building the dictionary of shows:. ```. MathCore:. Cached entries: 217. Total parse requests: 54051. Cache hits: 53834. TreePlayer:. Cached entries: 183. Total parse requests: 57697. Cache hits: 57514. ```. This patch introduces the first set of caching functionality. In. particular, each LookupHelper::find* function allocates a memory buffer. which is then stored in the clang::SourceManager. We hash the buffer. content and keep a mapping between a hash and FileID and next time we. encounter the same content we do not allocate a new FileID but reuse the. old one. We see decrease in memory footprint by 7% for non-cxxmodules ROOT. For cxxmodules we see significant reduction of the pcm sizes (by half). which translates into rss improvements:. ```. master before:. cpu time = 0.291462 seconds. sys time = 0.064409 seconds. res memory = 345.816 Mbytes. vir memory = 573.508 Mbytes. master after:. cpu time = 0.235828 seconds. sys time = 0.098327 seconds. res memory = 260.012 Mbytes. vir memory = 377.945 Mbytes. ```. Patch by Yuka Takahashi and me.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2483
https://github.com/root-project/root/pull/2483:1344,performance,memor,memory,1344,"Implement parsing cache for the LookupHelper.; The operations done by the LookupHelper are costly in both memory and. performance. Almost every operation requires memory allocation and parsing. of often non-trivial C++ code. Unfortunately, the LookupHelper is used very intensively by rootcling and. ROOT. The callers usually do not use any caching mechanisms and redo the. expensive operations over and over even though the answer is known to be. the same as before. For instance, building the dictionary of shows:. ```. MathCore:. Cached entries: 217. Total parse requests: 54051. Cache hits: 53834. TreePlayer:. Cached entries: 183. Total parse requests: 57697. Cache hits: 57514. ```. This patch introduces the first set of caching functionality. In. particular, each LookupHelper::find* function allocates a memory buffer. which is then stored in the clang::SourceManager. We hash the buffer. content and keep a mapping between a hash and FileID and next time we. encounter the same content we do not allocate a new FileID but reuse the. old one. We see decrease in memory footprint by 7% for non-cxxmodules ROOT. For cxxmodules we see significant reduction of the pcm sizes (by half). which translates into rss improvements:. ```. master before:. cpu time = 0.291462 seconds. sys time = 0.064409 seconds. res memory = 345.816 Mbytes. vir memory = 573.508 Mbytes. master after:. cpu time = 0.235828 seconds. sys time = 0.098327 seconds. res memory = 260.012 Mbytes. vir memory = 377.945 Mbytes. ```. Patch by Yuka Takahashi and me.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2483
https://github.com/root-project/root/pull/2483:1384,performance,cpu,cpu,1384,"Implement parsing cache for the LookupHelper.; The operations done by the LookupHelper are costly in both memory and. performance. Almost every operation requires memory allocation and parsing. of often non-trivial C++ code. Unfortunately, the LookupHelper is used very intensively by rootcling and. ROOT. The callers usually do not use any caching mechanisms and redo the. expensive operations over and over even though the answer is known to be. the same as before. For instance, building the dictionary of shows:. ```. MathCore:. Cached entries: 217. Total parse requests: 54051. Cache hits: 53834. TreePlayer:. Cached entries: 183. Total parse requests: 57697. Cache hits: 57514. ```. This patch introduces the first set of caching functionality. In. particular, each LookupHelper::find* function allocates a memory buffer. which is then stored in the clang::SourceManager. We hash the buffer. content and keep a mapping between a hash and FileID and next time we. encounter the same content we do not allocate a new FileID but reuse the. old one. We see decrease in memory footprint by 7% for non-cxxmodules ROOT. For cxxmodules we see significant reduction of the pcm sizes (by half). which translates into rss improvements:. ```. master before:. cpu time = 0.291462 seconds. sys time = 0.064409 seconds. res memory = 345.816 Mbytes. vir memory = 573.508 Mbytes. master after:. cpu time = 0.235828 seconds. sys time = 0.098327 seconds. res memory = 260.012 Mbytes. vir memory = 377.945 Mbytes. ```. Patch by Yuka Takahashi and me.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2483
https://github.com/root-project/root/pull/2483:1388,performance,time,time,1388,"Implement parsing cache for the LookupHelper.; The operations done by the LookupHelper are costly in both memory and. performance. Almost every operation requires memory allocation and parsing. of often non-trivial C++ code. Unfortunately, the LookupHelper is used very intensively by rootcling and. ROOT. The callers usually do not use any caching mechanisms and redo the. expensive operations over and over even though the answer is known to be. the same as before. For instance, building the dictionary of shows:. ```. MathCore:. Cached entries: 217. Total parse requests: 54051. Cache hits: 53834. TreePlayer:. Cached entries: 183. Total parse requests: 57697. Cache hits: 57514. ```. This patch introduces the first set of caching functionality. In. particular, each LookupHelper::find* function allocates a memory buffer. which is then stored in the clang::SourceManager. We hash the buffer. content and keep a mapping between a hash and FileID and next time we. encounter the same content we do not allocate a new FileID but reuse the. old one. We see decrease in memory footprint by 7% for non-cxxmodules ROOT. For cxxmodules we see significant reduction of the pcm sizes (by half). which translates into rss improvements:. ```. master before:. cpu time = 0.291462 seconds. sys time = 0.064409 seconds. res memory = 345.816 Mbytes. vir memory = 573.508 Mbytes. master after:. cpu time = 0.235828 seconds. sys time = 0.098327 seconds. res memory = 260.012 Mbytes. vir memory = 377.945 Mbytes. ```. Patch by Yuka Takahashi and me.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2483
https://github.com/root-project/root/pull/2483:1417,performance,time,time,1417,"Implement parsing cache for the LookupHelper.; The operations done by the LookupHelper are costly in both memory and. performance. Almost every operation requires memory allocation and parsing. of often non-trivial C++ code. Unfortunately, the LookupHelper is used very intensively by rootcling and. ROOT. The callers usually do not use any caching mechanisms and redo the. expensive operations over and over even though the answer is known to be. the same as before. For instance, building the dictionary of shows:. ```. MathCore:. Cached entries: 217. Total parse requests: 54051. Cache hits: 53834. TreePlayer:. Cached entries: 183. Total parse requests: 57697. Cache hits: 57514. ```. This patch introduces the first set of caching functionality. In. particular, each LookupHelper::find* function allocates a memory buffer. which is then stored in the clang::SourceManager. We hash the buffer. content and keep a mapping between a hash and FileID and next time we. encounter the same content we do not allocate a new FileID but reuse the. old one. We see decrease in memory footprint by 7% for non-cxxmodules ROOT. For cxxmodules we see significant reduction of the pcm sizes (by half). which translates into rss improvements:. ```. master before:. cpu time = 0.291462 seconds. sys time = 0.064409 seconds. res memory = 345.816 Mbytes. vir memory = 573.508 Mbytes. master after:. cpu time = 0.235828 seconds. sys time = 0.098327 seconds. res memory = 260.012 Mbytes. vir memory = 377.945 Mbytes. ```. Patch by Yuka Takahashi and me.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2483
https://github.com/root-project/root/pull/2483:1446,performance,memor,memory,1446,"Implement parsing cache for the LookupHelper.; The operations done by the LookupHelper are costly in both memory and. performance. Almost every operation requires memory allocation and parsing. of often non-trivial C++ code. Unfortunately, the LookupHelper is used very intensively by rootcling and. ROOT. The callers usually do not use any caching mechanisms and redo the. expensive operations over and over even though the answer is known to be. the same as before. For instance, building the dictionary of shows:. ```. MathCore:. Cached entries: 217. Total parse requests: 54051. Cache hits: 53834. TreePlayer:. Cached entries: 183. Total parse requests: 57697. Cache hits: 57514. ```. This patch introduces the first set of caching functionality. In. particular, each LookupHelper::find* function allocates a memory buffer. which is then stored in the clang::SourceManager. We hash the buffer. content and keep a mapping between a hash and FileID and next time we. encounter the same content we do not allocate a new FileID but reuse the. old one. We see decrease in memory footprint by 7% for non-cxxmodules ROOT. For cxxmodules we see significant reduction of the pcm sizes (by half). which translates into rss improvements:. ```. master before:. cpu time = 0.291462 seconds. sys time = 0.064409 seconds. res memory = 345.816 Mbytes. vir memory = 573.508 Mbytes. master after:. cpu time = 0.235828 seconds. sys time = 0.098327 seconds. res memory = 260.012 Mbytes. vir memory = 377.945 Mbytes. ```. Patch by Yuka Takahashi and me.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2483
https://github.com/root-project/root/pull/2483:1475,performance,memor,memory,1475,"Implement parsing cache for the LookupHelper.; The operations done by the LookupHelper are costly in both memory and. performance. Almost every operation requires memory allocation and parsing. of often non-trivial C++ code. Unfortunately, the LookupHelper is used very intensively by rootcling and. ROOT. The callers usually do not use any caching mechanisms and redo the. expensive operations over and over even though the answer is known to be. the same as before. For instance, building the dictionary of shows:. ```. MathCore:. Cached entries: 217. Total parse requests: 54051. Cache hits: 53834. TreePlayer:. Cached entries: 183. Total parse requests: 57697. Cache hits: 57514. ```. This patch introduces the first set of caching functionality. In. particular, each LookupHelper::find* function allocates a memory buffer. which is then stored in the clang::SourceManager. We hash the buffer. content and keep a mapping between a hash and FileID and next time we. encounter the same content we do not allocate a new FileID but reuse the. old one. We see decrease in memory footprint by 7% for non-cxxmodules ROOT. For cxxmodules we see significant reduction of the pcm sizes (by half). which translates into rss improvements:. ```. master before:. cpu time = 0.291462 seconds. sys time = 0.064409 seconds. res memory = 345.816 Mbytes. vir memory = 573.508 Mbytes. master after:. cpu time = 0.235828 seconds. sys time = 0.098327 seconds. res memory = 260.012 Mbytes. vir memory = 377.945 Mbytes. ```. Patch by Yuka Takahashi and me.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2483
https://github.com/root-project/root/pull/2483:694,safety,patch,patch,694,"Implement parsing cache for the LookupHelper.; The operations done by the LookupHelper are costly in both memory and. performance. Almost every operation requires memory allocation and parsing. of often non-trivial C++ code. Unfortunately, the LookupHelper is used very intensively by rootcling and. ROOT. The callers usually do not use any caching mechanisms and redo the. expensive operations over and over even though the answer is known to be. the same as before. For instance, building the dictionary of shows:. ```. MathCore:. Cached entries: 217. Total parse requests: 54051. Cache hits: 53834. TreePlayer:. Cached entries: 183. Total parse requests: 57697. Cache hits: 57514. ```. This patch introduces the first set of caching functionality. In. particular, each LookupHelper::find* function allocates a memory buffer. which is then stored in the clang::SourceManager. We hash the buffer. content and keep a mapping between a hash and FileID and next time we. encounter the same content we do not allocate a new FileID but reuse the. old one. We see decrease in memory footprint by 7% for non-cxxmodules ROOT. For cxxmodules we see significant reduction of the pcm sizes (by half). which translates into rss improvements:. ```. master before:. cpu time = 0.291462 seconds. sys time = 0.064409 seconds. res memory = 345.816 Mbytes. vir memory = 573.508 Mbytes. master after:. cpu time = 0.235828 seconds. sys time = 0.098327 seconds. res memory = 260.012 Mbytes. vir memory = 377.945 Mbytes. ```. Patch by Yuka Takahashi and me.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2483
https://github.com/root-project/root/pull/2483:1505,safety,Patch,Patch,1505,"Implement parsing cache for the LookupHelper.; The operations done by the LookupHelper are costly in both memory and. performance. Almost every operation requires memory allocation and parsing. of often non-trivial C++ code. Unfortunately, the LookupHelper is used very intensively by rootcling and. ROOT. The callers usually do not use any caching mechanisms and redo the. expensive operations over and over even though the answer is known to be. the same as before. For instance, building the dictionary of shows:. ```. MathCore:. Cached entries: 217. Total parse requests: 54051. Cache hits: 53834. TreePlayer:. Cached entries: 183. Total parse requests: 57697. Cache hits: 57514. ```. This patch introduces the first set of caching functionality. In. particular, each LookupHelper::find* function allocates a memory buffer. which is then stored in the clang::SourceManager. We hash the buffer. content and keep a mapping between a hash and FileID and next time we. encounter the same content we do not allocate a new FileID but reuse the. old one. We see decrease in memory footprint by 7% for non-cxxmodules ROOT. For cxxmodules we see significant reduction of the pcm sizes (by half). which translates into rss improvements:. ```. master before:. cpu time = 0.291462 seconds. sys time = 0.064409 seconds. res memory = 345.816 Mbytes. vir memory = 573.508 Mbytes. master after:. cpu time = 0.235828 seconds. sys time = 0.098327 seconds. res memory = 260.012 Mbytes. vir memory = 377.945 Mbytes. ```. Patch by Yuka Takahashi and me.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2483
https://github.com/root-project/root/pull/2483:694,security,patch,patch,694,"Implement parsing cache for the LookupHelper.; The operations done by the LookupHelper are costly in both memory and. performance. Almost every operation requires memory allocation and parsing. of often non-trivial C++ code. Unfortunately, the LookupHelper is used very intensively by rootcling and. ROOT. The callers usually do not use any caching mechanisms and redo the. expensive operations over and over even though the answer is known to be. the same as before. For instance, building the dictionary of shows:. ```. MathCore:. Cached entries: 217. Total parse requests: 54051. Cache hits: 53834. TreePlayer:. Cached entries: 183. Total parse requests: 57697. Cache hits: 57514. ```. This patch introduces the first set of caching functionality. In. particular, each LookupHelper::find* function allocates a memory buffer. which is then stored in the clang::SourceManager. We hash the buffer. content and keep a mapping between a hash and FileID and next time we. encounter the same content we do not allocate a new FileID but reuse the. old one. We see decrease in memory footprint by 7% for non-cxxmodules ROOT. For cxxmodules we see significant reduction of the pcm sizes (by half). which translates into rss improvements:. ```. master before:. cpu time = 0.291462 seconds. sys time = 0.064409 seconds. res memory = 345.816 Mbytes. vir memory = 573.508 Mbytes. master after:. cpu time = 0.235828 seconds. sys time = 0.098327 seconds. res memory = 260.012 Mbytes. vir memory = 377.945 Mbytes. ```. Patch by Yuka Takahashi and me.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2483
https://github.com/root-project/root/pull/2483:881,security,hash,hash,881,"Implement parsing cache for the LookupHelper.; The operations done by the LookupHelper are costly in both memory and. performance. Almost every operation requires memory allocation and parsing. of often non-trivial C++ code. Unfortunately, the LookupHelper is used very intensively by rootcling and. ROOT. The callers usually do not use any caching mechanisms and redo the. expensive operations over and over even though the answer is known to be. the same as before. For instance, building the dictionary of shows:. ```. MathCore:. Cached entries: 217. Total parse requests: 54051. Cache hits: 53834. TreePlayer:. Cached entries: 183. Total parse requests: 57697. Cache hits: 57514. ```. This patch introduces the first set of caching functionality. In. particular, each LookupHelper::find* function allocates a memory buffer. which is then stored in the clang::SourceManager. We hash the buffer. content and keep a mapping between a hash and FileID and next time we. encounter the same content we do not allocate a new FileID but reuse the. old one. We see decrease in memory footprint by 7% for non-cxxmodules ROOT. For cxxmodules we see significant reduction of the pcm sizes (by half). which translates into rss improvements:. ```. master before:. cpu time = 0.291462 seconds. sys time = 0.064409 seconds. res memory = 345.816 Mbytes. vir memory = 573.508 Mbytes. master after:. cpu time = 0.235828 seconds. sys time = 0.098327 seconds. res memory = 260.012 Mbytes. vir memory = 377.945 Mbytes. ```. Patch by Yuka Takahashi and me.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2483
https://github.com/root-project/root/pull/2483:935,security,hash,hash,935,"Implement parsing cache for the LookupHelper.; The operations done by the LookupHelper are costly in both memory and. performance. Almost every operation requires memory allocation and parsing. of often non-trivial C++ code. Unfortunately, the LookupHelper is used very intensively by rootcling and. ROOT. The callers usually do not use any caching mechanisms and redo the. expensive operations over and over even though the answer is known to be. the same as before. For instance, building the dictionary of shows:. ```. MathCore:. Cached entries: 217. Total parse requests: 54051. Cache hits: 53834. TreePlayer:. Cached entries: 183. Total parse requests: 57697. Cache hits: 57514. ```. This patch introduces the first set of caching functionality. In. particular, each LookupHelper::find* function allocates a memory buffer. which is then stored in the clang::SourceManager. We hash the buffer. content and keep a mapping between a hash and FileID and next time we. encounter the same content we do not allocate a new FileID but reuse the. old one. We see decrease in memory footprint by 7% for non-cxxmodules ROOT. For cxxmodules we see significant reduction of the pcm sizes (by half). which translates into rss improvements:. ```. master before:. cpu time = 0.291462 seconds. sys time = 0.064409 seconds. res memory = 345.816 Mbytes. vir memory = 573.508 Mbytes. master after:. cpu time = 0.235828 seconds. sys time = 0.098327 seconds. res memory = 260.012 Mbytes. vir memory = 377.945 Mbytes. ```. Patch by Yuka Takahashi and me.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2483
https://github.com/root-project/root/pull/2483:1141,security,sign,significant,1141,"Implement parsing cache for the LookupHelper.; The operations done by the LookupHelper are costly in both memory and. performance. Almost every operation requires memory allocation and parsing. of often non-trivial C++ code. Unfortunately, the LookupHelper is used very intensively by rootcling and. ROOT. The callers usually do not use any caching mechanisms and redo the. expensive operations over and over even though the answer is known to be. the same as before. For instance, building the dictionary of shows:. ```. MathCore:. Cached entries: 217. Total parse requests: 54051. Cache hits: 53834. TreePlayer:. Cached entries: 183. Total parse requests: 57697. Cache hits: 57514. ```. This patch introduces the first set of caching functionality. In. particular, each LookupHelper::find* function allocates a memory buffer. which is then stored in the clang::SourceManager. We hash the buffer. content and keep a mapping between a hash and FileID and next time we. encounter the same content we do not allocate a new FileID but reuse the. old one. We see decrease in memory footprint by 7% for non-cxxmodules ROOT. For cxxmodules we see significant reduction of the pcm sizes (by half). which translates into rss improvements:. ```. master before:. cpu time = 0.291462 seconds. sys time = 0.064409 seconds. res memory = 345.816 Mbytes. vir memory = 573.508 Mbytes. master after:. cpu time = 0.235828 seconds. sys time = 0.098327 seconds. res memory = 260.012 Mbytes. vir memory = 377.945 Mbytes. ```. Patch by Yuka Takahashi and me.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2483
https://github.com/root-project/root/pull/2483:1505,security,Patch,Patch,1505,"Implement parsing cache for the LookupHelper.; The operations done by the LookupHelper are costly in both memory and. performance. Almost every operation requires memory allocation and parsing. of often non-trivial C++ code. Unfortunately, the LookupHelper is used very intensively by rootcling and. ROOT. The callers usually do not use any caching mechanisms and redo the. expensive operations over and over even though the answer is known to be. the same as before. For instance, building the dictionary of shows:. ```. MathCore:. Cached entries: 217. Total parse requests: 54051. Cache hits: 53834. TreePlayer:. Cached entries: 183. Total parse requests: 57697. Cache hits: 57514. ```. This patch introduces the first set of caching functionality. In. particular, each LookupHelper::find* function allocates a memory buffer. which is then stored in the clang::SourceManager. We hash the buffer. content and keep a mapping between a hash and FileID and next time we. encounter the same content we do not allocate a new FileID but reuse the. old one. We see decrease in memory footprint by 7% for non-cxxmodules ROOT. For cxxmodules we see significant reduction of the pcm sizes (by half). which translates into rss improvements:. ```. master before:. cpu time = 0.291462 seconds. sys time = 0.064409 seconds. res memory = 345.816 Mbytes. vir memory = 573.508 Mbytes. master after:. cpu time = 0.235828 seconds. sys time = 0.098327 seconds. res memory = 260.012 Mbytes. vir memory = 377.945 Mbytes. ```. Patch by Yuka Takahashi and me.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2483
https://github.com/root-project/root/pull/2483:106,usability,memor,memory,106,"Implement parsing cache for the LookupHelper.; The operations done by the LookupHelper are costly in both memory and. performance. Almost every operation requires memory allocation and parsing. of often non-trivial C++ code. Unfortunately, the LookupHelper is used very intensively by rootcling and. ROOT. The callers usually do not use any caching mechanisms and redo the. expensive operations over and over even though the answer is known to be. the same as before. For instance, building the dictionary of shows:. ```. MathCore:. Cached entries: 217. Total parse requests: 54051. Cache hits: 53834. TreePlayer:. Cached entries: 183. Total parse requests: 57697. Cache hits: 57514. ```. This patch introduces the first set of caching functionality. In. particular, each LookupHelper::find* function allocates a memory buffer. which is then stored in the clang::SourceManager. We hash the buffer. content and keep a mapping between a hash and FileID and next time we. encounter the same content we do not allocate a new FileID but reuse the. old one. We see decrease in memory footprint by 7% for non-cxxmodules ROOT. For cxxmodules we see significant reduction of the pcm sizes (by half). which translates into rss improvements:. ```. master before:. cpu time = 0.291462 seconds. sys time = 0.064409 seconds. res memory = 345.816 Mbytes. vir memory = 573.508 Mbytes. master after:. cpu time = 0.235828 seconds. sys time = 0.098327 seconds. res memory = 260.012 Mbytes. vir memory = 377.945 Mbytes. ```. Patch by Yuka Takahashi and me.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2483
https://github.com/root-project/root/pull/2483:118,usability,perform,performance,118,"Implement parsing cache for the LookupHelper.; The operations done by the LookupHelper are costly in both memory and. performance. Almost every operation requires memory allocation and parsing. of often non-trivial C++ code. Unfortunately, the LookupHelper is used very intensively by rootcling and. ROOT. The callers usually do not use any caching mechanisms and redo the. expensive operations over and over even though the answer is known to be. the same as before. For instance, building the dictionary of shows:. ```. MathCore:. Cached entries: 217. Total parse requests: 54051. Cache hits: 53834. TreePlayer:. Cached entries: 183. Total parse requests: 57697. Cache hits: 57514. ```. This patch introduces the first set of caching functionality. In. particular, each LookupHelper::find* function allocates a memory buffer. which is then stored in the clang::SourceManager. We hash the buffer. content and keep a mapping between a hash and FileID and next time we. encounter the same content we do not allocate a new FileID but reuse the. old one. We see decrease in memory footprint by 7% for non-cxxmodules ROOT. For cxxmodules we see significant reduction of the pcm sizes (by half). which translates into rss improvements:. ```. master before:. cpu time = 0.291462 seconds. sys time = 0.064409 seconds. res memory = 345.816 Mbytes. vir memory = 573.508 Mbytes. master after:. cpu time = 0.235828 seconds. sys time = 0.098327 seconds. res memory = 260.012 Mbytes. vir memory = 377.945 Mbytes. ```. Patch by Yuka Takahashi and me.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2483
https://github.com/root-project/root/pull/2483:163,usability,memor,memory,163,"Implement parsing cache for the LookupHelper.; The operations done by the LookupHelper are costly in both memory and. performance. Almost every operation requires memory allocation and parsing. of often non-trivial C++ code. Unfortunately, the LookupHelper is used very intensively by rootcling and. ROOT. The callers usually do not use any caching mechanisms and redo the. expensive operations over and over even though the answer is known to be. the same as before. For instance, building the dictionary of shows:. ```. MathCore:. Cached entries: 217. Total parse requests: 54051. Cache hits: 53834. TreePlayer:. Cached entries: 183. Total parse requests: 57697. Cache hits: 57514. ```. This patch introduces the first set of caching functionality. In. particular, each LookupHelper::find* function allocates a memory buffer. which is then stored in the clang::SourceManager. We hash the buffer. content and keep a mapping between a hash and FileID and next time we. encounter the same content we do not allocate a new FileID but reuse the. old one. We see decrease in memory footprint by 7% for non-cxxmodules ROOT. For cxxmodules we see significant reduction of the pcm sizes (by half). which translates into rss improvements:. ```. master before:. cpu time = 0.291462 seconds. sys time = 0.064409 seconds. res memory = 345.816 Mbytes. vir memory = 573.508 Mbytes. master after:. cpu time = 0.235828 seconds. sys time = 0.098327 seconds. res memory = 260.012 Mbytes. vir memory = 377.945 Mbytes. ```. Patch by Yuka Takahashi and me.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2483
https://github.com/root-project/root/pull/2483:813,usability,memor,memory,813,"Implement parsing cache for the LookupHelper.; The operations done by the LookupHelper are costly in both memory and. performance. Almost every operation requires memory allocation and parsing. of often non-trivial C++ code. Unfortunately, the LookupHelper is used very intensively by rootcling and. ROOT. The callers usually do not use any caching mechanisms and redo the. expensive operations over and over even though the answer is known to be. the same as before. For instance, building the dictionary of shows:. ```. MathCore:. Cached entries: 217. Total parse requests: 54051. Cache hits: 53834. TreePlayer:. Cached entries: 183. Total parse requests: 57697. Cache hits: 57514. ```. This patch introduces the first set of caching functionality. In. particular, each LookupHelper::find* function allocates a memory buffer. which is then stored in the clang::SourceManager. We hash the buffer. content and keep a mapping between a hash and FileID and next time we. encounter the same content we do not allocate a new FileID but reuse the. old one. We see decrease in memory footprint by 7% for non-cxxmodules ROOT. For cxxmodules we see significant reduction of the pcm sizes (by half). which translates into rss improvements:. ```. master before:. cpu time = 0.291462 seconds. sys time = 0.064409 seconds. res memory = 345.816 Mbytes. vir memory = 573.508 Mbytes. master after:. cpu time = 0.235828 seconds. sys time = 0.098327 seconds. res memory = 260.012 Mbytes. vir memory = 377.945 Mbytes. ```. Patch by Yuka Takahashi and me.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2483
https://github.com/root-project/root/pull/2483:1071,usability,memor,memory,1071,"Implement parsing cache for the LookupHelper.; The operations done by the LookupHelper are costly in both memory and. performance. Almost every operation requires memory allocation and parsing. of often non-trivial C++ code. Unfortunately, the LookupHelper is used very intensively by rootcling and. ROOT. The callers usually do not use any caching mechanisms and redo the. expensive operations over and over even though the answer is known to be. the same as before. For instance, building the dictionary of shows:. ```. MathCore:. Cached entries: 217. Total parse requests: 54051. Cache hits: 53834. TreePlayer:. Cached entries: 183. Total parse requests: 57697. Cache hits: 57514. ```. This patch introduces the first set of caching functionality. In. particular, each LookupHelper::find* function allocates a memory buffer. which is then stored in the clang::SourceManager. We hash the buffer. content and keep a mapping between a hash and FileID and next time we. encounter the same content we do not allocate a new FileID but reuse the. old one. We see decrease in memory footprint by 7% for non-cxxmodules ROOT. For cxxmodules we see significant reduction of the pcm sizes (by half). which translates into rss improvements:. ```. master before:. cpu time = 0.291462 seconds. sys time = 0.064409 seconds. res memory = 345.816 Mbytes. vir memory = 573.508 Mbytes. master after:. cpu time = 0.235828 seconds. sys time = 0.098327 seconds. res memory = 260.012 Mbytes. vir memory = 377.945 Mbytes. ```. Patch by Yuka Takahashi and me.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2483
https://github.com/root-project/root/pull/2483:1315,usability,memor,memory,1315,"Implement parsing cache for the LookupHelper.; The operations done by the LookupHelper are costly in both memory and. performance. Almost every operation requires memory allocation and parsing. of often non-trivial C++ code. Unfortunately, the LookupHelper is used very intensively by rootcling and. ROOT. The callers usually do not use any caching mechanisms and redo the. expensive operations over and over even though the answer is known to be. the same as before. For instance, building the dictionary of shows:. ```. MathCore:. Cached entries: 217. Total parse requests: 54051. Cache hits: 53834. TreePlayer:. Cached entries: 183. Total parse requests: 57697. Cache hits: 57514. ```. This patch introduces the first set of caching functionality. In. particular, each LookupHelper::find* function allocates a memory buffer. which is then stored in the clang::SourceManager. We hash the buffer. content and keep a mapping between a hash and FileID and next time we. encounter the same content we do not allocate a new FileID but reuse the. old one. We see decrease in memory footprint by 7% for non-cxxmodules ROOT. For cxxmodules we see significant reduction of the pcm sizes (by half). which translates into rss improvements:. ```. master before:. cpu time = 0.291462 seconds. sys time = 0.064409 seconds. res memory = 345.816 Mbytes. vir memory = 573.508 Mbytes. master after:. cpu time = 0.235828 seconds. sys time = 0.098327 seconds. res memory = 260.012 Mbytes. vir memory = 377.945 Mbytes. ```. Patch by Yuka Takahashi and me.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2483
https://github.com/root-project/root/pull/2483:1344,usability,memor,memory,1344,"Implement parsing cache for the LookupHelper.; The operations done by the LookupHelper are costly in both memory and. performance. Almost every operation requires memory allocation and parsing. of often non-trivial C++ code. Unfortunately, the LookupHelper is used very intensively by rootcling and. ROOT. The callers usually do not use any caching mechanisms and redo the. expensive operations over and over even though the answer is known to be. the same as before. For instance, building the dictionary of shows:. ```. MathCore:. Cached entries: 217. Total parse requests: 54051. Cache hits: 53834. TreePlayer:. Cached entries: 183. Total parse requests: 57697. Cache hits: 57514. ```. This patch introduces the first set of caching functionality. In. particular, each LookupHelper::find* function allocates a memory buffer. which is then stored in the clang::SourceManager. We hash the buffer. content and keep a mapping between a hash and FileID and next time we. encounter the same content we do not allocate a new FileID but reuse the. old one. We see decrease in memory footprint by 7% for non-cxxmodules ROOT. For cxxmodules we see significant reduction of the pcm sizes (by half). which translates into rss improvements:. ```. master before:. cpu time = 0.291462 seconds. sys time = 0.064409 seconds. res memory = 345.816 Mbytes. vir memory = 573.508 Mbytes. master after:. cpu time = 0.235828 seconds. sys time = 0.098327 seconds. res memory = 260.012 Mbytes. vir memory = 377.945 Mbytes. ```. Patch by Yuka Takahashi and me.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2483
https://github.com/root-project/root/pull/2483:1446,usability,memor,memory,1446,"Implement parsing cache for the LookupHelper.; The operations done by the LookupHelper are costly in both memory and. performance. Almost every operation requires memory allocation and parsing. of often non-trivial C++ code. Unfortunately, the LookupHelper is used very intensively by rootcling and. ROOT. The callers usually do not use any caching mechanisms and redo the. expensive operations over and over even though the answer is known to be. the same as before. For instance, building the dictionary of shows:. ```. MathCore:. Cached entries: 217. Total parse requests: 54051. Cache hits: 53834. TreePlayer:. Cached entries: 183. Total parse requests: 57697. Cache hits: 57514. ```. This patch introduces the first set of caching functionality. In. particular, each LookupHelper::find* function allocates a memory buffer. which is then stored in the clang::SourceManager. We hash the buffer. content and keep a mapping between a hash and FileID and next time we. encounter the same content we do not allocate a new FileID but reuse the. old one. We see decrease in memory footprint by 7% for non-cxxmodules ROOT. For cxxmodules we see significant reduction of the pcm sizes (by half). which translates into rss improvements:. ```. master before:. cpu time = 0.291462 seconds. sys time = 0.064409 seconds. res memory = 345.816 Mbytes. vir memory = 573.508 Mbytes. master after:. cpu time = 0.235828 seconds. sys time = 0.098327 seconds. res memory = 260.012 Mbytes. vir memory = 377.945 Mbytes. ```. Patch by Yuka Takahashi and me.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2483
https://github.com/root-project/root/pull/2483:1475,usability,memor,memory,1475,"Implement parsing cache for the LookupHelper.; The operations done by the LookupHelper are costly in both memory and. performance. Almost every operation requires memory allocation and parsing. of often non-trivial C++ code. Unfortunately, the LookupHelper is used very intensively by rootcling and. ROOT. The callers usually do not use any caching mechanisms and redo the. expensive operations over and over even though the answer is known to be. the same as before. For instance, building the dictionary of shows:. ```. MathCore:. Cached entries: 217. Total parse requests: 54051. Cache hits: 53834. TreePlayer:. Cached entries: 183. Total parse requests: 57697. Cache hits: 57514. ```. This patch introduces the first set of caching functionality. In. particular, each LookupHelper::find* function allocates a memory buffer. which is then stored in the clang::SourceManager. We hash the buffer. content and keep a mapping between a hash and FileID and next time we. encounter the same content we do not allocate a new FileID but reuse the. old one. We see decrease in memory footprint by 7% for non-cxxmodules ROOT. For cxxmodules we see significant reduction of the pcm sizes (by half). which translates into rss improvements:. ```. master before:. cpu time = 0.291462 seconds. sys time = 0.064409 seconds. res memory = 345.816 Mbytes. vir memory = 573.508 Mbytes. master after:. cpu time = 0.235828 seconds. sys time = 0.098327 seconds. res memory = 260.012 Mbytes. vir memory = 377.945 Mbytes. ```. Patch by Yuka Takahashi and me.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2483
https://github.com/root-project/root/pull/2484:0,deployability,Updat,Updating,0,Updating clad build CMake arguments for the case when buiting_llvm an…; …d buitin_clang is OFF,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2484
https://github.com/root-project/root/pull/2484:14,deployability,build,build,14,Updating clad build CMake arguments for the case when buiting_llvm an…; …d buitin_clang is OFF,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2484
https://github.com/root-project/root/pull/2484:0,safety,Updat,Updating,0,Updating clad build CMake arguments for the case when buiting_llvm an…; …d buitin_clang is OFF,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2484
https://github.com/root-project/root/pull/2484:0,security,Updat,Updating,0,Updating clad build CMake arguments for the case when buiting_llvm an…; …d buitin_clang is OFF,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2484
https://github.com/root-project/root/pull/2485:12,safety,test,test,12,Fix typo in test.;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2485
https://github.com/root-project/root/pull/2485:12,testability,test,test,12,Fix typo in test.;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2485
https://github.com/root-project/root/pull/2486:9,deployability,patch,patches,9,v6-14-00-patches: Fix typo in test.;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2486
https://github.com/root-project/root/pull/2486:9,safety,patch,patches,9,v6-14-00-patches: Fix typo in test.;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2486
https://github.com/root-project/root/pull/2486:30,safety,test,test,30,v6-14-00-patches: Fix typo in test.;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2486
https://github.com/root-project/root/pull/2486:9,security,patch,patches,9,v6-14-00-patches: Fix typo in test.;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2486
https://github.com/root-project/root/pull/2486:30,testability,test,test,30,v6-14-00-patches: Fix typo in test.;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2486
https://github.com/root-project/root/pull/2487:806,availability,slo,slowest,806,"Muti-threading support in Web GUI classes; This is first step to make Web GUI code running in multiple threads. There are many threads introduced and many communication patterns are supported. 1. Provide special thread, which handle all http requests of THttpServer. This thread serves JavaScript/HTML files and redirects websocket requests to recipients. Try to reduce access to global ROOT structures from that thread - only list of classes for the moment is used. Potentially many THttpServer instances with such thread can run fully independent. 2. Let run websocket handlers (THttpWSHandler) in separate threads. . Requires correct locking of shared resources. 3. Support special threads for sending data via websocket from server to clients. . When many clients are connected to the same TWebWindow, slowest client can . break down performance. With use of such specialized thread problem can be solved. 4. Implement all necessary locks and protections to use many TWebWindows in different user threads. . Provide TWebWindow::Run(double) method, which allows to run window code in any user thread. . Introduce tutorials/v7/draw_mt.cxx macro, which shows how three RCanvas instances can run. in three independent thread and regularly updated. Of course, default behavior should work - all functionality runs in main thread. . Means THttpServer requests processing, websockets processing, RCanvas handling - . everything runs in main application thread. Simple, but not scalable. This is very preliminary code , I still plan to change/extend some API. Any comments comments are welcome",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2487
https://github.com/root-project/root/pull/2487:833,availability,down,down,833,"Muti-threading support in Web GUI classes; This is first step to make Web GUI code running in multiple threads. There are many threads introduced and many communication patterns are supported. 1. Provide special thread, which handle all http requests of THttpServer. This thread serves JavaScript/HTML files and redirects websocket requests to recipients. Try to reduce access to global ROOT structures from that thread - only list of classes for the moment is used. Potentially many THttpServer instances with such thread can run fully independent. 2. Let run websocket handlers (THttpWSHandler) in separate threads. . Requires correct locking of shared resources. 3. Support special threads for sending data via websocket from server to clients. . When many clients are connected to the same TWebWindow, slowest client can . break down performance. With use of such specialized thread problem can be solved. 4. Implement all necessary locks and protections to use many TWebWindows in different user threads. . Provide TWebWindow::Run(double) method, which allows to run window code in any user thread. . Introduce tutorials/v7/draw_mt.cxx macro, which shows how three RCanvas instances can run. in three independent thread and regularly updated. Of course, default behavior should work - all functionality runs in main thread. . Means THttpServer requests processing, websockets processing, RCanvas handling - . everything runs in main application thread. Simple, but not scalable. This is very preliminary code , I still plan to change/extend some API. Any comments comments are welcome",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2487
https://github.com/root-project/root/pull/2487:655,deployability,resourc,resources,655,"Muti-threading support in Web GUI classes; This is first step to make Web GUI code running in multiple threads. There are many threads introduced and many communication patterns are supported. 1. Provide special thread, which handle all http requests of THttpServer. This thread serves JavaScript/HTML files and redirects websocket requests to recipients. Try to reduce access to global ROOT structures from that thread - only list of classes for the moment is used. Potentially many THttpServer instances with such thread can run fully independent. 2. Let run websocket handlers (THttpWSHandler) in separate threads. . Requires correct locking of shared resources. 3. Support special threads for sending data via websocket from server to clients. . When many clients are connected to the same TWebWindow, slowest client can . break down performance. With use of such specialized thread problem can be solved. 4. Implement all necessary locks and protections to use many TWebWindows in different user threads. . Provide TWebWindow::Run(double) method, which allows to run window code in any user thread. . Introduce tutorials/v7/draw_mt.cxx macro, which shows how three RCanvas instances can run. in three independent thread and regularly updated. Of course, default behavior should work - all functionality runs in main thread. . Means THttpServer requests processing, websockets processing, RCanvas handling - . everything runs in main application thread. Simple, but not scalable. This is very preliminary code , I still plan to change/extend some API. Any comments comments are welcome",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2487
https://github.com/root-project/root/pull/2487:1239,deployability,updat,updated,1239,"Muti-threading support in Web GUI classes; This is first step to make Web GUI code running in multiple threads. There are many threads introduced and many communication patterns are supported. 1. Provide special thread, which handle all http requests of THttpServer. This thread serves JavaScript/HTML files and redirects websocket requests to recipients. Try to reduce access to global ROOT structures from that thread - only list of classes for the moment is used. Potentially many THttpServer instances with such thread can run fully independent. 2. Let run websocket handlers (THttpWSHandler) in separate threads. . Requires correct locking of shared resources. 3. Support special threads for sending data via websocket from server to clients. . When many clients are connected to the same TWebWindow, slowest client can . break down performance. With use of such specialized thread problem can be solved. 4. Implement all necessary locks and protections to use many TWebWindows in different user threads. . Provide TWebWindow::Run(double) method, which allows to run window code in any user thread. . Introduce tutorials/v7/draw_mt.cxx macro, which shows how three RCanvas instances can run. in three independent thread and regularly updated. Of course, default behavior should work - all functionality runs in main thread. . Means THttpServer requests processing, websockets processing, RCanvas handling - . everything runs in main application thread. Simple, but not scalable. This is very preliminary code , I still plan to change/extend some API. Any comments comments are welcome",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2487
https://github.com/root-project/root/pull/2487:1551,deployability,API,API,1551,"Muti-threading support in Web GUI classes; This is first step to make Web GUI code running in multiple threads. There are many threads introduced and many communication patterns are supported. 1. Provide special thread, which handle all http requests of THttpServer. This thread serves JavaScript/HTML files and redirects websocket requests to recipients. Try to reduce access to global ROOT structures from that thread - only list of classes for the moment is used. Potentially many THttpServer instances with such thread can run fully independent. 2. Let run websocket handlers (THttpWSHandler) in separate threads. . Requires correct locking of shared resources. 3. Support special threads for sending data via websocket from server to clients. . When many clients are connected to the same TWebWindow, slowest client can . break down performance. With use of such specialized thread problem can be solved. 4. Implement all necessary locks and protections to use many TWebWindows in different user threads. . Provide TWebWindow::Run(double) method, which allows to run window code in any user thread. . Introduce tutorials/v7/draw_mt.cxx macro, which shows how three RCanvas instances can run. in three independent thread and regularly updated. Of course, default behavior should work - all functionality runs in main thread. . Means THttpServer requests processing, websockets processing, RCanvas handling - . everything runs in main application thread. Simple, but not scalable. This is very preliminary code , I still plan to change/extend some API. Any comments comments are welcome",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2487
https://github.com/root-project/root/pull/2487:363,energy efficiency,reduc,reduce,363,"Muti-threading support in Web GUI classes; This is first step to make Web GUI code running in multiple threads. There are many threads introduced and many communication patterns are supported. 1. Provide special thread, which handle all http requests of THttpServer. This thread serves JavaScript/HTML files and redirects websocket requests to recipients. Try to reduce access to global ROOT structures from that thread - only list of classes for the moment is used. Potentially many THttpServer instances with such thread can run fully independent. 2. Let run websocket handlers (THttpWSHandler) in separate threads. . Requires correct locking of shared resources. 3. Support special threads for sending data via websocket from server to clients. . When many clients are connected to the same TWebWindow, slowest client can . break down performance. With use of such specialized thread problem can be solved. 4. Implement all necessary locks and protections to use many TWebWindows in different user threads. . Provide TWebWindow::Run(double) method, which allows to run window code in any user thread. . Introduce tutorials/v7/draw_mt.cxx macro, which shows how three RCanvas instances can run. in three independent thread and regularly updated. Of course, default behavior should work - all functionality runs in main thread. . Means THttpServer requests processing, websockets processing, RCanvas handling - . everything runs in main application thread. Simple, but not scalable. This is very preliminary code , I still plan to change/extend some API. Any comments comments are welcome",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2487
https://github.com/root-project/root/pull/2487:655,energy efficiency,resourc,resources,655,"Muti-threading support in Web GUI classes; This is first step to make Web GUI code running in multiple threads. There are many threads introduced and many communication patterns are supported. 1. Provide special thread, which handle all http requests of THttpServer. This thread serves JavaScript/HTML files and redirects websocket requests to recipients. Try to reduce access to global ROOT structures from that thread - only list of classes for the moment is used. Potentially many THttpServer instances with such thread can run fully independent. 2. Let run websocket handlers (THttpWSHandler) in separate threads. . Requires correct locking of shared resources. 3. Support special threads for sending data via websocket from server to clients. . When many clients are connected to the same TWebWindow, slowest client can . break down performance. With use of such specialized thread problem can be solved. 4. Implement all necessary locks and protections to use many TWebWindows in different user threads. . Provide TWebWindow::Run(double) method, which allows to run window code in any user thread. . Introduce tutorials/v7/draw_mt.cxx macro, which shows how three RCanvas instances can run. in three independent thread and regularly updated. Of course, default behavior should work - all functionality runs in main thread. . Means THttpServer requests processing, websockets processing, RCanvas handling - . everything runs in main application thread. Simple, but not scalable. This is very preliminary code , I still plan to change/extend some API. Any comments comments are welcome",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2487
https://github.com/root-project/root/pull/2487:1551,integrability,API,API,1551,"Muti-threading support in Web GUI classes; This is first step to make Web GUI code running in multiple threads. There are many threads introduced and many communication patterns are supported. 1. Provide special thread, which handle all http requests of THttpServer. This thread serves JavaScript/HTML files and redirects websocket requests to recipients. Try to reduce access to global ROOT structures from that thread - only list of classes for the moment is used. Potentially many THttpServer instances with such thread can run fully independent. 2. Let run websocket handlers (THttpWSHandler) in separate threads. . Requires correct locking of shared resources. 3. Support special threads for sending data via websocket from server to clients. . When many clients are connected to the same TWebWindow, slowest client can . break down performance. With use of such specialized thread problem can be solved. 4. Implement all necessary locks and protections to use many TWebWindows in different user threads. . Provide TWebWindow::Run(double) method, which allows to run window code in any user thread. . Introduce tutorials/v7/draw_mt.cxx macro, which shows how three RCanvas instances can run. in three independent thread and regularly updated. Of course, default behavior should work - all functionality runs in main thread. . Means THttpServer requests processing, websockets processing, RCanvas handling - . everything runs in main application thread. Simple, but not scalable. This is very preliminary code , I still plan to change/extend some API. Any comments comments are welcome",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2487
https://github.com/root-project/root/pull/2487:648,interoperability,share,shared,648,"Muti-threading support in Web GUI classes; This is first step to make Web GUI code running in multiple threads. There are many threads introduced and many communication patterns are supported. 1. Provide special thread, which handle all http requests of THttpServer. This thread serves JavaScript/HTML files and redirects websocket requests to recipients. Try to reduce access to global ROOT structures from that thread - only list of classes for the moment is used. Potentially many THttpServer instances with such thread can run fully independent. 2. Let run websocket handlers (THttpWSHandler) in separate threads. . Requires correct locking of shared resources. 3. Support special threads for sending data via websocket from server to clients. . When many clients are connected to the same TWebWindow, slowest client can . break down performance. With use of such specialized thread problem can be solved. 4. Implement all necessary locks and protections to use many TWebWindows in different user threads. . Provide TWebWindow::Run(double) method, which allows to run window code in any user thread. . Introduce tutorials/v7/draw_mt.cxx macro, which shows how three RCanvas instances can run. in three independent thread and regularly updated. Of course, default behavior should work - all functionality runs in main thread. . Means THttpServer requests processing, websockets processing, RCanvas handling - . everything runs in main application thread. Simple, but not scalable. This is very preliminary code , I still plan to change/extend some API. Any comments comments are welcome",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2487
https://github.com/root-project/root/pull/2487:1551,interoperability,API,API,1551,"Muti-threading support in Web GUI classes; This is first step to make Web GUI code running in multiple threads. There are many threads introduced and many communication patterns are supported. 1. Provide special thread, which handle all http requests of THttpServer. This thread serves JavaScript/HTML files and redirects websocket requests to recipients. Try to reduce access to global ROOT structures from that thread - only list of classes for the moment is used. Potentially many THttpServer instances with such thread can run fully independent. 2. Let run websocket handlers (THttpWSHandler) in separate threads. . Requires correct locking of shared resources. 3. Support special threads for sending data via websocket from server to clients. . When many clients are connected to the same TWebWindow, slowest client can . break down performance. With use of such specialized thread problem can be solved. 4. Implement all necessary locks and protections to use many TWebWindows in different user threads. . Provide TWebWindow::Run(double) method, which allows to run window code in any user thread. . Introduce tutorials/v7/draw_mt.cxx macro, which shows how three RCanvas instances can run. in three independent thread and regularly updated. Of course, default behavior should work - all functionality runs in main thread. . Means THttpServer requests processing, websockets processing, RCanvas handling - . everything runs in main application thread. Simple, but not scalable. This is very preliminary code , I still plan to change/extend some API. Any comments comments are welcome",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2487
https://github.com/root-project/root/pull/2487:1474,modifiability,scal,scalable,1474,"Muti-threading support in Web GUI classes; This is first step to make Web GUI code running in multiple threads. There are many threads introduced and many communication patterns are supported. 1. Provide special thread, which handle all http requests of THttpServer. This thread serves JavaScript/HTML files and redirects websocket requests to recipients. Try to reduce access to global ROOT structures from that thread - only list of classes for the moment is used. Potentially many THttpServer instances with such thread can run fully independent. 2. Let run websocket handlers (THttpWSHandler) in separate threads. . Requires correct locking of shared resources. 3. Support special threads for sending data via websocket from server to clients. . When many clients are connected to the same TWebWindow, slowest client can . break down performance. With use of such specialized thread problem can be solved. 4. Implement all necessary locks and protections to use many TWebWindows in different user threads. . Provide TWebWindow::Run(double) method, which allows to run window code in any user thread. . Introduce tutorials/v7/draw_mt.cxx macro, which shows how three RCanvas instances can run. in three independent thread and regularly updated. Of course, default behavior should work - all functionality runs in main thread. . Means THttpServer requests processing, websockets processing, RCanvas handling - . everything runs in main application thread. Simple, but not scalable. This is very preliminary code , I still plan to change/extend some API. Any comments comments are welcome",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2487
https://github.com/root-project/root/pull/2487:1539,modifiability,exten,extend,1539,"Muti-threading support in Web GUI classes; This is first step to make Web GUI code running in multiple threads. There are many threads introduced and many communication patterns are supported. 1. Provide special thread, which handle all http requests of THttpServer. This thread serves JavaScript/HTML files and redirects websocket requests to recipients. Try to reduce access to global ROOT structures from that thread - only list of classes for the moment is used. Potentially many THttpServer instances with such thread can run fully independent. 2. Let run websocket handlers (THttpWSHandler) in separate threads. . Requires correct locking of shared resources. 3. Support special threads for sending data via websocket from server to clients. . When many clients are connected to the same TWebWindow, slowest client can . break down performance. With use of such specialized thread problem can be solved. 4. Implement all necessary locks and protections to use many TWebWindows in different user threads. . Provide TWebWindow::Run(double) method, which allows to run window code in any user thread. . Introduce tutorials/v7/draw_mt.cxx macro, which shows how three RCanvas instances can run. in three independent thread and regularly updated. Of course, default behavior should work - all functionality runs in main thread. . Means THttpServer requests processing, websockets processing, RCanvas handling - . everything runs in main application thread. Simple, but not scalable. This is very preliminary code , I still plan to change/extend some API. Any comments comments are welcome",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2487
https://github.com/root-project/root/pull/2487:637,performance,lock,locking,637,"Muti-threading support in Web GUI classes; This is first step to make Web GUI code running in multiple threads. There are many threads introduced and many communication patterns are supported. 1. Provide special thread, which handle all http requests of THttpServer. This thread serves JavaScript/HTML files and redirects websocket requests to recipients. Try to reduce access to global ROOT structures from that thread - only list of classes for the moment is used. Potentially many THttpServer instances with such thread can run fully independent. 2. Let run websocket handlers (THttpWSHandler) in separate threads. . Requires correct locking of shared resources. 3. Support special threads for sending data via websocket from server to clients. . When many clients are connected to the same TWebWindow, slowest client can . break down performance. With use of such specialized thread problem can be solved. 4. Implement all necessary locks and protections to use many TWebWindows in different user threads. . Provide TWebWindow::Run(double) method, which allows to run window code in any user thread. . Introduce tutorials/v7/draw_mt.cxx macro, which shows how three RCanvas instances can run. in three independent thread and regularly updated. Of course, default behavior should work - all functionality runs in main thread. . Means THttpServer requests processing, websockets processing, RCanvas handling - . everything runs in main application thread. Simple, but not scalable. This is very preliminary code , I still plan to change/extend some API. Any comments comments are welcome",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2487
https://github.com/root-project/root/pull/2487:655,performance,resourc,resources,655,"Muti-threading support in Web GUI classes; This is first step to make Web GUI code running in multiple threads. There are many threads introduced and many communication patterns are supported. 1. Provide special thread, which handle all http requests of THttpServer. This thread serves JavaScript/HTML files and redirects websocket requests to recipients. Try to reduce access to global ROOT structures from that thread - only list of classes for the moment is used. Potentially many THttpServer instances with such thread can run fully independent. 2. Let run websocket handlers (THttpWSHandler) in separate threads. . Requires correct locking of shared resources. 3. Support special threads for sending data via websocket from server to clients. . When many clients are connected to the same TWebWindow, slowest client can . break down performance. With use of such specialized thread problem can be solved. 4. Implement all necessary locks and protections to use many TWebWindows in different user threads. . Provide TWebWindow::Run(double) method, which allows to run window code in any user thread. . Introduce tutorials/v7/draw_mt.cxx macro, which shows how three RCanvas instances can run. in three independent thread and regularly updated. Of course, default behavior should work - all functionality runs in main thread. . Means THttpServer requests processing, websockets processing, RCanvas handling - . everything runs in main application thread. Simple, but not scalable. This is very preliminary code , I still plan to change/extend some API. Any comments comments are welcome",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2487
https://github.com/root-project/root/pull/2487:838,performance,perform,performance,838,"Muti-threading support in Web GUI classes; This is first step to make Web GUI code running in multiple threads. There are many threads introduced and many communication patterns are supported. 1. Provide special thread, which handle all http requests of THttpServer. This thread serves JavaScript/HTML files and redirects websocket requests to recipients. Try to reduce access to global ROOT structures from that thread - only list of classes for the moment is used. Potentially many THttpServer instances with such thread can run fully independent. 2. Let run websocket handlers (THttpWSHandler) in separate threads. . Requires correct locking of shared resources. 3. Support special threads for sending data via websocket from server to clients. . When many clients are connected to the same TWebWindow, slowest client can . break down performance. With use of such specialized thread problem can be solved. 4. Implement all necessary locks and protections to use many TWebWindows in different user threads. . Provide TWebWindow::Run(double) method, which allows to run window code in any user thread. . Introduce tutorials/v7/draw_mt.cxx macro, which shows how three RCanvas instances can run. in three independent thread and regularly updated. Of course, default behavior should work - all functionality runs in main thread. . Means THttpServer requests processing, websockets processing, RCanvas handling - . everything runs in main application thread. Simple, but not scalable. This is very preliminary code , I still plan to change/extend some API. Any comments comments are welcome",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2487
https://github.com/root-project/root/pull/2487:937,performance,lock,locks,937,"Muti-threading support in Web GUI classes; This is first step to make Web GUI code running in multiple threads. There are many threads introduced and many communication patterns are supported. 1. Provide special thread, which handle all http requests of THttpServer. This thread serves JavaScript/HTML files and redirects websocket requests to recipients. Try to reduce access to global ROOT structures from that thread - only list of classes for the moment is used. Potentially many THttpServer instances with such thread can run fully independent. 2. Let run websocket handlers (THttpWSHandler) in separate threads. . Requires correct locking of shared resources. 3. Support special threads for sending data via websocket from server to clients. . When many clients are connected to the same TWebWindow, slowest client can . break down performance. With use of such specialized thread problem can be solved. 4. Implement all necessary locks and protections to use many TWebWindows in different user threads. . Provide TWebWindow::Run(double) method, which allows to run window code in any user thread. . Introduce tutorials/v7/draw_mt.cxx macro, which shows how three RCanvas instances can run. in three independent thread and regularly updated. Of course, default behavior should work - all functionality runs in main thread. . Means THttpServer requests processing, websockets processing, RCanvas handling - . everything runs in main application thread. Simple, but not scalable. This is very preliminary code , I still plan to change/extend some API. Any comments comments are welcome",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2487
https://github.com/root-project/root/pull/2487:1474,performance,scalab,scalable,1474,"Muti-threading support in Web GUI classes; This is first step to make Web GUI code running in multiple threads. There are many threads introduced and many communication patterns are supported. 1. Provide special thread, which handle all http requests of THttpServer. This thread serves JavaScript/HTML files and redirects websocket requests to recipients. Try to reduce access to global ROOT structures from that thread - only list of classes for the moment is used. Potentially many THttpServer instances with such thread can run fully independent. 2. Let run websocket handlers (THttpWSHandler) in separate threads. . Requires correct locking of shared resources. 3. Support special threads for sending data via websocket from server to clients. . When many clients are connected to the same TWebWindow, slowest client can . break down performance. With use of such specialized thread problem can be solved. 4. Implement all necessary locks and protections to use many TWebWindows in different user threads. . Provide TWebWindow::Run(double) method, which allows to run window code in any user thread. . Introduce tutorials/v7/draw_mt.cxx macro, which shows how three RCanvas instances can run. in three independent thread and regularly updated. Of course, default behavior should work - all functionality runs in main thread. . Means THttpServer requests processing, websockets processing, RCanvas handling - . everything runs in main application thread. Simple, but not scalable. This is very preliminary code , I still plan to change/extend some API. Any comments comments are welcome",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2487
https://github.com/root-project/root/pull/2487:806,reliability,slo,slowest,806,"Muti-threading support in Web GUI classes; This is first step to make Web GUI code running in multiple threads. There are many threads introduced and many communication patterns are supported. 1. Provide special thread, which handle all http requests of THttpServer. This thread serves JavaScript/HTML files and redirects websocket requests to recipients. Try to reduce access to global ROOT structures from that thread - only list of classes for the moment is used. Potentially many THttpServer instances with such thread can run fully independent. 2. Let run websocket handlers (THttpWSHandler) in separate threads. . Requires correct locking of shared resources. 3. Support special threads for sending data via websocket from server to clients. . When many clients are connected to the same TWebWindow, slowest client can . break down performance. With use of such specialized thread problem can be solved. 4. Implement all necessary locks and protections to use many TWebWindows in different user threads. . Provide TWebWindow::Run(double) method, which allows to run window code in any user thread. . Introduce tutorials/v7/draw_mt.cxx macro, which shows how three RCanvas instances can run. in three independent thread and regularly updated. Of course, default behavior should work - all functionality runs in main thread. . Means THttpServer requests processing, websockets processing, RCanvas handling - . everything runs in main application thread. Simple, but not scalable. This is very preliminary code , I still plan to change/extend some API. Any comments comments are welcome",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2487
https://github.com/root-project/root/pull/2487:1170,reliability,RCa,RCanvas,1170,"Muti-threading support in Web GUI classes; This is first step to make Web GUI code running in multiple threads. There are many threads introduced and many communication patterns are supported. 1. Provide special thread, which handle all http requests of THttpServer. This thread serves JavaScript/HTML files and redirects websocket requests to recipients. Try to reduce access to global ROOT structures from that thread - only list of classes for the moment is used. Potentially many THttpServer instances with such thread can run fully independent. 2. Let run websocket handlers (THttpWSHandler) in separate threads. . Requires correct locking of shared resources. 3. Support special threads for sending data via websocket from server to clients. . When many clients are connected to the same TWebWindow, slowest client can . break down performance. With use of such specialized thread problem can be solved. 4. Implement all necessary locks and protections to use many TWebWindows in different user threads. . Provide TWebWindow::Run(double) method, which allows to run window code in any user thread. . Introduce tutorials/v7/draw_mt.cxx macro, which shows how three RCanvas instances can run. in three independent thread and regularly updated. Of course, default behavior should work - all functionality runs in main thread. . Means THttpServer requests processing, websockets processing, RCanvas handling - . everything runs in main application thread. Simple, but not scalable. This is very preliminary code , I still plan to change/extend some API. Any comments comments are welcome",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2487
https://github.com/root-project/root/pull/2487:1393,reliability,RCa,RCanvas,1393,"Muti-threading support in Web GUI classes; This is first step to make Web GUI code running in multiple threads. There are many threads introduced and many communication patterns are supported. 1. Provide special thread, which handle all http requests of THttpServer. This thread serves JavaScript/HTML files and redirects websocket requests to recipients. Try to reduce access to global ROOT structures from that thread - only list of classes for the moment is used. Potentially many THttpServer instances with such thread can run fully independent. 2. Let run websocket handlers (THttpWSHandler) in separate threads. . Requires correct locking of shared resources. 3. Support special threads for sending data via websocket from server to clients. . When many clients are connected to the same TWebWindow, slowest client can . break down performance. With use of such specialized thread problem can be solved. 4. Implement all necessary locks and protections to use many TWebWindows in different user threads. . Provide TWebWindow::Run(double) method, which allows to run window code in any user thread. . Introduce tutorials/v7/draw_mt.cxx macro, which shows how three RCanvas instances can run. in three independent thread and regularly updated. Of course, default behavior should work - all functionality runs in main thread. . Means THttpServer requests processing, websockets processing, RCanvas handling - . everything runs in main application thread. Simple, but not scalable. This is very preliminary code , I still plan to change/extend some API. Any comments comments are welcome",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2487
https://github.com/root-project/root/pull/2487:655,safety,resourc,resources,655,"Muti-threading support in Web GUI classes; This is first step to make Web GUI code running in multiple threads. There are many threads introduced and many communication patterns are supported. 1. Provide special thread, which handle all http requests of THttpServer. This thread serves JavaScript/HTML files and redirects websocket requests to recipients. Try to reduce access to global ROOT structures from that thread - only list of classes for the moment is used. Potentially many THttpServer instances with such thread can run fully independent. 2. Let run websocket handlers (THttpWSHandler) in separate threads. . Requires correct locking of shared resources. 3. Support special threads for sending data via websocket from server to clients. . When many clients are connected to the same TWebWindow, slowest client can . break down performance. With use of such specialized thread problem can be solved. 4. Implement all necessary locks and protections to use many TWebWindows in different user threads. . Provide TWebWindow::Run(double) method, which allows to run window code in any user thread. . Introduce tutorials/v7/draw_mt.cxx macro, which shows how three RCanvas instances can run. in three independent thread and regularly updated. Of course, default behavior should work - all functionality runs in main thread. . Means THttpServer requests processing, websockets processing, RCanvas handling - . everything runs in main application thread. Simple, but not scalable. This is very preliminary code , I still plan to change/extend some API. Any comments comments are welcome",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2487
https://github.com/root-project/root/pull/2487:1239,safety,updat,updated,1239,"Muti-threading support in Web GUI classes; This is first step to make Web GUI code running in multiple threads. There are many threads introduced and many communication patterns are supported. 1. Provide special thread, which handle all http requests of THttpServer. This thread serves JavaScript/HTML files and redirects websocket requests to recipients. Try to reduce access to global ROOT structures from that thread - only list of classes for the moment is used. Potentially many THttpServer instances with such thread can run fully independent. 2. Let run websocket handlers (THttpWSHandler) in separate threads. . Requires correct locking of shared resources. 3. Support special threads for sending data via websocket from server to clients. . When many clients are connected to the same TWebWindow, slowest client can . break down performance. With use of such specialized thread problem can be solved. 4. Implement all necessary locks and protections to use many TWebWindows in different user threads. . Provide TWebWindow::Run(double) method, which allows to run window code in any user thread. . Introduce tutorials/v7/draw_mt.cxx macro, which shows how three RCanvas instances can run. in three independent thread and regularly updated. Of course, default behavior should work - all functionality runs in main thread. . Means THttpServer requests processing, websockets processing, RCanvas handling - . everything runs in main application thread. Simple, but not scalable. This is very preliminary code , I still plan to change/extend some API. Any comments comments are welcome",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2487
https://github.com/root-project/root/pull/2487:370,security,access,access,370,"Muti-threading support in Web GUI classes; This is first step to make Web GUI code running in multiple threads. There are many threads introduced and many communication patterns are supported. 1. Provide special thread, which handle all http requests of THttpServer. This thread serves JavaScript/HTML files and redirects websocket requests to recipients. Try to reduce access to global ROOT structures from that thread - only list of classes for the moment is used. Potentially many THttpServer instances with such thread can run fully independent. 2. Let run websocket handlers (THttpWSHandler) in separate threads. . Requires correct locking of shared resources. 3. Support special threads for sending data via websocket from server to clients. . When many clients are connected to the same TWebWindow, slowest client can . break down performance. With use of such specialized thread problem can be solved. 4. Implement all necessary locks and protections to use many TWebWindows in different user threads. . Provide TWebWindow::Run(double) method, which allows to run window code in any user thread. . Introduce tutorials/v7/draw_mt.cxx macro, which shows how three RCanvas instances can run. in three independent thread and regularly updated. Of course, default behavior should work - all functionality runs in main thread. . Means THttpServer requests processing, websockets processing, RCanvas handling - . everything runs in main application thread. Simple, but not scalable. This is very preliminary code , I still plan to change/extend some API. Any comments comments are welcome",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2487
https://github.com/root-project/root/pull/2487:637,security,lock,locking,637,"Muti-threading support in Web GUI classes; This is first step to make Web GUI code running in multiple threads. There are many threads introduced and many communication patterns are supported. 1. Provide special thread, which handle all http requests of THttpServer. This thread serves JavaScript/HTML files and redirects websocket requests to recipients. Try to reduce access to global ROOT structures from that thread - only list of classes for the moment is used. Potentially many THttpServer instances with such thread can run fully independent. 2. Let run websocket handlers (THttpWSHandler) in separate threads. . Requires correct locking of shared resources. 3. Support special threads for sending data via websocket from server to clients. . When many clients are connected to the same TWebWindow, slowest client can . break down performance. With use of such specialized thread problem can be solved. 4. Implement all necessary locks and protections to use many TWebWindows in different user threads. . Provide TWebWindow::Run(double) method, which allows to run window code in any user thread. . Introduce tutorials/v7/draw_mt.cxx macro, which shows how three RCanvas instances can run. in three independent thread and regularly updated. Of course, default behavior should work - all functionality runs in main thread. . Means THttpServer requests processing, websockets processing, RCanvas handling - . everything runs in main application thread. Simple, but not scalable. This is very preliminary code , I still plan to change/extend some API. Any comments comments are welcome",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2487
https://github.com/root-project/root/pull/2487:937,security,lock,locks,937,"Muti-threading support in Web GUI classes; This is first step to make Web GUI code running in multiple threads. There are many threads introduced and many communication patterns are supported. 1. Provide special thread, which handle all http requests of THttpServer. This thread serves JavaScript/HTML files and redirects websocket requests to recipients. Try to reduce access to global ROOT structures from that thread - only list of classes for the moment is used. Potentially many THttpServer instances with such thread can run fully independent. 2. Let run websocket handlers (THttpWSHandler) in separate threads. . Requires correct locking of shared resources. 3. Support special threads for sending data via websocket from server to clients. . When many clients are connected to the same TWebWindow, slowest client can . break down performance. With use of such specialized thread problem can be solved. 4. Implement all necessary locks and protections to use many TWebWindows in different user threads. . Provide TWebWindow::Run(double) method, which allows to run window code in any user thread. . Introduce tutorials/v7/draw_mt.cxx macro, which shows how three RCanvas instances can run. in three independent thread and regularly updated. Of course, default behavior should work - all functionality runs in main thread. . Means THttpServer requests processing, websockets processing, RCanvas handling - . everything runs in main application thread. Simple, but not scalable. This is very preliminary code , I still plan to change/extend some API. Any comments comments are welcome",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2487
https://github.com/root-project/root/pull/2487:1239,security,updat,updated,1239,"Muti-threading support in Web GUI classes; This is first step to make Web GUI code running in multiple threads. There are many threads introduced and many communication patterns are supported. 1. Provide special thread, which handle all http requests of THttpServer. This thread serves JavaScript/HTML files and redirects websocket requests to recipients. Try to reduce access to global ROOT structures from that thread - only list of classes for the moment is used. Potentially many THttpServer instances with such thread can run fully independent. 2. Let run websocket handlers (THttpWSHandler) in separate threads. . Requires correct locking of shared resources. 3. Support special threads for sending data via websocket from server to clients. . When many clients are connected to the same TWebWindow, slowest client can . break down performance. With use of such specialized thread problem can be solved. 4. Implement all necessary locks and protections to use many TWebWindows in different user threads. . Provide TWebWindow::Run(double) method, which allows to run window code in any user thread. . Introduce tutorials/v7/draw_mt.cxx macro, which shows how three RCanvas instances can run. in three independent thread and regularly updated. Of course, default behavior should work - all functionality runs in main thread. . Means THttpServer requests processing, websockets processing, RCanvas handling - . everything runs in main application thread. Simple, but not scalable. This is very preliminary code , I still plan to change/extend some API. Any comments comments are welcome",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2487
https://github.com/root-project/root/pull/2487:655,testability,resourc,resources,655,"Muti-threading support in Web GUI classes; This is first step to make Web GUI code running in multiple threads. There are many threads introduced and many communication patterns are supported. 1. Provide special thread, which handle all http requests of THttpServer. This thread serves JavaScript/HTML files and redirects websocket requests to recipients. Try to reduce access to global ROOT structures from that thread - only list of classes for the moment is used. Potentially many THttpServer instances with such thread can run fully independent. 2. Let run websocket handlers (THttpWSHandler) in separate threads. . Requires correct locking of shared resources. 3. Support special threads for sending data via websocket from server to clients. . When many clients are connected to the same TWebWindow, slowest client can . break down performance. With use of such specialized thread problem can be solved. 4. Implement all necessary locks and protections to use many TWebWindows in different user threads. . Provide TWebWindow::Run(double) method, which allows to run window code in any user thread. . Introduce tutorials/v7/draw_mt.cxx macro, which shows how three RCanvas instances can run. in three independent thread and regularly updated. Of course, default behavior should work - all functionality runs in main thread. . Means THttpServer requests processing, websockets processing, RCanvas handling - . everything runs in main application thread. Simple, but not scalable. This is very preliminary code , I still plan to change/extend some API. Any comments comments are welcome",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2487
https://github.com/root-project/root/pull/2487:1458,testability,Simpl,Simple,1458,"Muti-threading support in Web GUI classes; This is first step to make Web GUI code running in multiple threads. There are many threads introduced and many communication patterns are supported. 1. Provide special thread, which handle all http requests of THttpServer. This thread serves JavaScript/HTML files and redirects websocket requests to recipients. Try to reduce access to global ROOT structures from that thread - only list of classes for the moment is used. Potentially many THttpServer instances with such thread can run fully independent. 2. Let run websocket handlers (THttpWSHandler) in separate threads. . Requires correct locking of shared resources. 3. Support special threads for sending data via websocket from server to clients. . When many clients are connected to the same TWebWindow, slowest client can . break down performance. With use of such specialized thread problem can be solved. 4. Implement all necessary locks and protections to use many TWebWindows in different user threads. . Provide TWebWindow::Run(double) method, which allows to run window code in any user thread. . Introduce tutorials/v7/draw_mt.cxx macro, which shows how three RCanvas instances can run. in three independent thread and regularly updated. Of course, default behavior should work - all functionality runs in main thread. . Means THttpServer requests processing, websockets processing, RCanvas handling - . everything runs in main application thread. Simple, but not scalable. This is very preliminary code , I still plan to change/extend some API. Any comments comments are welcome",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2487
https://github.com/root-project/root/pull/2487:1524,testability,plan,plan,1524,"Muti-threading support in Web GUI classes; This is first step to make Web GUI code running in multiple threads. There are many threads introduced and many communication patterns are supported. 1. Provide special thread, which handle all http requests of THttpServer. This thread serves JavaScript/HTML files and redirects websocket requests to recipients. Try to reduce access to global ROOT structures from that thread - only list of classes for the moment is used. Potentially many THttpServer instances with such thread can run fully independent. 2. Let run websocket handlers (THttpWSHandler) in separate threads. . Requires correct locking of shared resources. 3. Support special threads for sending data via websocket from server to clients. . When many clients are connected to the same TWebWindow, slowest client can . break down performance. With use of such specialized thread problem can be solved. 4. Implement all necessary locks and protections to use many TWebWindows in different user threads. . Provide TWebWindow::Run(double) method, which allows to run window code in any user thread. . Introduce tutorials/v7/draw_mt.cxx macro, which shows how three RCanvas instances can run. in three independent thread and regularly updated. Of course, default behavior should work - all functionality runs in main thread. . Means THttpServer requests processing, websockets processing, RCanvas handling - . everything runs in main application thread. Simple, but not scalable. This is very preliminary code , I still plan to change/extend some API. Any comments comments are welcome",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2487
https://github.com/root-project/root/pull/2487:15,usability,support,support,15,"Muti-threading support in Web GUI classes; This is first step to make Web GUI code running in multiple threads. There are many threads introduced and many communication patterns are supported. 1. Provide special thread, which handle all http requests of THttpServer. This thread serves JavaScript/HTML files and redirects websocket requests to recipients. Try to reduce access to global ROOT structures from that thread - only list of classes for the moment is used. Potentially many THttpServer instances with such thread can run fully independent. 2. Let run websocket handlers (THttpWSHandler) in separate threads. . Requires correct locking of shared resources. 3. Support special threads for sending data via websocket from server to clients. . When many clients are connected to the same TWebWindow, slowest client can . break down performance. With use of such specialized thread problem can be solved. 4. Implement all necessary locks and protections to use many TWebWindows in different user threads. . Provide TWebWindow::Run(double) method, which allows to run window code in any user thread. . Introduce tutorials/v7/draw_mt.cxx macro, which shows how three RCanvas instances can run. in three independent thread and regularly updated. Of course, default behavior should work - all functionality runs in main thread. . Means THttpServer requests processing, websockets processing, RCanvas handling - . everything runs in main application thread. Simple, but not scalable. This is very preliminary code , I still plan to change/extend some API. Any comments comments are welcome",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2487
https://github.com/root-project/root/pull/2487:182,usability,support,supported,182,"Muti-threading support in Web GUI classes; This is first step to make Web GUI code running in multiple threads. There are many threads introduced and many communication patterns are supported. 1. Provide special thread, which handle all http requests of THttpServer. This thread serves JavaScript/HTML files and redirects websocket requests to recipients. Try to reduce access to global ROOT structures from that thread - only list of classes for the moment is used. Potentially many THttpServer instances with such thread can run fully independent. 2. Let run websocket handlers (THttpWSHandler) in separate threads. . Requires correct locking of shared resources. 3. Support special threads for sending data via websocket from server to clients. . When many clients are connected to the same TWebWindow, slowest client can . break down performance. With use of such specialized thread problem can be solved. 4. Implement all necessary locks and protections to use many TWebWindows in different user threads. . Provide TWebWindow::Run(double) method, which allows to run window code in any user thread. . Introduce tutorials/v7/draw_mt.cxx macro, which shows how three RCanvas instances can run. in three independent thread and regularly updated. Of course, default behavior should work - all functionality runs in main thread. . Means THttpServer requests processing, websockets processing, RCanvas handling - . everything runs in main application thread. Simple, but not scalable. This is very preliminary code , I still plan to change/extend some API. Any comments comments are welcome",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2487
https://github.com/root-project/root/pull/2487:669,usability,Support,Support,669,"Muti-threading support in Web GUI classes; This is first step to make Web GUI code running in multiple threads. There are many threads introduced and many communication patterns are supported. 1. Provide special thread, which handle all http requests of THttpServer. This thread serves JavaScript/HTML files and redirects websocket requests to recipients. Try to reduce access to global ROOT structures from that thread - only list of classes for the moment is used. Potentially many THttpServer instances with such thread can run fully independent. 2. Let run websocket handlers (THttpWSHandler) in separate threads. . Requires correct locking of shared resources. 3. Support special threads for sending data via websocket from server to clients. . When many clients are connected to the same TWebWindow, slowest client can . break down performance. With use of such specialized thread problem can be solved. 4. Implement all necessary locks and protections to use many TWebWindows in different user threads. . Provide TWebWindow::Run(double) method, which allows to run window code in any user thread. . Introduce tutorials/v7/draw_mt.cxx macro, which shows how three RCanvas instances can run. in three independent thread and regularly updated. Of course, default behavior should work - all functionality runs in main thread. . Means THttpServer requests processing, websockets processing, RCanvas handling - . everything runs in main application thread. Simple, but not scalable. This is very preliminary code , I still plan to change/extend some API. Any comments comments are welcome",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2487
https://github.com/root-project/root/pull/2487:838,usability,perform,performance,838,"Muti-threading support in Web GUI classes; This is first step to make Web GUI code running in multiple threads. There are many threads introduced and many communication patterns are supported. 1. Provide special thread, which handle all http requests of THttpServer. This thread serves JavaScript/HTML files and redirects websocket requests to recipients. Try to reduce access to global ROOT structures from that thread - only list of classes for the moment is used. Potentially many THttpServer instances with such thread can run fully independent. 2. Let run websocket handlers (THttpWSHandler) in separate threads. . Requires correct locking of shared resources. 3. Support special threads for sending data via websocket from server to clients. . When many clients are connected to the same TWebWindow, slowest client can . break down performance. With use of such specialized thread problem can be solved. 4. Implement all necessary locks and protections to use many TWebWindows in different user threads. . Provide TWebWindow::Run(double) method, which allows to run window code in any user thread. . Introduce tutorials/v7/draw_mt.cxx macro, which shows how three RCanvas instances can run. in three independent thread and regularly updated. Of course, default behavior should work - all functionality runs in main thread. . Means THttpServer requests processing, websockets processing, RCanvas handling - . everything runs in main application thread. Simple, but not scalable. This is very preliminary code , I still plan to change/extend some API. Any comments comments are welcome",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2487
https://github.com/root-project/root/pull/2487:996,usability,user,user,996,"Muti-threading support in Web GUI classes; This is first step to make Web GUI code running in multiple threads. There are many threads introduced and many communication patterns are supported. 1. Provide special thread, which handle all http requests of THttpServer. This thread serves JavaScript/HTML files and redirects websocket requests to recipients. Try to reduce access to global ROOT structures from that thread - only list of classes for the moment is used. Potentially many THttpServer instances with such thread can run fully independent. 2. Let run websocket handlers (THttpWSHandler) in separate threads. . Requires correct locking of shared resources. 3. Support special threads for sending data via websocket from server to clients. . When many clients are connected to the same TWebWindow, slowest client can . break down performance. With use of such specialized thread problem can be solved. 4. Implement all necessary locks and protections to use many TWebWindows in different user threads. . Provide TWebWindow::Run(double) method, which allows to run window code in any user thread. . Introduce tutorials/v7/draw_mt.cxx macro, which shows how three RCanvas instances can run. in three independent thread and regularly updated. Of course, default behavior should work - all functionality runs in main thread. . Means THttpServer requests processing, websockets processing, RCanvas handling - . everything runs in main application thread. Simple, but not scalable. This is very preliminary code , I still plan to change/extend some API. Any comments comments are welcome",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2487
https://github.com/root-project/root/pull/2487:1091,usability,user,user,1091,"Muti-threading support in Web GUI classes; This is first step to make Web GUI code running in multiple threads. There are many threads introduced and many communication patterns are supported. 1. Provide special thread, which handle all http requests of THttpServer. This thread serves JavaScript/HTML files and redirects websocket requests to recipients. Try to reduce access to global ROOT structures from that thread - only list of classes for the moment is used. Potentially many THttpServer instances with such thread can run fully independent. 2. Let run websocket handlers (THttpWSHandler) in separate threads. . Requires correct locking of shared resources. 3. Support special threads for sending data via websocket from server to clients. . When many clients are connected to the same TWebWindow, slowest client can . break down performance. With use of such specialized thread problem can be solved. 4. Implement all necessary locks and protections to use many TWebWindows in different user threads. . Provide TWebWindow::Run(double) method, which allows to run window code in any user thread. . Introduce tutorials/v7/draw_mt.cxx macro, which shows how three RCanvas instances can run. in three independent thread and regularly updated. Of course, default behavior should work - all functionality runs in main thread. . Means THttpServer requests processing, websockets processing, RCanvas handling - . everything runs in main application thread. Simple, but not scalable. This is very preliminary code , I still plan to change/extend some API. Any comments comments are welcome",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2487
https://github.com/root-project/root/pull/2487:1267,usability,behavi,behavior,1267,"Muti-threading support in Web GUI classes; This is first step to make Web GUI code running in multiple threads. There are many threads introduced and many communication patterns are supported. 1. Provide special thread, which handle all http requests of THttpServer. This thread serves JavaScript/HTML files and redirects websocket requests to recipients. Try to reduce access to global ROOT structures from that thread - only list of classes for the moment is used. Potentially many THttpServer instances with such thread can run fully independent. 2. Let run websocket handlers (THttpWSHandler) in separate threads. . Requires correct locking of shared resources. 3. Support special threads for sending data via websocket from server to clients. . When many clients are connected to the same TWebWindow, slowest client can . break down performance. With use of such specialized thread problem can be solved. 4. Implement all necessary locks and protections to use many TWebWindows in different user threads. . Provide TWebWindow::Run(double) method, which allows to run window code in any user thread. . Introduce tutorials/v7/draw_mt.cxx macro, which shows how three RCanvas instances can run. in three independent thread and regularly updated. Of course, default behavior should work - all functionality runs in main thread. . Means THttpServer requests processing, websockets processing, RCanvas handling - . everything runs in main application thread. Simple, but not scalable. This is very preliminary code , I still plan to change/extend some API. Any comments comments are welcome",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2487
https://github.com/root-project/root/pull/2487:1458,usability,Simpl,Simple,1458,"Muti-threading support in Web GUI classes; This is first step to make Web GUI code running in multiple threads. There are many threads introduced and many communication patterns are supported. 1. Provide special thread, which handle all http requests of THttpServer. This thread serves JavaScript/HTML files and redirects websocket requests to recipients. Try to reduce access to global ROOT structures from that thread - only list of classes for the moment is used. Potentially many THttpServer instances with such thread can run fully independent. 2. Let run websocket handlers (THttpWSHandler) in separate threads. . Requires correct locking of shared resources. 3. Support special threads for sending data via websocket from server to clients. . When many clients are connected to the same TWebWindow, slowest client can . break down performance. With use of such specialized thread problem can be solved. 4. Implement all necessary locks and protections to use many TWebWindows in different user threads. . Provide TWebWindow::Run(double) method, which allows to run window code in any user thread. . Introduce tutorials/v7/draw_mt.cxx macro, which shows how three RCanvas instances can run. in three independent thread and regularly updated. Of course, default behavior should work - all functionality runs in main thread. . Means THttpServer requests processing, websockets processing, RCanvas handling - . everything runs in main application thread. Simple, but not scalable. This is very preliminary code , I still plan to change/extend some API. Any comments comments are welcome",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2487
https://github.com/root-project/root/pull/2488:298,deployability,stage,stage,298,"v614: Additional fix for rule scheduling in TBranchElement; This is a backport of #2482 . This is an additional fix for cms-sw/cmssw#22594. If one of the class associated with a TBranchElement has a base class and/or sub-object class that. has a rule that requires a cache (onfileObject) object to stage the original data but does not. have branch in the TTree that also neeed the same cache object, we need to associate a cache. object with the StreamerInfoAction sequence that needs it (associated with a higher level branch). and give it ownership of this cache object)",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2488
https://github.com/root-project/root/pull/2488:30,energy efficiency,schedul,scheduling,30,"v614: Additional fix for rule scheduling in TBranchElement; This is a backport of #2482 . This is an additional fix for cms-sw/cmssw#22594. If one of the class associated with a TBranchElement has a base class and/or sub-object class that. has a rule that requires a cache (onfileObject) object to stage the original data but does not. have branch in the TTree that also neeed the same cache object, we need to associate a cache. object with the StreamerInfoAction sequence that needs it (associated with a higher level branch). and give it ownership of this cache object)",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2488
https://github.com/root-project/root/pull/2488:217,integrability,sub,sub-object,217,"v614: Additional fix for rule scheduling in TBranchElement; This is a backport of #2482 . This is an additional fix for cms-sw/cmssw#22594. If one of the class associated with a TBranchElement has a base class and/or sub-object class that. has a rule that requires a cache (onfileObject) object to stage the original data but does not. have branch in the TTree that also neeed the same cache object, we need to associate a cache. object with the StreamerInfoAction sequence that needs it (associated with a higher level branch). and give it ownership of this cache object)",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2488
https://github.com/root-project/root/pull/2488:30,performance,schedul,scheduling,30,"v614: Additional fix for rule scheduling in TBranchElement; This is a backport of #2482 . This is an additional fix for cms-sw/cmssw#22594. If one of the class associated with a TBranchElement has a base class and/or sub-object class that. has a rule that requires a cache (onfileObject) object to stage the original data but does not. have branch in the TTree that also neeed the same cache object, we need to associate a cache. object with the StreamerInfoAction sequence that needs it (associated with a higher level branch). and give it ownership of this cache object)",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2488
https://github.com/root-project/root/pull/2488:267,performance,cach,cache,267,"v614: Additional fix for rule scheduling in TBranchElement; This is a backport of #2482 . This is an additional fix for cms-sw/cmssw#22594. If one of the class associated with a TBranchElement has a base class and/or sub-object class that. has a rule that requires a cache (onfileObject) object to stage the original data but does not. have branch in the TTree that also neeed the same cache object, we need to associate a cache. object with the StreamerInfoAction sequence that needs it (associated with a higher level branch). and give it ownership of this cache object)",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2488
https://github.com/root-project/root/pull/2488:386,performance,cach,cache,386,"v614: Additional fix for rule scheduling in TBranchElement; This is a backport of #2482 . This is an additional fix for cms-sw/cmssw#22594. If one of the class associated with a TBranchElement has a base class and/or sub-object class that. has a rule that requires a cache (onfileObject) object to stage the original data but does not. have branch in the TTree that also neeed the same cache object, we need to associate a cache. object with the StreamerInfoAction sequence that needs it (associated with a higher level branch). and give it ownership of this cache object)",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2488
https://github.com/root-project/root/pull/2488:423,performance,cach,cache,423,"v614: Additional fix for rule scheduling in TBranchElement; This is a backport of #2482 . This is an additional fix for cms-sw/cmssw#22594. If one of the class associated with a TBranchElement has a base class and/or sub-object class that. has a rule that requires a cache (onfileObject) object to stage the original data but does not. have branch in the TTree that also neeed the same cache object, we need to associate a cache. object with the StreamerInfoAction sequence that needs it (associated with a higher level branch). and give it ownership of this cache object)",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2488
https://github.com/root-project/root/pull/2488:559,performance,cach,cache,559,"v614: Additional fix for rule scheduling in TBranchElement; This is a backport of #2482 . This is an additional fix for cms-sw/cmssw#22594. If one of the class associated with a TBranchElement has a base class and/or sub-object class that. has a rule that requires a cache (onfileObject) object to stage the original data but does not. have branch in the TTree that also neeed the same cache object, we need to associate a cache. object with the StreamerInfoAction sequence that needs it (associated with a higher level branch). and give it ownership of this cache object)",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2488
https://github.com/root-project/root/pull/2488:326,reliability,doe,does,326,"v614: Additional fix for rule scheduling in TBranchElement; This is a backport of #2482 . This is an additional fix for cms-sw/cmssw#22594. If one of the class associated with a TBranchElement has a base class and/or sub-object class that. has a rule that requires a cache (onfileObject) object to stage the original data but does not. have branch in the TTree that also neeed the same cache object, we need to associate a cache. object with the StreamerInfoAction sequence that needs it (associated with a higher level branch). and give it ownership of this cache object)",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2488
https://github.com/root-project/root/pull/2490:71,availability,error,errors,71,[VecOps] Add missing push_back signature; This allows to avoid awkward errors due to the absence of moves.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2490
https://github.com/root-project/root/pull/2490:71,performance,error,errors,71,[VecOps] Add missing push_back signature; This allows to avoid awkward errors due to the absence of moves.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2490
https://github.com/root-project/root/pull/2490:57,safety,avoid,avoid,57,[VecOps] Add missing push_back signature; This allows to avoid awkward errors due to the absence of moves.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2490
https://github.com/root-project/root/pull/2490:71,safety,error,errors,71,[VecOps] Add missing push_back signature; This allows to avoid awkward errors due to the absence of moves.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2490
https://github.com/root-project/root/pull/2490:31,security,sign,signature,31,[VecOps] Add missing push_back signature; This allows to avoid awkward errors due to the absence of moves.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2490
https://github.com/root-project/root/pull/2490:71,usability,error,errors,71,[VecOps] Add missing push_back signature; This allows to avoid awkward errors due to the absence of moves.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2490
https://github.com/root-project/root/pull/2491:56,deployability,modul,modules,56,"[D47118] Backport r337353.; Original commit message:. ""[modules] Print input files when -module-file-info file switch is passed. This patch improves traceability of duplicated header files which end up in multiple pcms. """,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2491
https://github.com/root-project/root/pull/2491:89,deployability,modul,module-file-info,89,"[D47118] Backport r337353.; Original commit message:. ""[modules] Print input files when -module-file-info file switch is passed. This patch improves traceability of duplicated header files which end up in multiple pcms. """,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2491
https://github.com/root-project/root/pull/2491:134,deployability,patch,patch,134,"[D47118] Backport r337353.; Original commit message:. ""[modules] Print input files when -module-file-info file switch is passed. This patch improves traceability of duplicated header files which end up in multiple pcms. """,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2491
https://github.com/root-project/root/pull/2491:44,integrability,messag,message,44,"[D47118] Backport r337353.; Original commit message:. ""[modules] Print input files when -module-file-info file switch is passed. This patch improves traceability of duplicated header files which end up in multiple pcms. """,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2491
https://github.com/root-project/root/pull/2491:44,interoperability,messag,message,44,"[D47118] Backport r337353.; Original commit message:. ""[modules] Print input files when -module-file-info file switch is passed. This patch improves traceability of duplicated header files which end up in multiple pcms. """,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2491
https://github.com/root-project/root/pull/2491:56,modifiability,modul,modules,56,"[D47118] Backport r337353.; Original commit message:. ""[modules] Print input files when -module-file-info file switch is passed. This patch improves traceability of duplicated header files which end up in multiple pcms. """,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2491
https://github.com/root-project/root/pull/2491:89,modifiability,modul,module-file-info,89,"[D47118] Backport r337353.; Original commit message:. ""[modules] Print input files when -module-file-info file switch is passed. This patch improves traceability of duplicated header files which end up in multiple pcms. """,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2491
https://github.com/root-project/root/pull/2491:56,safety,modul,modules,56,"[D47118] Backport r337353.; Original commit message:. ""[modules] Print input files when -module-file-info file switch is passed. This patch improves traceability of duplicated header files which end up in multiple pcms. """,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2491
https://github.com/root-project/root/pull/2491:71,safety,input,input,71,"[D47118] Backport r337353.; Original commit message:. ""[modules] Print input files when -module-file-info file switch is passed. This patch improves traceability of duplicated header files which end up in multiple pcms. """,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2491
https://github.com/root-project/root/pull/2491:89,safety,modul,module-file-info,89,"[D47118] Backport r337353.; Original commit message:. ""[modules] Print input files when -module-file-info file switch is passed. This patch improves traceability of duplicated header files which end up in multiple pcms. """,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2491
https://github.com/root-project/root/pull/2491:134,safety,patch,patch,134,"[D47118] Backport r337353.; Original commit message:. ""[modules] Print input files when -module-file-info file switch is passed. This patch improves traceability of duplicated header files which end up in multiple pcms. """,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2491
https://github.com/root-project/root/pull/2491:134,security,patch,patch,134,"[D47118] Backport r337353.; Original commit message:. ""[modules] Print input files when -module-file-info file switch is passed. This patch improves traceability of duplicated header files which end up in multiple pcms. """,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2491
https://github.com/root-project/root/pull/2491:149,testability,trace,traceability,149,"[D47118] Backport r337353.; Original commit message:. ""[modules] Print input files when -module-file-info file switch is passed. This patch improves traceability of duplicated header files which end up in multiple pcms. """,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2491
https://github.com/root-project/root/pull/2491:71,usability,input,input,71,"[D47118] Backport r337353.; Original commit message:. ""[modules] Print input files when -module-file-info file switch is passed. This patch improves traceability of duplicated header files which end up in multiple pcms. """,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2491
https://github.com/root-project/root/pull/2492:48,deployability,patch,patches,48,Changing default compression algorithm for 6-14-patches to zlib;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2492
https://github.com/root-project/root/pull/2492:48,safety,patch,patches,48,Changing default compression algorithm for 6-14-patches to zlib;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2492
https://github.com/root-project/root/pull/2492:48,security,patch,patches,48,Changing default compression algorithm for 6-14-patches to zlib;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2492
https://github.com/root-project/root/pull/2493:0,usability,Support,Support,0,Support unsigned integer types in RArrowDS;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2493
https://github.com/root-project/root/pull/2494:9,deployability,patch,patches,9,v6-14-00-patches: Support unsigned integer types in RArrowDS;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2494
https://github.com/root-project/root/pull/2494:9,safety,patch,patches,9,v6-14-00-patches: Support unsigned integer types in RArrowDS;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2494
https://github.com/root-project/root/pull/2494:9,security,patch,patches,9,v6-14-00-patches: Support unsigned integer types in RArrowDS;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2494
https://github.com/root-project/root/pull/2494:18,usability,Support,Support,18,v6-14-00-patches: Support unsigned integer types in RArrowDS;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2494
https://github.com/root-project/root/pull/2495:187,deployability,fail,failing,187,[PyROOT] Add all STL vector pythonizations to RVecs; Add all STL vector pythonizations to RVec. ~Possibly needs further unit-tests.~ Added further unit-tests. This ~should fix~ fixes the failing iterating over RVecs in Python on mac machines. Made selection of appropriate pythonizations more fool proof (`RVec<RVec<T>>` was treated similar to `RVec<T>` due to lazy string magic). Thanks @amadio!,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2495
https://github.com/root-project/root/pull/2495:187,reliability,fail,failing,187,[PyROOT] Add all STL vector pythonizations to RVecs; Add all STL vector pythonizations to RVec. ~Possibly needs further unit-tests.~ Added further unit-tests. This ~should fix~ fixes the failing iterating over RVecs in Python on mac machines. Made selection of appropriate pythonizations more fool proof (`RVec<RVec<T>>` was treated similar to `RVec<T>` due to lazy string magic). Thanks @amadio!,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2495
https://github.com/root-project/root/pull/2495:125,safety,test,tests,125,[PyROOT] Add all STL vector pythonizations to RVecs; Add all STL vector pythonizations to RVec. ~Possibly needs further unit-tests.~ Added further unit-tests. This ~should fix~ fixes the failing iterating over RVecs in Python on mac machines. Made selection of appropriate pythonizations more fool proof (`RVec<RVec<T>>` was treated similar to `RVec<T>` due to lazy string magic). Thanks @amadio!,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2495
https://github.com/root-project/root/pull/2495:152,safety,test,tests,152,[PyROOT] Add all STL vector pythonizations to RVecs; Add all STL vector pythonizations to RVec. ~Possibly needs further unit-tests.~ Added further unit-tests. This ~should fix~ fixes the failing iterating over RVecs in Python on mac machines. Made selection of appropriate pythonizations more fool proof (`RVec<RVec<T>>` was treated similar to `RVec<T>` due to lazy string magic). Thanks @amadio!,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2495
https://github.com/root-project/root/pull/2495:120,testability,unit,unit-tests,120,[PyROOT] Add all STL vector pythonizations to RVecs; Add all STL vector pythonizations to RVec. ~Possibly needs further unit-tests.~ Added further unit-tests. This ~should fix~ fixes the failing iterating over RVecs in Python on mac machines. Made selection of appropriate pythonizations more fool proof (`RVec<RVec<T>>` was treated similar to `RVec<T>` due to lazy string magic). Thanks @amadio!,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2495
https://github.com/root-project/root/pull/2495:147,testability,unit,unit-tests,147,[PyROOT] Add all STL vector pythonizations to RVecs; Add all STL vector pythonizations to RVec. ~Possibly needs further unit-tests.~ Added further unit-tests. This ~should fix~ fixes the failing iterating over RVecs in Python on mac machines. Made selection of appropriate pythonizations more fool proof (`RVec<RVec<T>>` was treated similar to `RVec<T>` due to lazy string magic). Thanks @amadio!,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2495
https://github.com/root-project/root/pull/2496:0,security,Modif,Modify,0,Modify python; added the modifications to the python files,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2496
https://github.com/root-project/root/pull/2496:25,security,modif,modifications,25,Modify python; added the modifications to the python files,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2496
https://github.com/root-project/root/pull/2497:22,safety,Prevent,Prevent,22,[Experimental PyROOT] Prevent wrong selection of RVec and STL vector …; …pythonizations,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2497
https://github.com/root-project/root/pull/2497:22,security,Preven,Prevent,22,[Experimental PyROOT] Prevent wrong selection of RVec and STL vector …; …pythonizations,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2497
https://github.com/root-project/root/pull/2498:68,deployability,build,build,68,[clad] Specify where the clang header files are.; In cases where we build ROOT with -Dbuiltin_llvm=Off -Dbuiltin_clang=On and we have installed both llvm and clang in /usr/ clad will pick up the clang headers from there too. This patch gives higher priority to the header files which ROOT is supposed to use. It fixes a very obscure initialization issue due to different versions of the ASTContext.h installed and used by ROOT.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2498
https://github.com/root-project/root/pull/2498:134,deployability,instal,installed,134,[clad] Specify where the clang header files are.; In cases where we build ROOT with -Dbuiltin_llvm=Off -Dbuiltin_clang=On and we have installed both llvm and clang in /usr/ clad will pick up the clang headers from there too. This patch gives higher priority to the header files which ROOT is supposed to use. It fixes a very obscure initialization issue due to different versions of the ASTContext.h installed and used by ROOT.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2498
https://github.com/root-project/root/pull/2498:230,deployability,patch,patch,230,[clad] Specify where the clang header files are.; In cases where we build ROOT with -Dbuiltin_llvm=Off -Dbuiltin_clang=On and we have installed both llvm and clang in /usr/ clad will pick up the clang headers from there too. This patch gives higher priority to the header files which ROOT is supposed to use. It fixes a very obscure initialization issue due to different versions of the ASTContext.h installed and used by ROOT.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2498
https://github.com/root-project/root/pull/2498:371,deployability,version,versions,371,[clad] Specify where the clang header files are.; In cases where we build ROOT with -Dbuiltin_llvm=Off -Dbuiltin_clang=On and we have installed both llvm and clang in /usr/ clad will pick up the clang headers from there too. This patch gives higher priority to the header files which ROOT is supposed to use. It fixes a very obscure initialization issue due to different versions of the ASTContext.h installed and used by ROOT.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2498
https://github.com/root-project/root/pull/2498:400,deployability,instal,installed,400,[clad] Specify where the clang header files are.; In cases where we build ROOT with -Dbuiltin_llvm=Off -Dbuiltin_clang=On and we have installed both llvm and clang in /usr/ clad will pick up the clang headers from there too. This patch gives higher priority to the header files which ROOT is supposed to use. It fixes a very obscure initialization issue due to different versions of the ASTContext.h installed and used by ROOT.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2498
https://github.com/root-project/root/pull/2498:371,integrability,version,versions,371,[clad] Specify where the clang header files are.; In cases where we build ROOT with -Dbuiltin_llvm=Off -Dbuiltin_clang=On and we have installed both llvm and clang in /usr/ clad will pick up the clang headers from there too. This patch gives higher priority to the header files which ROOT is supposed to use. It fixes a very obscure initialization issue due to different versions of the ASTContext.h installed and used by ROOT.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2498
https://github.com/root-project/root/pull/2498:7,interoperability,Specif,Specify,7,[clad] Specify where the clang header files are.; In cases where we build ROOT with -Dbuiltin_llvm=Off -Dbuiltin_clang=On and we have installed both llvm and clang in /usr/ clad will pick up the clang headers from there too. This patch gives higher priority to the header files which ROOT is supposed to use. It fixes a very obscure initialization issue due to different versions of the ASTContext.h installed and used by ROOT.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2498
https://github.com/root-project/root/pull/2498:371,modifiability,version,versions,371,[clad] Specify where the clang header files are.; In cases where we build ROOT with -Dbuiltin_llvm=Off -Dbuiltin_clang=On and we have installed both llvm and clang in /usr/ clad will pick up the clang headers from there too. This patch gives higher priority to the header files which ROOT is supposed to use. It fixes a very obscure initialization issue due to different versions of the ASTContext.h installed and used by ROOT.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2498
https://github.com/root-project/root/pull/2498:230,safety,patch,patch,230,[clad] Specify where the clang header files are.; In cases where we build ROOT with -Dbuiltin_llvm=Off -Dbuiltin_clang=On and we have installed both llvm and clang in /usr/ clad will pick up the clang headers from there too. This patch gives higher priority to the header files which ROOT is supposed to use. It fixes a very obscure initialization issue due to different versions of the ASTContext.h installed and used by ROOT.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2498
https://github.com/root-project/root/pull/2498:230,security,patch,patch,230,[clad] Specify where the clang header files are.; In cases where we build ROOT with -Dbuiltin_llvm=Off -Dbuiltin_clang=On and we have installed both llvm and clang in /usr/ clad will pick up the clang headers from there too. This patch gives higher priority to the header files which ROOT is supposed to use. It fixes a very obscure initialization issue due to different versions of the ASTContext.h installed and used by ROOT.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2498
https://github.com/root-project/root/pull/2499:4,deployability,depend,dependencies,4,"Fix dependencies in periodic target; This is a problem for the cxxmodules build, as can be seen [here](http://cdash.cern.ch/testDetails.php?test=49835249&build=552849).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2499
https://github.com/root-project/root/pull/2499:74,deployability,build,build,74,"Fix dependencies in periodic target; This is a problem for the cxxmodules build, as can be seen [here](http://cdash.cern.ch/testDetails.php?test=49835249&build=552849).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2499
https://github.com/root-project/root/pull/2499:154,deployability,build,build,154,"Fix dependencies in periodic target; This is a problem for the cxxmodules build, as can be seen [here](http://cdash.cern.ch/testDetails.php?test=49835249&build=552849).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2499
https://github.com/root-project/root/pull/2499:4,integrability,depend,dependencies,4,"Fix dependencies in periodic target; This is a problem for the cxxmodules build, as can be seen [here](http://cdash.cern.ch/testDetails.php?test=49835249&build=552849).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2499
https://github.com/root-project/root/pull/2499:4,modifiability,depend,dependencies,4,"Fix dependencies in periodic target; This is a problem for the cxxmodules build, as can be seen [here](http://cdash.cern.ch/testDetails.php?test=49835249&build=552849).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2499
https://github.com/root-project/root/pull/2499:4,safety,depend,dependencies,4,"Fix dependencies in periodic target; This is a problem for the cxxmodules build, as can be seen [here](http://cdash.cern.ch/testDetails.php?test=49835249&build=552849).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2499
https://github.com/root-project/root/pull/2499:124,safety,test,testDetails,124,"Fix dependencies in periodic target; This is a problem for the cxxmodules build, as can be seen [here](http://cdash.cern.ch/testDetails.php?test=49835249&build=552849).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2499
https://github.com/root-project/root/pull/2499:140,safety,test,test,140,"Fix dependencies in periodic target; This is a problem for the cxxmodules build, as can be seen [here](http://cdash.cern.ch/testDetails.php?test=49835249&build=552849).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2499
https://github.com/root-project/root/pull/2499:4,testability,depend,dependencies,4,"Fix dependencies in periodic target; This is a problem for the cxxmodules build, as can be seen [here](http://cdash.cern.ch/testDetails.php?test=49835249&build=552849).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2499
https://github.com/root-project/root/pull/2499:124,testability,test,testDetails,124,"Fix dependencies in periodic target; This is a problem for the cxxmodules build, as can be seen [here](http://cdash.cern.ch/testDetails.php?test=49835249&build=552849).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2499
https://github.com/root-project/root/pull/2499:140,testability,test,test,140,"Fix dependencies in periodic target; This is a problem for the cxxmodules build, as can be seen [here](http://cdash.cern.ch/testDetails.php?test=49835249&build=552849).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2499
https://github.com/root-project/root/pull/2500:100,deployability,patch,patch,100,[cxxmodules] Remove uses of Gtypes.h; It was deprecated (by comment) and emptied in year 2000. This patch removes all uses of it and 'registers' it to the modulemap to avoid our duplication algorithms do not find it as a duplicate.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2500
https://github.com/root-project/root/pull/2500:155,deployability,modul,modulemap,155,[cxxmodules] Remove uses of Gtypes.h; It was deprecated (by comment) and emptied in year 2000. This patch removes all uses of it and 'registers' it to the modulemap to avoid our duplication algorithms do not find it as a duplicate.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2500
https://github.com/root-project/root/pull/2500:155,modifiability,modul,modulemap,155,[cxxmodules] Remove uses of Gtypes.h; It was deprecated (by comment) and emptied in year 2000. This patch removes all uses of it and 'registers' it to the modulemap to avoid our duplication algorithms do not find it as a duplicate.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2500
https://github.com/root-project/root/pull/2500:100,safety,patch,patch,100,[cxxmodules] Remove uses of Gtypes.h; It was deprecated (by comment) and emptied in year 2000. This patch removes all uses of it and 'registers' it to the modulemap to avoid our duplication algorithms do not find it as a duplicate.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2500
https://github.com/root-project/root/pull/2500:155,safety,modul,modulemap,155,[cxxmodules] Remove uses of Gtypes.h; It was deprecated (by comment) and emptied in year 2000. This patch removes all uses of it and 'registers' it to the modulemap to avoid our duplication algorithms do not find it as a duplicate.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2500
https://github.com/root-project/root/pull/2500:168,safety,avoid,avoid,168,[cxxmodules] Remove uses of Gtypes.h; It was deprecated (by comment) and emptied in year 2000. This patch removes all uses of it and 'registers' it to the modulemap to avoid our duplication algorithms do not find it as a duplicate.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2500
https://github.com/root-project/root/pull/2500:100,security,patch,patch,100,[cxxmodules] Remove uses of Gtypes.h; It was deprecated (by comment) and emptied in year 2000. This patch removes all uses of it and 'registers' it to the modulemap to avoid our duplication algorithms do not find it as a duplicate.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2500
https://github.com/root-project/root/pull/2501:37,deployability,modul,modulemap,37,[cxxmodules] Add more headers to the modulemap.;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2501
https://github.com/root-project/root/pull/2501:37,modifiability,modul,modulemap,37,[cxxmodules] Add more headers to the modulemap.;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2501
https://github.com/root-project/root/pull/2501:37,safety,modul,modulemap,37,[cxxmodules] Add more headers to the modulemap.;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2501
https://github.com/root-project/root/pull/2502:435,availability,restor,restore,435,DNN optimisations for Cuda; This PR provides some optimisation for Cuda. In particular: . - optimise update weights/gradient of SGD and ADAM/ . - We avoid now creating temporary matrices in SGD . - For ADAM it is more efficient defining three new updates functions which will blenched on the GPU. This also avoid creating temporaries . These changes speed-up by almost a factor of 2 the code in GPU for dense layer when using ADAM and restore the previous performances for SGD. - optimise also computation of convolutional weight gradients. . In this case we can just simply use the ScaleAdd function. A speed up of ~ 20% is obtained . This PR also adds a commit that remove the computation of correlation matrix in case of large number of variables,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2502
https://github.com/root-project/root/pull/2502:101,deployability,updat,update,101,DNN optimisations for Cuda; This PR provides some optimisation for Cuda. In particular: . - optimise update weights/gradient of SGD and ADAM/ . - We avoid now creating temporary matrices in SGD . - For ADAM it is more efficient defining three new updates functions which will blenched on the GPU. This also avoid creating temporaries . These changes speed-up by almost a factor of 2 the code in GPU for dense layer when using ADAM and restore the previous performances for SGD. - optimise also computation of convolutional weight gradients. . In this case we can just simply use the ScaleAdd function. A speed up of ~ 20% is obtained . This PR also adds a commit that remove the computation of correlation matrix in case of large number of variables,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2502
https://github.com/root-project/root/pull/2502:247,deployability,updat,updates,247,DNN optimisations for Cuda; This PR provides some optimisation for Cuda. In particular: . - optimise update weights/gradient of SGD and ADAM/ . - We avoid now creating temporary matrices in SGD . - For ADAM it is more efficient defining three new updates functions which will blenched on the GPU. This also avoid creating temporaries . These changes speed-up by almost a factor of 2 the code in GPU for dense layer when using ADAM and restore the previous performances for SGD. - optimise also computation of convolutional weight gradients. . In this case we can just simply use the ScaleAdd function. A speed up of ~ 20% is obtained . This PR also adds a commit that remove the computation of correlation matrix in case of large number of variables,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2502
https://github.com/root-project/root/pull/2502:583,deployability,Scale,ScaleAdd,583,DNN optimisations for Cuda; This PR provides some optimisation for Cuda. In particular: . - optimise update weights/gradient of SGD and ADAM/ . - We avoid now creating temporary matrices in SGD . - For ADAM it is more efficient defining three new updates functions which will blenched on the GPU. This also avoid creating temporaries . These changes speed-up by almost a factor of 2 the code in GPU for dense layer when using ADAM and restore the previous performances for SGD. - optimise also computation of convolutional weight gradients. . In this case we can just simply use the ScaleAdd function. A speed up of ~ 20% is obtained . This PR also adds a commit that remove the computation of correlation matrix in case of large number of variables,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2502
https://github.com/root-project/root/pull/2502:4,energy efficiency,optim,optimisations,4,DNN optimisations for Cuda; This PR provides some optimisation for Cuda. In particular: . - optimise update weights/gradient of SGD and ADAM/ . - We avoid now creating temporary matrices in SGD . - For ADAM it is more efficient defining three new updates functions which will blenched on the GPU. This also avoid creating temporaries . These changes speed-up by almost a factor of 2 the code in GPU for dense layer when using ADAM and restore the previous performances for SGD. - optimise also computation of convolutional weight gradients. . In this case we can just simply use the ScaleAdd function. A speed up of ~ 20% is obtained . This PR also adds a commit that remove the computation of correlation matrix in case of large number of variables,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2502
https://github.com/root-project/root/pull/2502:50,energy efficiency,optim,optimisation,50,DNN optimisations for Cuda; This PR provides some optimisation for Cuda. In particular: . - optimise update weights/gradient of SGD and ADAM/ . - We avoid now creating temporary matrices in SGD . - For ADAM it is more efficient defining three new updates functions which will blenched on the GPU. This also avoid creating temporaries . These changes speed-up by almost a factor of 2 the code in GPU for dense layer when using ADAM and restore the previous performances for SGD. - optimise also computation of convolutional weight gradients. . In this case we can just simply use the ScaleAdd function. A speed up of ~ 20% is obtained . This PR also adds a commit that remove the computation of correlation matrix in case of large number of variables,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2502
https://github.com/root-project/root/pull/2502:92,energy efficiency,optim,optimise,92,DNN optimisations for Cuda; This PR provides some optimisation for Cuda. In particular: . - optimise update weights/gradient of SGD and ADAM/ . - We avoid now creating temporary matrices in SGD . - For ADAM it is more efficient defining three new updates functions which will blenched on the GPU. This also avoid creating temporaries . These changes speed-up by almost a factor of 2 the code in GPU for dense layer when using ADAM and restore the previous performances for SGD. - optimise also computation of convolutional weight gradients. . In this case we can just simply use the ScaleAdd function. A speed up of ~ 20% is obtained . This PR also adds a commit that remove the computation of correlation matrix in case of large number of variables,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2502
https://github.com/root-project/root/pull/2502:292,energy efficiency,GPU,GPU,292,DNN optimisations for Cuda; This PR provides some optimisation for Cuda. In particular: . - optimise update weights/gradient of SGD and ADAM/ . - We avoid now creating temporary matrices in SGD . - For ADAM it is more efficient defining three new updates functions which will blenched on the GPU. This also avoid creating temporaries . These changes speed-up by almost a factor of 2 the code in GPU for dense layer when using ADAM and restore the previous performances for SGD. - optimise also computation of convolutional weight gradients. . In this case we can just simply use the ScaleAdd function. A speed up of ~ 20% is obtained . This PR also adds a commit that remove the computation of correlation matrix in case of large number of variables,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2502
https://github.com/root-project/root/pull/2502:395,energy efficiency,GPU,GPU,395,DNN optimisations for Cuda; This PR provides some optimisation for Cuda. In particular: . - optimise update weights/gradient of SGD and ADAM/ . - We avoid now creating temporary matrices in SGD . - For ADAM it is more efficient defining three new updates functions which will blenched on the GPU. This also avoid creating temporaries . These changes speed-up by almost a factor of 2 the code in GPU for dense layer when using ADAM and restore the previous performances for SGD. - optimise also computation of convolutional weight gradients. . In this case we can just simply use the ScaleAdd function. A speed up of ~ 20% is obtained . This PR also adds a commit that remove the computation of correlation matrix in case of large number of variables,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2502
https://github.com/root-project/root/pull/2502:480,energy efficiency,optim,optimise,480,DNN optimisations for Cuda; This PR provides some optimisation for Cuda. In particular: . - optimise update weights/gradient of SGD and ADAM/ . - We avoid now creating temporary matrices in SGD . - For ADAM it is more efficient defining three new updates functions which will blenched on the GPU. This also avoid creating temporaries . These changes speed-up by almost a factor of 2 the code in GPU for dense layer when using ADAM and restore the previous performances for SGD. - optimise also computation of convolutional weight gradients. . In this case we can just simply use the ScaleAdd function. A speed up of ~ 20% is obtained . This PR also adds a commit that remove the computation of correlation matrix in case of large number of variables,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2502
https://github.com/root-project/root/pull/2502:583,energy efficiency,Scale,ScaleAdd,583,DNN optimisations for Cuda; This PR provides some optimisation for Cuda. In particular: . - optimise update weights/gradient of SGD and ADAM/ . - We avoid now creating temporary matrices in SGD . - For ADAM it is more efficient defining three new updates functions which will blenched on the GPU. This also avoid creating temporaries . These changes speed-up by almost a factor of 2 the code in GPU for dense layer when using ADAM and restore the previous performances for SGD. - optimise also computation of convolutional weight gradients. . In this case we can just simply use the ScaleAdd function. A speed up of ~ 20% is obtained . This PR also adds a commit that remove the computation of correlation matrix in case of large number of variables,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2502
https://github.com/root-project/root/pull/2502:409,modifiability,layer,layer,409,DNN optimisations for Cuda; This PR provides some optimisation for Cuda. In particular: . - optimise update weights/gradient of SGD and ADAM/ . - We avoid now creating temporary matrices in SGD . - For ADAM it is more efficient defining three new updates functions which will blenched on the GPU. This also avoid creating temporaries . These changes speed-up by almost a factor of 2 the code in GPU for dense layer when using ADAM and restore the previous performances for SGD. - optimise also computation of convolutional weight gradients. . In this case we can just simply use the ScaleAdd function. A speed up of ~ 20% is obtained . This PR also adds a commit that remove the computation of correlation matrix in case of large number of variables,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2502
https://github.com/root-project/root/pull/2502:583,modifiability,Scal,ScaleAdd,583,DNN optimisations for Cuda; This PR provides some optimisation for Cuda. In particular: . - optimise update weights/gradient of SGD and ADAM/ . - We avoid now creating temporary matrices in SGD . - For ADAM it is more efficient defining three new updates functions which will blenched on the GPU. This also avoid creating temporaries . These changes speed-up by almost a factor of 2 the code in GPU for dense layer when using ADAM and restore the previous performances for SGD. - optimise also computation of convolutional weight gradients. . In this case we can just simply use the ScaleAdd function. A speed up of ~ 20% is obtained . This PR also adds a commit that remove the computation of correlation matrix in case of large number of variables,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2502
https://github.com/root-project/root/pull/2502:740,modifiability,variab,variables,740,DNN optimisations for Cuda; This PR provides some optimisation for Cuda. In particular: . - optimise update weights/gradient of SGD and ADAM/ . - We avoid now creating temporary matrices in SGD . - For ADAM it is more efficient defining three new updates functions which will blenched on the GPU. This also avoid creating temporaries . These changes speed-up by almost a factor of 2 the code in GPU for dense layer when using ADAM and restore the previous performances for SGD. - optimise also computation of convolutional weight gradients. . In this case we can just simply use the ScaleAdd function. A speed up of ~ 20% is obtained . This PR also adds a commit that remove the computation of correlation matrix in case of large number of variables,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2502
https://github.com/root-project/root/pull/2502:292,performance,GPU,GPU,292,DNN optimisations for Cuda; This PR provides some optimisation for Cuda. In particular: . - optimise update weights/gradient of SGD and ADAM/ . - We avoid now creating temporary matrices in SGD . - For ADAM it is more efficient defining three new updates functions which will blenched on the GPU. This also avoid creating temporaries . These changes speed-up by almost a factor of 2 the code in GPU for dense layer when using ADAM and restore the previous performances for SGD. - optimise also computation of convolutional weight gradients. . In this case we can just simply use the ScaleAdd function. A speed up of ~ 20% is obtained . This PR also adds a commit that remove the computation of correlation matrix in case of large number of variables,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2502
https://github.com/root-project/root/pull/2502:395,performance,GPU,GPU,395,DNN optimisations for Cuda; This PR provides some optimisation for Cuda. In particular: . - optimise update weights/gradient of SGD and ADAM/ . - We avoid now creating temporary matrices in SGD . - For ADAM it is more efficient defining three new updates functions which will blenched on the GPU. This also avoid creating temporaries . These changes speed-up by almost a factor of 2 the code in GPU for dense layer when using ADAM and restore the previous performances for SGD. - optimise also computation of convolutional weight gradients. . In this case we can just simply use the ScaleAdd function. A speed up of ~ 20% is obtained . This PR also adds a commit that remove the computation of correlation matrix in case of large number of variables,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2502
https://github.com/root-project/root/pull/2502:456,performance,perform,performances,456,DNN optimisations for Cuda; This PR provides some optimisation for Cuda. In particular: . - optimise update weights/gradient of SGD and ADAM/ . - We avoid now creating temporary matrices in SGD . - For ADAM it is more efficient defining three new updates functions which will blenched on the GPU. This also avoid creating temporaries . These changes speed-up by almost a factor of 2 the code in GPU for dense layer when using ADAM and restore the previous performances for SGD. - optimise also computation of convolutional weight gradients. . In this case we can just simply use the ScaleAdd function. A speed up of ~ 20% is obtained . This PR also adds a commit that remove the computation of correlation matrix in case of large number of variables,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2502
https://github.com/root-project/root/pull/2502:583,performance,Scale,ScaleAdd,583,DNN optimisations for Cuda; This PR provides some optimisation for Cuda. In particular: . - optimise update weights/gradient of SGD and ADAM/ . - We avoid now creating temporary matrices in SGD . - For ADAM it is more efficient defining three new updates functions which will blenched on the GPU. This also avoid creating temporaries . These changes speed-up by almost a factor of 2 the code in GPU for dense layer when using ADAM and restore the previous performances for SGD. - optimise also computation of convolutional weight gradients. . In this case we can just simply use the ScaleAdd function. A speed up of ~ 20% is obtained . This PR also adds a commit that remove the computation of correlation matrix in case of large number of variables,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2502
https://github.com/root-project/root/pull/2502:435,reliability,restor,restore,435,DNN optimisations for Cuda; This PR provides some optimisation for Cuda. In particular: . - optimise update weights/gradient of SGD and ADAM/ . - We avoid now creating temporary matrices in SGD . - For ADAM it is more efficient defining three new updates functions which will blenched on the GPU. This also avoid creating temporaries . These changes speed-up by almost a factor of 2 the code in GPU for dense layer when using ADAM and restore the previous performances for SGD. - optimise also computation of convolutional weight gradients. . In this case we can just simply use the ScaleAdd function. A speed up of ~ 20% is obtained . This PR also adds a commit that remove the computation of correlation matrix in case of large number of variables,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2502
https://github.com/root-project/root/pull/2502:101,safety,updat,update,101,DNN optimisations for Cuda; This PR provides some optimisation for Cuda. In particular: . - optimise update weights/gradient of SGD and ADAM/ . - We avoid now creating temporary matrices in SGD . - For ADAM it is more efficient defining three new updates functions which will blenched on the GPU. This also avoid creating temporaries . These changes speed-up by almost a factor of 2 the code in GPU for dense layer when using ADAM and restore the previous performances for SGD. - optimise also computation of convolutional weight gradients. . In this case we can just simply use the ScaleAdd function. A speed up of ~ 20% is obtained . This PR also adds a commit that remove the computation of correlation matrix in case of large number of variables,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2502
https://github.com/root-project/root/pull/2502:149,safety,avoid,avoid,149,DNN optimisations for Cuda; This PR provides some optimisation for Cuda. In particular: . - optimise update weights/gradient of SGD and ADAM/ . - We avoid now creating temporary matrices in SGD . - For ADAM it is more efficient defining three new updates functions which will blenched on the GPU. This also avoid creating temporaries . These changes speed-up by almost a factor of 2 the code in GPU for dense layer when using ADAM and restore the previous performances for SGD. - optimise also computation of convolutional weight gradients. . In this case we can just simply use the ScaleAdd function. A speed up of ~ 20% is obtained . This PR also adds a commit that remove the computation of correlation matrix in case of large number of variables,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2502
https://github.com/root-project/root/pull/2502:247,safety,updat,updates,247,DNN optimisations for Cuda; This PR provides some optimisation for Cuda. In particular: . - optimise update weights/gradient of SGD and ADAM/ . - We avoid now creating temporary matrices in SGD . - For ADAM it is more efficient defining three new updates functions which will blenched on the GPU. This also avoid creating temporaries . These changes speed-up by almost a factor of 2 the code in GPU for dense layer when using ADAM and restore the previous performances for SGD. - optimise also computation of convolutional weight gradients. . In this case we can just simply use the ScaleAdd function. A speed up of ~ 20% is obtained . This PR also adds a commit that remove the computation of correlation matrix in case of large number of variables,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2502
https://github.com/root-project/root/pull/2502:307,safety,avoid,avoid,307,DNN optimisations for Cuda; This PR provides some optimisation for Cuda. In particular: . - optimise update weights/gradient of SGD and ADAM/ . - We avoid now creating temporary matrices in SGD . - For ADAM it is more efficient defining three new updates functions which will blenched on the GPU. This also avoid creating temporaries . These changes speed-up by almost a factor of 2 the code in GPU for dense layer when using ADAM and restore the previous performances for SGD. - optimise also computation of convolutional weight gradients. . In this case we can just simply use the ScaleAdd function. A speed up of ~ 20% is obtained . This PR also adds a commit that remove the computation of correlation matrix in case of large number of variables,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2502
https://github.com/root-project/root/pull/2502:101,security,updat,update,101,DNN optimisations for Cuda; This PR provides some optimisation for Cuda. In particular: . - optimise update weights/gradient of SGD and ADAM/ . - We avoid now creating temporary matrices in SGD . - For ADAM it is more efficient defining three new updates functions which will blenched on the GPU. This also avoid creating temporaries . These changes speed-up by almost a factor of 2 the code in GPU for dense layer when using ADAM and restore the previous performances for SGD. - optimise also computation of convolutional weight gradients. . In this case we can just simply use the ScaleAdd function. A speed up of ~ 20% is obtained . This PR also adds a commit that remove the computation of correlation matrix in case of large number of variables,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2502
https://github.com/root-project/root/pull/2502:247,security,updat,updates,247,DNN optimisations for Cuda; This PR provides some optimisation for Cuda. In particular: . - optimise update weights/gradient of SGD and ADAM/ . - We avoid now creating temporary matrices in SGD . - For ADAM it is more efficient defining three new updates functions which will blenched on the GPU. This also avoid creating temporaries . These changes speed-up by almost a factor of 2 the code in GPU for dense layer when using ADAM and restore the previous performances for SGD. - optimise also computation of convolutional weight gradients. . In this case we can just simply use the ScaleAdd function. A speed up of ~ 20% is obtained . This PR also adds a commit that remove the computation of correlation matrix in case of large number of variables,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2502
https://github.com/root-project/root/pull/2502:568,testability,simpl,simply,568,DNN optimisations for Cuda; This PR provides some optimisation for Cuda. In particular: . - optimise update weights/gradient of SGD and ADAM/ . - We avoid now creating temporary matrices in SGD . - For ADAM it is more efficient defining three new updates functions which will blenched on the GPU. This also avoid creating temporaries . These changes speed-up by almost a factor of 2 the code in GPU for dense layer when using ADAM and restore the previous performances for SGD. - optimise also computation of convolutional weight gradients. . In this case we can just simply use the ScaleAdd function. A speed up of ~ 20% is obtained . This PR also adds a commit that remove the computation of correlation matrix in case of large number of variables,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2502
https://github.com/root-project/root/pull/2502:218,usability,efficien,efficient,218,DNN optimisations for Cuda; This PR provides some optimisation for Cuda. In particular: . - optimise update weights/gradient of SGD and ADAM/ . - We avoid now creating temporary matrices in SGD . - For ADAM it is more efficient defining three new updates functions which will blenched on the GPU. This also avoid creating temporaries . These changes speed-up by almost a factor of 2 the code in GPU for dense layer when using ADAM and restore the previous performances for SGD. - optimise also computation of convolutional weight gradients. . In this case we can just simply use the ScaleAdd function. A speed up of ~ 20% is obtained . This PR also adds a commit that remove the computation of correlation matrix in case of large number of variables,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2502
https://github.com/root-project/root/pull/2502:456,usability,perform,performances,456,DNN optimisations for Cuda; This PR provides some optimisation for Cuda. In particular: . - optimise update weights/gradient of SGD and ADAM/ . - We avoid now creating temporary matrices in SGD . - For ADAM it is more efficient defining three new updates functions which will blenched on the GPU. This also avoid creating temporaries . These changes speed-up by almost a factor of 2 the code in GPU for dense layer when using ADAM and restore the previous performances for SGD. - optimise also computation of convolutional weight gradients. . In this case we can just simply use the ScaleAdd function. A speed up of ~ 20% is obtained . This PR also adds a commit that remove the computation of correlation matrix in case of large number of variables,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2502
https://github.com/root-project/root/pull/2502:568,usability,simpl,simply,568,DNN optimisations for Cuda; This PR provides some optimisation for Cuda. In particular: . - optimise update weights/gradient of SGD and ADAM/ . - We avoid now creating temporary matrices in SGD . - For ADAM it is more efficient defining three new updates functions which will blenched on the GPU. This also avoid creating temporaries . These changes speed-up by almost a factor of 2 the code in GPU for dense layer when using ADAM and restore the previous performances for SGD. - optimise also computation of convolutional weight gradients. . In this case we can just simply use the ScaleAdd function. A speed up of ~ 20% is obtained . This PR also adds a commit that remove the computation of correlation matrix in case of large number of variables,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2502
https://github.com/root-project/root/pull/2503:13,energy efficiency,Reduc,Reduce,13,[cxxmodules] Reduce pcm file duplications.;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2503
https://github.com/root-project/root/pull/2504:13,energy efficiency,Reduc,Reduce,13,[cxxmodules] Reduce the pcm duplicates.; X3DBuffer.h is used indirectly by Graf3d and EG dictionaries. It does not. make a lot of sense to exclude it. Digging git history shows the import from. cvs so the real reason will probably remain unknown.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2504
https://github.com/root-project/root/pull/2504:106,reliability,doe,does,106,[cxxmodules] Reduce the pcm duplicates.; X3DBuffer.h is used indirectly by Graf3d and EG dictionaries. It does not. make a lot of sense to exclude it. Digging git history shows the import from. cvs so the real reason will probably remain unknown.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2504
https://github.com/root-project/root/pull/2505:0,deployability,Updat,Updates,0,Updates to the build system;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2505
https://github.com/root-project/root/pull/2505:15,deployability,build,build,15,Updates to the build system;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2505
https://github.com/root-project/root/pull/2505:0,safety,Updat,Updates,0,Updates to the build system;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2505
https://github.com/root-project/root/pull/2505:0,security,Updat,Updates,0,Updates to the build system;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2505
https://github.com/root-project/root/pull/2506:25,deployability,depend,dependencies,25,[cxxmodules] Add missing dependencies to MathCore.;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2506
https://github.com/root-project/root/pull/2506:25,integrability,depend,dependencies,25,[cxxmodules] Add missing dependencies to MathCore.;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2506
https://github.com/root-project/root/pull/2506:25,modifiability,depend,dependencies,25,[cxxmodules] Add missing dependencies to MathCore.;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2506
https://github.com/root-project/root/pull/2506:25,safety,depend,dependencies,25,[cxxmodules] Add missing dependencies to MathCore.;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2506
https://github.com/root-project/root/pull/2506:25,testability,depend,dependencies,25,[cxxmodules] Add missing dependencies to MathCore.;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2506
https://github.com/root-project/root/pull/2507:9,deployability,modul,modulemap,9,Adjust a modulemap file to be able to treat separate .c files; Patch by Vassil Vassilev.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2507
https://github.com/root-project/root/pull/2507:63,deployability,Patch,Patch,63,Adjust a modulemap file to be able to treat separate .c files; Patch by Vassil Vassilev.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2507
https://github.com/root-project/root/pull/2507:9,modifiability,modul,modulemap,9,Adjust a modulemap file to be able to treat separate .c files; Patch by Vassil Vassilev.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2507
https://github.com/root-project/root/pull/2507:9,safety,modul,modulemap,9,Adjust a modulemap file to be able to treat separate .c files; Patch by Vassil Vassilev.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2507
https://github.com/root-project/root/pull/2507:63,safety,Patch,Patch,63,Adjust a modulemap file to be able to treat separate .c files; Patch by Vassil Vassilev.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2507
https://github.com/root-project/root/pull/2507:63,security,Patch,Patch,63,Adjust a modulemap file to be able to treat separate .c files; Patch by Vassil Vassilev.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2507
https://github.com/root-project/root/pull/2508:60,deployability,modul,modular,60,"Add missing headers in TMVA and MathCore; These headers are modular, this makes pcm a little bit smaller.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2508
https://github.com/root-project/root/pull/2508:60,integrability,modular,modular,60,"Add missing headers in TMVA and MathCore; These headers are modular, this makes pcm a little bit smaller.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2508
https://github.com/root-project/root/pull/2508:60,modifiability,modul,modular,60,"Add missing headers in TMVA and MathCore; These headers are modular, this makes pcm a little bit smaller.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2508
https://github.com/root-project/root/pull/2508:60,safety,modul,modular,60,"Add missing headers in TMVA and MathCore; These headers are modular, this makes pcm a little bit smaller.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2508
https://github.com/root-project/root/pull/2508:60,testability,modula,modular,60,"Add missing headers in TMVA and MathCore; These headers are modular, this makes pcm a little bit smaller.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2508
https://github.com/root-project/root/pull/2510:19,energy efficiency,Core,Core,19,Remove globbing of Core;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2510
https://github.com/root-project/root/pull/2514:5,safety,Test,Tests,5,[DF] Tests and doc for the Display action; - Introduced tests for some use case of the Display action. - Added the Display action to the documentation.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2514
https://github.com/root-project/root/pull/2514:56,safety,test,tests,56,[DF] Tests and doc for the Display action; - Introduced tests for some use case of the Display action. - Added the Display action to the documentation.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2514
https://github.com/root-project/root/pull/2514:5,testability,Test,Tests,5,[DF] Tests and doc for the Display action; - Introduced tests for some use case of the Display action. - Added the Display action to the documentation.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2514
https://github.com/root-project/root/pull/2514:56,testability,test,tests,56,[DF] Tests and doc for the Display action; - Introduced tests for some use case of the Display action. - Added the Display action to the documentation.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2514
https://github.com/root-project/root/pull/2514:137,usability,document,documentation,137,[DF] Tests and doc for the Display action; - Introduced tests for some use case of the Display action. - Added the Display action to the documentation.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2514
https://github.com/root-project/root/pull/2515:30,performance,Cach,Cache,30,"[DF] Fixed ambiguous calls to Cache and Display; Using Cache() and Display() with an argument like {""x"", ""y""} matches both the string_view and the vector<string> overloads, causing ambiguity.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2515
https://github.com/root-project/root/pull/2515:55,performance,Cach,Cache,55,"[DF] Fixed ambiguous calls to Cache and Display; Using Cache() and Display() with an argument like {""x"", ""y""} matches both the string_view and the vector<string> overloads, causing ambiguity.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2515
https://github.com/root-project/root/pull/2517:41,deployability,modul,modulemap,41,[cxxmodules] Add missing headers to libc.modulemap;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2517
https://github.com/root-project/root/pull/2517:41,modifiability,modul,modulemap,41,[cxxmodules] Add missing headers to libc.modulemap;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2517
https://github.com/root-project/root/pull/2517:41,safety,modul,modulemap,41,[cxxmodules] Add missing headers to libc.modulemap;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2517
https://github.com/root-project/root/pull/2518:253,deployability,modul,module,253,"[PyROOT] [ROOT-9636] Tfile, TDirectoryFile and TDirectory pythonisations; This PR introduces the pythonisations mentioned in the title and introduces the concept of ""instant pythonisations"". Those are non-lazy pythonisations which are triggered at ROOT module import. The pythonisation of TDirectory is part of those since it is ""too late"" to pythonise it lazily.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2518
https://github.com/root-project/root/pull/2518:253,modifiability,modul,module,253,"[PyROOT] [ROOT-9636] Tfile, TDirectoryFile and TDirectory pythonisations; This PR introduces the pythonisations mentioned in the title and introduces the concept of ""instant pythonisations"". Those are non-lazy pythonisations which are triggered at ROOT module import. The pythonisation of TDirectory is part of those since it is ""too late"" to pythonise it lazily.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2518
https://github.com/root-project/root/pull/2518:253,safety,modul,module,253,"[PyROOT] [ROOT-9636] Tfile, TDirectoryFile and TDirectory pythonisations; This PR introduces the pythonisations mentioned in the title and introduces the concept of ""instant pythonisations"". Those are non-lazy pythonisations which are triggered at ROOT module import. The pythonisation of TDirectory is part of those since it is ""too late"" to pythonise it lazily.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2518
https://github.com/root-project/root/pull/2519:66,deployability,API,APIs,66,"Bulk I/O - v3; This is the second attempt for merging the bulk IO APIs (hidden inside the ROOT::Internal namespace); original version was in #943; this is a large rebase and update (particularly, removing the buffer sharing between the ROOT and the user). The initial goal here is to get another round of @pcanal attention and feedback. The bulk IO APIs aim to provide as fast access as possible in the case where the user is using relatively simple datatypes and the objects can be deserialized ""non-destructively"" (i.e., in-place in the existing memory buffers). For example - primitives and lists of primitives are supported; anything needing references are not. Despite the fact many of ROOT's very useful deserialization features are not supported, the limited feature set covers a range of analysis use cases. In particular, the current version of CMS's proposed NanoAOD format can be read solely via bulk APIs. Where the bulk IO APIs are applicable, we see an order-magnitude improvement in event rates for some use cases (including CMS NanoAOD). The speedup becomes more noticeable as the data read from the event is smaller (meaning the traditional ROOT IO overheads are larger). Somewhere in the neighborhood of 5KB objects read per event is the point where bulk IO APIs should be noticeably faster. This PR provides three interfaces:. * A raw, low-level interface. The user can invoke the bulk IO APIs directly and get a buffer of primitive types, serialized or deserialized. It is not envisioned this is used by users, but by API writers. * A TTreeReader-like interface, TTreeReaderFast. This is meant to be used by users; it utilizes the low-level interface to retrieve the serialized buffer and templated code will inline the deserializing as part of iterating through the tree. The speed here is gained by the fact only a single pass in memory is needed to deserialize and apply user code, rather than two. * The intent is that this interface can be extended in the future and used by ",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2519
https://github.com/root-project/root/pull/2519:126,deployability,version,version,126,"Bulk I/O - v3; This is the second attempt for merging the bulk IO APIs (hidden inside the ROOT::Internal namespace); original version was in #943; this is a large rebase and update (particularly, removing the buffer sharing between the ROOT and the user). The initial goal here is to get another round of @pcanal attention and feedback. The bulk IO APIs aim to provide as fast access as possible in the case where the user is using relatively simple datatypes and the objects can be deserialized ""non-destructively"" (i.e., in-place in the existing memory buffers). For example - primitives and lists of primitives are supported; anything needing references are not. Despite the fact many of ROOT's very useful deserialization features are not supported, the limited feature set covers a range of analysis use cases. In particular, the current version of CMS's proposed NanoAOD format can be read solely via bulk APIs. Where the bulk IO APIs are applicable, we see an order-magnitude improvement in event rates for some use cases (including CMS NanoAOD). The speedup becomes more noticeable as the data read from the event is smaller (meaning the traditional ROOT IO overheads are larger). Somewhere in the neighborhood of 5KB objects read per event is the point where bulk IO APIs should be noticeably faster. This PR provides three interfaces:. * A raw, low-level interface. The user can invoke the bulk IO APIs directly and get a buffer of primitive types, serialized or deserialized. It is not envisioned this is used by users, but by API writers. * A TTreeReader-like interface, TTreeReaderFast. This is meant to be used by users; it utilizes the low-level interface to retrieve the serialized buffer and templated code will inline the deserializing as part of iterating through the tree. The speed here is gained by the fact only a single pass in memory is needed to deserialize and apply user code, rather than two. * The intent is that this interface can be extended in the future and used by ",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2519
https://github.com/root-project/root/pull/2519:174,deployability,updat,update,174,"Bulk I/O - v3; This is the second attempt for merging the bulk IO APIs (hidden inside the ROOT::Internal namespace); original version was in #943; this is a large rebase and update (particularly, removing the buffer sharing between the ROOT and the user). The initial goal here is to get another round of @pcanal attention and feedback. The bulk IO APIs aim to provide as fast access as possible in the case where the user is using relatively simple datatypes and the objects can be deserialized ""non-destructively"" (i.e., in-place in the existing memory buffers). For example - primitives and lists of primitives are supported; anything needing references are not. Despite the fact many of ROOT's very useful deserialization features are not supported, the limited feature set covers a range of analysis use cases. In particular, the current version of CMS's proposed NanoAOD format can be read solely via bulk APIs. Where the bulk IO APIs are applicable, we see an order-magnitude improvement in event rates for some use cases (including CMS NanoAOD). The speedup becomes more noticeable as the data read from the event is smaller (meaning the traditional ROOT IO overheads are larger). Somewhere in the neighborhood of 5KB objects read per event is the point where bulk IO APIs should be noticeably faster. This PR provides three interfaces:. * A raw, low-level interface. The user can invoke the bulk IO APIs directly and get a buffer of primitive types, serialized or deserialized. It is not envisioned this is used by users, but by API writers. * A TTreeReader-like interface, TTreeReaderFast. This is meant to be used by users; it utilizes the low-level interface to retrieve the serialized buffer and templated code will inline the deserializing as part of iterating through the tree. The speed here is gained by the fact only a single pass in memory is needed to deserialize and apply user code, rather than two. * The intent is that this interface can be extended in the future and used by ",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2519
https://github.com/root-project/root/pull/2519:349,deployability,API,APIs,349,"Bulk I/O - v3; This is the second attempt for merging the bulk IO APIs (hidden inside the ROOT::Internal namespace); original version was in #943; this is a large rebase and update (particularly, removing the buffer sharing between the ROOT and the user). The initial goal here is to get another round of @pcanal attention and feedback. The bulk IO APIs aim to provide as fast access as possible in the case where the user is using relatively simple datatypes and the objects can be deserialized ""non-destructively"" (i.e., in-place in the existing memory buffers). For example - primitives and lists of primitives are supported; anything needing references are not. Despite the fact many of ROOT's very useful deserialization features are not supported, the limited feature set covers a range of analysis use cases. In particular, the current version of CMS's proposed NanoAOD format can be read solely via bulk APIs. Where the bulk IO APIs are applicable, we see an order-magnitude improvement in event rates for some use cases (including CMS NanoAOD). The speedup becomes more noticeable as the data read from the event is smaller (meaning the traditional ROOT IO overheads are larger). Somewhere in the neighborhood of 5KB objects read per event is the point where bulk IO APIs should be noticeably faster. This PR provides three interfaces:. * A raw, low-level interface. The user can invoke the bulk IO APIs directly and get a buffer of primitive types, serialized or deserialized. It is not envisioned this is used by users, but by API writers. * A TTreeReader-like interface, TTreeReaderFast. This is meant to be used by users; it utilizes the low-level interface to retrieve the serialized buffer and templated code will inline the deserializing as part of iterating through the tree. The speed here is gained by the fact only a single pass in memory is needed to deserialize and apply user code, rather than two. * The intent is that this interface can be extended in the future and used by ",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2519
https://github.com/root-project/root/pull/2519:843,deployability,version,version,843,"Bulk I/O - v3; This is the second attempt for merging the bulk IO APIs (hidden inside the ROOT::Internal namespace); original version was in #943; this is a large rebase and update (particularly, removing the buffer sharing between the ROOT and the user). The initial goal here is to get another round of @pcanal attention and feedback. The bulk IO APIs aim to provide as fast access as possible in the case where the user is using relatively simple datatypes and the objects can be deserialized ""non-destructively"" (i.e., in-place in the existing memory buffers). For example - primitives and lists of primitives are supported; anything needing references are not. Despite the fact many of ROOT's very useful deserialization features are not supported, the limited feature set covers a range of analysis use cases. In particular, the current version of CMS's proposed NanoAOD format can be read solely via bulk APIs. Where the bulk IO APIs are applicable, we see an order-magnitude improvement in event rates for some use cases (including CMS NanoAOD). The speedup becomes more noticeable as the data read from the event is smaller (meaning the traditional ROOT IO overheads are larger). Somewhere in the neighborhood of 5KB objects read per event is the point where bulk IO APIs should be noticeably faster. This PR provides three interfaces:. * A raw, low-level interface. The user can invoke the bulk IO APIs directly and get a buffer of primitive types, serialized or deserialized. It is not envisioned this is used by users, but by API writers. * A TTreeReader-like interface, TTreeReaderFast. This is meant to be used by users; it utilizes the low-level interface to retrieve the serialized buffer and templated code will inline the deserializing as part of iterating through the tree. The speed here is gained by the fact only a single pass in memory is needed to deserialize and apply user code, rather than two. * The intent is that this interface can be extended in the future and used by ",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2519
https://github.com/root-project/root/pull/2519:912,deployability,API,APIs,912,"Bulk I/O - v3; This is the second attempt for merging the bulk IO APIs (hidden inside the ROOT::Internal namespace); original version was in #943; this is a large rebase and update (particularly, removing the buffer sharing between the ROOT and the user). The initial goal here is to get another round of @pcanal attention and feedback. The bulk IO APIs aim to provide as fast access as possible in the case where the user is using relatively simple datatypes and the objects can be deserialized ""non-destructively"" (i.e., in-place in the existing memory buffers). For example - primitives and lists of primitives are supported; anything needing references are not. Despite the fact many of ROOT's very useful deserialization features are not supported, the limited feature set covers a range of analysis use cases. In particular, the current version of CMS's proposed NanoAOD format can be read solely via bulk APIs. Where the bulk IO APIs are applicable, we see an order-magnitude improvement in event rates for some use cases (including CMS NanoAOD). The speedup becomes more noticeable as the data read from the event is smaller (meaning the traditional ROOT IO overheads are larger). Somewhere in the neighborhood of 5KB objects read per event is the point where bulk IO APIs should be noticeably faster. This PR provides three interfaces:. * A raw, low-level interface. The user can invoke the bulk IO APIs directly and get a buffer of primitive types, serialized or deserialized. It is not envisioned this is used by users, but by API writers. * A TTreeReader-like interface, TTreeReaderFast. This is meant to be used by users; it utilizes the low-level interface to retrieve the serialized buffer and templated code will inline the deserializing as part of iterating through the tree. The speed here is gained by the fact only a single pass in memory is needed to deserialize and apply user code, rather than two. * The intent is that this interface can be extended in the future and used by ",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2519
https://github.com/root-project/root/pull/2519:936,deployability,API,APIs,936,"Bulk I/O - v3; This is the second attempt for merging the bulk IO APIs (hidden inside the ROOT::Internal namespace); original version was in #943; this is a large rebase and update (particularly, removing the buffer sharing between the ROOT and the user). The initial goal here is to get another round of @pcanal attention and feedback. The bulk IO APIs aim to provide as fast access as possible in the case where the user is using relatively simple datatypes and the objects can be deserialized ""non-destructively"" (i.e., in-place in the existing memory buffers). For example - primitives and lists of primitives are supported; anything needing references are not. Despite the fact many of ROOT's very useful deserialization features are not supported, the limited feature set covers a range of analysis use cases. In particular, the current version of CMS's proposed NanoAOD format can be read solely via bulk APIs. Where the bulk IO APIs are applicable, we see an order-magnitude improvement in event rates for some use cases (including CMS NanoAOD). The speedup becomes more noticeable as the data read from the event is smaller (meaning the traditional ROOT IO overheads are larger). Somewhere in the neighborhood of 5KB objects read per event is the point where bulk IO APIs should be noticeably faster. This PR provides three interfaces:. * A raw, low-level interface. The user can invoke the bulk IO APIs directly and get a buffer of primitive types, serialized or deserialized. It is not envisioned this is used by users, but by API writers. * A TTreeReader-like interface, TTreeReaderFast. This is meant to be used by users; it utilizes the low-level interface to retrieve the serialized buffer and templated code will inline the deserializing as part of iterating through the tree. The speed here is gained by the fact only a single pass in memory is needed to deserialize and apply user code, rather than two. * The intent is that this interface can be extended in the future and used by ",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2519
https://github.com/root-project/root/pull/2519:1276,deployability,API,APIs,1276,"is to get another round of @pcanal attention and feedback. The bulk IO APIs aim to provide as fast access as possible in the case where the user is using relatively simple datatypes and the objects can be deserialized ""non-destructively"" (i.e., in-place in the existing memory buffers). For example - primitives and lists of primitives are supported; anything needing references are not. Despite the fact many of ROOT's very useful deserialization features are not supported, the limited feature set covers a range of analysis use cases. In particular, the current version of CMS's proposed NanoAOD format can be read solely via bulk APIs. Where the bulk IO APIs are applicable, we see an order-magnitude improvement in event rates for some use cases (including CMS NanoAOD). The speedup becomes more noticeable as the data read from the event is smaller (meaning the traditional ROOT IO overheads are larger). Somewhere in the neighborhood of 5KB objects read per event is the point where bulk IO APIs should be noticeably faster. This PR provides three interfaces:. * A raw, low-level interface. The user can invoke the bulk IO APIs directly and get a buffer of primitive types, serialized or deserialized. It is not envisioned this is used by users, but by API writers. * A TTreeReader-like interface, TTreeReaderFast. This is meant to be used by users; it utilizes the low-level interface to retrieve the serialized buffer and templated code will inline the deserializing as part of iterating through the tree. The speed here is gained by the fact only a single pass in memory is needed to deserialize and apply user code, rather than two. * The intent is that this interface can be extended in the future and used by TDataFrame, provided we can make TDataFrame sufficiently fast. * A Python-based numpy export interface. This exposes the (possibly serialized) memory directly as numpy arrays, allowing for efficient access to ROOT-serialized objects from the Python language. * The numpy interfa",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2519
https://github.com/root-project/root/pull/2519:1408,deployability,API,APIs,1408,"ere the user is using relatively simple datatypes and the objects can be deserialized ""non-destructively"" (i.e., in-place in the existing memory buffers). For example - primitives and lists of primitives are supported; anything needing references are not. Despite the fact many of ROOT's very useful deserialization features are not supported, the limited feature set covers a range of analysis use cases. In particular, the current version of CMS's proposed NanoAOD format can be read solely via bulk APIs. Where the bulk IO APIs are applicable, we see an order-magnitude improvement in event rates for some use cases (including CMS NanoAOD). The speedup becomes more noticeable as the data read from the event is smaller (meaning the traditional ROOT IO overheads are larger). Somewhere in the neighborhood of 5KB objects read per event is the point where bulk IO APIs should be noticeably faster. This PR provides three interfaces:. * A raw, low-level interface. The user can invoke the bulk IO APIs directly and get a buffer of primitive types, serialized or deserialized. It is not envisioned this is used by users, but by API writers. * A TTreeReader-like interface, TTreeReaderFast. This is meant to be used by users; it utilizes the low-level interface to retrieve the serialized buffer and templated code will inline the deserializing as part of iterating through the tree. The speed here is gained by the fact only a single pass in memory is needed to deserialize and apply user code, rather than two. * The intent is that this interface can be extended in the future and used by TDataFrame, provided we can make TDataFrame sufficiently fast. * A Python-based numpy export interface. This exposes the (possibly serialized) memory directly as numpy arrays, allowing for efficient access to ROOT-serialized objects from the Python language. * The numpy interface is low-level for Python; we intend to further explore the ideal high-level interface through projects such as https://github.com/",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2519
https://github.com/root-project/root/pull/2519:1538,deployability,API,API,1538,"in-place in the existing memory buffers). For example - primitives and lists of primitives are supported; anything needing references are not. Despite the fact many of ROOT's very useful deserialization features are not supported, the limited feature set covers a range of analysis use cases. In particular, the current version of CMS's proposed NanoAOD format can be read solely via bulk APIs. Where the bulk IO APIs are applicable, we see an order-magnitude improvement in event rates for some use cases (including CMS NanoAOD). The speedup becomes more noticeable as the data read from the event is smaller (meaning the traditional ROOT IO overheads are larger). Somewhere in the neighborhood of 5KB objects read per event is the point where bulk IO APIs should be noticeably faster. This PR provides three interfaces:. * A raw, low-level interface. The user can invoke the bulk IO APIs directly and get a buffer of primitive types, serialized or deserialized. It is not envisioned this is used by users, but by API writers. * A TTreeReader-like interface, TTreeReaderFast. This is meant to be used by users; it utilizes the low-level interface to retrieve the serialized buffer and templated code will inline the deserializing as part of iterating through the tree. The speed here is gained by the fact only a single pass in memory is needed to deserialize and apply user code, rather than two. * The intent is that this interface can be extended in the future and used by TDataFrame, provided we can make TDataFrame sufficiently fast. * A Python-based numpy export interface. This exposes the (possibly serialized) memory directly as numpy arrays, allowing for efficient access to ROOT-serialized objects from the Python language. * The numpy interface is low-level for Python; we intend to further explore the ideal high-level interface through projects such as https://github.com/diana-hep/plur. (With many thanks to @jpivarski for driving this work home with his work on a python interface!).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2519
https://github.com/root-project/root/pull/2519:835,energy efficiency,current,current,835,"Bulk I/O - v3; This is the second attempt for merging the bulk IO APIs (hidden inside the ROOT::Internal namespace); original version was in #943; this is a large rebase and update (particularly, removing the buffer sharing between the ROOT and the user). The initial goal here is to get another round of @pcanal attention and feedback. The bulk IO APIs aim to provide as fast access as possible in the case where the user is using relatively simple datatypes and the objects can be deserialized ""non-destructively"" (i.e., in-place in the existing memory buffers). For example - primitives and lists of primitives are supported; anything needing references are not. Despite the fact many of ROOT's very useful deserialization features are not supported, the limited feature set covers a range of analysis use cases. In particular, the current version of CMS's proposed NanoAOD format can be read solely via bulk APIs. Where the bulk IO APIs are applicable, we see an order-magnitude improvement in event rates for some use cases (including CMS NanoAOD). The speedup becomes more noticeable as the data read from the event is smaller (meaning the traditional ROOT IO overheads are larger). Somewhere in the neighborhood of 5KB objects read per event is the point where bulk IO APIs should be noticeably faster. This PR provides three interfaces:. * A raw, low-level interface. The user can invoke the bulk IO APIs directly and get a buffer of primitive types, serialized or deserialized. It is not envisioned this is used by users, but by API writers. * A TTreeReader-like interface, TTreeReaderFast. This is meant to be used by users; it utilizes the low-level interface to retrieve the serialized buffer and templated code will inline the deserializing as part of iterating through the tree. The speed here is gained by the fact only a single pass in memory is needed to deserialize and apply user code, rather than two. * The intent is that this interface can be extended in the future and used by ",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2519
https://github.com/root-project/root/pull/2519:66,integrability,API,APIs,66,"Bulk I/O - v3; This is the second attempt for merging the bulk IO APIs (hidden inside the ROOT::Internal namespace); original version was in #943; this is a large rebase and update (particularly, removing the buffer sharing between the ROOT and the user). The initial goal here is to get another round of @pcanal attention and feedback. The bulk IO APIs aim to provide as fast access as possible in the case where the user is using relatively simple datatypes and the objects can be deserialized ""non-destructively"" (i.e., in-place in the existing memory buffers). For example - primitives and lists of primitives are supported; anything needing references are not. Despite the fact many of ROOT's very useful deserialization features are not supported, the limited feature set covers a range of analysis use cases. In particular, the current version of CMS's proposed NanoAOD format can be read solely via bulk APIs. Where the bulk IO APIs are applicable, we see an order-magnitude improvement in event rates for some use cases (including CMS NanoAOD). The speedup becomes more noticeable as the data read from the event is smaller (meaning the traditional ROOT IO overheads are larger). Somewhere in the neighborhood of 5KB objects read per event is the point where bulk IO APIs should be noticeably faster. This PR provides three interfaces:. * A raw, low-level interface. The user can invoke the bulk IO APIs directly and get a buffer of primitive types, serialized or deserialized. It is not envisioned this is used by users, but by API writers. * A TTreeReader-like interface, TTreeReaderFast. This is meant to be used by users; it utilizes the low-level interface to retrieve the serialized buffer and templated code will inline the deserializing as part of iterating through the tree. The speed here is gained by the fact only a single pass in memory is needed to deserialize and apply user code, rather than two. * The intent is that this interface can be extended in the future and used by ",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2519
https://github.com/root-project/root/pull/2519:126,integrability,version,version,126,"Bulk I/O - v3; This is the second attempt for merging the bulk IO APIs (hidden inside the ROOT::Internal namespace); original version was in #943; this is a large rebase and update (particularly, removing the buffer sharing between the ROOT and the user). The initial goal here is to get another round of @pcanal attention and feedback. The bulk IO APIs aim to provide as fast access as possible in the case where the user is using relatively simple datatypes and the objects can be deserialized ""non-destructively"" (i.e., in-place in the existing memory buffers). For example - primitives and lists of primitives are supported; anything needing references are not. Despite the fact many of ROOT's very useful deserialization features are not supported, the limited feature set covers a range of analysis use cases. In particular, the current version of CMS's proposed NanoAOD format can be read solely via bulk APIs. Where the bulk IO APIs are applicable, we see an order-magnitude improvement in event rates for some use cases (including CMS NanoAOD). The speedup becomes more noticeable as the data read from the event is smaller (meaning the traditional ROOT IO overheads are larger). Somewhere in the neighborhood of 5KB objects read per event is the point where bulk IO APIs should be noticeably faster. This PR provides three interfaces:. * A raw, low-level interface. The user can invoke the bulk IO APIs directly and get a buffer of primitive types, serialized or deserialized. It is not envisioned this is used by users, but by API writers. * A TTreeReader-like interface, TTreeReaderFast. This is meant to be used by users; it utilizes the low-level interface to retrieve the serialized buffer and templated code will inline the deserializing as part of iterating through the tree. The speed here is gained by the fact only a single pass in memory is needed to deserialize and apply user code, rather than two. * The intent is that this interface can be extended in the future and used by ",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2519
https://github.com/root-project/root/pull/2519:209,integrability,buffer,buffer,209,"Bulk I/O - v3; This is the second attempt for merging the bulk IO APIs (hidden inside the ROOT::Internal namespace); original version was in #943; this is a large rebase and update (particularly, removing the buffer sharing between the ROOT and the user). The initial goal here is to get another round of @pcanal attention and feedback. The bulk IO APIs aim to provide as fast access as possible in the case where the user is using relatively simple datatypes and the objects can be deserialized ""non-destructively"" (i.e., in-place in the existing memory buffers). For example - primitives and lists of primitives are supported; anything needing references are not. Despite the fact many of ROOT's very useful deserialization features are not supported, the limited feature set covers a range of analysis use cases. In particular, the current version of CMS's proposed NanoAOD format can be read solely via bulk APIs. Where the bulk IO APIs are applicable, we see an order-magnitude improvement in event rates for some use cases (including CMS NanoAOD). The speedup becomes more noticeable as the data read from the event is smaller (meaning the traditional ROOT IO overheads are larger). Somewhere in the neighborhood of 5KB objects read per event is the point where bulk IO APIs should be noticeably faster. This PR provides three interfaces:. * A raw, low-level interface. The user can invoke the bulk IO APIs directly and get a buffer of primitive types, serialized or deserialized. It is not envisioned this is used by users, but by API writers. * A TTreeReader-like interface, TTreeReaderFast. This is meant to be used by users; it utilizes the low-level interface to retrieve the serialized buffer and templated code will inline the deserializing as part of iterating through the tree. The speed here is gained by the fact only a single pass in memory is needed to deserialize and apply user code, rather than two. * The intent is that this interface can be extended in the future and used by ",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2519
https://github.com/root-project/root/pull/2519:349,integrability,API,APIs,349,"Bulk I/O - v3; This is the second attempt for merging the bulk IO APIs (hidden inside the ROOT::Internal namespace); original version was in #943; this is a large rebase and update (particularly, removing the buffer sharing between the ROOT and the user). The initial goal here is to get another round of @pcanal attention and feedback. The bulk IO APIs aim to provide as fast access as possible in the case where the user is using relatively simple datatypes and the objects can be deserialized ""non-destructively"" (i.e., in-place in the existing memory buffers). For example - primitives and lists of primitives are supported; anything needing references are not. Despite the fact many of ROOT's very useful deserialization features are not supported, the limited feature set covers a range of analysis use cases. In particular, the current version of CMS's proposed NanoAOD format can be read solely via bulk APIs. Where the bulk IO APIs are applicable, we see an order-magnitude improvement in event rates for some use cases (including CMS NanoAOD). The speedup becomes more noticeable as the data read from the event is smaller (meaning the traditional ROOT IO overheads are larger). Somewhere in the neighborhood of 5KB objects read per event is the point where bulk IO APIs should be noticeably faster. This PR provides three interfaces:. * A raw, low-level interface. The user can invoke the bulk IO APIs directly and get a buffer of primitive types, serialized or deserialized. It is not envisioned this is used by users, but by API writers. * A TTreeReader-like interface, TTreeReaderFast. This is meant to be used by users; it utilizes the low-level interface to retrieve the serialized buffer and templated code will inline the deserializing as part of iterating through the tree. The speed here is gained by the fact only a single pass in memory is needed to deserialize and apply user code, rather than two. * The intent is that this interface can be extended in the future and used by ",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2519
https://github.com/root-project/root/pull/2519:555,integrability,buffer,buffers,555,"Bulk I/O - v3; This is the second attempt for merging the bulk IO APIs (hidden inside the ROOT::Internal namespace); original version was in #943; this is a large rebase and update (particularly, removing the buffer sharing between the ROOT and the user). The initial goal here is to get another round of @pcanal attention and feedback. The bulk IO APIs aim to provide as fast access as possible in the case where the user is using relatively simple datatypes and the objects can be deserialized ""non-destructively"" (i.e., in-place in the existing memory buffers). For example - primitives and lists of primitives are supported; anything needing references are not. Despite the fact many of ROOT's very useful deserialization features are not supported, the limited feature set covers a range of analysis use cases. In particular, the current version of CMS's proposed NanoAOD format can be read solely via bulk APIs. Where the bulk IO APIs are applicable, we see an order-magnitude improvement in event rates for some use cases (including CMS NanoAOD). The speedup becomes more noticeable as the data read from the event is smaller (meaning the traditional ROOT IO overheads are larger). Somewhere in the neighborhood of 5KB objects read per event is the point where bulk IO APIs should be noticeably faster. This PR provides three interfaces:. * A raw, low-level interface. The user can invoke the bulk IO APIs directly and get a buffer of primitive types, serialized or deserialized. It is not envisioned this is used by users, but by API writers. * A TTreeReader-like interface, TTreeReaderFast. This is meant to be used by users; it utilizes the low-level interface to retrieve the serialized buffer and templated code will inline the deserializing as part of iterating through the tree. The speed here is gained by the fact only a single pass in memory is needed to deserialize and apply user code, rather than two. * The intent is that this interface can be extended in the future and used by ",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2519
https://github.com/root-project/root/pull/2519:843,integrability,version,version,843,"Bulk I/O - v3; This is the second attempt for merging the bulk IO APIs (hidden inside the ROOT::Internal namespace); original version was in #943; this is a large rebase and update (particularly, removing the buffer sharing between the ROOT and the user). The initial goal here is to get another round of @pcanal attention and feedback. The bulk IO APIs aim to provide as fast access as possible in the case where the user is using relatively simple datatypes and the objects can be deserialized ""non-destructively"" (i.e., in-place in the existing memory buffers). For example - primitives and lists of primitives are supported; anything needing references are not. Despite the fact many of ROOT's very useful deserialization features are not supported, the limited feature set covers a range of analysis use cases. In particular, the current version of CMS's proposed NanoAOD format can be read solely via bulk APIs. Where the bulk IO APIs are applicable, we see an order-magnitude improvement in event rates for some use cases (including CMS NanoAOD). The speedup becomes more noticeable as the data read from the event is smaller (meaning the traditional ROOT IO overheads are larger). Somewhere in the neighborhood of 5KB objects read per event is the point where bulk IO APIs should be noticeably faster. This PR provides three interfaces:. * A raw, low-level interface. The user can invoke the bulk IO APIs directly and get a buffer of primitive types, serialized or deserialized. It is not envisioned this is used by users, but by API writers. * A TTreeReader-like interface, TTreeReaderFast. This is meant to be used by users; it utilizes the low-level interface to retrieve the serialized buffer and templated code will inline the deserializing as part of iterating through the tree. The speed here is gained by the fact only a single pass in memory is needed to deserialize and apply user code, rather than two. * The intent is that this interface can be extended in the future and used by ",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2519
https://github.com/root-project/root/pull/2519:912,integrability,API,APIs,912,"Bulk I/O - v3; This is the second attempt for merging the bulk IO APIs (hidden inside the ROOT::Internal namespace); original version was in #943; this is a large rebase and update (particularly, removing the buffer sharing between the ROOT and the user). The initial goal here is to get another round of @pcanal attention and feedback. The bulk IO APIs aim to provide as fast access as possible in the case where the user is using relatively simple datatypes and the objects can be deserialized ""non-destructively"" (i.e., in-place in the existing memory buffers). For example - primitives and lists of primitives are supported; anything needing references are not. Despite the fact many of ROOT's very useful deserialization features are not supported, the limited feature set covers a range of analysis use cases. In particular, the current version of CMS's proposed NanoAOD format can be read solely via bulk APIs. Where the bulk IO APIs are applicable, we see an order-magnitude improvement in event rates for some use cases (including CMS NanoAOD). The speedup becomes more noticeable as the data read from the event is smaller (meaning the traditional ROOT IO overheads are larger). Somewhere in the neighborhood of 5KB objects read per event is the point where bulk IO APIs should be noticeably faster. This PR provides three interfaces:. * A raw, low-level interface. The user can invoke the bulk IO APIs directly and get a buffer of primitive types, serialized or deserialized. It is not envisioned this is used by users, but by API writers. * A TTreeReader-like interface, TTreeReaderFast. This is meant to be used by users; it utilizes the low-level interface to retrieve the serialized buffer and templated code will inline the deserializing as part of iterating through the tree. The speed here is gained by the fact only a single pass in memory is needed to deserialize and apply user code, rather than two. * The intent is that this interface can be extended in the future and used by ",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2519
https://github.com/root-project/root/pull/2519:936,integrability,API,APIs,936,"Bulk I/O - v3; This is the second attempt for merging the bulk IO APIs (hidden inside the ROOT::Internal namespace); original version was in #943; this is a large rebase and update (particularly, removing the buffer sharing between the ROOT and the user). The initial goal here is to get another round of @pcanal attention and feedback. The bulk IO APIs aim to provide as fast access as possible in the case where the user is using relatively simple datatypes and the objects can be deserialized ""non-destructively"" (i.e., in-place in the existing memory buffers). For example - primitives and lists of primitives are supported; anything needing references are not. Despite the fact many of ROOT's very useful deserialization features are not supported, the limited feature set covers a range of analysis use cases. In particular, the current version of CMS's proposed NanoAOD format can be read solely via bulk APIs. Where the bulk IO APIs are applicable, we see an order-magnitude improvement in event rates for some use cases (including CMS NanoAOD). The speedup becomes more noticeable as the data read from the event is smaller (meaning the traditional ROOT IO overheads are larger). Somewhere in the neighborhood of 5KB objects read per event is the point where bulk IO APIs should be noticeably faster. This PR provides three interfaces:. * A raw, low-level interface. The user can invoke the bulk IO APIs directly and get a buffer of primitive types, serialized or deserialized. It is not envisioned this is used by users, but by API writers. * A TTreeReader-like interface, TTreeReaderFast. This is meant to be used by users; it utilizes the low-level interface to retrieve the serialized buffer and templated code will inline the deserializing as part of iterating through the tree. The speed here is gained by the fact only a single pass in memory is needed to deserialize and apply user code, rather than two. * The intent is that this interface can be extended in the future and used by ",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2519
https://github.com/root-project/root/pull/2519:998,integrability,event,event,998,"ulk I/O - v3; This is the second attempt for merging the bulk IO APIs (hidden inside the ROOT::Internal namespace); original version was in #943; this is a large rebase and update (particularly, removing the buffer sharing between the ROOT and the user). The initial goal here is to get another round of @pcanal attention and feedback. The bulk IO APIs aim to provide as fast access as possible in the case where the user is using relatively simple datatypes and the objects can be deserialized ""non-destructively"" (i.e., in-place in the existing memory buffers). For example - primitives and lists of primitives are supported; anything needing references are not. Despite the fact many of ROOT's very useful deserialization features are not supported, the limited feature set covers a range of analysis use cases. In particular, the current version of CMS's proposed NanoAOD format can be read solely via bulk APIs. Where the bulk IO APIs are applicable, we see an order-magnitude improvement in event rates for some use cases (including CMS NanoAOD). The speedup becomes more noticeable as the data read from the event is smaller (meaning the traditional ROOT IO overheads are larger). Somewhere in the neighborhood of 5KB objects read per event is the point where bulk IO APIs should be noticeably faster. This PR provides three interfaces:. * A raw, low-level interface. The user can invoke the bulk IO APIs directly and get a buffer of primitive types, serialized or deserialized. It is not envisioned this is used by users, but by API writers. * A TTreeReader-like interface, TTreeReaderFast. This is meant to be used by users; it utilizes the low-level interface to retrieve the serialized buffer and templated code will inline the deserializing as part of iterating through the tree. The speed here is gained by the fact only a single pass in memory is needed to deserialize and apply user code, rather than two. * The intent is that this interface can be extended in the future and used by T",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2519
https://github.com/root-project/root/pull/2519:1116,integrability,event,event,1116,"iginal version was in #943; this is a large rebase and update (particularly, removing the buffer sharing between the ROOT and the user). The initial goal here is to get another round of @pcanal attention and feedback. The bulk IO APIs aim to provide as fast access as possible in the case where the user is using relatively simple datatypes and the objects can be deserialized ""non-destructively"" (i.e., in-place in the existing memory buffers). For example - primitives and lists of primitives are supported; anything needing references are not. Despite the fact many of ROOT's very useful deserialization features are not supported, the limited feature set covers a range of analysis use cases. In particular, the current version of CMS's proposed NanoAOD format can be read solely via bulk APIs. Where the bulk IO APIs are applicable, we see an order-magnitude improvement in event rates for some use cases (including CMS NanoAOD). The speedup becomes more noticeable as the data read from the event is smaller (meaning the traditional ROOT IO overheads are larger). Somewhere in the neighborhood of 5KB objects read per event is the point where bulk IO APIs should be noticeably faster. This PR provides three interfaces:. * A raw, low-level interface. The user can invoke the bulk IO APIs directly and get a buffer of primitive types, serialized or deserialized. It is not envisioned this is used by users, but by API writers. * A TTreeReader-like interface, TTreeReaderFast. This is meant to be used by users; it utilizes the low-level interface to retrieve the serialized buffer and templated code will inline the deserializing as part of iterating through the tree. The speed here is gained by the fact only a single pass in memory is needed to deserialize and apply user code, rather than two. * The intent is that this interface can be extended in the future and used by TDataFrame, provided we can make TDataFrame sufficiently fast. * A Python-based numpy export interface. This exposes th",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2519
https://github.com/root-project/root/pull/2519:1243,integrability,event,event,1243,"he user). The initial goal here is to get another round of @pcanal attention and feedback. The bulk IO APIs aim to provide as fast access as possible in the case where the user is using relatively simple datatypes and the objects can be deserialized ""non-destructively"" (i.e., in-place in the existing memory buffers). For example - primitives and lists of primitives are supported; anything needing references are not. Despite the fact many of ROOT's very useful deserialization features are not supported, the limited feature set covers a range of analysis use cases. In particular, the current version of CMS's proposed NanoAOD format can be read solely via bulk APIs. Where the bulk IO APIs are applicable, we see an order-magnitude improvement in event rates for some use cases (including CMS NanoAOD). The speedup becomes more noticeable as the data read from the event is smaller (meaning the traditional ROOT IO overheads are larger). Somewhere in the neighborhood of 5KB objects read per event is the point where bulk IO APIs should be noticeably faster. This PR provides three interfaces:. * A raw, low-level interface. The user can invoke the bulk IO APIs directly and get a buffer of primitive types, serialized or deserialized. It is not envisioned this is used by users, but by API writers. * A TTreeReader-like interface, TTreeReaderFast. This is meant to be used by users; it utilizes the low-level interface to retrieve the serialized buffer and templated code will inline the deserializing as part of iterating through the tree. The speed here is gained by the fact only a single pass in memory is needed to deserialize and apply user code, rather than two. * The intent is that this interface can be extended in the future and used by TDataFrame, provided we can make TDataFrame sufficiently fast. * A Python-based numpy export interface. This exposes the (possibly serialized) memory directly as numpy arrays, allowing for efficient access to ROOT-serialized objects from the Pyth",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2519
https://github.com/root-project/root/pull/2519:1276,integrability,API,APIs,1276,"is to get another round of @pcanal attention and feedback. The bulk IO APIs aim to provide as fast access as possible in the case where the user is using relatively simple datatypes and the objects can be deserialized ""non-destructively"" (i.e., in-place in the existing memory buffers). For example - primitives and lists of primitives are supported; anything needing references are not. Despite the fact many of ROOT's very useful deserialization features are not supported, the limited feature set covers a range of analysis use cases. In particular, the current version of CMS's proposed NanoAOD format can be read solely via bulk APIs. Where the bulk IO APIs are applicable, we see an order-magnitude improvement in event rates for some use cases (including CMS NanoAOD). The speedup becomes more noticeable as the data read from the event is smaller (meaning the traditional ROOT IO overheads are larger). Somewhere in the neighborhood of 5KB objects read per event is the point where bulk IO APIs should be noticeably faster. This PR provides three interfaces:. * A raw, low-level interface. The user can invoke the bulk IO APIs directly and get a buffer of primitive types, serialized or deserialized. It is not envisioned this is used by users, but by API writers. * A TTreeReader-like interface, TTreeReaderFast. This is meant to be used by users; it utilizes the low-level interface to retrieve the serialized buffer and templated code will inline the deserializing as part of iterating through the tree. The speed here is gained by the fact only a single pass in memory is needed to deserialize and apply user code, rather than two. * The intent is that this interface can be extended in the future and used by TDataFrame, provided we can make TDataFrame sufficiently fast. * A Python-based numpy export interface. This exposes the (possibly serialized) memory directly as numpy arrays, allowing for efficient access to ROOT-serialized objects from the Python language. * The numpy interfa",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2519
https://github.com/root-project/root/pull/2519:1333,integrability,interfac,interfaces,1333,"he bulk IO APIs aim to provide as fast access as possible in the case where the user is using relatively simple datatypes and the objects can be deserialized ""non-destructively"" (i.e., in-place in the existing memory buffers). For example - primitives and lists of primitives are supported; anything needing references are not. Despite the fact many of ROOT's very useful deserialization features are not supported, the limited feature set covers a range of analysis use cases. In particular, the current version of CMS's proposed NanoAOD format can be read solely via bulk APIs. Where the bulk IO APIs are applicable, we see an order-magnitude improvement in event rates for some use cases (including CMS NanoAOD). The speedup becomes more noticeable as the data read from the event is smaller (meaning the traditional ROOT IO overheads are larger). Somewhere in the neighborhood of 5KB objects read per event is the point where bulk IO APIs should be noticeably faster. This PR provides three interfaces:. * A raw, low-level interface. The user can invoke the bulk IO APIs directly and get a buffer of primitive types, serialized or deserialized. It is not envisioned this is used by users, but by API writers. * A TTreeReader-like interface, TTreeReaderFast. This is meant to be used by users; it utilizes the low-level interface to retrieve the serialized buffer and templated code will inline the deserializing as part of iterating through the tree. The speed here is gained by the fact only a single pass in memory is needed to deserialize and apply user code, rather than two. * The intent is that this interface can be extended in the future and used by TDataFrame, provided we can make TDataFrame sufficiently fast. * A Python-based numpy export interface. This exposes the (possibly serialized) memory directly as numpy arrays, allowing for efficient access to ROOT-serialized objects from the Python language. * The numpy interface is low-level for Python; we intend to further explore the",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2519
https://github.com/root-project/root/pull/2519:1365,integrability,interfac,interface,1365,"s fast access as possible in the case where the user is using relatively simple datatypes and the objects can be deserialized ""non-destructively"" (i.e., in-place in the existing memory buffers). For example - primitives and lists of primitives are supported; anything needing references are not. Despite the fact many of ROOT's very useful deserialization features are not supported, the limited feature set covers a range of analysis use cases. In particular, the current version of CMS's proposed NanoAOD format can be read solely via bulk APIs. Where the bulk IO APIs are applicable, we see an order-magnitude improvement in event rates for some use cases (including CMS NanoAOD). The speedup becomes more noticeable as the data read from the event is smaller (meaning the traditional ROOT IO overheads are larger). Somewhere in the neighborhood of 5KB objects read per event is the point where bulk IO APIs should be noticeably faster. This PR provides three interfaces:. * A raw, low-level interface. The user can invoke the bulk IO APIs directly and get a buffer of primitive types, serialized or deserialized. It is not envisioned this is used by users, but by API writers. * A TTreeReader-like interface, TTreeReaderFast. This is meant to be used by users; it utilizes the low-level interface to retrieve the serialized buffer and templated code will inline the deserializing as part of iterating through the tree. The speed here is gained by the fact only a single pass in memory is needed to deserialize and apply user code, rather than two. * The intent is that this interface can be extended in the future and used by TDataFrame, provided we can make TDataFrame sufficiently fast. * A Python-based numpy export interface. This exposes the (possibly serialized) memory directly as numpy arrays, allowing for efficient access to ROOT-serialized objects from the Python language. * The numpy interface is low-level for Python; we intend to further explore the ideal high-level interface thro",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2519
https://github.com/root-project/root/pull/2519:1408,integrability,API,APIs,1408,"ere the user is using relatively simple datatypes and the objects can be deserialized ""non-destructively"" (i.e., in-place in the existing memory buffers). For example - primitives and lists of primitives are supported; anything needing references are not. Despite the fact many of ROOT's very useful deserialization features are not supported, the limited feature set covers a range of analysis use cases. In particular, the current version of CMS's proposed NanoAOD format can be read solely via bulk APIs. Where the bulk IO APIs are applicable, we see an order-magnitude improvement in event rates for some use cases (including CMS NanoAOD). The speedup becomes more noticeable as the data read from the event is smaller (meaning the traditional ROOT IO overheads are larger). Somewhere in the neighborhood of 5KB objects read per event is the point where bulk IO APIs should be noticeably faster. This PR provides three interfaces:. * A raw, low-level interface. The user can invoke the bulk IO APIs directly and get a buffer of primitive types, serialized or deserialized. It is not envisioned this is used by users, but by API writers. * A TTreeReader-like interface, TTreeReaderFast. This is meant to be used by users; it utilizes the low-level interface to retrieve the serialized buffer and templated code will inline the deserializing as part of iterating through the tree. The speed here is gained by the fact only a single pass in memory is needed to deserialize and apply user code, rather than two. * The intent is that this interface can be extended in the future and used by TDataFrame, provided we can make TDataFrame sufficiently fast. * A Python-based numpy export interface. This exposes the (possibly serialized) memory directly as numpy arrays, allowing for efficient access to ROOT-serialized objects from the Python language. * The numpy interface is low-level for Python; we intend to further explore the ideal high-level interface through projects such as https://github.com/",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2519
https://github.com/root-project/root/pull/2519:1432,integrability,buffer,buffer,1432,"atively simple datatypes and the objects can be deserialized ""non-destructively"" (i.e., in-place in the existing memory buffers). For example - primitives and lists of primitives are supported; anything needing references are not. Despite the fact many of ROOT's very useful deserialization features are not supported, the limited feature set covers a range of analysis use cases. In particular, the current version of CMS's proposed NanoAOD format can be read solely via bulk APIs. Where the bulk IO APIs are applicable, we see an order-magnitude improvement in event rates for some use cases (including CMS NanoAOD). The speedup becomes more noticeable as the data read from the event is smaller (meaning the traditional ROOT IO overheads are larger). Somewhere in the neighborhood of 5KB objects read per event is the point where bulk IO APIs should be noticeably faster. This PR provides three interfaces:. * A raw, low-level interface. The user can invoke the bulk IO APIs directly and get a buffer of primitive types, serialized or deserialized. It is not envisioned this is used by users, but by API writers. * A TTreeReader-like interface, TTreeReaderFast. This is meant to be used by users; it utilizes the low-level interface to retrieve the serialized buffer and templated code will inline the deserializing as part of iterating through the tree. The speed here is gained by the fact only a single pass in memory is needed to deserialize and apply user code, rather than two. * The intent is that this interface can be extended in the future and used by TDataFrame, provided we can make TDataFrame sufficiently fast. * A Python-based numpy export interface. This exposes the (possibly serialized) memory directly as numpy arrays, allowing for efficient access to ROOT-serialized objects from the Python language. * The numpy interface is low-level for Python; we intend to further explore the ideal high-level interface through projects such as https://github.com/diana-hep/plur. (With man",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2519
https://github.com/root-project/root/pull/2519:1538,integrability,API,API,1538,"in-place in the existing memory buffers). For example - primitives and lists of primitives are supported; anything needing references are not. Despite the fact many of ROOT's very useful deserialization features are not supported, the limited feature set covers a range of analysis use cases. In particular, the current version of CMS's proposed NanoAOD format can be read solely via bulk APIs. Where the bulk IO APIs are applicable, we see an order-magnitude improvement in event rates for some use cases (including CMS NanoAOD). The speedup becomes more noticeable as the data read from the event is smaller (meaning the traditional ROOT IO overheads are larger). Somewhere in the neighborhood of 5KB objects read per event is the point where bulk IO APIs should be noticeably faster. This PR provides three interfaces:. * A raw, low-level interface. The user can invoke the bulk IO APIs directly and get a buffer of primitive types, serialized or deserialized. It is not envisioned this is used by users, but by API writers. * A TTreeReader-like interface, TTreeReaderFast. This is meant to be used by users; it utilizes the low-level interface to retrieve the serialized buffer and templated code will inline the deserializing as part of iterating through the tree. The speed here is gained by the fact only a single pass in memory is needed to deserialize and apply user code, rather than two. * The intent is that this interface can be extended in the future and used by TDataFrame, provided we can make TDataFrame sufficiently fast. * A Python-based numpy export interface. This exposes the (possibly serialized) memory directly as numpy arrays, allowing for efficient access to ROOT-serialized objects from the Python language. * The numpy interface is low-level for Python; we intend to further explore the ideal high-level interface through projects such as https://github.com/diana-hep/plur. (With many thanks to @jpivarski for driving this work home with his work on a python interface!).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2519
https://github.com/root-project/root/pull/2519:1572,integrability,interfac,interface,1572,"in-place in the existing memory buffers). For example - primitives and lists of primitives are supported; anything needing references are not. Despite the fact many of ROOT's very useful deserialization features are not supported, the limited feature set covers a range of analysis use cases. In particular, the current version of CMS's proposed NanoAOD format can be read solely via bulk APIs. Where the bulk IO APIs are applicable, we see an order-magnitude improvement in event rates for some use cases (including CMS NanoAOD). The speedup becomes more noticeable as the data read from the event is smaller (meaning the traditional ROOT IO overheads are larger). Somewhere in the neighborhood of 5KB objects read per event is the point where bulk IO APIs should be noticeably faster. This PR provides three interfaces:. * A raw, low-level interface. The user can invoke the bulk IO APIs directly and get a buffer of primitive types, serialized or deserialized. It is not envisioned this is used by users, but by API writers. * A TTreeReader-like interface, TTreeReaderFast. This is meant to be used by users; it utilizes the low-level interface to retrieve the serialized buffer and templated code will inline the deserializing as part of iterating through the tree. The speed here is gained by the fact only a single pass in memory is needed to deserialize and apply user code, rather than two. * The intent is that this interface can be extended in the future and used by TDataFrame, provided we can make TDataFrame sufficiently fast. * A Python-based numpy export interface. This exposes the (possibly serialized) memory directly as numpy arrays, allowing for efficient access to ROOT-serialized objects from the Python language. * The numpy interface is low-level for Python; we intend to further explore the ideal high-level interface through projects such as https://github.com/diana-hep/plur. (With many thanks to @jpivarski for driving this work home with his work on a python interface!).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2519
https://github.com/root-project/root/pull/2519:1661,integrability,interfac,interface,1661,"in-place in the existing memory buffers). For example - primitives and lists of primitives are supported; anything needing references are not. Despite the fact many of ROOT's very useful deserialization features are not supported, the limited feature set covers a range of analysis use cases. In particular, the current version of CMS's proposed NanoAOD format can be read solely via bulk APIs. Where the bulk IO APIs are applicable, we see an order-magnitude improvement in event rates for some use cases (including CMS NanoAOD). The speedup becomes more noticeable as the data read from the event is smaller (meaning the traditional ROOT IO overheads are larger). Somewhere in the neighborhood of 5KB objects read per event is the point where bulk IO APIs should be noticeably faster. This PR provides three interfaces:. * A raw, low-level interface. The user can invoke the bulk IO APIs directly and get a buffer of primitive types, serialized or deserialized. It is not envisioned this is used by users, but by API writers. * A TTreeReader-like interface, TTreeReaderFast. This is meant to be used by users; it utilizes the low-level interface to retrieve the serialized buffer and templated code will inline the deserializing as part of iterating through the tree. The speed here is gained by the fact only a single pass in memory is needed to deserialize and apply user code, rather than two. * The intent is that this interface can be extended in the future and used by TDataFrame, provided we can make TDataFrame sufficiently fast. * A Python-based numpy export interface. This exposes the (possibly serialized) memory directly as numpy arrays, allowing for efficient access to ROOT-serialized objects from the Python language. * The numpy interface is low-level for Python; we intend to further explore the ideal high-level interface through projects such as https://github.com/diana-hep/plur. (With many thanks to @jpivarski for driving this work home with his work on a python interface!).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2519
https://github.com/root-project/root/pull/2519:1698,integrability,buffer,buffer,1698,"in-place in the existing memory buffers). For example - primitives and lists of primitives are supported; anything needing references are not. Despite the fact many of ROOT's very useful deserialization features are not supported, the limited feature set covers a range of analysis use cases. In particular, the current version of CMS's proposed NanoAOD format can be read solely via bulk APIs. Where the bulk IO APIs are applicable, we see an order-magnitude improvement in event rates for some use cases (including CMS NanoAOD). The speedup becomes more noticeable as the data read from the event is smaller (meaning the traditional ROOT IO overheads are larger). Somewhere in the neighborhood of 5KB objects read per event is the point where bulk IO APIs should be noticeably faster. This PR provides three interfaces:. * A raw, low-level interface. The user can invoke the bulk IO APIs directly and get a buffer of primitive types, serialized or deserialized. It is not envisioned this is used by users, but by API writers. * A TTreeReader-like interface, TTreeReaderFast. This is meant to be used by users; it utilizes the low-level interface to retrieve the serialized buffer and templated code will inline the deserializing as part of iterating through the tree. The speed here is gained by the fact only a single pass in memory is needed to deserialize and apply user code, rather than two. * The intent is that this interface can be extended in the future and used by TDataFrame, provided we can make TDataFrame sufficiently fast. * A Python-based numpy export interface. This exposes the (possibly serialized) memory directly as numpy arrays, allowing for efficient access to ROOT-serialized objects from the Python language. * The numpy interface is low-level for Python; we intend to further explore the ideal high-level interface through projects such as https://github.com/diana-hep/plur. (With many thanks to @jpivarski for driving this work home with his work on a python interface!).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2519
https://github.com/root-project/root/pull/2519:1948,integrability,interfac,interface,1948,"in-place in the existing memory buffers). For example - primitives and lists of primitives are supported; anything needing references are not. Despite the fact many of ROOT's very useful deserialization features are not supported, the limited feature set covers a range of analysis use cases. In particular, the current version of CMS's proposed NanoAOD format can be read solely via bulk APIs. Where the bulk IO APIs are applicable, we see an order-magnitude improvement in event rates for some use cases (including CMS NanoAOD). The speedup becomes more noticeable as the data read from the event is smaller (meaning the traditional ROOT IO overheads are larger). Somewhere in the neighborhood of 5KB objects read per event is the point where bulk IO APIs should be noticeably faster. This PR provides three interfaces:. * A raw, low-level interface. The user can invoke the bulk IO APIs directly and get a buffer of primitive types, serialized or deserialized. It is not envisioned this is used by users, but by API writers. * A TTreeReader-like interface, TTreeReaderFast. This is meant to be used by users; it utilizes the low-level interface to retrieve the serialized buffer and templated code will inline the deserializing as part of iterating through the tree. The speed here is gained by the fact only a single pass in memory is needed to deserialize and apply user code, rather than two. * The intent is that this interface can be extended in the future and used by TDataFrame, provided we can make TDataFrame sufficiently fast. * A Python-based numpy export interface. This exposes the (possibly serialized) memory directly as numpy arrays, allowing for efficient access to ROOT-serialized objects from the Python language. * The numpy interface is low-level for Python; we intend to further explore the ideal high-level interface through projects such as https://github.com/diana-hep/plur. (With many thanks to @jpivarski for driving this work home with his work on a python interface!).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2519
https://github.com/root-project/root/pull/2519:2093,integrability,interfac,interface,2093,"in-place in the existing memory buffers). For example - primitives and lists of primitives are supported; anything needing references are not. Despite the fact many of ROOT's very useful deserialization features are not supported, the limited feature set covers a range of analysis use cases. In particular, the current version of CMS's proposed NanoAOD format can be read solely via bulk APIs. Where the bulk IO APIs are applicable, we see an order-magnitude improvement in event rates for some use cases (including CMS NanoAOD). The speedup becomes more noticeable as the data read from the event is smaller (meaning the traditional ROOT IO overheads are larger). Somewhere in the neighborhood of 5KB objects read per event is the point where bulk IO APIs should be noticeably faster. This PR provides three interfaces:. * A raw, low-level interface. The user can invoke the bulk IO APIs directly and get a buffer of primitive types, serialized or deserialized. It is not envisioned this is used by users, but by API writers. * A TTreeReader-like interface, TTreeReaderFast. This is meant to be used by users; it utilizes the low-level interface to retrieve the serialized buffer and templated code will inline the deserializing as part of iterating through the tree. The speed here is gained by the fact only a single pass in memory is needed to deserialize and apply user code, rather than two. * The intent is that this interface can be extended in the future and used by TDataFrame, provided we can make TDataFrame sufficiently fast. * A Python-based numpy export interface. This exposes the (possibly serialized) memory directly as numpy arrays, allowing for efficient access to ROOT-serialized objects from the Python language. * The numpy interface is low-level for Python; we intend to further explore the ideal high-level interface through projects such as https://github.com/diana-hep/plur. (With many thanks to @jpivarski for driving this work home with his work on a python interface!).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2519
https://github.com/root-project/root/pull/2519:2271,integrability,interfac,interface,2271,"in-place in the existing memory buffers). For example - primitives and lists of primitives are supported; anything needing references are not. Despite the fact many of ROOT's very useful deserialization features are not supported, the limited feature set covers a range of analysis use cases. In particular, the current version of CMS's proposed NanoAOD format can be read solely via bulk APIs. Where the bulk IO APIs are applicable, we see an order-magnitude improvement in event rates for some use cases (including CMS NanoAOD). The speedup becomes more noticeable as the data read from the event is smaller (meaning the traditional ROOT IO overheads are larger). Somewhere in the neighborhood of 5KB objects read per event is the point where bulk IO APIs should be noticeably faster. This PR provides three interfaces:. * A raw, low-level interface. The user can invoke the bulk IO APIs directly and get a buffer of primitive types, serialized or deserialized. It is not envisioned this is used by users, but by API writers. * A TTreeReader-like interface, TTreeReaderFast. This is meant to be used by users; it utilizes the low-level interface to retrieve the serialized buffer and templated code will inline the deserializing as part of iterating through the tree. The speed here is gained by the fact only a single pass in memory is needed to deserialize and apply user code, rather than two. * The intent is that this interface can be extended in the future and used by TDataFrame, provided we can make TDataFrame sufficiently fast. * A Python-based numpy export interface. This exposes the (possibly serialized) memory directly as numpy arrays, allowing for efficient access to ROOT-serialized objects from the Python language. * The numpy interface is low-level for Python; we intend to further explore the ideal high-level interface through projects such as https://github.com/diana-hep/plur. (With many thanks to @jpivarski for driving this work home with his work on a python interface!).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2519
https://github.com/root-project/root/pull/2519:2356,integrability,interfac,interface,2356,"in-place in the existing memory buffers). For example - primitives and lists of primitives are supported; anything needing references are not. Despite the fact many of ROOT's very useful deserialization features are not supported, the limited feature set covers a range of analysis use cases. In particular, the current version of CMS's proposed NanoAOD format can be read solely via bulk APIs. Where the bulk IO APIs are applicable, we see an order-magnitude improvement in event rates for some use cases (including CMS NanoAOD). The speedup becomes more noticeable as the data read from the event is smaller (meaning the traditional ROOT IO overheads are larger). Somewhere in the neighborhood of 5KB objects read per event is the point where bulk IO APIs should be noticeably faster. This PR provides three interfaces:. * A raw, low-level interface. The user can invoke the bulk IO APIs directly and get a buffer of primitive types, serialized or deserialized. It is not envisioned this is used by users, but by API writers. * A TTreeReader-like interface, TTreeReaderFast. This is meant to be used by users; it utilizes the low-level interface to retrieve the serialized buffer and templated code will inline the deserializing as part of iterating through the tree. The speed here is gained by the fact only a single pass in memory is needed to deserialize and apply user code, rather than two. * The intent is that this interface can be extended in the future and used by TDataFrame, provided we can make TDataFrame sufficiently fast. * A Python-based numpy export interface. This exposes the (possibly serialized) memory directly as numpy arrays, allowing for efficient access to ROOT-serialized objects from the Python language. * The numpy interface is low-level for Python; we intend to further explore the ideal high-level interface through projects such as https://github.com/diana-hep/plur. (With many thanks to @jpivarski for driving this work home with his work on a python interface!).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2519
https://github.com/root-project/root/pull/2519:2511,integrability,interfac,interface,2511,"in-place in the existing memory buffers). For example - primitives and lists of primitives are supported; anything needing references are not. Despite the fact many of ROOT's very useful deserialization features are not supported, the limited feature set covers a range of analysis use cases. In particular, the current version of CMS's proposed NanoAOD format can be read solely via bulk APIs. Where the bulk IO APIs are applicable, we see an order-magnitude improvement in event rates for some use cases (including CMS NanoAOD). The speedup becomes more noticeable as the data read from the event is smaller (meaning the traditional ROOT IO overheads are larger). Somewhere in the neighborhood of 5KB objects read per event is the point where bulk IO APIs should be noticeably faster. This PR provides three interfaces:. * A raw, low-level interface. The user can invoke the bulk IO APIs directly and get a buffer of primitive types, serialized or deserialized. It is not envisioned this is used by users, but by API writers. * A TTreeReader-like interface, TTreeReaderFast. This is meant to be used by users; it utilizes the low-level interface to retrieve the serialized buffer and templated code will inline the deserializing as part of iterating through the tree. The speed here is gained by the fact only a single pass in memory is needed to deserialize and apply user code, rather than two. * The intent is that this interface can be extended in the future and used by TDataFrame, provided we can make TDataFrame sufficiently fast. * A Python-based numpy export interface. This exposes the (possibly serialized) memory directly as numpy arrays, allowing for efficient access to ROOT-serialized objects from the Python language. * The numpy interface is low-level for Python; we intend to further explore the ideal high-level interface through projects such as https://github.com/diana-hep/plur. (With many thanks to @jpivarski for driving this work home with his work on a python interface!).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2519
https://github.com/root-project/root/pull/2519:66,interoperability,API,APIs,66,"Bulk I/O - v3; This is the second attempt for merging the bulk IO APIs (hidden inside the ROOT::Internal namespace); original version was in #943; this is a large rebase and update (particularly, removing the buffer sharing between the ROOT and the user). The initial goal here is to get another round of @pcanal attention and feedback. The bulk IO APIs aim to provide as fast access as possible in the case where the user is using relatively simple datatypes and the objects can be deserialized ""non-destructively"" (i.e., in-place in the existing memory buffers). For example - primitives and lists of primitives are supported; anything needing references are not. Despite the fact many of ROOT's very useful deserialization features are not supported, the limited feature set covers a range of analysis use cases. In particular, the current version of CMS's proposed NanoAOD format can be read solely via bulk APIs. Where the bulk IO APIs are applicable, we see an order-magnitude improvement in event rates for some use cases (including CMS NanoAOD). The speedup becomes more noticeable as the data read from the event is smaller (meaning the traditional ROOT IO overheads are larger). Somewhere in the neighborhood of 5KB objects read per event is the point where bulk IO APIs should be noticeably faster. This PR provides three interfaces:. * A raw, low-level interface. The user can invoke the bulk IO APIs directly and get a buffer of primitive types, serialized or deserialized. It is not envisioned this is used by users, but by API writers. * A TTreeReader-like interface, TTreeReaderFast. This is meant to be used by users; it utilizes the low-level interface to retrieve the serialized buffer and templated code will inline the deserializing as part of iterating through the tree. The speed here is gained by the fact only a single pass in memory is needed to deserialize and apply user code, rather than two. * The intent is that this interface can be extended in the future and used by ",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2519
https://github.com/root-project/root/pull/2519:349,interoperability,API,APIs,349,"Bulk I/O - v3; This is the second attempt for merging the bulk IO APIs (hidden inside the ROOT::Internal namespace); original version was in #943; this is a large rebase and update (particularly, removing the buffer sharing between the ROOT and the user). The initial goal here is to get another round of @pcanal attention and feedback. The bulk IO APIs aim to provide as fast access as possible in the case where the user is using relatively simple datatypes and the objects can be deserialized ""non-destructively"" (i.e., in-place in the existing memory buffers). For example - primitives and lists of primitives are supported; anything needing references are not. Despite the fact many of ROOT's very useful deserialization features are not supported, the limited feature set covers a range of analysis use cases. In particular, the current version of CMS's proposed NanoAOD format can be read solely via bulk APIs. Where the bulk IO APIs are applicable, we see an order-magnitude improvement in event rates for some use cases (including CMS NanoAOD). The speedup becomes more noticeable as the data read from the event is smaller (meaning the traditional ROOT IO overheads are larger). Somewhere in the neighborhood of 5KB objects read per event is the point where bulk IO APIs should be noticeably faster. This PR provides three interfaces:. * A raw, low-level interface. The user can invoke the bulk IO APIs directly and get a buffer of primitive types, serialized or deserialized. It is not envisioned this is used by users, but by API writers. * A TTreeReader-like interface, TTreeReaderFast. This is meant to be used by users; it utilizes the low-level interface to retrieve the serialized buffer and templated code will inline the deserializing as part of iterating through the tree. The speed here is gained by the fact only a single pass in memory is needed to deserialize and apply user code, rather than two. * The intent is that this interface can be extended in the future and used by ",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2519
https://github.com/root-project/root/pull/2519:877,interoperability,format,format,877,"Bulk I/O - v3; This is the second attempt for merging the bulk IO APIs (hidden inside the ROOT::Internal namespace); original version was in #943; this is a large rebase and update (particularly, removing the buffer sharing between the ROOT and the user). The initial goal here is to get another round of @pcanal attention and feedback. The bulk IO APIs aim to provide as fast access as possible in the case where the user is using relatively simple datatypes and the objects can be deserialized ""non-destructively"" (i.e., in-place in the existing memory buffers). For example - primitives and lists of primitives are supported; anything needing references are not. Despite the fact many of ROOT's very useful deserialization features are not supported, the limited feature set covers a range of analysis use cases. In particular, the current version of CMS's proposed NanoAOD format can be read solely via bulk APIs. Where the bulk IO APIs are applicable, we see an order-magnitude improvement in event rates for some use cases (including CMS NanoAOD). The speedup becomes more noticeable as the data read from the event is smaller (meaning the traditional ROOT IO overheads are larger). Somewhere in the neighborhood of 5KB objects read per event is the point where bulk IO APIs should be noticeably faster. This PR provides three interfaces:. * A raw, low-level interface. The user can invoke the bulk IO APIs directly and get a buffer of primitive types, serialized or deserialized. It is not envisioned this is used by users, but by API writers. * A TTreeReader-like interface, TTreeReaderFast. This is meant to be used by users; it utilizes the low-level interface to retrieve the serialized buffer and templated code will inline the deserializing as part of iterating through the tree. The speed here is gained by the fact only a single pass in memory is needed to deserialize and apply user code, rather than two. * The intent is that this interface can be extended in the future and used by ",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2519
https://github.com/root-project/root/pull/2519:912,interoperability,API,APIs,912,"Bulk I/O - v3; This is the second attempt for merging the bulk IO APIs (hidden inside the ROOT::Internal namespace); original version was in #943; this is a large rebase and update (particularly, removing the buffer sharing between the ROOT and the user). The initial goal here is to get another round of @pcanal attention and feedback. The bulk IO APIs aim to provide as fast access as possible in the case where the user is using relatively simple datatypes and the objects can be deserialized ""non-destructively"" (i.e., in-place in the existing memory buffers). For example - primitives and lists of primitives are supported; anything needing references are not. Despite the fact many of ROOT's very useful deserialization features are not supported, the limited feature set covers a range of analysis use cases. In particular, the current version of CMS's proposed NanoAOD format can be read solely via bulk APIs. Where the bulk IO APIs are applicable, we see an order-magnitude improvement in event rates for some use cases (including CMS NanoAOD). The speedup becomes more noticeable as the data read from the event is smaller (meaning the traditional ROOT IO overheads are larger). Somewhere in the neighborhood of 5KB objects read per event is the point where bulk IO APIs should be noticeably faster. This PR provides three interfaces:. * A raw, low-level interface. The user can invoke the bulk IO APIs directly and get a buffer of primitive types, serialized or deserialized. It is not envisioned this is used by users, but by API writers. * A TTreeReader-like interface, TTreeReaderFast. This is meant to be used by users; it utilizes the low-level interface to retrieve the serialized buffer and templated code will inline the deserializing as part of iterating through the tree. The speed here is gained by the fact only a single pass in memory is needed to deserialize and apply user code, rather than two. * The intent is that this interface can be extended in the future and used by ",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2519
https://github.com/root-project/root/pull/2519:936,interoperability,API,APIs,936,"Bulk I/O - v3; This is the second attempt for merging the bulk IO APIs (hidden inside the ROOT::Internal namespace); original version was in #943; this is a large rebase and update (particularly, removing the buffer sharing between the ROOT and the user). The initial goal here is to get another round of @pcanal attention and feedback. The bulk IO APIs aim to provide as fast access as possible in the case where the user is using relatively simple datatypes and the objects can be deserialized ""non-destructively"" (i.e., in-place in the existing memory buffers). For example - primitives and lists of primitives are supported; anything needing references are not. Despite the fact many of ROOT's very useful deserialization features are not supported, the limited feature set covers a range of analysis use cases. In particular, the current version of CMS's proposed NanoAOD format can be read solely via bulk APIs. Where the bulk IO APIs are applicable, we see an order-magnitude improvement in event rates for some use cases (including CMS NanoAOD). The speedup becomes more noticeable as the data read from the event is smaller (meaning the traditional ROOT IO overheads are larger). Somewhere in the neighborhood of 5KB objects read per event is the point where bulk IO APIs should be noticeably faster. This PR provides three interfaces:. * A raw, low-level interface. The user can invoke the bulk IO APIs directly and get a buffer of primitive types, serialized or deserialized. It is not envisioned this is used by users, but by API writers. * A TTreeReader-like interface, TTreeReaderFast. This is meant to be used by users; it utilizes the low-level interface to retrieve the serialized buffer and templated code will inline the deserializing as part of iterating through the tree. The speed here is gained by the fact only a single pass in memory is needed to deserialize and apply user code, rather than two. * The intent is that this interface can be extended in the future and used by ",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2519
https://github.com/root-project/root/pull/2519:1276,interoperability,API,APIs,1276,"is to get another round of @pcanal attention and feedback. The bulk IO APIs aim to provide as fast access as possible in the case where the user is using relatively simple datatypes and the objects can be deserialized ""non-destructively"" (i.e., in-place in the existing memory buffers). For example - primitives and lists of primitives are supported; anything needing references are not. Despite the fact many of ROOT's very useful deserialization features are not supported, the limited feature set covers a range of analysis use cases. In particular, the current version of CMS's proposed NanoAOD format can be read solely via bulk APIs. Where the bulk IO APIs are applicable, we see an order-magnitude improvement in event rates for some use cases (including CMS NanoAOD). The speedup becomes more noticeable as the data read from the event is smaller (meaning the traditional ROOT IO overheads are larger). Somewhere in the neighborhood of 5KB objects read per event is the point where bulk IO APIs should be noticeably faster. This PR provides three interfaces:. * A raw, low-level interface. The user can invoke the bulk IO APIs directly and get a buffer of primitive types, serialized or deserialized. It is not envisioned this is used by users, but by API writers. * A TTreeReader-like interface, TTreeReaderFast. This is meant to be used by users; it utilizes the low-level interface to retrieve the serialized buffer and templated code will inline the deserializing as part of iterating through the tree. The speed here is gained by the fact only a single pass in memory is needed to deserialize and apply user code, rather than two. * The intent is that this interface can be extended in the future and used by TDataFrame, provided we can make TDataFrame sufficiently fast. * A Python-based numpy export interface. This exposes the (possibly serialized) memory directly as numpy arrays, allowing for efficient access to ROOT-serialized objects from the Python language. * The numpy interfa",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2519
https://github.com/root-project/root/pull/2519:1333,interoperability,interfac,interfaces,1333,"he bulk IO APIs aim to provide as fast access as possible in the case where the user is using relatively simple datatypes and the objects can be deserialized ""non-destructively"" (i.e., in-place in the existing memory buffers). For example - primitives and lists of primitives are supported; anything needing references are not. Despite the fact many of ROOT's very useful deserialization features are not supported, the limited feature set covers a range of analysis use cases. In particular, the current version of CMS's proposed NanoAOD format can be read solely via bulk APIs. Where the bulk IO APIs are applicable, we see an order-magnitude improvement in event rates for some use cases (including CMS NanoAOD). The speedup becomes more noticeable as the data read from the event is smaller (meaning the traditional ROOT IO overheads are larger). Somewhere in the neighborhood of 5KB objects read per event is the point where bulk IO APIs should be noticeably faster. This PR provides three interfaces:. * A raw, low-level interface. The user can invoke the bulk IO APIs directly and get a buffer of primitive types, serialized or deserialized. It is not envisioned this is used by users, but by API writers. * A TTreeReader-like interface, TTreeReaderFast. This is meant to be used by users; it utilizes the low-level interface to retrieve the serialized buffer and templated code will inline the deserializing as part of iterating through the tree. The speed here is gained by the fact only a single pass in memory is needed to deserialize and apply user code, rather than two. * The intent is that this interface can be extended in the future and used by TDataFrame, provided we can make TDataFrame sufficiently fast. * A Python-based numpy export interface. This exposes the (possibly serialized) memory directly as numpy arrays, allowing for efficient access to ROOT-serialized objects from the Python language. * The numpy interface is low-level for Python; we intend to further explore the",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2519
https://github.com/root-project/root/pull/2519:1365,interoperability,interfac,interface,1365,"s fast access as possible in the case where the user is using relatively simple datatypes and the objects can be deserialized ""non-destructively"" (i.e., in-place in the existing memory buffers). For example - primitives and lists of primitives are supported; anything needing references are not. Despite the fact many of ROOT's very useful deserialization features are not supported, the limited feature set covers a range of analysis use cases. In particular, the current version of CMS's proposed NanoAOD format can be read solely via bulk APIs. Where the bulk IO APIs are applicable, we see an order-magnitude improvement in event rates for some use cases (including CMS NanoAOD). The speedup becomes more noticeable as the data read from the event is smaller (meaning the traditional ROOT IO overheads are larger). Somewhere in the neighborhood of 5KB objects read per event is the point where bulk IO APIs should be noticeably faster. This PR provides three interfaces:. * A raw, low-level interface. The user can invoke the bulk IO APIs directly and get a buffer of primitive types, serialized or deserialized. It is not envisioned this is used by users, but by API writers. * A TTreeReader-like interface, TTreeReaderFast. This is meant to be used by users; it utilizes the low-level interface to retrieve the serialized buffer and templated code will inline the deserializing as part of iterating through the tree. The speed here is gained by the fact only a single pass in memory is needed to deserialize and apply user code, rather than two. * The intent is that this interface can be extended in the future and used by TDataFrame, provided we can make TDataFrame sufficiently fast. * A Python-based numpy export interface. This exposes the (possibly serialized) memory directly as numpy arrays, allowing for efficient access to ROOT-serialized objects from the Python language. * The numpy interface is low-level for Python; we intend to further explore the ideal high-level interface thro",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2519
https://github.com/root-project/root/pull/2519:1408,interoperability,API,APIs,1408,"ere the user is using relatively simple datatypes and the objects can be deserialized ""non-destructively"" (i.e., in-place in the existing memory buffers). For example - primitives and lists of primitives are supported; anything needing references are not. Despite the fact many of ROOT's very useful deserialization features are not supported, the limited feature set covers a range of analysis use cases. In particular, the current version of CMS's proposed NanoAOD format can be read solely via bulk APIs. Where the bulk IO APIs are applicable, we see an order-magnitude improvement in event rates for some use cases (including CMS NanoAOD). The speedup becomes more noticeable as the data read from the event is smaller (meaning the traditional ROOT IO overheads are larger). Somewhere in the neighborhood of 5KB objects read per event is the point where bulk IO APIs should be noticeably faster. This PR provides three interfaces:. * A raw, low-level interface. The user can invoke the bulk IO APIs directly and get a buffer of primitive types, serialized or deserialized. It is not envisioned this is used by users, but by API writers. * A TTreeReader-like interface, TTreeReaderFast. This is meant to be used by users; it utilizes the low-level interface to retrieve the serialized buffer and templated code will inline the deserializing as part of iterating through the tree. The speed here is gained by the fact only a single pass in memory is needed to deserialize and apply user code, rather than two. * The intent is that this interface can be extended in the future and used by TDataFrame, provided we can make TDataFrame sufficiently fast. * A Python-based numpy export interface. This exposes the (possibly serialized) memory directly as numpy arrays, allowing for efficient access to ROOT-serialized objects from the Python language. * The numpy interface is low-level for Python; we intend to further explore the ideal high-level interface through projects such as https://github.com/",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2519
https://github.com/root-project/root/pull/2519:1538,interoperability,API,API,1538,"in-place in the existing memory buffers). For example - primitives and lists of primitives are supported; anything needing references are not. Despite the fact many of ROOT's very useful deserialization features are not supported, the limited feature set covers a range of analysis use cases. In particular, the current version of CMS's proposed NanoAOD format can be read solely via bulk APIs. Where the bulk IO APIs are applicable, we see an order-magnitude improvement in event rates for some use cases (including CMS NanoAOD). The speedup becomes more noticeable as the data read from the event is smaller (meaning the traditional ROOT IO overheads are larger). Somewhere in the neighborhood of 5KB objects read per event is the point where bulk IO APIs should be noticeably faster. This PR provides three interfaces:. * A raw, low-level interface. The user can invoke the bulk IO APIs directly and get a buffer of primitive types, serialized or deserialized. It is not envisioned this is used by users, but by API writers. * A TTreeReader-like interface, TTreeReaderFast. This is meant to be used by users; it utilizes the low-level interface to retrieve the serialized buffer and templated code will inline the deserializing as part of iterating through the tree. The speed here is gained by the fact only a single pass in memory is needed to deserialize and apply user code, rather than two. * The intent is that this interface can be extended in the future and used by TDataFrame, provided we can make TDataFrame sufficiently fast. * A Python-based numpy export interface. This exposes the (possibly serialized) memory directly as numpy arrays, allowing for efficient access to ROOT-serialized objects from the Python language. * The numpy interface is low-level for Python; we intend to further explore the ideal high-level interface through projects such as https://github.com/diana-hep/plur. (With many thanks to @jpivarski for driving this work home with his work on a python interface!).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2519
https://github.com/root-project/root/pull/2519:1572,interoperability,interfac,interface,1572,"in-place in the existing memory buffers). For example - primitives and lists of primitives are supported; anything needing references are not. Despite the fact many of ROOT's very useful deserialization features are not supported, the limited feature set covers a range of analysis use cases. In particular, the current version of CMS's proposed NanoAOD format can be read solely via bulk APIs. Where the bulk IO APIs are applicable, we see an order-magnitude improvement in event rates for some use cases (including CMS NanoAOD). The speedup becomes more noticeable as the data read from the event is smaller (meaning the traditional ROOT IO overheads are larger). Somewhere in the neighborhood of 5KB objects read per event is the point where bulk IO APIs should be noticeably faster. This PR provides three interfaces:. * A raw, low-level interface. The user can invoke the bulk IO APIs directly and get a buffer of primitive types, serialized or deserialized. It is not envisioned this is used by users, but by API writers. * A TTreeReader-like interface, TTreeReaderFast. This is meant to be used by users; it utilizes the low-level interface to retrieve the serialized buffer and templated code will inline the deserializing as part of iterating through the tree. The speed here is gained by the fact only a single pass in memory is needed to deserialize and apply user code, rather than two. * The intent is that this interface can be extended in the future and used by TDataFrame, provided we can make TDataFrame sufficiently fast. * A Python-based numpy export interface. This exposes the (possibly serialized) memory directly as numpy arrays, allowing for efficient access to ROOT-serialized objects from the Python language. * The numpy interface is low-level for Python; we intend to further explore the ideal high-level interface through projects such as https://github.com/diana-hep/plur. (With many thanks to @jpivarski for driving this work home with his work on a python interface!).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2519
https://github.com/root-project/root/pull/2519:1661,interoperability,interfac,interface,1661,"in-place in the existing memory buffers). For example - primitives and lists of primitives are supported; anything needing references are not. Despite the fact many of ROOT's very useful deserialization features are not supported, the limited feature set covers a range of analysis use cases. In particular, the current version of CMS's proposed NanoAOD format can be read solely via bulk APIs. Where the bulk IO APIs are applicable, we see an order-magnitude improvement in event rates for some use cases (including CMS NanoAOD). The speedup becomes more noticeable as the data read from the event is smaller (meaning the traditional ROOT IO overheads are larger). Somewhere in the neighborhood of 5KB objects read per event is the point where bulk IO APIs should be noticeably faster. This PR provides three interfaces:. * A raw, low-level interface. The user can invoke the bulk IO APIs directly and get a buffer of primitive types, serialized or deserialized. It is not envisioned this is used by users, but by API writers. * A TTreeReader-like interface, TTreeReaderFast. This is meant to be used by users; it utilizes the low-level interface to retrieve the serialized buffer and templated code will inline the deserializing as part of iterating through the tree. The speed here is gained by the fact only a single pass in memory is needed to deserialize and apply user code, rather than two. * The intent is that this interface can be extended in the future and used by TDataFrame, provided we can make TDataFrame sufficiently fast. * A Python-based numpy export interface. This exposes the (possibly serialized) memory directly as numpy arrays, allowing for efficient access to ROOT-serialized objects from the Python language. * The numpy interface is low-level for Python; we intend to further explore the ideal high-level interface through projects such as https://github.com/diana-hep/plur. (With many thanks to @jpivarski for driving this work home with his work on a python interface!).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2519
https://github.com/root-project/root/pull/2519:1948,interoperability,interfac,interface,1948,"in-place in the existing memory buffers). For example - primitives and lists of primitives are supported; anything needing references are not. Despite the fact many of ROOT's very useful deserialization features are not supported, the limited feature set covers a range of analysis use cases. In particular, the current version of CMS's proposed NanoAOD format can be read solely via bulk APIs. Where the bulk IO APIs are applicable, we see an order-magnitude improvement in event rates for some use cases (including CMS NanoAOD). The speedup becomes more noticeable as the data read from the event is smaller (meaning the traditional ROOT IO overheads are larger). Somewhere in the neighborhood of 5KB objects read per event is the point where bulk IO APIs should be noticeably faster. This PR provides three interfaces:. * A raw, low-level interface. The user can invoke the bulk IO APIs directly and get a buffer of primitive types, serialized or deserialized. It is not envisioned this is used by users, but by API writers. * A TTreeReader-like interface, TTreeReaderFast. This is meant to be used by users; it utilizes the low-level interface to retrieve the serialized buffer and templated code will inline the deserializing as part of iterating through the tree. The speed here is gained by the fact only a single pass in memory is needed to deserialize and apply user code, rather than two. * The intent is that this interface can be extended in the future and used by TDataFrame, provided we can make TDataFrame sufficiently fast. * A Python-based numpy export interface. This exposes the (possibly serialized) memory directly as numpy arrays, allowing for efficient access to ROOT-serialized objects from the Python language. * The numpy interface is low-level for Python; we intend to further explore the ideal high-level interface through projects such as https://github.com/diana-hep/plur. (With many thanks to @jpivarski for driving this work home with his work on a python interface!).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2519
https://github.com/root-project/root/pull/2519:2093,interoperability,interfac,interface,2093,"in-place in the existing memory buffers). For example - primitives and lists of primitives are supported; anything needing references are not. Despite the fact many of ROOT's very useful deserialization features are not supported, the limited feature set covers a range of analysis use cases. In particular, the current version of CMS's proposed NanoAOD format can be read solely via bulk APIs. Where the bulk IO APIs are applicable, we see an order-magnitude improvement in event rates for some use cases (including CMS NanoAOD). The speedup becomes more noticeable as the data read from the event is smaller (meaning the traditional ROOT IO overheads are larger). Somewhere in the neighborhood of 5KB objects read per event is the point where bulk IO APIs should be noticeably faster. This PR provides three interfaces:. * A raw, low-level interface. The user can invoke the bulk IO APIs directly and get a buffer of primitive types, serialized or deserialized. It is not envisioned this is used by users, but by API writers. * A TTreeReader-like interface, TTreeReaderFast. This is meant to be used by users; it utilizes the low-level interface to retrieve the serialized buffer and templated code will inline the deserializing as part of iterating through the tree. The speed here is gained by the fact only a single pass in memory is needed to deserialize and apply user code, rather than two. * The intent is that this interface can be extended in the future and used by TDataFrame, provided we can make TDataFrame sufficiently fast. * A Python-based numpy export interface. This exposes the (possibly serialized) memory directly as numpy arrays, allowing for efficient access to ROOT-serialized objects from the Python language. * The numpy interface is low-level for Python; we intend to further explore the ideal high-level interface through projects such as https://github.com/diana-hep/plur. (With many thanks to @jpivarski for driving this work home with his work on a python interface!).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2519
https://github.com/root-project/root/pull/2519:2271,interoperability,interfac,interface,2271,"in-place in the existing memory buffers). For example - primitives and lists of primitives are supported; anything needing references are not. Despite the fact many of ROOT's very useful deserialization features are not supported, the limited feature set covers a range of analysis use cases. In particular, the current version of CMS's proposed NanoAOD format can be read solely via bulk APIs. Where the bulk IO APIs are applicable, we see an order-magnitude improvement in event rates for some use cases (including CMS NanoAOD). The speedup becomes more noticeable as the data read from the event is smaller (meaning the traditional ROOT IO overheads are larger). Somewhere in the neighborhood of 5KB objects read per event is the point where bulk IO APIs should be noticeably faster. This PR provides three interfaces:. * A raw, low-level interface. The user can invoke the bulk IO APIs directly and get a buffer of primitive types, serialized or deserialized. It is not envisioned this is used by users, but by API writers. * A TTreeReader-like interface, TTreeReaderFast. This is meant to be used by users; it utilizes the low-level interface to retrieve the serialized buffer and templated code will inline the deserializing as part of iterating through the tree. The speed here is gained by the fact only a single pass in memory is needed to deserialize and apply user code, rather than two. * The intent is that this interface can be extended in the future and used by TDataFrame, provided we can make TDataFrame sufficiently fast. * A Python-based numpy export interface. This exposes the (possibly serialized) memory directly as numpy arrays, allowing for efficient access to ROOT-serialized objects from the Python language. * The numpy interface is low-level for Python; we intend to further explore the ideal high-level interface through projects such as https://github.com/diana-hep/plur. (With many thanks to @jpivarski for driving this work home with his work on a python interface!).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2519
https://github.com/root-project/root/pull/2519:2356,interoperability,interfac,interface,2356,"in-place in the existing memory buffers). For example - primitives and lists of primitives are supported; anything needing references are not. Despite the fact many of ROOT's very useful deserialization features are not supported, the limited feature set covers a range of analysis use cases. In particular, the current version of CMS's proposed NanoAOD format can be read solely via bulk APIs. Where the bulk IO APIs are applicable, we see an order-magnitude improvement in event rates for some use cases (including CMS NanoAOD). The speedup becomes more noticeable as the data read from the event is smaller (meaning the traditional ROOT IO overheads are larger). Somewhere in the neighborhood of 5KB objects read per event is the point where bulk IO APIs should be noticeably faster. This PR provides three interfaces:. * A raw, low-level interface. The user can invoke the bulk IO APIs directly and get a buffer of primitive types, serialized or deserialized. It is not envisioned this is used by users, but by API writers. * A TTreeReader-like interface, TTreeReaderFast. This is meant to be used by users; it utilizes the low-level interface to retrieve the serialized buffer and templated code will inline the deserializing as part of iterating through the tree. The speed here is gained by the fact only a single pass in memory is needed to deserialize and apply user code, rather than two. * The intent is that this interface can be extended in the future and used by TDataFrame, provided we can make TDataFrame sufficiently fast. * A Python-based numpy export interface. This exposes the (possibly serialized) memory directly as numpy arrays, allowing for efficient access to ROOT-serialized objects from the Python language. * The numpy interface is low-level for Python; we intend to further explore the ideal high-level interface through projects such as https://github.com/diana-hep/plur. (With many thanks to @jpivarski for driving this work home with his work on a python interface!).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2519
https://github.com/root-project/root/pull/2519:2511,interoperability,interfac,interface,2511,"in-place in the existing memory buffers). For example - primitives and lists of primitives are supported; anything needing references are not. Despite the fact many of ROOT's very useful deserialization features are not supported, the limited feature set covers a range of analysis use cases. In particular, the current version of CMS's proposed NanoAOD format can be read solely via bulk APIs. Where the bulk IO APIs are applicable, we see an order-magnitude improvement in event rates for some use cases (including CMS NanoAOD). The speedup becomes more noticeable as the data read from the event is smaller (meaning the traditional ROOT IO overheads are larger). Somewhere in the neighborhood of 5KB objects read per event is the point where bulk IO APIs should be noticeably faster. This PR provides three interfaces:. * A raw, low-level interface. The user can invoke the bulk IO APIs directly and get a buffer of primitive types, serialized or deserialized. It is not envisioned this is used by users, but by API writers. * A TTreeReader-like interface, TTreeReaderFast. This is meant to be used by users; it utilizes the low-level interface to retrieve the serialized buffer and templated code will inline the deserializing as part of iterating through the tree. The speed here is gained by the fact only a single pass in memory is needed to deserialize and apply user code, rather than two. * The intent is that this interface can be extended in the future and used by TDataFrame, provided we can make TDataFrame sufficiently fast. * A Python-based numpy export interface. This exposes the (possibly serialized) memory directly as numpy arrays, allowing for efficient access to ROOT-serialized objects from the Python language. * The numpy interface is low-level for Python; we intend to further explore the ideal high-level interface through projects such as https://github.com/diana-hep/plur. (With many thanks to @jpivarski for driving this work home with his work on a python interface!).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2519
https://github.com/root-project/root/pull/2519:126,modifiability,version,version,126,"Bulk I/O - v3; This is the second attempt for merging the bulk IO APIs (hidden inside the ROOT::Internal namespace); original version was in #943; this is a large rebase and update (particularly, removing the buffer sharing between the ROOT and the user). The initial goal here is to get another round of @pcanal attention and feedback. The bulk IO APIs aim to provide as fast access as possible in the case where the user is using relatively simple datatypes and the objects can be deserialized ""non-destructively"" (i.e., in-place in the existing memory buffers). For example - primitives and lists of primitives are supported; anything needing references are not. Despite the fact many of ROOT's very useful deserialization features are not supported, the limited feature set covers a range of analysis use cases. In particular, the current version of CMS's proposed NanoAOD format can be read solely via bulk APIs. Where the bulk IO APIs are applicable, we see an order-magnitude improvement in event rates for some use cases (including CMS NanoAOD). The speedup becomes more noticeable as the data read from the event is smaller (meaning the traditional ROOT IO overheads are larger). Somewhere in the neighborhood of 5KB objects read per event is the point where bulk IO APIs should be noticeably faster. This PR provides three interfaces:. * A raw, low-level interface. The user can invoke the bulk IO APIs directly and get a buffer of primitive types, serialized or deserialized. It is not envisioned this is used by users, but by API writers. * A TTreeReader-like interface, TTreeReaderFast. This is meant to be used by users; it utilizes the low-level interface to retrieve the serialized buffer and templated code will inline the deserializing as part of iterating through the tree. The speed here is gained by the fact only a single pass in memory is needed to deserialize and apply user code, rather than two. * The intent is that this interface can be extended in the future and used by ",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2519
https://github.com/root-project/root/pull/2519:843,modifiability,version,version,843,"Bulk I/O - v3; This is the second attempt for merging the bulk IO APIs (hidden inside the ROOT::Internal namespace); original version was in #943; this is a large rebase and update (particularly, removing the buffer sharing between the ROOT and the user). The initial goal here is to get another round of @pcanal attention and feedback. The bulk IO APIs aim to provide as fast access as possible in the case where the user is using relatively simple datatypes and the objects can be deserialized ""non-destructively"" (i.e., in-place in the existing memory buffers). For example - primitives and lists of primitives are supported; anything needing references are not. Despite the fact many of ROOT's very useful deserialization features are not supported, the limited feature set covers a range of analysis use cases. In particular, the current version of CMS's proposed NanoAOD format can be read solely via bulk APIs. Where the bulk IO APIs are applicable, we see an order-magnitude improvement in event rates for some use cases (including CMS NanoAOD). The speedup becomes more noticeable as the data read from the event is smaller (meaning the traditional ROOT IO overheads are larger). Somewhere in the neighborhood of 5KB objects read per event is the point where bulk IO APIs should be noticeably faster. This PR provides three interfaces:. * A raw, low-level interface. The user can invoke the bulk IO APIs directly and get a buffer of primitive types, serialized or deserialized. It is not envisioned this is used by users, but by API writers. * A TTreeReader-like interface, TTreeReaderFast. This is meant to be used by users; it utilizes the low-level interface to retrieve the serialized buffer and templated code will inline the deserializing as part of iterating through the tree. The speed here is gained by the fact only a single pass in memory is needed to deserialize and apply user code, rather than two. * The intent is that this interface can be extended in the future and used by ",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2519
https://github.com/root-project/root/pull/2519:1333,modifiability,interfac,interfaces,1333,"he bulk IO APIs aim to provide as fast access as possible in the case where the user is using relatively simple datatypes and the objects can be deserialized ""non-destructively"" (i.e., in-place in the existing memory buffers). For example - primitives and lists of primitives are supported; anything needing references are not. Despite the fact many of ROOT's very useful deserialization features are not supported, the limited feature set covers a range of analysis use cases. In particular, the current version of CMS's proposed NanoAOD format can be read solely via bulk APIs. Where the bulk IO APIs are applicable, we see an order-magnitude improvement in event rates for some use cases (including CMS NanoAOD). The speedup becomes more noticeable as the data read from the event is smaller (meaning the traditional ROOT IO overheads are larger). Somewhere in the neighborhood of 5KB objects read per event is the point where bulk IO APIs should be noticeably faster. This PR provides three interfaces:. * A raw, low-level interface. The user can invoke the bulk IO APIs directly and get a buffer of primitive types, serialized or deserialized. It is not envisioned this is used by users, but by API writers. * A TTreeReader-like interface, TTreeReaderFast. This is meant to be used by users; it utilizes the low-level interface to retrieve the serialized buffer and templated code will inline the deserializing as part of iterating through the tree. The speed here is gained by the fact only a single pass in memory is needed to deserialize and apply user code, rather than two. * The intent is that this interface can be extended in the future and used by TDataFrame, provided we can make TDataFrame sufficiently fast. * A Python-based numpy export interface. This exposes the (possibly serialized) memory directly as numpy arrays, allowing for efficient access to ROOT-serialized objects from the Python language. * The numpy interface is low-level for Python; we intend to further explore the",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2519
https://github.com/root-project/root/pull/2519:1365,modifiability,interfac,interface,1365,"s fast access as possible in the case where the user is using relatively simple datatypes and the objects can be deserialized ""non-destructively"" (i.e., in-place in the existing memory buffers). For example - primitives and lists of primitives are supported; anything needing references are not. Despite the fact many of ROOT's very useful deserialization features are not supported, the limited feature set covers a range of analysis use cases. In particular, the current version of CMS's proposed NanoAOD format can be read solely via bulk APIs. Where the bulk IO APIs are applicable, we see an order-magnitude improvement in event rates for some use cases (including CMS NanoAOD). The speedup becomes more noticeable as the data read from the event is smaller (meaning the traditional ROOT IO overheads are larger). Somewhere in the neighborhood of 5KB objects read per event is the point where bulk IO APIs should be noticeably faster. This PR provides three interfaces:. * A raw, low-level interface. The user can invoke the bulk IO APIs directly and get a buffer of primitive types, serialized or deserialized. It is not envisioned this is used by users, but by API writers. * A TTreeReader-like interface, TTreeReaderFast. This is meant to be used by users; it utilizes the low-level interface to retrieve the serialized buffer and templated code will inline the deserializing as part of iterating through the tree. The speed here is gained by the fact only a single pass in memory is needed to deserialize and apply user code, rather than two. * The intent is that this interface can be extended in the future and used by TDataFrame, provided we can make TDataFrame sufficiently fast. * A Python-based numpy export interface. This exposes the (possibly serialized) memory directly as numpy arrays, allowing for efficient access to ROOT-serialized objects from the Python language. * The numpy interface is low-level for Python; we intend to further explore the ideal high-level interface thro",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2519
https://github.com/root-project/root/pull/2519:1572,modifiability,interfac,interface,1572,"in-place in the existing memory buffers). For example - primitives and lists of primitives are supported; anything needing references are not. Despite the fact many of ROOT's very useful deserialization features are not supported, the limited feature set covers a range of analysis use cases. In particular, the current version of CMS's proposed NanoAOD format can be read solely via bulk APIs. Where the bulk IO APIs are applicable, we see an order-magnitude improvement in event rates for some use cases (including CMS NanoAOD). The speedup becomes more noticeable as the data read from the event is smaller (meaning the traditional ROOT IO overheads are larger). Somewhere in the neighborhood of 5KB objects read per event is the point where bulk IO APIs should be noticeably faster. This PR provides three interfaces:. * A raw, low-level interface. The user can invoke the bulk IO APIs directly and get a buffer of primitive types, serialized or deserialized. It is not envisioned this is used by users, but by API writers. * A TTreeReader-like interface, TTreeReaderFast. This is meant to be used by users; it utilizes the low-level interface to retrieve the serialized buffer and templated code will inline the deserializing as part of iterating through the tree. The speed here is gained by the fact only a single pass in memory is needed to deserialize and apply user code, rather than two. * The intent is that this interface can be extended in the future and used by TDataFrame, provided we can make TDataFrame sufficiently fast. * A Python-based numpy export interface. This exposes the (possibly serialized) memory directly as numpy arrays, allowing for efficient access to ROOT-serialized objects from the Python language. * The numpy interface is low-level for Python; we intend to further explore the ideal high-level interface through projects such as https://github.com/diana-hep/plur. (With many thanks to @jpivarski for driving this work home with his work on a python interface!).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2519
https://github.com/root-project/root/pull/2519:1661,modifiability,interfac,interface,1661,"in-place in the existing memory buffers). For example - primitives and lists of primitives are supported; anything needing references are not. Despite the fact many of ROOT's very useful deserialization features are not supported, the limited feature set covers a range of analysis use cases. In particular, the current version of CMS's proposed NanoAOD format can be read solely via bulk APIs. Where the bulk IO APIs are applicable, we see an order-magnitude improvement in event rates for some use cases (including CMS NanoAOD). The speedup becomes more noticeable as the data read from the event is smaller (meaning the traditional ROOT IO overheads are larger). Somewhere in the neighborhood of 5KB objects read per event is the point where bulk IO APIs should be noticeably faster. This PR provides three interfaces:. * A raw, low-level interface. The user can invoke the bulk IO APIs directly and get a buffer of primitive types, serialized or deserialized. It is not envisioned this is used by users, but by API writers. * A TTreeReader-like interface, TTreeReaderFast. This is meant to be used by users; it utilizes the low-level interface to retrieve the serialized buffer and templated code will inline the deserializing as part of iterating through the tree. The speed here is gained by the fact only a single pass in memory is needed to deserialize and apply user code, rather than two. * The intent is that this interface can be extended in the future and used by TDataFrame, provided we can make TDataFrame sufficiently fast. * A Python-based numpy export interface. This exposes the (possibly serialized) memory directly as numpy arrays, allowing for efficient access to ROOT-serialized objects from the Python language. * The numpy interface is low-level for Python; we intend to further explore the ideal high-level interface through projects such as https://github.com/diana-hep/plur. (With many thanks to @jpivarski for driving this work home with his work on a python interface!).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2519
https://github.com/root-project/root/pull/2519:1948,modifiability,interfac,interface,1948,"in-place in the existing memory buffers). For example - primitives and lists of primitives are supported; anything needing references are not. Despite the fact many of ROOT's very useful deserialization features are not supported, the limited feature set covers a range of analysis use cases. In particular, the current version of CMS's proposed NanoAOD format can be read solely via bulk APIs. Where the bulk IO APIs are applicable, we see an order-magnitude improvement in event rates for some use cases (including CMS NanoAOD). The speedup becomes more noticeable as the data read from the event is smaller (meaning the traditional ROOT IO overheads are larger). Somewhere in the neighborhood of 5KB objects read per event is the point where bulk IO APIs should be noticeably faster. This PR provides three interfaces:. * A raw, low-level interface. The user can invoke the bulk IO APIs directly and get a buffer of primitive types, serialized or deserialized. It is not envisioned this is used by users, but by API writers. * A TTreeReader-like interface, TTreeReaderFast. This is meant to be used by users; it utilizes the low-level interface to retrieve the serialized buffer and templated code will inline the deserializing as part of iterating through the tree. The speed here is gained by the fact only a single pass in memory is needed to deserialize and apply user code, rather than two. * The intent is that this interface can be extended in the future and used by TDataFrame, provided we can make TDataFrame sufficiently fast. * A Python-based numpy export interface. This exposes the (possibly serialized) memory directly as numpy arrays, allowing for efficient access to ROOT-serialized objects from the Python language. * The numpy interface is low-level for Python; we intend to further explore the ideal high-level interface through projects such as https://github.com/diana-hep/plur. (With many thanks to @jpivarski for driving this work home with his work on a python interface!).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2519
https://github.com/root-project/root/pull/2519:1965,modifiability,exten,extended,1965,"in-place in the existing memory buffers). For example - primitives and lists of primitives are supported; anything needing references are not. Despite the fact many of ROOT's very useful deserialization features are not supported, the limited feature set covers a range of analysis use cases. In particular, the current version of CMS's proposed NanoAOD format can be read solely via bulk APIs. Where the bulk IO APIs are applicable, we see an order-magnitude improvement in event rates for some use cases (including CMS NanoAOD). The speedup becomes more noticeable as the data read from the event is smaller (meaning the traditional ROOT IO overheads are larger). Somewhere in the neighborhood of 5KB objects read per event is the point where bulk IO APIs should be noticeably faster. This PR provides three interfaces:. * A raw, low-level interface. The user can invoke the bulk IO APIs directly and get a buffer of primitive types, serialized or deserialized. It is not envisioned this is used by users, but by API writers. * A TTreeReader-like interface, TTreeReaderFast. This is meant to be used by users; it utilizes the low-level interface to retrieve the serialized buffer and templated code will inline the deserializing as part of iterating through the tree. The speed here is gained by the fact only a single pass in memory is needed to deserialize and apply user code, rather than two. * The intent is that this interface can be extended in the future and used by TDataFrame, provided we can make TDataFrame sufficiently fast. * A Python-based numpy export interface. This exposes the (possibly serialized) memory directly as numpy arrays, allowing for efficient access to ROOT-serialized objects from the Python language. * The numpy interface is low-level for Python; we intend to further explore the ideal high-level interface through projects such as https://github.com/diana-hep/plur. (With many thanks to @jpivarski for driving this work home with his work on a python interface!).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2519
https://github.com/root-project/root/pull/2519:2093,modifiability,interfac,interface,2093,"in-place in the existing memory buffers). For example - primitives and lists of primitives are supported; anything needing references are not. Despite the fact many of ROOT's very useful deserialization features are not supported, the limited feature set covers a range of analysis use cases. In particular, the current version of CMS's proposed NanoAOD format can be read solely via bulk APIs. Where the bulk IO APIs are applicable, we see an order-magnitude improvement in event rates for some use cases (including CMS NanoAOD). The speedup becomes more noticeable as the data read from the event is smaller (meaning the traditional ROOT IO overheads are larger). Somewhere in the neighborhood of 5KB objects read per event is the point where bulk IO APIs should be noticeably faster. This PR provides three interfaces:. * A raw, low-level interface. The user can invoke the bulk IO APIs directly and get a buffer of primitive types, serialized or deserialized. It is not envisioned this is used by users, but by API writers. * A TTreeReader-like interface, TTreeReaderFast. This is meant to be used by users; it utilizes the low-level interface to retrieve the serialized buffer and templated code will inline the deserializing as part of iterating through the tree. The speed here is gained by the fact only a single pass in memory is needed to deserialize and apply user code, rather than two. * The intent is that this interface can be extended in the future and used by TDataFrame, provided we can make TDataFrame sufficiently fast. * A Python-based numpy export interface. This exposes the (possibly serialized) memory directly as numpy arrays, allowing for efficient access to ROOT-serialized objects from the Python language. * The numpy interface is low-level for Python; we intend to further explore the ideal high-level interface through projects such as https://github.com/diana-hep/plur. (With many thanks to @jpivarski for driving this work home with his work on a python interface!).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2519
https://github.com/root-project/root/pull/2519:2271,modifiability,interfac,interface,2271,"in-place in the existing memory buffers). For example - primitives and lists of primitives are supported; anything needing references are not. Despite the fact many of ROOT's very useful deserialization features are not supported, the limited feature set covers a range of analysis use cases. In particular, the current version of CMS's proposed NanoAOD format can be read solely via bulk APIs. Where the bulk IO APIs are applicable, we see an order-magnitude improvement in event rates for some use cases (including CMS NanoAOD). The speedup becomes more noticeable as the data read from the event is smaller (meaning the traditional ROOT IO overheads are larger). Somewhere in the neighborhood of 5KB objects read per event is the point where bulk IO APIs should be noticeably faster. This PR provides three interfaces:. * A raw, low-level interface. The user can invoke the bulk IO APIs directly and get a buffer of primitive types, serialized or deserialized. It is not envisioned this is used by users, but by API writers. * A TTreeReader-like interface, TTreeReaderFast. This is meant to be used by users; it utilizes the low-level interface to retrieve the serialized buffer and templated code will inline the deserializing as part of iterating through the tree. The speed here is gained by the fact only a single pass in memory is needed to deserialize and apply user code, rather than two. * The intent is that this interface can be extended in the future and used by TDataFrame, provided we can make TDataFrame sufficiently fast. * A Python-based numpy export interface. This exposes the (possibly serialized) memory directly as numpy arrays, allowing for efficient access to ROOT-serialized objects from the Python language. * The numpy interface is low-level for Python; we intend to further explore the ideal high-level interface through projects such as https://github.com/diana-hep/plur. (With many thanks to @jpivarski for driving this work home with his work on a python interface!).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2519
https://github.com/root-project/root/pull/2519:2356,modifiability,interfac,interface,2356,"in-place in the existing memory buffers). For example - primitives and lists of primitives are supported; anything needing references are not. Despite the fact many of ROOT's very useful deserialization features are not supported, the limited feature set covers a range of analysis use cases. In particular, the current version of CMS's proposed NanoAOD format can be read solely via bulk APIs. Where the bulk IO APIs are applicable, we see an order-magnitude improvement in event rates for some use cases (including CMS NanoAOD). The speedup becomes more noticeable as the data read from the event is smaller (meaning the traditional ROOT IO overheads are larger). Somewhere in the neighborhood of 5KB objects read per event is the point where bulk IO APIs should be noticeably faster. This PR provides three interfaces:. * A raw, low-level interface. The user can invoke the bulk IO APIs directly and get a buffer of primitive types, serialized or deserialized. It is not envisioned this is used by users, but by API writers. * A TTreeReader-like interface, TTreeReaderFast. This is meant to be used by users; it utilizes the low-level interface to retrieve the serialized buffer and templated code will inline the deserializing as part of iterating through the tree. The speed here is gained by the fact only a single pass in memory is needed to deserialize and apply user code, rather than two. * The intent is that this interface can be extended in the future and used by TDataFrame, provided we can make TDataFrame sufficiently fast. * A Python-based numpy export interface. This exposes the (possibly serialized) memory directly as numpy arrays, allowing for efficient access to ROOT-serialized objects from the Python language. * The numpy interface is low-level for Python; we intend to further explore the ideal high-level interface through projects such as https://github.com/diana-hep/plur. (With many thanks to @jpivarski for driving this work home with his work on a python interface!).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2519
https://github.com/root-project/root/pull/2519:2511,modifiability,interfac,interface,2511,"in-place in the existing memory buffers). For example - primitives and lists of primitives are supported; anything needing references are not. Despite the fact many of ROOT's very useful deserialization features are not supported, the limited feature set covers a range of analysis use cases. In particular, the current version of CMS's proposed NanoAOD format can be read solely via bulk APIs. Where the bulk IO APIs are applicable, we see an order-magnitude improvement in event rates for some use cases (including CMS NanoAOD). The speedup becomes more noticeable as the data read from the event is smaller (meaning the traditional ROOT IO overheads are larger). Somewhere in the neighborhood of 5KB objects read per event is the point where bulk IO APIs should be noticeably faster. This PR provides three interfaces:. * A raw, low-level interface. The user can invoke the bulk IO APIs directly and get a buffer of primitive types, serialized or deserialized. It is not envisioned this is used by users, but by API writers. * A TTreeReader-like interface, TTreeReaderFast. This is meant to be used by users; it utilizes the low-level interface to retrieve the serialized buffer and templated code will inline the deserializing as part of iterating through the tree. The speed here is gained by the fact only a single pass in memory is needed to deserialize and apply user code, rather than two. * The intent is that this interface can be extended in the future and used by TDataFrame, provided we can make TDataFrame sufficiently fast. * A Python-based numpy export interface. This exposes the (possibly serialized) memory directly as numpy arrays, allowing for efficient access to ROOT-serialized objects from the Python language. * The numpy interface is low-level for Python; we intend to further explore the ideal high-level interface through projects such as https://github.com/diana-hep/plur. (With many thanks to @jpivarski for driving this work home with his work on a python interface!).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2519
https://github.com/root-project/root/pull/2519:5,performance,I/O,I/O,5,"Bulk I/O - v3; This is the second attempt for merging the bulk IO APIs (hidden inside the ROOT::Internal namespace); original version was in #943; this is a large rebase and update (particularly, removing the buffer sharing between the ROOT and the user). The initial goal here is to get another round of @pcanal attention and feedback. The bulk IO APIs aim to provide as fast access as possible in the case where the user is using relatively simple datatypes and the objects can be deserialized ""non-destructively"" (i.e., in-place in the existing memory buffers). For example - primitives and lists of primitives are supported; anything needing references are not. Despite the fact many of ROOT's very useful deserialization features are not supported, the limited feature set covers a range of analysis use cases. In particular, the current version of CMS's proposed NanoAOD format can be read solely via bulk APIs. Where the bulk IO APIs are applicable, we see an order-magnitude improvement in event rates for some use cases (including CMS NanoAOD). The speedup becomes more noticeable as the data read from the event is smaller (meaning the traditional ROOT IO overheads are larger). Somewhere in the neighborhood of 5KB objects read per event is the point where bulk IO APIs should be noticeably faster. This PR provides three interfaces:. * A raw, low-level interface. The user can invoke the bulk IO APIs directly and get a buffer of primitive types, serialized or deserialized. It is not envisioned this is used by users, but by API writers. * A TTreeReader-like interface, TTreeReaderFast. This is meant to be used by users; it utilizes the low-level interface to retrieve the serialized buffer and templated code will inline the deserializing as part of iterating through the tree. The speed here is gained by the fact only a single pass in memory is needed to deserialize and apply user code, rather than two. * The intent is that this interface can be extended in the future and used by ",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2519
https://github.com/root-project/root/pull/2519:548,performance,memor,memory,548,"Bulk I/O - v3; This is the second attempt for merging the bulk IO APIs (hidden inside the ROOT::Internal namespace); original version was in #943; this is a large rebase and update (particularly, removing the buffer sharing between the ROOT and the user). The initial goal here is to get another round of @pcanal attention and feedback. The bulk IO APIs aim to provide as fast access as possible in the case where the user is using relatively simple datatypes and the objects can be deserialized ""non-destructively"" (i.e., in-place in the existing memory buffers). For example - primitives and lists of primitives are supported; anything needing references are not. Despite the fact many of ROOT's very useful deserialization features are not supported, the limited feature set covers a range of analysis use cases. In particular, the current version of CMS's proposed NanoAOD format can be read solely via bulk APIs. Where the bulk IO APIs are applicable, we see an order-magnitude improvement in event rates for some use cases (including CMS NanoAOD). The speedup becomes more noticeable as the data read from the event is smaller (meaning the traditional ROOT IO overheads are larger). Somewhere in the neighborhood of 5KB objects read per event is the point where bulk IO APIs should be noticeably faster. This PR provides three interfaces:. * A raw, low-level interface. The user can invoke the bulk IO APIs directly and get a buffer of primitive types, serialized or deserialized. It is not envisioned this is used by users, but by API writers. * A TTreeReader-like interface, TTreeReaderFast. This is meant to be used by users; it utilizes the low-level interface to retrieve the serialized buffer and templated code will inline the deserializing as part of iterating through the tree. The speed here is gained by the fact only a single pass in memory is needed to deserialize and apply user code, rather than two. * The intent is that this interface can be extended in the future and used by ",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2519
https://github.com/root-project/root/pull/2519:1166,performance,overhead,overheads,1166,"nd update (particularly, removing the buffer sharing between the ROOT and the user). The initial goal here is to get another round of @pcanal attention and feedback. The bulk IO APIs aim to provide as fast access as possible in the case where the user is using relatively simple datatypes and the objects can be deserialized ""non-destructively"" (i.e., in-place in the existing memory buffers). For example - primitives and lists of primitives are supported; anything needing references are not. Despite the fact many of ROOT's very useful deserialization features are not supported, the limited feature set covers a range of analysis use cases. In particular, the current version of CMS's proposed NanoAOD format can be read solely via bulk APIs. Where the bulk IO APIs are applicable, we see an order-magnitude improvement in event rates for some use cases (including CMS NanoAOD). The speedup becomes more noticeable as the data read from the event is smaller (meaning the traditional ROOT IO overheads are larger). Somewhere in the neighborhood of 5KB objects read per event is the point where bulk IO APIs should be noticeably faster. This PR provides three interfaces:. * A raw, low-level interface. The user can invoke the bulk IO APIs directly and get a buffer of primitive types, serialized or deserialized. It is not envisioned this is used by users, but by API writers. * A TTreeReader-like interface, TTreeReaderFast. This is meant to be used by users; it utilizes the low-level interface to retrieve the serialized buffer and templated code will inline the deserializing as part of iterating through the tree. The speed here is gained by the fact only a single pass in memory is needed to deserialize and apply user code, rather than two. * The intent is that this interface can be extended in the future and used by TDataFrame, provided we can make TDataFrame sufficiently fast. * A Python-based numpy export interface. This exposes the (possibly serialized) memory directly as numpy arr",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2519
https://github.com/root-project/root/pull/2519:1852,performance,memor,memory,1852,"in-place in the existing memory buffers). For example - primitives and lists of primitives are supported; anything needing references are not. Despite the fact many of ROOT's very useful deserialization features are not supported, the limited feature set covers a range of analysis use cases. In particular, the current version of CMS's proposed NanoAOD format can be read solely via bulk APIs. Where the bulk IO APIs are applicable, we see an order-magnitude improvement in event rates for some use cases (including CMS NanoAOD). The speedup becomes more noticeable as the data read from the event is smaller (meaning the traditional ROOT IO overheads are larger). Somewhere in the neighborhood of 5KB objects read per event is the point where bulk IO APIs should be noticeably faster. This PR provides three interfaces:. * A raw, low-level interface. The user can invoke the bulk IO APIs directly and get a buffer of primitive types, serialized or deserialized. It is not envisioned this is used by users, but by API writers. * A TTreeReader-like interface, TTreeReaderFast. This is meant to be used by users; it utilizes the low-level interface to retrieve the serialized buffer and templated code will inline the deserializing as part of iterating through the tree. The speed here is gained by the fact only a single pass in memory is needed to deserialize and apply user code, rather than two. * The intent is that this interface can be extended in the future and used by TDataFrame, provided we can make TDataFrame sufficiently fast. * A Python-based numpy export interface. This exposes the (possibly serialized) memory directly as numpy arrays, allowing for efficient access to ROOT-serialized objects from the Python language. * The numpy interface is low-level for Python; we intend to further explore the ideal high-level interface through projects such as https://github.com/diana-hep/plur. (With many thanks to @jpivarski for driving this work home with his work on a python interface!).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2519
https://github.com/root-project/root/pull/2519:2143,performance,memor,memory,2143,"in-place in the existing memory buffers). For example - primitives and lists of primitives are supported; anything needing references are not. Despite the fact many of ROOT's very useful deserialization features are not supported, the limited feature set covers a range of analysis use cases. In particular, the current version of CMS's proposed NanoAOD format can be read solely via bulk APIs. Where the bulk IO APIs are applicable, we see an order-magnitude improvement in event rates for some use cases (including CMS NanoAOD). The speedup becomes more noticeable as the data read from the event is smaller (meaning the traditional ROOT IO overheads are larger). Somewhere in the neighborhood of 5KB objects read per event is the point where bulk IO APIs should be noticeably faster. This PR provides three interfaces:. * A raw, low-level interface. The user can invoke the bulk IO APIs directly and get a buffer of primitive types, serialized or deserialized. It is not envisioned this is used by users, but by API writers. * A TTreeReader-like interface, TTreeReaderFast. This is meant to be used by users; it utilizes the low-level interface to retrieve the serialized buffer and templated code will inline the deserializing as part of iterating through the tree. The speed here is gained by the fact only a single pass in memory is needed to deserialize and apply user code, rather than two. * The intent is that this interface can be extended in the future and used by TDataFrame, provided we can make TDataFrame sufficiently fast. * A Python-based numpy export interface. This exposes the (possibly serialized) memory directly as numpy arrays, allowing for efficient access to ROOT-serialized objects from the Python language. * The numpy interface is low-level for Python; we intend to further explore the ideal high-level interface through projects such as https://github.com/diana-hep/plur. (With many thanks to @jpivarski for driving this work home with his work on a python interface!).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2519
https://github.com/root-project/root/pull/2519:174,safety,updat,update,174,"Bulk I/O - v3; This is the second attempt for merging the bulk IO APIs (hidden inside the ROOT::Internal namespace); original version was in #943; this is a large rebase and update (particularly, removing the buffer sharing between the ROOT and the user). The initial goal here is to get another round of @pcanal attention and feedback. The bulk IO APIs aim to provide as fast access as possible in the case where the user is using relatively simple datatypes and the objects can be deserialized ""non-destructively"" (i.e., in-place in the existing memory buffers). For example - primitives and lists of primitives are supported; anything needing references are not. Despite the fact many of ROOT's very useful deserialization features are not supported, the limited feature set covers a range of analysis use cases. In particular, the current version of CMS's proposed NanoAOD format can be read solely via bulk APIs. Where the bulk IO APIs are applicable, we see an order-magnitude improvement in event rates for some use cases (including CMS NanoAOD). The speedup becomes more noticeable as the data read from the event is smaller (meaning the traditional ROOT IO overheads are larger). Somewhere in the neighborhood of 5KB objects read per event is the point where bulk IO APIs should be noticeably faster. This PR provides three interfaces:. * A raw, low-level interface. The user can invoke the bulk IO APIs directly and get a buffer of primitive types, serialized or deserialized. It is not envisioned this is used by users, but by API writers. * A TTreeReader-like interface, TTreeReaderFast. This is meant to be used by users; it utilizes the low-level interface to retrieve the serialized buffer and templated code will inline the deserializing as part of iterating through the tree. The speed here is gained by the fact only a single pass in memory is needed to deserialize and apply user code, rather than two. * The intent is that this interface can be extended in the future and used by ",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2519
https://github.com/root-project/root/pull/2519:174,security,updat,update,174,"Bulk I/O - v3; This is the second attempt for merging the bulk IO APIs (hidden inside the ROOT::Internal namespace); original version was in #943; this is a large rebase and update (particularly, removing the buffer sharing between the ROOT and the user). The initial goal here is to get another round of @pcanal attention and feedback. The bulk IO APIs aim to provide as fast access as possible in the case where the user is using relatively simple datatypes and the objects can be deserialized ""non-destructively"" (i.e., in-place in the existing memory buffers). For example - primitives and lists of primitives are supported; anything needing references are not. Despite the fact many of ROOT's very useful deserialization features are not supported, the limited feature set covers a range of analysis use cases. In particular, the current version of CMS's proposed NanoAOD format can be read solely via bulk APIs. Where the bulk IO APIs are applicable, we see an order-magnitude improvement in event rates for some use cases (including CMS NanoAOD). The speedup becomes more noticeable as the data read from the event is smaller (meaning the traditional ROOT IO overheads are larger). Somewhere in the neighborhood of 5KB objects read per event is the point where bulk IO APIs should be noticeably faster. This PR provides three interfaces:. * A raw, low-level interface. The user can invoke the bulk IO APIs directly and get a buffer of primitive types, serialized or deserialized. It is not envisioned this is used by users, but by API writers. * A TTreeReader-like interface, TTreeReaderFast. This is meant to be used by users; it utilizes the low-level interface to retrieve the serialized buffer and templated code will inline the deserializing as part of iterating through the tree. The speed here is gained by the fact only a single pass in memory is needed to deserialize and apply user code, rather than two. * The intent is that this interface can be extended in the future and used by ",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2519
https://github.com/root-project/root/pull/2519:377,security,access,access,377,"Bulk I/O - v3; This is the second attempt for merging the bulk IO APIs (hidden inside the ROOT::Internal namespace); original version was in #943; this is a large rebase and update (particularly, removing the buffer sharing between the ROOT and the user). The initial goal here is to get another round of @pcanal attention and feedback. The bulk IO APIs aim to provide as fast access as possible in the case where the user is using relatively simple datatypes and the objects can be deserialized ""non-destructively"" (i.e., in-place in the existing memory buffers). For example - primitives and lists of primitives are supported; anything needing references are not. Despite the fact many of ROOT's very useful deserialization features are not supported, the limited feature set covers a range of analysis use cases. In particular, the current version of CMS's proposed NanoAOD format can be read solely via bulk APIs. Where the bulk IO APIs are applicable, we see an order-magnitude improvement in event rates for some use cases (including CMS NanoAOD). The speedup becomes more noticeable as the data read from the event is smaller (meaning the traditional ROOT IO overheads are larger). Somewhere in the neighborhood of 5KB objects read per event is the point where bulk IO APIs should be noticeably faster. This PR provides three interfaces:. * A raw, low-level interface. The user can invoke the bulk IO APIs directly and get a buffer of primitive types, serialized or deserialized. It is not envisioned this is used by users, but by API writers. * A TTreeReader-like interface, TTreeReaderFast. This is meant to be used by users; it utilizes the low-level interface to retrieve the serialized buffer and templated code will inline the deserializing as part of iterating through the tree. The speed here is gained by the fact only a single pass in memory is needed to deserialize and apply user code, rather than two. * The intent is that this interface can be extended in the future and used by ",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2519
https://github.com/root-project/root/pull/2519:2109,security,expos,exposes,2109,"in-place in the existing memory buffers). For example - primitives and lists of primitives are supported; anything needing references are not. Despite the fact many of ROOT's very useful deserialization features are not supported, the limited feature set covers a range of analysis use cases. In particular, the current version of CMS's proposed NanoAOD format can be read solely via bulk APIs. Where the bulk IO APIs are applicable, we see an order-magnitude improvement in event rates for some use cases (including CMS NanoAOD). The speedup becomes more noticeable as the data read from the event is smaller (meaning the traditional ROOT IO overheads are larger). Somewhere in the neighborhood of 5KB objects read per event is the point where bulk IO APIs should be noticeably faster. This PR provides three interfaces:. * A raw, low-level interface. The user can invoke the bulk IO APIs directly and get a buffer of primitive types, serialized or deserialized. It is not envisioned this is used by users, but by API writers. * A TTreeReader-like interface, TTreeReaderFast. This is meant to be used by users; it utilizes the low-level interface to retrieve the serialized buffer and templated code will inline the deserializing as part of iterating through the tree. The speed here is gained by the fact only a single pass in memory is needed to deserialize and apply user code, rather than two. * The intent is that this interface can be extended in the future and used by TDataFrame, provided we can make TDataFrame sufficiently fast. * A Python-based numpy export interface. This exposes the (possibly serialized) memory directly as numpy arrays, allowing for efficient access to ROOT-serialized objects from the Python language. * The numpy interface is low-level for Python; we intend to further explore the ideal high-level interface through projects such as https://github.com/diana-hep/plur. (With many thanks to @jpivarski for driving this work home with his work on a python interface!).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2519
https://github.com/root-project/root/pull/2519:2199,security,access,access,2199,"in-place in the existing memory buffers). For example - primitives and lists of primitives are supported; anything needing references are not. Despite the fact many of ROOT's very useful deserialization features are not supported, the limited feature set covers a range of analysis use cases. In particular, the current version of CMS's proposed NanoAOD format can be read solely via bulk APIs. Where the bulk IO APIs are applicable, we see an order-magnitude improvement in event rates for some use cases (including CMS NanoAOD). The speedup becomes more noticeable as the data read from the event is smaller (meaning the traditional ROOT IO overheads are larger). Somewhere in the neighborhood of 5KB objects read per event is the point where bulk IO APIs should be noticeably faster. This PR provides three interfaces:. * A raw, low-level interface. The user can invoke the bulk IO APIs directly and get a buffer of primitive types, serialized or deserialized. It is not envisioned this is used by users, but by API writers. * A TTreeReader-like interface, TTreeReaderFast. This is meant to be used by users; it utilizes the low-level interface to retrieve the serialized buffer and templated code will inline the deserializing as part of iterating through the tree. The speed here is gained by the fact only a single pass in memory is needed to deserialize and apply user code, rather than two. * The intent is that this interface can be extended in the future and used by TDataFrame, provided we can make TDataFrame sufficiently fast. * A Python-based numpy export interface. This exposes the (possibly serialized) memory directly as numpy arrays, allowing for efficient access to ROOT-serialized objects from the Python language. * The numpy interface is low-level for Python; we intend to further explore the ideal high-level interface through projects such as https://github.com/diana-hep/plur. (With many thanks to @jpivarski for driving this work home with his work on a python interface!).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2519
https://github.com/root-project/root/pull/2519:443,testability,simpl,simple,443,"Bulk I/O - v3; This is the second attempt for merging the bulk IO APIs (hidden inside the ROOT::Internal namespace); original version was in #943; this is a large rebase and update (particularly, removing the buffer sharing between the ROOT and the user). The initial goal here is to get another round of @pcanal attention and feedback. The bulk IO APIs aim to provide as fast access as possible in the case where the user is using relatively simple datatypes and the objects can be deserialized ""non-destructively"" (i.e., in-place in the existing memory buffers). For example - primitives and lists of primitives are supported; anything needing references are not. Despite the fact many of ROOT's very useful deserialization features are not supported, the limited feature set covers a range of analysis use cases. In particular, the current version of CMS's proposed NanoAOD format can be read solely via bulk APIs. Where the bulk IO APIs are applicable, we see an order-magnitude improvement in event rates for some use cases (including CMS NanoAOD). The speedup becomes more noticeable as the data read from the event is smaller (meaning the traditional ROOT IO overheads are larger). Somewhere in the neighborhood of 5KB objects read per event is the point where bulk IO APIs should be noticeably faster. This PR provides three interfaces:. * A raw, low-level interface. The user can invoke the bulk IO APIs directly and get a buffer of primitive types, serialized or deserialized. It is not envisioned this is used by users, but by API writers. * A TTreeReader-like interface, TTreeReaderFast. This is meant to be used by users; it utilizes the low-level interface to retrieve the serialized buffer and templated code will inline the deserializing as part of iterating through the tree. The speed here is gained by the fact only a single pass in memory is needed to deserialize and apply user code, rather than two. * The intent is that this interface can be extended in the future and used by ",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2519
https://github.com/root-project/root/pull/2519:249,usability,user,user,249,"Bulk I/O - v3; This is the second attempt for merging the bulk IO APIs (hidden inside the ROOT::Internal namespace); original version was in #943; this is a large rebase and update (particularly, removing the buffer sharing between the ROOT and the user). The initial goal here is to get another round of @pcanal attention and feedback. The bulk IO APIs aim to provide as fast access as possible in the case where the user is using relatively simple datatypes and the objects can be deserialized ""non-destructively"" (i.e., in-place in the existing memory buffers). For example - primitives and lists of primitives are supported; anything needing references are not. Despite the fact many of ROOT's very useful deserialization features are not supported, the limited feature set covers a range of analysis use cases. In particular, the current version of CMS's proposed NanoAOD format can be read solely via bulk APIs. Where the bulk IO APIs are applicable, we see an order-magnitude improvement in event rates for some use cases (including CMS NanoAOD). The speedup becomes more noticeable as the data read from the event is smaller (meaning the traditional ROOT IO overheads are larger). Somewhere in the neighborhood of 5KB objects read per event is the point where bulk IO APIs should be noticeably faster. This PR provides three interfaces:. * A raw, low-level interface. The user can invoke the bulk IO APIs directly and get a buffer of primitive types, serialized or deserialized. It is not envisioned this is used by users, but by API writers. * A TTreeReader-like interface, TTreeReaderFast. This is meant to be used by users; it utilizes the low-level interface to retrieve the serialized buffer and templated code will inline the deserializing as part of iterating through the tree. The speed here is gained by the fact only a single pass in memory is needed to deserialize and apply user code, rather than two. * The intent is that this interface can be extended in the future and used by ",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2519
https://github.com/root-project/root/pull/2519:327,usability,feedback,feedback,327,"Bulk I/O - v3; This is the second attempt for merging the bulk IO APIs (hidden inside the ROOT::Internal namespace); original version was in #943; this is a large rebase and update (particularly, removing the buffer sharing between the ROOT and the user). The initial goal here is to get another round of @pcanal attention and feedback. The bulk IO APIs aim to provide as fast access as possible in the case where the user is using relatively simple datatypes and the objects can be deserialized ""non-destructively"" (i.e., in-place in the existing memory buffers). For example - primitives and lists of primitives are supported; anything needing references are not. Despite the fact many of ROOT's very useful deserialization features are not supported, the limited feature set covers a range of analysis use cases. In particular, the current version of CMS's proposed NanoAOD format can be read solely via bulk APIs. Where the bulk IO APIs are applicable, we see an order-magnitude improvement in event rates for some use cases (including CMS NanoAOD). The speedup becomes more noticeable as the data read from the event is smaller (meaning the traditional ROOT IO overheads are larger). Somewhere in the neighborhood of 5KB objects read per event is the point where bulk IO APIs should be noticeably faster. This PR provides three interfaces:. * A raw, low-level interface. The user can invoke the bulk IO APIs directly and get a buffer of primitive types, serialized or deserialized. It is not envisioned this is used by users, but by API writers. * A TTreeReader-like interface, TTreeReaderFast. This is meant to be used by users; it utilizes the low-level interface to retrieve the serialized buffer and templated code will inline the deserializing as part of iterating through the tree. The speed here is gained by the fact only a single pass in memory is needed to deserialize and apply user code, rather than two. * The intent is that this interface can be extended in the future and used by ",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2519
https://github.com/root-project/root/pull/2519:418,usability,user,user,418,"Bulk I/O - v3; This is the second attempt for merging the bulk IO APIs (hidden inside the ROOT::Internal namespace); original version was in #943; this is a large rebase and update (particularly, removing the buffer sharing between the ROOT and the user). The initial goal here is to get another round of @pcanal attention and feedback. The bulk IO APIs aim to provide as fast access as possible in the case where the user is using relatively simple datatypes and the objects can be deserialized ""non-destructively"" (i.e., in-place in the existing memory buffers). For example - primitives and lists of primitives are supported; anything needing references are not. Despite the fact many of ROOT's very useful deserialization features are not supported, the limited feature set covers a range of analysis use cases. In particular, the current version of CMS's proposed NanoAOD format can be read solely via bulk APIs. Where the bulk IO APIs are applicable, we see an order-magnitude improvement in event rates for some use cases (including CMS NanoAOD). The speedup becomes more noticeable as the data read from the event is smaller (meaning the traditional ROOT IO overheads are larger). Somewhere in the neighborhood of 5KB objects read per event is the point where bulk IO APIs should be noticeably faster. This PR provides three interfaces:. * A raw, low-level interface. The user can invoke the bulk IO APIs directly and get a buffer of primitive types, serialized or deserialized. It is not envisioned this is used by users, but by API writers. * A TTreeReader-like interface, TTreeReaderFast. This is meant to be used by users; it utilizes the low-level interface to retrieve the serialized buffer and templated code will inline the deserializing as part of iterating through the tree. The speed here is gained by the fact only a single pass in memory is needed to deserialize and apply user code, rather than two. * The intent is that this interface can be extended in the future and used by ",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2519
https://github.com/root-project/root/pull/2519:443,usability,simpl,simple,443,"Bulk I/O - v3; This is the second attempt for merging the bulk IO APIs (hidden inside the ROOT::Internal namespace); original version was in #943; this is a large rebase and update (particularly, removing the buffer sharing between the ROOT and the user). The initial goal here is to get another round of @pcanal attention and feedback. The bulk IO APIs aim to provide as fast access as possible in the case where the user is using relatively simple datatypes and the objects can be deserialized ""non-destructively"" (i.e., in-place in the existing memory buffers). For example - primitives and lists of primitives are supported; anything needing references are not. Despite the fact many of ROOT's very useful deserialization features are not supported, the limited feature set covers a range of analysis use cases. In particular, the current version of CMS's proposed NanoAOD format can be read solely via bulk APIs. Where the bulk IO APIs are applicable, we see an order-magnitude improvement in event rates for some use cases (including CMS NanoAOD). The speedup becomes more noticeable as the data read from the event is smaller (meaning the traditional ROOT IO overheads are larger). Somewhere in the neighborhood of 5KB objects read per event is the point where bulk IO APIs should be noticeably faster. This PR provides three interfaces:. * A raw, low-level interface. The user can invoke the bulk IO APIs directly and get a buffer of primitive types, serialized or deserialized. It is not envisioned this is used by users, but by API writers. * A TTreeReader-like interface, TTreeReaderFast. This is meant to be used by users; it utilizes the low-level interface to retrieve the serialized buffer and templated code will inline the deserializing as part of iterating through the tree. The speed here is gained by the fact only a single pass in memory is needed to deserialize and apply user code, rather than two. * The intent is that this interface can be extended in the future and used by ",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2519
https://github.com/root-project/root/pull/2519:548,usability,memor,memory,548,"Bulk I/O - v3; This is the second attempt for merging the bulk IO APIs (hidden inside the ROOT::Internal namespace); original version was in #943; this is a large rebase and update (particularly, removing the buffer sharing between the ROOT and the user). The initial goal here is to get another round of @pcanal attention and feedback. The bulk IO APIs aim to provide as fast access as possible in the case where the user is using relatively simple datatypes and the objects can be deserialized ""non-destructively"" (i.e., in-place in the existing memory buffers). For example - primitives and lists of primitives are supported; anything needing references are not. Despite the fact many of ROOT's very useful deserialization features are not supported, the limited feature set covers a range of analysis use cases. In particular, the current version of CMS's proposed NanoAOD format can be read solely via bulk APIs. Where the bulk IO APIs are applicable, we see an order-magnitude improvement in event rates for some use cases (including CMS NanoAOD). The speedup becomes more noticeable as the data read from the event is smaller (meaning the traditional ROOT IO overheads are larger). Somewhere in the neighborhood of 5KB objects read per event is the point where bulk IO APIs should be noticeably faster. This PR provides three interfaces:. * A raw, low-level interface. The user can invoke the bulk IO APIs directly and get a buffer of primitive types, serialized or deserialized. It is not envisioned this is used by users, but by API writers. * A TTreeReader-like interface, TTreeReaderFast. This is meant to be used by users; it utilizes the low-level interface to retrieve the serialized buffer and templated code will inline the deserializing as part of iterating through the tree. The speed here is gained by the fact only a single pass in memory is needed to deserialize and apply user code, rather than two. * The intent is that this interface can be extended in the future and used by ",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2519
https://github.com/root-project/root/pull/2519:618,usability,support,supported,618,"Bulk I/O - v3; This is the second attempt for merging the bulk IO APIs (hidden inside the ROOT::Internal namespace); original version was in #943; this is a large rebase and update (particularly, removing the buffer sharing between the ROOT and the user). The initial goal here is to get another round of @pcanal attention and feedback. The bulk IO APIs aim to provide as fast access as possible in the case where the user is using relatively simple datatypes and the objects can be deserialized ""non-destructively"" (i.e., in-place in the existing memory buffers). For example - primitives and lists of primitives are supported; anything needing references are not. Despite the fact many of ROOT's very useful deserialization features are not supported, the limited feature set covers a range of analysis use cases. In particular, the current version of CMS's proposed NanoAOD format can be read solely via bulk APIs. Where the bulk IO APIs are applicable, we see an order-magnitude improvement in event rates for some use cases (including CMS NanoAOD). The speedup becomes more noticeable as the data read from the event is smaller (meaning the traditional ROOT IO overheads are larger). Somewhere in the neighborhood of 5KB objects read per event is the point where bulk IO APIs should be noticeably faster. This PR provides three interfaces:. * A raw, low-level interface. The user can invoke the bulk IO APIs directly and get a buffer of primitive types, serialized or deserialized. It is not envisioned this is used by users, but by API writers. * A TTreeReader-like interface, TTreeReaderFast. This is meant to be used by users; it utilizes the low-level interface to retrieve the serialized buffer and templated code will inline the deserializing as part of iterating through the tree. The speed here is gained by the fact only a single pass in memory is needed to deserialize and apply user code, rather than two. * The intent is that this interface can be extended in the future and used by ",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2519
https://github.com/root-project/root/pull/2519:743,usability,support,supported,743,"Bulk I/O - v3; This is the second attempt for merging the bulk IO APIs (hidden inside the ROOT::Internal namespace); original version was in #943; this is a large rebase and update (particularly, removing the buffer sharing between the ROOT and the user). The initial goal here is to get another round of @pcanal attention and feedback. The bulk IO APIs aim to provide as fast access as possible in the case where the user is using relatively simple datatypes and the objects can be deserialized ""non-destructively"" (i.e., in-place in the existing memory buffers). For example - primitives and lists of primitives are supported; anything needing references are not. Despite the fact many of ROOT's very useful deserialization features are not supported, the limited feature set covers a range of analysis use cases. In particular, the current version of CMS's proposed NanoAOD format can be read solely via bulk APIs. Where the bulk IO APIs are applicable, we see an order-magnitude improvement in event rates for some use cases (including CMS NanoAOD). The speedup becomes more noticeable as the data read from the event is smaller (meaning the traditional ROOT IO overheads are larger). Somewhere in the neighborhood of 5KB objects read per event is the point where bulk IO APIs should be noticeably faster. This PR provides three interfaces:. * A raw, low-level interface. The user can invoke the bulk IO APIs directly and get a buffer of primitive types, serialized or deserialized. It is not envisioned this is used by users, but by API writers. * A TTreeReader-like interface, TTreeReaderFast. This is meant to be used by users; it utilizes the low-level interface to retrieve the serialized buffer and templated code will inline the deserializing as part of iterating through the tree. The speed here is gained by the fact only a single pass in memory is needed to deserialize and apply user code, rather than two. * The intent is that this interface can be extended in the future and used by ",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2519
https://github.com/root-project/root/pull/2519:1380,usability,user,user,1380,"s as possible in the case where the user is using relatively simple datatypes and the objects can be deserialized ""non-destructively"" (i.e., in-place in the existing memory buffers). For example - primitives and lists of primitives are supported; anything needing references are not. Despite the fact many of ROOT's very useful deserialization features are not supported, the limited feature set covers a range of analysis use cases. In particular, the current version of CMS's proposed NanoAOD format can be read solely via bulk APIs. Where the bulk IO APIs are applicable, we see an order-magnitude improvement in event rates for some use cases (including CMS NanoAOD). The speedup becomes more noticeable as the data read from the event is smaller (meaning the traditional ROOT IO overheads are larger). Somewhere in the neighborhood of 5KB objects read per event is the point where bulk IO APIs should be noticeably faster. This PR provides three interfaces:. * A raw, low-level interface. The user can invoke the bulk IO APIs directly and get a buffer of primitive types, serialized or deserialized. It is not envisioned this is used by users, but by API writers. * A TTreeReader-like interface, TTreeReaderFast. This is meant to be used by users; it utilizes the low-level interface to retrieve the serialized buffer and templated code will inline the deserializing as part of iterating through the tree. The speed here is gained by the fact only a single pass in memory is needed to deserialize and apply user code, rather than two. * The intent is that this interface can be extended in the future and used by TDataFrame, provided we can make TDataFrame sufficiently fast. * A Python-based numpy export interface. This exposes the (possibly serialized) memory directly as numpy arrays, allowing for efficient access to ROOT-serialized objects from the Python language. * The numpy interface is low-level for Python; we intend to further explore the ideal high-level interface through projects",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2519
https://github.com/root-project/root/pull/2519:1524,usability,user,users,1524,"in-place in the existing memory buffers). For example - primitives and lists of primitives are supported; anything needing references are not. Despite the fact many of ROOT's very useful deserialization features are not supported, the limited feature set covers a range of analysis use cases. In particular, the current version of CMS's proposed NanoAOD format can be read solely via bulk APIs. Where the bulk IO APIs are applicable, we see an order-magnitude improvement in event rates for some use cases (including CMS NanoAOD). The speedup becomes more noticeable as the data read from the event is smaller (meaning the traditional ROOT IO overheads are larger). Somewhere in the neighborhood of 5KB objects read per event is the point where bulk IO APIs should be noticeably faster. This PR provides three interfaces:. * A raw, low-level interface. The user can invoke the bulk IO APIs directly and get a buffer of primitive types, serialized or deserialized. It is not envisioned this is used by users, but by API writers. * A TTreeReader-like interface, TTreeReaderFast. This is meant to be used by users; it utilizes the low-level interface to retrieve the serialized buffer and templated code will inline the deserializing as part of iterating through the tree. The speed here is gained by the fact only a single pass in memory is needed to deserialize and apply user code, rather than two. * The intent is that this interface can be extended in the future and used by TDataFrame, provided we can make TDataFrame sufficiently fast. * A Python-based numpy export interface. This exposes the (possibly serialized) memory directly as numpy arrays, allowing for efficient access to ROOT-serialized objects from the Python language. * The numpy interface is low-level for Python; we intend to further explore the ideal high-level interface through projects such as https://github.com/diana-hep/plur. (With many thanks to @jpivarski for driving this work home with his work on a python interface!).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2519
https://github.com/root-project/root/pull/2519:1628,usability,user,users,1628,"in-place in the existing memory buffers). For example - primitives and lists of primitives are supported; anything needing references are not. Despite the fact many of ROOT's very useful deserialization features are not supported, the limited feature set covers a range of analysis use cases. In particular, the current version of CMS's proposed NanoAOD format can be read solely via bulk APIs. Where the bulk IO APIs are applicable, we see an order-magnitude improvement in event rates for some use cases (including CMS NanoAOD). The speedup becomes more noticeable as the data read from the event is smaller (meaning the traditional ROOT IO overheads are larger). Somewhere in the neighborhood of 5KB objects read per event is the point where bulk IO APIs should be noticeably faster. This PR provides three interfaces:. * A raw, low-level interface. The user can invoke the bulk IO APIs directly and get a buffer of primitive types, serialized or deserialized. It is not envisioned this is used by users, but by API writers. * A TTreeReader-like interface, TTreeReaderFast. This is meant to be used by users; it utilizes the low-level interface to retrieve the serialized buffer and templated code will inline the deserializing as part of iterating through the tree. The speed here is gained by the fact only a single pass in memory is needed to deserialize and apply user code, rather than two. * The intent is that this interface can be extended in the future and used by TDataFrame, provided we can make TDataFrame sufficiently fast. * A Python-based numpy export interface. This exposes the (possibly serialized) memory directly as numpy arrays, allowing for efficient access to ROOT-serialized objects from the Python language. * The numpy interface is low-level for Python; we intend to further explore the ideal high-level interface through projects such as https://github.com/diana-hep/plur. (With many thanks to @jpivarski for driving this work home with his work on a python interface!).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2519
https://github.com/root-project/root/pull/2519:1852,usability,memor,memory,1852,"in-place in the existing memory buffers). For example - primitives and lists of primitives are supported; anything needing references are not. Despite the fact many of ROOT's very useful deserialization features are not supported, the limited feature set covers a range of analysis use cases. In particular, the current version of CMS's proposed NanoAOD format can be read solely via bulk APIs. Where the bulk IO APIs are applicable, we see an order-magnitude improvement in event rates for some use cases (including CMS NanoAOD). The speedup becomes more noticeable as the data read from the event is smaller (meaning the traditional ROOT IO overheads are larger). Somewhere in the neighborhood of 5KB objects read per event is the point where bulk IO APIs should be noticeably faster. This PR provides three interfaces:. * A raw, low-level interface. The user can invoke the bulk IO APIs directly and get a buffer of primitive types, serialized or deserialized. It is not envisioned this is used by users, but by API writers. * A TTreeReader-like interface, TTreeReaderFast. This is meant to be used by users; it utilizes the low-level interface to retrieve the serialized buffer and templated code will inline the deserializing as part of iterating through the tree. The speed here is gained by the fact only a single pass in memory is needed to deserialize and apply user code, rather than two. * The intent is that this interface can be extended in the future and used by TDataFrame, provided we can make TDataFrame sufficiently fast. * A Python-based numpy export interface. This exposes the (possibly serialized) memory directly as numpy arrays, allowing for efficient access to ROOT-serialized objects from the Python language. * The numpy interface is low-level for Python; we intend to further explore the ideal high-level interface through projects such as https://github.com/diana-hep/plur. (With many thanks to @jpivarski for driving this work home with his work on a python interface!).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2519
https://github.com/root-project/root/pull/2519:1894,usability,user,user,1894,"in-place in the existing memory buffers). For example - primitives and lists of primitives are supported; anything needing references are not. Despite the fact many of ROOT's very useful deserialization features are not supported, the limited feature set covers a range of analysis use cases. In particular, the current version of CMS's proposed NanoAOD format can be read solely via bulk APIs. Where the bulk IO APIs are applicable, we see an order-magnitude improvement in event rates for some use cases (including CMS NanoAOD). The speedup becomes more noticeable as the data read from the event is smaller (meaning the traditional ROOT IO overheads are larger). Somewhere in the neighborhood of 5KB objects read per event is the point where bulk IO APIs should be noticeably faster. This PR provides three interfaces:. * A raw, low-level interface. The user can invoke the bulk IO APIs directly and get a buffer of primitive types, serialized or deserialized. It is not envisioned this is used by users, but by API writers. * A TTreeReader-like interface, TTreeReaderFast. This is meant to be used by users; it utilizes the low-level interface to retrieve the serialized buffer and templated code will inline the deserializing as part of iterating through the tree. The speed here is gained by the fact only a single pass in memory is needed to deserialize and apply user code, rather than two. * The intent is that this interface can be extended in the future and used by TDataFrame, provided we can make TDataFrame sufficiently fast. * A Python-based numpy export interface. This exposes the (possibly serialized) memory directly as numpy arrays, allowing for efficient access to ROOT-serialized objects from the Python language. * The numpy interface is low-level for Python; we intend to further explore the ideal high-level interface through projects such as https://github.com/diana-hep/plur. (With many thanks to @jpivarski for driving this work home with his work on a python interface!).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2519
https://github.com/root-project/root/pull/2519:2143,usability,memor,memory,2143,"in-place in the existing memory buffers). For example - primitives and lists of primitives are supported; anything needing references are not. Despite the fact many of ROOT's very useful deserialization features are not supported, the limited feature set covers a range of analysis use cases. In particular, the current version of CMS's proposed NanoAOD format can be read solely via bulk APIs. Where the bulk IO APIs are applicable, we see an order-magnitude improvement in event rates for some use cases (including CMS NanoAOD). The speedup becomes more noticeable as the data read from the event is smaller (meaning the traditional ROOT IO overheads are larger). Somewhere in the neighborhood of 5KB objects read per event is the point where bulk IO APIs should be noticeably faster. This PR provides three interfaces:. * A raw, low-level interface. The user can invoke the bulk IO APIs directly and get a buffer of primitive types, serialized or deserialized. It is not envisioned this is used by users, but by API writers. * A TTreeReader-like interface, TTreeReaderFast. This is meant to be used by users; it utilizes the low-level interface to retrieve the serialized buffer and templated code will inline the deserializing as part of iterating through the tree. The speed here is gained by the fact only a single pass in memory is needed to deserialize and apply user code, rather than two. * The intent is that this interface can be extended in the future and used by TDataFrame, provided we can make TDataFrame sufficiently fast. * A Python-based numpy export interface. This exposes the (possibly serialized) memory directly as numpy arrays, allowing for efficient access to ROOT-serialized objects from the Python language. * The numpy interface is low-level for Python; we intend to further explore the ideal high-level interface through projects such as https://github.com/diana-hep/plur. (With many thanks to @jpivarski for driving this work home with his work on a python interface!).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2519
https://github.com/root-project/root/pull/2519:2189,usability,efficien,efficient,2189,"in-place in the existing memory buffers). For example - primitives and lists of primitives are supported; anything needing references are not. Despite the fact many of ROOT's very useful deserialization features are not supported, the limited feature set covers a range of analysis use cases. In particular, the current version of CMS's proposed NanoAOD format can be read solely via bulk APIs. Where the bulk IO APIs are applicable, we see an order-magnitude improvement in event rates for some use cases (including CMS NanoAOD). The speedup becomes more noticeable as the data read from the event is smaller (meaning the traditional ROOT IO overheads are larger). Somewhere in the neighborhood of 5KB objects read per event is the point where bulk IO APIs should be noticeably faster. This PR provides three interfaces:. * A raw, low-level interface. The user can invoke the bulk IO APIs directly and get a buffer of primitive types, serialized or deserialized. It is not envisioned this is used by users, but by API writers. * A TTreeReader-like interface, TTreeReaderFast. This is meant to be used by users; it utilizes the low-level interface to retrieve the serialized buffer and templated code will inline the deserializing as part of iterating through the tree. The speed here is gained by the fact only a single pass in memory is needed to deserialize and apply user code, rather than two. * The intent is that this interface can be extended in the future and used by TDataFrame, provided we can make TDataFrame sufficiently fast. * A Python-based numpy export interface. This exposes the (possibly serialized) memory directly as numpy arrays, allowing for efficient access to ROOT-serialized objects from the Python language. * The numpy interface is low-level for Python; we intend to further explore the ideal high-level interface through projects such as https://github.com/diana-hep/plur. (With many thanks to @jpivarski for driving this work home with his work on a python interface!).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2519
https://github.com/root-project/root/pull/2520:313,deployability,updat,update,313,"[TMVA] remove usage of TFormula in TActivation*; Effectively do what the old comments already say should be done. Remove the indirection to TFormula in TActivationFunction and call C++ code directly. This makes the constructors and destructors trivial (might even default them,. no opinion from my side, happy to update if you have a preference). Also update the comments, where they seem just copy&paste from other files along with the skeleton. PS: I thought I had already discussed (part of?) this at some point in the past but really don't recall. And I guess in a moment from now I will see what clang-format thinks about the change.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2520
https://github.com/root-project/root/pull/2520:352,deployability,updat,update,352,"[TMVA] remove usage of TFormula in TActivation*; Effectively do what the old comments already say should be done. Remove the indirection to TFormula in TActivationFunction and call C++ code directly. This makes the constructors and destructors trivial (might even default them,. no opinion from my side, happy to update if you have a preference). Also update the comments, where they seem just copy&paste from other files along with the skeleton. PS: I thought I had already discussed (part of?) this at some point in the past but really don't recall. And I guess in a moment from now I will see what clang-format thinks about the change.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2520
https://github.com/root-project/root/pull/2520:437,interoperability,skeleton,skeleton,437,"[TMVA] remove usage of TFormula in TActivation*; Effectively do what the old comments already say should be done. Remove the indirection to TFormula in TActivationFunction and call C++ code directly. This makes the constructors and destructors trivial (might even default them,. no opinion from my side, happy to update if you have a preference). Also update the comments, where they seem just copy&paste from other files along with the skeleton. PS: I thought I had already discussed (part of?) this at some point in the past but really don't recall. And I guess in a moment from now I will see what clang-format thinks about the change.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2520
https://github.com/root-project/root/pull/2520:607,interoperability,format,format,607,"[TMVA] remove usage of TFormula in TActivation*; Effectively do what the old comments already say should be done. Remove the indirection to TFormula in TActivationFunction and call C++ code directly. This makes the constructors and destructors trivial (might even default them,. no opinion from my side, happy to update if you have a preference). Also update the comments, where they seem just copy&paste from other files along with the skeleton. PS: I thought I had already discussed (part of?) this at some point in the past but really don't recall. And I guess in a moment from now I will see what clang-format thinks about the change.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2520
https://github.com/root-project/root/pull/2520:313,safety,updat,update,313,"[TMVA] remove usage of TFormula in TActivation*; Effectively do what the old comments already say should be done. Remove the indirection to TFormula in TActivationFunction and call C++ code directly. This makes the constructors and destructors trivial (might even default them,. no opinion from my side, happy to update if you have a preference). Also update the comments, where they seem just copy&paste from other files along with the skeleton. PS: I thought I had already discussed (part of?) this at some point in the past but really don't recall. And I guess in a moment from now I will see what clang-format thinks about the change.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2520
https://github.com/root-project/root/pull/2520:352,safety,updat,update,352,"[TMVA] remove usage of TFormula in TActivation*; Effectively do what the old comments already say should be done. Remove the indirection to TFormula in TActivationFunction and call C++ code directly. This makes the constructors and destructors trivial (might even default them,. no opinion from my side, happy to update if you have a preference). Also update the comments, where they seem just copy&paste from other files along with the skeleton. PS: I thought I had already discussed (part of?) this at some point in the past but really don't recall. And I guess in a moment from now I will see what clang-format thinks about the change.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2520
https://github.com/root-project/root/pull/2520:313,security,updat,update,313,"[TMVA] remove usage of TFormula in TActivation*; Effectively do what the old comments already say should be done. Remove the indirection to TFormula in TActivationFunction and call C++ code directly. This makes the constructors and destructors trivial (might even default them,. no opinion from my side, happy to update if you have a preference). Also update the comments, where they seem just copy&paste from other files along with the skeleton. PS: I thought I had already discussed (part of?) this at some point in the past but really don't recall. And I guess in a moment from now I will see what clang-format thinks about the change.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2520
https://github.com/root-project/root/pull/2520:352,security,updat,update,352,"[TMVA] remove usage of TFormula in TActivation*; Effectively do what the old comments already say should be done. Remove the indirection to TFormula in TActivationFunction and call C++ code directly. This makes the constructors and destructors trivial (might even default them,. no opinion from my side, happy to update if you have a preference). Also update the comments, where they seem just copy&paste from other files along with the skeleton. PS: I thought I had already discussed (part of?) this at some point in the past but really don't recall. And I guess in a moment from now I will see what clang-format thinks about the change.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2520
https://github.com/root-project/root/pull/2520:49,usability,Effectiv,Effectively,49,"[TMVA] remove usage of TFormula in TActivation*; Effectively do what the old comments already say should be done. Remove the indirection to TFormula in TActivationFunction and call C++ code directly. This makes the constructors and destructors trivial (might even default them,. no opinion from my side, happy to update if you have a preference). Also update the comments, where they seem just copy&paste from other files along with the skeleton. PS: I thought I had already discussed (part of?) this at some point in the past but really don't recall. And I guess in a moment from now I will see what clang-format thinks about the change.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2520
https://github.com/root-project/root/pull/2520:334,usability,prefer,preference,334,"[TMVA] remove usage of TFormula in TActivation*; Effectively do what the old comments already say should be done. Remove the indirection to TFormula in TActivationFunction and call C++ code directly. This makes the constructors and destructors trivial (might even default them,. no opinion from my side, happy to update if you have a preference). Also update the comments, where they seem just copy&paste from other files along with the skeleton. PS: I thought I had already discussed (part of?) this at some point in the past but really don't recall. And I guess in a moment from now I will see what clang-format thinks about the change.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2520
https://github.com/root-project/root/pull/2521:62,performance,lock,lock,62,Fix https://sft.its.cern.ch/jira/browse/ROOT-9253; Narrow the lock scope and its rate of use.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2521
https://github.com/root-project/root/pull/2521:62,security,lock,lock,62,Fix https://sft.its.cern.ch/jira/browse/ROOT-9253; Narrow the lock scope and its rate of use.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2521
https://github.com/root-project/root/pull/2522:104,deployability,build,builds,104,[v7hist] Outline ~RHistDrawableBase() to help cling/GCC.; Should fix cling's missing dtor symbol on GCC builds.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2522
https://github.com/root-project/root/pull/2522:41,usability,help,help,41,[v7hist] Outline ~RHistDrawableBase() to help cling/GCC.; Should fix cling's missing dtor symbol on GCC builds.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2522
https://github.com/root-project/root/pull/2523:296,safety,test,testdir,296,"[RDF Helpers] Add helper to create filelists from system directories; @dpiparo @bluehood Do we need such a thing? It would be super convenient in the analysis workflow because often a bunch of files are laying around in a directory, which you need to put all together into RDF:. ```bash. $ ls -l testdir/. a.root. b.root. c.root. ```. ```cpp. RDataFrame df(""tree"", CreateFilelist(""testdir""));. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2523
https://github.com/root-project/root/pull/2523:381,safety,test,testdir,381,"[RDF Helpers] Add helper to create filelists from system directories; @dpiparo @bluehood Do we need such a thing? It would be super convenient in the analysis workflow because often a bunch of files are laying around in a directory, which you need to put all together into RDF:. ```bash. $ ls -l testdir/. a.root. b.root. c.root. ```. ```cpp. RDataFrame df(""tree"", CreateFilelist(""testdir""));. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2523
https://github.com/root-project/root/pull/2523:296,testability,test,testdir,296,"[RDF Helpers] Add helper to create filelists from system directories; @dpiparo @bluehood Do we need such a thing? It would be super convenient in the analysis workflow because often a bunch of files are laying around in a directory, which you need to put all together into RDF:. ```bash. $ ls -l testdir/. a.root. b.root. c.root. ```. ```cpp. RDataFrame df(""tree"", CreateFilelist(""testdir""));. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2523
https://github.com/root-project/root/pull/2523:381,testability,test,testdir,381,"[RDF Helpers] Add helper to create filelists from system directories; @dpiparo @bluehood Do we need such a thing? It would be super convenient in the analysis workflow because often a bunch of files are laying around in a directory, which you need to put all together into RDF:. ```bash. $ ls -l testdir/. a.root. b.root. c.root. ```. ```cpp. RDataFrame df(""tree"", CreateFilelist(""testdir""));. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2523
https://github.com/root-project/root/pull/2523:5,usability,Help,Helpers,5,"[RDF Helpers] Add helper to create filelists from system directories; @dpiparo @bluehood Do we need such a thing? It would be super convenient in the analysis workflow because often a bunch of files are laying around in a directory, which you need to put all together into RDF:. ```bash. $ ls -l testdir/. a.root. b.root. c.root. ```. ```cpp. RDataFrame df(""tree"", CreateFilelist(""testdir""));. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2523
https://github.com/root-project/root/pull/2523:18,usability,help,helper,18,"[RDF Helpers] Add helper to create filelists from system directories; @dpiparo @bluehood Do we need such a thing? It would be super convenient in the analysis workflow because often a bunch of files are laying around in a directory, which you need to put all together into RDF:. ```bash. $ ls -l testdir/. a.root. b.root. c.root. ```. ```cpp. RDataFrame df(""tree"", CreateFilelist(""testdir""));. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2523
https://github.com/root-project/root/pull/2523:159,usability,workflow,workflow,159,"[RDF Helpers] Add helper to create filelists from system directories; @dpiparo @bluehood Do we need such a thing? It would be super convenient in the analysis workflow because often a bunch of files are laying around in a directory, which you need to put all together into RDF:. ```bash. $ ls -l testdir/. a.root. b.root. c.root. ```. ```cpp. RDataFrame df(""tree"", CreateFilelist(""testdir""));. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2523
https://github.com/root-project/root/pull/2524:10,deployability,Updat,Updated,10,[DF][Doc] Updated the release notes;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2524
https://github.com/root-project/root/pull/2524:22,deployability,releas,release,22,[DF][Doc] Updated the release notes;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2524
https://github.com/root-project/root/pull/2524:10,safety,Updat,Updated,10,[DF][Doc] Updated the release notes;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2524
https://github.com/root-project/root/pull/2524:10,security,Updat,Updated,10,[DF][Doc] Updated the release notes;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2524
https://github.com/root-project/root/pull/2525:7,energy efficiency,Reduc,Reduce,7,[TMVA] Reduce work done in CV regression tutorial; The tutorial was sometimes timing out on some nodes. To reduce the workload running the MLP has been skipped and the number of trees in the BDT is cut by a factor 4. On my machine running time went from 77s to 10s.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2525
https://github.com/root-project/root/pull/2525:107,energy efficiency,reduc,reduce,107,[TMVA] Reduce work done in CV regression tutorial; The tutorial was sometimes timing out on some nodes. To reduce the workload running the MLP has been skipped and the number of trees in the BDT is cut by a factor 4. On my machine running time went from 77s to 10s.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2525
https://github.com/root-project/root/pull/2525:118,performance,workload,workload,118,[TMVA] Reduce work done in CV regression tutorial; The tutorial was sometimes timing out on some nodes. To reduce the workload running the MLP has been skipped and the number of trees in the BDT is cut by a factor 4. On my machine running time went from 77s to 10s.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2525
https://github.com/root-project/root/pull/2525:239,performance,time,time,239,[TMVA] Reduce work done in CV regression tutorial; The tutorial was sometimes timing out on some nodes. To reduce the workload running the MLP has been skipped and the number of trees in the BDT is cut by a factor 4. On my machine running time went from 77s to 10s.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2525
https://github.com/root-project/root/pull/2525:30,testability,regress,regression,30,[TMVA] Reduce work done in CV regression tutorial; The tutorial was sometimes timing out on some nodes. To reduce the workload running the MLP has been skipped and the number of trees in the BDT is cut by a factor 4. On my machine running time went from 77s to 10s.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2525
https://github.com/root-project/root/pull/2526:13,deployability,log,logging,13,[TMVA] Extra logging for cv multiproc test; Sometimes the serialised methods will fail to load due to missing files. This adds more output and checks to verify that required files exist to. better pinpoint the problem.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2526
https://github.com/root-project/root/pull/2526:82,deployability,fail,fail,82,[TMVA] Extra logging for cv multiproc test; Sometimes the serialised methods will fail to load due to missing files. This adds more output and checks to verify that required files exist to. better pinpoint the problem.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2526
https://github.com/root-project/root/pull/2526:90,energy efficiency,load,load,90,[TMVA] Extra logging for cv multiproc test; Sometimes the serialised methods will fail to load due to missing files. This adds more output and checks to verify that required files exist to. better pinpoint the problem.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2526
https://github.com/root-project/root/pull/2526:90,performance,load,load,90,[TMVA] Extra logging for cv multiproc test; Sometimes the serialised methods will fail to load due to missing files. This adds more output and checks to verify that required files exist to. better pinpoint the problem.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2526
https://github.com/root-project/root/pull/2526:82,reliability,fail,fail,82,[TMVA] Extra logging for cv multiproc test; Sometimes the serialised methods will fail to load due to missing files. This adds more output and checks to verify that required files exist to. better pinpoint the problem.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2526
https://github.com/root-project/root/pull/2526:13,safety,log,logging,13,[TMVA] Extra logging for cv multiproc test; Sometimes the serialised methods will fail to load due to missing files. This adds more output and checks to verify that required files exist to. better pinpoint the problem.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2526
https://github.com/root-project/root/pull/2526:38,safety,test,test,38,[TMVA] Extra logging for cv multiproc test; Sometimes the serialised methods will fail to load due to missing files. This adds more output and checks to verify that required files exist to. better pinpoint the problem.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2526
https://github.com/root-project/root/pull/2526:13,security,log,logging,13,[TMVA] Extra logging for cv multiproc test; Sometimes the serialised methods will fail to load due to missing files. This adds more output and checks to verify that required files exist to. better pinpoint the problem.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2526
https://github.com/root-project/root/pull/2526:13,testability,log,logging,13,[TMVA] Extra logging for cv multiproc test; Sometimes the serialised methods will fail to load due to missing files. This adds more output and checks to verify that required files exist to. better pinpoint the problem.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2526
https://github.com/root-project/root/pull/2526:38,testability,test,test,38,[TMVA] Extra logging for cv multiproc test; Sometimes the serialised methods will fail to load due to missing files. This adds more output and checks to verify that required files exist to. better pinpoint the problem.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2526
https://github.com/root-project/root/pull/2526:153,testability,verif,verify,153,[TMVA] Extra logging for cv multiproc test; Sometimes the serialised methods will fail to load due to missing files. This adds more output and checks to verify that required files exist to. better pinpoint the problem.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2526
https://github.com/root-project/root/pull/2527:6,safety,except,exceptions,6,Catch exceptions by reference and not by value in TMVA; Fix ROOT-9611 by catching exceptions by reference.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2527
https://github.com/root-project/root/pull/2527:82,safety,except,exceptions,82,Catch exceptions by reference and not by value in TMVA; Fix ROOT-9611 by catching exceptions by reference.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2527
https://github.com/root-project/root/pull/2529:55,deployability,patch,patch,55,[cmake] Narrow the list of included directories.; This patch makes the include paths related to the given target. The side effect is that rootcling gets significantly less -I flags because cmake scans for the content of the INCLUDE_DIRECTORIES which now contains less and more relevant information.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2529
https://github.com/root-project/root/pull/2529:254,deployability,contain,contains,254,[cmake] Narrow the list of included directories.; This patch makes the include paths related to the given target. The side effect is that rootcling gets significantly less -I flags because cmake scans for the content of the INCLUDE_DIRECTORIES which now contains less and more relevant information.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2529
https://github.com/root-project/root/pull/2529:209,performance,content,content,209,[cmake] Narrow the list of included directories.; This patch makes the include paths related to the given target. The side effect is that rootcling gets significantly less -I flags because cmake scans for the content of the INCLUDE_DIRECTORIES which now contains less and more relevant information.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2529
https://github.com/root-project/root/pull/2529:55,safety,patch,patch,55,[cmake] Narrow the list of included directories.; This patch makes the include paths related to the given target. The side effect is that rootcling gets significantly less -I flags because cmake scans for the content of the INCLUDE_DIRECTORIES which now contains less and more relevant information.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2529
https://github.com/root-project/root/pull/2529:55,security,patch,patch,55,[cmake] Narrow the list of included directories.; This patch makes the include paths related to the given target. The side effect is that rootcling gets significantly less -I flags because cmake scans for the content of the INCLUDE_DIRECTORIES which now contains less and more relevant information.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2529
https://github.com/root-project/root/pull/2529:153,security,sign,significantly,153,[cmake] Narrow the list of included directories.; This patch makes the include paths related to the given target. The side effect is that rootcling gets significantly less -I flags because cmake scans for the content of the INCLUDE_DIRECTORIES which now contains less and more relevant information.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2529
https://github.com/root-project/root/pull/2530:13,deployability,Build,Build,13,[cxxmodules] Build cling runtime into module.; The improvement is by 20% from (260MB to 210MB) and within the reach of the PCH.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2530
https://github.com/root-project/root/pull/2530:38,deployability,modul,module,38,[cxxmodules] Build cling runtime into module.; The improvement is by 20% from (260MB to 210MB) and within the reach of the PCH.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2530
https://github.com/root-project/root/pull/2530:38,modifiability,modul,module,38,[cxxmodules] Build cling runtime into module.; The improvement is by 20% from (260MB to 210MB) and within the reach of the PCH.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2530
https://github.com/root-project/root/pull/2530:38,safety,modul,module,38,[cxxmodules] Build cling runtime into module.; The improvement is by 20% from (260MB to 210MB) and within the reach of the PCH.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2530
https://github.com/root-project/root/pull/2532:62,performance,lock,lock,62,Fix https://sft.its.cern.ch/jira/browse/ROOT-9253; Narrow the lock scope and its rate of use.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2532
https://github.com/root-project/root/pull/2532:62,security,lock,lock,62,Fix https://sft.its.cern.ch/jira/browse/ROOT-9253; Narrow the lock scope and its rate of use.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2532
https://github.com/root-project/root/pull/2533:42,deployability,version,version,42,Remove obsolete script. We use the python version of it.;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2533
https://github.com/root-project/root/pull/2533:42,integrability,version,version,42,Remove obsolete script. We use the python version of it.;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2533
https://github.com/root-project/root/pull/2533:42,modifiability,version,version,42,Remove obsolete script. We use the python version of it.;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2533
https://github.com/root-project/root/pull/2534:4,deployability,modul,modules,4,Cxx modules remove root maps;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2534
https://github.com/root-project/root/pull/2534:4,modifiability,modul,modules,4,Cxx modules remove root maps;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2534
https://github.com/root-project/root/pull/2534:4,safety,modul,modules,4,Cxx modules remove root maps;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2534
https://github.com/root-project/root/pull/2536:5,testability,Simpl,Simplify,5,[DF] Simplify internals now that lifetimes are simpler;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2536
https://github.com/root-project/root/pull/2536:47,testability,simpl,simpler,47,[DF] Simplify internals now that lifetimes are simpler;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2536
https://github.com/root-project/root/pull/2536:5,usability,Simpl,Simplify,5,[DF] Simplify internals now that lifetimes are simpler;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2536
https://github.com/root-project/root/pull/2536:47,usability,simpl,simpler,47,[DF] Simplify internals now that lifetimes are simpler;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2536
https://github.com/root-project/root/pull/2537:18,performance,Cach,Cache,18,[DF][NFC] Improve Cache documentation; Fixes ROOT-9646. @JavierCVilla what do you think?,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2537
https://github.com/root-project/root/pull/2537:24,usability,document,documentation,24,[DF][NFC] Improve Cache documentation; Fixes ROOT-9646. @JavierCVilla what do you think?,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2537
https://github.com/root-project/root/pull/2539:79,safety,test,test,79,"[DF] Fix ROOT-9542 ""RDataFrame Sum gets confused by std::string""; ...and add a test for this case",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2539
https://github.com/root-project/root/pull/2539:79,testability,test,test,79,"[DF] Fix ROOT-9542 ""RDataFrame Sum gets confused by std::string""; ...and add a test for this case",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2539
https://github.com/root-project/root/pull/2541:22,safety,test,tests,22,[DF] Improved Display tests;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2541
https://github.com/root-project/root/pull/2541:22,testability,test,tests,22,[DF] Improved Display tests;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2541
https://github.com/root-project/root/pull/2542:4,deployability,modul,modules,4,Cxx modules remove root maps;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2542
https://github.com/root-project/root/pull/2542:4,modifiability,modul,modules,4,Cxx modules remove root maps;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2542
https://github.com/root-project/root/pull/2542:4,safety,modul,modules,4,Cxx modules remove root maps;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2542
https://github.com/root-project/root/pull/2543:26,deployability,build,build,26,"[RDF] Remove RDF from the build in case the platform is a 32bits one; this is done until the present ABI issues between gcc and clang are. solved, most notably the ones concerning shared_ptrs. This PR relates to roottest PR https://github.com/root-project/roottest/pull/216",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2543
https://github.com/root-project/root/pull/2543:44,interoperability,platform,platform,44,"[RDF] Remove RDF from the build in case the platform is a 32bits one; this is done until the present ABI issues between gcc and clang are. solved, most notably the ones concerning shared_ptrs. This PR relates to roottest PR https://github.com/root-project/roottest/pull/216",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2543
https://github.com/root-project/root/pull/2543:169,modifiability,concern,concerning,169,"[RDF] Remove RDF from the build in case the platform is a 32bits one; this is done until the present ABI issues between gcc and clang are. solved, most notably the ones concerning shared_ptrs. This PR relates to roottest PR https://github.com/root-project/roottest/pull/216",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2543
https://github.com/root-project/root/pull/2543:169,testability,concern,concerning,169,"[RDF] Remove RDF from the build in case the platform is a 32bits one; this is done until the present ABI issues between gcc and clang are. solved, most notably the ones concerning shared_ptrs. This PR relates to roottest PR https://github.com/root-project/roottest/pull/216",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2543
https://github.com/root-project/root/pull/2544:26,deployability,build,build,26,"[RDF] Remove RDF from the build in case the platform is a 32bits one; this is done until the present ABI issues between gcc and clang are. solved, most notably the ones concerning shared_ptrs. This PR relates to roottest PR root-project/roottest#216",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2544
https://github.com/root-project/root/pull/2544:44,interoperability,platform,platform,44,"[RDF] Remove RDF from the build in case the platform is a 32bits one; this is done until the present ABI issues between gcc and clang are. solved, most notably the ones concerning shared_ptrs. This PR relates to roottest PR root-project/roottest#216",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2544
https://github.com/root-project/root/pull/2544:169,modifiability,concern,concerning,169,"[RDF] Remove RDF from the build in case the platform is a 32bits one; this is done until the present ABI issues between gcc and clang are. solved, most notably the ones concerning shared_ptrs. This PR relates to roottest PR root-project/roottest#216",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2544
https://github.com/root-project/root/pull/2544:169,testability,concern,concerning,169,"[RDF] Remove RDF from the build in case the platform is a 32bits one; this is done until the present ABI issues between gcc and clang are. solved, most notably the ones concerning shared_ptrs. This PR relates to roottest PR root-project/roottest#216",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2544
https://github.com/root-project/root/pull/2545:35,deployability,build,build,35,"[cxxmodules] Fix the test-periodic-build if -Dcxxmodules=On.; The test should reuse the prebuilt modules in the module cache. However, due to a mismatch between the module-related flags exported by FindROOT. There is a mismatch between the -D passed by cmake. This patch make sure that the build arguments match more closely the build setup of ROOT. Thus we can reuse the already built module files.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2545
https://github.com/root-project/root/pull/2545:97,deployability,modul,modules,97,"[cxxmodules] Fix the test-periodic-build if -Dcxxmodules=On.; The test should reuse the prebuilt modules in the module cache. However, due to a mismatch between the module-related flags exported by FindROOT. There is a mismatch between the -D passed by cmake. This patch make sure that the build arguments match more closely the build setup of ROOT. Thus we can reuse the already built module files.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2545
https://github.com/root-project/root/pull/2545:112,deployability,modul,module,112,"[cxxmodules] Fix the test-periodic-build if -Dcxxmodules=On.; The test should reuse the prebuilt modules in the module cache. However, due to a mismatch between the module-related flags exported by FindROOT. There is a mismatch between the -D passed by cmake. This patch make sure that the build arguments match more closely the build setup of ROOT. Thus we can reuse the already built module files.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2545
https://github.com/root-project/root/pull/2545:165,deployability,modul,module-related,165,"[cxxmodules] Fix the test-periodic-build if -Dcxxmodules=On.; The test should reuse the prebuilt modules in the module cache. However, due to a mismatch between the module-related flags exported by FindROOT. There is a mismatch between the -D passed by cmake. This patch make sure that the build arguments match more closely the build setup of ROOT. Thus we can reuse the already built module files.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2545
https://github.com/root-project/root/pull/2545:265,deployability,patch,patch,265,"[cxxmodules] Fix the test-periodic-build if -Dcxxmodules=On.; The test should reuse the prebuilt modules in the module cache. However, due to a mismatch between the module-related flags exported by FindROOT. There is a mismatch between the -D passed by cmake. This patch make sure that the build arguments match more closely the build setup of ROOT. Thus we can reuse the already built module files.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2545
https://github.com/root-project/root/pull/2545:290,deployability,build,build,290,"[cxxmodules] Fix the test-periodic-build if -Dcxxmodules=On.; The test should reuse the prebuilt modules in the module cache. However, due to a mismatch between the module-related flags exported by FindROOT. There is a mismatch between the -D passed by cmake. This patch make sure that the build arguments match more closely the build setup of ROOT. Thus we can reuse the already built module files.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2545
https://github.com/root-project/root/pull/2545:329,deployability,build,build,329,"[cxxmodules] Fix the test-periodic-build if -Dcxxmodules=On.; The test should reuse the prebuilt modules in the module cache. However, due to a mismatch between the module-related flags exported by FindROOT. There is a mismatch between the -D passed by cmake. This patch make sure that the build arguments match more closely the build setup of ROOT. Thus we can reuse the already built module files.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2545
https://github.com/root-project/root/pull/2545:386,deployability,modul,module,386,"[cxxmodules] Fix the test-periodic-build if -Dcxxmodules=On.; The test should reuse the prebuilt modules in the module cache. However, due to a mismatch between the module-related flags exported by FindROOT. There is a mismatch between the -D passed by cmake. This patch make sure that the build arguments match more closely the build setup of ROOT. Thus we can reuse the already built module files.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2545
https://github.com/root-project/root/pull/2545:144,interoperability,mismatch,mismatch,144,"[cxxmodules] Fix the test-periodic-build if -Dcxxmodules=On.; The test should reuse the prebuilt modules in the module cache. However, due to a mismatch between the module-related flags exported by FindROOT. There is a mismatch between the -D passed by cmake. This patch make sure that the build arguments match more closely the build setup of ROOT. Thus we can reuse the already built module files.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2545
https://github.com/root-project/root/pull/2545:219,interoperability,mismatch,mismatch,219,"[cxxmodules] Fix the test-periodic-build if -Dcxxmodules=On.; The test should reuse the prebuilt modules in the module cache. However, due to a mismatch between the module-related flags exported by FindROOT. There is a mismatch between the -D passed by cmake. This patch make sure that the build arguments match more closely the build setup of ROOT. Thus we can reuse the already built module files.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2545
https://github.com/root-project/root/pull/2545:78,modifiability,reu,reuse,78,"[cxxmodules] Fix the test-periodic-build if -Dcxxmodules=On.; The test should reuse the prebuilt modules in the module cache. However, due to a mismatch between the module-related flags exported by FindROOT. There is a mismatch between the -D passed by cmake. This patch make sure that the build arguments match more closely the build setup of ROOT. Thus we can reuse the already built module files.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2545
https://github.com/root-project/root/pull/2545:97,modifiability,modul,modules,97,"[cxxmodules] Fix the test-periodic-build if -Dcxxmodules=On.; The test should reuse the prebuilt modules in the module cache. However, due to a mismatch between the module-related flags exported by FindROOT. There is a mismatch between the -D passed by cmake. This patch make sure that the build arguments match more closely the build setup of ROOT. Thus we can reuse the already built module files.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2545
https://github.com/root-project/root/pull/2545:112,modifiability,modul,module,112,"[cxxmodules] Fix the test-periodic-build if -Dcxxmodules=On.; The test should reuse the prebuilt modules in the module cache. However, due to a mismatch between the module-related flags exported by FindROOT. There is a mismatch between the -D passed by cmake. This patch make sure that the build arguments match more closely the build setup of ROOT. Thus we can reuse the already built module files.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2545
https://github.com/root-project/root/pull/2545:165,modifiability,modul,module-related,165,"[cxxmodules] Fix the test-periodic-build if -Dcxxmodules=On.; The test should reuse the prebuilt modules in the module cache. However, due to a mismatch between the module-related flags exported by FindROOT. There is a mismatch between the -D passed by cmake. This patch make sure that the build arguments match more closely the build setup of ROOT. Thus we can reuse the already built module files.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2545
https://github.com/root-project/root/pull/2545:362,modifiability,reu,reuse,362,"[cxxmodules] Fix the test-periodic-build if -Dcxxmodules=On.; The test should reuse the prebuilt modules in the module cache. However, due to a mismatch between the module-related flags exported by FindROOT. There is a mismatch between the -D passed by cmake. This patch make sure that the build arguments match more closely the build setup of ROOT. Thus we can reuse the already built module files.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2545
https://github.com/root-project/root/pull/2545:386,modifiability,modul,module,386,"[cxxmodules] Fix the test-periodic-build if -Dcxxmodules=On.; The test should reuse the prebuilt modules in the module cache. However, due to a mismatch between the module-related flags exported by FindROOT. There is a mismatch between the -D passed by cmake. This patch make sure that the build arguments match more closely the build setup of ROOT. Thus we can reuse the already built module files.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2545
https://github.com/root-project/root/pull/2545:119,performance,cach,cache,119,"[cxxmodules] Fix the test-periodic-build if -Dcxxmodules=On.; The test should reuse the prebuilt modules in the module cache. However, due to a mismatch between the module-related flags exported by FindROOT. There is a mismatch between the -D passed by cmake. This patch make sure that the build arguments match more closely the build setup of ROOT. Thus we can reuse the already built module files.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2545
https://github.com/root-project/root/pull/2545:21,safety,test,test-periodic-build,21,"[cxxmodules] Fix the test-periodic-build if -Dcxxmodules=On.; The test should reuse the prebuilt modules in the module cache. However, due to a mismatch between the module-related flags exported by FindROOT. There is a mismatch between the -D passed by cmake. This patch make sure that the build arguments match more closely the build setup of ROOT. Thus we can reuse the already built module files.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2545
https://github.com/root-project/root/pull/2545:66,safety,test,test,66,"[cxxmodules] Fix the test-periodic-build if -Dcxxmodules=On.; The test should reuse the prebuilt modules in the module cache. However, due to a mismatch between the module-related flags exported by FindROOT. There is a mismatch between the -D passed by cmake. This patch make sure that the build arguments match more closely the build setup of ROOT. Thus we can reuse the already built module files.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2545
https://github.com/root-project/root/pull/2545:97,safety,modul,modules,97,"[cxxmodules] Fix the test-periodic-build if -Dcxxmodules=On.; The test should reuse the prebuilt modules in the module cache. However, due to a mismatch between the module-related flags exported by FindROOT. There is a mismatch between the -D passed by cmake. This patch make sure that the build arguments match more closely the build setup of ROOT. Thus we can reuse the already built module files.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2545
https://github.com/root-project/root/pull/2545:112,safety,modul,module,112,"[cxxmodules] Fix the test-periodic-build if -Dcxxmodules=On.; The test should reuse the prebuilt modules in the module cache. However, due to a mismatch between the module-related flags exported by FindROOT. There is a mismatch between the -D passed by cmake. This patch make sure that the build arguments match more closely the build setup of ROOT. Thus we can reuse the already built module files.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2545
https://github.com/root-project/root/pull/2545:165,safety,modul,module-related,165,"[cxxmodules] Fix the test-periodic-build if -Dcxxmodules=On.; The test should reuse the prebuilt modules in the module cache. However, due to a mismatch between the module-related flags exported by FindROOT. There is a mismatch between the -D passed by cmake. This patch make sure that the build arguments match more closely the build setup of ROOT. Thus we can reuse the already built module files.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2545
https://github.com/root-project/root/pull/2545:265,safety,patch,patch,265,"[cxxmodules] Fix the test-periodic-build if -Dcxxmodules=On.; The test should reuse the prebuilt modules in the module cache. However, due to a mismatch between the module-related flags exported by FindROOT. There is a mismatch between the -D passed by cmake. This patch make sure that the build arguments match more closely the build setup of ROOT. Thus we can reuse the already built module files.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2545
https://github.com/root-project/root/pull/2545:386,safety,modul,module,386,"[cxxmodules] Fix the test-periodic-build if -Dcxxmodules=On.; The test should reuse the prebuilt modules in the module cache. However, due to a mismatch between the module-related flags exported by FindROOT. There is a mismatch between the -D passed by cmake. This patch make sure that the build arguments match more closely the build setup of ROOT. Thus we can reuse the already built module files.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2545
https://github.com/root-project/root/pull/2545:265,security,patch,patch,265,"[cxxmodules] Fix the test-periodic-build if -Dcxxmodules=On.; The test should reuse the prebuilt modules in the module cache. However, due to a mismatch between the module-related flags exported by FindROOT. There is a mismatch between the -D passed by cmake. This patch make sure that the build arguments match more closely the build setup of ROOT. Thus we can reuse the already built module files.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2545
https://github.com/root-project/root/pull/2545:21,testability,test,test-periodic-build,21,"[cxxmodules] Fix the test-periodic-build if -Dcxxmodules=On.; The test should reuse the prebuilt modules in the module cache. However, due to a mismatch between the module-related flags exported by FindROOT. There is a mismatch between the -D passed by cmake. This patch make sure that the build arguments match more closely the build setup of ROOT. Thus we can reuse the already built module files.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2545
https://github.com/root-project/root/pull/2545:66,testability,test,test,66,"[cxxmodules] Fix the test-periodic-build if -Dcxxmodules=On.; The test should reuse the prebuilt modules in the module cache. However, due to a mismatch between the module-related flags exported by FindROOT. There is a mismatch between the -D passed by cmake. This patch make sure that the build arguments match more closely the build setup of ROOT. Thus we can reuse the already built module files.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2545
https://github.com/root-project/root/pull/2545:317,usability,close,closely,317,"[cxxmodules] Fix the test-periodic-build if -Dcxxmodules=On.; The test should reuse the prebuilt modules in the module cache. However, due to a mismatch between the module-related flags exported by FindROOT. There is a mismatch between the -D passed by cmake. This patch make sure that the build arguments match more closely the build setup of ROOT. Thus we can reuse the already built module files.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2545
https://github.com/root-project/root/pull/2547:142,availability,error,errors,142,"[TMVA] configure tutorials/tmva/makefile; example makefile from tutorials/tmva should use the same c++ standard as. root (otherwise one faces errors from string views in the root headers. when building the tutorials). Note:. - I also changed in the global CMakeLists.txt `tutorial_file` into `artifact_file`, (which looks like a renaming variable mistake to me, but didn't go through the history). - This causes `tutorials/tmva/makefile` to not appear in artifact_files_builddir and thus the dependencies of move_artifacts. → I am not sure if I broke dependencies here. (WIP struggeling to install the configured makefile)",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2547
https://github.com/root-project/root/pull/2547:193,deployability,build,building,193,"[TMVA] configure tutorials/tmva/makefile; example makefile from tutorials/tmva should use the same c++ standard as. root (otherwise one faces errors from string views in the root headers. when building the tutorials). Note:. - I also changed in the global CMakeLists.txt `tutorial_file` into `artifact_file`, (which looks like a renaming variable mistake to me, but didn't go through the history). - This causes `tutorials/tmva/makefile` to not appear in artifact_files_builddir and thus the dependencies of move_artifacts. → I am not sure if I broke dependencies here. (WIP struggeling to install the configured makefile)",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2547
https://github.com/root-project/root/pull/2547:492,deployability,depend,dependencies,492,"[TMVA] configure tutorials/tmva/makefile; example makefile from tutorials/tmva should use the same c++ standard as. root (otherwise one faces errors from string views in the root headers. when building the tutorials). Note:. - I also changed in the global CMakeLists.txt `tutorial_file` into `artifact_file`, (which looks like a renaming variable mistake to me, but didn't go through the history). - This causes `tutorials/tmva/makefile` to not appear in artifact_files_builddir and thus the dependencies of move_artifacts. → I am not sure if I broke dependencies here. (WIP struggeling to install the configured makefile)",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2547
https://github.com/root-project/root/pull/2547:551,deployability,depend,dependencies,551,"[TMVA] configure tutorials/tmva/makefile; example makefile from tutorials/tmva should use the same c++ standard as. root (otherwise one faces errors from string views in the root headers. when building the tutorials). Note:. - I also changed in the global CMakeLists.txt `tutorial_file` into `artifact_file`, (which looks like a renaming variable mistake to me, but didn't go through the history). - This causes `tutorials/tmva/makefile` to not appear in artifact_files_builddir and thus the dependencies of move_artifacts. → I am not sure if I broke dependencies here. (WIP struggeling to install the configured makefile)",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2547
https://github.com/root-project/root/pull/2547:590,deployability,instal,install,590,"[TMVA] configure tutorials/tmva/makefile; example makefile from tutorials/tmva should use the same c++ standard as. root (otherwise one faces errors from string views in the root headers. when building the tutorials). Note:. - I also changed in the global CMakeLists.txt `tutorial_file` into `artifact_file`, (which looks like a renaming variable mistake to me, but didn't go through the history). - This causes `tutorials/tmva/makefile` to not appear in artifact_files_builddir and thus the dependencies of move_artifacts. → I am not sure if I broke dependencies here. (WIP struggeling to install the configured makefile)",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2547
https://github.com/root-project/root/pull/2547:7,integrability,configur,configure,7,"[TMVA] configure tutorials/tmva/makefile; example makefile from tutorials/tmva should use the same c++ standard as. root (otherwise one faces errors from string views in the root headers. when building the tutorials). Note:. - I also changed in the global CMakeLists.txt `tutorial_file` into `artifact_file`, (which looks like a renaming variable mistake to me, but didn't go through the history). - This causes `tutorials/tmva/makefile` to not appear in artifact_files_builddir and thus the dependencies of move_artifacts. → I am not sure if I broke dependencies here. (WIP struggeling to install the configured makefile)",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2547
https://github.com/root-project/root/pull/2547:492,integrability,depend,dependencies,492,"[TMVA] configure tutorials/tmva/makefile; example makefile from tutorials/tmva should use the same c++ standard as. root (otherwise one faces errors from string views in the root headers. when building the tutorials). Note:. - I also changed in the global CMakeLists.txt `tutorial_file` into `artifact_file`, (which looks like a renaming variable mistake to me, but didn't go through the history). - This causes `tutorials/tmva/makefile` to not appear in artifact_files_builddir and thus the dependencies of move_artifacts. → I am not sure if I broke dependencies here. (WIP struggeling to install the configured makefile)",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2547
https://github.com/root-project/root/pull/2547:551,integrability,depend,dependencies,551,"[TMVA] configure tutorials/tmva/makefile; example makefile from tutorials/tmva should use the same c++ standard as. root (otherwise one faces errors from string views in the root headers. when building the tutorials). Note:. - I also changed in the global CMakeLists.txt `tutorial_file` into `artifact_file`, (which looks like a renaming variable mistake to me, but didn't go through the history). - This causes `tutorials/tmva/makefile` to not appear in artifact_files_builddir and thus the dependencies of move_artifacts. → I am not sure if I broke dependencies here. (WIP struggeling to install the configured makefile)",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2547
https://github.com/root-project/root/pull/2547:602,integrability,configur,configured,602,"[TMVA] configure tutorials/tmva/makefile; example makefile from tutorials/tmva should use the same c++ standard as. root (otherwise one faces errors from string views in the root headers. when building the tutorials). Note:. - I also changed in the global CMakeLists.txt `tutorial_file` into `artifact_file`, (which looks like a renaming variable mistake to me, but didn't go through the history). - This causes `tutorials/tmva/makefile` to not appear in artifact_files_builddir and thus the dependencies of move_artifacts. → I am not sure if I broke dependencies here. (WIP struggeling to install the configured makefile)",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2547
https://github.com/root-project/root/pull/2547:103,interoperability,standard,standard,103,"[TMVA] configure tutorials/tmva/makefile; example makefile from tutorials/tmva should use the same c++ standard as. root (otherwise one faces errors from string views in the root headers. when building the tutorials). Note:. - I also changed in the global CMakeLists.txt `tutorial_file` into `artifact_file`, (which looks like a renaming variable mistake to me, but didn't go through the history). - This causes `tutorials/tmva/makefile` to not appear in artifact_files_builddir and thus the dependencies of move_artifacts. → I am not sure if I broke dependencies here. (WIP struggeling to install the configured makefile)",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2547
https://github.com/root-project/root/pull/2547:7,modifiability,configur,configure,7,"[TMVA] configure tutorials/tmva/makefile; example makefile from tutorials/tmva should use the same c++ standard as. root (otherwise one faces errors from string views in the root headers. when building the tutorials). Note:. - I also changed in the global CMakeLists.txt `tutorial_file` into `artifact_file`, (which looks like a renaming variable mistake to me, but didn't go through the history). - This causes `tutorials/tmva/makefile` to not appear in artifact_files_builddir and thus the dependencies of move_artifacts. → I am not sure if I broke dependencies here. (WIP struggeling to install the configured makefile)",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2547
https://github.com/root-project/root/pull/2547:338,modifiability,variab,variable,338,"[TMVA] configure tutorials/tmva/makefile; example makefile from tutorials/tmva should use the same c++ standard as. root (otherwise one faces errors from string views in the root headers. when building the tutorials). Note:. - I also changed in the global CMakeLists.txt `tutorial_file` into `artifact_file`, (which looks like a renaming variable mistake to me, but didn't go through the history). - This causes `tutorials/tmva/makefile` to not appear in artifact_files_builddir and thus the dependencies of move_artifacts. → I am not sure if I broke dependencies here. (WIP struggeling to install the configured makefile)",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2547
https://github.com/root-project/root/pull/2547:492,modifiability,depend,dependencies,492,"[TMVA] configure tutorials/tmva/makefile; example makefile from tutorials/tmva should use the same c++ standard as. root (otherwise one faces errors from string views in the root headers. when building the tutorials). Note:. - I also changed in the global CMakeLists.txt `tutorial_file` into `artifact_file`, (which looks like a renaming variable mistake to me, but didn't go through the history). - This causes `tutorials/tmva/makefile` to not appear in artifact_files_builddir and thus the dependencies of move_artifacts. → I am not sure if I broke dependencies here. (WIP struggeling to install the configured makefile)",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2547
https://github.com/root-project/root/pull/2547:551,modifiability,depend,dependencies,551,"[TMVA] configure tutorials/tmva/makefile; example makefile from tutorials/tmva should use the same c++ standard as. root (otherwise one faces errors from string views in the root headers. when building the tutorials). Note:. - I also changed in the global CMakeLists.txt `tutorial_file` into `artifact_file`, (which looks like a renaming variable mistake to me, but didn't go through the history). - This causes `tutorials/tmva/makefile` to not appear in artifact_files_builddir and thus the dependencies of move_artifacts. → I am not sure if I broke dependencies here. (WIP struggeling to install the configured makefile)",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2547
https://github.com/root-project/root/pull/2547:602,modifiability,configur,configured,602,"[TMVA] configure tutorials/tmva/makefile; example makefile from tutorials/tmva should use the same c++ standard as. root (otherwise one faces errors from string views in the root headers. when building the tutorials). Note:. - I also changed in the global CMakeLists.txt `tutorial_file` into `artifact_file`, (which looks like a renaming variable mistake to me, but didn't go through the history). - This causes `tutorials/tmva/makefile` to not appear in artifact_files_builddir and thus the dependencies of move_artifacts. → I am not sure if I broke dependencies here. (WIP struggeling to install the configured makefile)",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2547
https://github.com/root-project/root/pull/2547:142,performance,error,errors,142,"[TMVA] configure tutorials/tmva/makefile; example makefile from tutorials/tmva should use the same c++ standard as. root (otherwise one faces errors from string views in the root headers. when building the tutorials). Note:. - I also changed in the global CMakeLists.txt `tutorial_file` into `artifact_file`, (which looks like a renaming variable mistake to me, but didn't go through the history). - This causes `tutorials/tmva/makefile` to not appear in artifact_files_builddir and thus the dependencies of move_artifacts. → I am not sure if I broke dependencies here. (WIP struggeling to install the configured makefile)",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2547
https://github.com/root-project/root/pull/2547:142,safety,error,errors,142,"[TMVA] configure tutorials/tmva/makefile; example makefile from tutorials/tmva should use the same c++ standard as. root (otherwise one faces errors from string views in the root headers. when building the tutorials). Note:. - I also changed in the global CMakeLists.txt `tutorial_file` into `artifact_file`, (which looks like a renaming variable mistake to me, but didn't go through the history). - This causes `tutorials/tmva/makefile` to not appear in artifact_files_builddir and thus the dependencies of move_artifacts. → I am not sure if I broke dependencies here. (WIP struggeling to install the configured makefile)",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2547
https://github.com/root-project/root/pull/2547:492,safety,depend,dependencies,492,"[TMVA] configure tutorials/tmva/makefile; example makefile from tutorials/tmva should use the same c++ standard as. root (otherwise one faces errors from string views in the root headers. when building the tutorials). Note:. - I also changed in the global CMakeLists.txt `tutorial_file` into `artifact_file`, (which looks like a renaming variable mistake to me, but didn't go through the history). - This causes `tutorials/tmva/makefile` to not appear in artifact_files_builddir and thus the dependencies of move_artifacts. → I am not sure if I broke dependencies here. (WIP struggeling to install the configured makefile)",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2547
https://github.com/root-project/root/pull/2547:551,safety,depend,dependencies,551,"[TMVA] configure tutorials/tmva/makefile; example makefile from tutorials/tmva should use the same c++ standard as. root (otherwise one faces errors from string views in the root headers. when building the tutorials). Note:. - I also changed in the global CMakeLists.txt `tutorial_file` into `artifact_file`, (which looks like a renaming variable mistake to me, but didn't go through the history). - This causes `tutorials/tmva/makefile` to not appear in artifact_files_builddir and thus the dependencies of move_artifacts. → I am not sure if I broke dependencies here. (WIP struggeling to install the configured makefile)",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2547
https://github.com/root-project/root/pull/2547:7,security,configur,configure,7,"[TMVA] configure tutorials/tmva/makefile; example makefile from tutorials/tmva should use the same c++ standard as. root (otherwise one faces errors from string views in the root headers. when building the tutorials). Note:. - I also changed in the global CMakeLists.txt `tutorial_file` into `artifact_file`, (which looks like a renaming variable mistake to me, but didn't go through the history). - This causes `tutorials/tmva/makefile` to not appear in artifact_files_builddir and thus the dependencies of move_artifacts. → I am not sure if I broke dependencies here. (WIP struggeling to install the configured makefile)",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2547
https://github.com/root-project/root/pull/2547:602,security,configur,configured,602,"[TMVA] configure tutorials/tmva/makefile; example makefile from tutorials/tmva should use the same c++ standard as. root (otherwise one faces errors from string views in the root headers. when building the tutorials). Note:. - I also changed in the global CMakeLists.txt `tutorial_file` into `artifact_file`, (which looks like a renaming variable mistake to me, but didn't go through the history). - This causes `tutorials/tmva/makefile` to not appear in artifact_files_builddir and thus the dependencies of move_artifacts. → I am not sure if I broke dependencies here. (WIP struggeling to install the configured makefile)",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2547
https://github.com/root-project/root/pull/2547:492,testability,depend,dependencies,492,"[TMVA] configure tutorials/tmva/makefile; example makefile from tutorials/tmva should use the same c++ standard as. root (otherwise one faces errors from string views in the root headers. when building the tutorials). Note:. - I also changed in the global CMakeLists.txt `tutorial_file` into `artifact_file`, (which looks like a renaming variable mistake to me, but didn't go through the history). - This causes `tutorials/tmva/makefile` to not appear in artifact_files_builddir and thus the dependencies of move_artifacts. → I am not sure if I broke dependencies here. (WIP struggeling to install the configured makefile)",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2547
https://github.com/root-project/root/pull/2547:551,testability,depend,dependencies,551,"[TMVA] configure tutorials/tmva/makefile; example makefile from tutorials/tmva should use the same c++ standard as. root (otherwise one faces errors from string views in the root headers. when building the tutorials). Note:. - I also changed in the global CMakeLists.txt `tutorial_file` into `artifact_file`, (which looks like a renaming variable mistake to me, but didn't go through the history). - This causes `tutorials/tmva/makefile` to not appear in artifact_files_builddir and thus the dependencies of move_artifacts. → I am not sure if I broke dependencies here. (WIP struggeling to install the configured makefile)",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2547
https://github.com/root-project/root/pull/2547:142,usability,error,errors,142,"[TMVA] configure tutorials/tmva/makefile; example makefile from tutorials/tmva should use the same c++ standard as. root (otherwise one faces errors from string views in the root headers. when building the tutorials). Note:. - I also changed in the global CMakeLists.txt `tutorial_file` into `artifact_file`, (which looks like a renaming variable mistake to me, but didn't go through the history). - This causes `tutorials/tmva/makefile` to not appear in artifact_files_builddir and thus the dependencies of move_artifacts. → I am not sure if I broke dependencies here. (WIP struggeling to install the configured makefile)",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2547
https://github.com/root-project/root/pull/2551:10,deployability,Updat,Updated,10,[DF][Doc] Updated the doc; - Ordered entries. - Added GetFilterNames,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2551
https://github.com/root-project/root/pull/2551:10,safety,Updat,Updated,10,[DF][Doc] Updated the doc; - Ordered entries. - Added GetFilterNames,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2551
https://github.com/root-project/root/pull/2551:10,security,Updat,Updated,10,[DF][Doc] Updated the doc; - Ordered entries. - Added GetFilterNames,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2551
https://github.com/root-project/root/pull/2554:203,availability,error,error,203,"Updates for generation of correct relative path for core headers; After removing globbing in core module, if to fetch a fresh master. some headers were not able to get a valid relative path causing next error:. -- /home/oksana/CERN_sources/root/builds/include/TArrayF.h. CMake Error at cmake/modules/RootNewMacros.cmake:292 (message):. Header path '/home/oksana/CERN_sources/root/builds/include/TArrayF.h'. TArrayF.h is not relative! Call Stack (most recent call first):. core/base/CMakeLists.txt:232 (ROOT_GENERATE_DICTIONARY). Adding extra replacement pattern, matching - $CMAKE_BUILD_DIR/include, fixes issue.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2554
https://github.com/root-project/root/pull/2554:277,availability,Error,Error,277,"Updates for generation of correct relative path for core headers; After removing globbing in core module, if to fetch a fresh master. some headers were not able to get a valid relative path causing next error:. -- /home/oksana/CERN_sources/root/builds/include/TArrayF.h. CMake Error at cmake/modules/RootNewMacros.cmake:292 (message):. Header path '/home/oksana/CERN_sources/root/builds/include/TArrayF.h'. TArrayF.h is not relative! Call Stack (most recent call first):. core/base/CMakeLists.txt:232 (ROOT_GENERATE_DICTIONARY). Adding extra replacement pattern, matching - $CMAKE_BUILD_DIR/include, fixes issue.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2554
https://github.com/root-project/root/pull/2554:0,deployability,Updat,Updates,0,"Updates for generation of correct relative path for core headers; After removing globbing in core module, if to fetch a fresh master. some headers were not able to get a valid relative path causing next error:. -- /home/oksana/CERN_sources/root/builds/include/TArrayF.h. CMake Error at cmake/modules/RootNewMacros.cmake:292 (message):. Header path '/home/oksana/CERN_sources/root/builds/include/TArrayF.h'. TArrayF.h is not relative! Call Stack (most recent call first):. core/base/CMakeLists.txt:232 (ROOT_GENERATE_DICTIONARY). Adding extra replacement pattern, matching - $CMAKE_BUILD_DIR/include, fixes issue.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2554
https://github.com/root-project/root/pull/2554:98,deployability,modul,module,98,"Updates for generation of correct relative path for core headers; After removing globbing in core module, if to fetch a fresh master. some headers were not able to get a valid relative path causing next error:. -- /home/oksana/CERN_sources/root/builds/include/TArrayF.h. CMake Error at cmake/modules/RootNewMacros.cmake:292 (message):. Header path '/home/oksana/CERN_sources/root/builds/include/TArrayF.h'. TArrayF.h is not relative! Call Stack (most recent call first):. core/base/CMakeLists.txt:232 (ROOT_GENERATE_DICTIONARY). Adding extra replacement pattern, matching - $CMAKE_BUILD_DIR/include, fixes issue.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2554
https://github.com/root-project/root/pull/2554:245,deployability,build,builds,245,"Updates for generation of correct relative path for core headers; After removing globbing in core module, if to fetch a fresh master. some headers were not able to get a valid relative path causing next error:. -- /home/oksana/CERN_sources/root/builds/include/TArrayF.h. CMake Error at cmake/modules/RootNewMacros.cmake:292 (message):. Header path '/home/oksana/CERN_sources/root/builds/include/TArrayF.h'. TArrayF.h is not relative! Call Stack (most recent call first):. core/base/CMakeLists.txt:232 (ROOT_GENERATE_DICTIONARY). Adding extra replacement pattern, matching - $CMAKE_BUILD_DIR/include, fixes issue.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2554
https://github.com/root-project/root/pull/2554:292,deployability,modul,modules,292,"Updates for generation of correct relative path for core headers; After removing globbing in core module, if to fetch a fresh master. some headers were not able to get a valid relative path causing next error:. -- /home/oksana/CERN_sources/root/builds/include/TArrayF.h. CMake Error at cmake/modules/RootNewMacros.cmake:292 (message):. Header path '/home/oksana/CERN_sources/root/builds/include/TArrayF.h'. TArrayF.h is not relative! Call Stack (most recent call first):. core/base/CMakeLists.txt:232 (ROOT_GENERATE_DICTIONARY). Adding extra replacement pattern, matching - $CMAKE_BUILD_DIR/include, fixes issue.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2554
https://github.com/root-project/root/pull/2554:380,deployability,build,builds,380,"Updates for generation of correct relative path for core headers; After removing globbing in core module, if to fetch a fresh master. some headers were not able to get a valid relative path causing next error:. -- /home/oksana/CERN_sources/root/builds/include/TArrayF.h. CMake Error at cmake/modules/RootNewMacros.cmake:292 (message):. Header path '/home/oksana/CERN_sources/root/builds/include/TArrayF.h'. TArrayF.h is not relative! Call Stack (most recent call first):. core/base/CMakeLists.txt:232 (ROOT_GENERATE_DICTIONARY). Adding extra replacement pattern, matching - $CMAKE_BUILD_DIR/include, fixes issue.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2554
https://github.com/root-project/root/pull/2554:439,deployability,Stack,Stack,439,"Updates for generation of correct relative path for core headers; After removing globbing in core module, if to fetch a fresh master. some headers were not able to get a valid relative path causing next error:. -- /home/oksana/CERN_sources/root/builds/include/TArrayF.h. CMake Error at cmake/modules/RootNewMacros.cmake:292 (message):. Header path '/home/oksana/CERN_sources/root/builds/include/TArrayF.h'. TArrayF.h is not relative! Call Stack (most recent call first):. core/base/CMakeLists.txt:232 (ROOT_GENERATE_DICTIONARY). Adding extra replacement pattern, matching - $CMAKE_BUILD_DIR/include, fixes issue.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2554
https://github.com/root-project/root/pull/2554:52,energy efficiency,core,core,52,"Updates for generation of correct relative path for core headers; After removing globbing in core module, if to fetch a fresh master. some headers were not able to get a valid relative path causing next error:. -- /home/oksana/CERN_sources/root/builds/include/TArrayF.h. CMake Error at cmake/modules/RootNewMacros.cmake:292 (message):. Header path '/home/oksana/CERN_sources/root/builds/include/TArrayF.h'. TArrayF.h is not relative! Call Stack (most recent call first):. core/base/CMakeLists.txt:232 (ROOT_GENERATE_DICTIONARY). Adding extra replacement pattern, matching - $CMAKE_BUILD_DIR/include, fixes issue.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2554
https://github.com/root-project/root/pull/2554:93,energy efficiency,core,core,93,"Updates for generation of correct relative path for core headers; After removing globbing in core module, if to fetch a fresh master. some headers were not able to get a valid relative path causing next error:. -- /home/oksana/CERN_sources/root/builds/include/TArrayF.h. CMake Error at cmake/modules/RootNewMacros.cmake:292 (message):. Header path '/home/oksana/CERN_sources/root/builds/include/TArrayF.h'. TArrayF.h is not relative! Call Stack (most recent call first):. core/base/CMakeLists.txt:232 (ROOT_GENERATE_DICTIONARY). Adding extra replacement pattern, matching - $CMAKE_BUILD_DIR/include, fixes issue.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2554
https://github.com/root-project/root/pull/2554:472,energy efficiency,core,core,472,"Updates for generation of correct relative path for core headers; After removing globbing in core module, if to fetch a fresh master. some headers were not able to get a valid relative path causing next error:. -- /home/oksana/CERN_sources/root/builds/include/TArrayF.h. CMake Error at cmake/modules/RootNewMacros.cmake:292 (message):. Header path '/home/oksana/CERN_sources/root/builds/include/TArrayF.h'. TArrayF.h is not relative! Call Stack (most recent call first):. core/base/CMakeLists.txt:232 (ROOT_GENERATE_DICTIONARY). Adding extra replacement pattern, matching - $CMAKE_BUILD_DIR/include, fixes issue.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2554
https://github.com/root-project/root/pull/2554:325,integrability,messag,message,325,"Updates for generation of correct relative path for core headers; After removing globbing in core module, if to fetch a fresh master. some headers were not able to get a valid relative path causing next error:. -- /home/oksana/CERN_sources/root/builds/include/TArrayF.h. CMake Error at cmake/modules/RootNewMacros.cmake:292 (message):. Header path '/home/oksana/CERN_sources/root/builds/include/TArrayF.h'. TArrayF.h is not relative! Call Stack (most recent call first):. core/base/CMakeLists.txt:232 (ROOT_GENERATE_DICTIONARY). Adding extra replacement pattern, matching - $CMAKE_BUILD_DIR/include, fixes issue.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2554
https://github.com/root-project/root/pull/2554:325,interoperability,messag,message,325,"Updates for generation of correct relative path for core headers; After removing globbing in core module, if to fetch a fresh master. some headers were not able to get a valid relative path causing next error:. -- /home/oksana/CERN_sources/root/builds/include/TArrayF.h. CMake Error at cmake/modules/RootNewMacros.cmake:292 (message):. Header path '/home/oksana/CERN_sources/root/builds/include/TArrayF.h'. TArrayF.h is not relative! Call Stack (most recent call first):. core/base/CMakeLists.txt:232 (ROOT_GENERATE_DICTIONARY). Adding extra replacement pattern, matching - $CMAKE_BUILD_DIR/include, fixes issue.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2554
https://github.com/root-project/root/pull/2554:98,modifiability,modul,module,98,"Updates for generation of correct relative path for core headers; After removing globbing in core module, if to fetch a fresh master. some headers were not able to get a valid relative path causing next error:. -- /home/oksana/CERN_sources/root/builds/include/TArrayF.h. CMake Error at cmake/modules/RootNewMacros.cmake:292 (message):. Header path '/home/oksana/CERN_sources/root/builds/include/TArrayF.h'. TArrayF.h is not relative! Call Stack (most recent call first):. core/base/CMakeLists.txt:232 (ROOT_GENERATE_DICTIONARY). Adding extra replacement pattern, matching - $CMAKE_BUILD_DIR/include, fixes issue.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2554
https://github.com/root-project/root/pull/2554:292,modifiability,modul,modules,292,"Updates for generation of correct relative path for core headers; After removing globbing in core module, if to fetch a fresh master. some headers were not able to get a valid relative path causing next error:. -- /home/oksana/CERN_sources/root/builds/include/TArrayF.h. CMake Error at cmake/modules/RootNewMacros.cmake:292 (message):. Header path '/home/oksana/CERN_sources/root/builds/include/TArrayF.h'. TArrayF.h is not relative! Call Stack (most recent call first):. core/base/CMakeLists.txt:232 (ROOT_GENERATE_DICTIONARY). Adding extra replacement pattern, matching - $CMAKE_BUILD_DIR/include, fixes issue.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2554
https://github.com/root-project/root/pull/2554:203,performance,error,error,203,"Updates for generation of correct relative path for core headers; After removing globbing in core module, if to fetch a fresh master. some headers were not able to get a valid relative path causing next error:. -- /home/oksana/CERN_sources/root/builds/include/TArrayF.h. CMake Error at cmake/modules/RootNewMacros.cmake:292 (message):. Header path '/home/oksana/CERN_sources/root/builds/include/TArrayF.h'. TArrayF.h is not relative! Call Stack (most recent call first):. core/base/CMakeLists.txt:232 (ROOT_GENERATE_DICTIONARY). Adding extra replacement pattern, matching - $CMAKE_BUILD_DIR/include, fixes issue.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2554
https://github.com/root-project/root/pull/2554:277,performance,Error,Error,277,"Updates for generation of correct relative path for core headers; After removing globbing in core module, if to fetch a fresh master. some headers were not able to get a valid relative path causing next error:. -- /home/oksana/CERN_sources/root/builds/include/TArrayF.h. CMake Error at cmake/modules/RootNewMacros.cmake:292 (message):. Header path '/home/oksana/CERN_sources/root/builds/include/TArrayF.h'. TArrayF.h is not relative! Call Stack (most recent call first):. core/base/CMakeLists.txt:232 (ROOT_GENERATE_DICTIONARY). Adding extra replacement pattern, matching - $CMAKE_BUILD_DIR/include, fixes issue.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2554
https://github.com/root-project/root/pull/2554:0,safety,Updat,Updates,0,"Updates for generation of correct relative path for core headers; After removing globbing in core module, if to fetch a fresh master. some headers were not able to get a valid relative path causing next error:. -- /home/oksana/CERN_sources/root/builds/include/TArrayF.h. CMake Error at cmake/modules/RootNewMacros.cmake:292 (message):. Header path '/home/oksana/CERN_sources/root/builds/include/TArrayF.h'. TArrayF.h is not relative! Call Stack (most recent call first):. core/base/CMakeLists.txt:232 (ROOT_GENERATE_DICTIONARY). Adding extra replacement pattern, matching - $CMAKE_BUILD_DIR/include, fixes issue.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2554
https://github.com/root-project/root/pull/2554:98,safety,modul,module,98,"Updates for generation of correct relative path for core headers; After removing globbing in core module, if to fetch a fresh master. some headers were not able to get a valid relative path causing next error:. -- /home/oksana/CERN_sources/root/builds/include/TArrayF.h. CMake Error at cmake/modules/RootNewMacros.cmake:292 (message):. Header path '/home/oksana/CERN_sources/root/builds/include/TArrayF.h'. TArrayF.h is not relative! Call Stack (most recent call first):. core/base/CMakeLists.txt:232 (ROOT_GENERATE_DICTIONARY). Adding extra replacement pattern, matching - $CMAKE_BUILD_DIR/include, fixes issue.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2554
https://github.com/root-project/root/pull/2554:170,safety,valid,valid,170,"Updates for generation of correct relative path for core headers; After removing globbing in core module, if to fetch a fresh master. some headers were not able to get a valid relative path causing next error:. -- /home/oksana/CERN_sources/root/builds/include/TArrayF.h. CMake Error at cmake/modules/RootNewMacros.cmake:292 (message):. Header path '/home/oksana/CERN_sources/root/builds/include/TArrayF.h'. TArrayF.h is not relative! Call Stack (most recent call first):. core/base/CMakeLists.txt:232 (ROOT_GENERATE_DICTIONARY). Adding extra replacement pattern, matching - $CMAKE_BUILD_DIR/include, fixes issue.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2554
https://github.com/root-project/root/pull/2554:203,safety,error,error,203,"Updates for generation of correct relative path for core headers; After removing globbing in core module, if to fetch a fresh master. some headers were not able to get a valid relative path causing next error:. -- /home/oksana/CERN_sources/root/builds/include/TArrayF.h. CMake Error at cmake/modules/RootNewMacros.cmake:292 (message):. Header path '/home/oksana/CERN_sources/root/builds/include/TArrayF.h'. TArrayF.h is not relative! Call Stack (most recent call first):. core/base/CMakeLists.txt:232 (ROOT_GENERATE_DICTIONARY). Adding extra replacement pattern, matching - $CMAKE_BUILD_DIR/include, fixes issue.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2554
https://github.com/root-project/root/pull/2554:277,safety,Error,Error,277,"Updates for generation of correct relative path for core headers; After removing globbing in core module, if to fetch a fresh master. some headers were not able to get a valid relative path causing next error:. -- /home/oksana/CERN_sources/root/builds/include/TArrayF.h. CMake Error at cmake/modules/RootNewMacros.cmake:292 (message):. Header path '/home/oksana/CERN_sources/root/builds/include/TArrayF.h'. TArrayF.h is not relative! Call Stack (most recent call first):. core/base/CMakeLists.txt:232 (ROOT_GENERATE_DICTIONARY). Adding extra replacement pattern, matching - $CMAKE_BUILD_DIR/include, fixes issue.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2554
https://github.com/root-project/root/pull/2554:292,safety,modul,modules,292,"Updates for generation of correct relative path for core headers; After removing globbing in core module, if to fetch a fresh master. some headers were not able to get a valid relative path causing next error:. -- /home/oksana/CERN_sources/root/builds/include/TArrayF.h. CMake Error at cmake/modules/RootNewMacros.cmake:292 (message):. Header path '/home/oksana/CERN_sources/root/builds/include/TArrayF.h'. TArrayF.h is not relative! Call Stack (most recent call first):. core/base/CMakeLists.txt:232 (ROOT_GENERATE_DICTIONARY). Adding extra replacement pattern, matching - $CMAKE_BUILD_DIR/include, fixes issue.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2554
https://github.com/root-project/root/pull/2554:0,security,Updat,Updates,0,"Updates for generation of correct relative path for core headers; After removing globbing in core module, if to fetch a fresh master. some headers were not able to get a valid relative path causing next error:. -- /home/oksana/CERN_sources/root/builds/include/TArrayF.h. CMake Error at cmake/modules/RootNewMacros.cmake:292 (message):. Header path '/home/oksana/CERN_sources/root/builds/include/TArrayF.h'. TArrayF.h is not relative! Call Stack (most recent call first):. core/base/CMakeLists.txt:232 (ROOT_GENERATE_DICTIONARY). Adding extra replacement pattern, matching - $CMAKE_BUILD_DIR/include, fixes issue.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2554
https://github.com/root-project/root/pull/2554:203,usability,error,error,203,"Updates for generation of correct relative path for core headers; After removing globbing in core module, if to fetch a fresh master. some headers were not able to get a valid relative path causing next error:. -- /home/oksana/CERN_sources/root/builds/include/TArrayF.h. CMake Error at cmake/modules/RootNewMacros.cmake:292 (message):. Header path '/home/oksana/CERN_sources/root/builds/include/TArrayF.h'. TArrayF.h is not relative! Call Stack (most recent call first):. core/base/CMakeLists.txt:232 (ROOT_GENERATE_DICTIONARY). Adding extra replacement pattern, matching - $CMAKE_BUILD_DIR/include, fixes issue.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2554
https://github.com/root-project/root/pull/2554:277,usability,Error,Error,277,"Updates for generation of correct relative path for core headers; After removing globbing in core module, if to fetch a fresh master. some headers were not able to get a valid relative path causing next error:. -- /home/oksana/CERN_sources/root/builds/include/TArrayF.h. CMake Error at cmake/modules/RootNewMacros.cmake:292 (message):. Header path '/home/oksana/CERN_sources/root/builds/include/TArrayF.h'. TArrayF.h is not relative! Call Stack (most recent call first):. core/base/CMakeLists.txt:232 (ROOT_GENERATE_DICTIONARY). Adding extra replacement pattern, matching - $CMAKE_BUILD_DIR/include, fixes issue.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2554
https://github.com/root-project/root/pull/2556:148,safety,input,input,148,"[RDF,Tutorial] Add tutorial processing NanoAOD-like file producing di…; …muon spectrum from Run2011A CMS OpenData. Full documentation regarding the input file can be found here: http://github.com/stwunsch/root-dataframe-nanoaod. The tutorial runs out of the box fetching the file from http://root.cern.ch/files/.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2556
https://github.com/root-project/root/pull/2556:120,usability,document,documentation,120,"[RDF,Tutorial] Add tutorial processing NanoAOD-like file producing di…; …muon spectrum from Run2011A CMS OpenData. Full documentation regarding the input file can be found here: http://github.com/stwunsch/root-dataframe-nanoaod. The tutorial runs out of the box fetching the file from http://root.cern.ch/files/.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2556
https://github.com/root-project/root/pull/2556:148,usability,input,input,148,"[RDF,Tutorial] Add tutorial processing NanoAOD-like file producing di…; …muon spectrum from Run2011A CMS OpenData. Full documentation regarding the input file can be found here: http://github.com/stwunsch/root-dataframe-nanoaod. The tutorial runs out of the box fetching the file from http://root.cern.ch/files/.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2556
https://github.com/root-project/root/pull/2557:71,safety,Valid,Validation,71,Stratified CV Split; Stratified Splitting Functionality Added in Cross Validation in TMVA,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2557
https://github.com/root-project/root/pull/2557:71,security,Validat,Validation,71,Stratified CV Split; Stratified Splitting Functionality Added in Cross Validation in TMVA,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2557
https://github.com/root-project/root/pull/2558:150,availability,error,error,150,[DF] Fix ROOT-9632: do not throw if define return type is unknown to the interpreter; but add a comment to the jitted code in order to obtain a clear error message if. the user tries to use this type in a jitted action/transformation later in the chain.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2558
https://github.com/root-project/root/pull/2558:156,integrability,messag,message,156,[DF] Fix ROOT-9632: do not throw if define return type is unknown to the interpreter; but add a comment to the jitted code in order to obtain a clear error message if. the user tries to use this type in a jitted action/transformation later in the chain.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2558
https://github.com/root-project/root/pull/2558:219,integrability,transform,transformation,219,[DF] Fix ROOT-9632: do not throw if define return type is unknown to the interpreter; but add a comment to the jitted code in order to obtain a clear error message if. the user tries to use this type in a jitted action/transformation later in the chain.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2558
https://github.com/root-project/root/pull/2558:156,interoperability,messag,message,156,[DF] Fix ROOT-9632: do not throw if define return type is unknown to the interpreter; but add a comment to the jitted code in order to obtain a clear error message if. the user tries to use this type in a jitted action/transformation later in the chain.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2558
https://github.com/root-project/root/pull/2558:219,interoperability,transform,transformation,219,[DF] Fix ROOT-9632: do not throw if define return type is unknown to the interpreter; but add a comment to the jitted code in order to obtain a clear error message if. the user tries to use this type in a jitted action/transformation later in the chain.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2558
https://github.com/root-project/root/pull/2558:150,performance,error,error,150,[DF] Fix ROOT-9632: do not throw if define return type is unknown to the interpreter; but add a comment to the jitted code in order to obtain a clear error message if. the user tries to use this type in a jitted action/transformation later in the chain.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2558
https://github.com/root-project/root/pull/2558:150,safety,error,error,150,[DF] Fix ROOT-9632: do not throw if define return type is unknown to the interpreter; but add a comment to the jitted code in order to obtain a clear error message if. the user tries to use this type in a jitted action/transformation later in the chain.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2558
https://github.com/root-project/root/pull/2558:144,usability,clear,clear,144,[DF] Fix ROOT-9632: do not throw if define return type is unknown to the interpreter; but add a comment to the jitted code in order to obtain a clear error message if. the user tries to use this type in a jitted action/transformation later in the chain.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2558
https://github.com/root-project/root/pull/2558:150,usability,error,error,150,[DF] Fix ROOT-9632: do not throw if define return type is unknown to the interpreter; but add a comment to the jitted code in order to obtain a clear error message if. the user tries to use this type in a jitted action/transformation later in the chain.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2558
https://github.com/root-project/root/pull/2558:172,usability,user,user,172,[DF] Fix ROOT-9632: do not throw if define return type is unknown to the interpreter; but add a comment to the jitted code in order to obtain a clear error message if. the user tries to use this type in a jitted action/transformation later in the chain.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2558
https://github.com/root-project/root/pull/2559:232,deployability,modul,modules,232,"[Cxxmodules] Rely LazyFunctionAutoLoad for loading libraries; ""InterestingDecl""s are decls which compiler needs to know its address. We were causing deserialization from HandleInterestingDecl to resolve. the address, but given that modules have smarter way to solve symbols. in LazyFunctionAutoLoad, we can use this instead. This improves modules startup time by 13Mbytes.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2559
https://github.com/root-project/root/pull/2559:339,deployability,modul,modules,339,"[Cxxmodules] Rely LazyFunctionAutoLoad for loading libraries; ""InterestingDecl""s are decls which compiler needs to know its address. We were causing deserialization from HandleInterestingDecl to resolve. the address, but given that modules have smarter way to solve symbols. in LazyFunctionAutoLoad, we can use this instead. This improves modules startup time by 13Mbytes.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2559
https://github.com/root-project/root/pull/2559:43,energy efficiency,load,loading,43,"[Cxxmodules] Rely LazyFunctionAutoLoad for loading libraries; ""InterestingDecl""s are decls which compiler needs to know its address. We were causing deserialization from HandleInterestingDecl to resolve. the address, but given that modules have smarter way to solve symbols. in LazyFunctionAutoLoad, we can use this instead. This improves modules startup time by 13Mbytes.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2559
https://github.com/root-project/root/pull/2559:232,modifiability,modul,modules,232,"[Cxxmodules] Rely LazyFunctionAutoLoad for loading libraries; ""InterestingDecl""s are decls which compiler needs to know its address. We were causing deserialization from HandleInterestingDecl to resolve. the address, but given that modules have smarter way to solve symbols. in LazyFunctionAutoLoad, we can use this instead. This improves modules startup time by 13Mbytes.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2559
https://github.com/root-project/root/pull/2559:339,modifiability,modul,modules,339,"[Cxxmodules] Rely LazyFunctionAutoLoad for loading libraries; ""InterestingDecl""s are decls which compiler needs to know its address. We were causing deserialization from HandleInterestingDecl to resolve. the address, but given that modules have smarter way to solve symbols. in LazyFunctionAutoLoad, we can use this instead. This improves modules startup time by 13Mbytes.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2559
https://github.com/root-project/root/pull/2559:43,performance,load,loading,43,"[Cxxmodules] Rely LazyFunctionAutoLoad for loading libraries; ""InterestingDecl""s are decls which compiler needs to know its address. We were causing deserialization from HandleInterestingDecl to resolve. the address, but given that modules have smarter way to solve symbols. in LazyFunctionAutoLoad, we can use this instead. This improves modules startup time by 13Mbytes.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2559
https://github.com/root-project/root/pull/2559:355,performance,time,time,355,"[Cxxmodules] Rely LazyFunctionAutoLoad for loading libraries; ""InterestingDecl""s are decls which compiler needs to know its address. We were causing deserialization from HandleInterestingDecl to resolve. the address, but given that modules have smarter way to solve symbols. in LazyFunctionAutoLoad, we can use this instead. This improves modules startup time by 13Mbytes.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2559
https://github.com/root-project/root/pull/2559:232,safety,modul,modules,232,"[Cxxmodules] Rely LazyFunctionAutoLoad for loading libraries; ""InterestingDecl""s are decls which compiler needs to know its address. We were causing deserialization from HandleInterestingDecl to resolve. the address, but given that modules have smarter way to solve symbols. in LazyFunctionAutoLoad, we can use this instead. This improves modules startup time by 13Mbytes.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2559
https://github.com/root-project/root/pull/2559:339,safety,modul,modules,339,"[Cxxmodules] Rely LazyFunctionAutoLoad for loading libraries; ""InterestingDecl""s are decls which compiler needs to know its address. We were causing deserialization from HandleInterestingDecl to resolve. the address, but given that modules have smarter way to solve symbols. in LazyFunctionAutoLoad, we can use this instead. This improves modules startup time by 13Mbytes.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2559
https://github.com/root-project/root/pull/2561:447,availability,error,error,447,"Implement --pedantic option; This option gives clean/raw Cling without any includes. This is intended. to be used for debugging purposes. ```. [yuka@yuka-arch module-release]$ bin/root.exe --pedantic. ****************** CLING ******************. * Type C++ code and press enter to run it *. * Type .q to exit *. *******************************************. [cling]$ int a = 1;. [cling]$ a. (int) 1. [cling]$ std::vector<int> b;. input_line_7:2:7: error: no member named 'vector' in namespace 'std'. std::vector<int> b;. ~~~~~^. input_line_7:2:17: error: expected '(' for function-style cast or type construction. std::vector<int> b;. ~~~^. input_line_7:2:19: error: use of undeclared identifier 'b'. std::vector<int> b;. ^. [cling]$ #include <vector>. [cling]$ std::vector<int> b;. [cling]$ b = {1,2,3}. (std::vector &) { 1, 2, 3 }. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2561
https://github.com/root-project/root/pull/2561:547,availability,error,error,547,"Implement --pedantic option; This option gives clean/raw Cling without any includes. This is intended. to be used for debugging purposes. ```. [yuka@yuka-arch module-release]$ bin/root.exe --pedantic. ****************** CLING ******************. * Type C++ code and press enter to run it *. * Type .q to exit *. *******************************************. [cling]$ int a = 1;. [cling]$ a. (int) 1. [cling]$ std::vector<int> b;. input_line_7:2:7: error: no member named 'vector' in namespace 'std'. std::vector<int> b;. ~~~~~^. input_line_7:2:17: error: expected '(' for function-style cast or type construction. std::vector<int> b;. ~~~^. input_line_7:2:19: error: use of undeclared identifier 'b'. std::vector<int> b;. ^. [cling]$ #include <vector>. [cling]$ std::vector<int> b;. [cling]$ b = {1,2,3}. (std::vector &) { 1, 2, 3 }. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2561
https://github.com/root-project/root/pull/2561:659,availability,error,error,659,"Implement --pedantic option; This option gives clean/raw Cling without any includes. This is intended. to be used for debugging purposes. ```. [yuka@yuka-arch module-release]$ bin/root.exe --pedantic. ****************** CLING ******************. * Type C++ code and press enter to run it *. * Type .q to exit *. *******************************************. [cling]$ int a = 1;. [cling]$ a. (int) 1. [cling]$ std::vector<int> b;. input_line_7:2:7: error: no member named 'vector' in namespace 'std'. std::vector<int> b;. ~~~~~^. input_line_7:2:17: error: expected '(' for function-style cast or type construction. std::vector<int> b;. ~~~^. input_line_7:2:19: error: use of undeclared identifier 'b'. std::vector<int> b;. ^. [cling]$ #include <vector>. [cling]$ std::vector<int> b;. [cling]$ b = {1,2,3}. (std::vector &) { 1, 2, 3 }. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2561
https://github.com/root-project/root/pull/2561:159,deployability,modul,module-release,159,"Implement --pedantic option; This option gives clean/raw Cling without any includes. This is intended. to be used for debugging purposes. ```. [yuka@yuka-arch module-release]$ bin/root.exe --pedantic. ****************** CLING ******************. * Type C++ code and press enter to run it *. * Type .q to exit *. *******************************************. [cling]$ int a = 1;. [cling]$ a. (int) 1. [cling]$ std::vector<int> b;. input_line_7:2:7: error: no member named 'vector' in namespace 'std'. std::vector<int> b;. ~~~~~^. input_line_7:2:17: error: expected '(' for function-style cast or type construction. std::vector<int> b;. ~~~^. input_line_7:2:19: error: use of undeclared identifier 'b'. std::vector<int> b;. ^. [cling]$ #include <vector>. [cling]$ std::vector<int> b;. [cling]$ b = {1,2,3}. (std::vector &) { 1, 2, 3 }. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2561
https://github.com/root-project/root/pull/2561:159,modifiability,modul,module-release,159,"Implement --pedantic option; This option gives clean/raw Cling without any includes. This is intended. to be used for debugging purposes. ```. [yuka@yuka-arch module-release]$ bin/root.exe --pedantic. ****************** CLING ******************. * Type C++ code and press enter to run it *. * Type .q to exit *. *******************************************. [cling]$ int a = 1;. [cling]$ a. (int) 1. [cling]$ std::vector<int> b;. input_line_7:2:7: error: no member named 'vector' in namespace 'std'. std::vector<int> b;. ~~~~~^. input_line_7:2:17: error: expected '(' for function-style cast or type construction. std::vector<int> b;. ~~~^. input_line_7:2:19: error: use of undeclared identifier 'b'. std::vector<int> b;. ^. [cling]$ #include <vector>. [cling]$ std::vector<int> b;. [cling]$ b = {1,2,3}. (std::vector &) { 1, 2, 3 }. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2561
https://github.com/root-project/root/pull/2561:447,performance,error,error,447,"Implement --pedantic option; This option gives clean/raw Cling without any includes. This is intended. to be used for debugging purposes. ```. [yuka@yuka-arch module-release]$ bin/root.exe --pedantic. ****************** CLING ******************. * Type C++ code and press enter to run it *. * Type .q to exit *. *******************************************. [cling]$ int a = 1;. [cling]$ a. (int) 1. [cling]$ std::vector<int> b;. input_line_7:2:7: error: no member named 'vector' in namespace 'std'. std::vector<int> b;. ~~~~~^. input_line_7:2:17: error: expected '(' for function-style cast or type construction. std::vector<int> b;. ~~~^. input_line_7:2:19: error: use of undeclared identifier 'b'. std::vector<int> b;. ^. [cling]$ #include <vector>. [cling]$ std::vector<int> b;. [cling]$ b = {1,2,3}. (std::vector &) { 1, 2, 3 }. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2561
https://github.com/root-project/root/pull/2561:547,performance,error,error,547,"Implement --pedantic option; This option gives clean/raw Cling without any includes. This is intended. to be used for debugging purposes. ```. [yuka@yuka-arch module-release]$ bin/root.exe --pedantic. ****************** CLING ******************. * Type C++ code and press enter to run it *. * Type .q to exit *. *******************************************. [cling]$ int a = 1;. [cling]$ a. (int) 1. [cling]$ std::vector<int> b;. input_line_7:2:7: error: no member named 'vector' in namespace 'std'. std::vector<int> b;. ~~~~~^. input_line_7:2:17: error: expected '(' for function-style cast or type construction. std::vector<int> b;. ~~~^. input_line_7:2:19: error: use of undeclared identifier 'b'. std::vector<int> b;. ^. [cling]$ #include <vector>. [cling]$ std::vector<int> b;. [cling]$ b = {1,2,3}. (std::vector &) { 1, 2, 3 }. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2561
https://github.com/root-project/root/pull/2561:659,performance,error,error,659,"Implement --pedantic option; This option gives clean/raw Cling without any includes. This is intended. to be used for debugging purposes. ```. [yuka@yuka-arch module-release]$ bin/root.exe --pedantic. ****************** CLING ******************. * Type C++ code and press enter to run it *. * Type .q to exit *. *******************************************. [cling]$ int a = 1;. [cling]$ a. (int) 1. [cling]$ std::vector<int> b;. input_line_7:2:7: error: no member named 'vector' in namespace 'std'. std::vector<int> b;. ~~~~~^. input_line_7:2:17: error: expected '(' for function-style cast or type construction. std::vector<int> b;. ~~~^. input_line_7:2:19: error: use of undeclared identifier 'b'. std::vector<int> b;. ^. [cling]$ #include <vector>. [cling]$ std::vector<int> b;. [cling]$ b = {1,2,3}. (std::vector &) { 1, 2, 3 }. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2561
https://github.com/root-project/root/pull/2561:159,safety,modul,module-release,159,"Implement --pedantic option; This option gives clean/raw Cling without any includes. This is intended. to be used for debugging purposes. ```. [yuka@yuka-arch module-release]$ bin/root.exe --pedantic. ****************** CLING ******************. * Type C++ code and press enter to run it *. * Type .q to exit *. *******************************************. [cling]$ int a = 1;. [cling]$ a. (int) 1. [cling]$ std::vector<int> b;. input_line_7:2:7: error: no member named 'vector' in namespace 'std'. std::vector<int> b;. ~~~~~^. input_line_7:2:17: error: expected '(' for function-style cast or type construction. std::vector<int> b;. ~~~^. input_line_7:2:19: error: use of undeclared identifier 'b'. std::vector<int> b;. ^. [cling]$ #include <vector>. [cling]$ std::vector<int> b;. [cling]$ b = {1,2,3}. (std::vector &) { 1, 2, 3 }. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2561
https://github.com/root-project/root/pull/2561:447,safety,error,error,447,"Implement --pedantic option; This option gives clean/raw Cling without any includes. This is intended. to be used for debugging purposes. ```. [yuka@yuka-arch module-release]$ bin/root.exe --pedantic. ****************** CLING ******************. * Type C++ code and press enter to run it *. * Type .q to exit *. *******************************************. [cling]$ int a = 1;. [cling]$ a. (int) 1. [cling]$ std::vector<int> b;. input_line_7:2:7: error: no member named 'vector' in namespace 'std'. std::vector<int> b;. ~~~~~^. input_line_7:2:17: error: expected '(' for function-style cast or type construction. std::vector<int> b;. ~~~^. input_line_7:2:19: error: use of undeclared identifier 'b'. std::vector<int> b;. ^. [cling]$ #include <vector>. [cling]$ std::vector<int> b;. [cling]$ b = {1,2,3}. (std::vector &) { 1, 2, 3 }. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2561
https://github.com/root-project/root/pull/2561:547,safety,error,error,547,"Implement --pedantic option; This option gives clean/raw Cling without any includes. This is intended. to be used for debugging purposes. ```. [yuka@yuka-arch module-release]$ bin/root.exe --pedantic. ****************** CLING ******************. * Type C++ code and press enter to run it *. * Type .q to exit *. *******************************************. [cling]$ int a = 1;. [cling]$ a. (int) 1. [cling]$ std::vector<int> b;. input_line_7:2:7: error: no member named 'vector' in namespace 'std'. std::vector<int> b;. ~~~~~^. input_line_7:2:17: error: expected '(' for function-style cast or type construction. std::vector<int> b;. ~~~^. input_line_7:2:19: error: use of undeclared identifier 'b'. std::vector<int> b;. ^. [cling]$ #include <vector>. [cling]$ std::vector<int> b;. [cling]$ b = {1,2,3}. (std::vector &) { 1, 2, 3 }. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2561
https://github.com/root-project/root/pull/2561:659,safety,error,error,659,"Implement --pedantic option; This option gives clean/raw Cling without any includes. This is intended. to be used for debugging purposes. ```. [yuka@yuka-arch module-release]$ bin/root.exe --pedantic. ****************** CLING ******************. * Type C++ code and press enter to run it *. * Type .q to exit *. *******************************************. [cling]$ int a = 1;. [cling]$ a. (int) 1. [cling]$ std::vector<int> b;. input_line_7:2:7: error: no member named 'vector' in namespace 'std'. std::vector<int> b;. ~~~~~^. input_line_7:2:17: error: expected '(' for function-style cast or type construction. std::vector<int> b;. ~~~^. input_line_7:2:19: error: use of undeclared identifier 'b'. std::vector<int> b;. ^. [cling]$ #include <vector>. [cling]$ std::vector<int> b;. [cling]$ b = {1,2,3}. (std::vector &) { 1, 2, 3 }. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2561
https://github.com/root-project/root/pull/2561:684,security,ident,identifier,684,"Implement --pedantic option; This option gives clean/raw Cling without any includes. This is intended. to be used for debugging purposes. ```. [yuka@yuka-arch module-release]$ bin/root.exe --pedantic. ****************** CLING ******************. * Type C++ code and press enter to run it *. * Type .q to exit *. *******************************************. [cling]$ int a = 1;. [cling]$ a. (int) 1. [cling]$ std::vector<int> b;. input_line_7:2:7: error: no member named 'vector' in namespace 'std'. std::vector<int> b;. ~~~~~^. input_line_7:2:17: error: expected '(' for function-style cast or type construction. std::vector<int> b;. ~~~^. input_line_7:2:19: error: use of undeclared identifier 'b'. std::vector<int> b;. ^. [cling]$ #include <vector>. [cling]$ std::vector<int> b;. [cling]$ b = {1,2,3}. (std::vector &) { 1, 2, 3 }. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2561
https://github.com/root-project/root/pull/2561:447,usability,error,error,447,"Implement --pedantic option; This option gives clean/raw Cling without any includes. This is intended. to be used for debugging purposes. ```. [yuka@yuka-arch module-release]$ bin/root.exe --pedantic. ****************** CLING ******************. * Type C++ code and press enter to run it *. * Type .q to exit *. *******************************************. [cling]$ int a = 1;. [cling]$ a. (int) 1. [cling]$ std::vector<int> b;. input_line_7:2:7: error: no member named 'vector' in namespace 'std'. std::vector<int> b;. ~~~~~^. input_line_7:2:17: error: expected '(' for function-style cast or type construction. std::vector<int> b;. ~~~^. input_line_7:2:19: error: use of undeclared identifier 'b'. std::vector<int> b;. ^. [cling]$ #include <vector>. [cling]$ std::vector<int> b;. [cling]$ b = {1,2,3}. (std::vector &) { 1, 2, 3 }. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2561
https://github.com/root-project/root/pull/2561:547,usability,error,error,547,"Implement --pedantic option; This option gives clean/raw Cling without any includes. This is intended. to be used for debugging purposes. ```. [yuka@yuka-arch module-release]$ bin/root.exe --pedantic. ****************** CLING ******************. * Type C++ code and press enter to run it *. * Type .q to exit *. *******************************************. [cling]$ int a = 1;. [cling]$ a. (int) 1. [cling]$ std::vector<int> b;. input_line_7:2:7: error: no member named 'vector' in namespace 'std'. std::vector<int> b;. ~~~~~^. input_line_7:2:17: error: expected '(' for function-style cast or type construction. std::vector<int> b;. ~~~^. input_line_7:2:19: error: use of undeclared identifier 'b'. std::vector<int> b;. ^. [cling]$ #include <vector>. [cling]$ std::vector<int> b;. [cling]$ b = {1,2,3}. (std::vector &) { 1, 2, 3 }. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2561
https://github.com/root-project/root/pull/2561:659,usability,error,error,659,"Implement --pedantic option; This option gives clean/raw Cling without any includes. This is intended. to be used for debugging purposes. ```. [yuka@yuka-arch module-release]$ bin/root.exe --pedantic. ****************** CLING ******************. * Type C++ code and press enter to run it *. * Type .q to exit *. *******************************************. [cling]$ int a = 1;. [cling]$ a. (int) 1. [cling]$ std::vector<int> b;. input_line_7:2:7: error: no member named 'vector' in namespace 'std'. std::vector<int> b;. ~~~~~^. input_line_7:2:17: error: expected '(' for function-style cast or type construction. std::vector<int> b;. ~~~^. input_line_7:2:19: error: use of undeclared identifier 'b'. std::vector<int> b;. ^. [cling]$ #include <vector>. [cling]$ std::vector<int> b;. [cling]$ b = {1,2,3}. (std::vector &) { 1, 2, 3 }. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2561
https://github.com/root-project/root/pull/2562:165,availability,error,errors,165,"[RDF,Tutorial] Fetch file for NanoAOD tutorial from root.cern.ch/files; Fixes [these](http://cdash.cern.ch/testDetails.php?test=51845718&build=557493) kind of build errors due to missing file. @Axel-Naumann Are the tutorial as well run in the CI/ nightlies? Since the file has 1.5GB, this could create some traffic to http://root.cern.ch/.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2562
https://github.com/root-project/root/pull/2562:137,deployability,build,build,137,"[RDF,Tutorial] Fetch file for NanoAOD tutorial from root.cern.ch/files; Fixes [these](http://cdash.cern.ch/testDetails.php?test=51845718&build=557493) kind of build errors due to missing file. @Axel-Naumann Are the tutorial as well run in the CI/ nightlies? Since the file has 1.5GB, this could create some traffic to http://root.cern.ch/.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2562
https://github.com/root-project/root/pull/2562:159,deployability,build,build,159,"[RDF,Tutorial] Fetch file for NanoAOD tutorial from root.cern.ch/files; Fixes [these](http://cdash.cern.ch/testDetails.php?test=51845718&build=557493) kind of build errors due to missing file. @Axel-Naumann Are the tutorial as well run in the CI/ nightlies? Since the file has 1.5GB, this could create some traffic to http://root.cern.ch/.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2562
https://github.com/root-project/root/pull/2562:165,performance,error,errors,165,"[RDF,Tutorial] Fetch file for NanoAOD tutorial from root.cern.ch/files; Fixes [these](http://cdash.cern.ch/testDetails.php?test=51845718&build=557493) kind of build errors due to missing file. @Axel-Naumann Are the tutorial as well run in the CI/ nightlies? Since the file has 1.5GB, this could create some traffic to http://root.cern.ch/.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2562
https://github.com/root-project/root/pull/2562:107,safety,test,testDetails,107,"[RDF,Tutorial] Fetch file for NanoAOD tutorial from root.cern.ch/files; Fixes [these](http://cdash.cern.ch/testDetails.php?test=51845718&build=557493) kind of build errors due to missing file. @Axel-Naumann Are the tutorial as well run in the CI/ nightlies? Since the file has 1.5GB, this could create some traffic to http://root.cern.ch/.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2562
https://github.com/root-project/root/pull/2562:123,safety,test,test,123,"[RDF,Tutorial] Fetch file for NanoAOD tutorial from root.cern.ch/files; Fixes [these](http://cdash.cern.ch/testDetails.php?test=51845718&build=557493) kind of build errors due to missing file. @Axel-Naumann Are the tutorial as well run in the CI/ nightlies? Since the file has 1.5GB, this could create some traffic to http://root.cern.ch/.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2562
https://github.com/root-project/root/pull/2562:165,safety,error,errors,165,"[RDF,Tutorial] Fetch file for NanoAOD tutorial from root.cern.ch/files; Fixes [these](http://cdash.cern.ch/testDetails.php?test=51845718&build=557493) kind of build errors due to missing file. @Axel-Naumann Are the tutorial as well run in the CI/ nightlies? Since the file has 1.5GB, this could create some traffic to http://root.cern.ch/.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2562
https://github.com/root-project/root/pull/2562:107,testability,test,testDetails,107,"[RDF,Tutorial] Fetch file for NanoAOD tutorial from root.cern.ch/files; Fixes [these](http://cdash.cern.ch/testDetails.php?test=51845718&build=557493) kind of build errors due to missing file. @Axel-Naumann Are the tutorial as well run in the CI/ nightlies? Since the file has 1.5GB, this could create some traffic to http://root.cern.ch/.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2562
https://github.com/root-project/root/pull/2562:123,testability,test,test,123,"[RDF,Tutorial] Fetch file for NanoAOD tutorial from root.cern.ch/files; Fixes [these](http://cdash.cern.ch/testDetails.php?test=51845718&build=557493) kind of build errors due to missing file. @Axel-Naumann Are the tutorial as well run in the CI/ nightlies? Since the file has 1.5GB, this could create some traffic to http://root.cern.ch/.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2562
https://github.com/root-project/root/pull/2562:165,usability,error,errors,165,"[RDF,Tutorial] Fetch file for NanoAOD tutorial from root.cern.ch/files; Fixes [these](http://cdash.cern.ch/testDetails.php?test=51845718&build=557493) kind of build errors due to missing file. @Axel-Naumann Are the tutorial as well run in the CI/ nightlies? Since the file has 1.5GB, this could create some traffic to http://root.cern.ch/.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2562
https://github.com/root-project/root/pull/2563:121,availability,error,error-cling-loadlibrary-while-compiling-in-root-,121,"Empty the ClassImp definition.; These days, ClassImp() causes more harm than good. See e.g. https://root-forum.cern.ch/t/error-cling-loadlibrary-while-compiling-in-root-6-08/30340/5",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2563
https://github.com/root-project/root/pull/2563:133,energy efficiency,load,loadlibrary-while-compiling-in-root-,133,"Empty the ClassImp definition.; These days, ClassImp() causes more harm than good. See e.g. https://root-forum.cern.ch/t/error-cling-loadlibrary-while-compiling-in-root-6-08/30340/5",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2563
https://github.com/root-project/root/pull/2563:121,performance,error,error-cling-loadlibrary-while-compiling-in-root-,121,"Empty the ClassImp definition.; These days, ClassImp() causes more harm than good. See e.g. https://root-forum.cern.ch/t/error-cling-loadlibrary-while-compiling-in-root-6-08/30340/5",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2563
https://github.com/root-project/root/pull/2563:121,safety,error,error-cling-loadlibrary-while-compiling-in-root-,121,"Empty the ClassImp definition.; These days, ClassImp() causes more harm than good. See e.g. https://root-forum.cern.ch/t/error-cling-loadlibrary-while-compiling-in-root-6-08/30340/5",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2563
https://github.com/root-project/root/pull/2563:121,usability,error,error-cling-loadlibrary-while-compiling-in-root-,121,"Empty the ClassImp definition.; These days, ClassImp() causes more harm than good. See e.g. https://root-forum.cern.ch/t/error-cling-loadlibrary-while-compiling-in-root-6-08/30340/5",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2563
https://github.com/root-project/root/pull/2564:13,deployability,version,version,13,"Added root --version feature; **Changes:** . Added feature to get root version using (root --version) command. **Screenshot:** . <img width=""357"" alt=""root version"" src=""https://user-images.githubusercontent.com/25840461/44907404-40fb1680-ad31-11e8-8d62-06a862496d65.png"">.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2564
https://github.com/root-project/root/pull/2564:71,deployability,version,version,71,"Added root --version feature; **Changes:** . Added feature to get root version using (root --version) command. **Screenshot:** . <img width=""357"" alt=""root version"" src=""https://user-images.githubusercontent.com/25840461/44907404-40fb1680-ad31-11e8-8d62-06a862496d65.png"">.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2564
https://github.com/root-project/root/pull/2564:93,deployability,version,version,93,"Added root --version feature; **Changes:** . Added feature to get root version using (root --version) command. **Screenshot:** . <img width=""357"" alt=""root version"" src=""https://user-images.githubusercontent.com/25840461/44907404-40fb1680-ad31-11e8-8d62-06a862496d65.png"">.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2564
https://github.com/root-project/root/pull/2564:156,deployability,version,version,156,"Added root --version feature; **Changes:** . Added feature to get root version using (root --version) command. **Screenshot:** . <img width=""357"" alt=""root version"" src=""https://user-images.githubusercontent.com/25840461/44907404-40fb1680-ad31-11e8-8d62-06a862496d65.png"">.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2564
https://github.com/root-project/root/pull/2564:13,integrability,version,version,13,"Added root --version feature; **Changes:** . Added feature to get root version using (root --version) command. **Screenshot:** . <img width=""357"" alt=""root version"" src=""https://user-images.githubusercontent.com/25840461/44907404-40fb1680-ad31-11e8-8d62-06a862496d65.png"">.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2564
https://github.com/root-project/root/pull/2564:71,integrability,version,version,71,"Added root --version feature; **Changes:** . Added feature to get root version using (root --version) command. **Screenshot:** . <img width=""357"" alt=""root version"" src=""https://user-images.githubusercontent.com/25840461/44907404-40fb1680-ad31-11e8-8d62-06a862496d65.png"">.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2564
https://github.com/root-project/root/pull/2564:93,integrability,version,version,93,"Added root --version feature; **Changes:** . Added feature to get root version using (root --version) command. **Screenshot:** . <img width=""357"" alt=""root version"" src=""https://user-images.githubusercontent.com/25840461/44907404-40fb1680-ad31-11e8-8d62-06a862496d65.png"">.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2564
https://github.com/root-project/root/pull/2564:156,integrability,version,version,156,"Added root --version feature; **Changes:** . Added feature to get root version using (root --version) command. **Screenshot:** . <img width=""357"" alt=""root version"" src=""https://user-images.githubusercontent.com/25840461/44907404-40fb1680-ad31-11e8-8d62-06a862496d65.png"">.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2564
https://github.com/root-project/root/pull/2564:13,modifiability,version,version,13,"Added root --version feature; **Changes:** . Added feature to get root version using (root --version) command. **Screenshot:** . <img width=""357"" alt=""root version"" src=""https://user-images.githubusercontent.com/25840461/44907404-40fb1680-ad31-11e8-8d62-06a862496d65.png"">.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2564
https://github.com/root-project/root/pull/2564:71,modifiability,version,version,71,"Added root --version feature; **Changes:** . Added feature to get root version using (root --version) command. **Screenshot:** . <img width=""357"" alt=""root version"" src=""https://user-images.githubusercontent.com/25840461/44907404-40fb1680-ad31-11e8-8d62-06a862496d65.png"">.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2564
https://github.com/root-project/root/pull/2564:93,modifiability,version,version,93,"Added root --version feature; **Changes:** . Added feature to get root version using (root --version) command. **Screenshot:** . <img width=""357"" alt=""root version"" src=""https://user-images.githubusercontent.com/25840461/44907404-40fb1680-ad31-11e8-8d62-06a862496d65.png"">.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2564
https://github.com/root-project/root/pull/2564:156,modifiability,version,version,156,"Added root --version feature; **Changes:** . Added feature to get root version using (root --version) command. **Screenshot:** . <img width=""357"" alt=""root version"" src=""https://user-images.githubusercontent.com/25840461/44907404-40fb1680-ad31-11e8-8d62-06a862496d65.png"">.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2564
https://github.com/root-project/root/pull/2564:102,usability,command,command,102,"Added root --version feature; **Changes:** . Added feature to get root version using (root --version) command. **Screenshot:** . <img width=""357"" alt=""root version"" src=""https://user-images.githubusercontent.com/25840461/44907404-40fb1680-ad31-11e8-8d62-06a862496d65.png"">.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2564
https://github.com/root-project/root/pull/2564:178,usability,user,user-images,178,"Added root --version feature; **Changes:** . Added feature to get root version using (root --version) command. **Screenshot:** . <img width=""357"" alt=""root version"" src=""https://user-images.githubusercontent.com/25840461/44907404-40fb1680-ad31-11e8-8d62-06a862496d65.png"">.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2564
https://github.com/root-project/root/pull/2565:28,energy efficiency,Core,Core,28,Temporary revert globing of Core; We need additionally investigate the wrong behaviour of ROOT_GENERATE_DICTIONARY for Core in case of when ROOTSYS was already setup before.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2565
https://github.com/root-project/root/pull/2565:119,energy efficiency,Core,Core,119,Temporary revert globing of Core; We need additionally investigate the wrong behaviour of ROOT_GENERATE_DICTIONARY for Core in case of when ROOTSYS was already setup before.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2565
https://github.com/root-project/root/pull/2565:77,usability,behavi,behaviour,77,Temporary revert globing of Core; We need additionally investigate the wrong behaviour of ROOT_GENERATE_DICTIONARY for Core in case of when ROOTSYS was already setup before.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2565
https://github.com/root-project/root/pull/2566:281,availability,avail,available,281,"Add possibility to specify number of threads in PyKeras and set specific Tensorflow session config options; Add a new option in PyKeras to select the number of running threads when running keras with tensorflow. If nothing is specified, tensor flow normally runs by default on all available cores",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2566
https://github.com/root-project/root/pull/2566:291,energy efficiency,core,cores,291,"Add possibility to specify number of threads in PyKeras and set specific Tensorflow session config options; Add a new option in PyKeras to select the number of running threads when running keras with tensorflow. If nothing is specified, tensor flow normally runs by default on all available cores",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2566
https://github.com/root-project/root/pull/2566:19,interoperability,specif,specify,19,"Add possibility to specify number of threads in PyKeras and set specific Tensorflow session config options; Add a new option in PyKeras to select the number of running threads when running keras with tensorflow. If nothing is specified, tensor flow normally runs by default on all available cores",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2566
https://github.com/root-project/root/pull/2566:64,interoperability,specif,specific,64,"Add possibility to specify number of threads in PyKeras and set specific Tensorflow session config options; Add a new option in PyKeras to select the number of running threads when running keras with tensorflow. If nothing is specified, tensor flow normally runs by default on all available cores",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2566
https://github.com/root-project/root/pull/2566:226,interoperability,specif,specified,226,"Add possibility to specify number of threads in PyKeras and set specific Tensorflow session config options; Add a new option in PyKeras to select the number of running threads when running keras with tensorflow. If nothing is specified, tensor flow normally runs by default on all available cores",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2566
https://github.com/root-project/root/pull/2566:281,reliability,availab,available,281,"Add possibility to specify number of threads in PyKeras and set specific Tensorflow session config options; Add a new option in PyKeras to select the number of running threads when running keras with tensorflow. If nothing is specified, tensor flow normally runs by default on all available cores",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2566
https://github.com/root-project/root/pull/2566:281,safety,avail,available,281,"Add possibility to specify number of threads in PyKeras and set specific Tensorflow session config options; Add a new option in PyKeras to select the number of running threads when running keras with tensorflow. If nothing is specified, tensor flow normally runs by default on all available cores",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2566
https://github.com/root-project/root/pull/2566:84,security,session,session,84,"Add possibility to specify number of threads in PyKeras and set specific Tensorflow session config options; Add a new option in PyKeras to select the number of running threads when running keras with tensorflow. If nothing is specified, tensor flow normally runs by default on all available cores",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2566
https://github.com/root-project/root/pull/2566:281,security,availab,available,281,"Add possibility to specify number of threads in PyKeras and set specific Tensorflow session config options; Add a new option in PyKeras to select the number of running threads when running keras with tensorflow. If nothing is specified, tensor flow normally runs by default on all available cores",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2566
https://github.com/root-project/root/pull/2567:79,availability,avail,available,79,Use in the Deep Learning the Fast implementation of Tanh from VDT; When VDT is available use for the CPU implementation the fast implementation of tanh,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2567
https://github.com/root-project/root/pull/2567:101,energy efficiency,CPU,CPU,101,Use in the Deep Learning the Fast implementation of Tanh from VDT; When VDT is available use for the CPU implementation the fast implementation of tanh,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2567
https://github.com/root-project/root/pull/2567:101,performance,CPU,CPU,101,Use in the Deep Learning the Fast implementation of Tanh from VDT; When VDT is available use for the CPU implementation the fast implementation of tanh,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2567
https://github.com/root-project/root/pull/2567:79,reliability,availab,available,79,Use in the Deep Learning the Fast implementation of Tanh from VDT; When VDT is available use for the CPU implementation the fast implementation of tanh,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2567
https://github.com/root-project/root/pull/2567:79,safety,avail,available,79,Use in the Deep Learning the Fast implementation of Tanh from VDT; When VDT is available use for the CPU implementation the fast implementation of tanh,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2567
https://github.com/root-project/root/pull/2567:79,security,availab,available,79,Use in the Deep Learning the Fast implementation of Tanh from VDT; When VDT is available use for the CPU implementation the fast implementation of tanh,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2567
https://github.com/root-project/root/pull/2567:16,usability,Learn,Learning,16,Use in the Deep Learning the Fast implementation of Tanh from VDT; When VDT is available use for the CPU implementation the fast implementation of tanh,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2567
https://github.com/root-project/root/pull/2570:108,availability,avail,available,108,ROOT-9595: Replace XrdSecEntity::Reset() with assignment to default object; `XrdSecEntity::Reset()` is only available in newer versions of XRootD. Fixes [ROOT-9595](https://sft.its.cern.ch/jira/browse/ROOT-9595).,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2570
https://github.com/root-project/root/pull/2570:127,deployability,version,versions,127,ROOT-9595: Replace XrdSecEntity::Reset() with assignment to default object; `XrdSecEntity::Reset()` is only available in newer versions of XRootD. Fixes [ROOT-9595](https://sft.its.cern.ch/jira/browse/ROOT-9595).,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2570
https://github.com/root-project/root/pull/2570:127,integrability,version,versions,127,ROOT-9595: Replace XrdSecEntity::Reset() with assignment to default object; `XrdSecEntity::Reset()` is only available in newer versions of XRootD. Fixes [ROOT-9595](https://sft.its.cern.ch/jira/browse/ROOT-9595).,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2570
https://github.com/root-project/root/pull/2570:127,modifiability,version,versions,127,ROOT-9595: Replace XrdSecEntity::Reset() with assignment to default object; `XrdSecEntity::Reset()` is only available in newer versions of XRootD. Fixes [ROOT-9595](https://sft.its.cern.ch/jira/browse/ROOT-9595).,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2570
https://github.com/root-project/root/pull/2570:108,reliability,availab,available,108,ROOT-9595: Replace XrdSecEntity::Reset() with assignment to default object; `XrdSecEntity::Reset()` is only available in newer versions of XRootD. Fixes [ROOT-9595](https://sft.its.cern.ch/jira/browse/ROOT-9595).,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2570
https://github.com/root-project/root/pull/2570:108,safety,avail,available,108,ROOT-9595: Replace XrdSecEntity::Reset() with assignment to default object; `XrdSecEntity::Reset()` is only available in newer versions of XRootD. Fixes [ROOT-9595](https://sft.its.cern.ch/jira/browse/ROOT-9595).,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2570
https://github.com/root-project/root/pull/2570:108,security,availab,available,108,ROOT-9595: Replace XrdSecEntity::Reset() with assignment to default object; `XrdSecEntity::Reset()` is only available in newer versions of XRootD. Fixes [ROOT-9595](https://sft.its.cern.ch/jira/browse/ROOT-9595).,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2570
https://github.com/root-project/root/pull/2573:102,energy efficiency,draw,drawing,102,"[RDF,Tutorial] Restructure code of NanoAOD tutorial for nicer noteboo…; …k representation. Issue with drawing canvas in notebooks while having `ROOT::EnableImplicitMT()` enabled is tracked [here](https://sft.its.cern.ch/jira/browse/ROOT-9659).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2573
https://github.com/root-project/root/pull/2574:142,availability,error,error-while-importing-gdml-file-in-,142,Avoid surrounding functions with brackets in GDML expressions.; Fix for problem reported on the [forum](https://root-forum.cern.ch/t/tformula-error-while-importing-gdml-file-in-root6/30345),MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2574
https://github.com/root-project/root/pull/2574:142,performance,error,error-while-importing-gdml-file-in-,142,Avoid surrounding functions with brackets in GDML expressions.; Fix for problem reported on the [forum](https://root-forum.cern.ch/t/tformula-error-while-importing-gdml-file-in-root6/30345),MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2574
https://github.com/root-project/root/pull/2574:0,safety,Avoid,Avoid,0,Avoid surrounding functions with brackets in GDML expressions.; Fix for problem reported on the [forum](https://root-forum.cern.ch/t/tformula-error-while-importing-gdml-file-in-root6/30345),MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2574
https://github.com/root-project/root/pull/2574:142,safety,error,error-while-importing-gdml-file-in-,142,Avoid surrounding functions with brackets in GDML expressions.; Fix for problem reported on the [forum](https://root-forum.cern.ch/t/tformula-error-while-importing-gdml-file-in-root6/30345),MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2574
https://github.com/root-project/root/pull/2574:142,usability,error,error-while-importing-gdml-file-in-,142,Avoid surrounding functions with brackets in GDML expressions.; Fix for problem reported on the [forum](https://root-forum.cern.ch/t/tformula-error-while-importing-gdml-file-in-root6/30345),MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2574
https://github.com/root-project/root/pull/2575:24,security,access,accessing,24,[RooFit] Fix ROOT-6378: accessing the binning of default ctr'd RooRealVar objects leads to crashes;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2575
https://github.com/root-project/root/pull/2576:627,deployability,Build,Build,627,"[RooFit] Add cling value printing for RooAbsArg; therewith for tens of widely used classes such as variables, constants,. distributions and combinations thereof. For example:. ```. root [1] // Create two Gaussian PDFs g1(x,mean1,sigma) anf g2(x,mean2,sigma) and their parameters. root [2] RooRealVar mean(""mean"",""mean of gaussians"",5,0,10) ;. root [3] RooRealVar sigma1(""sigma1"",""width of gaussians"",0.5) ;. root [4] RooRealVar sigma2(""sigma2"",""width of gaussians"",1) ;. root [5] RooGaussian sig1(""sig1"",""Signal component 1"",x,mean,sigma1) ;. root [6] RooGaussian sig2(""sig2"",""Signal component 2"",x,mean,sigma2) ;. root [7] // Build Chebychev polynomial p.d.f. root [8] RooRealVar a0(""a0"",""a0"",0.5,0.,1.) ;. root [9] RooRealVar a1(""a1"",""a1"",0.2,0.,1.) ;. root [10] RooChebychev bkg(""bkg"",""Background"",x,RooArgSet(a0,a1)) ;. root [11] // Sum the signal components into a composite signal p.d.f. root [12] RooRealVar sig1frac(""sig1frac"",""fraction of component 1 in signal"",0.8,0.,1.) ;. root [13] RooAddPdf sig(""sig"",""Signal"",RooArgList(sig1,sig2),sig1frac) ;. root [14] // Sum the composite signal and background. root [15] RooRealVar bkgfrac(""bkgfrac"",""fraction of background"",0.5,0.,1.) ;. root [16] RooAddPdf model(""model"",""g1+g2+a"",RooArgList(bkg,sig),bkgfrac) ;. root [17] model. (RooAddPdf &) RooAddPdf::model[ bkgfrac * bkg + [%] * sig ] = 0.9. root [18] sig. (RooAddPdf &) RooAddPdf::sig[ sig1frac * sig1 + [%] * sig2 ] = 1. root [19] sig1. (RooGaussian &) RooGaussian::sig1[ x=x mean=mean sigma=sigma1 ] = 1. root [20] sigma1. (RooRealVar &) RooRealVar::sigma1 = 0.5 C L(-INF - +INF) . root [21] mean. (RooRealVar &) RooRealVar::mean = 5 L(0 - 10) . ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2576
https://github.com/root-project/root/pull/2576:870,deployability,compos,composite,870,"[RooFit] Add cling value printing for RooAbsArg; therewith for tens of widely used classes such as variables, constants,. distributions and combinations thereof. For example:. ```. root [1] // Create two Gaussian PDFs g1(x,mean1,sigma) anf g2(x,mean2,sigma) and their parameters. root [2] RooRealVar mean(""mean"",""mean of gaussians"",5,0,10) ;. root [3] RooRealVar sigma1(""sigma1"",""width of gaussians"",0.5) ;. root [4] RooRealVar sigma2(""sigma2"",""width of gaussians"",1) ;. root [5] RooGaussian sig1(""sig1"",""Signal component 1"",x,mean,sigma1) ;. root [6] RooGaussian sig2(""sig2"",""Signal component 2"",x,mean,sigma2) ;. root [7] // Build Chebychev polynomial p.d.f. root [8] RooRealVar a0(""a0"",""a0"",0.5,0.,1.) ;. root [9] RooRealVar a1(""a1"",""a1"",0.2,0.,1.) ;. root [10] RooChebychev bkg(""bkg"",""Background"",x,RooArgSet(a0,a1)) ;. root [11] // Sum the signal components into a composite signal p.d.f. root [12] RooRealVar sig1frac(""sig1frac"",""fraction of component 1 in signal"",0.8,0.,1.) ;. root [13] RooAddPdf sig(""sig"",""Signal"",RooArgList(sig1,sig2),sig1frac) ;. root [14] // Sum the composite signal and background. root [15] RooRealVar bkgfrac(""bkgfrac"",""fraction of background"",0.5,0.,1.) ;. root [16] RooAddPdf model(""model"",""g1+g2+a"",RooArgList(bkg,sig),bkgfrac) ;. root [17] model. (RooAddPdf &) RooAddPdf::model[ bkgfrac * bkg + [%] * sig ] = 0.9. root [18] sig. (RooAddPdf &) RooAddPdf::sig[ sig1frac * sig1 + [%] * sig2 ] = 1. root [19] sig1. (RooGaussian &) RooGaussian::sig1[ x=x mean=mean sigma=sigma1 ] = 1. root [20] sigma1. (RooRealVar &) RooRealVar::sigma1 = 0.5 C L(-INF - +INF) . root [21] mean. (RooRealVar &) RooRealVar::mean = 5 L(0 - 10) . ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2576
https://github.com/root-project/root/pull/2576:1080,deployability,compos,composite,1080,"[RooFit] Add cling value printing for RooAbsArg; therewith for tens of widely used classes such as variables, constants,. distributions and combinations thereof. For example:. ```. root [1] // Create two Gaussian PDFs g1(x,mean1,sigma) anf g2(x,mean2,sigma) and their parameters. root [2] RooRealVar mean(""mean"",""mean of gaussians"",5,0,10) ;. root [3] RooRealVar sigma1(""sigma1"",""width of gaussians"",0.5) ;. root [4] RooRealVar sigma2(""sigma2"",""width of gaussians"",1) ;. root [5] RooGaussian sig1(""sig1"",""Signal component 1"",x,mean,sigma1) ;. root [6] RooGaussian sig2(""sig2"",""Signal component 2"",x,mean,sigma2) ;. root [7] // Build Chebychev polynomial p.d.f. root [8] RooRealVar a0(""a0"",""a0"",0.5,0.,1.) ;. root [9] RooRealVar a1(""a1"",""a1"",0.2,0.,1.) ;. root [10] RooChebychev bkg(""bkg"",""Background"",x,RooArgSet(a0,a1)) ;. root [11] // Sum the signal components into a composite signal p.d.f. root [12] RooRealVar sig1frac(""sig1frac"",""fraction of component 1 in signal"",0.8,0.,1.) ;. root [13] RooAddPdf sig(""sig"",""Signal"",RooArgList(sig1,sig2),sig1frac) ;. root [14] // Sum the composite signal and background. root [15] RooRealVar bkgfrac(""bkgfrac"",""fraction of background"",0.5,0.,1.) ;. root [16] RooAddPdf model(""model"",""g1+g2+a"",RooArgList(bkg,sig),bkgfrac) ;. root [17] model. (RooAddPdf &) RooAddPdf::model[ bkgfrac * bkg + [%] * sig ] = 0.9. root [18] sig. (RooAddPdf &) RooAddPdf::sig[ sig1frac * sig1 + [%] * sig2 ] = 1. root [19] sig1. (RooGaussian &) RooGaussian::sig1[ x=x mean=mean sigma=sigma1 ] = 1. root [20] sigma1. (RooRealVar &) RooRealVar::sigma1 = 0.5 C L(-INF - +INF) . root [21] mean. (RooRealVar &) RooRealVar::mean = 5 L(0 - 10) . ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2576
https://github.com/root-project/root/pull/2576:1211,energy efficiency,model,model,1211,"[RooFit] Add cling value printing for RooAbsArg; therewith for tens of widely used classes such as variables, constants,. distributions and combinations thereof. For example:. ```. root [1] // Create two Gaussian PDFs g1(x,mean1,sigma) anf g2(x,mean2,sigma) and their parameters. root [2] RooRealVar mean(""mean"",""mean of gaussians"",5,0,10) ;. root [3] RooRealVar sigma1(""sigma1"",""width of gaussians"",0.5) ;. root [4] RooRealVar sigma2(""sigma2"",""width of gaussians"",1) ;. root [5] RooGaussian sig1(""sig1"",""Signal component 1"",x,mean,sigma1) ;. root [6] RooGaussian sig2(""sig2"",""Signal component 2"",x,mean,sigma2) ;. root [7] // Build Chebychev polynomial p.d.f. root [8] RooRealVar a0(""a0"",""a0"",0.5,0.,1.) ;. root [9] RooRealVar a1(""a1"",""a1"",0.2,0.,1.) ;. root [10] RooChebychev bkg(""bkg"",""Background"",x,RooArgSet(a0,a1)) ;. root [11] // Sum the signal components into a composite signal p.d.f. root [12] RooRealVar sig1frac(""sig1frac"",""fraction of component 1 in signal"",0.8,0.,1.) ;. root [13] RooAddPdf sig(""sig"",""Signal"",RooArgList(sig1,sig2),sig1frac) ;. root [14] // Sum the composite signal and background. root [15] RooRealVar bkgfrac(""bkgfrac"",""fraction of background"",0.5,0.,1.) ;. root [16] RooAddPdf model(""model"",""g1+g2+a"",RooArgList(bkg,sig),bkgfrac) ;. root [17] model. (RooAddPdf &) RooAddPdf::model[ bkgfrac * bkg + [%] * sig ] = 0.9. root [18] sig. (RooAddPdf &) RooAddPdf::sig[ sig1frac * sig1 + [%] * sig2 ] = 1. root [19] sig1. (RooGaussian &) RooGaussian::sig1[ x=x mean=mean sigma=sigma1 ] = 1. root [20] sigma1. (RooRealVar &) RooRealVar::sigma1 = 0.5 C L(-INF - +INF) . root [21] mean. (RooRealVar &) RooRealVar::mean = 5 L(0 - 10) . ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2576
https://github.com/root-project/root/pull/2576:1218,energy efficiency,model,model,1218,"[RooFit] Add cling value printing for RooAbsArg; therewith for tens of widely used classes such as variables, constants,. distributions and combinations thereof. For example:. ```. root [1] // Create two Gaussian PDFs g1(x,mean1,sigma) anf g2(x,mean2,sigma) and their parameters. root [2] RooRealVar mean(""mean"",""mean of gaussians"",5,0,10) ;. root [3] RooRealVar sigma1(""sigma1"",""width of gaussians"",0.5) ;. root [4] RooRealVar sigma2(""sigma2"",""width of gaussians"",1) ;. root [5] RooGaussian sig1(""sig1"",""Signal component 1"",x,mean,sigma1) ;. root [6] RooGaussian sig2(""sig2"",""Signal component 2"",x,mean,sigma2) ;. root [7] // Build Chebychev polynomial p.d.f. root [8] RooRealVar a0(""a0"",""a0"",0.5,0.,1.) ;. root [9] RooRealVar a1(""a1"",""a1"",0.2,0.,1.) ;. root [10] RooChebychev bkg(""bkg"",""Background"",x,RooArgSet(a0,a1)) ;. root [11] // Sum the signal components into a composite signal p.d.f. root [12] RooRealVar sig1frac(""sig1frac"",""fraction of component 1 in signal"",0.8,0.,1.) ;. root [13] RooAddPdf sig(""sig"",""Signal"",RooArgList(sig1,sig2),sig1frac) ;. root [14] // Sum the composite signal and background. root [15] RooRealVar bkgfrac(""bkgfrac"",""fraction of background"",0.5,0.,1.) ;. root [16] RooAddPdf model(""model"",""g1+g2+a"",RooArgList(bkg,sig),bkgfrac) ;. root [17] model. (RooAddPdf &) RooAddPdf::model[ bkgfrac * bkg + [%] * sig ] = 0.9. root [18] sig. (RooAddPdf &) RooAddPdf::sig[ sig1frac * sig1 + [%] * sig2 ] = 1. root [19] sig1. (RooGaussian &) RooGaussian::sig1[ x=x mean=mean sigma=sigma1 ] = 1. root [20] sigma1. (RooRealVar &) RooRealVar::sigma1 = 0.5 C L(-INF - +INF) . root [21] mean. (RooRealVar &) RooRealVar::mean = 5 L(0 - 10) . ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2576
https://github.com/root-project/root/pull/2576:1277,energy efficiency,model,model,1277,"[RooFit] Add cling value printing for RooAbsArg; therewith for tens of widely used classes such as variables, constants,. distributions and combinations thereof. For example:. ```. root [1] // Create two Gaussian PDFs g1(x,mean1,sigma) anf g2(x,mean2,sigma) and their parameters. root [2] RooRealVar mean(""mean"",""mean of gaussians"",5,0,10) ;. root [3] RooRealVar sigma1(""sigma1"",""width of gaussians"",0.5) ;. root [4] RooRealVar sigma2(""sigma2"",""width of gaussians"",1) ;. root [5] RooGaussian sig1(""sig1"",""Signal component 1"",x,mean,sigma1) ;. root [6] RooGaussian sig2(""sig2"",""Signal component 2"",x,mean,sigma2) ;. root [7] // Build Chebychev polynomial p.d.f. root [8] RooRealVar a0(""a0"",""a0"",0.5,0.,1.) ;. root [9] RooRealVar a1(""a1"",""a1"",0.2,0.,1.) ;. root [10] RooChebychev bkg(""bkg"",""Background"",x,RooArgSet(a0,a1)) ;. root [11] // Sum the signal components into a composite signal p.d.f. root [12] RooRealVar sig1frac(""sig1frac"",""fraction of component 1 in signal"",0.8,0.,1.) ;. root [13] RooAddPdf sig(""sig"",""Signal"",RooArgList(sig1,sig2),sig1frac) ;. root [14] // Sum the composite signal and background. root [15] RooRealVar bkgfrac(""bkgfrac"",""fraction of background"",0.5,0.,1.) ;. root [16] RooAddPdf model(""model"",""g1+g2+a"",RooArgList(bkg,sig),bkgfrac) ;. root [17] model. (RooAddPdf &) RooAddPdf::model[ bkgfrac * bkg + [%] * sig ] = 0.9. root [18] sig. (RooAddPdf &) RooAddPdf::sig[ sig1frac * sig1 + [%] * sig2 ] = 1. root [19] sig1. (RooGaussian &) RooGaussian::sig1[ x=x mean=mean sigma=sigma1 ] = 1. root [20] sigma1. (RooRealVar &) RooRealVar::sigma1 = 0.5 C L(-INF - +INF) . root [21] mean. (RooRealVar &) RooRealVar::mean = 5 L(0 - 10) . ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2576
https://github.com/root-project/root/pull/2576:1309,energy efficiency,model,model,1309,"[RooFit] Add cling value printing for RooAbsArg; therewith for tens of widely used classes such as variables, constants,. distributions and combinations thereof. For example:. ```. root [1] // Create two Gaussian PDFs g1(x,mean1,sigma) anf g2(x,mean2,sigma) and their parameters. root [2] RooRealVar mean(""mean"",""mean of gaussians"",5,0,10) ;. root [3] RooRealVar sigma1(""sigma1"",""width of gaussians"",0.5) ;. root [4] RooRealVar sigma2(""sigma2"",""width of gaussians"",1) ;. root [5] RooGaussian sig1(""sig1"",""Signal component 1"",x,mean,sigma1) ;. root [6] RooGaussian sig2(""sig2"",""Signal component 2"",x,mean,sigma2) ;. root [7] // Build Chebychev polynomial p.d.f. root [8] RooRealVar a0(""a0"",""a0"",0.5,0.,1.) ;. root [9] RooRealVar a1(""a1"",""a1"",0.2,0.,1.) ;. root [10] RooChebychev bkg(""bkg"",""Background"",x,RooArgSet(a0,a1)) ;. root [11] // Sum the signal components into a composite signal p.d.f. root [12] RooRealVar sig1frac(""sig1frac"",""fraction of component 1 in signal"",0.8,0.,1.) ;. root [13] RooAddPdf sig(""sig"",""Signal"",RooArgList(sig1,sig2),sig1frac) ;. root [14] // Sum the composite signal and background. root [15] RooRealVar bkgfrac(""bkgfrac"",""fraction of background"",0.5,0.,1.) ;. root [16] RooAddPdf model(""model"",""g1+g2+a"",RooArgList(bkg,sig),bkgfrac) ;. root [17] model. (RooAddPdf &) RooAddPdf::model[ bkgfrac * bkg + [%] * sig ] = 0.9. root [18] sig. (RooAddPdf &) RooAddPdf::sig[ sig1frac * sig1 + [%] * sig2 ] = 1. root [19] sig1. (RooGaussian &) RooGaussian::sig1[ x=x mean=mean sigma=sigma1 ] = 1. root [20] sigma1. (RooRealVar &) RooRealVar::sigma1 = 0.5 C L(-INF - +INF) . root [21] mean. (RooRealVar &) RooRealVar::mean = 5 L(0 - 10) . ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2576
https://github.com/root-project/root/pull/2576:512,integrability,compon,component,512,"[RooFit] Add cling value printing for RooAbsArg; therewith for tens of widely used classes such as variables, constants,. distributions and combinations thereof. For example:. ```. root [1] // Create two Gaussian PDFs g1(x,mean1,sigma) anf g2(x,mean2,sigma) and their parameters. root [2] RooRealVar mean(""mean"",""mean of gaussians"",5,0,10) ;. root [3] RooRealVar sigma1(""sigma1"",""width of gaussians"",0.5) ;. root [4] RooRealVar sigma2(""sigma2"",""width of gaussians"",1) ;. root [5] RooGaussian sig1(""sig1"",""Signal component 1"",x,mean,sigma1) ;. root [6] RooGaussian sig2(""sig2"",""Signal component 2"",x,mean,sigma2) ;. root [7] // Build Chebychev polynomial p.d.f. root [8] RooRealVar a0(""a0"",""a0"",0.5,0.,1.) ;. root [9] RooRealVar a1(""a1"",""a1"",0.2,0.,1.) ;. root [10] RooChebychev bkg(""bkg"",""Background"",x,RooArgSet(a0,a1)) ;. root [11] // Sum the signal components into a composite signal p.d.f. root [12] RooRealVar sig1frac(""sig1frac"",""fraction of component 1 in signal"",0.8,0.,1.) ;. root [13] RooAddPdf sig(""sig"",""Signal"",RooArgList(sig1,sig2),sig1frac) ;. root [14] // Sum the composite signal and background. root [15] RooRealVar bkgfrac(""bkgfrac"",""fraction of background"",0.5,0.,1.) ;. root [16] RooAddPdf model(""model"",""g1+g2+a"",RooArgList(bkg,sig),bkgfrac) ;. root [17] model. (RooAddPdf &) RooAddPdf::model[ bkgfrac * bkg + [%] * sig ] = 0.9. root [18] sig. (RooAddPdf &) RooAddPdf::sig[ sig1frac * sig1 + [%] * sig2 ] = 1. root [19] sig1. (RooGaussian &) RooGaussian::sig1[ x=x mean=mean sigma=sigma1 ] = 1. root [20] sigma1. (RooRealVar &) RooRealVar::sigma1 = 0.5 C L(-INF - +INF) . root [21] mean. (RooRealVar &) RooRealVar::mean = 5 L(0 - 10) . ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2576
https://github.com/root-project/root/pull/2576:584,integrability,compon,component,584,"[RooFit] Add cling value printing for RooAbsArg; therewith for tens of widely used classes such as variables, constants,. distributions and combinations thereof. For example:. ```. root [1] // Create two Gaussian PDFs g1(x,mean1,sigma) anf g2(x,mean2,sigma) and their parameters. root [2] RooRealVar mean(""mean"",""mean of gaussians"",5,0,10) ;. root [3] RooRealVar sigma1(""sigma1"",""width of gaussians"",0.5) ;. root [4] RooRealVar sigma2(""sigma2"",""width of gaussians"",1) ;. root [5] RooGaussian sig1(""sig1"",""Signal component 1"",x,mean,sigma1) ;. root [6] RooGaussian sig2(""sig2"",""Signal component 2"",x,mean,sigma2) ;. root [7] // Build Chebychev polynomial p.d.f. root [8] RooRealVar a0(""a0"",""a0"",0.5,0.,1.) ;. root [9] RooRealVar a1(""a1"",""a1"",0.2,0.,1.) ;. root [10] RooChebychev bkg(""bkg"",""Background"",x,RooArgSet(a0,a1)) ;. root [11] // Sum the signal components into a composite signal p.d.f. root [12] RooRealVar sig1frac(""sig1frac"",""fraction of component 1 in signal"",0.8,0.,1.) ;. root [13] RooAddPdf sig(""sig"",""Signal"",RooArgList(sig1,sig2),sig1frac) ;. root [14] // Sum the composite signal and background. root [15] RooRealVar bkgfrac(""bkgfrac"",""fraction of background"",0.5,0.,1.) ;. root [16] RooAddPdf model(""model"",""g1+g2+a"",RooArgList(bkg,sig),bkgfrac) ;. root [17] model. (RooAddPdf &) RooAddPdf::model[ bkgfrac * bkg + [%] * sig ] = 0.9. root [18] sig. (RooAddPdf &) RooAddPdf::sig[ sig1frac * sig1 + [%] * sig2 ] = 1. root [19] sig1. (RooGaussian &) RooGaussian::sig1[ x=x mean=mean sigma=sigma1 ] = 1. root [20] sigma1. (RooRealVar &) RooRealVar::sigma1 = 0.5 C L(-INF - +INF) . root [21] mean. (RooRealVar &) RooRealVar::mean = 5 L(0 - 10) . ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2576
https://github.com/root-project/root/pull/2576:852,integrability,compon,components,852,"[RooFit] Add cling value printing for RooAbsArg; therewith for tens of widely used classes such as variables, constants,. distributions and combinations thereof. For example:. ```. root [1] // Create two Gaussian PDFs g1(x,mean1,sigma) anf g2(x,mean2,sigma) and their parameters. root [2] RooRealVar mean(""mean"",""mean of gaussians"",5,0,10) ;. root [3] RooRealVar sigma1(""sigma1"",""width of gaussians"",0.5) ;. root [4] RooRealVar sigma2(""sigma2"",""width of gaussians"",1) ;. root [5] RooGaussian sig1(""sig1"",""Signal component 1"",x,mean,sigma1) ;. root [6] RooGaussian sig2(""sig2"",""Signal component 2"",x,mean,sigma2) ;. root [7] // Build Chebychev polynomial p.d.f. root [8] RooRealVar a0(""a0"",""a0"",0.5,0.,1.) ;. root [9] RooRealVar a1(""a1"",""a1"",0.2,0.,1.) ;. root [10] RooChebychev bkg(""bkg"",""Background"",x,RooArgSet(a0,a1)) ;. root [11] // Sum the signal components into a composite signal p.d.f. root [12] RooRealVar sig1frac(""sig1frac"",""fraction of component 1 in signal"",0.8,0.,1.) ;. root [13] RooAddPdf sig(""sig"",""Signal"",RooArgList(sig1,sig2),sig1frac) ;. root [14] // Sum the composite signal and background. root [15] RooRealVar bkgfrac(""bkgfrac"",""fraction of background"",0.5,0.,1.) ;. root [16] RooAddPdf model(""model"",""g1+g2+a"",RooArgList(bkg,sig),bkgfrac) ;. root [17] model. (RooAddPdf &) RooAddPdf::model[ bkgfrac * bkg + [%] * sig ] = 0.9. root [18] sig. (RooAddPdf &) RooAddPdf::sig[ sig1frac * sig1 + [%] * sig2 ] = 1. root [19] sig1. (RooGaussian &) RooGaussian::sig1[ x=x mean=mean sigma=sigma1 ] = 1. root [20] sigma1. (RooRealVar &) RooRealVar::sigma1 = 0.5 C L(-INF - +INF) . root [21] mean. (RooRealVar &) RooRealVar::mean = 5 L(0 - 10) . ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2576
https://github.com/root-project/root/pull/2576:948,integrability,compon,component,948,"[RooFit] Add cling value printing for RooAbsArg; therewith for tens of widely used classes such as variables, constants,. distributions and combinations thereof. For example:. ```. root [1] // Create two Gaussian PDFs g1(x,mean1,sigma) anf g2(x,mean2,sigma) and their parameters. root [2] RooRealVar mean(""mean"",""mean of gaussians"",5,0,10) ;. root [3] RooRealVar sigma1(""sigma1"",""width of gaussians"",0.5) ;. root [4] RooRealVar sigma2(""sigma2"",""width of gaussians"",1) ;. root [5] RooGaussian sig1(""sig1"",""Signal component 1"",x,mean,sigma1) ;. root [6] RooGaussian sig2(""sig2"",""Signal component 2"",x,mean,sigma2) ;. root [7] // Build Chebychev polynomial p.d.f. root [8] RooRealVar a0(""a0"",""a0"",0.5,0.,1.) ;. root [9] RooRealVar a1(""a1"",""a1"",0.2,0.,1.) ;. root [10] RooChebychev bkg(""bkg"",""Background"",x,RooArgSet(a0,a1)) ;. root [11] // Sum the signal components into a composite signal p.d.f. root [12] RooRealVar sig1frac(""sig1frac"",""fraction of component 1 in signal"",0.8,0.,1.) ;. root [13] RooAddPdf sig(""sig"",""Signal"",RooArgList(sig1,sig2),sig1frac) ;. root [14] // Sum the composite signal and background. root [15] RooRealVar bkgfrac(""bkgfrac"",""fraction of background"",0.5,0.,1.) ;. root [16] RooAddPdf model(""model"",""g1+g2+a"",RooArgList(bkg,sig),bkgfrac) ;. root [17] model. (RooAddPdf &) RooAddPdf::model[ bkgfrac * bkg + [%] * sig ] = 0.9. root [18] sig. (RooAddPdf &) RooAddPdf::sig[ sig1frac * sig1 + [%] * sig2 ] = 1. root [19] sig1. (RooGaussian &) RooGaussian::sig1[ x=x mean=mean sigma=sigma1 ] = 1. root [20] sigma1. (RooRealVar &) RooRealVar::sigma1 = 0.5 C L(-INF - +INF) . root [21] mean. (RooRealVar &) RooRealVar::mean = 5 L(0 - 10) . ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2576
https://github.com/root-project/root/pull/2576:122,interoperability,distribut,distributions,122,"[RooFit] Add cling value printing for RooAbsArg; therewith for tens of widely used classes such as variables, constants,. distributions and combinations thereof. For example:. ```. root [1] // Create two Gaussian PDFs g1(x,mean1,sigma) anf g2(x,mean2,sigma) and their parameters. root [2] RooRealVar mean(""mean"",""mean of gaussians"",5,0,10) ;. root [3] RooRealVar sigma1(""sigma1"",""width of gaussians"",0.5) ;. root [4] RooRealVar sigma2(""sigma2"",""width of gaussians"",1) ;. root [5] RooGaussian sig1(""sig1"",""Signal component 1"",x,mean,sigma1) ;. root [6] RooGaussian sig2(""sig2"",""Signal component 2"",x,mean,sigma2) ;. root [7] // Build Chebychev polynomial p.d.f. root [8] RooRealVar a0(""a0"",""a0"",0.5,0.,1.) ;. root [9] RooRealVar a1(""a1"",""a1"",0.2,0.,1.) ;. root [10] RooChebychev bkg(""bkg"",""Background"",x,RooArgSet(a0,a1)) ;. root [11] // Sum the signal components into a composite signal p.d.f. root [12] RooRealVar sig1frac(""sig1frac"",""fraction of component 1 in signal"",0.8,0.,1.) ;. root [13] RooAddPdf sig(""sig"",""Signal"",RooArgList(sig1,sig2),sig1frac) ;. root [14] // Sum the composite signal and background. root [15] RooRealVar bkgfrac(""bkgfrac"",""fraction of background"",0.5,0.,1.) ;. root [16] RooAddPdf model(""model"",""g1+g2+a"",RooArgList(bkg,sig),bkgfrac) ;. root [17] model. (RooAddPdf &) RooAddPdf::model[ bkgfrac * bkg + [%] * sig ] = 0.9. root [18] sig. (RooAddPdf &) RooAddPdf::sig[ sig1frac * sig1 + [%] * sig2 ] = 1. root [19] sig1. (RooGaussian &) RooGaussian::sig1[ x=x mean=mean sigma=sigma1 ] = 1. root [20] sigma1. (RooRealVar &) RooRealVar::sigma1 = 0.5 C L(-INF - +INF) . root [21] mean. (RooRealVar &) RooRealVar::mean = 5 L(0 - 10) . ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2576
https://github.com/root-project/root/pull/2576:512,interoperability,compon,component,512,"[RooFit] Add cling value printing for RooAbsArg; therewith for tens of widely used classes such as variables, constants,. distributions and combinations thereof. For example:. ```. root [1] // Create two Gaussian PDFs g1(x,mean1,sigma) anf g2(x,mean2,sigma) and their parameters. root [2] RooRealVar mean(""mean"",""mean of gaussians"",5,0,10) ;. root [3] RooRealVar sigma1(""sigma1"",""width of gaussians"",0.5) ;. root [4] RooRealVar sigma2(""sigma2"",""width of gaussians"",1) ;. root [5] RooGaussian sig1(""sig1"",""Signal component 1"",x,mean,sigma1) ;. root [6] RooGaussian sig2(""sig2"",""Signal component 2"",x,mean,sigma2) ;. root [7] // Build Chebychev polynomial p.d.f. root [8] RooRealVar a0(""a0"",""a0"",0.5,0.,1.) ;. root [9] RooRealVar a1(""a1"",""a1"",0.2,0.,1.) ;. root [10] RooChebychev bkg(""bkg"",""Background"",x,RooArgSet(a0,a1)) ;. root [11] // Sum the signal components into a composite signal p.d.f. root [12] RooRealVar sig1frac(""sig1frac"",""fraction of component 1 in signal"",0.8,0.,1.) ;. root [13] RooAddPdf sig(""sig"",""Signal"",RooArgList(sig1,sig2),sig1frac) ;. root [14] // Sum the composite signal and background. root [15] RooRealVar bkgfrac(""bkgfrac"",""fraction of background"",0.5,0.,1.) ;. root [16] RooAddPdf model(""model"",""g1+g2+a"",RooArgList(bkg,sig),bkgfrac) ;. root [17] model. (RooAddPdf &) RooAddPdf::model[ bkgfrac * bkg + [%] * sig ] = 0.9. root [18] sig. (RooAddPdf &) RooAddPdf::sig[ sig1frac * sig1 + [%] * sig2 ] = 1. root [19] sig1. (RooGaussian &) RooGaussian::sig1[ x=x mean=mean sigma=sigma1 ] = 1. root [20] sigma1. (RooRealVar &) RooRealVar::sigma1 = 0.5 C L(-INF - +INF) . root [21] mean. (RooRealVar &) RooRealVar::mean = 5 L(0 - 10) . ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2576
https://github.com/root-project/root/pull/2576:584,interoperability,compon,component,584,"[RooFit] Add cling value printing for RooAbsArg; therewith for tens of widely used classes such as variables, constants,. distributions and combinations thereof. For example:. ```. root [1] // Create two Gaussian PDFs g1(x,mean1,sigma) anf g2(x,mean2,sigma) and their parameters. root [2] RooRealVar mean(""mean"",""mean of gaussians"",5,0,10) ;. root [3] RooRealVar sigma1(""sigma1"",""width of gaussians"",0.5) ;. root [4] RooRealVar sigma2(""sigma2"",""width of gaussians"",1) ;. root [5] RooGaussian sig1(""sig1"",""Signal component 1"",x,mean,sigma1) ;. root [6] RooGaussian sig2(""sig2"",""Signal component 2"",x,mean,sigma2) ;. root [7] // Build Chebychev polynomial p.d.f. root [8] RooRealVar a0(""a0"",""a0"",0.5,0.,1.) ;. root [9] RooRealVar a1(""a1"",""a1"",0.2,0.,1.) ;. root [10] RooChebychev bkg(""bkg"",""Background"",x,RooArgSet(a0,a1)) ;. root [11] // Sum the signal components into a composite signal p.d.f. root [12] RooRealVar sig1frac(""sig1frac"",""fraction of component 1 in signal"",0.8,0.,1.) ;. root [13] RooAddPdf sig(""sig"",""Signal"",RooArgList(sig1,sig2),sig1frac) ;. root [14] // Sum the composite signal and background. root [15] RooRealVar bkgfrac(""bkgfrac"",""fraction of background"",0.5,0.,1.) ;. root [16] RooAddPdf model(""model"",""g1+g2+a"",RooArgList(bkg,sig),bkgfrac) ;. root [17] model. (RooAddPdf &) RooAddPdf::model[ bkgfrac * bkg + [%] * sig ] = 0.9. root [18] sig. (RooAddPdf &) RooAddPdf::sig[ sig1frac * sig1 + [%] * sig2 ] = 1. root [19] sig1. (RooGaussian &) RooGaussian::sig1[ x=x mean=mean sigma=sigma1 ] = 1. root [20] sigma1. (RooRealVar &) RooRealVar::sigma1 = 0.5 C L(-INF - +INF) . root [21] mean. (RooRealVar &) RooRealVar::mean = 5 L(0 - 10) . ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2576
https://github.com/root-project/root/pull/2576:852,interoperability,compon,components,852,"[RooFit] Add cling value printing for RooAbsArg; therewith for tens of widely used classes such as variables, constants,. distributions and combinations thereof. For example:. ```. root [1] // Create two Gaussian PDFs g1(x,mean1,sigma) anf g2(x,mean2,sigma) and their parameters. root [2] RooRealVar mean(""mean"",""mean of gaussians"",5,0,10) ;. root [3] RooRealVar sigma1(""sigma1"",""width of gaussians"",0.5) ;. root [4] RooRealVar sigma2(""sigma2"",""width of gaussians"",1) ;. root [5] RooGaussian sig1(""sig1"",""Signal component 1"",x,mean,sigma1) ;. root [6] RooGaussian sig2(""sig2"",""Signal component 2"",x,mean,sigma2) ;. root [7] // Build Chebychev polynomial p.d.f. root [8] RooRealVar a0(""a0"",""a0"",0.5,0.,1.) ;. root [9] RooRealVar a1(""a1"",""a1"",0.2,0.,1.) ;. root [10] RooChebychev bkg(""bkg"",""Background"",x,RooArgSet(a0,a1)) ;. root [11] // Sum the signal components into a composite signal p.d.f. root [12] RooRealVar sig1frac(""sig1frac"",""fraction of component 1 in signal"",0.8,0.,1.) ;. root [13] RooAddPdf sig(""sig"",""Signal"",RooArgList(sig1,sig2),sig1frac) ;. root [14] // Sum the composite signal and background. root [15] RooRealVar bkgfrac(""bkgfrac"",""fraction of background"",0.5,0.,1.) ;. root [16] RooAddPdf model(""model"",""g1+g2+a"",RooArgList(bkg,sig),bkgfrac) ;. root [17] model. (RooAddPdf &) RooAddPdf::model[ bkgfrac * bkg + [%] * sig ] = 0.9. root [18] sig. (RooAddPdf &) RooAddPdf::sig[ sig1frac * sig1 + [%] * sig2 ] = 1. root [19] sig1. (RooGaussian &) RooGaussian::sig1[ x=x mean=mean sigma=sigma1 ] = 1. root [20] sigma1. (RooRealVar &) RooRealVar::sigma1 = 0.5 C L(-INF - +INF) . root [21] mean. (RooRealVar &) RooRealVar::mean = 5 L(0 - 10) . ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2576
https://github.com/root-project/root/pull/2576:948,interoperability,compon,component,948,"[RooFit] Add cling value printing for RooAbsArg; therewith for tens of widely used classes such as variables, constants,. distributions and combinations thereof. For example:. ```. root [1] // Create two Gaussian PDFs g1(x,mean1,sigma) anf g2(x,mean2,sigma) and their parameters. root [2] RooRealVar mean(""mean"",""mean of gaussians"",5,0,10) ;. root [3] RooRealVar sigma1(""sigma1"",""width of gaussians"",0.5) ;. root [4] RooRealVar sigma2(""sigma2"",""width of gaussians"",1) ;. root [5] RooGaussian sig1(""sig1"",""Signal component 1"",x,mean,sigma1) ;. root [6] RooGaussian sig2(""sig2"",""Signal component 2"",x,mean,sigma2) ;. root [7] // Build Chebychev polynomial p.d.f. root [8] RooRealVar a0(""a0"",""a0"",0.5,0.,1.) ;. root [9] RooRealVar a1(""a1"",""a1"",0.2,0.,1.) ;. root [10] RooChebychev bkg(""bkg"",""Background"",x,RooArgSet(a0,a1)) ;. root [11] // Sum the signal components into a composite signal p.d.f. root [12] RooRealVar sig1frac(""sig1frac"",""fraction of component 1 in signal"",0.8,0.,1.) ;. root [13] RooAddPdf sig(""sig"",""Signal"",RooArgList(sig1,sig2),sig1frac) ;. root [14] // Sum the composite signal and background. root [15] RooRealVar bkgfrac(""bkgfrac"",""fraction of background"",0.5,0.,1.) ;. root [16] RooAddPdf model(""model"",""g1+g2+a"",RooArgList(bkg,sig),bkgfrac) ;. root [17] model. (RooAddPdf &) RooAddPdf::model[ bkgfrac * bkg + [%] * sig ] = 0.9. root [18] sig. (RooAddPdf &) RooAddPdf::sig[ sig1frac * sig1 + [%] * sig2 ] = 1. root [19] sig1. (RooGaussian &) RooGaussian::sig1[ x=x mean=mean sigma=sigma1 ] = 1. root [20] sigma1. (RooRealVar &) RooRealVar::sigma1 = 0.5 C L(-INF - +INF) . root [21] mean. (RooRealVar &) RooRealVar::mean = 5 L(0 - 10) . ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2576
https://github.com/root-project/root/pull/2576:99,modifiability,variab,variables,99,"[RooFit] Add cling value printing for RooAbsArg; therewith for tens of widely used classes such as variables, constants,. distributions and combinations thereof. For example:. ```. root [1] // Create two Gaussian PDFs g1(x,mean1,sigma) anf g2(x,mean2,sigma) and their parameters. root [2] RooRealVar mean(""mean"",""mean of gaussians"",5,0,10) ;. root [3] RooRealVar sigma1(""sigma1"",""width of gaussians"",0.5) ;. root [4] RooRealVar sigma2(""sigma2"",""width of gaussians"",1) ;. root [5] RooGaussian sig1(""sig1"",""Signal component 1"",x,mean,sigma1) ;. root [6] RooGaussian sig2(""sig2"",""Signal component 2"",x,mean,sigma2) ;. root [7] // Build Chebychev polynomial p.d.f. root [8] RooRealVar a0(""a0"",""a0"",0.5,0.,1.) ;. root [9] RooRealVar a1(""a1"",""a1"",0.2,0.,1.) ;. root [10] RooChebychev bkg(""bkg"",""Background"",x,RooArgSet(a0,a1)) ;. root [11] // Sum the signal components into a composite signal p.d.f. root [12] RooRealVar sig1frac(""sig1frac"",""fraction of component 1 in signal"",0.8,0.,1.) ;. root [13] RooAddPdf sig(""sig"",""Signal"",RooArgList(sig1,sig2),sig1frac) ;. root [14] // Sum the composite signal and background. root [15] RooRealVar bkgfrac(""bkgfrac"",""fraction of background"",0.5,0.,1.) ;. root [16] RooAddPdf model(""model"",""g1+g2+a"",RooArgList(bkg,sig),bkgfrac) ;. root [17] model. (RooAddPdf &) RooAddPdf::model[ bkgfrac * bkg + [%] * sig ] = 0.9. root [18] sig. (RooAddPdf &) RooAddPdf::sig[ sig1frac * sig1 + [%] * sig2 ] = 1. root [19] sig1. (RooGaussian &) RooGaussian::sig1[ x=x mean=mean sigma=sigma1 ] = 1. root [20] sigma1. (RooRealVar &) RooRealVar::sigma1 = 0.5 C L(-INF - +INF) . root [21] mean. (RooRealVar &) RooRealVar::mean = 5 L(0 - 10) . ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2576
https://github.com/root-project/root/pull/2576:268,modifiability,paramet,parameters,268,"[RooFit] Add cling value printing for RooAbsArg; therewith for tens of widely used classes such as variables, constants,. distributions and combinations thereof. For example:. ```. root [1] // Create two Gaussian PDFs g1(x,mean1,sigma) anf g2(x,mean2,sigma) and their parameters. root [2] RooRealVar mean(""mean"",""mean of gaussians"",5,0,10) ;. root [3] RooRealVar sigma1(""sigma1"",""width of gaussians"",0.5) ;. root [4] RooRealVar sigma2(""sigma2"",""width of gaussians"",1) ;. root [5] RooGaussian sig1(""sig1"",""Signal component 1"",x,mean,sigma1) ;. root [6] RooGaussian sig2(""sig2"",""Signal component 2"",x,mean,sigma2) ;. root [7] // Build Chebychev polynomial p.d.f. root [8] RooRealVar a0(""a0"",""a0"",0.5,0.,1.) ;. root [9] RooRealVar a1(""a1"",""a1"",0.2,0.,1.) ;. root [10] RooChebychev bkg(""bkg"",""Background"",x,RooArgSet(a0,a1)) ;. root [11] // Sum the signal components into a composite signal p.d.f. root [12] RooRealVar sig1frac(""sig1frac"",""fraction of component 1 in signal"",0.8,0.,1.) ;. root [13] RooAddPdf sig(""sig"",""Signal"",RooArgList(sig1,sig2),sig1frac) ;. root [14] // Sum the composite signal and background. root [15] RooRealVar bkgfrac(""bkgfrac"",""fraction of background"",0.5,0.,1.) ;. root [16] RooAddPdf model(""model"",""g1+g2+a"",RooArgList(bkg,sig),bkgfrac) ;. root [17] model. (RooAddPdf &) RooAddPdf::model[ bkgfrac * bkg + [%] * sig ] = 0.9. root [18] sig. (RooAddPdf &) RooAddPdf::sig[ sig1frac * sig1 + [%] * sig2 ] = 1. root [19] sig1. (RooGaussian &) RooGaussian::sig1[ x=x mean=mean sigma=sigma1 ] = 1. root [20] sigma1. (RooRealVar &) RooRealVar::sigma1 = 0.5 C L(-INF - +INF) . root [21] mean. (RooRealVar &) RooRealVar::mean = 5 L(0 - 10) . ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2576
https://github.com/root-project/root/pull/2576:512,modifiability,compon,component,512,"[RooFit] Add cling value printing for RooAbsArg; therewith for tens of widely used classes such as variables, constants,. distributions and combinations thereof. For example:. ```. root [1] // Create two Gaussian PDFs g1(x,mean1,sigma) anf g2(x,mean2,sigma) and their parameters. root [2] RooRealVar mean(""mean"",""mean of gaussians"",5,0,10) ;. root [3] RooRealVar sigma1(""sigma1"",""width of gaussians"",0.5) ;. root [4] RooRealVar sigma2(""sigma2"",""width of gaussians"",1) ;. root [5] RooGaussian sig1(""sig1"",""Signal component 1"",x,mean,sigma1) ;. root [6] RooGaussian sig2(""sig2"",""Signal component 2"",x,mean,sigma2) ;. root [7] // Build Chebychev polynomial p.d.f. root [8] RooRealVar a0(""a0"",""a0"",0.5,0.,1.) ;. root [9] RooRealVar a1(""a1"",""a1"",0.2,0.,1.) ;. root [10] RooChebychev bkg(""bkg"",""Background"",x,RooArgSet(a0,a1)) ;. root [11] // Sum the signal components into a composite signal p.d.f. root [12] RooRealVar sig1frac(""sig1frac"",""fraction of component 1 in signal"",0.8,0.,1.) ;. root [13] RooAddPdf sig(""sig"",""Signal"",RooArgList(sig1,sig2),sig1frac) ;. root [14] // Sum the composite signal and background. root [15] RooRealVar bkgfrac(""bkgfrac"",""fraction of background"",0.5,0.,1.) ;. root [16] RooAddPdf model(""model"",""g1+g2+a"",RooArgList(bkg,sig),bkgfrac) ;. root [17] model. (RooAddPdf &) RooAddPdf::model[ bkgfrac * bkg + [%] * sig ] = 0.9. root [18] sig. (RooAddPdf &) RooAddPdf::sig[ sig1frac * sig1 + [%] * sig2 ] = 1. root [19] sig1. (RooGaussian &) RooGaussian::sig1[ x=x mean=mean sigma=sigma1 ] = 1. root [20] sigma1. (RooRealVar &) RooRealVar::sigma1 = 0.5 C L(-INF - +INF) . root [21] mean. (RooRealVar &) RooRealVar::mean = 5 L(0 - 10) . ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2576
https://github.com/root-project/root/pull/2576:584,modifiability,compon,component,584,"[RooFit] Add cling value printing for RooAbsArg; therewith for tens of widely used classes such as variables, constants,. distributions and combinations thereof. For example:. ```. root [1] // Create two Gaussian PDFs g1(x,mean1,sigma) anf g2(x,mean2,sigma) and their parameters. root [2] RooRealVar mean(""mean"",""mean of gaussians"",5,0,10) ;. root [3] RooRealVar sigma1(""sigma1"",""width of gaussians"",0.5) ;. root [4] RooRealVar sigma2(""sigma2"",""width of gaussians"",1) ;. root [5] RooGaussian sig1(""sig1"",""Signal component 1"",x,mean,sigma1) ;. root [6] RooGaussian sig2(""sig2"",""Signal component 2"",x,mean,sigma2) ;. root [7] // Build Chebychev polynomial p.d.f. root [8] RooRealVar a0(""a0"",""a0"",0.5,0.,1.) ;. root [9] RooRealVar a1(""a1"",""a1"",0.2,0.,1.) ;. root [10] RooChebychev bkg(""bkg"",""Background"",x,RooArgSet(a0,a1)) ;. root [11] // Sum the signal components into a composite signal p.d.f. root [12] RooRealVar sig1frac(""sig1frac"",""fraction of component 1 in signal"",0.8,0.,1.) ;. root [13] RooAddPdf sig(""sig"",""Signal"",RooArgList(sig1,sig2),sig1frac) ;. root [14] // Sum the composite signal and background. root [15] RooRealVar bkgfrac(""bkgfrac"",""fraction of background"",0.5,0.,1.) ;. root [16] RooAddPdf model(""model"",""g1+g2+a"",RooArgList(bkg,sig),bkgfrac) ;. root [17] model. (RooAddPdf &) RooAddPdf::model[ bkgfrac * bkg + [%] * sig ] = 0.9. root [18] sig. (RooAddPdf &) RooAddPdf::sig[ sig1frac * sig1 + [%] * sig2 ] = 1. root [19] sig1. (RooGaussian &) RooGaussian::sig1[ x=x mean=mean sigma=sigma1 ] = 1. root [20] sigma1. (RooRealVar &) RooRealVar::sigma1 = 0.5 C L(-INF - +INF) . root [21] mean. (RooRealVar &) RooRealVar::mean = 5 L(0 - 10) . ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2576
https://github.com/root-project/root/pull/2576:852,modifiability,compon,components,852,"[RooFit] Add cling value printing for RooAbsArg; therewith for tens of widely used classes such as variables, constants,. distributions and combinations thereof. For example:. ```. root [1] // Create two Gaussian PDFs g1(x,mean1,sigma) anf g2(x,mean2,sigma) and their parameters. root [2] RooRealVar mean(""mean"",""mean of gaussians"",5,0,10) ;. root [3] RooRealVar sigma1(""sigma1"",""width of gaussians"",0.5) ;. root [4] RooRealVar sigma2(""sigma2"",""width of gaussians"",1) ;. root [5] RooGaussian sig1(""sig1"",""Signal component 1"",x,mean,sigma1) ;. root [6] RooGaussian sig2(""sig2"",""Signal component 2"",x,mean,sigma2) ;. root [7] // Build Chebychev polynomial p.d.f. root [8] RooRealVar a0(""a0"",""a0"",0.5,0.,1.) ;. root [9] RooRealVar a1(""a1"",""a1"",0.2,0.,1.) ;. root [10] RooChebychev bkg(""bkg"",""Background"",x,RooArgSet(a0,a1)) ;. root [11] // Sum the signal components into a composite signal p.d.f. root [12] RooRealVar sig1frac(""sig1frac"",""fraction of component 1 in signal"",0.8,0.,1.) ;. root [13] RooAddPdf sig(""sig"",""Signal"",RooArgList(sig1,sig2),sig1frac) ;. root [14] // Sum the composite signal and background. root [15] RooRealVar bkgfrac(""bkgfrac"",""fraction of background"",0.5,0.,1.) ;. root [16] RooAddPdf model(""model"",""g1+g2+a"",RooArgList(bkg,sig),bkgfrac) ;. root [17] model. (RooAddPdf &) RooAddPdf::model[ bkgfrac * bkg + [%] * sig ] = 0.9. root [18] sig. (RooAddPdf &) RooAddPdf::sig[ sig1frac * sig1 + [%] * sig2 ] = 1. root [19] sig1. (RooGaussian &) RooGaussian::sig1[ x=x mean=mean sigma=sigma1 ] = 1. root [20] sigma1. (RooRealVar &) RooRealVar::sigma1 = 0.5 C L(-INF - +INF) . root [21] mean. (RooRealVar &) RooRealVar::mean = 5 L(0 - 10) . ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2576
https://github.com/root-project/root/pull/2576:870,modifiability,compos,composite,870,"[RooFit] Add cling value printing for RooAbsArg; therewith for tens of widely used classes such as variables, constants,. distributions and combinations thereof. For example:. ```. root [1] // Create two Gaussian PDFs g1(x,mean1,sigma) anf g2(x,mean2,sigma) and their parameters. root [2] RooRealVar mean(""mean"",""mean of gaussians"",5,0,10) ;. root [3] RooRealVar sigma1(""sigma1"",""width of gaussians"",0.5) ;. root [4] RooRealVar sigma2(""sigma2"",""width of gaussians"",1) ;. root [5] RooGaussian sig1(""sig1"",""Signal component 1"",x,mean,sigma1) ;. root [6] RooGaussian sig2(""sig2"",""Signal component 2"",x,mean,sigma2) ;. root [7] // Build Chebychev polynomial p.d.f. root [8] RooRealVar a0(""a0"",""a0"",0.5,0.,1.) ;. root [9] RooRealVar a1(""a1"",""a1"",0.2,0.,1.) ;. root [10] RooChebychev bkg(""bkg"",""Background"",x,RooArgSet(a0,a1)) ;. root [11] // Sum the signal components into a composite signal p.d.f. root [12] RooRealVar sig1frac(""sig1frac"",""fraction of component 1 in signal"",0.8,0.,1.) ;. root [13] RooAddPdf sig(""sig"",""Signal"",RooArgList(sig1,sig2),sig1frac) ;. root [14] // Sum the composite signal and background. root [15] RooRealVar bkgfrac(""bkgfrac"",""fraction of background"",0.5,0.,1.) ;. root [16] RooAddPdf model(""model"",""g1+g2+a"",RooArgList(bkg,sig),bkgfrac) ;. root [17] model. (RooAddPdf &) RooAddPdf::model[ bkgfrac * bkg + [%] * sig ] = 0.9. root [18] sig. (RooAddPdf &) RooAddPdf::sig[ sig1frac * sig1 + [%] * sig2 ] = 1. root [19] sig1. (RooGaussian &) RooGaussian::sig1[ x=x mean=mean sigma=sigma1 ] = 1. root [20] sigma1. (RooRealVar &) RooRealVar::sigma1 = 0.5 C L(-INF - +INF) . root [21] mean. (RooRealVar &) RooRealVar::mean = 5 L(0 - 10) . ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2576
https://github.com/root-project/root/pull/2576:948,modifiability,compon,component,948,"[RooFit] Add cling value printing for RooAbsArg; therewith for tens of widely used classes such as variables, constants,. distributions and combinations thereof. For example:. ```. root [1] // Create two Gaussian PDFs g1(x,mean1,sigma) anf g2(x,mean2,sigma) and their parameters. root [2] RooRealVar mean(""mean"",""mean of gaussians"",5,0,10) ;. root [3] RooRealVar sigma1(""sigma1"",""width of gaussians"",0.5) ;. root [4] RooRealVar sigma2(""sigma2"",""width of gaussians"",1) ;. root [5] RooGaussian sig1(""sig1"",""Signal component 1"",x,mean,sigma1) ;. root [6] RooGaussian sig2(""sig2"",""Signal component 2"",x,mean,sigma2) ;. root [7] // Build Chebychev polynomial p.d.f. root [8] RooRealVar a0(""a0"",""a0"",0.5,0.,1.) ;. root [9] RooRealVar a1(""a1"",""a1"",0.2,0.,1.) ;. root [10] RooChebychev bkg(""bkg"",""Background"",x,RooArgSet(a0,a1)) ;. root [11] // Sum the signal components into a composite signal p.d.f. root [12] RooRealVar sig1frac(""sig1frac"",""fraction of component 1 in signal"",0.8,0.,1.) ;. root [13] RooAddPdf sig(""sig"",""Signal"",RooArgList(sig1,sig2),sig1frac) ;. root [14] // Sum the composite signal and background. root [15] RooRealVar bkgfrac(""bkgfrac"",""fraction of background"",0.5,0.,1.) ;. root [16] RooAddPdf model(""model"",""g1+g2+a"",RooArgList(bkg,sig),bkgfrac) ;. root [17] model. (RooAddPdf &) RooAddPdf::model[ bkgfrac * bkg + [%] * sig ] = 0.9. root [18] sig. (RooAddPdf &) RooAddPdf::sig[ sig1frac * sig1 + [%] * sig2 ] = 1. root [19] sig1. (RooGaussian &) RooGaussian::sig1[ x=x mean=mean sigma=sigma1 ] = 1. root [20] sigma1. (RooRealVar &) RooRealVar::sigma1 = 0.5 C L(-INF - +INF) . root [21] mean. (RooRealVar &) RooRealVar::mean = 5 L(0 - 10) . ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2576
https://github.com/root-project/root/pull/2576:1080,modifiability,compos,composite,1080,"[RooFit] Add cling value printing for RooAbsArg; therewith for tens of widely used classes such as variables, constants,. distributions and combinations thereof. For example:. ```. root [1] // Create two Gaussian PDFs g1(x,mean1,sigma) anf g2(x,mean2,sigma) and their parameters. root [2] RooRealVar mean(""mean"",""mean of gaussians"",5,0,10) ;. root [3] RooRealVar sigma1(""sigma1"",""width of gaussians"",0.5) ;. root [4] RooRealVar sigma2(""sigma2"",""width of gaussians"",1) ;. root [5] RooGaussian sig1(""sig1"",""Signal component 1"",x,mean,sigma1) ;. root [6] RooGaussian sig2(""sig2"",""Signal component 2"",x,mean,sigma2) ;. root [7] // Build Chebychev polynomial p.d.f. root [8] RooRealVar a0(""a0"",""a0"",0.5,0.,1.) ;. root [9] RooRealVar a1(""a1"",""a1"",0.2,0.,1.) ;. root [10] RooChebychev bkg(""bkg"",""Background"",x,RooArgSet(a0,a1)) ;. root [11] // Sum the signal components into a composite signal p.d.f. root [12] RooRealVar sig1frac(""sig1frac"",""fraction of component 1 in signal"",0.8,0.,1.) ;. root [13] RooAddPdf sig(""sig"",""Signal"",RooArgList(sig1,sig2),sig1frac) ;. root [14] // Sum the composite signal and background. root [15] RooRealVar bkgfrac(""bkgfrac"",""fraction of background"",0.5,0.,1.) ;. root [16] RooAddPdf model(""model"",""g1+g2+a"",RooArgList(bkg,sig),bkgfrac) ;. root [17] model. (RooAddPdf &) RooAddPdf::model[ bkgfrac * bkg + [%] * sig ] = 0.9. root [18] sig. (RooAddPdf &) RooAddPdf::sig[ sig1frac * sig1 + [%] * sig2 ] = 1. root [19] sig1. (RooGaussian &) RooGaussian::sig1[ x=x mean=mean sigma=sigma1 ] = 1. root [20] sigma1. (RooRealVar &) RooRealVar::sigma1 = 0.5 C L(-INF - +INF) . root [21] mean. (RooRealVar &) RooRealVar::mean = 5 L(0 - 10) . ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2576
https://github.com/root-project/root/pull/2576:505,security,Sign,Signal,505,"[RooFit] Add cling value printing for RooAbsArg; therewith for tens of widely used classes such as variables, constants,. distributions and combinations thereof. For example:. ```. root [1] // Create two Gaussian PDFs g1(x,mean1,sigma) anf g2(x,mean2,sigma) and their parameters. root [2] RooRealVar mean(""mean"",""mean of gaussians"",5,0,10) ;. root [3] RooRealVar sigma1(""sigma1"",""width of gaussians"",0.5) ;. root [4] RooRealVar sigma2(""sigma2"",""width of gaussians"",1) ;. root [5] RooGaussian sig1(""sig1"",""Signal component 1"",x,mean,sigma1) ;. root [6] RooGaussian sig2(""sig2"",""Signal component 2"",x,mean,sigma2) ;. root [7] // Build Chebychev polynomial p.d.f. root [8] RooRealVar a0(""a0"",""a0"",0.5,0.,1.) ;. root [9] RooRealVar a1(""a1"",""a1"",0.2,0.,1.) ;. root [10] RooChebychev bkg(""bkg"",""Background"",x,RooArgSet(a0,a1)) ;. root [11] // Sum the signal components into a composite signal p.d.f. root [12] RooRealVar sig1frac(""sig1frac"",""fraction of component 1 in signal"",0.8,0.,1.) ;. root [13] RooAddPdf sig(""sig"",""Signal"",RooArgList(sig1,sig2),sig1frac) ;. root [14] // Sum the composite signal and background. root [15] RooRealVar bkgfrac(""bkgfrac"",""fraction of background"",0.5,0.,1.) ;. root [16] RooAddPdf model(""model"",""g1+g2+a"",RooArgList(bkg,sig),bkgfrac) ;. root [17] model. (RooAddPdf &) RooAddPdf::model[ bkgfrac * bkg + [%] * sig ] = 0.9. root [18] sig. (RooAddPdf &) RooAddPdf::sig[ sig1frac * sig1 + [%] * sig2 ] = 1. root [19] sig1. (RooGaussian &) RooGaussian::sig1[ x=x mean=mean sigma=sigma1 ] = 1. root [20] sigma1. (RooRealVar &) RooRealVar::sigma1 = 0.5 C L(-INF - +INF) . root [21] mean. (RooRealVar &) RooRealVar::mean = 5 L(0 - 10) . ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2576
https://github.com/root-project/root/pull/2576:577,security,Sign,Signal,577,"[RooFit] Add cling value printing for RooAbsArg; therewith for tens of widely used classes such as variables, constants,. distributions and combinations thereof. For example:. ```. root [1] // Create two Gaussian PDFs g1(x,mean1,sigma) anf g2(x,mean2,sigma) and their parameters. root [2] RooRealVar mean(""mean"",""mean of gaussians"",5,0,10) ;. root [3] RooRealVar sigma1(""sigma1"",""width of gaussians"",0.5) ;. root [4] RooRealVar sigma2(""sigma2"",""width of gaussians"",1) ;. root [5] RooGaussian sig1(""sig1"",""Signal component 1"",x,mean,sigma1) ;. root [6] RooGaussian sig2(""sig2"",""Signal component 2"",x,mean,sigma2) ;. root [7] // Build Chebychev polynomial p.d.f. root [8] RooRealVar a0(""a0"",""a0"",0.5,0.,1.) ;. root [9] RooRealVar a1(""a1"",""a1"",0.2,0.,1.) ;. root [10] RooChebychev bkg(""bkg"",""Background"",x,RooArgSet(a0,a1)) ;. root [11] // Sum the signal components into a composite signal p.d.f. root [12] RooRealVar sig1frac(""sig1frac"",""fraction of component 1 in signal"",0.8,0.,1.) ;. root [13] RooAddPdf sig(""sig"",""Signal"",RooArgList(sig1,sig2),sig1frac) ;. root [14] // Sum the composite signal and background. root [15] RooRealVar bkgfrac(""bkgfrac"",""fraction of background"",0.5,0.,1.) ;. root [16] RooAddPdf model(""model"",""g1+g2+a"",RooArgList(bkg,sig),bkgfrac) ;. root [17] model. (RooAddPdf &) RooAddPdf::model[ bkgfrac * bkg + [%] * sig ] = 0.9. root [18] sig. (RooAddPdf &) RooAddPdf::sig[ sig1frac * sig1 + [%] * sig2 ] = 1. root [19] sig1. (RooGaussian &) RooGaussian::sig1[ x=x mean=mean sigma=sigma1 ] = 1. root [20] sigma1. (RooRealVar &) RooRealVar::sigma1 = 0.5 C L(-INF - +INF) . root [21] mean. (RooRealVar &) RooRealVar::mean = 5 L(0 - 10) . ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2576
https://github.com/root-project/root/pull/2576:845,security,sign,signal,845,"[RooFit] Add cling value printing for RooAbsArg; therewith for tens of widely used classes such as variables, constants,. distributions and combinations thereof. For example:. ```. root [1] // Create two Gaussian PDFs g1(x,mean1,sigma) anf g2(x,mean2,sigma) and their parameters. root [2] RooRealVar mean(""mean"",""mean of gaussians"",5,0,10) ;. root [3] RooRealVar sigma1(""sigma1"",""width of gaussians"",0.5) ;. root [4] RooRealVar sigma2(""sigma2"",""width of gaussians"",1) ;. root [5] RooGaussian sig1(""sig1"",""Signal component 1"",x,mean,sigma1) ;. root [6] RooGaussian sig2(""sig2"",""Signal component 2"",x,mean,sigma2) ;. root [7] // Build Chebychev polynomial p.d.f. root [8] RooRealVar a0(""a0"",""a0"",0.5,0.,1.) ;. root [9] RooRealVar a1(""a1"",""a1"",0.2,0.,1.) ;. root [10] RooChebychev bkg(""bkg"",""Background"",x,RooArgSet(a0,a1)) ;. root [11] // Sum the signal components into a composite signal p.d.f. root [12] RooRealVar sig1frac(""sig1frac"",""fraction of component 1 in signal"",0.8,0.,1.) ;. root [13] RooAddPdf sig(""sig"",""Signal"",RooArgList(sig1,sig2),sig1frac) ;. root [14] // Sum the composite signal and background. root [15] RooRealVar bkgfrac(""bkgfrac"",""fraction of background"",0.5,0.,1.) ;. root [16] RooAddPdf model(""model"",""g1+g2+a"",RooArgList(bkg,sig),bkgfrac) ;. root [17] model. (RooAddPdf &) RooAddPdf::model[ bkgfrac * bkg + [%] * sig ] = 0.9. root [18] sig. (RooAddPdf &) RooAddPdf::sig[ sig1frac * sig1 + [%] * sig2 ] = 1. root [19] sig1. (RooGaussian &) RooGaussian::sig1[ x=x mean=mean sigma=sigma1 ] = 1. root [20] sigma1. (RooRealVar &) RooRealVar::sigma1 = 0.5 C L(-INF - +INF) . root [21] mean. (RooRealVar &) RooRealVar::mean = 5 L(0 - 10) . ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2576
https://github.com/root-project/root/pull/2576:880,security,sign,signal,880,"[RooFit] Add cling value printing for RooAbsArg; therewith for tens of widely used classes such as variables, constants,. distributions and combinations thereof. For example:. ```. root [1] // Create two Gaussian PDFs g1(x,mean1,sigma) anf g2(x,mean2,sigma) and their parameters. root [2] RooRealVar mean(""mean"",""mean of gaussians"",5,0,10) ;. root [3] RooRealVar sigma1(""sigma1"",""width of gaussians"",0.5) ;. root [4] RooRealVar sigma2(""sigma2"",""width of gaussians"",1) ;. root [5] RooGaussian sig1(""sig1"",""Signal component 1"",x,mean,sigma1) ;. root [6] RooGaussian sig2(""sig2"",""Signal component 2"",x,mean,sigma2) ;. root [7] // Build Chebychev polynomial p.d.f. root [8] RooRealVar a0(""a0"",""a0"",0.5,0.,1.) ;. root [9] RooRealVar a1(""a1"",""a1"",0.2,0.,1.) ;. root [10] RooChebychev bkg(""bkg"",""Background"",x,RooArgSet(a0,a1)) ;. root [11] // Sum the signal components into a composite signal p.d.f. root [12] RooRealVar sig1frac(""sig1frac"",""fraction of component 1 in signal"",0.8,0.,1.) ;. root [13] RooAddPdf sig(""sig"",""Signal"",RooArgList(sig1,sig2),sig1frac) ;. root [14] // Sum the composite signal and background. root [15] RooRealVar bkgfrac(""bkgfrac"",""fraction of background"",0.5,0.,1.) ;. root [16] RooAddPdf model(""model"",""g1+g2+a"",RooArgList(bkg,sig),bkgfrac) ;. root [17] model. (RooAddPdf &) RooAddPdf::model[ bkgfrac * bkg + [%] * sig ] = 0.9. root [18] sig. (RooAddPdf &) RooAddPdf::sig[ sig1frac * sig1 + [%] * sig2 ] = 1. root [19] sig1. (RooGaussian &) RooGaussian::sig1[ x=x mean=mean sigma=sigma1 ] = 1. root [20] sigma1. (RooRealVar &) RooRealVar::sigma1 = 0.5 C L(-INF - +INF) . root [21] mean. (RooRealVar &) RooRealVar::mean = 5 L(0 - 10) . ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2576
https://github.com/root-project/root/pull/2576:963,security,sign,signal,963,"[RooFit] Add cling value printing for RooAbsArg; therewith for tens of widely used classes such as variables, constants,. distributions and combinations thereof. For example:. ```. root [1] // Create two Gaussian PDFs g1(x,mean1,sigma) anf g2(x,mean2,sigma) and their parameters. root [2] RooRealVar mean(""mean"",""mean of gaussians"",5,0,10) ;. root [3] RooRealVar sigma1(""sigma1"",""width of gaussians"",0.5) ;. root [4] RooRealVar sigma2(""sigma2"",""width of gaussians"",1) ;. root [5] RooGaussian sig1(""sig1"",""Signal component 1"",x,mean,sigma1) ;. root [6] RooGaussian sig2(""sig2"",""Signal component 2"",x,mean,sigma2) ;. root [7] // Build Chebychev polynomial p.d.f. root [8] RooRealVar a0(""a0"",""a0"",0.5,0.,1.) ;. root [9] RooRealVar a1(""a1"",""a1"",0.2,0.,1.) ;. root [10] RooChebychev bkg(""bkg"",""Background"",x,RooArgSet(a0,a1)) ;. root [11] // Sum the signal components into a composite signal p.d.f. root [12] RooRealVar sig1frac(""sig1frac"",""fraction of component 1 in signal"",0.8,0.,1.) ;. root [13] RooAddPdf sig(""sig"",""Signal"",RooArgList(sig1,sig2),sig1frac) ;. root [14] // Sum the composite signal and background. root [15] RooRealVar bkgfrac(""bkgfrac"",""fraction of background"",0.5,0.,1.) ;. root [16] RooAddPdf model(""model"",""g1+g2+a"",RooArgList(bkg,sig),bkgfrac) ;. root [17] model. (RooAddPdf &) RooAddPdf::model[ bkgfrac * bkg + [%] * sig ] = 0.9. root [18] sig. (RooAddPdf &) RooAddPdf::sig[ sig1frac * sig1 + [%] * sig2 ] = 1. root [19] sig1. (RooGaussian &) RooGaussian::sig1[ x=x mean=mean sigma=sigma1 ] = 1. root [20] sigma1. (RooRealVar &) RooRealVar::sigma1 = 0.5 C L(-INF - +INF) . root [21] mean. (RooRealVar &) RooRealVar::mean = 5 L(0 - 10) . ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2576
https://github.com/root-project/root/pull/2576:1016,security,Sign,Signal,1016,"[RooFit] Add cling value printing for RooAbsArg; therewith for tens of widely used classes such as variables, constants,. distributions and combinations thereof. For example:. ```. root [1] // Create two Gaussian PDFs g1(x,mean1,sigma) anf g2(x,mean2,sigma) and their parameters. root [2] RooRealVar mean(""mean"",""mean of gaussians"",5,0,10) ;. root [3] RooRealVar sigma1(""sigma1"",""width of gaussians"",0.5) ;. root [4] RooRealVar sigma2(""sigma2"",""width of gaussians"",1) ;. root [5] RooGaussian sig1(""sig1"",""Signal component 1"",x,mean,sigma1) ;. root [6] RooGaussian sig2(""sig2"",""Signal component 2"",x,mean,sigma2) ;. root [7] // Build Chebychev polynomial p.d.f. root [8] RooRealVar a0(""a0"",""a0"",0.5,0.,1.) ;. root [9] RooRealVar a1(""a1"",""a1"",0.2,0.,1.) ;. root [10] RooChebychev bkg(""bkg"",""Background"",x,RooArgSet(a0,a1)) ;. root [11] // Sum the signal components into a composite signal p.d.f. root [12] RooRealVar sig1frac(""sig1frac"",""fraction of component 1 in signal"",0.8,0.,1.) ;. root [13] RooAddPdf sig(""sig"",""Signal"",RooArgList(sig1,sig2),sig1frac) ;. root [14] // Sum the composite signal and background. root [15] RooRealVar bkgfrac(""bkgfrac"",""fraction of background"",0.5,0.,1.) ;. root [16] RooAddPdf model(""model"",""g1+g2+a"",RooArgList(bkg,sig),bkgfrac) ;. root [17] model. (RooAddPdf &) RooAddPdf::model[ bkgfrac * bkg + [%] * sig ] = 0.9. root [18] sig. (RooAddPdf &) RooAddPdf::sig[ sig1frac * sig1 + [%] * sig2 ] = 1. root [19] sig1. (RooGaussian &) RooGaussian::sig1[ x=x mean=mean sigma=sigma1 ] = 1. root [20] sigma1. (RooRealVar &) RooRealVar::sigma1 = 0.5 C L(-INF - +INF) . root [21] mean. (RooRealVar &) RooRealVar::mean = 5 L(0 - 10) . ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2576
https://github.com/root-project/root/pull/2576:1090,security,sign,signal,1090,"[RooFit] Add cling value printing for RooAbsArg; therewith for tens of widely used classes such as variables, constants,. distributions and combinations thereof. For example:. ```. root [1] // Create two Gaussian PDFs g1(x,mean1,sigma) anf g2(x,mean2,sigma) and their parameters. root [2] RooRealVar mean(""mean"",""mean of gaussians"",5,0,10) ;. root [3] RooRealVar sigma1(""sigma1"",""width of gaussians"",0.5) ;. root [4] RooRealVar sigma2(""sigma2"",""width of gaussians"",1) ;. root [5] RooGaussian sig1(""sig1"",""Signal component 1"",x,mean,sigma1) ;. root [6] RooGaussian sig2(""sig2"",""Signal component 2"",x,mean,sigma2) ;. root [7] // Build Chebychev polynomial p.d.f. root [8] RooRealVar a0(""a0"",""a0"",0.5,0.,1.) ;. root [9] RooRealVar a1(""a1"",""a1"",0.2,0.,1.) ;. root [10] RooChebychev bkg(""bkg"",""Background"",x,RooArgSet(a0,a1)) ;. root [11] // Sum the signal components into a composite signal p.d.f. root [12] RooRealVar sig1frac(""sig1frac"",""fraction of component 1 in signal"",0.8,0.,1.) ;. root [13] RooAddPdf sig(""sig"",""Signal"",RooArgList(sig1,sig2),sig1frac) ;. root [14] // Sum the composite signal and background. root [15] RooRealVar bkgfrac(""bkgfrac"",""fraction of background"",0.5,0.,1.) ;. root [16] RooAddPdf model(""model"",""g1+g2+a"",RooArgList(bkg,sig),bkgfrac) ;. root [17] model. (RooAddPdf &) RooAddPdf::model[ bkgfrac * bkg + [%] * sig ] = 0.9. root [18] sig. (RooAddPdf &) RooAddPdf::sig[ sig1frac * sig1 + [%] * sig2 ] = 1. root [19] sig1. (RooGaussian &) RooGaussian::sig1[ x=x mean=mean sigma=sigma1 ] = 1. root [20] sigma1. (RooRealVar &) RooRealVar::sigma1 = 0.5 C L(-INF - +INF) . root [21] mean. (RooRealVar &) RooRealVar::mean = 5 L(0 - 10) . ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2576
https://github.com/root-project/root/pull/2576:1211,security,model,model,1211,"[RooFit] Add cling value printing for RooAbsArg; therewith for tens of widely used classes such as variables, constants,. distributions and combinations thereof. For example:. ```. root [1] // Create two Gaussian PDFs g1(x,mean1,sigma) anf g2(x,mean2,sigma) and their parameters. root [2] RooRealVar mean(""mean"",""mean of gaussians"",5,0,10) ;. root [3] RooRealVar sigma1(""sigma1"",""width of gaussians"",0.5) ;. root [4] RooRealVar sigma2(""sigma2"",""width of gaussians"",1) ;. root [5] RooGaussian sig1(""sig1"",""Signal component 1"",x,mean,sigma1) ;. root [6] RooGaussian sig2(""sig2"",""Signal component 2"",x,mean,sigma2) ;. root [7] // Build Chebychev polynomial p.d.f. root [8] RooRealVar a0(""a0"",""a0"",0.5,0.,1.) ;. root [9] RooRealVar a1(""a1"",""a1"",0.2,0.,1.) ;. root [10] RooChebychev bkg(""bkg"",""Background"",x,RooArgSet(a0,a1)) ;. root [11] // Sum the signal components into a composite signal p.d.f. root [12] RooRealVar sig1frac(""sig1frac"",""fraction of component 1 in signal"",0.8,0.,1.) ;. root [13] RooAddPdf sig(""sig"",""Signal"",RooArgList(sig1,sig2),sig1frac) ;. root [14] // Sum the composite signal and background. root [15] RooRealVar bkgfrac(""bkgfrac"",""fraction of background"",0.5,0.,1.) ;. root [16] RooAddPdf model(""model"",""g1+g2+a"",RooArgList(bkg,sig),bkgfrac) ;. root [17] model. (RooAddPdf &) RooAddPdf::model[ bkgfrac * bkg + [%] * sig ] = 0.9. root [18] sig. (RooAddPdf &) RooAddPdf::sig[ sig1frac * sig1 + [%] * sig2 ] = 1. root [19] sig1. (RooGaussian &) RooGaussian::sig1[ x=x mean=mean sigma=sigma1 ] = 1. root [20] sigma1. (RooRealVar &) RooRealVar::sigma1 = 0.5 C L(-INF - +INF) . root [21] mean. (RooRealVar &) RooRealVar::mean = 5 L(0 - 10) . ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2576
https://github.com/root-project/root/pull/2576:1218,security,model,model,1218,"[RooFit] Add cling value printing for RooAbsArg; therewith for tens of widely used classes such as variables, constants,. distributions and combinations thereof. For example:. ```. root [1] // Create two Gaussian PDFs g1(x,mean1,sigma) anf g2(x,mean2,sigma) and their parameters. root [2] RooRealVar mean(""mean"",""mean of gaussians"",5,0,10) ;. root [3] RooRealVar sigma1(""sigma1"",""width of gaussians"",0.5) ;. root [4] RooRealVar sigma2(""sigma2"",""width of gaussians"",1) ;. root [5] RooGaussian sig1(""sig1"",""Signal component 1"",x,mean,sigma1) ;. root [6] RooGaussian sig2(""sig2"",""Signal component 2"",x,mean,sigma2) ;. root [7] // Build Chebychev polynomial p.d.f. root [8] RooRealVar a0(""a0"",""a0"",0.5,0.,1.) ;. root [9] RooRealVar a1(""a1"",""a1"",0.2,0.,1.) ;. root [10] RooChebychev bkg(""bkg"",""Background"",x,RooArgSet(a0,a1)) ;. root [11] // Sum the signal components into a composite signal p.d.f. root [12] RooRealVar sig1frac(""sig1frac"",""fraction of component 1 in signal"",0.8,0.,1.) ;. root [13] RooAddPdf sig(""sig"",""Signal"",RooArgList(sig1,sig2),sig1frac) ;. root [14] // Sum the composite signal and background. root [15] RooRealVar bkgfrac(""bkgfrac"",""fraction of background"",0.5,0.,1.) ;. root [16] RooAddPdf model(""model"",""g1+g2+a"",RooArgList(bkg,sig),bkgfrac) ;. root [17] model. (RooAddPdf &) RooAddPdf::model[ bkgfrac * bkg + [%] * sig ] = 0.9. root [18] sig. (RooAddPdf &) RooAddPdf::sig[ sig1frac * sig1 + [%] * sig2 ] = 1. root [19] sig1. (RooGaussian &) RooGaussian::sig1[ x=x mean=mean sigma=sigma1 ] = 1. root [20] sigma1. (RooRealVar &) RooRealVar::sigma1 = 0.5 C L(-INF - +INF) . root [21] mean. (RooRealVar &) RooRealVar::mean = 5 L(0 - 10) . ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2576
https://github.com/root-project/root/pull/2576:1277,security,model,model,1277,"[RooFit] Add cling value printing for RooAbsArg; therewith for tens of widely used classes such as variables, constants,. distributions and combinations thereof. For example:. ```. root [1] // Create two Gaussian PDFs g1(x,mean1,sigma) anf g2(x,mean2,sigma) and their parameters. root [2] RooRealVar mean(""mean"",""mean of gaussians"",5,0,10) ;. root [3] RooRealVar sigma1(""sigma1"",""width of gaussians"",0.5) ;. root [4] RooRealVar sigma2(""sigma2"",""width of gaussians"",1) ;. root [5] RooGaussian sig1(""sig1"",""Signal component 1"",x,mean,sigma1) ;. root [6] RooGaussian sig2(""sig2"",""Signal component 2"",x,mean,sigma2) ;. root [7] // Build Chebychev polynomial p.d.f. root [8] RooRealVar a0(""a0"",""a0"",0.5,0.,1.) ;. root [9] RooRealVar a1(""a1"",""a1"",0.2,0.,1.) ;. root [10] RooChebychev bkg(""bkg"",""Background"",x,RooArgSet(a0,a1)) ;. root [11] // Sum the signal components into a composite signal p.d.f. root [12] RooRealVar sig1frac(""sig1frac"",""fraction of component 1 in signal"",0.8,0.,1.) ;. root [13] RooAddPdf sig(""sig"",""Signal"",RooArgList(sig1,sig2),sig1frac) ;. root [14] // Sum the composite signal and background. root [15] RooRealVar bkgfrac(""bkgfrac"",""fraction of background"",0.5,0.,1.) ;. root [16] RooAddPdf model(""model"",""g1+g2+a"",RooArgList(bkg,sig),bkgfrac) ;. root [17] model. (RooAddPdf &) RooAddPdf::model[ bkgfrac * bkg + [%] * sig ] = 0.9. root [18] sig. (RooAddPdf &) RooAddPdf::sig[ sig1frac * sig1 + [%] * sig2 ] = 1. root [19] sig1. (RooGaussian &) RooGaussian::sig1[ x=x mean=mean sigma=sigma1 ] = 1. root [20] sigma1. (RooRealVar &) RooRealVar::sigma1 = 0.5 C L(-INF - +INF) . root [21] mean. (RooRealVar &) RooRealVar::mean = 5 L(0 - 10) . ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2576
https://github.com/root-project/root/pull/2576:1309,security,model,model,1309,"[RooFit] Add cling value printing for RooAbsArg; therewith for tens of widely used classes such as variables, constants,. distributions and combinations thereof. For example:. ```. root [1] // Create two Gaussian PDFs g1(x,mean1,sigma) anf g2(x,mean2,sigma) and their parameters. root [2] RooRealVar mean(""mean"",""mean of gaussians"",5,0,10) ;. root [3] RooRealVar sigma1(""sigma1"",""width of gaussians"",0.5) ;. root [4] RooRealVar sigma2(""sigma2"",""width of gaussians"",1) ;. root [5] RooGaussian sig1(""sig1"",""Signal component 1"",x,mean,sigma1) ;. root [6] RooGaussian sig2(""sig2"",""Signal component 2"",x,mean,sigma2) ;. root [7] // Build Chebychev polynomial p.d.f. root [8] RooRealVar a0(""a0"",""a0"",0.5,0.,1.) ;. root [9] RooRealVar a1(""a1"",""a1"",0.2,0.,1.) ;. root [10] RooChebychev bkg(""bkg"",""Background"",x,RooArgSet(a0,a1)) ;. root [11] // Sum the signal components into a composite signal p.d.f. root [12] RooRealVar sig1frac(""sig1frac"",""fraction of component 1 in signal"",0.8,0.,1.) ;. root [13] RooAddPdf sig(""sig"",""Signal"",RooArgList(sig1,sig2),sig1frac) ;. root [14] // Sum the composite signal and background. root [15] RooRealVar bkgfrac(""bkgfrac"",""fraction of background"",0.5,0.,1.) ;. root [16] RooAddPdf model(""model"",""g1+g2+a"",RooArgList(bkg,sig),bkgfrac) ;. root [17] model. (RooAddPdf &) RooAddPdf::model[ bkgfrac * bkg + [%] * sig ] = 0.9. root [18] sig. (RooAddPdf &) RooAddPdf::sig[ sig1frac * sig1 + [%] * sig2 ] = 1. root [19] sig1. (RooGaussian &) RooGaussian::sig1[ x=x mean=mean sigma=sigma1 ] = 1. root [20] sigma1. (RooRealVar &) RooRealVar::sigma1 = 0.5 C L(-INF - +INF) . root [21] mean. (RooRealVar &) RooRealVar::mean = 5 L(0 - 10) . ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2576
https://github.com/root-project/root/pull/2577:82,availability,failur,failure,82,"[cxxmodule] Fix nightlies; It looks these two commits are causing the compilation failure. The problem is in cmake, but it's complicated than it looks. I will fix and send this patch again, but first we really need to fix nightlies compilation failure.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2577
https://github.com/root-project/root/pull/2577:244,availability,failur,failure,244,"[cxxmodule] Fix nightlies; It looks these two commits are causing the compilation failure. The problem is in cmake, but it's complicated than it looks. I will fix and send this patch again, but first we really need to fix nightlies compilation failure.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2577
https://github.com/root-project/root/pull/2577:82,deployability,fail,failure,82,"[cxxmodule] Fix nightlies; It looks these two commits are causing the compilation failure. The problem is in cmake, but it's complicated than it looks. I will fix and send this patch again, but first we really need to fix nightlies compilation failure.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2577
https://github.com/root-project/root/pull/2577:177,deployability,patch,patch,177,"[cxxmodule] Fix nightlies; It looks these two commits are causing the compilation failure. The problem is in cmake, but it's complicated than it looks. I will fix and send this patch again, but first we really need to fix nightlies compilation failure.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2577
https://github.com/root-project/root/pull/2577:244,deployability,fail,failure,244,"[cxxmodule] Fix nightlies; It looks these two commits are causing the compilation failure. The problem is in cmake, but it's complicated than it looks. I will fix and send this patch again, but first we really need to fix nightlies compilation failure.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2577
https://github.com/root-project/root/pull/2577:82,performance,failur,failure,82,"[cxxmodule] Fix nightlies; It looks these two commits are causing the compilation failure. The problem is in cmake, but it's complicated than it looks. I will fix and send this patch again, but first we really need to fix nightlies compilation failure.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2577
https://github.com/root-project/root/pull/2577:244,performance,failur,failure,244,"[cxxmodule] Fix nightlies; It looks these two commits are causing the compilation failure. The problem is in cmake, but it's complicated than it looks. I will fix and send this patch again, but first we really need to fix nightlies compilation failure.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2577
https://github.com/root-project/root/pull/2577:82,reliability,fail,failure,82,"[cxxmodule] Fix nightlies; It looks these two commits are causing the compilation failure. The problem is in cmake, but it's complicated than it looks. I will fix and send this patch again, but first we really need to fix nightlies compilation failure.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2577
https://github.com/root-project/root/pull/2577:244,reliability,fail,failure,244,"[cxxmodule] Fix nightlies; It looks these two commits are causing the compilation failure. The problem is in cmake, but it's complicated than it looks. I will fix and send this patch again, but first we really need to fix nightlies compilation failure.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2577
https://github.com/root-project/root/pull/2577:125,safety,compl,complicated,125,"[cxxmodule] Fix nightlies; It looks these two commits are causing the compilation failure. The problem is in cmake, but it's complicated than it looks. I will fix and send this patch again, but first we really need to fix nightlies compilation failure.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2577
https://github.com/root-project/root/pull/2577:177,safety,patch,patch,177,"[cxxmodule] Fix nightlies; It looks these two commits are causing the compilation failure. The problem is in cmake, but it's complicated than it looks. I will fix and send this patch again, but first we really need to fix nightlies compilation failure.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2577
https://github.com/root-project/root/pull/2577:125,security,compl,complicated,125,"[cxxmodule] Fix nightlies; It looks these two commits are causing the compilation failure. The problem is in cmake, but it's complicated than it looks. I will fix and send this patch again, but first we really need to fix nightlies compilation failure.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2577
https://github.com/root-project/root/pull/2577:177,security,patch,patch,177,"[cxxmodule] Fix nightlies; It looks these two commits are causing the compilation failure. The problem is in cmake, but it's complicated than it looks. I will fix and send this patch again, but first we really need to fix nightlies compilation failure.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2577
https://github.com/root-project/root/pull/2578:20,modifiability,layer,layer,20,[WIP] add Davix vfs layer to sqlite ds; This PR has a customised vfs layer and it can read remote http(s) files.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2578
https://github.com/root-project/root/pull/2578:69,modifiability,layer,layer,69,[WIP] add Davix vfs layer to sqlite ds; This PR has a customised vfs layer and it can read remote http(s) files.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2578
https://github.com/root-project/root/pull/2578:54,usability,custom,customised,54,[WIP] add Davix vfs layer to sqlite ds; This PR has a customised vfs layer and it can read remote http(s) files.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2578
https://github.com/root-project/root/pull/2579:461,safety,Test,Test,461,"[TMVA] Fixes ROOT-9444 -- Global symbol pollutes namespace; The `PrintMatrix` was declared in the global namespace which means that if you define your own PrintMatrix macro you can have a name clash. This PR ""qualifies"" the name using the prefix `TMVA_DNN_`. Verified to be working as intended with compiled code both with `DEBUG_TMVA_TCPUMATRIX` defined and not using code. ```. TMVA::DNN::TCpuMatrix<double> t{3, 3};. t.Print();. TMVA_DNN_PrintTCpuMatrix(t, ""Test"");. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2579
https://github.com/root-project/root/pull/2579:259,testability,Verif,Verified,259,"[TMVA] Fixes ROOT-9444 -- Global symbol pollutes namespace; The `PrintMatrix` was declared in the global namespace which means that if you define your own PrintMatrix macro you can have a name clash. This PR ""qualifies"" the name using the prefix `TMVA_DNN_`. Verified to be working as intended with compiled code both with `DEBUG_TMVA_TCPUMATRIX` defined and not using code. ```. TMVA::DNN::TCpuMatrix<double> t{3, 3};. t.Print();. TMVA_DNN_PrintTCpuMatrix(t, ""Test"");. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2579
https://github.com/root-project/root/pull/2579:461,testability,Test,Test,461,"[TMVA] Fixes ROOT-9444 -- Global symbol pollutes namespace; The `PrintMatrix` was declared in the global namespace which means that if you define your own PrintMatrix macro you can have a name clash. This PR ""qualifies"" the name using the prefix `TMVA_DNN_`. Verified to be working as intended with compiled code both with `DEBUG_TMVA_TCPUMATRIX` defined and not using code. ```. TMVA::DNN::TCpuMatrix<double> t{3, 3};. t.Print();. TMVA_DNN_PrintTCpuMatrix(t, ""Test"");. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2579
https://github.com/root-project/root/pull/2580:16,integrability,discover,discovered,16,Remove warnings discovered by -Wunused-private-field (Clang 8); Fix in LLVM trunk: https://reviews.llvm.org/rL317091,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2580
https://github.com/root-project/root/pull/2580:16,interoperability,discover,discovered,16,Remove warnings discovered by -Wunused-private-field (Clang 8); Fix in LLVM trunk: https://reviews.llvm.org/rL317091,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2580
https://github.com/root-project/root/pull/2580:91,safety,review,reviews,91,Remove warnings discovered by -Wunused-private-field (Clang 8); Fix in LLVM trunk: https://reviews.llvm.org/rL317091,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2580
https://github.com/root-project/root/pull/2580:91,testability,review,reviews,91,Remove warnings discovered by -Wunused-private-field (Clang 8); Fix in LLVM trunk: https://reviews.llvm.org/rL317091,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2580
https://github.com/root-project/root/pull/2580:16,usability,discov,discovered,16,Remove warnings discovered by -Wunused-private-field (Clang 8); Fix in LLVM trunk: https://reviews.llvm.org/rL317091,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2580
https://github.com/root-project/root/pull/2581:394,availability,state,statements,394,"Fix civetweb warning for combination clang/linux; https://github.com/root-project/root/pull/1966 didn't help to remove warning visible with Clang 8.0.0:. /.../root/net/http/civetweb/civetweb.c:2701:8:warning: implicit declaration of function 'pthread_setname_np' is invalid in C99 [-Wimplicit-function-declaration]. (void)pthread_setname_np(pthread_self(), threadName);. Changing order of else-statements help to provide a right solution for the case of clang/linux combination, where pthread_setname_np() is not available but in the same time _GNU_SOURCE is defined.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2581
https://github.com/root-project/root/pull/2581:513,availability,avail,available,513,"Fix civetweb warning for combination clang/linux; https://github.com/root-project/root/pull/1966 didn't help to remove warning visible with Clang 8.0.0:. /.../root/net/http/civetweb/civetweb.c:2701:8:warning: implicit declaration of function 'pthread_setname_np' is invalid in C99 [-Wimplicit-function-declaration]. (void)pthread_setname_np(pthread_self(), threadName);. Changing order of else-statements help to provide a right solution for the case of clang/linux combination, where pthread_setname_np() is not available but in the same time _GNU_SOURCE is defined.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2581
https://github.com/root-project/root/pull/2581:394,integrability,state,statements,394,"Fix civetweb warning for combination clang/linux; https://github.com/root-project/root/pull/1966 didn't help to remove warning visible with Clang 8.0.0:. /.../root/net/http/civetweb/civetweb.c:2701:8:warning: implicit declaration of function 'pthread_setname_np' is invalid in C99 [-Wimplicit-function-declaration]. (void)pthread_setname_np(pthread_self(), threadName);. Changing order of else-statements help to provide a right solution for the case of clang/linux combination, where pthread_setname_np() is not available but in the same time _GNU_SOURCE is defined.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2581
https://github.com/root-project/root/pull/2581:539,performance,time,time,539,"Fix civetweb warning for combination clang/linux; https://github.com/root-project/root/pull/1966 didn't help to remove warning visible with Clang 8.0.0:. /.../root/net/http/civetweb/civetweb.c:2701:8:warning: implicit declaration of function 'pthread_setname_np' is invalid in C99 [-Wimplicit-function-declaration]. (void)pthread_setname_np(pthread_self(), threadName);. Changing order of else-statements help to provide a right solution for the case of clang/linux combination, where pthread_setname_np() is not available but in the same time _GNU_SOURCE is defined.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2581
https://github.com/root-project/root/pull/2581:513,reliability,availab,available,513,"Fix civetweb warning for combination clang/linux; https://github.com/root-project/root/pull/1966 didn't help to remove warning visible with Clang 8.0.0:. /.../root/net/http/civetweb/civetweb.c:2701:8:warning: implicit declaration of function 'pthread_setname_np' is invalid in C99 [-Wimplicit-function-declaration]. (void)pthread_setname_np(pthread_self(), threadName);. Changing order of else-statements help to provide a right solution for the case of clang/linux combination, where pthread_setname_np() is not available but in the same time _GNU_SOURCE is defined.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2581
https://github.com/root-project/root/pull/2581:513,safety,avail,available,513,"Fix civetweb warning for combination clang/linux; https://github.com/root-project/root/pull/1966 didn't help to remove warning visible with Clang 8.0.0:. /.../root/net/http/civetweb/civetweb.c:2701:8:warning: implicit declaration of function 'pthread_setname_np' is invalid in C99 [-Wimplicit-function-declaration]. (void)pthread_setname_np(pthread_self(), threadName);. Changing order of else-statements help to provide a right solution for the case of clang/linux combination, where pthread_setname_np() is not available but in the same time _GNU_SOURCE is defined.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2581
https://github.com/root-project/root/pull/2581:513,security,availab,available,513,"Fix civetweb warning for combination clang/linux; https://github.com/root-project/root/pull/1966 didn't help to remove warning visible with Clang 8.0.0:. /.../root/net/http/civetweb/civetweb.c:2701:8:warning: implicit declaration of function 'pthread_setname_np' is invalid in C99 [-Wimplicit-function-declaration]. (void)pthread_setname_np(pthread_self(), threadName);. Changing order of else-statements help to provide a right solution for the case of clang/linux combination, where pthread_setname_np() is not available but in the same time _GNU_SOURCE is defined.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2581
https://github.com/root-project/root/pull/2581:104,usability,help,help,104,"Fix civetweb warning for combination clang/linux; https://github.com/root-project/root/pull/1966 didn't help to remove warning visible with Clang 8.0.0:. /.../root/net/http/civetweb/civetweb.c:2701:8:warning: implicit declaration of function 'pthread_setname_np' is invalid in C99 [-Wimplicit-function-declaration]. (void)pthread_setname_np(pthread_self(), threadName);. Changing order of else-statements help to provide a right solution for the case of clang/linux combination, where pthread_setname_np() is not available but in the same time _GNU_SOURCE is defined.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2581
https://github.com/root-project/root/pull/2581:405,usability,help,help,405,"Fix civetweb warning for combination clang/linux; https://github.com/root-project/root/pull/1966 didn't help to remove warning visible with Clang 8.0.0:. /.../root/net/http/civetweb/civetweb.c:2701:8:warning: implicit declaration of function 'pthread_setname_np' is invalid in C99 [-Wimplicit-function-declaration]. (void)pthread_setname_np(pthread_self(), threadName);. Changing order of else-statements help to provide a right solution for the case of clang/linux combination, where pthread_setname_np() is not available but in the same time _GNU_SOURCE is defined.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2581
https://github.com/root-project/root/pull/2582:155,deployability,modul,module,155,Fix a warning about redefinition of DNNCPU macro for Clang 8.0.0; Fix to avoid redefinition of injected value of macro in test stressTMVA. Visible for C++ module build for Clang 8.0.0:. ```. In file included from <module-includes>:26: ../build/include/TMVA/MethodDNN.h:56:9: warning: 'DNNCPU' macro redefined [-Wmacro-redefined]. #define DNNCPU. While building module 'TMVA' imported from /.../root/test/stressTMVA.cxx:70: In file included from <built-in>:379:. #define DNNCPU 1. ```.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2582
https://github.com/root-project/root/pull/2582:162,deployability,build,build,162,Fix a warning about redefinition of DNNCPU macro for Clang 8.0.0; Fix to avoid redefinition of injected value of macro in test stressTMVA. Visible for C++ module build for Clang 8.0.0:. ```. In file included from <module-includes>:26: ../build/include/TMVA/MethodDNN.h:56:9: warning: 'DNNCPU' macro redefined [-Wmacro-redefined]. #define DNNCPU. While building module 'TMVA' imported from /.../root/test/stressTMVA.cxx:70: In file included from <built-in>:379:. #define DNNCPU 1. ```.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2582
https://github.com/root-project/root/pull/2582:214,deployability,modul,module-includes,214,Fix a warning about redefinition of DNNCPU macro for Clang 8.0.0; Fix to avoid redefinition of injected value of macro in test stressTMVA. Visible for C++ module build for Clang 8.0.0:. ```. In file included from <module-includes>:26: ../build/include/TMVA/MethodDNN.h:56:9: warning: 'DNNCPU' macro redefined [-Wmacro-redefined]. #define DNNCPU. While building module 'TMVA' imported from /.../root/test/stressTMVA.cxx:70: In file included from <built-in>:379:. #define DNNCPU 1. ```.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2582
https://github.com/root-project/root/pull/2582:238,deployability,build,build,238,Fix a warning about redefinition of DNNCPU macro for Clang 8.0.0; Fix to avoid redefinition of injected value of macro in test stressTMVA. Visible for C++ module build for Clang 8.0.0:. ```. In file included from <module-includes>:26: ../build/include/TMVA/MethodDNN.h:56:9: warning: 'DNNCPU' macro redefined [-Wmacro-redefined]. #define DNNCPU. While building module 'TMVA' imported from /.../root/test/stressTMVA.cxx:70: In file included from <built-in>:379:. #define DNNCPU 1. ```.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2582
https://github.com/root-project/root/pull/2582:352,deployability,build,building,352,Fix a warning about redefinition of DNNCPU macro for Clang 8.0.0; Fix to avoid redefinition of injected value of macro in test stressTMVA. Visible for C++ module build for Clang 8.0.0:. ```. In file included from <module-includes>:26: ../build/include/TMVA/MethodDNN.h:56:9: warning: 'DNNCPU' macro redefined [-Wmacro-redefined]. #define DNNCPU. While building module 'TMVA' imported from /.../root/test/stressTMVA.cxx:70: In file included from <built-in>:379:. #define DNNCPU 1. ```.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2582
https://github.com/root-project/root/pull/2582:361,deployability,modul,module,361,Fix a warning about redefinition of DNNCPU macro for Clang 8.0.0; Fix to avoid redefinition of injected value of macro in test stressTMVA. Visible for C++ module build for Clang 8.0.0:. ```. In file included from <module-includes>:26: ../build/include/TMVA/MethodDNN.h:56:9: warning: 'DNNCPU' macro redefined [-Wmacro-redefined]. #define DNNCPU. While building module 'TMVA' imported from /.../root/test/stressTMVA.cxx:70: In file included from <built-in>:379:. #define DNNCPU 1. ```.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2582
https://github.com/root-project/root/pull/2582:95,integrability,inject,injected,95,Fix a warning about redefinition of DNNCPU macro for Clang 8.0.0; Fix to avoid redefinition of injected value of macro in test stressTMVA. Visible for C++ module build for Clang 8.0.0:. ```. In file included from <module-includes>:26: ../build/include/TMVA/MethodDNN.h:56:9: warning: 'DNNCPU' macro redefined [-Wmacro-redefined]. #define DNNCPU. While building module 'TMVA' imported from /.../root/test/stressTMVA.cxx:70: In file included from <built-in>:379:. #define DNNCPU 1. ```.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2582
https://github.com/root-project/root/pull/2582:155,modifiability,modul,module,155,Fix a warning about redefinition of DNNCPU macro for Clang 8.0.0; Fix to avoid redefinition of injected value of macro in test stressTMVA. Visible for C++ module build for Clang 8.0.0:. ```. In file included from <module-includes>:26: ../build/include/TMVA/MethodDNN.h:56:9: warning: 'DNNCPU' macro redefined [-Wmacro-redefined]. #define DNNCPU. While building module 'TMVA' imported from /.../root/test/stressTMVA.cxx:70: In file included from <built-in>:379:. #define DNNCPU 1. ```.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2582
https://github.com/root-project/root/pull/2582:214,modifiability,modul,module-includes,214,Fix a warning about redefinition of DNNCPU macro for Clang 8.0.0; Fix to avoid redefinition of injected value of macro in test stressTMVA. Visible for C++ module build for Clang 8.0.0:. ```. In file included from <module-includes>:26: ../build/include/TMVA/MethodDNN.h:56:9: warning: 'DNNCPU' macro redefined [-Wmacro-redefined]. #define DNNCPU. While building module 'TMVA' imported from /.../root/test/stressTMVA.cxx:70: In file included from <built-in>:379:. #define DNNCPU 1. ```.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2582
https://github.com/root-project/root/pull/2582:361,modifiability,modul,module,361,Fix a warning about redefinition of DNNCPU macro for Clang 8.0.0; Fix to avoid redefinition of injected value of macro in test stressTMVA. Visible for C++ module build for Clang 8.0.0:. ```. In file included from <module-includes>:26: ../build/include/TMVA/MethodDNN.h:56:9: warning: 'DNNCPU' macro redefined [-Wmacro-redefined]. #define DNNCPU. While building module 'TMVA' imported from /.../root/test/stressTMVA.cxx:70: In file included from <built-in>:379:. #define DNNCPU 1. ```.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2582
https://github.com/root-project/root/pull/2582:73,safety,avoid,avoid,73,Fix a warning about redefinition of DNNCPU macro for Clang 8.0.0; Fix to avoid redefinition of injected value of macro in test stressTMVA. Visible for C++ module build for Clang 8.0.0:. ```. In file included from <module-includes>:26: ../build/include/TMVA/MethodDNN.h:56:9: warning: 'DNNCPU' macro redefined [-Wmacro-redefined]. #define DNNCPU. While building module 'TMVA' imported from /.../root/test/stressTMVA.cxx:70: In file included from <built-in>:379:. #define DNNCPU 1. ```.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2582
https://github.com/root-project/root/pull/2582:122,safety,test,test,122,Fix a warning about redefinition of DNNCPU macro for Clang 8.0.0; Fix to avoid redefinition of injected value of macro in test stressTMVA. Visible for C++ module build for Clang 8.0.0:. ```. In file included from <module-includes>:26: ../build/include/TMVA/MethodDNN.h:56:9: warning: 'DNNCPU' macro redefined [-Wmacro-redefined]. #define DNNCPU. While building module 'TMVA' imported from /.../root/test/stressTMVA.cxx:70: In file included from <built-in>:379:. #define DNNCPU 1. ```.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2582
https://github.com/root-project/root/pull/2582:155,safety,modul,module,155,Fix a warning about redefinition of DNNCPU macro for Clang 8.0.0; Fix to avoid redefinition of injected value of macro in test stressTMVA. Visible for C++ module build for Clang 8.0.0:. ```. In file included from <module-includes>:26: ../build/include/TMVA/MethodDNN.h:56:9: warning: 'DNNCPU' macro redefined [-Wmacro-redefined]. #define DNNCPU. While building module 'TMVA' imported from /.../root/test/stressTMVA.cxx:70: In file included from <built-in>:379:. #define DNNCPU 1. ```.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2582
https://github.com/root-project/root/pull/2582:214,safety,modul,module-includes,214,Fix a warning about redefinition of DNNCPU macro for Clang 8.0.0; Fix to avoid redefinition of injected value of macro in test stressTMVA. Visible for C++ module build for Clang 8.0.0:. ```. In file included from <module-includes>:26: ../build/include/TMVA/MethodDNN.h:56:9: warning: 'DNNCPU' macro redefined [-Wmacro-redefined]. #define DNNCPU. While building module 'TMVA' imported from /.../root/test/stressTMVA.cxx:70: In file included from <built-in>:379:. #define DNNCPU 1. ```.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2582
https://github.com/root-project/root/pull/2582:361,safety,modul,module,361,Fix a warning about redefinition of DNNCPU macro for Clang 8.0.0; Fix to avoid redefinition of injected value of macro in test stressTMVA. Visible for C++ module build for Clang 8.0.0:. ```. In file included from <module-includes>:26: ../build/include/TMVA/MethodDNN.h:56:9: warning: 'DNNCPU' macro redefined [-Wmacro-redefined]. #define DNNCPU. While building module 'TMVA' imported from /.../root/test/stressTMVA.cxx:70: In file included from <built-in>:379:. #define DNNCPU 1. ```.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2582
https://github.com/root-project/root/pull/2582:399,safety,test,test,399,Fix a warning about redefinition of DNNCPU macro for Clang 8.0.0; Fix to avoid redefinition of injected value of macro in test stressTMVA. Visible for C++ module build for Clang 8.0.0:. ```. In file included from <module-includes>:26: ../build/include/TMVA/MethodDNN.h:56:9: warning: 'DNNCPU' macro redefined [-Wmacro-redefined]. #define DNNCPU. While building module 'TMVA' imported from /.../root/test/stressTMVA.cxx:70: In file included from <built-in>:379:. #define DNNCPU 1. ```.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2582
https://github.com/root-project/root/pull/2582:95,security,inject,injected,95,Fix a warning about redefinition of DNNCPU macro for Clang 8.0.0; Fix to avoid redefinition of injected value of macro in test stressTMVA. Visible for C++ module build for Clang 8.0.0:. ```. In file included from <module-includes>:26: ../build/include/TMVA/MethodDNN.h:56:9: warning: 'DNNCPU' macro redefined [-Wmacro-redefined]. #define DNNCPU. While building module 'TMVA' imported from /.../root/test/stressTMVA.cxx:70: In file included from <built-in>:379:. #define DNNCPU 1. ```.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2582
https://github.com/root-project/root/pull/2582:122,testability,test,test,122,Fix a warning about redefinition of DNNCPU macro for Clang 8.0.0; Fix to avoid redefinition of injected value of macro in test stressTMVA. Visible for C++ module build for Clang 8.0.0:. ```. In file included from <module-includes>:26: ../build/include/TMVA/MethodDNN.h:56:9: warning: 'DNNCPU' macro redefined [-Wmacro-redefined]. #define DNNCPU. While building module 'TMVA' imported from /.../root/test/stressTMVA.cxx:70: In file included from <built-in>:379:. #define DNNCPU 1. ```.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2582
https://github.com/root-project/root/pull/2582:399,testability,test,test,399,Fix a warning about redefinition of DNNCPU macro for Clang 8.0.0; Fix to avoid redefinition of injected value of macro in test stressTMVA. Visible for C++ module build for Clang 8.0.0:. ```. In file included from <module-includes>:26: ../build/include/TMVA/MethodDNN.h:56:9: warning: 'DNNCPU' macro redefined [-Wmacro-redefined]. #define DNNCPU. While building module 'TMVA' imported from /.../root/test/stressTMVA.cxx:70: In file included from <built-in>:379:. #define DNNCPU 1. ```.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2582
https://github.com/root-project/root/pull/2583:90,availability,recov,recovered,90,"[DF] ROOT-9628: do not crash when dealing with corrupted files; in case keys could not be recovered, the file was made a Zombie. and caused the system to crash. Relates to https://github.com/root-project/roottest/pull/217",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2583
https://github.com/root-project/root/pull/2583:90,deployability,recov,recovered,90,"[DF] ROOT-9628: do not crash when dealing with corrupted files; in case keys could not be recovered, the file was made a Zombie. and caused the system to crash. Relates to https://github.com/root-project/roottest/pull/217",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2583
https://github.com/root-project/root/pull/2583:90,reliability,recov,recovered,90,"[DF] ROOT-9628: do not crash when dealing with corrupted files; in case keys could not be recovered, the file was made a Zombie. and caused the system to crash. Relates to https://github.com/root-project/roottest/pull/217",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2583
https://github.com/root-project/root/pull/2583:90,safety,recov,recovered,90,"[DF] ROOT-9628: do not crash when dealing with corrupted files; in case keys could not be recovered, the file was made a Zombie. and caused the system to crash. Relates to https://github.com/root-project/roottest/pull/217",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2583
https://github.com/root-project/root/pull/2583:90,security,recov,recovered,90,"[DF] ROOT-9628: do not crash when dealing with corrupted files; in case keys could not be recovered, the file was made a Zombie. and caused the system to crash. Relates to https://github.com/root-project/roottest/pull/217",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2583
https://github.com/root-project/root/pull/2586:29,integrability,batch,batchsize,29,[TMVA] root-8988 -- Validate batchsize before training; In MethodDNN one could start training with a batch size larger than the number of events in training leading to non-sensical output. TMVA now properly warns (fatal warning) and suggests a fix (decrease batch size).,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2586
https://github.com/root-project/root/pull/2586:101,integrability,batch,batch,101,[TMVA] root-8988 -- Validate batchsize before training; In MethodDNN one could start training with a batch size larger than the number of events in training leading to non-sensical output. TMVA now properly warns (fatal warning) and suggests a fix (decrease batch size).,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2586
https://github.com/root-project/root/pull/2586:138,integrability,event,events,138,[TMVA] root-8988 -- Validate batchsize before training; In MethodDNN one could start training with a batch size larger than the number of events in training leading to non-sensical output. TMVA now properly warns (fatal warning) and suggests a fix (decrease batch size).,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2586
https://github.com/root-project/root/pull/2586:258,integrability,batch,batch,258,[TMVA] root-8988 -- Validate batchsize before training; In MethodDNN one could start training with a batch size larger than the number of events in training leading to non-sensical output. TMVA now properly warns (fatal warning) and suggests a fix (decrease batch size).,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2586
https://github.com/root-project/root/pull/2586:29,performance,batch,batchsize,29,[TMVA] root-8988 -- Validate batchsize before training; In MethodDNN one could start training with a batch size larger than the number of events in training leading to non-sensical output. TMVA now properly warns (fatal warning) and suggests a fix (decrease batch size).,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2586
https://github.com/root-project/root/pull/2586:101,performance,batch,batch,101,[TMVA] root-8988 -- Validate batchsize before training; In MethodDNN one could start training with a batch size larger than the number of events in training leading to non-sensical output. TMVA now properly warns (fatal warning) and suggests a fix (decrease batch size).,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2586
https://github.com/root-project/root/pull/2586:258,performance,batch,batch,258,[TMVA] root-8988 -- Validate batchsize before training; In MethodDNN one could start training with a batch size larger than the number of events in training leading to non-sensical output. TMVA now properly warns (fatal warning) and suggests a fix (decrease batch size).,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2586
https://github.com/root-project/root/pull/2586:20,safety,Valid,Validate,20,[TMVA] root-8988 -- Validate batchsize before training; In MethodDNN one could start training with a batch size larger than the number of events in training leading to non-sensical output. TMVA now properly warns (fatal warning) and suggests a fix (decrease batch size).,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2586
https://github.com/root-project/root/pull/2586:20,security,Validat,Validate,20,[TMVA] root-8988 -- Validate batchsize before training; In MethodDNN one could start training with a batch size larger than the number of events in training leading to non-sensical output. TMVA now properly warns (fatal warning) and suggests a fix (decrease batch size).,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2586
https://github.com/root-project/root/pull/2589:20,availability,Slo,SlotStack,20,"[DF] Rename {T => R}SlotStack, move it to its own header and source files; This is the first change of a series of PRs that decouples the different RDF nodes from each other and, to make sure they stay decoupled, separates them in different headers and source files",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2589
https://github.com/root-project/root/pull/2589:124,modifiability,deco,decouples,124,"[DF] Rename {T => R}SlotStack, move it to its own header and source files; This is the first change of a series of PRs that decouples the different RDF nodes from each other and, to make sure they stay decoupled, separates them in different headers and source files",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2589
https://github.com/root-project/root/pull/2589:202,modifiability,deco,decoupled,202,"[DF] Rename {T => R}SlotStack, move it to its own header and source files; This is the first change of a series of PRs that decouples the different RDF nodes from each other and, to make sure they stay decoupled, separates them in different headers and source files",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2589
https://github.com/root-project/root/pull/2589:20,reliability,Slo,SlotStack,20,"[DF] Rename {T => R}SlotStack, move it to its own header and source files; This is the first change of a series of PRs that decouples the different RDF nodes from each other and, to make sure they stay decoupled, separates them in different headers and source files",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2589
https://github.com/root-project/root/pull/2591:306,availability,avail,available,306,[cxxmodule] Fix df tests by declaring RuntimePrintValue at Interprete…; …r init time. This declaration will be a no-op when we enable Cling module again (see. 2577). It looks that Cling users(in other part of ROOT code) are treating printValue. function as an utility function of Interpreter which must be available at their code's. initialization time (E.g. This failure was introduced by 40f3fa94677). So I think it makes sense to make this printValue function available at. Interpreter's initialization time.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2591
https://github.com/root-project/root/pull/2591:364,availability,failur,failure,364,[cxxmodule] Fix df tests by declaring RuntimePrintValue at Interprete…; …r init time. This declaration will be a no-op when we enable Cling module again (see. 2577). It looks that Cling users(in other part of ROOT code) are treating printValue. function as an utility function of Interpreter which must be available at their code's. initialization time (E.g. This failure was introduced by 40f3fa94677). So I think it makes sense to make this printValue function available at. Interpreter's initialization time.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2591
https://github.com/root-project/root/pull/2591:463,availability,avail,available,463,[cxxmodule] Fix df tests by declaring RuntimePrintValue at Interprete…; …r init time. This declaration will be a no-op when we enable Cling module again (see. 2577). It looks that Cling users(in other part of ROOT code) are treating printValue. function as an utility function of Interpreter which must be available at their code's. initialization time (E.g. This failure was introduced by 40f3fa94677). So I think it makes sense to make this printValue function available at. Interpreter's initialization time.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2591
https://github.com/root-project/root/pull/2591:140,deployability,modul,module,140,[cxxmodule] Fix df tests by declaring RuntimePrintValue at Interprete…; …r init time. This declaration will be a no-op when we enable Cling module again (see. 2577). It looks that Cling users(in other part of ROOT code) are treating printValue. function as an utility function of Interpreter which must be available at their code's. initialization time (E.g. This failure was introduced by 40f3fa94677). So I think it makes sense to make this printValue function available at. Interpreter's initialization time.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2591
https://github.com/root-project/root/pull/2591:364,deployability,fail,failure,364,[cxxmodule] Fix df tests by declaring RuntimePrintValue at Interprete…; …r init time. This declaration will be a no-op when we enable Cling module again (see. 2577). It looks that Cling users(in other part of ROOT code) are treating printValue. function as an utility function of Interpreter which must be available at their code's. initialization time (E.g. This failure was introduced by 40f3fa94677). So I think it makes sense to make this printValue function available at. Interpreter's initialization time.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2591
https://github.com/root-project/root/pull/2591:140,modifiability,modul,module,140,[cxxmodule] Fix df tests by declaring RuntimePrintValue at Interprete…; …r init time. This declaration will be a no-op when we enable Cling module again (see. 2577). It looks that Cling users(in other part of ROOT code) are treating printValue. function as an utility function of Interpreter which must be available at their code's. initialization time (E.g. This failure was introduced by 40f3fa94677). So I think it makes sense to make this printValue function available at. Interpreter's initialization time.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2591
https://github.com/root-project/root/pull/2591:80,performance,time,time,80,[cxxmodule] Fix df tests by declaring RuntimePrintValue at Interprete…; …r init time. This declaration will be a no-op when we enable Cling module again (see. 2577). It looks that Cling users(in other part of ROOT code) are treating printValue. function as an utility function of Interpreter which must be available at their code's. initialization time (E.g. This failure was introduced by 40f3fa94677). So I think it makes sense to make this printValue function available at. Interpreter's initialization time.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2591
https://github.com/root-project/root/pull/2591:348,performance,time,time,348,[cxxmodule] Fix df tests by declaring RuntimePrintValue at Interprete…; …r init time. This declaration will be a no-op when we enable Cling module again (see. 2577). It looks that Cling users(in other part of ROOT code) are treating printValue. function as an utility function of Interpreter which must be available at their code's. initialization time (E.g. This failure was introduced by 40f3fa94677). So I think it makes sense to make this printValue function available at. Interpreter's initialization time.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2591
https://github.com/root-project/root/pull/2591:364,performance,failur,failure,364,[cxxmodule] Fix df tests by declaring RuntimePrintValue at Interprete…; …r init time. This declaration will be a no-op when we enable Cling module again (see. 2577). It looks that Cling users(in other part of ROOT code) are treating printValue. function as an utility function of Interpreter which must be available at their code's. initialization time (E.g. This failure was introduced by 40f3fa94677). So I think it makes sense to make this printValue function available at. Interpreter's initialization time.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2591
https://github.com/root-project/root/pull/2591:506,performance,time,time,506,[cxxmodule] Fix df tests by declaring RuntimePrintValue at Interprete…; …r init time. This declaration will be a no-op when we enable Cling module again (see. 2577). It looks that Cling users(in other part of ROOT code) are treating printValue. function as an utility function of Interpreter which must be available at their code's. initialization time (E.g. This failure was introduced by 40f3fa94677). So I think it makes sense to make this printValue function available at. Interpreter's initialization time.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2591
https://github.com/root-project/root/pull/2591:306,reliability,availab,available,306,[cxxmodule] Fix df tests by declaring RuntimePrintValue at Interprete…; …r init time. This declaration will be a no-op when we enable Cling module again (see. 2577). It looks that Cling users(in other part of ROOT code) are treating printValue. function as an utility function of Interpreter which must be available at their code's. initialization time (E.g. This failure was introduced by 40f3fa94677). So I think it makes sense to make this printValue function available at. Interpreter's initialization time.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2591
https://github.com/root-project/root/pull/2591:364,reliability,fail,failure,364,[cxxmodule] Fix df tests by declaring RuntimePrintValue at Interprete…; …r init time. This declaration will be a no-op when we enable Cling module again (see. 2577). It looks that Cling users(in other part of ROOT code) are treating printValue. function as an utility function of Interpreter which must be available at their code's. initialization time (E.g. This failure was introduced by 40f3fa94677). So I think it makes sense to make this printValue function available at. Interpreter's initialization time.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2591
https://github.com/root-project/root/pull/2591:463,reliability,availab,available,463,[cxxmodule] Fix df tests by declaring RuntimePrintValue at Interprete…; …r init time. This declaration will be a no-op when we enable Cling module again (see. 2577). It looks that Cling users(in other part of ROOT code) are treating printValue. function as an utility function of Interpreter which must be available at their code's. initialization time (E.g. This failure was introduced by 40f3fa94677). So I think it makes sense to make this printValue function available at. Interpreter's initialization time.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2591
https://github.com/root-project/root/pull/2591:19,safety,test,tests,19,[cxxmodule] Fix df tests by declaring RuntimePrintValue at Interprete…; …r init time. This declaration will be a no-op when we enable Cling module again (see. 2577). It looks that Cling users(in other part of ROOT code) are treating printValue. function as an utility function of Interpreter which must be available at their code's. initialization time (E.g. This failure was introduced by 40f3fa94677). So I think it makes sense to make this printValue function available at. Interpreter's initialization time.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2591
https://github.com/root-project/root/pull/2591:140,safety,modul,module,140,[cxxmodule] Fix df tests by declaring RuntimePrintValue at Interprete…; …r init time. This declaration will be a no-op when we enable Cling module again (see. 2577). It looks that Cling users(in other part of ROOT code) are treating printValue. function as an utility function of Interpreter which must be available at their code's. initialization time (E.g. This failure was introduced by 40f3fa94677). So I think it makes sense to make this printValue function available at. Interpreter's initialization time.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2591
https://github.com/root-project/root/pull/2591:306,safety,avail,available,306,[cxxmodule] Fix df tests by declaring RuntimePrintValue at Interprete…; …r init time. This declaration will be a no-op when we enable Cling module again (see. 2577). It looks that Cling users(in other part of ROOT code) are treating printValue. function as an utility function of Interpreter which must be available at their code's. initialization time (E.g. This failure was introduced by 40f3fa94677). So I think it makes sense to make this printValue function available at. Interpreter's initialization time.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2591
https://github.com/root-project/root/pull/2591:463,safety,avail,available,463,[cxxmodule] Fix df tests by declaring RuntimePrintValue at Interprete…; …r init time. This declaration will be a no-op when we enable Cling module again (see. 2577). It looks that Cling users(in other part of ROOT code) are treating printValue. function as an utility function of Interpreter which must be available at their code's. initialization time (E.g. This failure was introduced by 40f3fa94677). So I think it makes sense to make this printValue function available at. Interpreter's initialization time.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2591
https://github.com/root-project/root/pull/2591:306,security,availab,available,306,[cxxmodule] Fix df tests by declaring RuntimePrintValue at Interprete…; …r init time. This declaration will be a no-op when we enable Cling module again (see. 2577). It looks that Cling users(in other part of ROOT code) are treating printValue. function as an utility function of Interpreter which must be available at their code's. initialization time (E.g. This failure was introduced by 40f3fa94677). So I think it makes sense to make this printValue function available at. Interpreter's initialization time.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2591
https://github.com/root-project/root/pull/2591:463,security,availab,available,463,[cxxmodule] Fix df tests by declaring RuntimePrintValue at Interprete…; …r init time. This declaration will be a no-op when we enable Cling module again (see. 2577). It looks that Cling users(in other part of ROOT code) are treating printValue. function as an utility function of Interpreter which must be available at their code's. initialization time (E.g. This failure was introduced by 40f3fa94677). So I think it makes sense to make this printValue function available at. Interpreter's initialization time.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2591
https://github.com/root-project/root/pull/2591:19,testability,test,tests,19,[cxxmodule] Fix df tests by declaring RuntimePrintValue at Interprete…; …r init time. This declaration will be a no-op when we enable Cling module again (see. 2577). It looks that Cling users(in other part of ROOT code) are treating printValue. function as an utility function of Interpreter which must be available at their code's. initialization time (E.g. This failure was introduced by 40f3fa94677). So I think it makes sense to make this printValue function available at. Interpreter's initialization time.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2591
https://github.com/root-project/root/pull/2591:186,usability,user,users,186,[cxxmodule] Fix df tests by declaring RuntimePrintValue at Interprete…; …r init time. This declaration will be a no-op when we enable Cling module again (see. 2577). It looks that Cling users(in other part of ROOT code) are treating printValue. function as an utility function of Interpreter which must be available at their code's. initialization time (E.g. This failure was introduced by 40f3fa94677). So I think it makes sense to make this printValue function available at. Interpreter's initialization time.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2591
https://github.com/root-project/root/pull/2592:55,deployability,modul,modules,55,"[cxxmodule] Don't declare fwd decls from rootmaps with modules; Fwd decls from rootmap files were injected to ROOT at the startup time. However with modules we don't need any foward decls as we're preloading. all modules. Our future plan is to remove rootmap files as a whole, but we still need. to support more features (see 2380). So for now we can remove what we. are not using. *edit This patch also contains adding fHasCxxModule as a private member. This improves startup memory by 10 mbytes.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2592
https://github.com/root-project/root/pull/2592:149,deployability,modul,modules,149,"[cxxmodule] Don't declare fwd decls from rootmaps with modules; Fwd decls from rootmap files were injected to ROOT at the startup time. However with modules we don't need any foward decls as we're preloading. all modules. Our future plan is to remove rootmap files as a whole, but we still need. to support more features (see 2380). So for now we can remove what we. are not using. *edit This patch also contains adding fHasCxxModule as a private member. This improves startup memory by 10 mbytes.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2592
https://github.com/root-project/root/pull/2592:213,deployability,modul,modules,213,"[cxxmodule] Don't declare fwd decls from rootmaps with modules; Fwd decls from rootmap files were injected to ROOT at the startup time. However with modules we don't need any foward decls as we're preloading. all modules. Our future plan is to remove rootmap files as a whole, but we still need. to support more features (see 2380). So for now we can remove what we. are not using. *edit This patch also contains adding fHasCxxModule as a private member. This improves startup memory by 10 mbytes.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2592
https://github.com/root-project/root/pull/2592:393,deployability,patch,patch,393,"[cxxmodule] Don't declare fwd decls from rootmaps with modules; Fwd decls from rootmap files were injected to ROOT at the startup time. However with modules we don't need any foward decls as we're preloading. all modules. Our future plan is to remove rootmap files as a whole, but we still need. to support more features (see 2380). So for now we can remove what we. are not using. *edit This patch also contains adding fHasCxxModule as a private member. This improves startup memory by 10 mbytes.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2592
https://github.com/root-project/root/pull/2592:404,deployability,contain,contains,404,"[cxxmodule] Don't declare fwd decls from rootmaps with modules; Fwd decls from rootmap files were injected to ROOT at the startup time. However with modules we don't need any foward decls as we're preloading. all modules. Our future plan is to remove rootmap files as a whole, but we still need. to support more features (see 2380). So for now we can remove what we. are not using. *edit This patch also contains adding fHasCxxModule as a private member. This improves startup memory by 10 mbytes.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2592
https://github.com/root-project/root/pull/2592:98,integrability,inject,injected,98,"[cxxmodule] Don't declare fwd decls from rootmaps with modules; Fwd decls from rootmap files were injected to ROOT at the startup time. However with modules we don't need any foward decls as we're preloading. all modules. Our future plan is to remove rootmap files as a whole, but we still need. to support more features (see 2380). So for now we can remove what we. are not using. *edit This patch also contains adding fHasCxxModule as a private member. This improves startup memory by 10 mbytes.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2592
https://github.com/root-project/root/pull/2592:55,modifiability,modul,modules,55,"[cxxmodule] Don't declare fwd decls from rootmaps with modules; Fwd decls from rootmap files were injected to ROOT at the startup time. However with modules we don't need any foward decls as we're preloading. all modules. Our future plan is to remove rootmap files as a whole, but we still need. to support more features (see 2380). So for now we can remove what we. are not using. *edit This patch also contains adding fHasCxxModule as a private member. This improves startup memory by 10 mbytes.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2592
https://github.com/root-project/root/pull/2592:149,modifiability,modul,modules,149,"[cxxmodule] Don't declare fwd decls from rootmaps with modules; Fwd decls from rootmap files were injected to ROOT at the startup time. However with modules we don't need any foward decls as we're preloading. all modules. Our future plan is to remove rootmap files as a whole, but we still need. to support more features (see 2380). So for now we can remove what we. are not using. *edit This patch also contains adding fHasCxxModule as a private member. This improves startup memory by 10 mbytes.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2592
https://github.com/root-project/root/pull/2592:213,modifiability,modul,modules,213,"[cxxmodule] Don't declare fwd decls from rootmaps with modules; Fwd decls from rootmap files were injected to ROOT at the startup time. However with modules we don't need any foward decls as we're preloading. all modules. Our future plan is to remove rootmap files as a whole, but we still need. to support more features (see 2380). So for now we can remove what we. are not using. *edit This patch also contains adding fHasCxxModule as a private member. This improves startup memory by 10 mbytes.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2592
https://github.com/root-project/root/pull/2592:130,performance,time,time,130,"[cxxmodule] Don't declare fwd decls from rootmaps with modules; Fwd decls from rootmap files were injected to ROOT at the startup time. However with modules we don't need any foward decls as we're preloading. all modules. Our future plan is to remove rootmap files as a whole, but we still need. to support more features (see 2380). So for now we can remove what we. are not using. *edit This patch also contains adding fHasCxxModule as a private member. This improves startup memory by 10 mbytes.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2592
https://github.com/root-project/root/pull/2592:477,performance,memor,memory,477,"[cxxmodule] Don't declare fwd decls from rootmaps with modules; Fwd decls from rootmap files were injected to ROOT at the startup time. However with modules we don't need any foward decls as we're preloading. all modules. Our future plan is to remove rootmap files as a whole, but we still need. to support more features (see 2380). So for now we can remove what we. are not using. *edit This patch also contains adding fHasCxxModule as a private member. This improves startup memory by 10 mbytes.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2592
https://github.com/root-project/root/pull/2592:55,safety,modul,modules,55,"[cxxmodule] Don't declare fwd decls from rootmaps with modules; Fwd decls from rootmap files were injected to ROOT at the startup time. However with modules we don't need any foward decls as we're preloading. all modules. Our future plan is to remove rootmap files as a whole, but we still need. to support more features (see 2380). So for now we can remove what we. are not using. *edit This patch also contains adding fHasCxxModule as a private member. This improves startup memory by 10 mbytes.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2592
https://github.com/root-project/root/pull/2592:149,safety,modul,modules,149,"[cxxmodule] Don't declare fwd decls from rootmaps with modules; Fwd decls from rootmap files were injected to ROOT at the startup time. However with modules we don't need any foward decls as we're preloading. all modules. Our future plan is to remove rootmap files as a whole, but we still need. to support more features (see 2380). So for now we can remove what we. are not using. *edit This patch also contains adding fHasCxxModule as a private member. This improves startup memory by 10 mbytes.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2592
https://github.com/root-project/root/pull/2592:213,safety,modul,modules,213,"[cxxmodule] Don't declare fwd decls from rootmaps with modules; Fwd decls from rootmap files were injected to ROOT at the startup time. However with modules we don't need any foward decls as we're preloading. all modules. Our future plan is to remove rootmap files as a whole, but we still need. to support more features (see 2380). So for now we can remove what we. are not using. *edit This patch also contains adding fHasCxxModule as a private member. This improves startup memory by 10 mbytes.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2592
https://github.com/root-project/root/pull/2592:393,safety,patch,patch,393,"[cxxmodule] Don't declare fwd decls from rootmaps with modules; Fwd decls from rootmap files were injected to ROOT at the startup time. However with modules we don't need any foward decls as we're preloading. all modules. Our future plan is to remove rootmap files as a whole, but we still need. to support more features (see 2380). So for now we can remove what we. are not using. *edit This patch also contains adding fHasCxxModule as a private member. This improves startup memory by 10 mbytes.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2592
https://github.com/root-project/root/pull/2592:98,security,inject,injected,98,"[cxxmodule] Don't declare fwd decls from rootmaps with modules; Fwd decls from rootmap files were injected to ROOT at the startup time. However with modules we don't need any foward decls as we're preloading. all modules. Our future plan is to remove rootmap files as a whole, but we still need. to support more features (see 2380). So for now we can remove what we. are not using. *edit This patch also contains adding fHasCxxModule as a private member. This improves startup memory by 10 mbytes.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2592
https://github.com/root-project/root/pull/2592:393,security,patch,patch,393,"[cxxmodule] Don't declare fwd decls from rootmaps with modules; Fwd decls from rootmap files were injected to ROOT at the startup time. However with modules we don't need any foward decls as we're preloading. all modules. Our future plan is to remove rootmap files as a whole, but we still need. to support more features (see 2380). So for now we can remove what we. are not using. *edit This patch also contains adding fHasCxxModule as a private member. This improves startup memory by 10 mbytes.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2592
https://github.com/root-project/root/pull/2592:233,testability,plan,plan,233,"[cxxmodule] Don't declare fwd decls from rootmaps with modules; Fwd decls from rootmap files were injected to ROOT at the startup time. However with modules we don't need any foward decls as we're preloading. all modules. Our future plan is to remove rootmap files as a whole, but we still need. to support more features (see 2380). So for now we can remove what we. are not using. *edit This patch also contains adding fHasCxxModule as a private member. This improves startup memory by 10 mbytes.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2592
https://github.com/root-project/root/pull/2592:299,usability,support,support,299,"[cxxmodule] Don't declare fwd decls from rootmaps with modules; Fwd decls from rootmap files were injected to ROOT at the startup time. However with modules we don't need any foward decls as we're preloading. all modules. Our future plan is to remove rootmap files as a whole, but we still need. to support more features (see 2380). So for now we can remove what we. are not using. *edit This patch also contains adding fHasCxxModule as a private member. This improves startup memory by 10 mbytes.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2592
https://github.com/root-project/root/pull/2592:477,usability,memor,memory,477,"[cxxmodule] Don't declare fwd decls from rootmaps with modules; Fwd decls from rootmap files were injected to ROOT at the startup time. However with modules we don't need any foward decls as we're preloading. all modules. Our future plan is to remove rootmap files as a whole, but we still need. to support more features (see 2380). So for now we can remove what we. are not using. *edit This patch also contains adding fHasCxxModule as a private member. This improves startup memory by 10 mbytes.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2592
https://github.com/root-project/root/pull/2593:35,deployability,contain,container,35,"[TMVA] RTensor: Proposal for a C++ container with shape information; A lot of code but mainly due to the tests and pythonizations. Main place to focus on is `TMVA/RTensor.hxx`. In the following examples of the implemented features:. C++ example:. ```cpp. using namespace TMVA::Experimental;. RTensor<float> x({2, 3}); // container with shape (2, 3). x(0,0) = 1; // set element (0,0) to 1. cout << x(0, 0) << endl; // read element (0,0). // Returns:. // 1. cout << x << endl;. // Returns:. // { { 1, 0, 0 } { 0, 0, 0 } }. ```. Python example:. ```python. import ROOT. x = ROOT.TMVA.Experimental.RTensor(""float"")((2, 3)) # container with shape (2, 3). x[0,0] = 1 # set element (0,0). print(x[0,0]) # read element (0,0). # Returns:. # 1. print(x). # Returns:. # { { 1, 0, 0 } { 0, 0, 0 } }. ```. Memory adoption capability:. ```cpp. using namespace TMVA::Experimental;. float data[6] = {1, 2, 3, 4, 5, 6};. RTensor<float> x(data, {2, 3}); // adopt memory with given shape. cout << x << endl;. // Returns:. // { { 1, 2, 3 } { 4, 5, 6 } }. ```. Column-major and row-major memory ordering:. ```cpp. using namespace TMVA::Experimental;. float data[6] = {1, 2, 3, 4, 5, 6};. RTensor<float> x(data, {2, 3}, MemoryOrder::RowMajor);. cout << x << endl;. // Returns:. // { { 1, 2, 3 } { 4, 5, 6 } }. RTensor<float> x(data, {2, 3}, MemoryOrder::ColumnMajor);. cout << x << endl;. // Returns:. // { { 1, 3, 5 } { 2, 4, 6 } }. ```. RTensor to numpy conversion:. ```python. import ROOT. data = ROOT.std.vector(""float"")((1, 2, 3, 4, 5, 6)). x = ROOT.TMVA.Experimental.RTensor(""float"")(data.data(), (2, 3)). print(x). # Returns:. # { { 1, 2, 3 } { 4, 5, 6 } }. import numpy. y = numpy.asarray(x). print(y). # Returns:. # [[1, 2, 3],. # [4, 5, 6]]. ```. Missing features:. - [ ] STL iterator interface. - [ ] `ExpandDim` and `Squeeze` methods (shape manipulation). - [ ] `Apply` method (element manipulation, similar to STL iterator). - [ ] `ROOT.AsTensor` method (`numpy.array` to `RTensor` conversion)",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2593
https://github.com/root-project/root/pull/2593:321,deployability,contain,container,321,"[TMVA] RTensor: Proposal for a C++ container with shape information; A lot of code but mainly due to the tests and pythonizations. Main place to focus on is `TMVA/RTensor.hxx`. In the following examples of the implemented features:. C++ example:. ```cpp. using namespace TMVA::Experimental;. RTensor<float> x({2, 3}); // container with shape (2, 3). x(0,0) = 1; // set element (0,0) to 1. cout << x(0, 0) << endl; // read element (0,0). // Returns:. // 1. cout << x << endl;. // Returns:. // { { 1, 0, 0 } { 0, 0, 0 } }. ```. Python example:. ```python. import ROOT. x = ROOT.TMVA.Experimental.RTensor(""float"")((2, 3)) # container with shape (2, 3). x[0,0] = 1 # set element (0,0). print(x[0,0]) # read element (0,0). # Returns:. # 1. print(x). # Returns:. # { { 1, 0, 0 } { 0, 0, 0 } }. ```. Memory adoption capability:. ```cpp. using namespace TMVA::Experimental;. float data[6] = {1, 2, 3, 4, 5, 6};. RTensor<float> x(data, {2, 3}); // adopt memory with given shape. cout << x << endl;. // Returns:. // { { 1, 2, 3 } { 4, 5, 6 } }. ```. Column-major and row-major memory ordering:. ```cpp. using namespace TMVA::Experimental;. float data[6] = {1, 2, 3, 4, 5, 6};. RTensor<float> x(data, {2, 3}, MemoryOrder::RowMajor);. cout << x << endl;. // Returns:. // { { 1, 2, 3 } { 4, 5, 6 } }. RTensor<float> x(data, {2, 3}, MemoryOrder::ColumnMajor);. cout << x << endl;. // Returns:. // { { 1, 3, 5 } { 2, 4, 6 } }. ```. RTensor to numpy conversion:. ```python. import ROOT. data = ROOT.std.vector(""float"")((1, 2, 3, 4, 5, 6)). x = ROOT.TMVA.Experimental.RTensor(""float"")(data.data(), (2, 3)). print(x). # Returns:. # { { 1, 2, 3 } { 4, 5, 6 } }. import numpy. y = numpy.asarray(x). print(y). # Returns:. # [[1, 2, 3],. # [4, 5, 6]]. ```. Missing features:. - [ ] STL iterator interface. - [ ] `ExpandDim` and `Squeeze` methods (shape manipulation). - [ ] `Apply` method (element manipulation, similar to STL iterator). - [ ] `ROOT.AsTensor` method (`numpy.array` to `RTensor` conversion)",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2593
https://github.com/root-project/root/pull/2593:621,deployability,contain,container,621,"[TMVA] RTensor: Proposal for a C++ container with shape information; A lot of code but mainly due to the tests and pythonizations. Main place to focus on is `TMVA/RTensor.hxx`. In the following examples of the implemented features:. C++ example:. ```cpp. using namespace TMVA::Experimental;. RTensor<float> x({2, 3}); // container with shape (2, 3). x(0,0) = 1; // set element (0,0) to 1. cout << x(0, 0) << endl; // read element (0,0). // Returns:. // 1. cout << x << endl;. // Returns:. // { { 1, 0, 0 } { 0, 0, 0 } }. ```. Python example:. ```python. import ROOT. x = ROOT.TMVA.Experimental.RTensor(""float"")((2, 3)) # container with shape (2, 3). x[0,0] = 1 # set element (0,0). print(x[0,0]) # read element (0,0). # Returns:. # 1. print(x). # Returns:. # { { 1, 0, 0 } { 0, 0, 0 } }. ```. Memory adoption capability:. ```cpp. using namespace TMVA::Experimental;. float data[6] = {1, 2, 3, 4, 5, 6};. RTensor<float> x(data, {2, 3}); // adopt memory with given shape. cout << x << endl;. // Returns:. // { { 1, 2, 3 } { 4, 5, 6 } }. ```. Column-major and row-major memory ordering:. ```cpp. using namespace TMVA::Experimental;. float data[6] = {1, 2, 3, 4, 5, 6};. RTensor<float> x(data, {2, 3}, MemoryOrder::RowMajor);. cout << x << endl;. // Returns:. // { { 1, 2, 3 } { 4, 5, 6 } }. RTensor<float> x(data, {2, 3}, MemoryOrder::ColumnMajor);. cout << x << endl;. // Returns:. // { { 1, 3, 5 } { 2, 4, 6 } }. ```. RTensor to numpy conversion:. ```python. import ROOT. data = ROOT.std.vector(""float"")((1, 2, 3, 4, 5, 6)). x = ROOT.TMVA.Experimental.RTensor(""float"")(data.data(), (2, 3)). print(x). # Returns:. # { { 1, 2, 3 } { 4, 5, 6 } }. import numpy. y = numpy.asarray(x). print(y). # Returns:. # [[1, 2, 3],. # [4, 5, 6]]. ```. Missing features:. - [ ] STL iterator interface. - [ ] `ExpandDim` and `Squeeze` methods (shape manipulation). - [ ] `Apply` method (element manipulation, similar to STL iterator). - [ ] `ROOT.AsTensor` method (`numpy.array` to `RTensor` conversion)",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2593
https://github.com/root-project/root/pull/2593:1773,integrability,interfac,interface,1773,"[TMVA] RTensor: Proposal for a C++ container with shape information; A lot of code but mainly due to the tests and pythonizations. Main place to focus on is `TMVA/RTensor.hxx`. In the following examples of the implemented features:. C++ example:. ```cpp. using namespace TMVA::Experimental;. RTensor<float> x({2, 3}); // container with shape (2, 3). x(0,0) = 1; // set element (0,0) to 1. cout << x(0, 0) << endl; // read element (0,0). // Returns:. // 1. cout << x << endl;. // Returns:. // { { 1, 0, 0 } { 0, 0, 0 } }. ```. Python example:. ```python. import ROOT. x = ROOT.TMVA.Experimental.RTensor(""float"")((2, 3)) # container with shape (2, 3). x[0,0] = 1 # set element (0,0). print(x[0,0]) # read element (0,0). # Returns:. # 1. print(x). # Returns:. # { { 1, 0, 0 } { 0, 0, 0 } }. ```. Memory adoption capability:. ```cpp. using namespace TMVA::Experimental;. float data[6] = {1, 2, 3, 4, 5, 6};. RTensor<float> x(data, {2, 3}); // adopt memory with given shape. cout << x << endl;. // Returns:. // { { 1, 2, 3 } { 4, 5, 6 } }. ```. Column-major and row-major memory ordering:. ```cpp. using namespace TMVA::Experimental;. float data[6] = {1, 2, 3, 4, 5, 6};. RTensor<float> x(data, {2, 3}, MemoryOrder::RowMajor);. cout << x << endl;. // Returns:. // { { 1, 2, 3 } { 4, 5, 6 } }. RTensor<float> x(data, {2, 3}, MemoryOrder::ColumnMajor);. cout << x << endl;. // Returns:. // { { 1, 3, 5 } { 2, 4, 6 } }. ```. RTensor to numpy conversion:. ```python. import ROOT. data = ROOT.std.vector(""float"")((1, 2, 3, 4, 5, 6)). x = ROOT.TMVA.Experimental.RTensor(""float"")(data.data(), (2, 3)). print(x). # Returns:. # { { 1, 2, 3 } { 4, 5, 6 } }. import numpy. y = numpy.asarray(x). print(y). # Returns:. # [[1, 2, 3],. # [4, 5, 6]]. ```. Missing features:. - [ ] STL iterator interface. - [ ] `ExpandDim` and `Squeeze` methods (shape manipulation). - [ ] `Apply` method (element manipulation, similar to STL iterator). - [ ] `ROOT.AsTensor` method (`numpy.array` to `RTensor` conversion)",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2593
https://github.com/root-project/root/pull/2593:1434,interoperability,convers,conversion,1434,"[TMVA] RTensor: Proposal for a C++ container with shape information; A lot of code but mainly due to the tests and pythonizations. Main place to focus on is `TMVA/RTensor.hxx`. In the following examples of the implemented features:. C++ example:. ```cpp. using namespace TMVA::Experimental;. RTensor<float> x({2, 3}); // container with shape (2, 3). x(0,0) = 1; // set element (0,0) to 1. cout << x(0, 0) << endl; // read element (0,0). // Returns:. // 1. cout << x << endl;. // Returns:. // { { 1, 0, 0 } { 0, 0, 0 } }. ```. Python example:. ```python. import ROOT. x = ROOT.TMVA.Experimental.RTensor(""float"")((2, 3)) # container with shape (2, 3). x[0,0] = 1 # set element (0,0). print(x[0,0]) # read element (0,0). # Returns:. # 1. print(x). # Returns:. # { { 1, 0, 0 } { 0, 0, 0 } }. ```. Memory adoption capability:. ```cpp. using namespace TMVA::Experimental;. float data[6] = {1, 2, 3, 4, 5, 6};. RTensor<float> x(data, {2, 3}); // adopt memory with given shape. cout << x << endl;. // Returns:. // { { 1, 2, 3 } { 4, 5, 6 } }. ```. Column-major and row-major memory ordering:. ```cpp. using namespace TMVA::Experimental;. float data[6] = {1, 2, 3, 4, 5, 6};. RTensor<float> x(data, {2, 3}, MemoryOrder::RowMajor);. cout << x << endl;. // Returns:. // { { 1, 2, 3 } { 4, 5, 6 } }. RTensor<float> x(data, {2, 3}, MemoryOrder::ColumnMajor);. cout << x << endl;. // Returns:. // { { 1, 3, 5 } { 2, 4, 6 } }. ```. RTensor to numpy conversion:. ```python. import ROOT. data = ROOT.std.vector(""float"")((1, 2, 3, 4, 5, 6)). x = ROOT.TMVA.Experimental.RTensor(""float"")(data.data(), (2, 3)). print(x). # Returns:. # { { 1, 2, 3 } { 4, 5, 6 } }. import numpy. y = numpy.asarray(x). print(y). # Returns:. # [[1, 2, 3],. # [4, 5, 6]]. ```. Missing features:. - [ ] STL iterator interface. - [ ] `ExpandDim` and `Squeeze` methods (shape manipulation). - [ ] `Apply` method (element manipulation, similar to STL iterator). - [ ] `ROOT.AsTensor` method (`numpy.array` to `RTensor` conversion)",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2593
https://github.com/root-project/root/pull/2593:1773,interoperability,interfac,interface,1773,"[TMVA] RTensor: Proposal for a C++ container with shape information; A lot of code but mainly due to the tests and pythonizations. Main place to focus on is `TMVA/RTensor.hxx`. In the following examples of the implemented features:. C++ example:. ```cpp. using namespace TMVA::Experimental;. RTensor<float> x({2, 3}); // container with shape (2, 3). x(0,0) = 1; // set element (0,0) to 1. cout << x(0, 0) << endl; // read element (0,0). // Returns:. // 1. cout << x << endl;. // Returns:. // { { 1, 0, 0 } { 0, 0, 0 } }. ```. Python example:. ```python. import ROOT. x = ROOT.TMVA.Experimental.RTensor(""float"")((2, 3)) # container with shape (2, 3). x[0,0] = 1 # set element (0,0). print(x[0,0]) # read element (0,0). # Returns:. # 1. print(x). # Returns:. # { { 1, 0, 0 } { 0, 0, 0 } }. ```. Memory adoption capability:. ```cpp. using namespace TMVA::Experimental;. float data[6] = {1, 2, 3, 4, 5, 6};. RTensor<float> x(data, {2, 3}); // adopt memory with given shape. cout << x << endl;. // Returns:. // { { 1, 2, 3 } { 4, 5, 6 } }. ```. Column-major and row-major memory ordering:. ```cpp. using namespace TMVA::Experimental;. float data[6] = {1, 2, 3, 4, 5, 6};. RTensor<float> x(data, {2, 3}, MemoryOrder::RowMajor);. cout << x << endl;. // Returns:. // { { 1, 2, 3 } { 4, 5, 6 } }. RTensor<float> x(data, {2, 3}, MemoryOrder::ColumnMajor);. cout << x << endl;. // Returns:. // { { 1, 3, 5 } { 2, 4, 6 } }. ```. RTensor to numpy conversion:. ```python. import ROOT. data = ROOT.std.vector(""float"")((1, 2, 3, 4, 5, 6)). x = ROOT.TMVA.Experimental.RTensor(""float"")(data.data(), (2, 3)). print(x). # Returns:. # { { 1, 2, 3 } { 4, 5, 6 } }. import numpy. y = numpy.asarray(x). print(y). # Returns:. # [[1, 2, 3],. # [4, 5, 6]]. ```. Missing features:. - [ ] STL iterator interface. - [ ] `ExpandDim` and `Squeeze` methods (shape manipulation). - [ ] `Apply` method (element manipulation, similar to STL iterator). - [ ] `ROOT.AsTensor` method (`numpy.array` to `RTensor` conversion)",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2593
https://github.com/root-project/root/pull/2593:1973,interoperability,convers,conversion,1973,"[TMVA] RTensor: Proposal for a C++ container with shape information; A lot of code but mainly due to the tests and pythonizations. Main place to focus on is `TMVA/RTensor.hxx`. In the following examples of the implemented features:. C++ example:. ```cpp. using namespace TMVA::Experimental;. RTensor<float> x({2, 3}); // container with shape (2, 3). x(0,0) = 1; // set element (0,0) to 1. cout << x(0, 0) << endl; // read element (0,0). // Returns:. // 1. cout << x << endl;. // Returns:. // { { 1, 0, 0 } { 0, 0, 0 } }. ```. Python example:. ```python. import ROOT. x = ROOT.TMVA.Experimental.RTensor(""float"")((2, 3)) # container with shape (2, 3). x[0,0] = 1 # set element (0,0). print(x[0,0]) # read element (0,0). # Returns:. # 1. print(x). # Returns:. # { { 1, 0, 0 } { 0, 0, 0 } }. ```. Memory adoption capability:. ```cpp. using namespace TMVA::Experimental;. float data[6] = {1, 2, 3, 4, 5, 6};. RTensor<float> x(data, {2, 3}); // adopt memory with given shape. cout << x << endl;. // Returns:. // { { 1, 2, 3 } { 4, 5, 6 } }. ```. Column-major and row-major memory ordering:. ```cpp. using namespace TMVA::Experimental;. float data[6] = {1, 2, 3, 4, 5, 6};. RTensor<float> x(data, {2, 3}, MemoryOrder::RowMajor);. cout << x << endl;. // Returns:. // { { 1, 2, 3 } { 4, 5, 6 } }. RTensor<float> x(data, {2, 3}, MemoryOrder::ColumnMajor);. cout << x << endl;. // Returns:. // { { 1, 3, 5 } { 2, 4, 6 } }. ```. RTensor to numpy conversion:. ```python. import ROOT. data = ROOT.std.vector(""float"")((1, 2, 3, 4, 5, 6)). x = ROOT.TMVA.Experimental.RTensor(""float"")(data.data(), (2, 3)). print(x). # Returns:. # { { 1, 2, 3 } { 4, 5, 6 } }. import numpy. y = numpy.asarray(x). print(y). # Returns:. # [[1, 2, 3],. # [4, 5, 6]]. ```. Missing features:. - [ ] STL iterator interface. - [ ] `ExpandDim` and `Squeeze` methods (shape manipulation). - [ ] `Apply` method (element manipulation, similar to STL iterator). - [ ] `ROOT.AsTensor` method (`numpy.array` to `RTensor` conversion)",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2593
https://github.com/root-project/root/pull/2593:1773,modifiability,interfac,interface,1773,"[TMVA] RTensor: Proposal for a C++ container with shape information; A lot of code but mainly due to the tests and pythonizations. Main place to focus on is `TMVA/RTensor.hxx`. In the following examples of the implemented features:. C++ example:. ```cpp. using namespace TMVA::Experimental;. RTensor<float> x({2, 3}); // container with shape (2, 3). x(0,0) = 1; // set element (0,0) to 1. cout << x(0, 0) << endl; // read element (0,0). // Returns:. // 1. cout << x << endl;. // Returns:. // { { 1, 0, 0 } { 0, 0, 0 } }. ```. Python example:. ```python. import ROOT. x = ROOT.TMVA.Experimental.RTensor(""float"")((2, 3)) # container with shape (2, 3). x[0,0] = 1 # set element (0,0). print(x[0,0]) # read element (0,0). # Returns:. # 1. print(x). # Returns:. # { { 1, 0, 0 } { 0, 0, 0 } }. ```. Memory adoption capability:. ```cpp. using namespace TMVA::Experimental;. float data[6] = {1, 2, 3, 4, 5, 6};. RTensor<float> x(data, {2, 3}); // adopt memory with given shape. cout << x << endl;. // Returns:. // { { 1, 2, 3 } { 4, 5, 6 } }. ```. Column-major and row-major memory ordering:. ```cpp. using namespace TMVA::Experimental;. float data[6] = {1, 2, 3, 4, 5, 6};. RTensor<float> x(data, {2, 3}, MemoryOrder::RowMajor);. cout << x << endl;. // Returns:. // { { 1, 2, 3 } { 4, 5, 6 } }. RTensor<float> x(data, {2, 3}, MemoryOrder::ColumnMajor);. cout << x << endl;. // Returns:. // { { 1, 3, 5 } { 2, 4, 6 } }. ```. RTensor to numpy conversion:. ```python. import ROOT. data = ROOT.std.vector(""float"")((1, 2, 3, 4, 5, 6)). x = ROOT.TMVA.Experimental.RTensor(""float"")(data.data(), (2, 3)). print(x). # Returns:. # { { 1, 2, 3 } { 4, 5, 6 } }. import numpy. y = numpy.asarray(x). print(y). # Returns:. # [[1, 2, 3],. # [4, 5, 6]]. ```. Missing features:. - [ ] STL iterator interface. - [ ] `ExpandDim` and `Squeeze` methods (shape manipulation). - [ ] `Apply` method (element manipulation, similar to STL iterator). - [ ] `ROOT.AsTensor` method (`numpy.array` to `RTensor` conversion)",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2593
https://github.com/root-project/root/pull/2593:793,performance,Memor,Memory,793,"[TMVA] RTensor: Proposal for a C++ container with shape information; A lot of code but mainly due to the tests and pythonizations. Main place to focus on is `TMVA/RTensor.hxx`. In the following examples of the implemented features:. C++ example:. ```cpp. using namespace TMVA::Experimental;. RTensor<float> x({2, 3}); // container with shape (2, 3). x(0,0) = 1; // set element (0,0) to 1. cout << x(0, 0) << endl; // read element (0,0). // Returns:. // 1. cout << x << endl;. // Returns:. // { { 1, 0, 0 } { 0, 0, 0 } }. ```. Python example:. ```python. import ROOT. x = ROOT.TMVA.Experimental.RTensor(""float"")((2, 3)) # container with shape (2, 3). x[0,0] = 1 # set element (0,0). print(x[0,0]) # read element (0,0). # Returns:. # 1. print(x). # Returns:. # { { 1, 0, 0 } { 0, 0, 0 } }. ```. Memory adoption capability:. ```cpp. using namespace TMVA::Experimental;. float data[6] = {1, 2, 3, 4, 5, 6};. RTensor<float> x(data, {2, 3}); // adopt memory with given shape. cout << x << endl;. // Returns:. // { { 1, 2, 3 } { 4, 5, 6 } }. ```. Column-major and row-major memory ordering:. ```cpp. using namespace TMVA::Experimental;. float data[6] = {1, 2, 3, 4, 5, 6};. RTensor<float> x(data, {2, 3}, MemoryOrder::RowMajor);. cout << x << endl;. // Returns:. // { { 1, 2, 3 } { 4, 5, 6 } }. RTensor<float> x(data, {2, 3}, MemoryOrder::ColumnMajor);. cout << x << endl;. // Returns:. // { { 1, 3, 5 } { 2, 4, 6 } }. ```. RTensor to numpy conversion:. ```python. import ROOT. data = ROOT.std.vector(""float"")((1, 2, 3, 4, 5, 6)). x = ROOT.TMVA.Experimental.RTensor(""float"")(data.data(), (2, 3)). print(x). # Returns:. # { { 1, 2, 3 } { 4, 5, 6 } }. import numpy. y = numpy.asarray(x). print(y). # Returns:. # [[1, 2, 3],. # [4, 5, 6]]. ```. Missing features:. - [ ] STL iterator interface. - [ ] `ExpandDim` and `Squeeze` methods (shape manipulation). - [ ] `Apply` method (element manipulation, similar to STL iterator). - [ ] `ROOT.AsTensor` method (`numpy.array` to `RTensor` conversion)",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2593
https://github.com/root-project/root/pull/2593:945,performance,memor,memory,945,"[TMVA] RTensor: Proposal for a C++ container with shape information; A lot of code but mainly due to the tests and pythonizations. Main place to focus on is `TMVA/RTensor.hxx`. In the following examples of the implemented features:. C++ example:. ```cpp. using namespace TMVA::Experimental;. RTensor<float> x({2, 3}); // container with shape (2, 3). x(0,0) = 1; // set element (0,0) to 1. cout << x(0, 0) << endl; // read element (0,0). // Returns:. // 1. cout << x << endl;. // Returns:. // { { 1, 0, 0 } { 0, 0, 0 } }. ```. Python example:. ```python. import ROOT. x = ROOT.TMVA.Experimental.RTensor(""float"")((2, 3)) # container with shape (2, 3). x[0,0] = 1 # set element (0,0). print(x[0,0]) # read element (0,0). # Returns:. # 1. print(x). # Returns:. # { { 1, 0, 0 } { 0, 0, 0 } }. ```. Memory adoption capability:. ```cpp. using namespace TMVA::Experimental;. float data[6] = {1, 2, 3, 4, 5, 6};. RTensor<float> x(data, {2, 3}); // adopt memory with given shape. cout << x << endl;. // Returns:. // { { 1, 2, 3 } { 4, 5, 6 } }. ```. Column-major and row-major memory ordering:. ```cpp. using namespace TMVA::Experimental;. float data[6] = {1, 2, 3, 4, 5, 6};. RTensor<float> x(data, {2, 3}, MemoryOrder::RowMajor);. cout << x << endl;. // Returns:. // { { 1, 2, 3 } { 4, 5, 6 } }. RTensor<float> x(data, {2, 3}, MemoryOrder::ColumnMajor);. cout << x << endl;. // Returns:. // { { 1, 3, 5 } { 2, 4, 6 } }. ```. RTensor to numpy conversion:. ```python. import ROOT. data = ROOT.std.vector(""float"")((1, 2, 3, 4, 5, 6)). x = ROOT.TMVA.Experimental.RTensor(""float"")(data.data(), (2, 3)). print(x). # Returns:. # { { 1, 2, 3 } { 4, 5, 6 } }. import numpy. y = numpy.asarray(x). print(y). # Returns:. # [[1, 2, 3],. # [4, 5, 6]]. ```. Missing features:. - [ ] STL iterator interface. - [ ] `ExpandDim` and `Squeeze` methods (shape manipulation). - [ ] `Apply` method (element manipulation, similar to STL iterator). - [ ] `ROOT.AsTensor` method (`numpy.array` to `RTensor` conversion)",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2593
https://github.com/root-project/root/pull/2593:1067,performance,memor,memory,1067,"[TMVA] RTensor: Proposal for a C++ container with shape information; A lot of code but mainly due to the tests and pythonizations. Main place to focus on is `TMVA/RTensor.hxx`. In the following examples of the implemented features:. C++ example:. ```cpp. using namespace TMVA::Experimental;. RTensor<float> x({2, 3}); // container with shape (2, 3). x(0,0) = 1; // set element (0,0) to 1. cout << x(0, 0) << endl; // read element (0,0). // Returns:. // 1. cout << x << endl;. // Returns:. // { { 1, 0, 0 } { 0, 0, 0 } }. ```. Python example:. ```python. import ROOT. x = ROOT.TMVA.Experimental.RTensor(""float"")((2, 3)) # container with shape (2, 3). x[0,0] = 1 # set element (0,0). print(x[0,0]) # read element (0,0). # Returns:. # 1. print(x). # Returns:. # { { 1, 0, 0 } { 0, 0, 0 } }. ```. Memory adoption capability:. ```cpp. using namespace TMVA::Experimental;. float data[6] = {1, 2, 3, 4, 5, 6};. RTensor<float> x(data, {2, 3}); // adopt memory with given shape. cout << x << endl;. // Returns:. // { { 1, 2, 3 } { 4, 5, 6 } }. ```. Column-major and row-major memory ordering:. ```cpp. using namespace TMVA::Experimental;. float data[6] = {1, 2, 3, 4, 5, 6};. RTensor<float> x(data, {2, 3}, MemoryOrder::RowMajor);. cout << x << endl;. // Returns:. // { { 1, 2, 3 } { 4, 5, 6 } }. RTensor<float> x(data, {2, 3}, MemoryOrder::ColumnMajor);. cout << x << endl;. // Returns:. // { { 1, 3, 5 } { 2, 4, 6 } }. ```. RTensor to numpy conversion:. ```python. import ROOT. data = ROOT.std.vector(""float"")((1, 2, 3, 4, 5, 6)). x = ROOT.TMVA.Experimental.RTensor(""float"")(data.data(), (2, 3)). print(x). # Returns:. # { { 1, 2, 3 } { 4, 5, 6 } }. import numpy. y = numpy.asarray(x). print(y). # Returns:. # [[1, 2, 3],. # [4, 5, 6]]. ```. Missing features:. - [ ] STL iterator interface. - [ ] `ExpandDim` and `Squeeze` methods (shape manipulation). - [ ] `Apply` method (element manipulation, similar to STL iterator). - [ ] `ROOT.AsTensor` method (`numpy.array` to `RTensor` conversion)",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2593
https://github.com/root-project/root/pull/2593:1198,performance,Memor,MemoryOrder,1198,"[TMVA] RTensor: Proposal for a C++ container with shape information; A lot of code but mainly due to the tests and pythonizations. Main place to focus on is `TMVA/RTensor.hxx`. In the following examples of the implemented features:. C++ example:. ```cpp. using namespace TMVA::Experimental;. RTensor<float> x({2, 3}); // container with shape (2, 3). x(0,0) = 1; // set element (0,0) to 1. cout << x(0, 0) << endl; // read element (0,0). // Returns:. // 1. cout << x << endl;. // Returns:. // { { 1, 0, 0 } { 0, 0, 0 } }. ```. Python example:. ```python. import ROOT. x = ROOT.TMVA.Experimental.RTensor(""float"")((2, 3)) # container with shape (2, 3). x[0,0] = 1 # set element (0,0). print(x[0,0]) # read element (0,0). # Returns:. # 1. print(x). # Returns:. # { { 1, 0, 0 } { 0, 0, 0 } }. ```. Memory adoption capability:. ```cpp. using namespace TMVA::Experimental;. float data[6] = {1, 2, 3, 4, 5, 6};. RTensor<float> x(data, {2, 3}); // adopt memory with given shape. cout << x << endl;. // Returns:. // { { 1, 2, 3 } { 4, 5, 6 } }. ```. Column-major and row-major memory ordering:. ```cpp. using namespace TMVA::Experimental;. float data[6] = {1, 2, 3, 4, 5, 6};. RTensor<float> x(data, {2, 3}, MemoryOrder::RowMajor);. cout << x << endl;. // Returns:. // { { 1, 2, 3 } { 4, 5, 6 } }. RTensor<float> x(data, {2, 3}, MemoryOrder::ColumnMajor);. cout << x << endl;. // Returns:. // { { 1, 3, 5 } { 2, 4, 6 } }. ```. RTensor to numpy conversion:. ```python. import ROOT. data = ROOT.std.vector(""float"")((1, 2, 3, 4, 5, 6)). x = ROOT.TMVA.Experimental.RTensor(""float"")(data.data(), (2, 3)). print(x). # Returns:. # { { 1, 2, 3 } { 4, 5, 6 } }. import numpy. y = numpy.asarray(x). print(y). # Returns:. # [[1, 2, 3],. # [4, 5, 6]]. ```. Missing features:. - [ ] STL iterator interface. - [ ] `ExpandDim` and `Squeeze` methods (shape manipulation). - [ ] `Apply` method (element manipulation, similar to STL iterator). - [ ] `ROOT.AsTensor` method (`numpy.array` to `RTensor` conversion)",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2593
https://github.com/root-project/root/pull/2593:1319,performance,Memor,MemoryOrder,1319,"[TMVA] RTensor: Proposal for a C++ container with shape information; A lot of code but mainly due to the tests and pythonizations. Main place to focus on is `TMVA/RTensor.hxx`. In the following examples of the implemented features:. C++ example:. ```cpp. using namespace TMVA::Experimental;. RTensor<float> x({2, 3}); // container with shape (2, 3). x(0,0) = 1; // set element (0,0) to 1. cout << x(0, 0) << endl; // read element (0,0). // Returns:. // 1. cout << x << endl;. // Returns:. // { { 1, 0, 0 } { 0, 0, 0 } }. ```. Python example:. ```python. import ROOT. x = ROOT.TMVA.Experimental.RTensor(""float"")((2, 3)) # container with shape (2, 3). x[0,0] = 1 # set element (0,0). print(x[0,0]) # read element (0,0). # Returns:. # 1. print(x). # Returns:. # { { 1, 0, 0 } { 0, 0, 0 } }. ```. Memory adoption capability:. ```cpp. using namespace TMVA::Experimental;. float data[6] = {1, 2, 3, 4, 5, 6};. RTensor<float> x(data, {2, 3}); // adopt memory with given shape. cout << x << endl;. // Returns:. // { { 1, 2, 3 } { 4, 5, 6 } }. ```. Column-major and row-major memory ordering:. ```cpp. using namespace TMVA::Experimental;. float data[6] = {1, 2, 3, 4, 5, 6};. RTensor<float> x(data, {2, 3}, MemoryOrder::RowMajor);. cout << x << endl;. // Returns:. // { { 1, 2, 3 } { 4, 5, 6 } }. RTensor<float> x(data, {2, 3}, MemoryOrder::ColumnMajor);. cout << x << endl;. // Returns:. // { { 1, 3, 5 } { 2, 4, 6 } }. ```. RTensor to numpy conversion:. ```python. import ROOT. data = ROOT.std.vector(""float"")((1, 2, 3, 4, 5, 6)). x = ROOT.TMVA.Experimental.RTensor(""float"")(data.data(), (2, 3)). print(x). # Returns:. # { { 1, 2, 3 } { 4, 5, 6 } }. import numpy. y = numpy.asarray(x). print(y). # Returns:. # [[1, 2, 3],. # [4, 5, 6]]. ```. Missing features:. - [ ] STL iterator interface. - [ ] `ExpandDim` and `Squeeze` methods (shape manipulation). - [ ] `Apply` method (element manipulation, similar to STL iterator). - [ ] `ROOT.AsTensor` method (`numpy.array` to `RTensor` conversion)",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2593
https://github.com/root-project/root/pull/2593:105,safety,test,tests,105,"[TMVA] RTensor: Proposal for a C++ container with shape information; A lot of code but mainly due to the tests and pythonizations. Main place to focus on is `TMVA/RTensor.hxx`. In the following examples of the implemented features:. C++ example:. ```cpp. using namespace TMVA::Experimental;. RTensor<float> x({2, 3}); // container with shape (2, 3). x(0,0) = 1; // set element (0,0) to 1. cout << x(0, 0) << endl; // read element (0,0). // Returns:. // 1. cout << x << endl;. // Returns:. // { { 1, 0, 0 } { 0, 0, 0 } }. ```. Python example:. ```python. import ROOT. x = ROOT.TMVA.Experimental.RTensor(""float"")((2, 3)) # container with shape (2, 3). x[0,0] = 1 # set element (0,0). print(x[0,0]) # read element (0,0). # Returns:. # 1. print(x). # Returns:. # { { 1, 0, 0 } { 0, 0, 0 } }. ```. Memory adoption capability:. ```cpp. using namespace TMVA::Experimental;. float data[6] = {1, 2, 3, 4, 5, 6};. RTensor<float> x(data, {2, 3}); // adopt memory with given shape. cout << x << endl;. // Returns:. // { { 1, 2, 3 } { 4, 5, 6 } }. ```. Column-major and row-major memory ordering:. ```cpp. using namespace TMVA::Experimental;. float data[6] = {1, 2, 3, 4, 5, 6};. RTensor<float> x(data, {2, 3}, MemoryOrder::RowMajor);. cout << x << endl;. // Returns:. // { { 1, 2, 3 } { 4, 5, 6 } }. RTensor<float> x(data, {2, 3}, MemoryOrder::ColumnMajor);. cout << x << endl;. // Returns:. // { { 1, 3, 5 } { 2, 4, 6 } }. ```. RTensor to numpy conversion:. ```python. import ROOT. data = ROOT.std.vector(""float"")((1, 2, 3, 4, 5, 6)). x = ROOT.TMVA.Experimental.RTensor(""float"")(data.data(), (2, 3)). print(x). # Returns:. # { { 1, 2, 3 } { 4, 5, 6 } }. import numpy. y = numpy.asarray(x). print(y). # Returns:. # [[1, 2, 3],. # [4, 5, 6]]. ```. Missing features:. - [ ] STL iterator interface. - [ ] `ExpandDim` and `Squeeze` methods (shape manipulation). - [ ] `Apply` method (element manipulation, similar to STL iterator). - [ ] `ROOT.AsTensor` method (`numpy.array` to `RTensor` conversion)",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2593
https://github.com/root-project/root/pull/2593:105,testability,test,tests,105,"[TMVA] RTensor: Proposal for a C++ container with shape information; A lot of code but mainly due to the tests and pythonizations. Main place to focus on is `TMVA/RTensor.hxx`. In the following examples of the implemented features:. C++ example:. ```cpp. using namespace TMVA::Experimental;. RTensor<float> x({2, 3}); // container with shape (2, 3). x(0,0) = 1; // set element (0,0) to 1. cout << x(0, 0) << endl; // read element (0,0). // Returns:. // 1. cout << x << endl;. // Returns:. // { { 1, 0, 0 } { 0, 0, 0 } }. ```. Python example:. ```python. import ROOT. x = ROOT.TMVA.Experimental.RTensor(""float"")((2, 3)) # container with shape (2, 3). x[0,0] = 1 # set element (0,0). print(x[0,0]) # read element (0,0). # Returns:. # 1. print(x). # Returns:. # { { 1, 0, 0 } { 0, 0, 0 } }. ```. Memory adoption capability:. ```cpp. using namespace TMVA::Experimental;. float data[6] = {1, 2, 3, 4, 5, 6};. RTensor<float> x(data, {2, 3}); // adopt memory with given shape. cout << x << endl;. // Returns:. // { { 1, 2, 3 } { 4, 5, 6 } }. ```. Column-major and row-major memory ordering:. ```cpp. using namespace TMVA::Experimental;. float data[6] = {1, 2, 3, 4, 5, 6};. RTensor<float> x(data, {2, 3}, MemoryOrder::RowMajor);. cout << x << endl;. // Returns:. // { { 1, 2, 3 } { 4, 5, 6 } }. RTensor<float> x(data, {2, 3}, MemoryOrder::ColumnMajor);. cout << x << endl;. // Returns:. // { { 1, 3, 5 } { 2, 4, 6 } }. ```. RTensor to numpy conversion:. ```python. import ROOT. data = ROOT.std.vector(""float"")((1, 2, 3, 4, 5, 6)). x = ROOT.TMVA.Experimental.RTensor(""float"")(data.data(), (2, 3)). print(x). # Returns:. # { { 1, 2, 3 } { 4, 5, 6 } }. import numpy. y = numpy.asarray(x). print(y). # Returns:. # [[1, 2, 3],. # [4, 5, 6]]. ```. Missing features:. - [ ] STL iterator interface. - [ ] `ExpandDim` and `Squeeze` methods (shape manipulation). - [ ] `Apply` method (element manipulation, similar to STL iterator). - [ ] `ROOT.AsTensor` method (`numpy.array` to `RTensor` conversion)",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2593
https://github.com/root-project/root/pull/2593:793,usability,Memor,Memory,793,"[TMVA] RTensor: Proposal for a C++ container with shape information; A lot of code but mainly due to the tests and pythonizations. Main place to focus on is `TMVA/RTensor.hxx`. In the following examples of the implemented features:. C++ example:. ```cpp. using namespace TMVA::Experimental;. RTensor<float> x({2, 3}); // container with shape (2, 3). x(0,0) = 1; // set element (0,0) to 1. cout << x(0, 0) << endl; // read element (0,0). // Returns:. // 1. cout << x << endl;. // Returns:. // { { 1, 0, 0 } { 0, 0, 0 } }. ```. Python example:. ```python. import ROOT. x = ROOT.TMVA.Experimental.RTensor(""float"")((2, 3)) # container with shape (2, 3). x[0,0] = 1 # set element (0,0). print(x[0,0]) # read element (0,0). # Returns:. # 1. print(x). # Returns:. # { { 1, 0, 0 } { 0, 0, 0 } }. ```. Memory adoption capability:. ```cpp. using namespace TMVA::Experimental;. float data[6] = {1, 2, 3, 4, 5, 6};. RTensor<float> x(data, {2, 3}); // adopt memory with given shape. cout << x << endl;. // Returns:. // { { 1, 2, 3 } { 4, 5, 6 } }. ```. Column-major and row-major memory ordering:. ```cpp. using namespace TMVA::Experimental;. float data[6] = {1, 2, 3, 4, 5, 6};. RTensor<float> x(data, {2, 3}, MemoryOrder::RowMajor);. cout << x << endl;. // Returns:. // { { 1, 2, 3 } { 4, 5, 6 } }. RTensor<float> x(data, {2, 3}, MemoryOrder::ColumnMajor);. cout << x << endl;. // Returns:. // { { 1, 3, 5 } { 2, 4, 6 } }. ```. RTensor to numpy conversion:. ```python. import ROOT. data = ROOT.std.vector(""float"")((1, 2, 3, 4, 5, 6)). x = ROOT.TMVA.Experimental.RTensor(""float"")(data.data(), (2, 3)). print(x). # Returns:. # { { 1, 2, 3 } { 4, 5, 6 } }. import numpy. y = numpy.asarray(x). print(y). # Returns:. # [[1, 2, 3],. # [4, 5, 6]]. ```. Missing features:. - [ ] STL iterator interface. - [ ] `ExpandDim` and `Squeeze` methods (shape manipulation). - [ ] `Apply` method (element manipulation, similar to STL iterator). - [ ] `ROOT.AsTensor` method (`numpy.array` to `RTensor` conversion)",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2593
https://github.com/root-project/root/pull/2593:945,usability,memor,memory,945,"[TMVA] RTensor: Proposal for a C++ container with shape information; A lot of code but mainly due to the tests and pythonizations. Main place to focus on is `TMVA/RTensor.hxx`. In the following examples of the implemented features:. C++ example:. ```cpp. using namespace TMVA::Experimental;. RTensor<float> x({2, 3}); // container with shape (2, 3). x(0,0) = 1; // set element (0,0) to 1. cout << x(0, 0) << endl; // read element (0,0). // Returns:. // 1. cout << x << endl;. // Returns:. // { { 1, 0, 0 } { 0, 0, 0 } }. ```. Python example:. ```python. import ROOT. x = ROOT.TMVA.Experimental.RTensor(""float"")((2, 3)) # container with shape (2, 3). x[0,0] = 1 # set element (0,0). print(x[0,0]) # read element (0,0). # Returns:. # 1. print(x). # Returns:. # { { 1, 0, 0 } { 0, 0, 0 } }. ```. Memory adoption capability:. ```cpp. using namespace TMVA::Experimental;. float data[6] = {1, 2, 3, 4, 5, 6};. RTensor<float> x(data, {2, 3}); // adopt memory with given shape. cout << x << endl;. // Returns:. // { { 1, 2, 3 } { 4, 5, 6 } }. ```. Column-major and row-major memory ordering:. ```cpp. using namespace TMVA::Experimental;. float data[6] = {1, 2, 3, 4, 5, 6};. RTensor<float> x(data, {2, 3}, MemoryOrder::RowMajor);. cout << x << endl;. // Returns:. // { { 1, 2, 3 } { 4, 5, 6 } }. RTensor<float> x(data, {2, 3}, MemoryOrder::ColumnMajor);. cout << x << endl;. // Returns:. // { { 1, 3, 5 } { 2, 4, 6 } }. ```. RTensor to numpy conversion:. ```python. import ROOT. data = ROOT.std.vector(""float"")((1, 2, 3, 4, 5, 6)). x = ROOT.TMVA.Experimental.RTensor(""float"")(data.data(), (2, 3)). print(x). # Returns:. # { { 1, 2, 3 } { 4, 5, 6 } }. import numpy. y = numpy.asarray(x). print(y). # Returns:. # [[1, 2, 3],. # [4, 5, 6]]. ```. Missing features:. - [ ] STL iterator interface. - [ ] `ExpandDim` and `Squeeze` methods (shape manipulation). - [ ] `Apply` method (element manipulation, similar to STL iterator). - [ ] `ROOT.AsTensor` method (`numpy.array` to `RTensor` conversion)",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2593
https://github.com/root-project/root/pull/2593:1067,usability,memor,memory,1067,"[TMVA] RTensor: Proposal for a C++ container with shape information; A lot of code but mainly due to the tests and pythonizations. Main place to focus on is `TMVA/RTensor.hxx`. In the following examples of the implemented features:. C++ example:. ```cpp. using namespace TMVA::Experimental;. RTensor<float> x({2, 3}); // container with shape (2, 3). x(0,0) = 1; // set element (0,0) to 1. cout << x(0, 0) << endl; // read element (0,0). // Returns:. // 1. cout << x << endl;. // Returns:. // { { 1, 0, 0 } { 0, 0, 0 } }. ```. Python example:. ```python. import ROOT. x = ROOT.TMVA.Experimental.RTensor(""float"")((2, 3)) # container with shape (2, 3). x[0,0] = 1 # set element (0,0). print(x[0,0]) # read element (0,0). # Returns:. # 1. print(x). # Returns:. # { { 1, 0, 0 } { 0, 0, 0 } }. ```. Memory adoption capability:. ```cpp. using namespace TMVA::Experimental;. float data[6] = {1, 2, 3, 4, 5, 6};. RTensor<float> x(data, {2, 3}); // adopt memory with given shape. cout << x << endl;. // Returns:. // { { 1, 2, 3 } { 4, 5, 6 } }. ```. Column-major and row-major memory ordering:. ```cpp. using namespace TMVA::Experimental;. float data[6] = {1, 2, 3, 4, 5, 6};. RTensor<float> x(data, {2, 3}, MemoryOrder::RowMajor);. cout << x << endl;. // Returns:. // { { 1, 2, 3 } { 4, 5, 6 } }. RTensor<float> x(data, {2, 3}, MemoryOrder::ColumnMajor);. cout << x << endl;. // Returns:. // { { 1, 3, 5 } { 2, 4, 6 } }. ```. RTensor to numpy conversion:. ```python. import ROOT. data = ROOT.std.vector(""float"")((1, 2, 3, 4, 5, 6)). x = ROOT.TMVA.Experimental.RTensor(""float"")(data.data(), (2, 3)). print(x). # Returns:. # { { 1, 2, 3 } { 4, 5, 6 } }. import numpy. y = numpy.asarray(x). print(y). # Returns:. # [[1, 2, 3],. # [4, 5, 6]]. ```. Missing features:. - [ ] STL iterator interface. - [ ] `ExpandDim` and `Squeeze` methods (shape manipulation). - [ ] `Apply` method (element manipulation, similar to STL iterator). - [ ] `ROOT.AsTensor` method (`numpy.array` to `RTensor` conversion)",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2593
https://github.com/root-project/root/pull/2593:1198,usability,Memor,MemoryOrder,1198,"[TMVA] RTensor: Proposal for a C++ container with shape information; A lot of code but mainly due to the tests and pythonizations. Main place to focus on is `TMVA/RTensor.hxx`. In the following examples of the implemented features:. C++ example:. ```cpp. using namespace TMVA::Experimental;. RTensor<float> x({2, 3}); // container with shape (2, 3). x(0,0) = 1; // set element (0,0) to 1. cout << x(0, 0) << endl; // read element (0,0). // Returns:. // 1. cout << x << endl;. // Returns:. // { { 1, 0, 0 } { 0, 0, 0 } }. ```. Python example:. ```python. import ROOT. x = ROOT.TMVA.Experimental.RTensor(""float"")((2, 3)) # container with shape (2, 3). x[0,0] = 1 # set element (0,0). print(x[0,0]) # read element (0,0). # Returns:. # 1. print(x). # Returns:. # { { 1, 0, 0 } { 0, 0, 0 } }. ```. Memory adoption capability:. ```cpp. using namespace TMVA::Experimental;. float data[6] = {1, 2, 3, 4, 5, 6};. RTensor<float> x(data, {2, 3}); // adopt memory with given shape. cout << x << endl;. // Returns:. // { { 1, 2, 3 } { 4, 5, 6 } }. ```. Column-major and row-major memory ordering:. ```cpp. using namespace TMVA::Experimental;. float data[6] = {1, 2, 3, 4, 5, 6};. RTensor<float> x(data, {2, 3}, MemoryOrder::RowMajor);. cout << x << endl;. // Returns:. // { { 1, 2, 3 } { 4, 5, 6 } }. RTensor<float> x(data, {2, 3}, MemoryOrder::ColumnMajor);. cout << x << endl;. // Returns:. // { { 1, 3, 5 } { 2, 4, 6 } }. ```. RTensor to numpy conversion:. ```python. import ROOT. data = ROOT.std.vector(""float"")((1, 2, 3, 4, 5, 6)). x = ROOT.TMVA.Experimental.RTensor(""float"")(data.data(), (2, 3)). print(x). # Returns:. # { { 1, 2, 3 } { 4, 5, 6 } }. import numpy. y = numpy.asarray(x). print(y). # Returns:. # [[1, 2, 3],. # [4, 5, 6]]. ```. Missing features:. - [ ] STL iterator interface. - [ ] `ExpandDim` and `Squeeze` methods (shape manipulation). - [ ] `Apply` method (element manipulation, similar to STL iterator). - [ ] `ROOT.AsTensor` method (`numpy.array` to `RTensor` conversion)",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2593
https://github.com/root-project/root/pull/2593:1319,usability,Memor,MemoryOrder,1319,"[TMVA] RTensor: Proposal for a C++ container with shape information; A lot of code but mainly due to the tests and pythonizations. Main place to focus on is `TMVA/RTensor.hxx`. In the following examples of the implemented features:. C++ example:. ```cpp. using namespace TMVA::Experimental;. RTensor<float> x({2, 3}); // container with shape (2, 3). x(0,0) = 1; // set element (0,0) to 1. cout << x(0, 0) << endl; // read element (0,0). // Returns:. // 1. cout << x << endl;. // Returns:. // { { 1, 0, 0 } { 0, 0, 0 } }. ```. Python example:. ```python. import ROOT. x = ROOT.TMVA.Experimental.RTensor(""float"")((2, 3)) # container with shape (2, 3). x[0,0] = 1 # set element (0,0). print(x[0,0]) # read element (0,0). # Returns:. # 1. print(x). # Returns:. # { { 1, 0, 0 } { 0, 0, 0 } }. ```. Memory adoption capability:. ```cpp. using namespace TMVA::Experimental;. float data[6] = {1, 2, 3, 4, 5, 6};. RTensor<float> x(data, {2, 3}); // adopt memory with given shape. cout << x << endl;. // Returns:. // { { 1, 2, 3 } { 4, 5, 6 } }. ```. Column-major and row-major memory ordering:. ```cpp. using namespace TMVA::Experimental;. float data[6] = {1, 2, 3, 4, 5, 6};. RTensor<float> x(data, {2, 3}, MemoryOrder::RowMajor);. cout << x << endl;. // Returns:. // { { 1, 2, 3 } { 4, 5, 6 } }. RTensor<float> x(data, {2, 3}, MemoryOrder::ColumnMajor);. cout << x << endl;. // Returns:. // { { 1, 3, 5 } { 2, 4, 6 } }. ```. RTensor to numpy conversion:. ```python. import ROOT. data = ROOT.std.vector(""float"")((1, 2, 3, 4, 5, 6)). x = ROOT.TMVA.Experimental.RTensor(""float"")(data.data(), (2, 3)). print(x). # Returns:. # { { 1, 2, 3 } { 4, 5, 6 } }. import numpy. y = numpy.asarray(x). print(y). # Returns:. # [[1, 2, 3],. # [4, 5, 6]]. ```. Missing features:. - [ ] STL iterator interface. - [ ] `ExpandDim` and `Squeeze` methods (shape manipulation). - [ ] `Apply` method (element manipulation, similar to STL iterator). - [ ] `ROOT.AsTensor` method (`numpy.array` to `RTensor` conversion)",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2593
https://github.com/root-project/root/pull/2594:14,deployability,log,logic,14,[DF] Simplify logic now that lifetimes are simpler; - RResultPtr now stores a raw pointer intead of a weak_ptr. - weak_ptr validity checks have been removed since RLoopManager cannot. go out of scope before these other entities,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2594
https://github.com/root-project/root/pull/2594:14,safety,log,logic,14,[DF] Simplify logic now that lifetimes are simpler; - RResultPtr now stores a raw pointer intead of a weak_ptr. - weak_ptr validity checks have been removed since RLoopManager cannot. go out of scope before these other entities,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2594
https://github.com/root-project/root/pull/2594:123,safety,valid,validity,123,[DF] Simplify logic now that lifetimes are simpler; - RResultPtr now stores a raw pointer intead of a weak_ptr. - weak_ptr validity checks have been removed since RLoopManager cannot. go out of scope before these other entities,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2594
https://github.com/root-project/root/pull/2594:14,security,log,logic,14,[DF] Simplify logic now that lifetimes are simpler; - RResultPtr now stores a raw pointer intead of a weak_ptr. - weak_ptr validity checks have been removed since RLoopManager cannot. go out of scope before these other entities,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2594
https://github.com/root-project/root/pull/2594:5,testability,Simpl,Simplify,5,[DF] Simplify logic now that lifetimes are simpler; - RResultPtr now stores a raw pointer intead of a weak_ptr. - weak_ptr validity checks have been removed since RLoopManager cannot. go out of scope before these other entities,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2594
https://github.com/root-project/root/pull/2594:14,testability,log,logic,14,[DF] Simplify logic now that lifetimes are simpler; - RResultPtr now stores a raw pointer intead of a weak_ptr. - weak_ptr validity checks have been removed since RLoopManager cannot. go out of scope before these other entities,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2594
https://github.com/root-project/root/pull/2594:43,testability,simpl,simpler,43,[DF] Simplify logic now that lifetimes are simpler; - RResultPtr now stores a raw pointer intead of a weak_ptr. - weak_ptr validity checks have been removed since RLoopManager cannot. go out of scope before these other entities,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2594
https://github.com/root-project/root/pull/2594:5,usability,Simpl,Simplify,5,[DF] Simplify logic now that lifetimes are simpler; - RResultPtr now stores a raw pointer intead of a weak_ptr. - weak_ptr validity checks have been removed since RLoopManager cannot. go out of scope before these other entities,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2594
https://github.com/root-project/root/pull/2594:43,usability,simpl,simpler,43,[DF] Simplify logic now that lifetimes are simpler; - RResultPtr now stores a raw pointer intead of a weak_ptr. - weak_ptr validity checks have been removed since RLoopManager cannot. go out of scope before these other entities,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2594
https://github.com/root-project/root/pull/2595:29,performance,time,times,29,[DF] Faster Snapshot compile times; TODO: find a way to avoid repeating `RAction`'s code three times without losing performance,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2595
https://github.com/root-project/root/pull/2595:95,performance,time,times,95,[DF] Faster Snapshot compile times; TODO: find a way to avoid repeating `RAction`'s code three times without losing performance,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2595
https://github.com/root-project/root/pull/2595:116,performance,perform,performance,116,[DF] Faster Snapshot compile times; TODO: find a way to avoid repeating `RAction`'s code three times without losing performance,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2595
https://github.com/root-project/root/pull/2595:56,safety,avoid,avoid,56,[DF] Faster Snapshot compile times; TODO: find a way to avoid repeating `RAction`'s code three times without losing performance,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2595
https://github.com/root-project/root/pull/2595:116,usability,perform,performance,116,[DF] Faster Snapshot compile times; TODO: find a way to avoid repeating `RAction`'s code three times without losing performance,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2595
https://github.com/root-project/root/pull/2597:4,interoperability,incompatib,incompatibility,4,Fix incompatibility with R on case-insensitive file systems; Fixes [ROOT-9620](https://sft.its.cern.ch/jira/browse/ROOT-9620).,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2597
https://github.com/root-project/root/pull/2598:155,deployability,modul,module,155,Fix a warning about redefinition of DNNCPU macro for Clang 8.0.0; Fix to avoid redefinition of injected value of macro in test stressTMVA. Visible for C++ module build for Clang 8.0.0:. In file included from <module-includes>:26: ../build/include/TMVA/MethodDNN.h:56:9: warning: 'DNNCPU' macro redefined [-Wmacro-redefined]. #define DNNCPU. While building module 'TMVA' imported from /.../root/test/stressTMVA.cxx:70: In file included from <built-in>:379:. #define DNNCPU 1.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2598
https://github.com/root-project/root/pull/2598:162,deployability,build,build,162,Fix a warning about redefinition of DNNCPU macro for Clang 8.0.0; Fix to avoid redefinition of injected value of macro in test stressTMVA. Visible for C++ module build for Clang 8.0.0:. In file included from <module-includes>:26: ../build/include/TMVA/MethodDNN.h:56:9: warning: 'DNNCPU' macro redefined [-Wmacro-redefined]. #define DNNCPU. While building module 'TMVA' imported from /.../root/test/stressTMVA.cxx:70: In file included from <built-in>:379:. #define DNNCPU 1.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2598
https://github.com/root-project/root/pull/2598:209,deployability,modul,module-includes,209,Fix a warning about redefinition of DNNCPU macro for Clang 8.0.0; Fix to avoid redefinition of injected value of macro in test stressTMVA. Visible for C++ module build for Clang 8.0.0:. In file included from <module-includes>:26: ../build/include/TMVA/MethodDNN.h:56:9: warning: 'DNNCPU' macro redefined [-Wmacro-redefined]. #define DNNCPU. While building module 'TMVA' imported from /.../root/test/stressTMVA.cxx:70: In file included from <built-in>:379:. #define DNNCPU 1.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2598
https://github.com/root-project/root/pull/2598:233,deployability,build,build,233,Fix a warning about redefinition of DNNCPU macro for Clang 8.0.0; Fix to avoid redefinition of injected value of macro in test stressTMVA. Visible for C++ module build for Clang 8.0.0:. In file included from <module-includes>:26: ../build/include/TMVA/MethodDNN.h:56:9: warning: 'DNNCPU' macro redefined [-Wmacro-redefined]. #define DNNCPU. While building module 'TMVA' imported from /.../root/test/stressTMVA.cxx:70: In file included from <built-in>:379:. #define DNNCPU 1.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2598
https://github.com/root-project/root/pull/2598:347,deployability,build,building,347,Fix a warning about redefinition of DNNCPU macro for Clang 8.0.0; Fix to avoid redefinition of injected value of macro in test stressTMVA. Visible for C++ module build for Clang 8.0.0:. In file included from <module-includes>:26: ../build/include/TMVA/MethodDNN.h:56:9: warning: 'DNNCPU' macro redefined [-Wmacro-redefined]. #define DNNCPU. While building module 'TMVA' imported from /.../root/test/stressTMVA.cxx:70: In file included from <built-in>:379:. #define DNNCPU 1.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2598
https://github.com/root-project/root/pull/2598:356,deployability,modul,module,356,Fix a warning about redefinition of DNNCPU macro for Clang 8.0.0; Fix to avoid redefinition of injected value of macro in test stressTMVA. Visible for C++ module build for Clang 8.0.0:. In file included from <module-includes>:26: ../build/include/TMVA/MethodDNN.h:56:9: warning: 'DNNCPU' macro redefined [-Wmacro-redefined]. #define DNNCPU. While building module 'TMVA' imported from /.../root/test/stressTMVA.cxx:70: In file included from <built-in>:379:. #define DNNCPU 1.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2598
https://github.com/root-project/root/pull/2598:95,integrability,inject,injected,95,Fix a warning about redefinition of DNNCPU macro for Clang 8.0.0; Fix to avoid redefinition of injected value of macro in test stressTMVA. Visible for C++ module build for Clang 8.0.0:. In file included from <module-includes>:26: ../build/include/TMVA/MethodDNN.h:56:9: warning: 'DNNCPU' macro redefined [-Wmacro-redefined]. #define DNNCPU. While building module 'TMVA' imported from /.../root/test/stressTMVA.cxx:70: In file included from <built-in>:379:. #define DNNCPU 1.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2598
https://github.com/root-project/root/pull/2598:155,modifiability,modul,module,155,Fix a warning about redefinition of DNNCPU macro for Clang 8.0.0; Fix to avoid redefinition of injected value of macro in test stressTMVA. Visible for C++ module build for Clang 8.0.0:. In file included from <module-includes>:26: ../build/include/TMVA/MethodDNN.h:56:9: warning: 'DNNCPU' macro redefined [-Wmacro-redefined]. #define DNNCPU. While building module 'TMVA' imported from /.../root/test/stressTMVA.cxx:70: In file included from <built-in>:379:. #define DNNCPU 1.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2598
https://github.com/root-project/root/pull/2598:209,modifiability,modul,module-includes,209,Fix a warning about redefinition of DNNCPU macro for Clang 8.0.0; Fix to avoid redefinition of injected value of macro in test stressTMVA. Visible for C++ module build for Clang 8.0.0:. In file included from <module-includes>:26: ../build/include/TMVA/MethodDNN.h:56:9: warning: 'DNNCPU' macro redefined [-Wmacro-redefined]. #define DNNCPU. While building module 'TMVA' imported from /.../root/test/stressTMVA.cxx:70: In file included from <built-in>:379:. #define DNNCPU 1.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2598
https://github.com/root-project/root/pull/2598:356,modifiability,modul,module,356,Fix a warning about redefinition of DNNCPU macro for Clang 8.0.0; Fix to avoid redefinition of injected value of macro in test stressTMVA. Visible for C++ module build for Clang 8.0.0:. In file included from <module-includes>:26: ../build/include/TMVA/MethodDNN.h:56:9: warning: 'DNNCPU' macro redefined [-Wmacro-redefined]. #define DNNCPU. While building module 'TMVA' imported from /.../root/test/stressTMVA.cxx:70: In file included from <built-in>:379:. #define DNNCPU 1.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2598
https://github.com/root-project/root/pull/2598:73,safety,avoid,avoid,73,Fix a warning about redefinition of DNNCPU macro for Clang 8.0.0; Fix to avoid redefinition of injected value of macro in test stressTMVA. Visible for C++ module build for Clang 8.0.0:. In file included from <module-includes>:26: ../build/include/TMVA/MethodDNN.h:56:9: warning: 'DNNCPU' macro redefined [-Wmacro-redefined]. #define DNNCPU. While building module 'TMVA' imported from /.../root/test/stressTMVA.cxx:70: In file included from <built-in>:379:. #define DNNCPU 1.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2598
https://github.com/root-project/root/pull/2598:122,safety,test,test,122,Fix a warning about redefinition of DNNCPU macro for Clang 8.0.0; Fix to avoid redefinition of injected value of macro in test stressTMVA. Visible for C++ module build for Clang 8.0.0:. In file included from <module-includes>:26: ../build/include/TMVA/MethodDNN.h:56:9: warning: 'DNNCPU' macro redefined [-Wmacro-redefined]. #define DNNCPU. While building module 'TMVA' imported from /.../root/test/stressTMVA.cxx:70: In file included from <built-in>:379:. #define DNNCPU 1.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2598
https://github.com/root-project/root/pull/2598:155,safety,modul,module,155,Fix a warning about redefinition of DNNCPU macro for Clang 8.0.0; Fix to avoid redefinition of injected value of macro in test stressTMVA. Visible for C++ module build for Clang 8.0.0:. In file included from <module-includes>:26: ../build/include/TMVA/MethodDNN.h:56:9: warning: 'DNNCPU' macro redefined [-Wmacro-redefined]. #define DNNCPU. While building module 'TMVA' imported from /.../root/test/stressTMVA.cxx:70: In file included from <built-in>:379:. #define DNNCPU 1.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2598
https://github.com/root-project/root/pull/2598:209,safety,modul,module-includes,209,Fix a warning about redefinition of DNNCPU macro for Clang 8.0.0; Fix to avoid redefinition of injected value of macro in test stressTMVA. Visible for C++ module build for Clang 8.0.0:. In file included from <module-includes>:26: ../build/include/TMVA/MethodDNN.h:56:9: warning: 'DNNCPU' macro redefined [-Wmacro-redefined]. #define DNNCPU. While building module 'TMVA' imported from /.../root/test/stressTMVA.cxx:70: In file included from <built-in>:379:. #define DNNCPU 1.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2598
https://github.com/root-project/root/pull/2598:356,safety,modul,module,356,Fix a warning about redefinition of DNNCPU macro for Clang 8.0.0; Fix to avoid redefinition of injected value of macro in test stressTMVA. Visible for C++ module build for Clang 8.0.0:. In file included from <module-includes>:26: ../build/include/TMVA/MethodDNN.h:56:9: warning: 'DNNCPU' macro redefined [-Wmacro-redefined]. #define DNNCPU. While building module 'TMVA' imported from /.../root/test/stressTMVA.cxx:70: In file included from <built-in>:379:. #define DNNCPU 1.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2598
https://github.com/root-project/root/pull/2598:394,safety,test,test,394,Fix a warning about redefinition of DNNCPU macro for Clang 8.0.0; Fix to avoid redefinition of injected value of macro in test stressTMVA. Visible for C++ module build for Clang 8.0.0:. In file included from <module-includes>:26: ../build/include/TMVA/MethodDNN.h:56:9: warning: 'DNNCPU' macro redefined [-Wmacro-redefined]. #define DNNCPU. While building module 'TMVA' imported from /.../root/test/stressTMVA.cxx:70: In file included from <built-in>:379:. #define DNNCPU 1.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2598
https://github.com/root-project/root/pull/2598:95,security,inject,injected,95,Fix a warning about redefinition of DNNCPU macro for Clang 8.0.0; Fix to avoid redefinition of injected value of macro in test stressTMVA. Visible for C++ module build for Clang 8.0.0:. In file included from <module-includes>:26: ../build/include/TMVA/MethodDNN.h:56:9: warning: 'DNNCPU' macro redefined [-Wmacro-redefined]. #define DNNCPU. While building module 'TMVA' imported from /.../root/test/stressTMVA.cxx:70: In file included from <built-in>:379:. #define DNNCPU 1.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2598
https://github.com/root-project/root/pull/2598:122,testability,test,test,122,Fix a warning about redefinition of DNNCPU macro for Clang 8.0.0; Fix to avoid redefinition of injected value of macro in test stressTMVA. Visible for C++ module build for Clang 8.0.0:. In file included from <module-includes>:26: ../build/include/TMVA/MethodDNN.h:56:9: warning: 'DNNCPU' macro redefined [-Wmacro-redefined]. #define DNNCPU. While building module 'TMVA' imported from /.../root/test/stressTMVA.cxx:70: In file included from <built-in>:379:. #define DNNCPU 1.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2598
https://github.com/root-project/root/pull/2598:394,testability,test,test,394,Fix a warning about redefinition of DNNCPU macro for Clang 8.0.0; Fix to avoid redefinition of injected value of macro in test stressTMVA. Visible for C++ module build for Clang 8.0.0:. In file included from <module-includes>:26: ../build/include/TMVA/MethodDNN.h:56:9: warning: 'DNNCPU' macro redefined [-Wmacro-redefined]. #define DNNCPU. While building module 'TMVA' imported from /.../root/test/stressTMVA.cxx:70: In file included from <built-in>:379:. #define DNNCPU 1.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2598
https://github.com/root-project/root/pull/2599:15,deployability,modul,modulemap,15,"Adding pthread modulemap for net/http/civetweb; Here we are trying to fix warning coming from civetweb.c:. ```. /.../root/net/http/civetweb/civetweb.c:2701:8warning implicit declaration of function 'pthread_setname_np' is invalid in C99 [-Wimplicit-function-declaration]. (void)pthread_setname_np(pthread_self(), threadName);. ```.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2599
https://github.com/root-project/root/pull/2599:15,modifiability,modul,modulemap,15,"Adding pthread modulemap for net/http/civetweb; Here we are trying to fix warning coming from civetweb.c:. ```. /.../root/net/http/civetweb/civetweb.c:2701:8warning implicit declaration of function 'pthread_setname_np' is invalid in C99 [-Wimplicit-function-declaration]. (void)pthread_setname_np(pthread_self(), threadName);. ```.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2599
https://github.com/root-project/root/pull/2599:15,safety,modul,modulemap,15,"Adding pthread modulemap for net/http/civetweb; Here we are trying to fix warning coming from civetweb.c:. ```. /.../root/net/http/civetweb/civetweb.c:2701:8warning implicit declaration of function 'pthread_setname_np' is invalid in C99 [-Wimplicit-function-declaration]. (void)pthread_setname_np(pthread_self(), threadName);. ```.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2599
https://github.com/root-project/root/pull/2601:13,availability,cluster,cluster,13,"Create event cluster at flush - backport for v614; Per request from @pcanal, this is a backport of the fix for ROOT-9574.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2601
https://github.com/root-project/root/pull/2601:13,deployability,cluster,cluster,13,"Create event cluster at flush - backport for v614; Per request from @pcanal, this is a backport of the fix for ROOT-9574.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2601
https://github.com/root-project/root/pull/2601:7,integrability,event,event,7,"Create event cluster at flush - backport for v614; Per request from @pcanal, this is a backport of the fix for ROOT-9574.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2601
https://github.com/root-project/root/pull/2602:4,interoperability,incompatib,incompatibility,4,"Fix incompatibility with R on case-insensitive file systems; Fix for [ROOT-9620](https://sft.its.cern.ch/jira/browse/ROOT-9620) backported to 6.14 (already merged into master, see #2597).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2602
https://github.com/root-project/root/pull/2603:1310,availability,consist,consistent,1310,"WebWindow: cummulative patch with many fixes and updates; 1. Fix problem with fragmented websocket messages - some browsers can split messages on 120KB chunks. Now message merged together before delivered to the user. 2. Solve problem with Google Chrome run in headless mode. . Now chrome browser can run in headless mode without any special debugging flags, which opens some http ports. Now THttpServer provides special ""dummy"" script, which timedout until batch job is not finished. 3. Try to correctly handle situation when browser hangs and does not reply for a long time. After pre-configured timeout (default 50 sec) such browser application will be closed. 4. Let identify connecting websocket at the very beginning - with the first connect request. Allows to reject not allowed connection. Also for the future simplifies re-connection. 5. In TWebWindow clearly identify batch job connection, let reuse if for consequent jobs, close it after 20 sec of idle time. 6. Use std::recursive_mutex in WebWindowsManager to protect methods which creates THttpServer / creates new window / starts new web browser. Comment out special manager guard - useless without special threading functionality. 7. Correspondent changes in JSROOT. . 8. Special handling of 2d/3d canvas when producing PNG images - now is more consistent, no need to use THREE.SVGRenderer for that. 9. Add canvas size to the RPadDisplayItem class, used only in batch mode to configure image size. 10. Use workaround with static mutex to protect I/O in canvas painter in multi-threaded environment.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2603
https://github.com/root-project/root/pull/2603:23,deployability,patch,patch,23,"WebWindow: cummulative patch with many fixes and updates; 1. Fix problem with fragmented websocket messages - some browsers can split messages on 120KB chunks. Now message merged together before delivered to the user. 2. Solve problem with Google Chrome run in headless mode. . Now chrome browser can run in headless mode without any special debugging flags, which opens some http ports. Now THttpServer provides special ""dummy"" script, which timedout until batch job is not finished. 3. Try to correctly handle situation when browser hangs and does not reply for a long time. After pre-configured timeout (default 50 sec) such browser application will be closed. 4. Let identify connecting websocket at the very beginning - with the first connect request. Allows to reject not allowed connection. Also for the future simplifies re-connection. 5. In TWebWindow clearly identify batch job connection, let reuse if for consequent jobs, close it after 20 sec of idle time. 6. Use std::recursive_mutex in WebWindowsManager to protect methods which creates THttpServer / creates new window / starts new web browser. Comment out special manager guard - useless without special threading functionality. 7. Correspondent changes in JSROOT. . 8. Special handling of 2d/3d canvas when producing PNG images - now is more consistent, no need to use THREE.SVGRenderer for that. 9. Add canvas size to the RPadDisplayItem class, used only in batch mode to configure image size. 10. Use workaround with static mutex to protect I/O in canvas painter in multi-threaded environment.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2603
https://github.com/root-project/root/pull/2603:49,deployability,updat,updates,49,"WebWindow: cummulative patch with many fixes and updates; 1. Fix problem with fragmented websocket messages - some browsers can split messages on 120KB chunks. Now message merged together before delivered to the user. 2. Solve problem with Google Chrome run in headless mode. . Now chrome browser can run in headless mode without any special debugging flags, which opens some http ports. Now THttpServer provides special ""dummy"" script, which timedout until batch job is not finished. 3. Try to correctly handle situation when browser hangs and does not reply for a long time. After pre-configured timeout (default 50 sec) such browser application will be closed. 4. Let identify connecting websocket at the very beginning - with the first connect request. Allows to reject not allowed connection. Also for the future simplifies re-connection. 5. In TWebWindow clearly identify batch job connection, let reuse if for consequent jobs, close it after 20 sec of idle time. 6. Use std::recursive_mutex in WebWindowsManager to protect methods which creates THttpServer / creates new window / starts new web browser. Comment out special manager guard - useless without special threading functionality. 7. Correspondent changes in JSROOT. . 8. Special handling of 2d/3d canvas when producing PNG images - now is more consistent, no need to use THREE.SVGRenderer for that. 9. Add canvas size to the RPadDisplayItem class, used only in batch mode to configure image size. 10. Use workaround with static mutex to protect I/O in canvas painter in multi-threaded environment.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2603
https://github.com/root-project/root/pull/2603:1131,deployability,manag,manager,1131,"WebWindow: cummulative patch with many fixes and updates; 1. Fix problem with fragmented websocket messages - some browsers can split messages on 120KB chunks. Now message merged together before delivered to the user. 2. Solve problem with Google Chrome run in headless mode. . Now chrome browser can run in headless mode without any special debugging flags, which opens some http ports. Now THttpServer provides special ""dummy"" script, which timedout until batch job is not finished. 3. Try to correctly handle situation when browser hangs and does not reply for a long time. After pre-configured timeout (default 50 sec) such browser application will be closed. 4. Let identify connecting websocket at the very beginning - with the first connect request. Allows to reject not allowed connection. Also for the future simplifies re-connection. 5. In TWebWindow clearly identify batch job connection, let reuse if for consequent jobs, close it after 20 sec of idle time. 6. Use std::recursive_mutex in WebWindowsManager to protect methods which creates THttpServer / creates new window / starts new web browser. Comment out special manager guard - useless without special threading functionality. 7. Correspondent changes in JSROOT. . 8. Special handling of 2d/3d canvas when producing PNG images - now is more consistent, no need to use THREE.SVGRenderer for that. 9. Add canvas size to the RPadDisplayItem class, used only in batch mode to configure image size. 10. Use workaround with static mutex to protect I/O in canvas painter in multi-threaded environment.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2603
https://github.com/root-project/root/pull/2603:959,energy efficiency,idl,idle,959,"WebWindow: cummulative patch with many fixes and updates; 1. Fix problem with fragmented websocket messages - some browsers can split messages on 120KB chunks. Now message merged together before delivered to the user. 2. Solve problem with Google Chrome run in headless mode. . Now chrome browser can run in headless mode without any special debugging flags, which opens some http ports. Now THttpServer provides special ""dummy"" script, which timedout until batch job is not finished. 3. Try to correctly handle situation when browser hangs and does not reply for a long time. After pre-configured timeout (default 50 sec) such browser application will be closed. 4. Let identify connecting websocket at the very beginning - with the first connect request. Allows to reject not allowed connection. Also for the future simplifies re-connection. 5. In TWebWindow clearly identify batch job connection, let reuse if for consequent jobs, close it after 20 sec of idle time. 6. Use std::recursive_mutex in WebWindowsManager to protect methods which creates THttpServer / creates new window / starts new web browser. Comment out special manager guard - useless without special threading functionality. 7. Correspondent changes in JSROOT. . 8. Special handling of 2d/3d canvas when producing PNG images - now is more consistent, no need to use THREE.SVGRenderer for that. 9. Add canvas size to the RPadDisplayItem class, used only in batch mode to configure image size. 10. Use workaround with static mutex to protect I/O in canvas painter in multi-threaded environment.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2603
https://github.com/root-project/root/pull/2603:1131,energy efficiency,manag,manager,1131,"WebWindow: cummulative patch with many fixes and updates; 1. Fix problem with fragmented websocket messages - some browsers can split messages on 120KB chunks. Now message merged together before delivered to the user. 2. Solve problem with Google Chrome run in headless mode. . Now chrome browser can run in headless mode without any special debugging flags, which opens some http ports. Now THttpServer provides special ""dummy"" script, which timedout until batch job is not finished. 3. Try to correctly handle situation when browser hangs and does not reply for a long time. After pre-configured timeout (default 50 sec) such browser application will be closed. 4. Let identify connecting websocket at the very beginning - with the first connect request. Allows to reject not allowed connection. Also for the future simplifies re-connection. 5. In TWebWindow clearly identify batch job connection, let reuse if for consequent jobs, close it after 20 sec of idle time. 6. Use std::recursive_mutex in WebWindowsManager to protect methods which creates THttpServer / creates new window / starts new web browser. Comment out special manager guard - useless without special threading functionality. 7. Correspondent changes in JSROOT. . 8. Special handling of 2d/3d canvas when producing PNG images - now is more consistent, no need to use THREE.SVGRenderer for that. 9. Add canvas size to the RPadDisplayItem class, used only in batch mode to configure image size. 10. Use workaround with static mutex to protect I/O in canvas painter in multi-threaded environment.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2603
https://github.com/root-project/root/pull/2603:99,integrability,messag,messages,99,"WebWindow: cummulative patch with many fixes and updates; 1. Fix problem with fragmented websocket messages - some browsers can split messages on 120KB chunks. Now message merged together before delivered to the user. 2. Solve problem with Google Chrome run in headless mode. . Now chrome browser can run in headless mode without any special debugging flags, which opens some http ports. Now THttpServer provides special ""dummy"" script, which timedout until batch job is not finished. 3. Try to correctly handle situation when browser hangs and does not reply for a long time. After pre-configured timeout (default 50 sec) such browser application will be closed. 4. Let identify connecting websocket at the very beginning - with the first connect request. Allows to reject not allowed connection. Also for the future simplifies re-connection. 5. In TWebWindow clearly identify batch job connection, let reuse if for consequent jobs, close it after 20 sec of idle time. 6. Use std::recursive_mutex in WebWindowsManager to protect methods which creates THttpServer / creates new window / starts new web browser. Comment out special manager guard - useless without special threading functionality. 7. Correspondent changes in JSROOT. . 8. Special handling of 2d/3d canvas when producing PNG images - now is more consistent, no need to use THREE.SVGRenderer for that. 9. Add canvas size to the RPadDisplayItem class, used only in batch mode to configure image size. 10. Use workaround with static mutex to protect I/O in canvas painter in multi-threaded environment.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2603
https://github.com/root-project/root/pull/2603:134,integrability,messag,messages,134,"WebWindow: cummulative patch with many fixes and updates; 1. Fix problem with fragmented websocket messages - some browsers can split messages on 120KB chunks. Now message merged together before delivered to the user. 2. Solve problem with Google Chrome run in headless mode. . Now chrome browser can run in headless mode without any special debugging flags, which opens some http ports. Now THttpServer provides special ""dummy"" script, which timedout until batch job is not finished. 3. Try to correctly handle situation when browser hangs and does not reply for a long time. After pre-configured timeout (default 50 sec) such browser application will be closed. 4. Let identify connecting websocket at the very beginning - with the first connect request. Allows to reject not allowed connection. Also for the future simplifies re-connection. 5. In TWebWindow clearly identify batch job connection, let reuse if for consequent jobs, close it after 20 sec of idle time. 6. Use std::recursive_mutex in WebWindowsManager to protect methods which creates THttpServer / creates new window / starts new web browser. Comment out special manager guard - useless without special threading functionality. 7. Correspondent changes in JSROOT. . 8. Special handling of 2d/3d canvas when producing PNG images - now is more consistent, no need to use THREE.SVGRenderer for that. 9. Add canvas size to the RPadDisplayItem class, used only in batch mode to configure image size. 10. Use workaround with static mutex to protect I/O in canvas painter in multi-threaded environment.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2603
https://github.com/root-project/root/pull/2603:164,integrability,messag,message,164,"WebWindow: cummulative patch with many fixes and updates; 1. Fix problem with fragmented websocket messages - some browsers can split messages on 120KB chunks. Now message merged together before delivered to the user. 2. Solve problem with Google Chrome run in headless mode. . Now chrome browser can run in headless mode without any special debugging flags, which opens some http ports. Now THttpServer provides special ""dummy"" script, which timedout until batch job is not finished. 3. Try to correctly handle situation when browser hangs and does not reply for a long time. After pre-configured timeout (default 50 sec) such browser application will be closed. 4. Let identify connecting websocket at the very beginning - with the first connect request. Allows to reject not allowed connection. Also for the future simplifies re-connection. 5. In TWebWindow clearly identify batch job connection, let reuse if for consequent jobs, close it after 20 sec of idle time. 6. Use std::recursive_mutex in WebWindowsManager to protect methods which creates THttpServer / creates new window / starts new web browser. Comment out special manager guard - useless without special threading functionality. 7. Correspondent changes in JSROOT. . 8. Special handling of 2d/3d canvas when producing PNG images - now is more consistent, no need to use THREE.SVGRenderer for that. 9. Add canvas size to the RPadDisplayItem class, used only in batch mode to configure image size. 10. Use workaround with static mutex to protect I/O in canvas painter in multi-threaded environment.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2603
https://github.com/root-project/root/pull/2603:458,integrability,batch,batch,458,"WebWindow: cummulative patch with many fixes and updates; 1. Fix problem with fragmented websocket messages - some browsers can split messages on 120KB chunks. Now message merged together before delivered to the user. 2. Solve problem with Google Chrome run in headless mode. . Now chrome browser can run in headless mode without any special debugging flags, which opens some http ports. Now THttpServer provides special ""dummy"" script, which timedout until batch job is not finished. 3. Try to correctly handle situation when browser hangs and does not reply for a long time. After pre-configured timeout (default 50 sec) such browser application will be closed. 4. Let identify connecting websocket at the very beginning - with the first connect request. Allows to reject not allowed connection. Also for the future simplifies re-connection. 5. In TWebWindow clearly identify batch job connection, let reuse if for consequent jobs, close it after 20 sec of idle time. 6. Use std::recursive_mutex in WebWindowsManager to protect methods which creates THttpServer / creates new window / starts new web browser. Comment out special manager guard - useless without special threading functionality. 7. Correspondent changes in JSROOT. . 8. Special handling of 2d/3d canvas when producing PNG images - now is more consistent, no need to use THREE.SVGRenderer for that. 9. Add canvas size to the RPadDisplayItem class, used only in batch mode to configure image size. 10. Use workaround with static mutex to protect I/O in canvas painter in multi-threaded environment.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2603
https://github.com/root-project/root/pull/2603:587,integrability,configur,configured,587,"WebWindow: cummulative patch with many fixes and updates; 1. Fix problem with fragmented websocket messages - some browsers can split messages on 120KB chunks. Now message merged together before delivered to the user. 2. Solve problem with Google Chrome run in headless mode. . Now chrome browser can run in headless mode without any special debugging flags, which opens some http ports. Now THttpServer provides special ""dummy"" script, which timedout until batch job is not finished. 3. Try to correctly handle situation when browser hangs and does not reply for a long time. After pre-configured timeout (default 50 sec) such browser application will be closed. 4. Let identify connecting websocket at the very beginning - with the first connect request. Allows to reject not allowed connection. Also for the future simplifies re-connection. 5. In TWebWindow clearly identify batch job connection, let reuse if for consequent jobs, close it after 20 sec of idle time. 6. Use std::recursive_mutex in WebWindowsManager to protect methods which creates THttpServer / creates new window / starts new web browser. Comment out special manager guard - useless without special threading functionality. 7. Correspondent changes in JSROOT. . 8. Special handling of 2d/3d canvas when producing PNG images - now is more consistent, no need to use THREE.SVGRenderer for that. 9. Add canvas size to the RPadDisplayItem class, used only in batch mode to configure image size. 10. Use workaround with static mutex to protect I/O in canvas painter in multi-threaded environment.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2603
https://github.com/root-project/root/pull/2603:878,integrability,batch,batch,878,"WebWindow: cummulative patch with many fixes and updates; 1. Fix problem with fragmented websocket messages - some browsers can split messages on 120KB chunks. Now message merged together before delivered to the user. 2. Solve problem with Google Chrome run in headless mode. . Now chrome browser can run in headless mode without any special debugging flags, which opens some http ports. Now THttpServer provides special ""dummy"" script, which timedout until batch job is not finished. 3. Try to correctly handle situation when browser hangs and does not reply for a long time. After pre-configured timeout (default 50 sec) such browser application will be closed. 4. Let identify connecting websocket at the very beginning - with the first connect request. Allows to reject not allowed connection. Also for the future simplifies re-connection. 5. In TWebWindow clearly identify batch job connection, let reuse if for consequent jobs, close it after 20 sec of idle time. 6. Use std::recursive_mutex in WebWindowsManager to protect methods which creates THttpServer / creates new window / starts new web browser. Comment out special manager guard - useless without special threading functionality. 7. Correspondent changes in JSROOT. . 8. Special handling of 2d/3d canvas when producing PNG images - now is more consistent, no need to use THREE.SVGRenderer for that. 9. Add canvas size to the RPadDisplayItem class, used only in batch mode to configure image size. 10. Use workaround with static mutex to protect I/O in canvas painter in multi-threaded environment.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2603
https://github.com/root-project/root/pull/2603:1427,integrability,batch,batch,1427,"WebWindow: cummulative patch with many fixes and updates; 1. Fix problem with fragmented websocket messages - some browsers can split messages on 120KB chunks. Now message merged together before delivered to the user. 2. Solve problem with Google Chrome run in headless mode. . Now chrome browser can run in headless mode without any special debugging flags, which opens some http ports. Now THttpServer provides special ""dummy"" script, which timedout until batch job is not finished. 3. Try to correctly handle situation when browser hangs and does not reply for a long time. After pre-configured timeout (default 50 sec) such browser application will be closed. 4. Let identify connecting websocket at the very beginning - with the first connect request. Allows to reject not allowed connection. Also for the future simplifies re-connection. 5. In TWebWindow clearly identify batch job connection, let reuse if for consequent jobs, close it after 20 sec of idle time. 6. Use std::recursive_mutex in WebWindowsManager to protect methods which creates THttpServer / creates new window / starts new web browser. Comment out special manager guard - useless without special threading functionality. 7. Correspondent changes in JSROOT. . 8. Special handling of 2d/3d canvas when producing PNG images - now is more consistent, no need to use THREE.SVGRenderer for that. 9. Add canvas size to the RPadDisplayItem class, used only in batch mode to configure image size. 10. Use workaround with static mutex to protect I/O in canvas painter in multi-threaded environment.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2603
https://github.com/root-project/root/pull/2603:1441,integrability,configur,configure,1441,"WebWindow: cummulative patch with many fixes and updates; 1. Fix problem with fragmented websocket messages - some browsers can split messages on 120KB chunks. Now message merged together before delivered to the user. 2. Solve problem with Google Chrome run in headless mode. . Now chrome browser can run in headless mode without any special debugging flags, which opens some http ports. Now THttpServer provides special ""dummy"" script, which timedout until batch job is not finished. 3. Try to correctly handle situation when browser hangs and does not reply for a long time. After pre-configured timeout (default 50 sec) such browser application will be closed. 4. Let identify connecting websocket at the very beginning - with the first connect request. Allows to reject not allowed connection. Also for the future simplifies re-connection. 5. In TWebWindow clearly identify batch job connection, let reuse if for consequent jobs, close it after 20 sec of idle time. 6. Use std::recursive_mutex in WebWindowsManager to protect methods which creates THttpServer / creates new window / starts new web browser. Comment out special manager guard - useless without special threading functionality. 7. Correspondent changes in JSROOT. . 8. Special handling of 2d/3d canvas when producing PNG images - now is more consistent, no need to use THREE.SVGRenderer for that. 9. Add canvas size to the RPadDisplayItem class, used only in batch mode to configure image size. 10. Use workaround with static mutex to protect I/O in canvas painter in multi-threaded environment.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2603
https://github.com/root-project/root/pull/2603:99,interoperability,messag,messages,99,"WebWindow: cummulative patch with many fixes and updates; 1. Fix problem with fragmented websocket messages - some browsers can split messages on 120KB chunks. Now message merged together before delivered to the user. 2. Solve problem with Google Chrome run in headless mode. . Now chrome browser can run in headless mode without any special debugging flags, which opens some http ports. Now THttpServer provides special ""dummy"" script, which timedout until batch job is not finished. 3. Try to correctly handle situation when browser hangs and does not reply for a long time. After pre-configured timeout (default 50 sec) such browser application will be closed. 4. Let identify connecting websocket at the very beginning - with the first connect request. Allows to reject not allowed connection. Also for the future simplifies re-connection. 5. In TWebWindow clearly identify batch job connection, let reuse if for consequent jobs, close it after 20 sec of idle time. 6. Use std::recursive_mutex in WebWindowsManager to protect methods which creates THttpServer / creates new window / starts new web browser. Comment out special manager guard - useless without special threading functionality. 7. Correspondent changes in JSROOT. . 8. Special handling of 2d/3d canvas when producing PNG images - now is more consistent, no need to use THREE.SVGRenderer for that. 9. Add canvas size to the RPadDisplayItem class, used only in batch mode to configure image size. 10. Use workaround with static mutex to protect I/O in canvas painter in multi-threaded environment.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2603
https://github.com/root-project/root/pull/2603:134,interoperability,messag,messages,134,"WebWindow: cummulative patch with many fixes and updates; 1. Fix problem with fragmented websocket messages - some browsers can split messages on 120KB chunks. Now message merged together before delivered to the user. 2. Solve problem with Google Chrome run in headless mode. . Now chrome browser can run in headless mode without any special debugging flags, which opens some http ports. Now THttpServer provides special ""dummy"" script, which timedout until batch job is not finished. 3. Try to correctly handle situation when browser hangs and does not reply for a long time. After pre-configured timeout (default 50 sec) such browser application will be closed. 4. Let identify connecting websocket at the very beginning - with the first connect request. Allows to reject not allowed connection. Also for the future simplifies re-connection. 5. In TWebWindow clearly identify batch job connection, let reuse if for consequent jobs, close it after 20 sec of idle time. 6. Use std::recursive_mutex in WebWindowsManager to protect methods which creates THttpServer / creates new window / starts new web browser. Comment out special manager guard - useless without special threading functionality. 7. Correspondent changes in JSROOT. . 8. Special handling of 2d/3d canvas when producing PNG images - now is more consistent, no need to use THREE.SVGRenderer for that. 9. Add canvas size to the RPadDisplayItem class, used only in batch mode to configure image size. 10. Use workaround with static mutex to protect I/O in canvas painter in multi-threaded environment.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2603
https://github.com/root-project/root/pull/2603:164,interoperability,messag,message,164,"WebWindow: cummulative patch with many fixes and updates; 1. Fix problem with fragmented websocket messages - some browsers can split messages on 120KB chunks. Now message merged together before delivered to the user. 2. Solve problem with Google Chrome run in headless mode. . Now chrome browser can run in headless mode without any special debugging flags, which opens some http ports. Now THttpServer provides special ""dummy"" script, which timedout until batch job is not finished. 3. Try to correctly handle situation when browser hangs and does not reply for a long time. After pre-configured timeout (default 50 sec) such browser application will be closed. 4. Let identify connecting websocket at the very beginning - with the first connect request. Allows to reject not allowed connection. Also for the future simplifies re-connection. 5. In TWebWindow clearly identify batch job connection, let reuse if for consequent jobs, close it after 20 sec of idle time. 6. Use std::recursive_mutex in WebWindowsManager to protect methods which creates THttpServer / creates new window / starts new web browser. Comment out special manager guard - useless without special threading functionality. 7. Correspondent changes in JSROOT. . 8. Special handling of 2d/3d canvas when producing PNG images - now is more consistent, no need to use THREE.SVGRenderer for that. 9. Add canvas size to the RPadDisplayItem class, used only in batch mode to configure image size. 10. Use workaround with static mutex to protect I/O in canvas painter in multi-threaded environment.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2603
https://github.com/root-project/root/pull/2603:959,interoperability,idl,idle,959,"WebWindow: cummulative patch with many fixes and updates; 1. Fix problem with fragmented websocket messages - some browsers can split messages on 120KB chunks. Now message merged together before delivered to the user. 2. Solve problem with Google Chrome run in headless mode. . Now chrome browser can run in headless mode without any special debugging flags, which opens some http ports. Now THttpServer provides special ""dummy"" script, which timedout until batch job is not finished. 3. Try to correctly handle situation when browser hangs and does not reply for a long time. After pre-configured timeout (default 50 sec) such browser application will be closed. 4. Let identify connecting websocket at the very beginning - with the first connect request. Allows to reject not allowed connection. Also for the future simplifies re-connection. 5. In TWebWindow clearly identify batch job connection, let reuse if for consequent jobs, close it after 20 sec of idle time. 6. Use std::recursive_mutex in WebWindowsManager to protect methods which creates THttpServer / creates new window / starts new web browser. Comment out special manager guard - useless without special threading functionality. 7. Correspondent changes in JSROOT. . 8. Special handling of 2d/3d canvas when producing PNG images - now is more consistent, no need to use THREE.SVGRenderer for that. 9. Add canvas size to the RPadDisplayItem class, used only in batch mode to configure image size. 10. Use workaround with static mutex to protect I/O in canvas painter in multi-threaded environment.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2603
https://github.com/root-project/root/pull/2603:587,modifiability,configur,configured,587,"WebWindow: cummulative patch with many fixes and updates; 1. Fix problem with fragmented websocket messages - some browsers can split messages on 120KB chunks. Now message merged together before delivered to the user. 2. Solve problem with Google Chrome run in headless mode. . Now chrome browser can run in headless mode without any special debugging flags, which opens some http ports. Now THttpServer provides special ""dummy"" script, which timedout until batch job is not finished. 3. Try to correctly handle situation when browser hangs and does not reply for a long time. After pre-configured timeout (default 50 sec) such browser application will be closed. 4. Let identify connecting websocket at the very beginning - with the first connect request. Allows to reject not allowed connection. Also for the future simplifies re-connection. 5. In TWebWindow clearly identify batch job connection, let reuse if for consequent jobs, close it after 20 sec of idle time. 6. Use std::recursive_mutex in WebWindowsManager to protect methods which creates THttpServer / creates new window / starts new web browser. Comment out special manager guard - useless without special threading functionality. 7. Correspondent changes in JSROOT. . 8. Special handling of 2d/3d canvas when producing PNG images - now is more consistent, no need to use THREE.SVGRenderer for that. 9. Add canvas size to the RPadDisplayItem class, used only in batch mode to configure image size. 10. Use workaround with static mutex to protect I/O in canvas painter in multi-threaded environment.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2603
https://github.com/root-project/root/pull/2603:904,modifiability,reu,reuse,904,"WebWindow: cummulative patch with many fixes and updates; 1. Fix problem with fragmented websocket messages - some browsers can split messages on 120KB chunks. Now message merged together before delivered to the user. 2. Solve problem with Google Chrome run in headless mode. . Now chrome browser can run in headless mode without any special debugging flags, which opens some http ports. Now THttpServer provides special ""dummy"" script, which timedout until batch job is not finished. 3. Try to correctly handle situation when browser hangs and does not reply for a long time. After pre-configured timeout (default 50 sec) such browser application will be closed. 4. Let identify connecting websocket at the very beginning - with the first connect request. Allows to reject not allowed connection. Also for the future simplifies re-connection. 5. In TWebWindow clearly identify batch job connection, let reuse if for consequent jobs, close it after 20 sec of idle time. 6. Use std::recursive_mutex in WebWindowsManager to protect methods which creates THttpServer / creates new window / starts new web browser. Comment out special manager guard - useless without special threading functionality. 7. Correspondent changes in JSROOT. . 8. Special handling of 2d/3d canvas when producing PNG images - now is more consistent, no need to use THREE.SVGRenderer for that. 9. Add canvas size to the RPadDisplayItem class, used only in batch mode to configure image size. 10. Use workaround with static mutex to protect I/O in canvas painter in multi-threaded environment.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2603
https://github.com/root-project/root/pull/2603:1441,modifiability,configur,configure,1441,"WebWindow: cummulative patch with many fixes and updates; 1. Fix problem with fragmented websocket messages - some browsers can split messages on 120KB chunks. Now message merged together before delivered to the user. 2. Solve problem with Google Chrome run in headless mode. . Now chrome browser can run in headless mode without any special debugging flags, which opens some http ports. Now THttpServer provides special ""dummy"" script, which timedout until batch job is not finished. 3. Try to correctly handle situation when browser hangs and does not reply for a long time. After pre-configured timeout (default 50 sec) such browser application will be closed. 4. Let identify connecting websocket at the very beginning - with the first connect request. Allows to reject not allowed connection. Also for the future simplifies re-connection. 5. In TWebWindow clearly identify batch job connection, let reuse if for consequent jobs, close it after 20 sec of idle time. 6. Use std::recursive_mutex in WebWindowsManager to protect methods which creates THttpServer / creates new window / starts new web browser. Comment out special manager guard - useless without special threading functionality. 7. Correspondent changes in JSROOT. . 8. Special handling of 2d/3d canvas when producing PNG images - now is more consistent, no need to use THREE.SVGRenderer for that. 9. Add canvas size to the RPadDisplayItem class, used only in batch mode to configure image size. 10. Use workaround with static mutex to protect I/O in canvas painter in multi-threaded environment.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2603
https://github.com/root-project/root/pull/2603:443,performance,time,timedout,443,"WebWindow: cummulative patch with many fixes and updates; 1. Fix problem with fragmented websocket messages - some browsers can split messages on 120KB chunks. Now message merged together before delivered to the user. 2. Solve problem with Google Chrome run in headless mode. . Now chrome browser can run in headless mode without any special debugging flags, which opens some http ports. Now THttpServer provides special ""dummy"" script, which timedout until batch job is not finished. 3. Try to correctly handle situation when browser hangs and does not reply for a long time. After pre-configured timeout (default 50 sec) such browser application will be closed. 4. Let identify connecting websocket at the very beginning - with the first connect request. Allows to reject not allowed connection. Also for the future simplifies re-connection. 5. In TWebWindow clearly identify batch job connection, let reuse if for consequent jobs, close it after 20 sec of idle time. 6. Use std::recursive_mutex in WebWindowsManager to protect methods which creates THttpServer / creates new window / starts new web browser. Comment out special manager guard - useless without special threading functionality. 7. Correspondent changes in JSROOT. . 8. Special handling of 2d/3d canvas when producing PNG images - now is more consistent, no need to use THREE.SVGRenderer for that. 9. Add canvas size to the RPadDisplayItem class, used only in batch mode to configure image size. 10. Use workaround with static mutex to protect I/O in canvas painter in multi-threaded environment.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2603
https://github.com/root-project/root/pull/2603:458,performance,batch,batch,458,"WebWindow: cummulative patch with many fixes and updates; 1. Fix problem with fragmented websocket messages - some browsers can split messages on 120KB chunks. Now message merged together before delivered to the user. 2. Solve problem with Google Chrome run in headless mode. . Now chrome browser can run in headless mode without any special debugging flags, which opens some http ports. Now THttpServer provides special ""dummy"" script, which timedout until batch job is not finished. 3. Try to correctly handle situation when browser hangs and does not reply for a long time. After pre-configured timeout (default 50 sec) such browser application will be closed. 4. Let identify connecting websocket at the very beginning - with the first connect request. Allows to reject not allowed connection. Also for the future simplifies re-connection. 5. In TWebWindow clearly identify batch job connection, let reuse if for consequent jobs, close it after 20 sec of idle time. 6. Use std::recursive_mutex in WebWindowsManager to protect methods which creates THttpServer / creates new window / starts new web browser. Comment out special manager guard - useless without special threading functionality. 7. Correspondent changes in JSROOT. . 8. Special handling of 2d/3d canvas when producing PNG images - now is more consistent, no need to use THREE.SVGRenderer for that. 9. Add canvas size to the RPadDisplayItem class, used only in batch mode to configure image size. 10. Use workaround with static mutex to protect I/O in canvas painter in multi-threaded environment.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2603
https://github.com/root-project/root/pull/2603:571,performance,time,time,571,"WebWindow: cummulative patch with many fixes and updates; 1. Fix problem with fragmented websocket messages - some browsers can split messages on 120KB chunks. Now message merged together before delivered to the user. 2. Solve problem with Google Chrome run in headless mode. . Now chrome browser can run in headless mode without any special debugging flags, which opens some http ports. Now THttpServer provides special ""dummy"" script, which timedout until batch job is not finished. 3. Try to correctly handle situation when browser hangs and does not reply for a long time. After pre-configured timeout (default 50 sec) such browser application will be closed. 4. Let identify connecting websocket at the very beginning - with the first connect request. Allows to reject not allowed connection. Also for the future simplifies re-connection. 5. In TWebWindow clearly identify batch job connection, let reuse if for consequent jobs, close it after 20 sec of idle time. 6. Use std::recursive_mutex in WebWindowsManager to protect methods which creates THttpServer / creates new window / starts new web browser. Comment out special manager guard - useless without special threading functionality. 7. Correspondent changes in JSROOT. . 8. Special handling of 2d/3d canvas when producing PNG images - now is more consistent, no need to use THREE.SVGRenderer for that. 9. Add canvas size to the RPadDisplayItem class, used only in batch mode to configure image size. 10. Use workaround with static mutex to protect I/O in canvas painter in multi-threaded environment.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2603
https://github.com/root-project/root/pull/2603:598,performance,time,timeout,598,"WebWindow: cummulative patch with many fixes and updates; 1. Fix problem with fragmented websocket messages - some browsers can split messages on 120KB chunks. Now message merged together before delivered to the user. 2. Solve problem with Google Chrome run in headless mode. . Now chrome browser can run in headless mode without any special debugging flags, which opens some http ports. Now THttpServer provides special ""dummy"" script, which timedout until batch job is not finished. 3. Try to correctly handle situation when browser hangs and does not reply for a long time. After pre-configured timeout (default 50 sec) such browser application will be closed. 4. Let identify connecting websocket at the very beginning - with the first connect request. Allows to reject not allowed connection. Also for the future simplifies re-connection. 5. In TWebWindow clearly identify batch job connection, let reuse if for consequent jobs, close it after 20 sec of idle time. 6. Use std::recursive_mutex in WebWindowsManager to protect methods which creates THttpServer / creates new window / starts new web browser. Comment out special manager guard - useless without special threading functionality. 7. Correspondent changes in JSROOT. . 8. Special handling of 2d/3d canvas when producing PNG images - now is more consistent, no need to use THREE.SVGRenderer for that. 9. Add canvas size to the RPadDisplayItem class, used only in batch mode to configure image size. 10. Use workaround with static mutex to protect I/O in canvas painter in multi-threaded environment.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2603
https://github.com/root-project/root/pull/2603:878,performance,batch,batch,878,"WebWindow: cummulative patch with many fixes and updates; 1. Fix problem with fragmented websocket messages - some browsers can split messages on 120KB chunks. Now message merged together before delivered to the user. 2. Solve problem with Google Chrome run in headless mode. . Now chrome browser can run in headless mode without any special debugging flags, which opens some http ports. Now THttpServer provides special ""dummy"" script, which timedout until batch job is not finished. 3. Try to correctly handle situation when browser hangs and does not reply for a long time. After pre-configured timeout (default 50 sec) such browser application will be closed. 4. Let identify connecting websocket at the very beginning - with the first connect request. Allows to reject not allowed connection. Also for the future simplifies re-connection. 5. In TWebWindow clearly identify batch job connection, let reuse if for consequent jobs, close it after 20 sec of idle time. 6. Use std::recursive_mutex in WebWindowsManager to protect methods which creates THttpServer / creates new window / starts new web browser. Comment out special manager guard - useless without special threading functionality. 7. Correspondent changes in JSROOT. . 8. Special handling of 2d/3d canvas when producing PNG images - now is more consistent, no need to use THREE.SVGRenderer for that. 9. Add canvas size to the RPadDisplayItem class, used only in batch mode to configure image size. 10. Use workaround with static mutex to protect I/O in canvas painter in multi-threaded environment.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2603
https://github.com/root-project/root/pull/2603:964,performance,time,time,964,"WebWindow: cummulative patch with many fixes and updates; 1. Fix problem with fragmented websocket messages - some browsers can split messages on 120KB chunks. Now message merged together before delivered to the user. 2. Solve problem with Google Chrome run in headless mode. . Now chrome browser can run in headless mode without any special debugging flags, which opens some http ports. Now THttpServer provides special ""dummy"" script, which timedout until batch job is not finished. 3. Try to correctly handle situation when browser hangs and does not reply for a long time. After pre-configured timeout (default 50 sec) such browser application will be closed. 4. Let identify connecting websocket at the very beginning - with the first connect request. Allows to reject not allowed connection. Also for the future simplifies re-connection. 5. In TWebWindow clearly identify batch job connection, let reuse if for consequent jobs, close it after 20 sec of idle time. 6. Use std::recursive_mutex in WebWindowsManager to protect methods which creates THttpServer / creates new window / starts new web browser. Comment out special manager guard - useless without special threading functionality. 7. Correspondent changes in JSROOT. . 8. Special handling of 2d/3d canvas when producing PNG images - now is more consistent, no need to use THREE.SVGRenderer for that. 9. Add canvas size to the RPadDisplayItem class, used only in batch mode to configure image size. 10. Use workaround with static mutex to protect I/O in canvas painter in multi-threaded environment.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2603
https://github.com/root-project/root/pull/2603:1427,performance,batch,batch,1427,"WebWindow: cummulative patch with many fixes and updates; 1. Fix problem with fragmented websocket messages - some browsers can split messages on 120KB chunks. Now message merged together before delivered to the user. 2. Solve problem with Google Chrome run in headless mode. . Now chrome browser can run in headless mode without any special debugging flags, which opens some http ports. Now THttpServer provides special ""dummy"" script, which timedout until batch job is not finished. 3. Try to correctly handle situation when browser hangs and does not reply for a long time. After pre-configured timeout (default 50 sec) such browser application will be closed. 4. Let identify connecting websocket at the very beginning - with the first connect request. Allows to reject not allowed connection. Also for the future simplifies re-connection. 5. In TWebWindow clearly identify batch job connection, let reuse if for consequent jobs, close it after 20 sec of idle time. 6. Use std::recursive_mutex in WebWindowsManager to protect methods which creates THttpServer / creates new window / starts new web browser. Comment out special manager guard - useless without special threading functionality. 7. Correspondent changes in JSROOT. . 8. Special handling of 2d/3d canvas when producing PNG images - now is more consistent, no need to use THREE.SVGRenderer for that. 9. Add canvas size to the RPadDisplayItem class, used only in batch mode to configure image size. 10. Use workaround with static mutex to protect I/O in canvas painter in multi-threaded environment.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2603
https://github.com/root-project/root/pull/2603:1511,performance,I/O,I/O,1511,"WebWindow: cummulative patch with many fixes and updates; 1. Fix problem with fragmented websocket messages - some browsers can split messages on 120KB chunks. Now message merged together before delivered to the user. 2. Solve problem with Google Chrome run in headless mode. . Now chrome browser can run in headless mode without any special debugging flags, which opens some http ports. Now THttpServer provides special ""dummy"" script, which timedout until batch job is not finished. 3. Try to correctly handle situation when browser hangs and does not reply for a long time. After pre-configured timeout (default 50 sec) such browser application will be closed. 4. Let identify connecting websocket at the very beginning - with the first connect request. Allows to reject not allowed connection. Also for the future simplifies re-connection. 5. In TWebWindow clearly identify batch job connection, let reuse if for consequent jobs, close it after 20 sec of idle time. 6. Use std::recursive_mutex in WebWindowsManager to protect methods which creates THttpServer / creates new window / starts new web browser. Comment out special manager guard - useless without special threading functionality. 7. Correspondent changes in JSROOT. . 8. Special handling of 2d/3d canvas when producing PNG images - now is more consistent, no need to use THREE.SVGRenderer for that. 9. Add canvas size to the RPadDisplayItem class, used only in batch mode to configure image size. 10. Use workaround with static mutex to protect I/O in canvas painter in multi-threaded environment.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2603
https://github.com/root-project/root/pull/2603:1536,performance,multi-thread,multi-threaded,1536,"WebWindow: cummulative patch with many fixes and updates; 1. Fix problem with fragmented websocket messages - some browsers can split messages on 120KB chunks. Now message merged together before delivered to the user. 2. Solve problem with Google Chrome run in headless mode. . Now chrome browser can run in headless mode without any special debugging flags, which opens some http ports. Now THttpServer provides special ""dummy"" script, which timedout until batch job is not finished. 3. Try to correctly handle situation when browser hangs and does not reply for a long time. After pre-configured timeout (default 50 sec) such browser application will be closed. 4. Let identify connecting websocket at the very beginning - with the first connect request. Allows to reject not allowed connection. Also for the future simplifies re-connection. 5. In TWebWindow clearly identify batch job connection, let reuse if for consequent jobs, close it after 20 sec of idle time. 6. Use std::recursive_mutex in WebWindowsManager to protect methods which creates THttpServer / creates new window / starts new web browser. Comment out special manager guard - useless without special threading functionality. 7. Correspondent changes in JSROOT. . 8. Special handling of 2d/3d canvas when producing PNG images - now is more consistent, no need to use THREE.SVGRenderer for that. 9. Add canvas size to the RPadDisplayItem class, used only in batch mode to configure image size. 10. Use workaround with static mutex to protect I/O in canvas painter in multi-threaded environment.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2603
https://github.com/root-project/root/pull/2603:545,reliability,doe,does,545,"WebWindow: cummulative patch with many fixes and updates; 1. Fix problem with fragmented websocket messages - some browsers can split messages on 120KB chunks. Now message merged together before delivered to the user. 2. Solve problem with Google Chrome run in headless mode. . Now chrome browser can run in headless mode without any special debugging flags, which opens some http ports. Now THttpServer provides special ""dummy"" script, which timedout until batch job is not finished. 3. Try to correctly handle situation when browser hangs and does not reply for a long time. After pre-configured timeout (default 50 sec) such browser application will be closed. 4. Let identify connecting websocket at the very beginning - with the first connect request. Allows to reject not allowed connection. Also for the future simplifies re-connection. 5. In TWebWindow clearly identify batch job connection, let reuse if for consequent jobs, close it after 20 sec of idle time. 6. Use std::recursive_mutex in WebWindowsManager to protect methods which creates THttpServer / creates new window / starts new web browser. Comment out special manager guard - useless without special threading functionality. 7. Correspondent changes in JSROOT. . 8. Special handling of 2d/3d canvas when producing PNG images - now is more consistent, no need to use THREE.SVGRenderer for that. 9. Add canvas size to the RPadDisplayItem class, used only in batch mode to configure image size. 10. Use workaround with static mutex to protect I/O in canvas painter in multi-threaded environment.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2603
https://github.com/root-project/root/pull/2603:23,safety,patch,patch,23,"WebWindow: cummulative patch with many fixes and updates; 1. Fix problem with fragmented websocket messages - some browsers can split messages on 120KB chunks. Now message merged together before delivered to the user. 2. Solve problem with Google Chrome run in headless mode. . Now chrome browser can run in headless mode without any special debugging flags, which opens some http ports. Now THttpServer provides special ""dummy"" script, which timedout until batch job is not finished. 3. Try to correctly handle situation when browser hangs and does not reply for a long time. After pre-configured timeout (default 50 sec) such browser application will be closed. 4. Let identify connecting websocket at the very beginning - with the first connect request. Allows to reject not allowed connection. Also for the future simplifies re-connection. 5. In TWebWindow clearly identify batch job connection, let reuse if for consequent jobs, close it after 20 sec of idle time. 6. Use std::recursive_mutex in WebWindowsManager to protect methods which creates THttpServer / creates new window / starts new web browser. Comment out special manager guard - useless without special threading functionality. 7. Correspondent changes in JSROOT. . 8. Special handling of 2d/3d canvas when producing PNG images - now is more consistent, no need to use THREE.SVGRenderer for that. 9. Add canvas size to the RPadDisplayItem class, used only in batch mode to configure image size. 10. Use workaround with static mutex to protect I/O in canvas painter in multi-threaded environment.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2603
https://github.com/root-project/root/pull/2603:49,safety,updat,updates,49,"WebWindow: cummulative patch with many fixes and updates; 1. Fix problem with fragmented websocket messages - some browsers can split messages on 120KB chunks. Now message merged together before delivered to the user. 2. Solve problem with Google Chrome run in headless mode. . Now chrome browser can run in headless mode without any special debugging flags, which opens some http ports. Now THttpServer provides special ""dummy"" script, which timedout until batch job is not finished. 3. Try to correctly handle situation when browser hangs and does not reply for a long time. After pre-configured timeout (default 50 sec) such browser application will be closed. 4. Let identify connecting websocket at the very beginning - with the first connect request. Allows to reject not allowed connection. Also for the future simplifies re-connection. 5. In TWebWindow clearly identify batch job connection, let reuse if for consequent jobs, close it after 20 sec of idle time. 6. Use std::recursive_mutex in WebWindowsManager to protect methods which creates THttpServer / creates new window / starts new web browser. Comment out special manager guard - useless without special threading functionality. 7. Correspondent changes in JSROOT. . 8. Special handling of 2d/3d canvas when producing PNG images - now is more consistent, no need to use THREE.SVGRenderer for that. 9. Add canvas size to the RPadDisplayItem class, used only in batch mode to configure image size. 10. Use workaround with static mutex to protect I/O in canvas painter in multi-threaded environment.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2603
https://github.com/root-project/root/pull/2603:598,safety,timeout,timeout,598,"WebWindow: cummulative patch with many fixes and updates; 1. Fix problem with fragmented websocket messages - some browsers can split messages on 120KB chunks. Now message merged together before delivered to the user. 2. Solve problem with Google Chrome run in headless mode. . Now chrome browser can run in headless mode without any special debugging flags, which opens some http ports. Now THttpServer provides special ""dummy"" script, which timedout until batch job is not finished. 3. Try to correctly handle situation when browser hangs and does not reply for a long time. After pre-configured timeout (default 50 sec) such browser application will be closed. 4. Let identify connecting websocket at the very beginning - with the first connect request. Allows to reject not allowed connection. Also for the future simplifies re-connection. 5. In TWebWindow clearly identify batch job connection, let reuse if for consequent jobs, close it after 20 sec of idle time. 6. Use std::recursive_mutex in WebWindowsManager to protect methods which creates THttpServer / creates new window / starts new web browser. Comment out special manager guard - useless without special threading functionality. 7. Correspondent changes in JSROOT. . 8. Special handling of 2d/3d canvas when producing PNG images - now is more consistent, no need to use THREE.SVGRenderer for that. 9. Add canvas size to the RPadDisplayItem class, used only in batch mode to configure image size. 10. Use workaround with static mutex to protect I/O in canvas painter in multi-threaded environment.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2603
https://github.com/root-project/root/pull/2603:1131,safety,manag,manager,1131,"WebWindow: cummulative patch with many fixes and updates; 1. Fix problem with fragmented websocket messages - some browsers can split messages on 120KB chunks. Now message merged together before delivered to the user. 2. Solve problem with Google Chrome run in headless mode. . Now chrome browser can run in headless mode without any special debugging flags, which opens some http ports. Now THttpServer provides special ""dummy"" script, which timedout until batch job is not finished. 3. Try to correctly handle situation when browser hangs and does not reply for a long time. After pre-configured timeout (default 50 sec) such browser application will be closed. 4. Let identify connecting websocket at the very beginning - with the first connect request. Allows to reject not allowed connection. Also for the future simplifies re-connection. 5. In TWebWindow clearly identify batch job connection, let reuse if for consequent jobs, close it after 20 sec of idle time. 6. Use std::recursive_mutex in WebWindowsManager to protect methods which creates THttpServer / creates new window / starts new web browser. Comment out special manager guard - useless without special threading functionality. 7. Correspondent changes in JSROOT. . 8. Special handling of 2d/3d canvas when producing PNG images - now is more consistent, no need to use THREE.SVGRenderer for that. 9. Add canvas size to the RPadDisplayItem class, used only in batch mode to configure image size. 10. Use workaround with static mutex to protect I/O in canvas painter in multi-threaded environment.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2603
https://github.com/root-project/root/pull/2603:23,security,patch,patch,23,"WebWindow: cummulative patch with many fixes and updates; 1. Fix problem with fragmented websocket messages - some browsers can split messages on 120KB chunks. Now message merged together before delivered to the user. 2. Solve problem with Google Chrome run in headless mode. . Now chrome browser can run in headless mode without any special debugging flags, which opens some http ports. Now THttpServer provides special ""dummy"" script, which timedout until batch job is not finished. 3. Try to correctly handle situation when browser hangs and does not reply for a long time. After pre-configured timeout (default 50 sec) such browser application will be closed. 4. Let identify connecting websocket at the very beginning - with the first connect request. Allows to reject not allowed connection. Also for the future simplifies re-connection. 5. In TWebWindow clearly identify batch job connection, let reuse if for consequent jobs, close it after 20 sec of idle time. 6. Use std::recursive_mutex in WebWindowsManager to protect methods which creates THttpServer / creates new window / starts new web browser. Comment out special manager guard - useless without special threading functionality. 7. Correspondent changes in JSROOT. . 8. Special handling of 2d/3d canvas when producing PNG images - now is more consistent, no need to use THREE.SVGRenderer for that. 9. Add canvas size to the RPadDisplayItem class, used only in batch mode to configure image size. 10. Use workaround with static mutex to protect I/O in canvas painter in multi-threaded environment.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2603
https://github.com/root-project/root/pull/2603:49,security,updat,updates,49,"WebWindow: cummulative patch with many fixes and updates; 1. Fix problem with fragmented websocket messages - some browsers can split messages on 120KB chunks. Now message merged together before delivered to the user. 2. Solve problem with Google Chrome run in headless mode. . Now chrome browser can run in headless mode without any special debugging flags, which opens some http ports. Now THttpServer provides special ""dummy"" script, which timedout until batch job is not finished. 3. Try to correctly handle situation when browser hangs and does not reply for a long time. After pre-configured timeout (default 50 sec) such browser application will be closed. 4. Let identify connecting websocket at the very beginning - with the first connect request. Allows to reject not allowed connection. Also for the future simplifies re-connection. 5. In TWebWindow clearly identify batch job connection, let reuse if for consequent jobs, close it after 20 sec of idle time. 6. Use std::recursive_mutex in WebWindowsManager to protect methods which creates THttpServer / creates new window / starts new web browser. Comment out special manager guard - useless without special threading functionality. 7. Correspondent changes in JSROOT. . 8. Special handling of 2d/3d canvas when producing PNG images - now is more consistent, no need to use THREE.SVGRenderer for that. 9. Add canvas size to the RPadDisplayItem class, used only in batch mode to configure image size. 10. Use workaround with static mutex to protect I/O in canvas painter in multi-threaded environment.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2603
https://github.com/root-project/root/pull/2603:587,security,configur,configured,587,"WebWindow: cummulative patch with many fixes and updates; 1. Fix problem with fragmented websocket messages - some browsers can split messages on 120KB chunks. Now message merged together before delivered to the user. 2. Solve problem with Google Chrome run in headless mode. . Now chrome browser can run in headless mode without any special debugging flags, which opens some http ports. Now THttpServer provides special ""dummy"" script, which timedout until batch job is not finished. 3. Try to correctly handle situation when browser hangs and does not reply for a long time. After pre-configured timeout (default 50 sec) such browser application will be closed. 4. Let identify connecting websocket at the very beginning - with the first connect request. Allows to reject not allowed connection. Also for the future simplifies re-connection. 5. In TWebWindow clearly identify batch job connection, let reuse if for consequent jobs, close it after 20 sec of idle time. 6. Use std::recursive_mutex in WebWindowsManager to protect methods which creates THttpServer / creates new window / starts new web browser. Comment out special manager guard - useless without special threading functionality. 7. Correspondent changes in JSROOT. . 8. Special handling of 2d/3d canvas when producing PNG images - now is more consistent, no need to use THREE.SVGRenderer for that. 9. Add canvas size to the RPadDisplayItem class, used only in batch mode to configure image size. 10. Use workaround with static mutex to protect I/O in canvas painter in multi-threaded environment.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2603
https://github.com/root-project/root/pull/2603:671,security,ident,identify,671,"WebWindow: cummulative patch with many fixes and updates; 1. Fix problem with fragmented websocket messages - some browsers can split messages on 120KB chunks. Now message merged together before delivered to the user. 2. Solve problem with Google Chrome run in headless mode. . Now chrome browser can run in headless mode without any special debugging flags, which opens some http ports. Now THttpServer provides special ""dummy"" script, which timedout until batch job is not finished. 3. Try to correctly handle situation when browser hangs and does not reply for a long time. After pre-configured timeout (default 50 sec) such browser application will be closed. 4. Let identify connecting websocket at the very beginning - with the first connect request. Allows to reject not allowed connection. Also for the future simplifies re-connection. 5. In TWebWindow clearly identify batch job connection, let reuse if for consequent jobs, close it after 20 sec of idle time. 6. Use std::recursive_mutex in WebWindowsManager to protect methods which creates THttpServer / creates new window / starts new web browser. Comment out special manager guard - useless without special threading functionality. 7. Correspondent changes in JSROOT. . 8. Special handling of 2d/3d canvas when producing PNG images - now is more consistent, no need to use THREE.SVGRenderer for that. 9. Add canvas size to the RPadDisplayItem class, used only in batch mode to configure image size. 10. Use workaround with static mutex to protect I/O in canvas painter in multi-threaded environment.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2603
https://github.com/root-project/root/pull/2603:869,security,ident,identify,869,"WebWindow: cummulative patch with many fixes and updates; 1. Fix problem with fragmented websocket messages - some browsers can split messages on 120KB chunks. Now message merged together before delivered to the user. 2. Solve problem with Google Chrome run in headless mode. . Now chrome browser can run in headless mode without any special debugging flags, which opens some http ports. Now THttpServer provides special ""dummy"" script, which timedout until batch job is not finished. 3. Try to correctly handle situation when browser hangs and does not reply for a long time. After pre-configured timeout (default 50 sec) such browser application will be closed. 4. Let identify connecting websocket at the very beginning - with the first connect request. Allows to reject not allowed connection. Also for the future simplifies re-connection. 5. In TWebWindow clearly identify batch job connection, let reuse if for consequent jobs, close it after 20 sec of idle time. 6. Use std::recursive_mutex in WebWindowsManager to protect methods which creates THttpServer / creates new window / starts new web browser. Comment out special manager guard - useless without special threading functionality. 7. Correspondent changes in JSROOT. . 8. Special handling of 2d/3d canvas when producing PNG images - now is more consistent, no need to use THREE.SVGRenderer for that. 9. Add canvas size to the RPadDisplayItem class, used only in batch mode to configure image size. 10. Use workaround with static mutex to protect I/O in canvas painter in multi-threaded environment.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2603
https://github.com/root-project/root/pull/2603:1441,security,configur,configure,1441,"WebWindow: cummulative patch with many fixes and updates; 1. Fix problem with fragmented websocket messages - some browsers can split messages on 120KB chunks. Now message merged together before delivered to the user. 2. Solve problem with Google Chrome run in headless mode. . Now chrome browser can run in headless mode without any special debugging flags, which opens some http ports. Now THttpServer provides special ""dummy"" script, which timedout until batch job is not finished. 3. Try to correctly handle situation when browser hangs and does not reply for a long time. After pre-configured timeout (default 50 sec) such browser application will be closed. 4. Let identify connecting websocket at the very beginning - with the first connect request. Allows to reject not allowed connection. Also for the future simplifies re-connection. 5. In TWebWindow clearly identify batch job connection, let reuse if for consequent jobs, close it after 20 sec of idle time. 6. Use std::recursive_mutex in WebWindowsManager to protect methods which creates THttpServer / creates new window / starts new web browser. Comment out special manager guard - useless without special threading functionality. 7. Correspondent changes in JSROOT. . 8. Special handling of 2d/3d canvas when producing PNG images - now is more consistent, no need to use THREE.SVGRenderer for that. 9. Add canvas size to the RPadDisplayItem class, used only in batch mode to configure image size. 10. Use workaround with static mutex to protect I/O in canvas painter in multi-threaded environment.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2603
https://github.com/root-project/root/pull/2603:818,testability,simpl,simplifies,818,"WebWindow: cummulative patch with many fixes and updates; 1. Fix problem with fragmented websocket messages - some browsers can split messages on 120KB chunks. Now message merged together before delivered to the user. 2. Solve problem with Google Chrome run in headless mode. . Now chrome browser can run in headless mode without any special debugging flags, which opens some http ports. Now THttpServer provides special ""dummy"" script, which timedout until batch job is not finished. 3. Try to correctly handle situation when browser hangs and does not reply for a long time. After pre-configured timeout (default 50 sec) such browser application will be closed. 4. Let identify connecting websocket at the very beginning - with the first connect request. Allows to reject not allowed connection. Also for the future simplifies re-connection. 5. In TWebWindow clearly identify batch job connection, let reuse if for consequent jobs, close it after 20 sec of idle time. 6. Use std::recursive_mutex in WebWindowsManager to protect methods which creates THttpServer / creates new window / starts new web browser. Comment out special manager guard - useless without special threading functionality. 7. Correspondent changes in JSROOT. . 8. Special handling of 2d/3d canvas when producing PNG images - now is more consistent, no need to use THREE.SVGRenderer for that. 9. Add canvas size to the RPadDisplayItem class, used only in batch mode to configure image size. 10. Use workaround with static mutex to protect I/O in canvas painter in multi-threaded environment.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2603
https://github.com/root-project/root/pull/2603:212,usability,user,user,212,"WebWindow: cummulative patch with many fixes and updates; 1. Fix problem with fragmented websocket messages - some browsers can split messages on 120KB chunks. Now message merged together before delivered to the user. 2. Solve problem with Google Chrome run in headless mode. . Now chrome browser can run in headless mode without any special debugging flags, which opens some http ports. Now THttpServer provides special ""dummy"" script, which timedout until batch job is not finished. 3. Try to correctly handle situation when browser hangs and does not reply for a long time. After pre-configured timeout (default 50 sec) such browser application will be closed. 4. Let identify connecting websocket at the very beginning - with the first connect request. Allows to reject not allowed connection. Also for the future simplifies re-connection. 5. In TWebWindow clearly identify batch job connection, let reuse if for consequent jobs, close it after 20 sec of idle time. 6. Use std::recursive_mutex in WebWindowsManager to protect methods which creates THttpServer / creates new window / starts new web browser. Comment out special manager guard - useless without special threading functionality. 7. Correspondent changes in JSROOT. . 8. Special handling of 2d/3d canvas when producing PNG images - now is more consistent, no need to use THREE.SVGRenderer for that. 9. Add canvas size to the RPadDisplayItem class, used only in batch mode to configure image size. 10. Use workaround with static mutex to protect I/O in canvas painter in multi-threaded environment.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2603
https://github.com/root-project/root/pull/2603:656,usability,close,closed,656,"WebWindow: cummulative patch with many fixes and updates; 1. Fix problem with fragmented websocket messages - some browsers can split messages on 120KB chunks. Now message merged together before delivered to the user. 2. Solve problem with Google Chrome run in headless mode. . Now chrome browser can run in headless mode without any special debugging flags, which opens some http ports. Now THttpServer provides special ""dummy"" script, which timedout until batch job is not finished. 3. Try to correctly handle situation when browser hangs and does not reply for a long time. After pre-configured timeout (default 50 sec) such browser application will be closed. 4. Let identify connecting websocket at the very beginning - with the first connect request. Allows to reject not allowed connection. Also for the future simplifies re-connection. 5. In TWebWindow clearly identify batch job connection, let reuse if for consequent jobs, close it after 20 sec of idle time. 6. Use std::recursive_mutex in WebWindowsManager to protect methods which creates THttpServer / creates new window / starts new web browser. Comment out special manager guard - useless without special threading functionality. 7. Correspondent changes in JSROOT. . 8. Special handling of 2d/3d canvas when producing PNG images - now is more consistent, no need to use THREE.SVGRenderer for that. 9. Add canvas size to the RPadDisplayItem class, used only in batch mode to configure image size. 10. Use workaround with static mutex to protect I/O in canvas painter in multi-threaded environment.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2603
https://github.com/root-project/root/pull/2603:818,usability,simpl,simplifies,818,"WebWindow: cummulative patch with many fixes and updates; 1. Fix problem with fragmented websocket messages - some browsers can split messages on 120KB chunks. Now message merged together before delivered to the user. 2. Solve problem with Google Chrome run in headless mode. . Now chrome browser can run in headless mode without any special debugging flags, which opens some http ports. Now THttpServer provides special ""dummy"" script, which timedout until batch job is not finished. 3. Try to correctly handle situation when browser hangs and does not reply for a long time. After pre-configured timeout (default 50 sec) such browser application will be closed. 4. Let identify connecting websocket at the very beginning - with the first connect request. Allows to reject not allowed connection. Also for the future simplifies re-connection. 5. In TWebWindow clearly identify batch job connection, let reuse if for consequent jobs, close it after 20 sec of idle time. 6. Use std::recursive_mutex in WebWindowsManager to protect methods which creates THttpServer / creates new window / starts new web browser. Comment out special manager guard - useless without special threading functionality. 7. Correspondent changes in JSROOT. . 8. Special handling of 2d/3d canvas when producing PNG images - now is more consistent, no need to use THREE.SVGRenderer for that. 9. Add canvas size to the RPadDisplayItem class, used only in batch mode to configure image size. 10. Use workaround with static mutex to protect I/O in canvas painter in multi-threaded environment.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2603
https://github.com/root-project/root/pull/2603:861,usability,clear,clearly,861,"WebWindow: cummulative patch with many fixes and updates; 1. Fix problem with fragmented websocket messages - some browsers can split messages on 120KB chunks. Now message merged together before delivered to the user. 2. Solve problem with Google Chrome run in headless mode. . Now chrome browser can run in headless mode without any special debugging flags, which opens some http ports. Now THttpServer provides special ""dummy"" script, which timedout until batch job is not finished. 3. Try to correctly handle situation when browser hangs and does not reply for a long time. After pre-configured timeout (default 50 sec) such browser application will be closed. 4. Let identify connecting websocket at the very beginning - with the first connect request. Allows to reject not allowed connection. Also for the future simplifies re-connection. 5. In TWebWindow clearly identify batch job connection, let reuse if for consequent jobs, close it after 20 sec of idle time. 6. Use std::recursive_mutex in WebWindowsManager to protect methods which creates THttpServer / creates new window / starts new web browser. Comment out special manager guard - useless without special threading functionality. 7. Correspondent changes in JSROOT. . 8. Special handling of 2d/3d canvas when producing PNG images - now is more consistent, no need to use THREE.SVGRenderer for that. 9. Add canvas size to the RPadDisplayItem class, used only in batch mode to configure image size. 10. Use workaround with static mutex to protect I/O in canvas painter in multi-threaded environment.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2603
https://github.com/root-project/root/pull/2603:934,usability,close,close,934,"WebWindow: cummulative patch with many fixes and updates; 1. Fix problem with fragmented websocket messages - some browsers can split messages on 120KB chunks. Now message merged together before delivered to the user. 2. Solve problem with Google Chrome run in headless mode. . Now chrome browser can run in headless mode without any special debugging flags, which opens some http ports. Now THttpServer provides special ""dummy"" script, which timedout until batch job is not finished. 3. Try to correctly handle situation when browser hangs and does not reply for a long time. After pre-configured timeout (default 50 sec) such browser application will be closed. 4. Let identify connecting websocket at the very beginning - with the first connect request. Allows to reject not allowed connection. Also for the future simplifies re-connection. 5. In TWebWindow clearly identify batch job connection, let reuse if for consequent jobs, close it after 20 sec of idle time. 6. Use std::recursive_mutex in WebWindowsManager to protect methods which creates THttpServer / creates new window / starts new web browser. Comment out special manager guard - useless without special threading functionality. 7. Correspondent changes in JSROOT. . 8. Special handling of 2d/3d canvas when producing PNG images - now is more consistent, no need to use THREE.SVGRenderer for that. 9. Add canvas size to the RPadDisplayItem class, used only in batch mode to configure image size. 10. Use workaround with static mutex to protect I/O in canvas painter in multi-threaded environment.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2603
https://github.com/root-project/root/pull/2603:1310,usability,consist,consistent,1310,"WebWindow: cummulative patch with many fixes and updates; 1. Fix problem with fragmented websocket messages - some browsers can split messages on 120KB chunks. Now message merged together before delivered to the user. 2. Solve problem with Google Chrome run in headless mode. . Now chrome browser can run in headless mode without any special debugging flags, which opens some http ports. Now THttpServer provides special ""dummy"" script, which timedout until batch job is not finished. 3. Try to correctly handle situation when browser hangs and does not reply for a long time. After pre-configured timeout (default 50 sec) such browser application will be closed. 4. Let identify connecting websocket at the very beginning - with the first connect request. Allows to reject not allowed connection. Also for the future simplifies re-connection. 5. In TWebWindow clearly identify batch job connection, let reuse if for consequent jobs, close it after 20 sec of idle time. 6. Use std::recursive_mutex in WebWindowsManager to protect methods which creates THttpServer / creates new window / starts new web browser. Comment out special manager guard - useless without special threading functionality. 7. Correspondent changes in JSROOT. . 8. Special handling of 2d/3d canvas when producing PNG images - now is more consistent, no need to use THREE.SVGRenderer for that. 9. Add canvas size to the RPadDisplayItem class, used only in batch mode to configure image size. 10. Use workaround with static mutex to protect I/O in canvas painter in multi-threaded environment.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2603
https://github.com/root-project/root/pull/2604:136,deployability,depend,dependencies,136,[DF] Move RNodeBase and RLoopManager to their own header and source files; This is part of a series of PRs aimed at disentangling class dependencies in RNodes.hxx by splitting it in multiple header files. The process also helps fixing includes and forward declarations.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2604
https://github.com/root-project/root/pull/2604:136,integrability,depend,dependencies,136,[DF] Move RNodeBase and RLoopManager to their own header and source files; This is part of a series of PRs aimed at disentangling class dependencies in RNodes.hxx by splitting it in multiple header files. The process also helps fixing includes and forward declarations.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2604
https://github.com/root-project/root/pull/2604:136,modifiability,depend,dependencies,136,[DF] Move RNodeBase and RLoopManager to their own header and source files; This is part of a series of PRs aimed at disentangling class dependencies in RNodes.hxx by splitting it in multiple header files. The process also helps fixing includes and forward declarations.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2604
https://github.com/root-project/root/pull/2604:136,safety,depend,dependencies,136,[DF] Move RNodeBase and RLoopManager to their own header and source files; This is part of a series of PRs aimed at disentangling class dependencies in RNodes.hxx by splitting it in multiple header files. The process also helps fixing includes and forward declarations.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2604
https://github.com/root-project/root/pull/2604:136,testability,depend,dependencies,136,[DF] Move RNodeBase and RLoopManager to their own header and source files; This is part of a series of PRs aimed at disentangling class dependencies in RNodes.hxx by splitting it in multiple header files. The process also helps fixing includes and forward declarations.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2604
https://github.com/root-project/root/pull/2604:222,usability,help,helps,222,[DF] Move RNodeBase and RLoopManager to their own header and source files; This is part of a series of PRs aimed at disentangling class dependencies in RNodes.hxx by splitting it in multiple header files. The process also helps fixing includes and forward declarations.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2604
https://github.com/root-project/root/pull/2605:234,availability,operat,operations,234,"[DF] Fix reading of std::vector<bool>; Now if the type of a column is `RVec<bool>`, `RDataFrame` reads it as. a `TTreeReaderValue<std::vector<bool>>` and performs a copy into a. `RVec<bool>` value when passing each entry value to RDF operations. This is a temporary patch, this changes should be reverted as soon. as ROOT-9570 (""TTreeReaderArray<bool> silently returns wrong results. when reading a vector<bool>"") is fixed. Note that C-arrays of bool are still broken, and that requires that. `TTreeReaderArray<bool>` be fixed. EDIT: or are they? does `TTreeReaderArray<bool>` work with C-arrays of bools?",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2605
https://github.com/root-project/root/pull/2605:266,deployability,patch,patch,266,"[DF] Fix reading of std::vector<bool>; Now if the type of a column is `RVec<bool>`, `RDataFrame` reads it as. a `TTreeReaderValue<std::vector<bool>>` and performs a copy into a. `RVec<bool>` value when passing each entry value to RDF operations. This is a temporary patch, this changes should be reverted as soon. as ROOT-9570 (""TTreeReaderArray<bool> silently returns wrong results. when reading a vector<bool>"") is fixed. Note that C-arrays of bool are still broken, and that requires that. `TTreeReaderArray<bool>` be fixed. EDIT: or are they? does `TTreeReaderArray<bool>` work with C-arrays of bools?",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2605
https://github.com/root-project/root/pull/2605:154,performance,perform,performs,154,"[DF] Fix reading of std::vector<bool>; Now if the type of a column is `RVec<bool>`, `RDataFrame` reads it as. a `TTreeReaderValue<std::vector<bool>>` and performs a copy into a. `RVec<bool>` value when passing each entry value to RDF operations. This is a temporary patch, this changes should be reverted as soon. as ROOT-9570 (""TTreeReaderArray<bool> silently returns wrong results. when reading a vector<bool>"") is fixed. Note that C-arrays of bool are still broken, and that requires that. `TTreeReaderArray<bool>` be fixed. EDIT: or are they? does `TTreeReaderArray<bool>` work with C-arrays of bools?",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2605
https://github.com/root-project/root/pull/2605:547,reliability,doe,does,547,"[DF] Fix reading of std::vector<bool>; Now if the type of a column is `RVec<bool>`, `RDataFrame` reads it as. a `TTreeReaderValue<std::vector<bool>>` and performs a copy into a. `RVec<bool>` value when passing each entry value to RDF operations. This is a temporary patch, this changes should be reverted as soon. as ROOT-9570 (""TTreeReaderArray<bool> silently returns wrong results. when reading a vector<bool>"") is fixed. Note that C-arrays of bool are still broken, and that requires that. `TTreeReaderArray<bool>` be fixed. EDIT: or are they? does `TTreeReaderArray<bool>` work with C-arrays of bools?",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2605
https://github.com/root-project/root/pull/2605:266,safety,patch,patch,266,"[DF] Fix reading of std::vector<bool>; Now if the type of a column is `RVec<bool>`, `RDataFrame` reads it as. a `TTreeReaderValue<std::vector<bool>>` and performs a copy into a. `RVec<bool>` value when passing each entry value to RDF operations. This is a temporary patch, this changes should be reverted as soon. as ROOT-9570 (""TTreeReaderArray<bool> silently returns wrong results. when reading a vector<bool>"") is fixed. Note that C-arrays of bool are still broken, and that requires that. `TTreeReaderArray<bool>` be fixed. EDIT: or are they? does `TTreeReaderArray<bool>` work with C-arrays of bools?",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2605
https://github.com/root-project/root/pull/2605:266,security,patch,patch,266,"[DF] Fix reading of std::vector<bool>; Now if the type of a column is `RVec<bool>`, `RDataFrame` reads it as. a `TTreeReaderValue<std::vector<bool>>` and performs a copy into a. `RVec<bool>` value when passing each entry value to RDF operations. This is a temporary patch, this changes should be reverted as soon. as ROOT-9570 (""TTreeReaderArray<bool> silently returns wrong results. when reading a vector<bool>"") is fixed. Note that C-arrays of bool are still broken, and that requires that. `TTreeReaderArray<bool>` be fixed. EDIT: or are they? does `TTreeReaderArray<bool>` work with C-arrays of bools?",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2605
https://github.com/root-project/root/pull/2605:154,usability,perform,performs,154,"[DF] Fix reading of std::vector<bool>; Now if the type of a column is `RVec<bool>`, `RDataFrame` reads it as. a `TTreeReaderValue<std::vector<bool>>` and performs a copy into a. `RVec<bool>` value when passing each entry value to RDF operations. This is a temporary patch, this changes should be reverted as soon. as ROOT-9570 (""TTreeReaderArray<bool> silently returns wrong results. when reading a vector<bool>"") is fixed. Note that C-arrays of bool are still broken, and that requires that. `TTreeReaderArray<bool>` be fixed. EDIT: or are they? does `TTreeReaderArray<bool>` work with C-arrays of bools?",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2605
https://github.com/root-project/root/pull/2606:31,integrability,Filter,Filter,31,[Dictgen] ROOT-9635 ROOT-9615: Filter out rootcling args;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2606
https://github.com/root-project/root/pull/2608:127,availability,avail,available,127,"[DF] add Davix vfs module to sqlite ds; Allows for direct processing of sqlite files under HTTP(S) URLs provided that Davix is available. Replaces #2578. I think it would be nice to have a unit test for reading of remote files. Unfortunately, Davix does not understand the `http:///path/to/file` trick to use a URL on local paths. Can we upload tree/dataframe/test/RSqliteDS_test.sqlite to https://root.cern.ch/... for testing?",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2608
https://github.com/root-project/root/pull/2608:19,deployability,modul,module,19,"[DF] add Davix vfs module to sqlite ds; Allows for direct processing of sqlite files under HTTP(S) URLs provided that Davix is available. Replaces #2578. I think it would be nice to have a unit test for reading of remote files. Unfortunately, Davix does not understand the `http:///path/to/file` trick to use a URL on local paths. Can we upload tree/dataframe/test/RSqliteDS_test.sqlite to https://root.cern.ch/... for testing?",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2608
https://github.com/root-project/root/pull/2608:19,modifiability,modul,module,19,"[DF] add Davix vfs module to sqlite ds; Allows for direct processing of sqlite files under HTTP(S) URLs provided that Davix is available. Replaces #2578. I think it would be nice to have a unit test for reading of remote files. Unfortunately, Davix does not understand the `http:///path/to/file` trick to use a URL on local paths. Can we upload tree/dataframe/test/RSqliteDS_test.sqlite to https://root.cern.ch/... for testing?",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2608
https://github.com/root-project/root/pull/2608:127,reliability,availab,available,127,"[DF] add Davix vfs module to sqlite ds; Allows for direct processing of sqlite files under HTTP(S) URLs provided that Davix is available. Replaces #2578. I think it would be nice to have a unit test for reading of remote files. Unfortunately, Davix does not understand the `http:///path/to/file` trick to use a URL on local paths. Can we upload tree/dataframe/test/RSqliteDS_test.sqlite to https://root.cern.ch/... for testing?",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2608
https://github.com/root-project/root/pull/2608:249,reliability,doe,does,249,"[DF] add Davix vfs module to sqlite ds; Allows for direct processing of sqlite files under HTTP(S) URLs provided that Davix is available. Replaces #2578. I think it would be nice to have a unit test for reading of remote files. Unfortunately, Davix does not understand the `http:///path/to/file` trick to use a URL on local paths. Can we upload tree/dataframe/test/RSqliteDS_test.sqlite to https://root.cern.ch/... for testing?",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2608
https://github.com/root-project/root/pull/2608:19,safety,modul,module,19,"[DF] add Davix vfs module to sqlite ds; Allows for direct processing of sqlite files under HTTP(S) URLs provided that Davix is available. Replaces #2578. I think it would be nice to have a unit test for reading of remote files. Unfortunately, Davix does not understand the `http:///path/to/file` trick to use a URL on local paths. Can we upload tree/dataframe/test/RSqliteDS_test.sqlite to https://root.cern.ch/... for testing?",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2608
https://github.com/root-project/root/pull/2608:127,safety,avail,available,127,"[DF] add Davix vfs module to sqlite ds; Allows for direct processing of sqlite files under HTTP(S) URLs provided that Davix is available. Replaces #2578. I think it would be nice to have a unit test for reading of remote files. Unfortunately, Davix does not understand the `http:///path/to/file` trick to use a URL on local paths. Can we upload tree/dataframe/test/RSqliteDS_test.sqlite to https://root.cern.ch/... for testing?",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2608
https://github.com/root-project/root/pull/2608:194,safety,test,test,194,"[DF] add Davix vfs module to sqlite ds; Allows for direct processing of sqlite files under HTTP(S) URLs provided that Davix is available. Replaces #2578. I think it would be nice to have a unit test for reading of remote files. Unfortunately, Davix does not understand the `http:///path/to/file` trick to use a URL on local paths. Can we upload tree/dataframe/test/RSqliteDS_test.sqlite to https://root.cern.ch/... for testing?",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2608
https://github.com/root-project/root/pull/2608:360,safety,test,test,360,"[DF] add Davix vfs module to sqlite ds; Allows for direct processing of sqlite files under HTTP(S) URLs provided that Davix is available. Replaces #2578. I think it would be nice to have a unit test for reading of remote files. Unfortunately, Davix does not understand the `http:///path/to/file` trick to use a URL on local paths. Can we upload tree/dataframe/test/RSqliteDS_test.sqlite to https://root.cern.ch/... for testing?",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2608
https://github.com/root-project/root/pull/2608:419,safety,test,testing,419,"[DF] add Davix vfs module to sqlite ds; Allows for direct processing of sqlite files under HTTP(S) URLs provided that Davix is available. Replaces #2578. I think it would be nice to have a unit test for reading of remote files. Unfortunately, Davix does not understand the `http:///path/to/file` trick to use a URL on local paths. Can we upload tree/dataframe/test/RSqliteDS_test.sqlite to https://root.cern.ch/... for testing?",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2608
https://github.com/root-project/root/pull/2608:127,security,availab,available,127,"[DF] add Davix vfs module to sqlite ds; Allows for direct processing of sqlite files under HTTP(S) URLs provided that Davix is available. Replaces #2578. I think it would be nice to have a unit test for reading of remote files. Unfortunately, Davix does not understand the `http:///path/to/file` trick to use a URL on local paths. Can we upload tree/dataframe/test/RSqliteDS_test.sqlite to https://root.cern.ch/... for testing?",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2608
https://github.com/root-project/root/pull/2608:189,testability,unit,unit,189,"[DF] add Davix vfs module to sqlite ds; Allows for direct processing of sqlite files under HTTP(S) URLs provided that Davix is available. Replaces #2578. I think it would be nice to have a unit test for reading of remote files. Unfortunately, Davix does not understand the `http:///path/to/file` trick to use a URL on local paths. Can we upload tree/dataframe/test/RSqliteDS_test.sqlite to https://root.cern.ch/... for testing?",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2608
https://github.com/root-project/root/pull/2608:194,testability,test,test,194,"[DF] add Davix vfs module to sqlite ds; Allows for direct processing of sqlite files under HTTP(S) URLs provided that Davix is available. Replaces #2578. I think it would be nice to have a unit test for reading of remote files. Unfortunately, Davix does not understand the `http:///path/to/file` trick to use a URL on local paths. Can we upload tree/dataframe/test/RSqliteDS_test.sqlite to https://root.cern.ch/... for testing?",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2608
https://github.com/root-project/root/pull/2608:258,testability,understand,understand,258,"[DF] add Davix vfs module to sqlite ds; Allows for direct processing of sqlite files under HTTP(S) URLs provided that Davix is available. Replaces #2578. I think it would be nice to have a unit test for reading of remote files. Unfortunately, Davix does not understand the `http:///path/to/file` trick to use a URL on local paths. Can we upload tree/dataframe/test/RSqliteDS_test.sqlite to https://root.cern.ch/... for testing?",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2608
https://github.com/root-project/root/pull/2608:360,testability,test,test,360,"[DF] add Davix vfs module to sqlite ds; Allows for direct processing of sqlite files under HTTP(S) URLs provided that Davix is available. Replaces #2578. I think it would be nice to have a unit test for reading of remote files. Unfortunately, Davix does not understand the `http:///path/to/file` trick to use a URL on local paths. Can we upload tree/dataframe/test/RSqliteDS_test.sqlite to https://root.cern.ch/... for testing?",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2608
https://github.com/root-project/root/pull/2608:419,testability,test,testing,419,"[DF] add Davix vfs module to sqlite ds; Allows for direct processing of sqlite files under HTTP(S) URLs provided that Davix is available. Replaces #2578. I think it would be nice to have a unit test for reading of remote files. Unfortunately, Davix does not understand the `http:///path/to/file` trick to use a URL on local paths. Can we upload tree/dataframe/test/RSqliteDS_test.sqlite to https://root.cern.ch/... for testing?",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2608
https://github.com/root-project/root/pull/2609:305,availability,failur,failure,305,set the neccesary c++ flags for checking the existence of e.g. string…; In order to use the right c++ version in `CHECK_CXX_SOURCE_COMPILES` one needs to set `CMAKE_REQUIRED_FLAGS`. Without this it wouldn't pick up e.g. `std::string_view` when compiling with `-D cxx17=ON` which would cause a compilation failure when ROOT later defines `std::string_view` itself.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2609
https://github.com/root-project/root/pull/2609:102,deployability,version,version,102,set the neccesary c++ flags for checking the existence of e.g. string…; In order to use the right c++ version in `CHECK_CXX_SOURCE_COMPILES` one needs to set `CMAKE_REQUIRED_FLAGS`. Without this it wouldn't pick up e.g. `std::string_view` when compiling with `-D cxx17=ON` which would cause a compilation failure when ROOT later defines `std::string_view` itself.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2609
https://github.com/root-project/root/pull/2609:305,deployability,fail,failure,305,set the neccesary c++ flags for checking the existence of e.g. string…; In order to use the right c++ version in `CHECK_CXX_SOURCE_COMPILES` one needs to set `CMAKE_REQUIRED_FLAGS`. Without this it wouldn't pick up e.g. `std::string_view` when compiling with `-D cxx17=ON` which would cause a compilation failure when ROOT later defines `std::string_view` itself.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2609
https://github.com/root-project/root/pull/2609:102,integrability,version,version,102,set the neccesary c++ flags for checking the existence of e.g. string…; In order to use the right c++ version in `CHECK_CXX_SOURCE_COMPILES` one needs to set `CMAKE_REQUIRED_FLAGS`. Without this it wouldn't pick up e.g. `std::string_view` when compiling with `-D cxx17=ON` which would cause a compilation failure when ROOT later defines `std::string_view` itself.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2609
https://github.com/root-project/root/pull/2609:102,modifiability,version,version,102,set the neccesary c++ flags for checking the existence of e.g. string…; In order to use the right c++ version in `CHECK_CXX_SOURCE_COMPILES` one needs to set `CMAKE_REQUIRED_FLAGS`. Without this it wouldn't pick up e.g. `std::string_view` when compiling with `-D cxx17=ON` which would cause a compilation failure when ROOT later defines `std::string_view` itself.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2609
https://github.com/root-project/root/pull/2609:305,performance,failur,failure,305,set the neccesary c++ flags for checking the existence of e.g. string…; In order to use the right c++ version in `CHECK_CXX_SOURCE_COMPILES` one needs to set `CMAKE_REQUIRED_FLAGS`. Without this it wouldn't pick up e.g. `std::string_view` when compiling with `-D cxx17=ON` which would cause a compilation failure when ROOT later defines `std::string_view` itself.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2609
https://github.com/root-project/root/pull/2609:305,reliability,fail,failure,305,set the neccesary c++ flags for checking the existence of e.g. string…; In order to use the right c++ version in `CHECK_CXX_SOURCE_COMPILES` one needs to set `CMAKE_REQUIRED_FLAGS`. Without this it wouldn't pick up e.g. `std::string_view` when compiling with `-D cxx17=ON` which would cause a compilation failure when ROOT later defines `std::string_view` itself.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2609
https://github.com/root-project/root/pull/2610:28,availability,error,errors,28,"TH::Divide warn if binomial errors are requested but not computed.; cf https://root-forum.cern.ch/t/th2-tefficiency-example/. This collides with #2611 . * I see two different approaches, either keep the computations consistent. * or implicitly do what the user probably wants. (of cause the warning could go in both cases)",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2610
https://github.com/root-project/root/pull/2610:216,availability,consist,consistent,216,"TH::Divide warn if binomial errors are requested but not computed.; cf https://root-forum.cern.ch/t/th2-tefficiency-example/. This collides with #2611 . * I see two different approaches, either keep the computations consistent. * or implicitly do what the user probably wants. (of cause the warning could go in both cases)",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2610
https://github.com/root-project/root/pull/2610:28,performance,error,errors,28,"TH::Divide warn if binomial errors are requested but not computed.; cf https://root-forum.cern.ch/t/th2-tefficiency-example/. This collides with #2611 . * I see two different approaches, either keep the computations consistent. * or implicitly do what the user probably wants. (of cause the warning could go in both cases)",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2610
https://github.com/root-project/root/pull/2610:28,safety,error,errors,28,"TH::Divide warn if binomial errors are requested but not computed.; cf https://root-forum.cern.ch/t/th2-tefficiency-example/. This collides with #2611 . * I see two different approaches, either keep the computations consistent. * or implicitly do what the user probably wants. (of cause the warning could go in both cases)",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2610
https://github.com/root-project/root/pull/2610:28,usability,error,errors,28,"TH::Divide warn if binomial errors are requested but not computed.; cf https://root-forum.cern.ch/t/th2-tefficiency-example/. This collides with #2611 . * I see two different approaches, either keep the computations consistent. * or implicitly do what the user probably wants. (of cause the warning could go in both cases)",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2610
https://github.com/root-project/root/pull/2610:216,usability,consist,consistent,216,"TH::Divide warn if binomial errors are requested but not computed.; cf https://root-forum.cern.ch/t/th2-tefficiency-example/. This collides with #2611 . * I see two different approaches, either keep the computations consistent. * or implicitly do what the user probably wants. (of cause the warning could go in both cases)",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2610
https://github.com/root-project/root/pull/2610:256,usability,user,user,256,"TH::Divide warn if binomial errors are requested but not computed.; cf https://root-forum.cern.ch/t/th2-tefficiency-example/. This collides with #2611 . * I see two different approaches, either keep the computations consistent. * or implicitly do what the user probably wants. (of cause the warning could go in both cases)",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2610
https://github.com/root-project/root/pull/2611:36,availability,error,errors,36,"TH::Divide create Sumw2 if binomial errors are requested; request wouldn't make sense otherwise. cf https://root-forum.cern.ch/t/th2-tefficiency-example/. This collides with #2610 . * I see two different approaches, either keep the computations consistent. * or implicitly do what the user probably wants. (of cause the warning could go in both cases)",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2611
https://github.com/root-project/root/pull/2611:245,availability,consist,consistent,245,"TH::Divide create Sumw2 if binomial errors are requested; request wouldn't make sense otherwise. cf https://root-forum.cern.ch/t/th2-tefficiency-example/. This collides with #2610 . * I see two different approaches, either keep the computations consistent. * or implicitly do what the user probably wants. (of cause the warning could go in both cases)",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2611
https://github.com/root-project/root/pull/2611:36,performance,error,errors,36,"TH::Divide create Sumw2 if binomial errors are requested; request wouldn't make sense otherwise. cf https://root-forum.cern.ch/t/th2-tefficiency-example/. This collides with #2610 . * I see two different approaches, either keep the computations consistent. * or implicitly do what the user probably wants. (of cause the warning could go in both cases)",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2611
https://github.com/root-project/root/pull/2611:36,safety,error,errors,36,"TH::Divide create Sumw2 if binomial errors are requested; request wouldn't make sense otherwise. cf https://root-forum.cern.ch/t/th2-tefficiency-example/. This collides with #2610 . * I see two different approaches, either keep the computations consistent. * or implicitly do what the user probably wants. (of cause the warning could go in both cases)",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2611
https://github.com/root-project/root/pull/2611:36,usability,error,errors,36,"TH::Divide create Sumw2 if binomial errors are requested; request wouldn't make sense otherwise. cf https://root-forum.cern.ch/t/th2-tefficiency-example/. This collides with #2610 . * I see two different approaches, either keep the computations consistent. * or implicitly do what the user probably wants. (of cause the warning could go in both cases)",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2611
https://github.com/root-project/root/pull/2611:245,usability,consist,consistent,245,"TH::Divide create Sumw2 if binomial errors are requested; request wouldn't make sense otherwise. cf https://root-forum.cern.ch/t/th2-tefficiency-example/. This collides with #2610 . * I see two different approaches, either keep the computations consistent. * or implicitly do what the user probably wants. (of cause the warning could go in both cases)",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2611
https://github.com/root-project/root/pull/2611:285,usability,user,user,285,"TH::Divide create Sumw2 if binomial errors are requested; request wouldn't make sense otherwise. cf https://root-forum.cern.ch/t/th2-tefficiency-example/. This collides with #2610 . * I see two different approaches, either keep the computations consistent. * or implicitly do what the user probably wants. (of cause the warning could go in both cases)",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2611
https://github.com/root-project/root/pull/2612:46,safety,valid,valid,46,[DF] Throw if name of defined column is not a valid C++ var name;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2612
https://github.com/root-project/root/pull/2614:27,deployability,version,version,27,[RooFit] Re-add a modified version of rf104 tutorial which works in ROOT6; yay :),MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2614
https://github.com/root-project/root/pull/2614:27,integrability,version,version,27,[RooFit] Re-add a modified version of rf104 tutorial which works in ROOT6; yay :),MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2614
https://github.com/root-project/root/pull/2614:27,modifiability,version,version,27,[RooFit] Re-add a modified version of rf104 tutorial which works in ROOT6; yay :),MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2614
https://github.com/root-project/root/pull/2614:18,security,modif,modified,18,[RooFit] Re-add a modified version of rf104 tutorial which works in ROOT6; yay :),MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2614
https://github.com/root-project/root/pull/2616:271,deployability,build,build,271,[WIP] Add support for combined short options; This enabled shorter command line invocations like `root -qle 1+1` instead of `root -q -l -e 1+1`. Not sure how we should go about testing. The non-combined options are sort of tested by running `hsimple.C` at the end of the build.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2616
https://github.com/root-project/root/pull/2616:177,safety,test,testing,177,[WIP] Add support for combined short options; This enabled shorter command line invocations like `root -qle 1+1` instead of `root -q -l -e 1+1`. Not sure how we should go about testing. The non-combined options are sort of tested by running `hsimple.C` at the end of the build.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2616
https://github.com/root-project/root/pull/2616:223,safety,test,tested,223,[WIP] Add support for combined short options; This enabled shorter command line invocations like `root -qle 1+1` instead of `root -q -l -e 1+1`. Not sure how we should go about testing. The non-combined options are sort of tested by running `hsimple.C` at the end of the build.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2616
https://github.com/root-project/root/pull/2616:177,testability,test,testing,177,[WIP] Add support for combined short options; This enabled shorter command line invocations like `root -qle 1+1` instead of `root -q -l -e 1+1`. Not sure how we should go about testing. The non-combined options are sort of tested by running `hsimple.C` at the end of the build.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2616
https://github.com/root-project/root/pull/2616:223,testability,test,tested,223,[WIP] Add support for combined short options; This enabled shorter command line invocations like `root -qle 1+1` instead of `root -q -l -e 1+1`. Not sure how we should go about testing. The non-combined options are sort of tested by running `hsimple.C` at the end of the build.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2616
https://github.com/root-project/root/pull/2616:10,usability,support,support,10,[WIP] Add support for combined short options; This enabled shorter command line invocations like `root -qle 1+1` instead of `root -q -l -e 1+1`. Not sure how we should go about testing. The non-combined options are sort of tested by running `hsimple.C` at the end of the build.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2616
https://github.com/root-project/root/pull/2616:67,usability,command,command,67,[WIP] Add support for combined short options; This enabled shorter command line invocations like `root -qle 1+1` instead of `root -q -l -e 1+1`. Not sure how we should go about testing. The non-combined options are sort of tested by running `hsimple.C` at the end of the build.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2616
https://github.com/root-project/root/pull/2620:359,deployability,log,log,359,"[VecOps] Add Nonzero and Intersect helpers; `Nonzero` simply goes through the input vector and checks whether an element is zero or not. The time complexity is `O(N)`. `Intersect` goes through the vector `v1` and searches each element in the vector `v2`. The approach is sorting `v2` first and loop trivially over `v1`. The resulting time complexity is `O(N1*log(N2))`. A common use-case is shown below:. ```cpp. using namespace ROOT::VecOps;. // user data, e.g., in NanoAOD format. RVec<int> Muon_charge = {1, -1, 1};. RVec<float> Muon_pt = {20.0, 30.0, 10.0};. RVec<float> Muon_eta = {1.0, -2.0, 0.5};. // make first selection based on Muon charge. auto idx_mask = Nonzero(Muon_charge>0);. // get indices that sort Muon pt with descending values and pass the previous selection. auto idx_sorted = Reverse(Argsort(Muon_pt));. auto idx_selection = Intersect(idx_sorted, idx_mask);. // get Muon eta of positive Muons sorted by pt. auto values = Take(Muon_eta, idx_selection);. // Returns: { 1, 0.5 }. ```. This scenario is very common in processing of NanoAOD files. The naming matches the numpy API with [`numpy.nonzero`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.nonzero.html) and [`numpy.intersect1d`](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.intersect1d.html). **TODO:**. - [x] Write a tutorial when we agreed on the functionality. - [x] Change the API for `Sorted` and `Reversed` to `Sort` and `Reverse` before merging this.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2620
https://github.com/root-project/root/pull/2620:1095,deployability,API,API,1095,"[VecOps] Add Nonzero and Intersect helpers; `Nonzero` simply goes through the input vector and checks whether an element is zero or not. The time complexity is `O(N)`. `Intersect` goes through the vector `v1` and searches each element in the vector `v2`. The approach is sorting `v2` first and loop trivially over `v1`. The resulting time complexity is `O(N1*log(N2))`. A common use-case is shown below:. ```cpp. using namespace ROOT::VecOps;. // user data, e.g., in NanoAOD format. RVec<int> Muon_charge = {1, -1, 1};. RVec<float> Muon_pt = {20.0, 30.0, 10.0};. RVec<float> Muon_eta = {1.0, -2.0, 0.5};. // make first selection based on Muon charge. auto idx_mask = Nonzero(Muon_charge>0);. // get indices that sort Muon pt with descending values and pass the previous selection. auto idx_sorted = Reverse(Argsort(Muon_pt));. auto idx_selection = Intersect(idx_sorted, idx_mask);. // get Muon eta of positive Muons sorted by pt. auto values = Take(Muon_eta, idx_selection);. // Returns: { 1, 0.5 }. ```. This scenario is very common in processing of NanoAOD files. The naming matches the numpy API with [`numpy.nonzero`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.nonzero.html) and [`numpy.intersect1d`](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.intersect1d.html). **TODO:**. - [x] Write a tutorial when we agreed on the functionality. - [x] Change the API for `Sorted` and `Reversed` to `Sort` and `Reverse` before merging this.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2620
https://github.com/root-project/root/pull/2620:1394,deployability,API,API,1394,"[VecOps] Add Nonzero and Intersect helpers; `Nonzero` simply goes through the input vector and checks whether an element is zero or not. The time complexity is `O(N)`. `Intersect` goes through the vector `v1` and searches each element in the vector `v2`. The approach is sorting `v2` first and loop trivially over `v1`. The resulting time complexity is `O(N1*log(N2))`. A common use-case is shown below:. ```cpp. using namespace ROOT::VecOps;. // user data, e.g., in NanoAOD format. RVec<int> Muon_charge = {1, -1, 1};. RVec<float> Muon_pt = {20.0, 30.0, 10.0};. RVec<float> Muon_eta = {1.0, -2.0, 0.5};. // make first selection based on Muon charge. auto idx_mask = Nonzero(Muon_charge>0);. // get indices that sort Muon pt with descending values and pass the previous selection. auto idx_sorted = Reverse(Argsort(Muon_pt));. auto idx_selection = Intersect(idx_sorted, idx_mask);. // get Muon eta of positive Muons sorted by pt. auto values = Take(Muon_eta, idx_selection);. // Returns: { 1, 0.5 }. ```. This scenario is very common in processing of NanoAOD files. The naming matches the numpy API with [`numpy.nonzero`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.nonzero.html) and [`numpy.intersect1d`](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.intersect1d.html). **TODO:**. - [x] Write a tutorial when we agreed on the functionality. - [x] Change the API for `Sorted` and `Reversed` to `Sort` and `Reverse` before merging this.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2620
https://github.com/root-project/root/pull/2620:643,energy efficiency,charg,charge,643,"[VecOps] Add Nonzero and Intersect helpers; `Nonzero` simply goes through the input vector and checks whether an element is zero or not. The time complexity is `O(N)`. `Intersect` goes through the vector `v1` and searches each element in the vector `v2`. The approach is sorting `v2` first and loop trivially over `v1`. The resulting time complexity is `O(N1*log(N2))`. A common use-case is shown below:. ```cpp. using namespace ROOT::VecOps;. // user data, e.g., in NanoAOD format. RVec<int> Muon_charge = {1, -1, 1};. RVec<float> Muon_pt = {20.0, 30.0, 10.0};. RVec<float> Muon_eta = {1.0, -2.0, 0.5};. // make first selection based on Muon charge. auto idx_mask = Nonzero(Muon_charge>0);. // get indices that sort Muon pt with descending values and pass the previous selection. auto idx_sorted = Reverse(Argsort(Muon_pt));. auto idx_selection = Intersect(idx_sorted, idx_mask);. // get Muon eta of positive Muons sorted by pt. auto values = Take(Muon_eta, idx_selection);. // Returns: { 1, 0.5 }. ```. This scenario is very common in processing of NanoAOD files. The naming matches the numpy API with [`numpy.nonzero`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.nonzero.html) and [`numpy.intersect1d`](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.intersect1d.html). **TODO:**. - [x] Write a tutorial when we agreed on the functionality. - [x] Change the API for `Sorted` and `Reversed` to `Sort` and `Reverse` before merging this.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2620
https://github.com/root-project/root/pull/2620:1095,integrability,API,API,1095,"[VecOps] Add Nonzero and Intersect helpers; `Nonzero` simply goes through the input vector and checks whether an element is zero or not. The time complexity is `O(N)`. `Intersect` goes through the vector `v1` and searches each element in the vector `v2`. The approach is sorting `v2` first and loop trivially over `v1`. The resulting time complexity is `O(N1*log(N2))`. A common use-case is shown below:. ```cpp. using namespace ROOT::VecOps;. // user data, e.g., in NanoAOD format. RVec<int> Muon_charge = {1, -1, 1};. RVec<float> Muon_pt = {20.0, 30.0, 10.0};. RVec<float> Muon_eta = {1.0, -2.0, 0.5};. // make first selection based on Muon charge. auto idx_mask = Nonzero(Muon_charge>0);. // get indices that sort Muon pt with descending values and pass the previous selection. auto idx_sorted = Reverse(Argsort(Muon_pt));. auto idx_selection = Intersect(idx_sorted, idx_mask);. // get Muon eta of positive Muons sorted by pt. auto values = Take(Muon_eta, idx_selection);. // Returns: { 1, 0.5 }. ```. This scenario is very common in processing of NanoAOD files. The naming matches the numpy API with [`numpy.nonzero`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.nonzero.html) and [`numpy.intersect1d`](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.intersect1d.html). **TODO:**. - [x] Write a tutorial when we agreed on the functionality. - [x] Change the API for `Sorted` and `Reversed` to `Sort` and `Reverse` before merging this.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2620
https://github.com/root-project/root/pull/2620:1394,integrability,API,API,1394,"[VecOps] Add Nonzero and Intersect helpers; `Nonzero` simply goes through the input vector and checks whether an element is zero or not. The time complexity is `O(N)`. `Intersect` goes through the vector `v1` and searches each element in the vector `v2`. The approach is sorting `v2` first and loop trivially over `v1`. The resulting time complexity is `O(N1*log(N2))`. A common use-case is shown below:. ```cpp. using namespace ROOT::VecOps;. // user data, e.g., in NanoAOD format. RVec<int> Muon_charge = {1, -1, 1};. RVec<float> Muon_pt = {20.0, 30.0, 10.0};. RVec<float> Muon_eta = {1.0, -2.0, 0.5};. // make first selection based on Muon charge. auto idx_mask = Nonzero(Muon_charge>0);. // get indices that sort Muon pt with descending values and pass the previous selection. auto idx_sorted = Reverse(Argsort(Muon_pt));. auto idx_selection = Intersect(idx_sorted, idx_mask);. // get Muon eta of positive Muons sorted by pt. auto values = Take(Muon_eta, idx_selection);. // Returns: { 1, 0.5 }. ```. This scenario is very common in processing of NanoAOD files. The naming matches the numpy API with [`numpy.nonzero`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.nonzero.html) and [`numpy.intersect1d`](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.intersect1d.html). **TODO:**. - [x] Write a tutorial when we agreed on the functionality. - [x] Change the API for `Sorted` and `Reversed` to `Sort` and `Reverse` before merging this.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2620
https://github.com/root-project/root/pull/2620:475,interoperability,format,format,475,"[VecOps] Add Nonzero and Intersect helpers; `Nonzero` simply goes through the input vector and checks whether an element is zero or not. The time complexity is `O(N)`. `Intersect` goes through the vector `v1` and searches each element in the vector `v2`. The approach is sorting `v2` first and loop trivially over `v1`. The resulting time complexity is `O(N1*log(N2))`. A common use-case is shown below:. ```cpp. using namespace ROOT::VecOps;. // user data, e.g., in NanoAOD format. RVec<int> Muon_charge = {1, -1, 1};. RVec<float> Muon_pt = {20.0, 30.0, 10.0};. RVec<float> Muon_eta = {1.0, -2.0, 0.5};. // make first selection based on Muon charge. auto idx_mask = Nonzero(Muon_charge>0);. // get indices that sort Muon pt with descending values and pass the previous selection. auto idx_sorted = Reverse(Argsort(Muon_pt));. auto idx_selection = Intersect(idx_sorted, idx_mask);. // get Muon eta of positive Muons sorted by pt. auto values = Take(Muon_eta, idx_selection);. // Returns: { 1, 0.5 }. ```. This scenario is very common in processing of NanoAOD files. The naming matches the numpy API with [`numpy.nonzero`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.nonzero.html) and [`numpy.intersect1d`](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.intersect1d.html). **TODO:**. - [x] Write a tutorial when we agreed on the functionality. - [x] Change the API for `Sorted` and `Reversed` to `Sort` and `Reverse` before merging this.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2620
https://github.com/root-project/root/pull/2620:1095,interoperability,API,API,1095,"[VecOps] Add Nonzero and Intersect helpers; `Nonzero` simply goes through the input vector and checks whether an element is zero or not. The time complexity is `O(N)`. `Intersect` goes through the vector `v1` and searches each element in the vector `v2`. The approach is sorting `v2` first and loop trivially over `v1`. The resulting time complexity is `O(N1*log(N2))`. A common use-case is shown below:. ```cpp. using namespace ROOT::VecOps;. // user data, e.g., in NanoAOD format. RVec<int> Muon_charge = {1, -1, 1};. RVec<float> Muon_pt = {20.0, 30.0, 10.0};. RVec<float> Muon_eta = {1.0, -2.0, 0.5};. // make first selection based on Muon charge. auto idx_mask = Nonzero(Muon_charge>0);. // get indices that sort Muon pt with descending values and pass the previous selection. auto idx_sorted = Reverse(Argsort(Muon_pt));. auto idx_selection = Intersect(idx_sorted, idx_mask);. // get Muon eta of positive Muons sorted by pt. auto values = Take(Muon_eta, idx_selection);. // Returns: { 1, 0.5 }. ```. This scenario is very common in processing of NanoAOD files. The naming matches the numpy API with [`numpy.nonzero`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.nonzero.html) and [`numpy.intersect1d`](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.intersect1d.html). **TODO:**. - [x] Write a tutorial when we agreed on the functionality. - [x] Change the API for `Sorted` and `Reversed` to `Sort` and `Reverse` before merging this.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2620
https://github.com/root-project/root/pull/2620:1394,interoperability,API,API,1394,"[VecOps] Add Nonzero and Intersect helpers; `Nonzero` simply goes through the input vector and checks whether an element is zero or not. The time complexity is `O(N)`. `Intersect` goes through the vector `v1` and searches each element in the vector `v2`. The approach is sorting `v2` first and loop trivially over `v1`. The resulting time complexity is `O(N1*log(N2))`. A common use-case is shown below:. ```cpp. using namespace ROOT::VecOps;. // user data, e.g., in NanoAOD format. RVec<int> Muon_charge = {1, -1, 1};. RVec<float> Muon_pt = {20.0, 30.0, 10.0};. RVec<float> Muon_eta = {1.0, -2.0, 0.5};. // make first selection based on Muon charge. auto idx_mask = Nonzero(Muon_charge>0);. // get indices that sort Muon pt with descending values and pass the previous selection. auto idx_sorted = Reverse(Argsort(Muon_pt));. auto idx_selection = Intersect(idx_sorted, idx_mask);. // get Muon eta of positive Muons sorted by pt. auto values = Take(Muon_eta, idx_selection);. // Returns: { 1, 0.5 }. ```. This scenario is very common in processing of NanoAOD files. The naming matches the numpy API with [`numpy.nonzero`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.nonzero.html) and [`numpy.intersect1d`](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.intersect1d.html). **TODO:**. - [x] Write a tutorial when we agreed on the functionality. - [x] Change the API for `Sorted` and `Reversed` to `Sort` and `Reverse` before merging this.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2620
https://github.com/root-project/root/pull/2620:1010,modifiability,scenario,scenario,1010,"[VecOps] Add Nonzero and Intersect helpers; `Nonzero` simply goes through the input vector and checks whether an element is zero or not. The time complexity is `O(N)`. `Intersect` goes through the vector `v1` and searches each element in the vector `v2`. The approach is sorting `v2` first and loop trivially over `v1`. The resulting time complexity is `O(N1*log(N2))`. A common use-case is shown below:. ```cpp. using namespace ROOT::VecOps;. // user data, e.g., in NanoAOD format. RVec<int> Muon_charge = {1, -1, 1};. RVec<float> Muon_pt = {20.0, 30.0, 10.0};. RVec<float> Muon_eta = {1.0, -2.0, 0.5};. // make first selection based on Muon charge. auto idx_mask = Nonzero(Muon_charge>0);. // get indices that sort Muon pt with descending values and pass the previous selection. auto idx_sorted = Reverse(Argsort(Muon_pt));. auto idx_selection = Intersect(idx_sorted, idx_mask);. // get Muon eta of positive Muons sorted by pt. auto values = Take(Muon_eta, idx_selection);. // Returns: { 1, 0.5 }. ```. This scenario is very common in processing of NanoAOD files. The naming matches the numpy API with [`numpy.nonzero`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.nonzero.html) and [`numpy.intersect1d`](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.intersect1d.html). **TODO:**. - [x] Write a tutorial when we agreed on the functionality. - [x] Change the API for `Sorted` and `Reversed` to `Sort` and `Reverse` before merging this.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2620
https://github.com/root-project/root/pull/2620:141,performance,time,time,141,"[VecOps] Add Nonzero and Intersect helpers; `Nonzero` simply goes through the input vector and checks whether an element is zero or not. The time complexity is `O(N)`. `Intersect` goes through the vector `v1` and searches each element in the vector `v2`. The approach is sorting `v2` first and loop trivially over `v1`. The resulting time complexity is `O(N1*log(N2))`. A common use-case is shown below:. ```cpp. using namespace ROOT::VecOps;. // user data, e.g., in NanoAOD format. RVec<int> Muon_charge = {1, -1, 1};. RVec<float> Muon_pt = {20.0, 30.0, 10.0};. RVec<float> Muon_eta = {1.0, -2.0, 0.5};. // make first selection based on Muon charge. auto idx_mask = Nonzero(Muon_charge>0);. // get indices that sort Muon pt with descending values and pass the previous selection. auto idx_sorted = Reverse(Argsort(Muon_pt));. auto idx_selection = Intersect(idx_sorted, idx_mask);. // get Muon eta of positive Muons sorted by pt. auto values = Take(Muon_eta, idx_selection);. // Returns: { 1, 0.5 }. ```. This scenario is very common in processing of NanoAOD files. The naming matches the numpy API with [`numpy.nonzero`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.nonzero.html) and [`numpy.intersect1d`](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.intersect1d.html). **TODO:**. - [x] Write a tutorial when we agreed on the functionality. - [x] Change the API for `Sorted` and `Reversed` to `Sort` and `Reverse` before merging this.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2620
https://github.com/root-project/root/pull/2620:334,performance,time,time,334,"[VecOps] Add Nonzero and Intersect helpers; `Nonzero` simply goes through the input vector and checks whether an element is zero or not. The time complexity is `O(N)`. `Intersect` goes through the vector `v1` and searches each element in the vector `v2`. The approach is sorting `v2` first and loop trivially over `v1`. The resulting time complexity is `O(N1*log(N2))`. A common use-case is shown below:. ```cpp. using namespace ROOT::VecOps;. // user data, e.g., in NanoAOD format. RVec<int> Muon_charge = {1, -1, 1};. RVec<float> Muon_pt = {20.0, 30.0, 10.0};. RVec<float> Muon_eta = {1.0, -2.0, 0.5};. // make first selection based on Muon charge. auto idx_mask = Nonzero(Muon_charge>0);. // get indices that sort Muon pt with descending values and pass the previous selection. auto idx_sorted = Reverse(Argsort(Muon_pt));. auto idx_selection = Intersect(idx_sorted, idx_mask);. // get Muon eta of positive Muons sorted by pt. auto values = Take(Muon_eta, idx_selection);. // Returns: { 1, 0.5 }. ```. This scenario is very common in processing of NanoAOD files. The naming matches the numpy API with [`numpy.nonzero`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.nonzero.html) and [`numpy.intersect1d`](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.intersect1d.html). **TODO:**. - [x] Write a tutorial when we agreed on the functionality. - [x] Change the API for `Sorted` and `Reversed` to `Sort` and `Reverse` before merging this.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2620
https://github.com/root-project/root/pull/2620:78,safety,input,input,78,"[VecOps] Add Nonzero and Intersect helpers; `Nonzero` simply goes through the input vector and checks whether an element is zero or not. The time complexity is `O(N)`. `Intersect` goes through the vector `v1` and searches each element in the vector `v2`. The approach is sorting `v2` first and loop trivially over `v1`. The resulting time complexity is `O(N1*log(N2))`. A common use-case is shown below:. ```cpp. using namespace ROOT::VecOps;. // user data, e.g., in NanoAOD format. RVec<int> Muon_charge = {1, -1, 1};. RVec<float> Muon_pt = {20.0, 30.0, 10.0};. RVec<float> Muon_eta = {1.0, -2.0, 0.5};. // make first selection based on Muon charge. auto idx_mask = Nonzero(Muon_charge>0);. // get indices that sort Muon pt with descending values and pass the previous selection. auto idx_sorted = Reverse(Argsort(Muon_pt));. auto idx_selection = Intersect(idx_sorted, idx_mask);. // get Muon eta of positive Muons sorted by pt. auto values = Take(Muon_eta, idx_selection);. // Returns: { 1, 0.5 }. ```. This scenario is very common in processing of NanoAOD files. The naming matches the numpy API with [`numpy.nonzero`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.nonzero.html) and [`numpy.intersect1d`](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.intersect1d.html). **TODO:**. - [x] Write a tutorial when we agreed on the functionality. - [x] Change the API for `Sorted` and `Reversed` to `Sort` and `Reverse` before merging this.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2620
https://github.com/root-project/root/pull/2620:146,safety,compl,complexity,146,"[VecOps] Add Nonzero and Intersect helpers; `Nonzero` simply goes through the input vector and checks whether an element is zero or not. The time complexity is `O(N)`. `Intersect` goes through the vector `v1` and searches each element in the vector `v2`. The approach is sorting `v2` first and loop trivially over `v1`. The resulting time complexity is `O(N1*log(N2))`. A common use-case is shown below:. ```cpp. using namespace ROOT::VecOps;. // user data, e.g., in NanoAOD format. RVec<int> Muon_charge = {1, -1, 1};. RVec<float> Muon_pt = {20.0, 30.0, 10.0};. RVec<float> Muon_eta = {1.0, -2.0, 0.5};. // make first selection based on Muon charge. auto idx_mask = Nonzero(Muon_charge>0);. // get indices that sort Muon pt with descending values and pass the previous selection. auto idx_sorted = Reverse(Argsort(Muon_pt));. auto idx_selection = Intersect(idx_sorted, idx_mask);. // get Muon eta of positive Muons sorted by pt. auto values = Take(Muon_eta, idx_selection);. // Returns: { 1, 0.5 }. ```. This scenario is very common in processing of NanoAOD files. The naming matches the numpy API with [`numpy.nonzero`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.nonzero.html) and [`numpy.intersect1d`](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.intersect1d.html). **TODO:**. - [x] Write a tutorial when we agreed on the functionality. - [x] Change the API for `Sorted` and `Reversed` to `Sort` and `Reverse` before merging this.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2620
https://github.com/root-project/root/pull/2620:339,safety,compl,complexity,339,"[VecOps] Add Nonzero and Intersect helpers; `Nonzero` simply goes through the input vector and checks whether an element is zero or not. The time complexity is `O(N)`. `Intersect` goes through the vector `v1` and searches each element in the vector `v2`. The approach is sorting `v2` first and loop trivially over `v1`. The resulting time complexity is `O(N1*log(N2))`. A common use-case is shown below:. ```cpp. using namespace ROOT::VecOps;. // user data, e.g., in NanoAOD format. RVec<int> Muon_charge = {1, -1, 1};. RVec<float> Muon_pt = {20.0, 30.0, 10.0};. RVec<float> Muon_eta = {1.0, -2.0, 0.5};. // make first selection based on Muon charge. auto idx_mask = Nonzero(Muon_charge>0);. // get indices that sort Muon pt with descending values and pass the previous selection. auto idx_sorted = Reverse(Argsort(Muon_pt));. auto idx_selection = Intersect(idx_sorted, idx_mask);. // get Muon eta of positive Muons sorted by pt. auto values = Take(Muon_eta, idx_selection);. // Returns: { 1, 0.5 }. ```. This scenario is very common in processing of NanoAOD files. The naming matches the numpy API with [`numpy.nonzero`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.nonzero.html) and [`numpy.intersect1d`](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.intersect1d.html). **TODO:**. - [x] Write a tutorial when we agreed on the functionality. - [x] Change the API for `Sorted` and `Reversed` to `Sort` and `Reverse` before merging this.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2620
https://github.com/root-project/root/pull/2620:359,safety,log,log,359,"[VecOps] Add Nonzero and Intersect helpers; `Nonzero` simply goes through the input vector and checks whether an element is zero or not. The time complexity is `O(N)`. `Intersect` goes through the vector `v1` and searches each element in the vector `v2`. The approach is sorting `v2` first and loop trivially over `v1`. The resulting time complexity is `O(N1*log(N2))`. A common use-case is shown below:. ```cpp. using namespace ROOT::VecOps;. // user data, e.g., in NanoAOD format. RVec<int> Muon_charge = {1, -1, 1};. RVec<float> Muon_pt = {20.0, 30.0, 10.0};. RVec<float> Muon_eta = {1.0, -2.0, 0.5};. // make first selection based on Muon charge. auto idx_mask = Nonzero(Muon_charge>0);. // get indices that sort Muon pt with descending values and pass the previous selection. auto idx_sorted = Reverse(Argsort(Muon_pt));. auto idx_selection = Intersect(idx_sorted, idx_mask);. // get Muon eta of positive Muons sorted by pt. auto values = Take(Muon_eta, idx_selection);. // Returns: { 1, 0.5 }. ```. This scenario is very common in processing of NanoAOD files. The naming matches the numpy API with [`numpy.nonzero`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.nonzero.html) and [`numpy.intersect1d`](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.intersect1d.html). **TODO:**. - [x] Write a tutorial when we agreed on the functionality. - [x] Change the API for `Sorted` and `Reversed` to `Sort` and `Reverse` before merging this.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2620
https://github.com/root-project/root/pull/2620:146,security,compl,complexity,146,"[VecOps] Add Nonzero and Intersect helpers; `Nonzero` simply goes through the input vector and checks whether an element is zero or not. The time complexity is `O(N)`. `Intersect` goes through the vector `v1` and searches each element in the vector `v2`. The approach is sorting `v2` first and loop trivially over `v1`. The resulting time complexity is `O(N1*log(N2))`. A common use-case is shown below:. ```cpp. using namespace ROOT::VecOps;. // user data, e.g., in NanoAOD format. RVec<int> Muon_charge = {1, -1, 1};. RVec<float> Muon_pt = {20.0, 30.0, 10.0};. RVec<float> Muon_eta = {1.0, -2.0, 0.5};. // make first selection based on Muon charge. auto idx_mask = Nonzero(Muon_charge>0);. // get indices that sort Muon pt with descending values and pass the previous selection. auto idx_sorted = Reverse(Argsort(Muon_pt));. auto idx_selection = Intersect(idx_sorted, idx_mask);. // get Muon eta of positive Muons sorted by pt. auto values = Take(Muon_eta, idx_selection);. // Returns: { 1, 0.5 }. ```. This scenario is very common in processing of NanoAOD files. The naming matches the numpy API with [`numpy.nonzero`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.nonzero.html) and [`numpy.intersect1d`](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.intersect1d.html). **TODO:**. - [x] Write a tutorial when we agreed on the functionality. - [x] Change the API for `Sorted` and `Reversed` to `Sort` and `Reverse` before merging this.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2620
https://github.com/root-project/root/pull/2620:339,security,compl,complexity,339,"[VecOps] Add Nonzero and Intersect helpers; `Nonzero` simply goes through the input vector and checks whether an element is zero or not. The time complexity is `O(N)`. `Intersect` goes through the vector `v1` and searches each element in the vector `v2`. The approach is sorting `v2` first and loop trivially over `v1`. The resulting time complexity is `O(N1*log(N2))`. A common use-case is shown below:. ```cpp. using namespace ROOT::VecOps;. // user data, e.g., in NanoAOD format. RVec<int> Muon_charge = {1, -1, 1};. RVec<float> Muon_pt = {20.0, 30.0, 10.0};. RVec<float> Muon_eta = {1.0, -2.0, 0.5};. // make first selection based on Muon charge. auto idx_mask = Nonzero(Muon_charge>0);. // get indices that sort Muon pt with descending values and pass the previous selection. auto idx_sorted = Reverse(Argsort(Muon_pt));. auto idx_selection = Intersect(idx_sorted, idx_mask);. // get Muon eta of positive Muons sorted by pt. auto values = Take(Muon_eta, idx_selection);. // Returns: { 1, 0.5 }. ```. This scenario is very common in processing of NanoAOD files. The naming matches the numpy API with [`numpy.nonzero`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.nonzero.html) and [`numpy.intersect1d`](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.intersect1d.html). **TODO:**. - [x] Write a tutorial when we agreed on the functionality. - [x] Change the API for `Sorted` and `Reversed` to `Sort` and `Reverse` before merging this.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2620
https://github.com/root-project/root/pull/2620:359,security,log,log,359,"[VecOps] Add Nonzero and Intersect helpers; `Nonzero` simply goes through the input vector and checks whether an element is zero or not. The time complexity is `O(N)`. `Intersect` goes through the vector `v1` and searches each element in the vector `v2`. The approach is sorting `v2` first and loop trivially over `v1`. The resulting time complexity is `O(N1*log(N2))`. A common use-case is shown below:. ```cpp. using namespace ROOT::VecOps;. // user data, e.g., in NanoAOD format. RVec<int> Muon_charge = {1, -1, 1};. RVec<float> Muon_pt = {20.0, 30.0, 10.0};. RVec<float> Muon_eta = {1.0, -2.0, 0.5};. // make first selection based on Muon charge. auto idx_mask = Nonzero(Muon_charge>0);. // get indices that sort Muon pt with descending values and pass the previous selection. auto idx_sorted = Reverse(Argsort(Muon_pt));. auto idx_selection = Intersect(idx_sorted, idx_mask);. // get Muon eta of positive Muons sorted by pt. auto values = Take(Muon_eta, idx_selection);. // Returns: { 1, 0.5 }. ```. This scenario is very common in processing of NanoAOD files. The naming matches the numpy API with [`numpy.nonzero`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.nonzero.html) and [`numpy.intersect1d`](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.intersect1d.html). **TODO:**. - [x] Write a tutorial when we agreed on the functionality. - [x] Change the API for `Sorted` and `Reversed` to `Sort` and `Reverse` before merging this.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2620
https://github.com/root-project/root/pull/2620:54,testability,simpl,simply,54,"[VecOps] Add Nonzero and Intersect helpers; `Nonzero` simply goes through the input vector and checks whether an element is zero or not. The time complexity is `O(N)`. `Intersect` goes through the vector `v1` and searches each element in the vector `v2`. The approach is sorting `v2` first and loop trivially over `v1`. The resulting time complexity is `O(N1*log(N2))`. A common use-case is shown below:. ```cpp. using namespace ROOT::VecOps;. // user data, e.g., in NanoAOD format. RVec<int> Muon_charge = {1, -1, 1};. RVec<float> Muon_pt = {20.0, 30.0, 10.0};. RVec<float> Muon_eta = {1.0, -2.0, 0.5};. // make first selection based on Muon charge. auto idx_mask = Nonzero(Muon_charge>0);. // get indices that sort Muon pt with descending values and pass the previous selection. auto idx_sorted = Reverse(Argsort(Muon_pt));. auto idx_selection = Intersect(idx_sorted, idx_mask);. // get Muon eta of positive Muons sorted by pt. auto values = Take(Muon_eta, idx_selection);. // Returns: { 1, 0.5 }. ```. This scenario is very common in processing of NanoAOD files. The naming matches the numpy API with [`numpy.nonzero`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.nonzero.html) and [`numpy.intersect1d`](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.intersect1d.html). **TODO:**. - [x] Write a tutorial when we agreed on the functionality. - [x] Change the API for `Sorted` and `Reversed` to `Sort` and `Reverse` before merging this.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2620
https://github.com/root-project/root/pull/2620:359,testability,log,log,359,"[VecOps] Add Nonzero and Intersect helpers; `Nonzero` simply goes through the input vector and checks whether an element is zero or not. The time complexity is `O(N)`. `Intersect` goes through the vector `v1` and searches each element in the vector `v2`. The approach is sorting `v2` first and loop trivially over `v1`. The resulting time complexity is `O(N1*log(N2))`. A common use-case is shown below:. ```cpp. using namespace ROOT::VecOps;. // user data, e.g., in NanoAOD format. RVec<int> Muon_charge = {1, -1, 1};. RVec<float> Muon_pt = {20.0, 30.0, 10.0};. RVec<float> Muon_eta = {1.0, -2.0, 0.5};. // make first selection based on Muon charge. auto idx_mask = Nonzero(Muon_charge>0);. // get indices that sort Muon pt with descending values and pass the previous selection. auto idx_sorted = Reverse(Argsort(Muon_pt));. auto idx_selection = Intersect(idx_sorted, idx_mask);. // get Muon eta of positive Muons sorted by pt. auto values = Take(Muon_eta, idx_selection);. // Returns: { 1, 0.5 }. ```. This scenario is very common in processing of NanoAOD files. The naming matches the numpy API with [`numpy.nonzero`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.nonzero.html) and [`numpy.intersect1d`](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.intersect1d.html). **TODO:**. - [x] Write a tutorial when we agreed on the functionality. - [x] Change the API for `Sorted` and `Reversed` to `Sort` and `Reverse` before merging this.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2620
https://github.com/root-project/root/pull/2620:35,usability,help,helpers,35,"[VecOps] Add Nonzero and Intersect helpers; `Nonzero` simply goes through the input vector and checks whether an element is zero or not. The time complexity is `O(N)`. `Intersect` goes through the vector `v1` and searches each element in the vector `v2`. The approach is sorting `v2` first and loop trivially over `v1`. The resulting time complexity is `O(N1*log(N2))`. A common use-case is shown below:. ```cpp. using namespace ROOT::VecOps;. // user data, e.g., in NanoAOD format. RVec<int> Muon_charge = {1, -1, 1};. RVec<float> Muon_pt = {20.0, 30.0, 10.0};. RVec<float> Muon_eta = {1.0, -2.0, 0.5};. // make first selection based on Muon charge. auto idx_mask = Nonzero(Muon_charge>0);. // get indices that sort Muon pt with descending values and pass the previous selection. auto idx_sorted = Reverse(Argsort(Muon_pt));. auto idx_selection = Intersect(idx_sorted, idx_mask);. // get Muon eta of positive Muons sorted by pt. auto values = Take(Muon_eta, idx_selection);. // Returns: { 1, 0.5 }. ```. This scenario is very common in processing of NanoAOD files. The naming matches the numpy API with [`numpy.nonzero`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.nonzero.html) and [`numpy.intersect1d`](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.intersect1d.html). **TODO:**. - [x] Write a tutorial when we agreed on the functionality. - [x] Change the API for `Sorted` and `Reversed` to `Sort` and `Reverse` before merging this.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2620
https://github.com/root-project/root/pull/2620:54,usability,simpl,simply,54,"[VecOps] Add Nonzero and Intersect helpers; `Nonzero` simply goes through the input vector and checks whether an element is zero or not. The time complexity is `O(N)`. `Intersect` goes through the vector `v1` and searches each element in the vector `v2`. The approach is sorting `v2` first and loop trivially over `v1`. The resulting time complexity is `O(N1*log(N2))`. A common use-case is shown below:. ```cpp. using namespace ROOT::VecOps;. // user data, e.g., in NanoAOD format. RVec<int> Muon_charge = {1, -1, 1};. RVec<float> Muon_pt = {20.0, 30.0, 10.0};. RVec<float> Muon_eta = {1.0, -2.0, 0.5};. // make first selection based on Muon charge. auto idx_mask = Nonzero(Muon_charge>0);. // get indices that sort Muon pt with descending values and pass the previous selection. auto idx_sorted = Reverse(Argsort(Muon_pt));. auto idx_selection = Intersect(idx_sorted, idx_mask);. // get Muon eta of positive Muons sorted by pt. auto values = Take(Muon_eta, idx_selection);. // Returns: { 1, 0.5 }. ```. This scenario is very common in processing of NanoAOD files. The naming matches the numpy API with [`numpy.nonzero`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.nonzero.html) and [`numpy.intersect1d`](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.intersect1d.html). **TODO:**. - [x] Write a tutorial when we agreed on the functionality. - [x] Change the API for `Sorted` and `Reversed` to `Sort` and `Reverse` before merging this.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2620
https://github.com/root-project/root/pull/2620:78,usability,input,input,78,"[VecOps] Add Nonzero and Intersect helpers; `Nonzero` simply goes through the input vector and checks whether an element is zero or not. The time complexity is `O(N)`. `Intersect` goes through the vector `v1` and searches each element in the vector `v2`. The approach is sorting `v2` first and loop trivially over `v1`. The resulting time complexity is `O(N1*log(N2))`. A common use-case is shown below:. ```cpp. using namespace ROOT::VecOps;. // user data, e.g., in NanoAOD format. RVec<int> Muon_charge = {1, -1, 1};. RVec<float> Muon_pt = {20.0, 30.0, 10.0};. RVec<float> Muon_eta = {1.0, -2.0, 0.5};. // make first selection based on Muon charge. auto idx_mask = Nonzero(Muon_charge>0);. // get indices that sort Muon pt with descending values and pass the previous selection. auto idx_sorted = Reverse(Argsort(Muon_pt));. auto idx_selection = Intersect(idx_sorted, idx_mask);. // get Muon eta of positive Muons sorted by pt. auto values = Take(Muon_eta, idx_selection);. // Returns: { 1, 0.5 }. ```. This scenario is very common in processing of NanoAOD files. The naming matches the numpy API with [`numpy.nonzero`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.nonzero.html) and [`numpy.intersect1d`](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.intersect1d.html). **TODO:**. - [x] Write a tutorial when we agreed on the functionality. - [x] Change the API for `Sorted` and `Reversed` to `Sort` and `Reverse` before merging this.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2620
https://github.com/root-project/root/pull/2620:447,usability,user,user,447,"[VecOps] Add Nonzero and Intersect helpers; `Nonzero` simply goes through the input vector and checks whether an element is zero or not. The time complexity is `O(N)`. `Intersect` goes through the vector `v1` and searches each element in the vector `v2`. The approach is sorting `v2` first and loop trivially over `v1`. The resulting time complexity is `O(N1*log(N2))`. A common use-case is shown below:. ```cpp. using namespace ROOT::VecOps;. // user data, e.g., in NanoAOD format. RVec<int> Muon_charge = {1, -1, 1};. RVec<float> Muon_pt = {20.0, 30.0, 10.0};. RVec<float> Muon_eta = {1.0, -2.0, 0.5};. // make first selection based on Muon charge. auto idx_mask = Nonzero(Muon_charge>0);. // get indices that sort Muon pt with descending values and pass the previous selection. auto idx_sorted = Reverse(Argsort(Muon_pt));. auto idx_selection = Intersect(idx_sorted, idx_mask);. // get Muon eta of positive Muons sorted by pt. auto values = Take(Muon_eta, idx_selection);. // Returns: { 1, 0.5 }. ```. This scenario is very common in processing of NanoAOD files. The naming matches the numpy API with [`numpy.nonzero`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.nonzero.html) and [`numpy.intersect1d`](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.intersect1d.html). **TODO:**. - [x] Write a tutorial when we agreed on the functionality. - [x] Change the API for `Sorted` and `Reversed` to `Sort` and `Reverse` before merging this.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2620
https://github.com/root-project/root/pull/2621:62,availability,operat,operator,62,"[VecOps] Get elements of RVec by passing vector of indices to operator[]; Add template specialization for `RVec<T>::operator[]` to access elements by passing a vector of indices. Here an example:. ```cpp. using namespace ROOT::VecOps;. RVec<float> v = {1, 2, 3, 4, 5};. RVec<size_t> idx = {0, 2, 4};. cout << v[idx] << endl;. // Returns: { 1, 3, 5 }. ```. This enable numpy-like element access and a more convenient handling of index magic. See following scenario:. ```cpp. using namespace ROOT::VecOps;. RVec<float> v = {2, 1, 0};. auto idx = Argsort(v);. cout << v[idx] << endl;. // Returns: { 0, 1, 2 }. ```. **TODO:**. - [ ] Add this feature to a tutorial",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2621
https://github.com/root-project/root/pull/2621:116,availability,operat,operator,116,"[VecOps] Get elements of RVec by passing vector of indices to operator[]; Add template specialization for `RVec<T>::operator[]` to access elements by passing a vector of indices. Here an example:. ```cpp. using namespace ROOT::VecOps;. RVec<float> v = {1, 2, 3, 4, 5};. RVec<size_t> idx = {0, 2, 4};. cout << v[idx] << endl;. // Returns: { 1, 3, 5 }. ```. This enable numpy-like element access and a more convenient handling of index magic. See following scenario:. ```cpp. using namespace ROOT::VecOps;. RVec<float> v = {2, 1, 0};. auto idx = Argsort(v);. cout << v[idx] << endl;. // Returns: { 0, 1, 2 }. ```. **TODO:**. - [ ] Add this feature to a tutorial",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2621
https://github.com/root-project/root/pull/2621:455,modifiability,scenario,scenario,455,"[VecOps] Get elements of RVec by passing vector of indices to operator[]; Add template specialization for `RVec<T>::operator[]` to access elements by passing a vector of indices. Here an example:. ```cpp. using namespace ROOT::VecOps;. RVec<float> v = {1, 2, 3, 4, 5};. RVec<size_t> idx = {0, 2, 4};. cout << v[idx] << endl;. // Returns: { 1, 3, 5 }. ```. This enable numpy-like element access and a more convenient handling of index magic. See following scenario:. ```cpp. using namespace ROOT::VecOps;. RVec<float> v = {2, 1, 0};. auto idx = Argsort(v);. cout << v[idx] << endl;. // Returns: { 0, 1, 2 }. ```. **TODO:**. - [ ] Add this feature to a tutorial",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2621
https://github.com/root-project/root/pull/2621:131,security,access,access,131,"[VecOps] Get elements of RVec by passing vector of indices to operator[]; Add template specialization for `RVec<T>::operator[]` to access elements by passing a vector of indices. Here an example:. ```cpp. using namespace ROOT::VecOps;. RVec<float> v = {1, 2, 3, 4, 5};. RVec<size_t> idx = {0, 2, 4};. cout << v[idx] << endl;. // Returns: { 1, 3, 5 }. ```. This enable numpy-like element access and a more convenient handling of index magic. See following scenario:. ```cpp. using namespace ROOT::VecOps;. RVec<float> v = {2, 1, 0};. auto idx = Argsort(v);. cout << v[idx] << endl;. // Returns: { 0, 1, 2 }. ```. **TODO:**. - [ ] Add this feature to a tutorial",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2621
https://github.com/root-project/root/pull/2621:387,security,access,access,387,"[VecOps] Get elements of RVec by passing vector of indices to operator[]; Add template specialization for `RVec<T>::operator[]` to access elements by passing a vector of indices. Here an example:. ```cpp. using namespace ROOT::VecOps;. RVec<float> v = {1, 2, 3, 4, 5};. RVec<size_t> idx = {0, 2, 4};. cout << v[idx] << endl;. // Returns: { 1, 3, 5 }. ```. This enable numpy-like element access and a more convenient handling of index magic. See following scenario:. ```cpp. using namespace ROOT::VecOps;. RVec<float> v = {2, 1, 0};. auto idx = Argsort(v);. cout << v[idx] << endl;. // Returns: { 0, 1, 2 }. ```. **TODO:**. - [ ] Add this feature to a tutorial",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2621
https://github.com/root-project/root/pull/2622:4,performance,memor,memory,4,Fix memory leak; Fix memory leak in:. TFormula::HandleParamRanges(TString &formula). TFormula::HandleParametrizedFunctions(TString &formula),MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2622
https://github.com/root-project/root/pull/2622:21,performance,memor,memory,21,Fix memory leak; Fix memory leak in:. TFormula::HandleParamRanges(TString &formula). TFormula::HandleParametrizedFunctions(TString &formula),MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2622
https://github.com/root-project/root/pull/2622:4,usability,memor,memory,4,Fix memory leak; Fix memory leak in:. TFormula::HandleParamRanges(TString &formula). TFormula::HandleParametrizedFunctions(TString &formula),MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2622
https://github.com/root-project/root/pull/2622:21,usability,memor,memory,21,Fix memory leak; Fix memory leak in:. TFormula::HandleParamRanges(TString &formula). TFormula::HandleParametrizedFunctions(TString &formula),MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2622
https://github.com/root-project/root/pull/2623:7,deployability,Updat,Update,7,[DOCS] Update release notes;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2623
https://github.com/root-project/root/pull/2623:14,deployability,releas,release,14,[DOCS] Update release notes;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2623
https://github.com/root-project/root/pull/2623:7,safety,Updat,Update,7,[DOCS] Update release notes;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2623
https://github.com/root-project/root/pull/2623:7,security,Updat,Update,7,[DOCS] Update release notes;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2623
https://github.com/root-project/root/pull/2624:7,deployability,Updat,Update,7,[DOCS] Update release notes;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2624
https://github.com/root-project/root/pull/2624:14,deployability,releas,release,14,[DOCS] Update release notes;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2624
https://github.com/root-project/root/pull/2624:7,safety,Updat,Update,7,[DOCS] Update release notes;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2624
https://github.com/root-project/root/pull/2624:7,security,Updat,Update,7,[DOCS] Update release notes;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2624
https://github.com/root-project/root/pull/2625:19,usability,help,helper,19,"[VecOps] Add Where helper; Add helper `Where` similar to `numpy.where`. See following examples for the use-cases:. ```cpp. using namespace ROOT::VecOps;. RVec<float> v1 = {1, 2, 3, 4};. RVec<float> v2 = {-1, -2, -3, -4};. // Case 1: I want a vector with the elements of v1 if a condition is true and the element of v2 otherwise. auto v3 = Where(v1>2, v1, v2);. // Returns: { -1, -2, 3, 4 }. // Case 2: I want the elements of vector v1 if a condition is true and the value v2 in the other cases. auto v4 = Where(v1 != 4, v1, -999.0f);. // Returns: { 1, 2, 3, -999 }. // Case 3: I want the elements of vector v2 if a condition is false and the value v1 in the other cases. auto v5 = Where(v1 == 4, -999.0f, v2);. // Returns: { -1, -2, -3, -999 }. // Case 4: I want a vector with the value v1 if a condition is true and the value v2 in the other cases. auto v6 = Where(v1 == 4, 1.0f, 0.0f);. // Returns: { 0, 0, 0, 1 }. ```. `numpy.where` is pretty often used ([see here](https://galeascience.wordpress.com/2016/08/10/top-10-pandas-numpy-and-scipy-functions-on-github/)):. ![](https://galeascience.files.wordpress.com/2016/08/popular_numpy_functions1.png?w=768). **TODO:**. - [x] Add feature to a tutorial",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2625
https://github.com/root-project/root/pull/2625:31,usability,help,helper,31,"[VecOps] Add Where helper; Add helper `Where` similar to `numpy.where`. See following examples for the use-cases:. ```cpp. using namespace ROOT::VecOps;. RVec<float> v1 = {1, 2, 3, 4};. RVec<float> v2 = {-1, -2, -3, -4};. // Case 1: I want a vector with the elements of v1 if a condition is true and the element of v2 otherwise. auto v3 = Where(v1>2, v1, v2);. // Returns: { -1, -2, 3, 4 }. // Case 2: I want the elements of vector v1 if a condition is true and the value v2 in the other cases. auto v4 = Where(v1 != 4, v1, -999.0f);. // Returns: { 1, 2, 3, -999 }. // Case 3: I want the elements of vector v2 if a condition is false and the value v1 in the other cases. auto v5 = Where(v1 == 4, -999.0f, v2);. // Returns: { -1, -2, -3, -999 }. // Case 4: I want a vector with the value v1 if a condition is true and the value v2 in the other cases. auto v6 = Where(v1 == 4, 1.0f, 0.0f);. // Returns: { 0, 0, 0, 1 }. ```. `numpy.where` is pretty often used ([see here](https://galeascience.wordpress.com/2016/08/10/top-10-pandas-numpy-and-scipy-functions-on-github/)):. ![](https://galeascience.files.wordpress.com/2016/08/popular_numpy_functions1.png?w=768). **TODO:**. - [x] Add feature to a tutorial",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2625
https://github.com/root-project/root/pull/2626:163,energy efficiency,adapt,adapt,163,"[VecOps] Rename Sorted and Reversed to Sort and Reverse; As discussed with @bluehood and @dpiparo, having ""passive"" function names is not desirable. Therefore, we adapt the interface accordingly.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2626
https://github.com/root-project/root/pull/2626:163,integrability,adapt,adapt,163,"[VecOps] Rename Sorted and Reversed to Sort and Reverse; As discussed with @bluehood and @dpiparo, having ""passive"" function names is not desirable. Therefore, we adapt the interface accordingly.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2626
https://github.com/root-project/root/pull/2626:173,integrability,interfac,interface,173,"[VecOps] Rename Sorted and Reversed to Sort and Reverse; As discussed with @bluehood and @dpiparo, having ""passive"" function names is not desirable. Therefore, we adapt the interface accordingly.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2626
https://github.com/root-project/root/pull/2626:163,interoperability,adapt,adapt,163,"[VecOps] Rename Sorted and Reversed to Sort and Reverse; As discussed with @bluehood and @dpiparo, having ""passive"" function names is not desirable. Therefore, we adapt the interface accordingly.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2626
https://github.com/root-project/root/pull/2626:173,interoperability,interfac,interface,173,"[VecOps] Rename Sorted and Reversed to Sort and Reverse; As discussed with @bluehood and @dpiparo, having ""passive"" function names is not desirable. Therefore, we adapt the interface accordingly.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2626
https://github.com/root-project/root/pull/2626:163,modifiability,adapt,adapt,163,"[VecOps] Rename Sorted and Reversed to Sort and Reverse; As discussed with @bluehood and @dpiparo, having ""passive"" function names is not desirable. Therefore, we adapt the interface accordingly.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2626
https://github.com/root-project/root/pull/2626:173,modifiability,interfac,interface,173,"[VecOps] Rename Sorted and Reversed to Sort and Reverse; As discussed with @bluehood and @dpiparo, having ""passive"" function names is not desirable. Therefore, we adapt the interface accordingly.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2626
https://github.com/root-project/root/pull/2627:33,deployability,modul,modules,33,[DF] Add missing include to help modules; The missing include broke modules,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2627
https://github.com/root-project/root/pull/2627:68,deployability,modul,modules,68,[DF] Add missing include to help modules; The missing include broke modules,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2627
https://github.com/root-project/root/pull/2627:33,modifiability,modul,modules,33,[DF] Add missing include to help modules; The missing include broke modules,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2627
https://github.com/root-project/root/pull/2627:68,modifiability,modul,modules,68,[DF] Add missing include to help modules; The missing include broke modules,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2627
https://github.com/root-project/root/pull/2627:33,safety,modul,modules,33,[DF] Add missing include to help modules; The missing include broke modules,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2627
https://github.com/root-project/root/pull/2627:68,safety,modul,modules,68,[DF] Add missing include to help modules; The missing include broke modules,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2627
https://github.com/root-project/root/pull/2627:28,usability,help,help,28,[DF] Add missing include to help modules; The missing include broke modules,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2627
https://github.com/root-project/root/pull/2628:32,deployability,modul,modules,32,[cxxmodules] Re-introduce Cling modules; This is a revert (316f553538d2474c566e0b5460ce79d90e3028c0) of revert. (76a9ce75d2c29c169812b0816163ac7ce27ffad5 and. 88c16588108508eff66cc160d244c4a59eb157ed).,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2628
https://github.com/root-project/root/pull/2628:32,modifiability,modul,modules,32,[cxxmodules] Re-introduce Cling modules; This is a revert (316f553538d2474c566e0b5460ce79d90e3028c0) of revert. (76a9ce75d2c29c169812b0816163ac7ce27ffad5 and. 88c16588108508eff66cc160d244c4a59eb157ed).,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2628
https://github.com/root-project/root/pull/2628:32,safety,modul,modules,32,[cxxmodules] Re-introduce Cling modules; This is a revert (316f553538d2474c566e0b5460ce79d90e3028c0) of revert. (76a9ce75d2c29c169812b0816163ac7ce27ffad5 and. 88c16588108508eff66cc160d244c4a59eb157ed).,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2628
https://github.com/root-project/root/pull/2629:52,safety,avoid,avoid,52,[cxxmodules] Add pthread_setname_np forward decl to avoid warning in; cxxmodules,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2629
https://github.com/root-project/root/pull/2630:50,deployability,Integr,Integration,50,[TMVA] Modernise CrossValidation with clang-tidy; Integration of improvements suggested by running clang-tidy on the CrossValidation class.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2630
https://github.com/root-project/root/pull/2630:50,integrability,Integr,Integration,50,[TMVA] Modernise CrossValidation with clang-tidy; Integration of improvements suggested by running clang-tidy on the CrossValidation class.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2630
https://github.com/root-project/root/pull/2630:50,interoperability,Integr,Integration,50,[TMVA] Modernise CrossValidation with clang-tidy; Integration of improvements suggested by running clang-tidy on the CrossValidation class.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2630
https://github.com/root-project/root/pull/2630:50,modifiability,Integr,Integration,50,[TMVA] Modernise CrossValidation with clang-tidy; Integration of improvements suggested by running clang-tidy on the CrossValidation class.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2630
https://github.com/root-project/root/pull/2630:50,reliability,Integr,Integration,50,[TMVA] Modernise CrossValidation with clang-tidy; Integration of improvements suggested by running clang-tidy on the CrossValidation class.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2630
https://github.com/root-project/root/pull/2630:50,security,Integr,Integration,50,[TMVA] Modernise CrossValidation with clang-tidy; Integration of improvements suggested by running clang-tidy on the CrossValidation class.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2630
https://github.com/root-project/root/pull/2630:50,testability,Integr,Integration,50,[TMVA] Modernise CrossValidation with clang-tidy; Integration of improvements suggested by running clang-tidy on the CrossValidation class.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2630
https://github.com/root-project/root/pull/2631:273,energy efficiency,cpu,cpu,273,"[TMVA] Further parallelisation of BDTG; ## Performance after further //-isation. Note these are small gains; Benchmarks are run on small sample sizes. Training takes ~50% of total run time. Before:. root -l -b -q 'TMVAClassification.C(""BDTG"")' 4.06s user 1.01s system 147% cpu 3.423 total. root -l -b -q 'TMVARegression.C(""BDTG"")' 14.52s user 2.47s system 149% cpu 11.396 total. root -l -b -q 'TMVAMulticlass.C(""BDTG"")' 16.51s user 2.81s system 181% cpu 10.645 total. After:. root -l -b -q 'TMVAClassification.C(""BDTG"")' 3.99s user 0.97s system 149% cpu 3.310 total. root -l -b -q 'TMVARegression.C(""BDTG"")' 13.60s user 2.51s system 147% cpu 10.924 total. root -l -b -q 'TMVAMulticlass.C(""BDTG"")' 19.52s user 3.87s system 242% cpu 9.631 total. ## Output comparison. Summary: output identical before and after. ```. Classification. ==============. Top line: before. Bottom line: after. DataSet MVA Signal efficiency: from test sample (from training sample). Name: Method: @B=0.01 @B=0.10 @B=0.30. ------------------------------------------------------------------------------------------. dataset BDTG : 0.206 (0.540) 0.705 (0.858) 0.902 (0.928). dataset BDTG : 0.206 (0.540) 0.705 (0.858) 0.902 (0.928). Regression. ==========. Top line: before. Bottom line: after. DataSet Name: MVA Method: <Bias> <Bias_T> RMS RMS_T | MutInf MutInf_T. dataset BDTG : 0.198 0.165 2.60 2.00 | 3.084 3.149. dataset BDTG : 0.198 0.165 2.60 2.00 | 3.084 3.149. Multiclass. ==========. Top line: before. Bottom line: after. Dataset MVA Method ROC AUC Sig eff@B=0.01 Sig eff@B=0.10 Sig eff@B=0.30. Name: / Class: test (train) test (train) test (train) test (train). After:. dataset BDTG. ------------------------------. Signal 0.966 (0.981) 0.486 (0.659) 0.910 (0.955) 0.994 (0.996). Signal 0.966 (0.981) 0.486 (0.659) 0.910 (0.955) 0.994 (0.996). . bg0 0.905 (0.932) 0.252 (0.325) 0.716 (0.801) 0.908 (0.955). bg0 0.905 (0.932) 0.252 (0.325) 0.716 (0.801) 0.908 (0.955). . bg1 0.951 (0.955) 0.463 (0.465) 0.850 (0.845) 0.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2631
https://github.com/root-project/root/pull/2631:361,energy efficiency,cpu,cpu,361,"[TMVA] Further parallelisation of BDTG; ## Performance after further //-isation. Note these are small gains; Benchmarks are run on small sample sizes. Training takes ~50% of total run time. Before:. root -l -b -q 'TMVAClassification.C(""BDTG"")' 4.06s user 1.01s system 147% cpu 3.423 total. root -l -b -q 'TMVARegression.C(""BDTG"")' 14.52s user 2.47s system 149% cpu 11.396 total. root -l -b -q 'TMVAMulticlass.C(""BDTG"")' 16.51s user 2.81s system 181% cpu 10.645 total. After:. root -l -b -q 'TMVAClassification.C(""BDTG"")' 3.99s user 0.97s system 149% cpu 3.310 total. root -l -b -q 'TMVARegression.C(""BDTG"")' 13.60s user 2.51s system 147% cpu 10.924 total. root -l -b -q 'TMVAMulticlass.C(""BDTG"")' 19.52s user 3.87s system 242% cpu 9.631 total. ## Output comparison. Summary: output identical before and after. ```. Classification. ==============. Top line: before. Bottom line: after. DataSet MVA Signal efficiency: from test sample (from training sample). Name: Method: @B=0.01 @B=0.10 @B=0.30. ------------------------------------------------------------------------------------------. dataset BDTG : 0.206 (0.540) 0.705 (0.858) 0.902 (0.928). dataset BDTG : 0.206 (0.540) 0.705 (0.858) 0.902 (0.928). Regression. ==========. Top line: before. Bottom line: after. DataSet Name: MVA Method: <Bias> <Bias_T> RMS RMS_T | MutInf MutInf_T. dataset BDTG : 0.198 0.165 2.60 2.00 | 3.084 3.149. dataset BDTG : 0.198 0.165 2.60 2.00 | 3.084 3.149. Multiclass. ==========. Top line: before. Bottom line: after. Dataset MVA Method ROC AUC Sig eff@B=0.01 Sig eff@B=0.10 Sig eff@B=0.30. Name: / Class: test (train) test (train) test (train) test (train). After:. dataset BDTG. ------------------------------. Signal 0.966 (0.981) 0.486 (0.659) 0.910 (0.955) 0.994 (0.996). Signal 0.966 (0.981) 0.486 (0.659) 0.910 (0.955) 0.994 (0.996). . bg0 0.905 (0.932) 0.252 (0.325) 0.716 (0.801) 0.908 (0.955). bg0 0.905 (0.932) 0.252 (0.325) 0.716 (0.801) 0.908 (0.955). . bg1 0.951 (0.955) 0.463 (0.465) 0.850 (0.845) 0.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2631
https://github.com/root-project/root/pull/2631:450,energy efficiency,cpu,cpu,450,"[TMVA] Further parallelisation of BDTG; ## Performance after further //-isation. Note these are small gains; Benchmarks are run on small sample sizes. Training takes ~50% of total run time. Before:. root -l -b -q 'TMVAClassification.C(""BDTG"")' 4.06s user 1.01s system 147% cpu 3.423 total. root -l -b -q 'TMVARegression.C(""BDTG"")' 14.52s user 2.47s system 149% cpu 11.396 total. root -l -b -q 'TMVAMulticlass.C(""BDTG"")' 16.51s user 2.81s system 181% cpu 10.645 total. After:. root -l -b -q 'TMVAClassification.C(""BDTG"")' 3.99s user 0.97s system 149% cpu 3.310 total. root -l -b -q 'TMVARegression.C(""BDTG"")' 13.60s user 2.51s system 147% cpu 10.924 total. root -l -b -q 'TMVAMulticlass.C(""BDTG"")' 19.52s user 3.87s system 242% cpu 9.631 total. ## Output comparison. Summary: output identical before and after. ```. Classification. ==============. Top line: before. Bottom line: after. DataSet MVA Signal efficiency: from test sample (from training sample). Name: Method: @B=0.01 @B=0.10 @B=0.30. ------------------------------------------------------------------------------------------. dataset BDTG : 0.206 (0.540) 0.705 (0.858) 0.902 (0.928). dataset BDTG : 0.206 (0.540) 0.705 (0.858) 0.902 (0.928). Regression. ==========. Top line: before. Bottom line: after. DataSet Name: MVA Method: <Bias> <Bias_T> RMS RMS_T | MutInf MutInf_T. dataset BDTG : 0.198 0.165 2.60 2.00 | 3.084 3.149. dataset BDTG : 0.198 0.165 2.60 2.00 | 3.084 3.149. Multiclass. ==========. Top line: before. Bottom line: after. Dataset MVA Method ROC AUC Sig eff@B=0.01 Sig eff@B=0.10 Sig eff@B=0.30. Name: / Class: test (train) test (train) test (train) test (train). After:. dataset BDTG. ------------------------------. Signal 0.966 (0.981) 0.486 (0.659) 0.910 (0.955) 0.994 (0.996). Signal 0.966 (0.981) 0.486 (0.659) 0.910 (0.955) 0.994 (0.996). . bg0 0.905 (0.932) 0.252 (0.325) 0.716 (0.801) 0.908 (0.955). bg0 0.905 (0.932) 0.252 (0.325) 0.716 (0.801) 0.908 (0.955). . bg1 0.951 (0.955) 0.463 (0.465) 0.850 (0.845) 0.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2631
https://github.com/root-project/root/pull/2631:550,energy efficiency,cpu,cpu,550,"[TMVA] Further parallelisation of BDTG; ## Performance after further //-isation. Note these are small gains; Benchmarks are run on small sample sizes. Training takes ~50% of total run time. Before:. root -l -b -q 'TMVAClassification.C(""BDTG"")' 4.06s user 1.01s system 147% cpu 3.423 total. root -l -b -q 'TMVARegression.C(""BDTG"")' 14.52s user 2.47s system 149% cpu 11.396 total. root -l -b -q 'TMVAMulticlass.C(""BDTG"")' 16.51s user 2.81s system 181% cpu 10.645 total. After:. root -l -b -q 'TMVAClassification.C(""BDTG"")' 3.99s user 0.97s system 149% cpu 3.310 total. root -l -b -q 'TMVARegression.C(""BDTG"")' 13.60s user 2.51s system 147% cpu 10.924 total. root -l -b -q 'TMVAMulticlass.C(""BDTG"")' 19.52s user 3.87s system 242% cpu 9.631 total. ## Output comparison. Summary: output identical before and after. ```. Classification. ==============. Top line: before. Bottom line: after. DataSet MVA Signal efficiency: from test sample (from training sample). Name: Method: @B=0.01 @B=0.10 @B=0.30. ------------------------------------------------------------------------------------------. dataset BDTG : 0.206 (0.540) 0.705 (0.858) 0.902 (0.928). dataset BDTG : 0.206 (0.540) 0.705 (0.858) 0.902 (0.928). Regression. ==========. Top line: before. Bottom line: after. DataSet Name: MVA Method: <Bias> <Bias_T> RMS RMS_T | MutInf MutInf_T. dataset BDTG : 0.198 0.165 2.60 2.00 | 3.084 3.149. dataset BDTG : 0.198 0.165 2.60 2.00 | 3.084 3.149. Multiclass. ==========. Top line: before. Bottom line: after. Dataset MVA Method ROC AUC Sig eff@B=0.01 Sig eff@B=0.10 Sig eff@B=0.30. Name: / Class: test (train) test (train) test (train) test (train). After:. dataset BDTG. ------------------------------. Signal 0.966 (0.981) 0.486 (0.659) 0.910 (0.955) 0.994 (0.996). Signal 0.966 (0.981) 0.486 (0.659) 0.910 (0.955) 0.994 (0.996). . bg0 0.905 (0.932) 0.252 (0.325) 0.716 (0.801) 0.908 (0.955). bg0 0.905 (0.932) 0.252 (0.325) 0.716 (0.801) 0.908 (0.955). . bg1 0.951 (0.955) 0.463 (0.465) 0.850 (0.845) 0.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2631
https://github.com/root-project/root/pull/2631:638,energy efficiency,cpu,cpu,638,"[TMVA] Further parallelisation of BDTG; ## Performance after further //-isation. Note these are small gains; Benchmarks are run on small sample sizes. Training takes ~50% of total run time. Before:. root -l -b -q 'TMVAClassification.C(""BDTG"")' 4.06s user 1.01s system 147% cpu 3.423 total. root -l -b -q 'TMVARegression.C(""BDTG"")' 14.52s user 2.47s system 149% cpu 11.396 total. root -l -b -q 'TMVAMulticlass.C(""BDTG"")' 16.51s user 2.81s system 181% cpu 10.645 total. After:. root -l -b -q 'TMVAClassification.C(""BDTG"")' 3.99s user 0.97s system 149% cpu 3.310 total. root -l -b -q 'TMVARegression.C(""BDTG"")' 13.60s user 2.51s system 147% cpu 10.924 total. root -l -b -q 'TMVAMulticlass.C(""BDTG"")' 19.52s user 3.87s system 242% cpu 9.631 total. ## Output comparison. Summary: output identical before and after. ```. Classification. ==============. Top line: before. Bottom line: after. DataSet MVA Signal efficiency: from test sample (from training sample). Name: Method: @B=0.01 @B=0.10 @B=0.30. ------------------------------------------------------------------------------------------. dataset BDTG : 0.206 (0.540) 0.705 (0.858) 0.902 (0.928). dataset BDTG : 0.206 (0.540) 0.705 (0.858) 0.902 (0.928). Regression. ==========. Top line: before. Bottom line: after. DataSet Name: MVA Method: <Bias> <Bias_T> RMS RMS_T | MutInf MutInf_T. dataset BDTG : 0.198 0.165 2.60 2.00 | 3.084 3.149. dataset BDTG : 0.198 0.165 2.60 2.00 | 3.084 3.149. Multiclass. ==========. Top line: before. Bottom line: after. Dataset MVA Method ROC AUC Sig eff@B=0.01 Sig eff@B=0.10 Sig eff@B=0.30. Name: / Class: test (train) test (train) test (train) test (train). After:. dataset BDTG. ------------------------------. Signal 0.966 (0.981) 0.486 (0.659) 0.910 (0.955) 0.994 (0.996). Signal 0.966 (0.981) 0.486 (0.659) 0.910 (0.955) 0.994 (0.996). . bg0 0.905 (0.932) 0.252 (0.325) 0.716 (0.801) 0.908 (0.955). bg0 0.905 (0.932) 0.252 (0.325) 0.716 (0.801) 0.908 (0.955). . bg1 0.951 (0.955) 0.463 (0.465) 0.850 (0.845) 0.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2631
https://github.com/root-project/root/pull/2631:727,energy efficiency,cpu,cpu,727,"[TMVA] Further parallelisation of BDTG; ## Performance after further //-isation. Note these are small gains; Benchmarks are run on small sample sizes. Training takes ~50% of total run time. Before:. root -l -b -q 'TMVAClassification.C(""BDTG"")' 4.06s user 1.01s system 147% cpu 3.423 total. root -l -b -q 'TMVARegression.C(""BDTG"")' 14.52s user 2.47s system 149% cpu 11.396 total. root -l -b -q 'TMVAMulticlass.C(""BDTG"")' 16.51s user 2.81s system 181% cpu 10.645 total. After:. root -l -b -q 'TMVAClassification.C(""BDTG"")' 3.99s user 0.97s system 149% cpu 3.310 total. root -l -b -q 'TMVARegression.C(""BDTG"")' 13.60s user 2.51s system 147% cpu 10.924 total. root -l -b -q 'TMVAMulticlass.C(""BDTG"")' 19.52s user 3.87s system 242% cpu 9.631 total. ## Output comparison. Summary: output identical before and after. ```. Classification. ==============. Top line: before. Bottom line: after. DataSet MVA Signal efficiency: from test sample (from training sample). Name: Method: @B=0.01 @B=0.10 @B=0.30. ------------------------------------------------------------------------------------------. dataset BDTG : 0.206 (0.540) 0.705 (0.858) 0.902 (0.928). dataset BDTG : 0.206 (0.540) 0.705 (0.858) 0.902 (0.928). Regression. ==========. Top line: before. Bottom line: after. DataSet Name: MVA Method: <Bias> <Bias_T> RMS RMS_T | MutInf MutInf_T. dataset BDTG : 0.198 0.165 2.60 2.00 | 3.084 3.149. dataset BDTG : 0.198 0.165 2.60 2.00 | 3.084 3.149. Multiclass. ==========. Top line: before. Bottom line: after. Dataset MVA Method ROC AUC Sig eff@B=0.01 Sig eff@B=0.10 Sig eff@B=0.30. Name: / Class: test (train) test (train) test (train) test (train). After:. dataset BDTG. ------------------------------. Signal 0.966 (0.981) 0.486 (0.659) 0.910 (0.955) 0.994 (0.996). Signal 0.966 (0.981) 0.486 (0.659) 0.910 (0.955) 0.994 (0.996). . bg0 0.905 (0.932) 0.252 (0.325) 0.716 (0.801) 0.908 (0.955). bg0 0.905 (0.932) 0.252 (0.325) 0.716 (0.801) 0.908 (0.955). . bg1 0.951 (0.955) 0.463 (0.465) 0.850 (0.845) 0.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2631
https://github.com/root-project/root/pull/2631:15,performance,parallel,parallelisation,15,"[TMVA] Further parallelisation of BDTG; ## Performance after further //-isation. Note these are small gains; Benchmarks are run on small sample sizes. Training takes ~50% of total run time. Before:. root -l -b -q 'TMVAClassification.C(""BDTG"")' 4.06s user 1.01s system 147% cpu 3.423 total. root -l -b -q 'TMVARegression.C(""BDTG"")' 14.52s user 2.47s system 149% cpu 11.396 total. root -l -b -q 'TMVAMulticlass.C(""BDTG"")' 16.51s user 2.81s system 181% cpu 10.645 total. After:. root -l -b -q 'TMVAClassification.C(""BDTG"")' 3.99s user 0.97s system 149% cpu 3.310 total. root -l -b -q 'TMVARegression.C(""BDTG"")' 13.60s user 2.51s system 147% cpu 10.924 total. root -l -b -q 'TMVAMulticlass.C(""BDTG"")' 19.52s user 3.87s system 242% cpu 9.631 total. ## Output comparison. Summary: output identical before and after. ```. Classification. ==============. Top line: before. Bottom line: after. DataSet MVA Signal efficiency: from test sample (from training sample). Name: Method: @B=0.01 @B=0.10 @B=0.30. ------------------------------------------------------------------------------------------. dataset BDTG : 0.206 (0.540) 0.705 (0.858) 0.902 (0.928). dataset BDTG : 0.206 (0.540) 0.705 (0.858) 0.902 (0.928). Regression. ==========. Top line: before. Bottom line: after. DataSet Name: MVA Method: <Bias> <Bias_T> RMS RMS_T | MutInf MutInf_T. dataset BDTG : 0.198 0.165 2.60 2.00 | 3.084 3.149. dataset BDTG : 0.198 0.165 2.60 2.00 | 3.084 3.149. Multiclass. ==========. Top line: before. Bottom line: after. Dataset MVA Method ROC AUC Sig eff@B=0.01 Sig eff@B=0.10 Sig eff@B=0.30. Name: / Class: test (train) test (train) test (train) test (train). After:. dataset BDTG. ------------------------------. Signal 0.966 (0.981) 0.486 (0.659) 0.910 (0.955) 0.994 (0.996). Signal 0.966 (0.981) 0.486 (0.659) 0.910 (0.955) 0.994 (0.996). . bg0 0.905 (0.932) 0.252 (0.325) 0.716 (0.801) 0.908 (0.955). bg0 0.905 (0.932) 0.252 (0.325) 0.716 (0.801) 0.908 (0.955). . bg1 0.951 (0.955) 0.463 (0.465) 0.850 (0.845) 0.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2631
https://github.com/root-project/root/pull/2631:43,performance,Perform,Performance,43,"[TMVA] Further parallelisation of BDTG; ## Performance after further //-isation. Note these are small gains; Benchmarks are run on small sample sizes. Training takes ~50% of total run time. Before:. root -l -b -q 'TMVAClassification.C(""BDTG"")' 4.06s user 1.01s system 147% cpu 3.423 total. root -l -b -q 'TMVARegression.C(""BDTG"")' 14.52s user 2.47s system 149% cpu 11.396 total. root -l -b -q 'TMVAMulticlass.C(""BDTG"")' 16.51s user 2.81s system 181% cpu 10.645 total. After:. root -l -b -q 'TMVAClassification.C(""BDTG"")' 3.99s user 0.97s system 149% cpu 3.310 total. root -l -b -q 'TMVARegression.C(""BDTG"")' 13.60s user 2.51s system 147% cpu 10.924 total. root -l -b -q 'TMVAMulticlass.C(""BDTG"")' 19.52s user 3.87s system 242% cpu 9.631 total. ## Output comparison. Summary: output identical before and after. ```. Classification. ==============. Top line: before. Bottom line: after. DataSet MVA Signal efficiency: from test sample (from training sample). Name: Method: @B=0.01 @B=0.10 @B=0.30. ------------------------------------------------------------------------------------------. dataset BDTG : 0.206 (0.540) 0.705 (0.858) 0.902 (0.928). dataset BDTG : 0.206 (0.540) 0.705 (0.858) 0.902 (0.928). Regression. ==========. Top line: before. Bottom line: after. DataSet Name: MVA Method: <Bias> <Bias_T> RMS RMS_T | MutInf MutInf_T. dataset BDTG : 0.198 0.165 2.60 2.00 | 3.084 3.149. dataset BDTG : 0.198 0.165 2.60 2.00 | 3.084 3.149. Multiclass. ==========. Top line: before. Bottom line: after. Dataset MVA Method ROC AUC Sig eff@B=0.01 Sig eff@B=0.10 Sig eff@B=0.30. Name: / Class: test (train) test (train) test (train) test (train). After:. dataset BDTG. ------------------------------. Signal 0.966 (0.981) 0.486 (0.659) 0.910 (0.955) 0.994 (0.996). Signal 0.966 (0.981) 0.486 (0.659) 0.910 (0.955) 0.994 (0.996). . bg0 0.905 (0.932) 0.252 (0.325) 0.716 (0.801) 0.908 (0.955). bg0 0.905 (0.932) 0.252 (0.325) 0.716 (0.801) 0.908 (0.955). . bg1 0.951 (0.955) 0.463 (0.465) 0.850 (0.845) 0.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2631
https://github.com/root-project/root/pull/2631:184,performance,time,time,184,"[TMVA] Further parallelisation of BDTG; ## Performance after further //-isation. Note these are small gains; Benchmarks are run on small sample sizes. Training takes ~50% of total run time. Before:. root -l -b -q 'TMVAClassification.C(""BDTG"")' 4.06s user 1.01s system 147% cpu 3.423 total. root -l -b -q 'TMVARegression.C(""BDTG"")' 14.52s user 2.47s system 149% cpu 11.396 total. root -l -b -q 'TMVAMulticlass.C(""BDTG"")' 16.51s user 2.81s system 181% cpu 10.645 total. After:. root -l -b -q 'TMVAClassification.C(""BDTG"")' 3.99s user 0.97s system 149% cpu 3.310 total. root -l -b -q 'TMVARegression.C(""BDTG"")' 13.60s user 2.51s system 147% cpu 10.924 total. root -l -b -q 'TMVAMulticlass.C(""BDTG"")' 19.52s user 3.87s system 242% cpu 9.631 total. ## Output comparison. Summary: output identical before and after. ```. Classification. ==============. Top line: before. Bottom line: after. DataSet MVA Signal efficiency: from test sample (from training sample). Name: Method: @B=0.01 @B=0.10 @B=0.30. ------------------------------------------------------------------------------------------. dataset BDTG : 0.206 (0.540) 0.705 (0.858) 0.902 (0.928). dataset BDTG : 0.206 (0.540) 0.705 (0.858) 0.902 (0.928). Regression. ==========. Top line: before. Bottom line: after. DataSet Name: MVA Method: <Bias> <Bias_T> RMS RMS_T | MutInf MutInf_T. dataset BDTG : 0.198 0.165 2.60 2.00 | 3.084 3.149. dataset BDTG : 0.198 0.165 2.60 2.00 | 3.084 3.149. Multiclass. ==========. Top line: before. Bottom line: after. Dataset MVA Method ROC AUC Sig eff@B=0.01 Sig eff@B=0.10 Sig eff@B=0.30. Name: / Class: test (train) test (train) test (train) test (train). After:. dataset BDTG. ------------------------------. Signal 0.966 (0.981) 0.486 (0.659) 0.910 (0.955) 0.994 (0.996). Signal 0.966 (0.981) 0.486 (0.659) 0.910 (0.955) 0.994 (0.996). . bg0 0.905 (0.932) 0.252 (0.325) 0.716 (0.801) 0.908 (0.955). bg0 0.905 (0.932) 0.252 (0.325) 0.716 (0.801) 0.908 (0.955). . bg1 0.951 (0.955) 0.463 (0.465) 0.850 (0.845) 0.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2631
https://github.com/root-project/root/pull/2631:273,performance,cpu,cpu,273,"[TMVA] Further parallelisation of BDTG; ## Performance after further //-isation. Note these are small gains; Benchmarks are run on small sample sizes. Training takes ~50% of total run time. Before:. root -l -b -q 'TMVAClassification.C(""BDTG"")' 4.06s user 1.01s system 147% cpu 3.423 total. root -l -b -q 'TMVARegression.C(""BDTG"")' 14.52s user 2.47s system 149% cpu 11.396 total. root -l -b -q 'TMVAMulticlass.C(""BDTG"")' 16.51s user 2.81s system 181% cpu 10.645 total. After:. root -l -b -q 'TMVAClassification.C(""BDTG"")' 3.99s user 0.97s system 149% cpu 3.310 total. root -l -b -q 'TMVARegression.C(""BDTG"")' 13.60s user 2.51s system 147% cpu 10.924 total. root -l -b -q 'TMVAMulticlass.C(""BDTG"")' 19.52s user 3.87s system 242% cpu 9.631 total. ## Output comparison. Summary: output identical before and after. ```. Classification. ==============. Top line: before. Bottom line: after. DataSet MVA Signal efficiency: from test sample (from training sample). Name: Method: @B=0.01 @B=0.10 @B=0.30. ------------------------------------------------------------------------------------------. dataset BDTG : 0.206 (0.540) 0.705 (0.858) 0.902 (0.928). dataset BDTG : 0.206 (0.540) 0.705 (0.858) 0.902 (0.928). Regression. ==========. Top line: before. Bottom line: after. DataSet Name: MVA Method: <Bias> <Bias_T> RMS RMS_T | MutInf MutInf_T. dataset BDTG : 0.198 0.165 2.60 2.00 | 3.084 3.149. dataset BDTG : 0.198 0.165 2.60 2.00 | 3.084 3.149. Multiclass. ==========. Top line: before. Bottom line: after. Dataset MVA Method ROC AUC Sig eff@B=0.01 Sig eff@B=0.10 Sig eff@B=0.30. Name: / Class: test (train) test (train) test (train) test (train). After:. dataset BDTG. ------------------------------. Signal 0.966 (0.981) 0.486 (0.659) 0.910 (0.955) 0.994 (0.996). Signal 0.966 (0.981) 0.486 (0.659) 0.910 (0.955) 0.994 (0.996). . bg0 0.905 (0.932) 0.252 (0.325) 0.716 (0.801) 0.908 (0.955). bg0 0.905 (0.932) 0.252 (0.325) 0.716 (0.801) 0.908 (0.955). . bg1 0.951 (0.955) 0.463 (0.465) 0.850 (0.845) 0.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2631
https://github.com/root-project/root/pull/2631:361,performance,cpu,cpu,361,"[TMVA] Further parallelisation of BDTG; ## Performance after further //-isation. Note these are small gains; Benchmarks are run on small sample sizes. Training takes ~50% of total run time. Before:. root -l -b -q 'TMVAClassification.C(""BDTG"")' 4.06s user 1.01s system 147% cpu 3.423 total. root -l -b -q 'TMVARegression.C(""BDTG"")' 14.52s user 2.47s system 149% cpu 11.396 total. root -l -b -q 'TMVAMulticlass.C(""BDTG"")' 16.51s user 2.81s system 181% cpu 10.645 total. After:. root -l -b -q 'TMVAClassification.C(""BDTG"")' 3.99s user 0.97s system 149% cpu 3.310 total. root -l -b -q 'TMVARegression.C(""BDTG"")' 13.60s user 2.51s system 147% cpu 10.924 total. root -l -b -q 'TMVAMulticlass.C(""BDTG"")' 19.52s user 3.87s system 242% cpu 9.631 total. ## Output comparison. Summary: output identical before and after. ```. Classification. ==============. Top line: before. Bottom line: after. DataSet MVA Signal efficiency: from test sample (from training sample). Name: Method: @B=0.01 @B=0.10 @B=0.30. ------------------------------------------------------------------------------------------. dataset BDTG : 0.206 (0.540) 0.705 (0.858) 0.902 (0.928). dataset BDTG : 0.206 (0.540) 0.705 (0.858) 0.902 (0.928). Regression. ==========. Top line: before. Bottom line: after. DataSet Name: MVA Method: <Bias> <Bias_T> RMS RMS_T | MutInf MutInf_T. dataset BDTG : 0.198 0.165 2.60 2.00 | 3.084 3.149. dataset BDTG : 0.198 0.165 2.60 2.00 | 3.084 3.149. Multiclass. ==========. Top line: before. Bottom line: after. Dataset MVA Method ROC AUC Sig eff@B=0.01 Sig eff@B=0.10 Sig eff@B=0.30. Name: / Class: test (train) test (train) test (train) test (train). After:. dataset BDTG. ------------------------------. Signal 0.966 (0.981) 0.486 (0.659) 0.910 (0.955) 0.994 (0.996). Signal 0.966 (0.981) 0.486 (0.659) 0.910 (0.955) 0.994 (0.996). . bg0 0.905 (0.932) 0.252 (0.325) 0.716 (0.801) 0.908 (0.955). bg0 0.905 (0.932) 0.252 (0.325) 0.716 (0.801) 0.908 (0.955). . bg1 0.951 (0.955) 0.463 (0.465) 0.850 (0.845) 0.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2631
https://github.com/root-project/root/pull/2631:450,performance,cpu,cpu,450,"[TMVA] Further parallelisation of BDTG; ## Performance after further //-isation. Note these are small gains; Benchmarks are run on small sample sizes. Training takes ~50% of total run time. Before:. root -l -b -q 'TMVAClassification.C(""BDTG"")' 4.06s user 1.01s system 147% cpu 3.423 total. root -l -b -q 'TMVARegression.C(""BDTG"")' 14.52s user 2.47s system 149% cpu 11.396 total. root -l -b -q 'TMVAMulticlass.C(""BDTG"")' 16.51s user 2.81s system 181% cpu 10.645 total. After:. root -l -b -q 'TMVAClassification.C(""BDTG"")' 3.99s user 0.97s system 149% cpu 3.310 total. root -l -b -q 'TMVARegression.C(""BDTG"")' 13.60s user 2.51s system 147% cpu 10.924 total. root -l -b -q 'TMVAMulticlass.C(""BDTG"")' 19.52s user 3.87s system 242% cpu 9.631 total. ## Output comparison. Summary: output identical before and after. ```. Classification. ==============. Top line: before. Bottom line: after. DataSet MVA Signal efficiency: from test sample (from training sample). Name: Method: @B=0.01 @B=0.10 @B=0.30. ------------------------------------------------------------------------------------------. dataset BDTG : 0.206 (0.540) 0.705 (0.858) 0.902 (0.928). dataset BDTG : 0.206 (0.540) 0.705 (0.858) 0.902 (0.928). Regression. ==========. Top line: before. Bottom line: after. DataSet Name: MVA Method: <Bias> <Bias_T> RMS RMS_T | MutInf MutInf_T. dataset BDTG : 0.198 0.165 2.60 2.00 | 3.084 3.149. dataset BDTG : 0.198 0.165 2.60 2.00 | 3.084 3.149. Multiclass. ==========. Top line: before. Bottom line: after. Dataset MVA Method ROC AUC Sig eff@B=0.01 Sig eff@B=0.10 Sig eff@B=0.30. Name: / Class: test (train) test (train) test (train) test (train). After:. dataset BDTG. ------------------------------. Signal 0.966 (0.981) 0.486 (0.659) 0.910 (0.955) 0.994 (0.996). Signal 0.966 (0.981) 0.486 (0.659) 0.910 (0.955) 0.994 (0.996). . bg0 0.905 (0.932) 0.252 (0.325) 0.716 (0.801) 0.908 (0.955). bg0 0.905 (0.932) 0.252 (0.325) 0.716 (0.801) 0.908 (0.955). . bg1 0.951 (0.955) 0.463 (0.465) 0.850 (0.845) 0.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2631
https://github.com/root-project/root/pull/2631:550,performance,cpu,cpu,550,"[TMVA] Further parallelisation of BDTG; ## Performance after further //-isation. Note these are small gains; Benchmarks are run on small sample sizes. Training takes ~50% of total run time. Before:. root -l -b -q 'TMVAClassification.C(""BDTG"")' 4.06s user 1.01s system 147% cpu 3.423 total. root -l -b -q 'TMVARegression.C(""BDTG"")' 14.52s user 2.47s system 149% cpu 11.396 total. root -l -b -q 'TMVAMulticlass.C(""BDTG"")' 16.51s user 2.81s system 181% cpu 10.645 total. After:. root -l -b -q 'TMVAClassification.C(""BDTG"")' 3.99s user 0.97s system 149% cpu 3.310 total. root -l -b -q 'TMVARegression.C(""BDTG"")' 13.60s user 2.51s system 147% cpu 10.924 total. root -l -b -q 'TMVAMulticlass.C(""BDTG"")' 19.52s user 3.87s system 242% cpu 9.631 total. ## Output comparison. Summary: output identical before and after. ```. Classification. ==============. Top line: before. Bottom line: after. DataSet MVA Signal efficiency: from test sample (from training sample). Name: Method: @B=0.01 @B=0.10 @B=0.30. ------------------------------------------------------------------------------------------. dataset BDTG : 0.206 (0.540) 0.705 (0.858) 0.902 (0.928). dataset BDTG : 0.206 (0.540) 0.705 (0.858) 0.902 (0.928). Regression. ==========. Top line: before. Bottom line: after. DataSet Name: MVA Method: <Bias> <Bias_T> RMS RMS_T | MutInf MutInf_T. dataset BDTG : 0.198 0.165 2.60 2.00 | 3.084 3.149. dataset BDTG : 0.198 0.165 2.60 2.00 | 3.084 3.149. Multiclass. ==========. Top line: before. Bottom line: after. Dataset MVA Method ROC AUC Sig eff@B=0.01 Sig eff@B=0.10 Sig eff@B=0.30. Name: / Class: test (train) test (train) test (train) test (train). After:. dataset BDTG. ------------------------------. Signal 0.966 (0.981) 0.486 (0.659) 0.910 (0.955) 0.994 (0.996). Signal 0.966 (0.981) 0.486 (0.659) 0.910 (0.955) 0.994 (0.996). . bg0 0.905 (0.932) 0.252 (0.325) 0.716 (0.801) 0.908 (0.955). bg0 0.905 (0.932) 0.252 (0.325) 0.716 (0.801) 0.908 (0.955). . bg1 0.951 (0.955) 0.463 (0.465) 0.850 (0.845) 0.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2631
https://github.com/root-project/root/pull/2631:638,performance,cpu,cpu,638,"[TMVA] Further parallelisation of BDTG; ## Performance after further //-isation. Note these are small gains; Benchmarks are run on small sample sizes. Training takes ~50% of total run time. Before:. root -l -b -q 'TMVAClassification.C(""BDTG"")' 4.06s user 1.01s system 147% cpu 3.423 total. root -l -b -q 'TMVARegression.C(""BDTG"")' 14.52s user 2.47s system 149% cpu 11.396 total. root -l -b -q 'TMVAMulticlass.C(""BDTG"")' 16.51s user 2.81s system 181% cpu 10.645 total. After:. root -l -b -q 'TMVAClassification.C(""BDTG"")' 3.99s user 0.97s system 149% cpu 3.310 total. root -l -b -q 'TMVARegression.C(""BDTG"")' 13.60s user 2.51s system 147% cpu 10.924 total. root -l -b -q 'TMVAMulticlass.C(""BDTG"")' 19.52s user 3.87s system 242% cpu 9.631 total. ## Output comparison. Summary: output identical before and after. ```. Classification. ==============. Top line: before. Bottom line: after. DataSet MVA Signal efficiency: from test sample (from training sample). Name: Method: @B=0.01 @B=0.10 @B=0.30. ------------------------------------------------------------------------------------------. dataset BDTG : 0.206 (0.540) 0.705 (0.858) 0.902 (0.928). dataset BDTG : 0.206 (0.540) 0.705 (0.858) 0.902 (0.928). Regression. ==========. Top line: before. Bottom line: after. DataSet Name: MVA Method: <Bias> <Bias_T> RMS RMS_T | MutInf MutInf_T. dataset BDTG : 0.198 0.165 2.60 2.00 | 3.084 3.149. dataset BDTG : 0.198 0.165 2.60 2.00 | 3.084 3.149. Multiclass. ==========. Top line: before. Bottom line: after. Dataset MVA Method ROC AUC Sig eff@B=0.01 Sig eff@B=0.10 Sig eff@B=0.30. Name: / Class: test (train) test (train) test (train) test (train). After:. dataset BDTG. ------------------------------. Signal 0.966 (0.981) 0.486 (0.659) 0.910 (0.955) 0.994 (0.996). Signal 0.966 (0.981) 0.486 (0.659) 0.910 (0.955) 0.994 (0.996). . bg0 0.905 (0.932) 0.252 (0.325) 0.716 (0.801) 0.908 (0.955). bg0 0.905 (0.932) 0.252 (0.325) 0.716 (0.801) 0.908 (0.955). . bg1 0.951 (0.955) 0.463 (0.465) 0.850 (0.845) 0.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2631
https://github.com/root-project/root/pull/2631:727,performance,cpu,cpu,727,"[TMVA] Further parallelisation of BDTG; ## Performance after further //-isation. Note these are small gains; Benchmarks are run on small sample sizes. Training takes ~50% of total run time. Before:. root -l -b -q 'TMVAClassification.C(""BDTG"")' 4.06s user 1.01s system 147% cpu 3.423 total. root -l -b -q 'TMVARegression.C(""BDTG"")' 14.52s user 2.47s system 149% cpu 11.396 total. root -l -b -q 'TMVAMulticlass.C(""BDTG"")' 16.51s user 2.81s system 181% cpu 10.645 total. After:. root -l -b -q 'TMVAClassification.C(""BDTG"")' 3.99s user 0.97s system 149% cpu 3.310 total. root -l -b -q 'TMVARegression.C(""BDTG"")' 13.60s user 2.51s system 147% cpu 10.924 total. root -l -b -q 'TMVAMulticlass.C(""BDTG"")' 19.52s user 3.87s system 242% cpu 9.631 total. ## Output comparison. Summary: output identical before and after. ```. Classification. ==============. Top line: before. Bottom line: after. DataSet MVA Signal efficiency: from test sample (from training sample). Name: Method: @B=0.01 @B=0.10 @B=0.30. ------------------------------------------------------------------------------------------. dataset BDTG : 0.206 (0.540) 0.705 (0.858) 0.902 (0.928). dataset BDTG : 0.206 (0.540) 0.705 (0.858) 0.902 (0.928). Regression. ==========. Top line: before. Bottom line: after. DataSet Name: MVA Method: <Bias> <Bias_T> RMS RMS_T | MutInf MutInf_T. dataset BDTG : 0.198 0.165 2.60 2.00 | 3.084 3.149. dataset BDTG : 0.198 0.165 2.60 2.00 | 3.084 3.149. Multiclass. ==========. Top line: before. Bottom line: after. Dataset MVA Method ROC AUC Sig eff@B=0.01 Sig eff@B=0.10 Sig eff@B=0.30. Name: / Class: test (train) test (train) test (train) test (train). After:. dataset BDTG. ------------------------------. Signal 0.966 (0.981) 0.486 (0.659) 0.910 (0.955) 0.994 (0.996). Signal 0.966 (0.981) 0.486 (0.659) 0.910 (0.955) 0.994 (0.996). . bg0 0.905 (0.932) 0.252 (0.325) 0.716 (0.801) 0.908 (0.955). bg0 0.905 (0.932) 0.252 (0.325) 0.716 (0.801) 0.908 (0.955). . bg1 0.951 (0.955) 0.463 (0.465) 0.850 (0.845) 0.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2631
https://github.com/root-project/root/pull/2631:921,safety,test,test,921,"[TMVA] Further parallelisation of BDTG; ## Performance after further //-isation. Note these are small gains; Benchmarks are run on small sample sizes. Training takes ~50% of total run time. Before:. root -l -b -q 'TMVAClassification.C(""BDTG"")' 4.06s user 1.01s system 147% cpu 3.423 total. root -l -b -q 'TMVARegression.C(""BDTG"")' 14.52s user 2.47s system 149% cpu 11.396 total. root -l -b -q 'TMVAMulticlass.C(""BDTG"")' 16.51s user 2.81s system 181% cpu 10.645 total. After:. root -l -b -q 'TMVAClassification.C(""BDTG"")' 3.99s user 0.97s system 149% cpu 3.310 total. root -l -b -q 'TMVARegression.C(""BDTG"")' 13.60s user 2.51s system 147% cpu 10.924 total. root -l -b -q 'TMVAMulticlass.C(""BDTG"")' 19.52s user 3.87s system 242% cpu 9.631 total. ## Output comparison. Summary: output identical before and after. ```. Classification. ==============. Top line: before. Bottom line: after. DataSet MVA Signal efficiency: from test sample (from training sample). Name: Method: @B=0.01 @B=0.10 @B=0.30. ------------------------------------------------------------------------------------------. dataset BDTG : 0.206 (0.540) 0.705 (0.858) 0.902 (0.928). dataset BDTG : 0.206 (0.540) 0.705 (0.858) 0.902 (0.928). Regression. ==========. Top line: before. Bottom line: after. DataSet Name: MVA Method: <Bias> <Bias_T> RMS RMS_T | MutInf MutInf_T. dataset BDTG : 0.198 0.165 2.60 2.00 | 3.084 3.149. dataset BDTG : 0.198 0.165 2.60 2.00 | 3.084 3.149. Multiclass. ==========. Top line: before. Bottom line: after. Dataset MVA Method ROC AUC Sig eff@B=0.01 Sig eff@B=0.10 Sig eff@B=0.30. Name: / Class: test (train) test (train) test (train) test (train). After:. dataset BDTG. ------------------------------. Signal 0.966 (0.981) 0.486 (0.659) 0.910 (0.955) 0.994 (0.996). Signal 0.966 (0.981) 0.486 (0.659) 0.910 (0.955) 0.994 (0.996). . bg0 0.905 (0.932) 0.252 (0.325) 0.716 (0.801) 0.908 (0.955). bg0 0.905 (0.932) 0.252 (0.325) 0.716 (0.801) 0.908 (0.955). . bg1 0.951 (0.955) 0.463 (0.465) 0.850 (0.845) 0.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2631
https://github.com/root-project/root/pull/2631:1591,safety,test,test,1591,"root -l -b -q 'TMVAClassification.C(""BDTG"")' 4.06s user 1.01s system 147% cpu 3.423 total. root -l -b -q 'TMVARegression.C(""BDTG"")' 14.52s user 2.47s system 149% cpu 11.396 total. root -l -b -q 'TMVAMulticlass.C(""BDTG"")' 16.51s user 2.81s system 181% cpu 10.645 total. After:. root -l -b -q 'TMVAClassification.C(""BDTG"")' 3.99s user 0.97s system 149% cpu 3.310 total. root -l -b -q 'TMVARegression.C(""BDTG"")' 13.60s user 2.51s system 147% cpu 10.924 total. root -l -b -q 'TMVAMulticlass.C(""BDTG"")' 19.52s user 3.87s system 242% cpu 9.631 total. ## Output comparison. Summary: output identical before and after. ```. Classification. ==============. Top line: before. Bottom line: after. DataSet MVA Signal efficiency: from test sample (from training sample). Name: Method: @B=0.01 @B=0.10 @B=0.30. ------------------------------------------------------------------------------------------. dataset BDTG : 0.206 (0.540) 0.705 (0.858) 0.902 (0.928). dataset BDTG : 0.206 (0.540) 0.705 (0.858) 0.902 (0.928). Regression. ==========. Top line: before. Bottom line: after. DataSet Name: MVA Method: <Bias> <Bias_T> RMS RMS_T | MutInf MutInf_T. dataset BDTG : 0.198 0.165 2.60 2.00 | 3.084 3.149. dataset BDTG : 0.198 0.165 2.60 2.00 | 3.084 3.149. Multiclass. ==========. Top line: before. Bottom line: after. Dataset MVA Method ROC AUC Sig eff@B=0.01 Sig eff@B=0.10 Sig eff@B=0.30. Name: / Class: test (train) test (train) test (train) test (train). After:. dataset BDTG. ------------------------------. Signal 0.966 (0.981) 0.486 (0.659) 0.910 (0.955) 0.994 (0.996). Signal 0.966 (0.981) 0.486 (0.659) 0.910 (0.955) 0.994 (0.996). . bg0 0.905 (0.932) 0.252 (0.325) 0.716 (0.801) 0.908 (0.955). bg0 0.905 (0.932) 0.252 (0.325) 0.716 (0.801) 0.908 (0.955). . bg1 0.951 (0.955) 0.463 (0.465) 0.850 (0.845) 0.970 (0.979). bg1 0.951 (0.955) 0.463 (0.465) 0.850 (0.845) 0.970 (0.979). bg2 0.977 (0.984) 0.598 (0.709) 0.945 (0.971) 0.996 (0.998). bg2 0.977 (0.984) 0.598 (0.709) 0.945 (0.971) 0.996 (0.998). ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2631
https://github.com/root-project/root/pull/2631:1604,safety,test,test,1604,"root -l -b -q 'TMVAClassification.C(""BDTG"")' 4.06s user 1.01s system 147% cpu 3.423 total. root -l -b -q 'TMVARegression.C(""BDTG"")' 14.52s user 2.47s system 149% cpu 11.396 total. root -l -b -q 'TMVAMulticlass.C(""BDTG"")' 16.51s user 2.81s system 181% cpu 10.645 total. After:. root -l -b -q 'TMVAClassification.C(""BDTG"")' 3.99s user 0.97s system 149% cpu 3.310 total. root -l -b -q 'TMVARegression.C(""BDTG"")' 13.60s user 2.51s system 147% cpu 10.924 total. root -l -b -q 'TMVAMulticlass.C(""BDTG"")' 19.52s user 3.87s system 242% cpu 9.631 total. ## Output comparison. Summary: output identical before and after. ```. Classification. ==============. Top line: before. Bottom line: after. DataSet MVA Signal efficiency: from test sample (from training sample). Name: Method: @B=0.01 @B=0.10 @B=0.30. ------------------------------------------------------------------------------------------. dataset BDTG : 0.206 (0.540) 0.705 (0.858) 0.902 (0.928). dataset BDTG : 0.206 (0.540) 0.705 (0.858) 0.902 (0.928). Regression. ==========. Top line: before. Bottom line: after. DataSet Name: MVA Method: <Bias> <Bias_T> RMS RMS_T | MutInf MutInf_T. dataset BDTG : 0.198 0.165 2.60 2.00 | 3.084 3.149. dataset BDTG : 0.198 0.165 2.60 2.00 | 3.084 3.149. Multiclass. ==========. Top line: before. Bottom line: after. Dataset MVA Method ROC AUC Sig eff@B=0.01 Sig eff@B=0.10 Sig eff@B=0.30. Name: / Class: test (train) test (train) test (train) test (train). After:. dataset BDTG. ------------------------------. Signal 0.966 (0.981) 0.486 (0.659) 0.910 (0.955) 0.994 (0.996). Signal 0.966 (0.981) 0.486 (0.659) 0.910 (0.955) 0.994 (0.996). . bg0 0.905 (0.932) 0.252 (0.325) 0.716 (0.801) 0.908 (0.955). bg0 0.905 (0.932) 0.252 (0.325) 0.716 (0.801) 0.908 (0.955). . bg1 0.951 (0.955) 0.463 (0.465) 0.850 (0.845) 0.970 (0.979). bg1 0.951 (0.955) 0.463 (0.465) 0.850 (0.845) 0.970 (0.979). bg2 0.977 (0.984) 0.598 (0.709) 0.945 (0.971) 0.996 (0.998). bg2 0.977 (0.984) 0.598 (0.709) 0.945 (0.971) 0.996 (0.998). ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2631
https://github.com/root-project/root/pull/2631:1617,safety,test,test,1617,"root -l -b -q 'TMVAClassification.C(""BDTG"")' 4.06s user 1.01s system 147% cpu 3.423 total. root -l -b -q 'TMVARegression.C(""BDTG"")' 14.52s user 2.47s system 149% cpu 11.396 total. root -l -b -q 'TMVAMulticlass.C(""BDTG"")' 16.51s user 2.81s system 181% cpu 10.645 total. After:. root -l -b -q 'TMVAClassification.C(""BDTG"")' 3.99s user 0.97s system 149% cpu 3.310 total. root -l -b -q 'TMVARegression.C(""BDTG"")' 13.60s user 2.51s system 147% cpu 10.924 total. root -l -b -q 'TMVAMulticlass.C(""BDTG"")' 19.52s user 3.87s system 242% cpu 9.631 total. ## Output comparison. Summary: output identical before and after. ```. Classification. ==============. Top line: before. Bottom line: after. DataSet MVA Signal efficiency: from test sample (from training sample). Name: Method: @B=0.01 @B=0.10 @B=0.30. ------------------------------------------------------------------------------------------. dataset BDTG : 0.206 (0.540) 0.705 (0.858) 0.902 (0.928). dataset BDTG : 0.206 (0.540) 0.705 (0.858) 0.902 (0.928). Regression. ==========. Top line: before. Bottom line: after. DataSet Name: MVA Method: <Bias> <Bias_T> RMS RMS_T | MutInf MutInf_T. dataset BDTG : 0.198 0.165 2.60 2.00 | 3.084 3.149. dataset BDTG : 0.198 0.165 2.60 2.00 | 3.084 3.149. Multiclass. ==========. Top line: before. Bottom line: after. Dataset MVA Method ROC AUC Sig eff@B=0.01 Sig eff@B=0.10 Sig eff@B=0.30. Name: / Class: test (train) test (train) test (train) test (train). After:. dataset BDTG. ------------------------------. Signal 0.966 (0.981) 0.486 (0.659) 0.910 (0.955) 0.994 (0.996). Signal 0.966 (0.981) 0.486 (0.659) 0.910 (0.955) 0.994 (0.996). . bg0 0.905 (0.932) 0.252 (0.325) 0.716 (0.801) 0.908 (0.955). bg0 0.905 (0.932) 0.252 (0.325) 0.716 (0.801) 0.908 (0.955). . bg1 0.951 (0.955) 0.463 (0.465) 0.850 (0.845) 0.970 (0.979). bg1 0.951 (0.955) 0.463 (0.465) 0.850 (0.845) 0.970 (0.979). bg2 0.977 (0.984) 0.598 (0.709) 0.945 (0.971) 0.996 (0.998). bg2 0.977 (0.984) 0.598 (0.709) 0.945 (0.971) 0.996 (0.998). ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2631
https://github.com/root-project/root/pull/2631:1630,safety,test,test,1630,"root -l -b -q 'TMVAClassification.C(""BDTG"")' 4.06s user 1.01s system 147% cpu 3.423 total. root -l -b -q 'TMVARegression.C(""BDTG"")' 14.52s user 2.47s system 149% cpu 11.396 total. root -l -b -q 'TMVAMulticlass.C(""BDTG"")' 16.51s user 2.81s system 181% cpu 10.645 total. After:. root -l -b -q 'TMVAClassification.C(""BDTG"")' 3.99s user 0.97s system 149% cpu 3.310 total. root -l -b -q 'TMVARegression.C(""BDTG"")' 13.60s user 2.51s system 147% cpu 10.924 total. root -l -b -q 'TMVAMulticlass.C(""BDTG"")' 19.52s user 3.87s system 242% cpu 9.631 total. ## Output comparison. Summary: output identical before and after. ```. Classification. ==============. Top line: before. Bottom line: after. DataSet MVA Signal efficiency: from test sample (from training sample). Name: Method: @B=0.01 @B=0.10 @B=0.30. ------------------------------------------------------------------------------------------. dataset BDTG : 0.206 (0.540) 0.705 (0.858) 0.902 (0.928). dataset BDTG : 0.206 (0.540) 0.705 (0.858) 0.902 (0.928). Regression. ==========. Top line: before. Bottom line: after. DataSet Name: MVA Method: <Bias> <Bias_T> RMS RMS_T | MutInf MutInf_T. dataset BDTG : 0.198 0.165 2.60 2.00 | 3.084 3.149. dataset BDTG : 0.198 0.165 2.60 2.00 | 3.084 3.149. Multiclass. ==========. Top line: before. Bottom line: after. Dataset MVA Method ROC AUC Sig eff@B=0.01 Sig eff@B=0.10 Sig eff@B=0.30. Name: / Class: test (train) test (train) test (train) test (train). After:. dataset BDTG. ------------------------------. Signal 0.966 (0.981) 0.486 (0.659) 0.910 (0.955) 0.994 (0.996). Signal 0.966 (0.981) 0.486 (0.659) 0.910 (0.955) 0.994 (0.996). . bg0 0.905 (0.932) 0.252 (0.325) 0.716 (0.801) 0.908 (0.955). bg0 0.905 (0.932) 0.252 (0.325) 0.716 (0.801) 0.908 (0.955). . bg1 0.951 (0.955) 0.463 (0.465) 0.850 (0.845) 0.970 (0.979). bg1 0.951 (0.955) 0.463 (0.465) 0.850 (0.845) 0.970 (0.979). bg2 0.977 (0.984) 0.598 (0.709) 0.945 (0.971) 0.996 (0.998). bg2 0.977 (0.984) 0.598 (0.709) 0.945 (0.971) 0.996 (0.998). ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2631
https://github.com/root-project/root/pull/2631:782,security,ident,identical,782,"[TMVA] Further parallelisation of BDTG; ## Performance after further //-isation. Note these are small gains; Benchmarks are run on small sample sizes. Training takes ~50% of total run time. Before:. root -l -b -q 'TMVAClassification.C(""BDTG"")' 4.06s user 1.01s system 147% cpu 3.423 total. root -l -b -q 'TMVARegression.C(""BDTG"")' 14.52s user 2.47s system 149% cpu 11.396 total. root -l -b -q 'TMVAMulticlass.C(""BDTG"")' 16.51s user 2.81s system 181% cpu 10.645 total. After:. root -l -b -q 'TMVAClassification.C(""BDTG"")' 3.99s user 0.97s system 149% cpu 3.310 total. root -l -b -q 'TMVARegression.C(""BDTG"")' 13.60s user 2.51s system 147% cpu 10.924 total. root -l -b -q 'TMVAMulticlass.C(""BDTG"")' 19.52s user 3.87s system 242% cpu 9.631 total. ## Output comparison. Summary: output identical before and after. ```. Classification. ==============. Top line: before. Bottom line: after. DataSet MVA Signal efficiency: from test sample (from training sample). Name: Method: @B=0.01 @B=0.10 @B=0.30. ------------------------------------------------------------------------------------------. dataset BDTG : 0.206 (0.540) 0.705 (0.858) 0.902 (0.928). dataset BDTG : 0.206 (0.540) 0.705 (0.858) 0.902 (0.928). Regression. ==========. Top line: before. Bottom line: after. DataSet Name: MVA Method: <Bias> <Bias_T> RMS RMS_T | MutInf MutInf_T. dataset BDTG : 0.198 0.165 2.60 2.00 | 3.084 3.149. dataset BDTG : 0.198 0.165 2.60 2.00 | 3.084 3.149. Multiclass. ==========. Top line: before. Bottom line: after. Dataset MVA Method ROC AUC Sig eff@B=0.01 Sig eff@B=0.10 Sig eff@B=0.30. Name: / Class: test (train) test (train) test (train) test (train). After:. dataset BDTG. ------------------------------. Signal 0.966 (0.981) 0.486 (0.659) 0.910 (0.955) 0.994 (0.996). Signal 0.966 (0.981) 0.486 (0.659) 0.910 (0.955) 0.994 (0.996). . bg0 0.905 (0.932) 0.252 (0.325) 0.716 (0.801) 0.908 (0.955). bg0 0.905 (0.932) 0.252 (0.325) 0.716 (0.801) 0.908 (0.955). . bg1 0.951 (0.955) 0.463 (0.465) 0.850 (0.845) 0.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2631
https://github.com/root-project/root/pull/2631:897,security,Sign,Signal,897,"[TMVA] Further parallelisation of BDTG; ## Performance after further //-isation. Note these are small gains; Benchmarks are run on small sample sizes. Training takes ~50% of total run time. Before:. root -l -b -q 'TMVAClassification.C(""BDTG"")' 4.06s user 1.01s system 147% cpu 3.423 total. root -l -b -q 'TMVARegression.C(""BDTG"")' 14.52s user 2.47s system 149% cpu 11.396 total. root -l -b -q 'TMVAMulticlass.C(""BDTG"")' 16.51s user 2.81s system 181% cpu 10.645 total. After:. root -l -b -q 'TMVAClassification.C(""BDTG"")' 3.99s user 0.97s system 149% cpu 3.310 total. root -l -b -q 'TMVARegression.C(""BDTG"")' 13.60s user 2.51s system 147% cpu 10.924 total. root -l -b -q 'TMVAMulticlass.C(""BDTG"")' 19.52s user 3.87s system 242% cpu 9.631 total. ## Output comparison. Summary: output identical before and after. ```. Classification. ==============. Top line: before. Bottom line: after. DataSet MVA Signal efficiency: from test sample (from training sample). Name: Method: @B=0.01 @B=0.10 @B=0.30. ------------------------------------------------------------------------------------------. dataset BDTG : 0.206 (0.540) 0.705 (0.858) 0.902 (0.928). dataset BDTG : 0.206 (0.540) 0.705 (0.858) 0.902 (0.928). Regression. ==========. Top line: before. Bottom line: after. DataSet Name: MVA Method: <Bias> <Bias_T> RMS RMS_T | MutInf MutInf_T. dataset BDTG : 0.198 0.165 2.60 2.00 | 3.084 3.149. dataset BDTG : 0.198 0.165 2.60 2.00 | 3.084 3.149. Multiclass. ==========. Top line: before. Bottom line: after. Dataset MVA Method ROC AUC Sig eff@B=0.01 Sig eff@B=0.10 Sig eff@B=0.30. Name: / Class: test (train) test (train) test (train) test (train). After:. dataset BDTG. ------------------------------. Signal 0.966 (0.981) 0.486 (0.659) 0.910 (0.955) 0.994 (0.996). Signal 0.966 (0.981) 0.486 (0.659) 0.910 (0.955) 0.994 (0.996). . bg0 0.905 (0.932) 0.252 (0.325) 0.716 (0.801) 0.908 (0.955). bg0 0.905 (0.932) 0.252 (0.325) 0.716 (0.801) 0.908 (0.955). . bg1 0.951 (0.955) 0.463 (0.465) 0.850 (0.845) 0.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2631
https://github.com/root-project/root/pull/2631:1698,security,Sign,Signal,1698,"root -l -b -q 'TMVAClassification.C(""BDTG"")' 4.06s user 1.01s system 147% cpu 3.423 total. root -l -b -q 'TMVARegression.C(""BDTG"")' 14.52s user 2.47s system 149% cpu 11.396 total. root -l -b -q 'TMVAMulticlass.C(""BDTG"")' 16.51s user 2.81s system 181% cpu 10.645 total. After:. root -l -b -q 'TMVAClassification.C(""BDTG"")' 3.99s user 0.97s system 149% cpu 3.310 total. root -l -b -q 'TMVARegression.C(""BDTG"")' 13.60s user 2.51s system 147% cpu 10.924 total. root -l -b -q 'TMVAMulticlass.C(""BDTG"")' 19.52s user 3.87s system 242% cpu 9.631 total. ## Output comparison. Summary: output identical before and after. ```. Classification. ==============. Top line: before. Bottom line: after. DataSet MVA Signal efficiency: from test sample (from training sample). Name: Method: @B=0.01 @B=0.10 @B=0.30. ------------------------------------------------------------------------------------------. dataset BDTG : 0.206 (0.540) 0.705 (0.858) 0.902 (0.928). dataset BDTG : 0.206 (0.540) 0.705 (0.858) 0.902 (0.928). Regression. ==========. Top line: before. Bottom line: after. DataSet Name: MVA Method: <Bias> <Bias_T> RMS RMS_T | MutInf MutInf_T. dataset BDTG : 0.198 0.165 2.60 2.00 | 3.084 3.149. dataset BDTG : 0.198 0.165 2.60 2.00 | 3.084 3.149. Multiclass. ==========. Top line: before. Bottom line: after. Dataset MVA Method ROC AUC Sig eff@B=0.01 Sig eff@B=0.10 Sig eff@B=0.30. Name: / Class: test (train) test (train) test (train) test (train). After:. dataset BDTG. ------------------------------. Signal 0.966 (0.981) 0.486 (0.659) 0.910 (0.955) 0.994 (0.996). Signal 0.966 (0.981) 0.486 (0.659) 0.910 (0.955) 0.994 (0.996). . bg0 0.905 (0.932) 0.252 (0.325) 0.716 (0.801) 0.908 (0.955). bg0 0.905 (0.932) 0.252 (0.325) 0.716 (0.801) 0.908 (0.955). . bg1 0.951 (0.955) 0.463 (0.465) 0.850 (0.845) 0.970 (0.979). bg1 0.951 (0.955) 0.463 (0.465) 0.850 (0.845) 0.970 (0.979). bg2 0.977 (0.984) 0.598 (0.709) 0.945 (0.971) 0.996 (0.998). bg2 0.977 (0.984) 0.598 (0.709) 0.945 (0.971) 0.996 (0.998). ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2631
https://github.com/root-project/root/pull/2631:1762,security,Sign,Signal,1762,"root -l -b -q 'TMVAClassification.C(""BDTG"")' 4.06s user 1.01s system 147% cpu 3.423 total. root -l -b -q 'TMVARegression.C(""BDTG"")' 14.52s user 2.47s system 149% cpu 11.396 total. root -l -b -q 'TMVAMulticlass.C(""BDTG"")' 16.51s user 2.81s system 181% cpu 10.645 total. After:. root -l -b -q 'TMVAClassification.C(""BDTG"")' 3.99s user 0.97s system 149% cpu 3.310 total. root -l -b -q 'TMVARegression.C(""BDTG"")' 13.60s user 2.51s system 147% cpu 10.924 total. root -l -b -q 'TMVAMulticlass.C(""BDTG"")' 19.52s user 3.87s system 242% cpu 9.631 total. ## Output comparison. Summary: output identical before and after. ```. Classification. ==============. Top line: before. Bottom line: after. DataSet MVA Signal efficiency: from test sample (from training sample). Name: Method: @B=0.01 @B=0.10 @B=0.30. ------------------------------------------------------------------------------------------. dataset BDTG : 0.206 (0.540) 0.705 (0.858) 0.902 (0.928). dataset BDTG : 0.206 (0.540) 0.705 (0.858) 0.902 (0.928). Regression. ==========. Top line: before. Bottom line: after. DataSet Name: MVA Method: <Bias> <Bias_T> RMS RMS_T | MutInf MutInf_T. dataset BDTG : 0.198 0.165 2.60 2.00 | 3.084 3.149. dataset BDTG : 0.198 0.165 2.60 2.00 | 3.084 3.149. Multiclass. ==========. Top line: before. Bottom line: after. Dataset MVA Method ROC AUC Sig eff@B=0.01 Sig eff@B=0.10 Sig eff@B=0.30. Name: / Class: test (train) test (train) test (train) test (train). After:. dataset BDTG. ------------------------------. Signal 0.966 (0.981) 0.486 (0.659) 0.910 (0.955) 0.994 (0.996). Signal 0.966 (0.981) 0.486 (0.659) 0.910 (0.955) 0.994 (0.996). . bg0 0.905 (0.932) 0.252 (0.325) 0.716 (0.801) 0.908 (0.955). bg0 0.905 (0.932) 0.252 (0.325) 0.716 (0.801) 0.908 (0.955). . bg1 0.951 (0.955) 0.463 (0.465) 0.850 (0.845) 0.970 (0.979). bg1 0.951 (0.955) 0.463 (0.465) 0.850 (0.845) 0.970 (0.979). bg2 0.977 (0.984) 0.598 (0.709) 0.945 (0.971) 0.996 (0.998). bg2 0.977 (0.984) 0.598 (0.709) 0.945 (0.971) 0.996 (0.998). ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2631
https://github.com/root-project/root/pull/2631:921,testability,test,test,921,"[TMVA] Further parallelisation of BDTG; ## Performance after further //-isation. Note these are small gains; Benchmarks are run on small sample sizes. Training takes ~50% of total run time. Before:. root -l -b -q 'TMVAClassification.C(""BDTG"")' 4.06s user 1.01s system 147% cpu 3.423 total. root -l -b -q 'TMVARegression.C(""BDTG"")' 14.52s user 2.47s system 149% cpu 11.396 total. root -l -b -q 'TMVAMulticlass.C(""BDTG"")' 16.51s user 2.81s system 181% cpu 10.645 total. After:. root -l -b -q 'TMVAClassification.C(""BDTG"")' 3.99s user 0.97s system 149% cpu 3.310 total. root -l -b -q 'TMVARegression.C(""BDTG"")' 13.60s user 2.51s system 147% cpu 10.924 total. root -l -b -q 'TMVAMulticlass.C(""BDTG"")' 19.52s user 3.87s system 242% cpu 9.631 total. ## Output comparison. Summary: output identical before and after. ```. Classification. ==============. Top line: before. Bottom line: after. DataSet MVA Signal efficiency: from test sample (from training sample). Name: Method: @B=0.01 @B=0.10 @B=0.30. ------------------------------------------------------------------------------------------. dataset BDTG : 0.206 (0.540) 0.705 (0.858) 0.902 (0.928). dataset BDTG : 0.206 (0.540) 0.705 (0.858) 0.902 (0.928). Regression. ==========. Top line: before. Bottom line: after. DataSet Name: MVA Method: <Bias> <Bias_T> RMS RMS_T | MutInf MutInf_T. dataset BDTG : 0.198 0.165 2.60 2.00 | 3.084 3.149. dataset BDTG : 0.198 0.165 2.60 2.00 | 3.084 3.149. Multiclass. ==========. Top line: before. Bottom line: after. Dataset MVA Method ROC AUC Sig eff@B=0.01 Sig eff@B=0.10 Sig eff@B=0.30. Name: / Class: test (train) test (train) test (train) test (train). After:. dataset BDTG. ------------------------------. Signal 0.966 (0.981) 0.486 (0.659) 0.910 (0.955) 0.994 (0.996). Signal 0.966 (0.981) 0.486 (0.659) 0.910 (0.955) 0.994 (0.996). . bg0 0.905 (0.932) 0.252 (0.325) 0.716 (0.801) 0.908 (0.955). bg0 0.905 (0.932) 0.252 (0.325) 0.716 (0.801) 0.908 (0.955). . bg1 0.951 (0.955) 0.463 (0.465) 0.850 (0.845) 0.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2631
https://github.com/root-project/root/pull/2631:1204,testability,Regress,Regression,1204,"root -l -b -q 'TMVAClassification.C(""BDTG"")' 4.06s user 1.01s system 147% cpu 3.423 total. root -l -b -q 'TMVARegression.C(""BDTG"")' 14.52s user 2.47s system 149% cpu 11.396 total. root -l -b -q 'TMVAMulticlass.C(""BDTG"")' 16.51s user 2.81s system 181% cpu 10.645 total. After:. root -l -b -q 'TMVAClassification.C(""BDTG"")' 3.99s user 0.97s system 149% cpu 3.310 total. root -l -b -q 'TMVARegression.C(""BDTG"")' 13.60s user 2.51s system 147% cpu 10.924 total. root -l -b -q 'TMVAMulticlass.C(""BDTG"")' 19.52s user 3.87s system 242% cpu 9.631 total. ## Output comparison. Summary: output identical before and after. ```. Classification. ==============. Top line: before. Bottom line: after. DataSet MVA Signal efficiency: from test sample (from training sample). Name: Method: @B=0.01 @B=0.10 @B=0.30. ------------------------------------------------------------------------------------------. dataset BDTG : 0.206 (0.540) 0.705 (0.858) 0.902 (0.928). dataset BDTG : 0.206 (0.540) 0.705 (0.858) 0.902 (0.928). Regression. ==========. Top line: before. Bottom line: after. DataSet Name: MVA Method: <Bias> <Bias_T> RMS RMS_T | MutInf MutInf_T. dataset BDTG : 0.198 0.165 2.60 2.00 | 3.084 3.149. dataset BDTG : 0.198 0.165 2.60 2.00 | 3.084 3.149. Multiclass. ==========. Top line: before. Bottom line: after. Dataset MVA Method ROC AUC Sig eff@B=0.01 Sig eff@B=0.10 Sig eff@B=0.30. Name: / Class: test (train) test (train) test (train) test (train). After:. dataset BDTG. ------------------------------. Signal 0.966 (0.981) 0.486 (0.659) 0.910 (0.955) 0.994 (0.996). Signal 0.966 (0.981) 0.486 (0.659) 0.910 (0.955) 0.994 (0.996). . bg0 0.905 (0.932) 0.252 (0.325) 0.716 (0.801) 0.908 (0.955). bg0 0.905 (0.932) 0.252 (0.325) 0.716 (0.801) 0.908 (0.955). . bg1 0.951 (0.955) 0.463 (0.465) 0.850 (0.845) 0.970 (0.979). bg1 0.951 (0.955) 0.463 (0.465) 0.850 (0.845) 0.970 (0.979). bg2 0.977 (0.984) 0.598 (0.709) 0.945 (0.971) 0.996 (0.998). bg2 0.977 (0.984) 0.598 (0.709) 0.945 (0.971) 0.996 (0.998). ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2631
https://github.com/root-project/root/pull/2631:1591,testability,test,test,1591,"root -l -b -q 'TMVAClassification.C(""BDTG"")' 4.06s user 1.01s system 147% cpu 3.423 total. root -l -b -q 'TMVARegression.C(""BDTG"")' 14.52s user 2.47s system 149% cpu 11.396 total. root -l -b -q 'TMVAMulticlass.C(""BDTG"")' 16.51s user 2.81s system 181% cpu 10.645 total. After:. root -l -b -q 'TMVAClassification.C(""BDTG"")' 3.99s user 0.97s system 149% cpu 3.310 total. root -l -b -q 'TMVARegression.C(""BDTG"")' 13.60s user 2.51s system 147% cpu 10.924 total. root -l -b -q 'TMVAMulticlass.C(""BDTG"")' 19.52s user 3.87s system 242% cpu 9.631 total. ## Output comparison. Summary: output identical before and after. ```. Classification. ==============. Top line: before. Bottom line: after. DataSet MVA Signal efficiency: from test sample (from training sample). Name: Method: @B=0.01 @B=0.10 @B=0.30. ------------------------------------------------------------------------------------------. dataset BDTG : 0.206 (0.540) 0.705 (0.858) 0.902 (0.928). dataset BDTG : 0.206 (0.540) 0.705 (0.858) 0.902 (0.928). Regression. ==========. Top line: before. Bottom line: after. DataSet Name: MVA Method: <Bias> <Bias_T> RMS RMS_T | MutInf MutInf_T. dataset BDTG : 0.198 0.165 2.60 2.00 | 3.084 3.149. dataset BDTG : 0.198 0.165 2.60 2.00 | 3.084 3.149. Multiclass. ==========. Top line: before. Bottom line: after. Dataset MVA Method ROC AUC Sig eff@B=0.01 Sig eff@B=0.10 Sig eff@B=0.30. Name: / Class: test (train) test (train) test (train) test (train). After:. dataset BDTG. ------------------------------. Signal 0.966 (0.981) 0.486 (0.659) 0.910 (0.955) 0.994 (0.996). Signal 0.966 (0.981) 0.486 (0.659) 0.910 (0.955) 0.994 (0.996). . bg0 0.905 (0.932) 0.252 (0.325) 0.716 (0.801) 0.908 (0.955). bg0 0.905 (0.932) 0.252 (0.325) 0.716 (0.801) 0.908 (0.955). . bg1 0.951 (0.955) 0.463 (0.465) 0.850 (0.845) 0.970 (0.979). bg1 0.951 (0.955) 0.463 (0.465) 0.850 (0.845) 0.970 (0.979). bg2 0.977 (0.984) 0.598 (0.709) 0.945 (0.971) 0.996 (0.998). bg2 0.977 (0.984) 0.598 (0.709) 0.945 (0.971) 0.996 (0.998). ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2631
https://github.com/root-project/root/pull/2631:1604,testability,test,test,1604,"root -l -b -q 'TMVAClassification.C(""BDTG"")' 4.06s user 1.01s system 147% cpu 3.423 total. root -l -b -q 'TMVARegression.C(""BDTG"")' 14.52s user 2.47s system 149% cpu 11.396 total. root -l -b -q 'TMVAMulticlass.C(""BDTG"")' 16.51s user 2.81s system 181% cpu 10.645 total. After:. root -l -b -q 'TMVAClassification.C(""BDTG"")' 3.99s user 0.97s system 149% cpu 3.310 total. root -l -b -q 'TMVARegression.C(""BDTG"")' 13.60s user 2.51s system 147% cpu 10.924 total. root -l -b -q 'TMVAMulticlass.C(""BDTG"")' 19.52s user 3.87s system 242% cpu 9.631 total. ## Output comparison. Summary: output identical before and after. ```. Classification. ==============. Top line: before. Bottom line: after. DataSet MVA Signal efficiency: from test sample (from training sample). Name: Method: @B=0.01 @B=0.10 @B=0.30. ------------------------------------------------------------------------------------------. dataset BDTG : 0.206 (0.540) 0.705 (0.858) 0.902 (0.928). dataset BDTG : 0.206 (0.540) 0.705 (0.858) 0.902 (0.928). Regression. ==========. Top line: before. Bottom line: after. DataSet Name: MVA Method: <Bias> <Bias_T> RMS RMS_T | MutInf MutInf_T. dataset BDTG : 0.198 0.165 2.60 2.00 | 3.084 3.149. dataset BDTG : 0.198 0.165 2.60 2.00 | 3.084 3.149. Multiclass. ==========. Top line: before. Bottom line: after. Dataset MVA Method ROC AUC Sig eff@B=0.01 Sig eff@B=0.10 Sig eff@B=0.30. Name: / Class: test (train) test (train) test (train) test (train). After:. dataset BDTG. ------------------------------. Signal 0.966 (0.981) 0.486 (0.659) 0.910 (0.955) 0.994 (0.996). Signal 0.966 (0.981) 0.486 (0.659) 0.910 (0.955) 0.994 (0.996). . bg0 0.905 (0.932) 0.252 (0.325) 0.716 (0.801) 0.908 (0.955). bg0 0.905 (0.932) 0.252 (0.325) 0.716 (0.801) 0.908 (0.955). . bg1 0.951 (0.955) 0.463 (0.465) 0.850 (0.845) 0.970 (0.979). bg1 0.951 (0.955) 0.463 (0.465) 0.850 (0.845) 0.970 (0.979). bg2 0.977 (0.984) 0.598 (0.709) 0.945 (0.971) 0.996 (0.998). bg2 0.977 (0.984) 0.598 (0.709) 0.945 (0.971) 0.996 (0.998). ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2631
https://github.com/root-project/root/pull/2631:1617,testability,test,test,1617,"root -l -b -q 'TMVAClassification.C(""BDTG"")' 4.06s user 1.01s system 147% cpu 3.423 total. root -l -b -q 'TMVARegression.C(""BDTG"")' 14.52s user 2.47s system 149% cpu 11.396 total. root -l -b -q 'TMVAMulticlass.C(""BDTG"")' 16.51s user 2.81s system 181% cpu 10.645 total. After:. root -l -b -q 'TMVAClassification.C(""BDTG"")' 3.99s user 0.97s system 149% cpu 3.310 total. root -l -b -q 'TMVARegression.C(""BDTG"")' 13.60s user 2.51s system 147% cpu 10.924 total. root -l -b -q 'TMVAMulticlass.C(""BDTG"")' 19.52s user 3.87s system 242% cpu 9.631 total. ## Output comparison. Summary: output identical before and after. ```. Classification. ==============. Top line: before. Bottom line: after. DataSet MVA Signal efficiency: from test sample (from training sample). Name: Method: @B=0.01 @B=0.10 @B=0.30. ------------------------------------------------------------------------------------------. dataset BDTG : 0.206 (0.540) 0.705 (0.858) 0.902 (0.928). dataset BDTG : 0.206 (0.540) 0.705 (0.858) 0.902 (0.928). Regression. ==========. Top line: before. Bottom line: after. DataSet Name: MVA Method: <Bias> <Bias_T> RMS RMS_T | MutInf MutInf_T. dataset BDTG : 0.198 0.165 2.60 2.00 | 3.084 3.149. dataset BDTG : 0.198 0.165 2.60 2.00 | 3.084 3.149. Multiclass. ==========. Top line: before. Bottom line: after. Dataset MVA Method ROC AUC Sig eff@B=0.01 Sig eff@B=0.10 Sig eff@B=0.30. Name: / Class: test (train) test (train) test (train) test (train). After:. dataset BDTG. ------------------------------. Signal 0.966 (0.981) 0.486 (0.659) 0.910 (0.955) 0.994 (0.996). Signal 0.966 (0.981) 0.486 (0.659) 0.910 (0.955) 0.994 (0.996). . bg0 0.905 (0.932) 0.252 (0.325) 0.716 (0.801) 0.908 (0.955). bg0 0.905 (0.932) 0.252 (0.325) 0.716 (0.801) 0.908 (0.955). . bg1 0.951 (0.955) 0.463 (0.465) 0.850 (0.845) 0.970 (0.979). bg1 0.951 (0.955) 0.463 (0.465) 0.850 (0.845) 0.970 (0.979). bg2 0.977 (0.984) 0.598 (0.709) 0.945 (0.971) 0.996 (0.998). bg2 0.977 (0.984) 0.598 (0.709) 0.945 (0.971) 0.996 (0.998). ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2631
https://github.com/root-project/root/pull/2631:1630,testability,test,test,1630,"root -l -b -q 'TMVAClassification.C(""BDTG"")' 4.06s user 1.01s system 147% cpu 3.423 total. root -l -b -q 'TMVARegression.C(""BDTG"")' 14.52s user 2.47s system 149% cpu 11.396 total. root -l -b -q 'TMVAMulticlass.C(""BDTG"")' 16.51s user 2.81s system 181% cpu 10.645 total. After:. root -l -b -q 'TMVAClassification.C(""BDTG"")' 3.99s user 0.97s system 149% cpu 3.310 total. root -l -b -q 'TMVARegression.C(""BDTG"")' 13.60s user 2.51s system 147% cpu 10.924 total. root -l -b -q 'TMVAMulticlass.C(""BDTG"")' 19.52s user 3.87s system 242% cpu 9.631 total. ## Output comparison. Summary: output identical before and after. ```. Classification. ==============. Top line: before. Bottom line: after. DataSet MVA Signal efficiency: from test sample (from training sample). Name: Method: @B=0.01 @B=0.10 @B=0.30. ------------------------------------------------------------------------------------------. dataset BDTG : 0.206 (0.540) 0.705 (0.858) 0.902 (0.928). dataset BDTG : 0.206 (0.540) 0.705 (0.858) 0.902 (0.928). Regression. ==========. Top line: before. Bottom line: after. DataSet Name: MVA Method: <Bias> <Bias_T> RMS RMS_T | MutInf MutInf_T. dataset BDTG : 0.198 0.165 2.60 2.00 | 3.084 3.149. dataset BDTG : 0.198 0.165 2.60 2.00 | 3.084 3.149. Multiclass. ==========. Top line: before. Bottom line: after. Dataset MVA Method ROC AUC Sig eff@B=0.01 Sig eff@B=0.10 Sig eff@B=0.30. Name: / Class: test (train) test (train) test (train) test (train). After:. dataset BDTG. ------------------------------. Signal 0.966 (0.981) 0.486 (0.659) 0.910 (0.955) 0.994 (0.996). Signal 0.966 (0.981) 0.486 (0.659) 0.910 (0.955) 0.994 (0.996). . bg0 0.905 (0.932) 0.252 (0.325) 0.716 (0.801) 0.908 (0.955). bg0 0.905 (0.932) 0.252 (0.325) 0.716 (0.801) 0.908 (0.955). . bg1 0.951 (0.955) 0.463 (0.465) 0.850 (0.845) 0.970 (0.979). bg1 0.951 (0.955) 0.463 (0.465) 0.850 (0.845) 0.970 (0.979). bg2 0.977 (0.984) 0.598 (0.709) 0.945 (0.971) 0.996 (0.998). bg2 0.977 (0.984) 0.598 (0.709) 0.945 (0.971) 0.996 (0.998). ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2631
https://github.com/root-project/root/pull/2631:43,usability,Perform,Performance,43,"[TMVA] Further parallelisation of BDTG; ## Performance after further //-isation. Note these are small gains; Benchmarks are run on small sample sizes. Training takes ~50% of total run time. Before:. root -l -b -q 'TMVAClassification.C(""BDTG"")' 4.06s user 1.01s system 147% cpu 3.423 total. root -l -b -q 'TMVARegression.C(""BDTG"")' 14.52s user 2.47s system 149% cpu 11.396 total. root -l -b -q 'TMVAMulticlass.C(""BDTG"")' 16.51s user 2.81s system 181% cpu 10.645 total. After:. root -l -b -q 'TMVAClassification.C(""BDTG"")' 3.99s user 0.97s system 149% cpu 3.310 total. root -l -b -q 'TMVARegression.C(""BDTG"")' 13.60s user 2.51s system 147% cpu 10.924 total. root -l -b -q 'TMVAMulticlass.C(""BDTG"")' 19.52s user 3.87s system 242% cpu 9.631 total. ## Output comparison. Summary: output identical before and after. ```. Classification. ==============. Top line: before. Bottom line: after. DataSet MVA Signal efficiency: from test sample (from training sample). Name: Method: @B=0.01 @B=0.10 @B=0.30. ------------------------------------------------------------------------------------------. dataset BDTG : 0.206 (0.540) 0.705 (0.858) 0.902 (0.928). dataset BDTG : 0.206 (0.540) 0.705 (0.858) 0.902 (0.928). Regression. ==========. Top line: before. Bottom line: after. DataSet Name: MVA Method: <Bias> <Bias_T> RMS RMS_T | MutInf MutInf_T. dataset BDTG : 0.198 0.165 2.60 2.00 | 3.084 3.149. dataset BDTG : 0.198 0.165 2.60 2.00 | 3.084 3.149. Multiclass. ==========. Top line: before. Bottom line: after. Dataset MVA Method ROC AUC Sig eff@B=0.01 Sig eff@B=0.10 Sig eff@B=0.30. Name: / Class: test (train) test (train) test (train) test (train). After:. dataset BDTG. ------------------------------. Signal 0.966 (0.981) 0.486 (0.659) 0.910 (0.955) 0.994 (0.996). Signal 0.966 (0.981) 0.486 (0.659) 0.910 (0.955) 0.994 (0.996). . bg0 0.905 (0.932) 0.252 (0.325) 0.716 (0.801) 0.908 (0.955). bg0 0.905 (0.932) 0.252 (0.325) 0.716 (0.801) 0.908 (0.955). . bg1 0.951 (0.955) 0.463 (0.465) 0.850 (0.845) 0.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2631
https://github.com/root-project/root/pull/2631:250,usability,user,user,250,"[TMVA] Further parallelisation of BDTG; ## Performance after further //-isation. Note these are small gains; Benchmarks are run on small sample sizes. Training takes ~50% of total run time. Before:. root -l -b -q 'TMVAClassification.C(""BDTG"")' 4.06s user 1.01s system 147% cpu 3.423 total. root -l -b -q 'TMVARegression.C(""BDTG"")' 14.52s user 2.47s system 149% cpu 11.396 total. root -l -b -q 'TMVAMulticlass.C(""BDTG"")' 16.51s user 2.81s system 181% cpu 10.645 total. After:. root -l -b -q 'TMVAClassification.C(""BDTG"")' 3.99s user 0.97s system 149% cpu 3.310 total. root -l -b -q 'TMVARegression.C(""BDTG"")' 13.60s user 2.51s system 147% cpu 10.924 total. root -l -b -q 'TMVAMulticlass.C(""BDTG"")' 19.52s user 3.87s system 242% cpu 9.631 total. ## Output comparison. Summary: output identical before and after. ```. Classification. ==============. Top line: before. Bottom line: after. DataSet MVA Signal efficiency: from test sample (from training sample). Name: Method: @B=0.01 @B=0.10 @B=0.30. ------------------------------------------------------------------------------------------. dataset BDTG : 0.206 (0.540) 0.705 (0.858) 0.902 (0.928). dataset BDTG : 0.206 (0.540) 0.705 (0.858) 0.902 (0.928). Regression. ==========. Top line: before. Bottom line: after. DataSet Name: MVA Method: <Bias> <Bias_T> RMS RMS_T | MutInf MutInf_T. dataset BDTG : 0.198 0.165 2.60 2.00 | 3.084 3.149. dataset BDTG : 0.198 0.165 2.60 2.00 | 3.084 3.149. Multiclass. ==========. Top line: before. Bottom line: after. Dataset MVA Method ROC AUC Sig eff@B=0.01 Sig eff@B=0.10 Sig eff@B=0.30. Name: / Class: test (train) test (train) test (train) test (train). After:. dataset BDTG. ------------------------------. Signal 0.966 (0.981) 0.486 (0.659) 0.910 (0.955) 0.994 (0.996). Signal 0.966 (0.981) 0.486 (0.659) 0.910 (0.955) 0.994 (0.996). . bg0 0.905 (0.932) 0.252 (0.325) 0.716 (0.801) 0.908 (0.955). bg0 0.905 (0.932) 0.252 (0.325) 0.716 (0.801) 0.908 (0.955). . bg1 0.951 (0.955) 0.463 (0.465) 0.850 (0.845) 0.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2631
https://github.com/root-project/root/pull/2631:338,usability,user,user,338,"[TMVA] Further parallelisation of BDTG; ## Performance after further //-isation. Note these are small gains; Benchmarks are run on small sample sizes. Training takes ~50% of total run time. Before:. root -l -b -q 'TMVAClassification.C(""BDTG"")' 4.06s user 1.01s system 147% cpu 3.423 total. root -l -b -q 'TMVARegression.C(""BDTG"")' 14.52s user 2.47s system 149% cpu 11.396 total. root -l -b -q 'TMVAMulticlass.C(""BDTG"")' 16.51s user 2.81s system 181% cpu 10.645 total. After:. root -l -b -q 'TMVAClassification.C(""BDTG"")' 3.99s user 0.97s system 149% cpu 3.310 total. root -l -b -q 'TMVARegression.C(""BDTG"")' 13.60s user 2.51s system 147% cpu 10.924 total. root -l -b -q 'TMVAMulticlass.C(""BDTG"")' 19.52s user 3.87s system 242% cpu 9.631 total. ## Output comparison. Summary: output identical before and after. ```. Classification. ==============. Top line: before. Bottom line: after. DataSet MVA Signal efficiency: from test sample (from training sample). Name: Method: @B=0.01 @B=0.10 @B=0.30. ------------------------------------------------------------------------------------------. dataset BDTG : 0.206 (0.540) 0.705 (0.858) 0.902 (0.928). dataset BDTG : 0.206 (0.540) 0.705 (0.858) 0.902 (0.928). Regression. ==========. Top line: before. Bottom line: after. DataSet Name: MVA Method: <Bias> <Bias_T> RMS RMS_T | MutInf MutInf_T. dataset BDTG : 0.198 0.165 2.60 2.00 | 3.084 3.149. dataset BDTG : 0.198 0.165 2.60 2.00 | 3.084 3.149. Multiclass. ==========. Top line: before. Bottom line: after. Dataset MVA Method ROC AUC Sig eff@B=0.01 Sig eff@B=0.10 Sig eff@B=0.30. Name: / Class: test (train) test (train) test (train) test (train). After:. dataset BDTG. ------------------------------. Signal 0.966 (0.981) 0.486 (0.659) 0.910 (0.955) 0.994 (0.996). Signal 0.966 (0.981) 0.486 (0.659) 0.910 (0.955) 0.994 (0.996). . bg0 0.905 (0.932) 0.252 (0.325) 0.716 (0.801) 0.908 (0.955). bg0 0.905 (0.932) 0.252 (0.325) 0.716 (0.801) 0.908 (0.955). . bg1 0.951 (0.955) 0.463 (0.465) 0.850 (0.845) 0.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2631
https://github.com/root-project/root/pull/2631:427,usability,user,user,427,"[TMVA] Further parallelisation of BDTG; ## Performance after further //-isation. Note these are small gains; Benchmarks are run on small sample sizes. Training takes ~50% of total run time. Before:. root -l -b -q 'TMVAClassification.C(""BDTG"")' 4.06s user 1.01s system 147% cpu 3.423 total. root -l -b -q 'TMVARegression.C(""BDTG"")' 14.52s user 2.47s system 149% cpu 11.396 total. root -l -b -q 'TMVAMulticlass.C(""BDTG"")' 16.51s user 2.81s system 181% cpu 10.645 total. After:. root -l -b -q 'TMVAClassification.C(""BDTG"")' 3.99s user 0.97s system 149% cpu 3.310 total. root -l -b -q 'TMVARegression.C(""BDTG"")' 13.60s user 2.51s system 147% cpu 10.924 total. root -l -b -q 'TMVAMulticlass.C(""BDTG"")' 19.52s user 3.87s system 242% cpu 9.631 total. ## Output comparison. Summary: output identical before and after. ```. Classification. ==============. Top line: before. Bottom line: after. DataSet MVA Signal efficiency: from test sample (from training sample). Name: Method: @B=0.01 @B=0.10 @B=0.30. ------------------------------------------------------------------------------------------. dataset BDTG : 0.206 (0.540) 0.705 (0.858) 0.902 (0.928). dataset BDTG : 0.206 (0.540) 0.705 (0.858) 0.902 (0.928). Regression. ==========. Top line: before. Bottom line: after. DataSet Name: MVA Method: <Bias> <Bias_T> RMS RMS_T | MutInf MutInf_T. dataset BDTG : 0.198 0.165 2.60 2.00 | 3.084 3.149. dataset BDTG : 0.198 0.165 2.60 2.00 | 3.084 3.149. Multiclass. ==========. Top line: before. Bottom line: after. Dataset MVA Method ROC AUC Sig eff@B=0.01 Sig eff@B=0.10 Sig eff@B=0.30. Name: / Class: test (train) test (train) test (train) test (train). After:. dataset BDTG. ------------------------------. Signal 0.966 (0.981) 0.486 (0.659) 0.910 (0.955) 0.994 (0.996). Signal 0.966 (0.981) 0.486 (0.659) 0.910 (0.955) 0.994 (0.996). . bg0 0.905 (0.932) 0.252 (0.325) 0.716 (0.801) 0.908 (0.955). bg0 0.905 (0.932) 0.252 (0.325) 0.716 (0.801) 0.908 (0.955). . bg1 0.951 (0.955) 0.463 (0.465) 0.850 (0.845) 0.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2631
https://github.com/root-project/root/pull/2631:527,usability,user,user,527,"[TMVA] Further parallelisation of BDTG; ## Performance after further //-isation. Note these are small gains; Benchmarks are run on small sample sizes. Training takes ~50% of total run time. Before:. root -l -b -q 'TMVAClassification.C(""BDTG"")' 4.06s user 1.01s system 147% cpu 3.423 total. root -l -b -q 'TMVARegression.C(""BDTG"")' 14.52s user 2.47s system 149% cpu 11.396 total. root -l -b -q 'TMVAMulticlass.C(""BDTG"")' 16.51s user 2.81s system 181% cpu 10.645 total. After:. root -l -b -q 'TMVAClassification.C(""BDTG"")' 3.99s user 0.97s system 149% cpu 3.310 total. root -l -b -q 'TMVARegression.C(""BDTG"")' 13.60s user 2.51s system 147% cpu 10.924 total. root -l -b -q 'TMVAMulticlass.C(""BDTG"")' 19.52s user 3.87s system 242% cpu 9.631 total. ## Output comparison. Summary: output identical before and after. ```. Classification. ==============. Top line: before. Bottom line: after. DataSet MVA Signal efficiency: from test sample (from training sample). Name: Method: @B=0.01 @B=0.10 @B=0.30. ------------------------------------------------------------------------------------------. dataset BDTG : 0.206 (0.540) 0.705 (0.858) 0.902 (0.928). dataset BDTG : 0.206 (0.540) 0.705 (0.858) 0.902 (0.928). Regression. ==========. Top line: before. Bottom line: after. DataSet Name: MVA Method: <Bias> <Bias_T> RMS RMS_T | MutInf MutInf_T. dataset BDTG : 0.198 0.165 2.60 2.00 | 3.084 3.149. dataset BDTG : 0.198 0.165 2.60 2.00 | 3.084 3.149. Multiclass. ==========. Top line: before. Bottom line: after. Dataset MVA Method ROC AUC Sig eff@B=0.01 Sig eff@B=0.10 Sig eff@B=0.30. Name: / Class: test (train) test (train) test (train) test (train). After:. dataset BDTG. ------------------------------. Signal 0.966 (0.981) 0.486 (0.659) 0.910 (0.955) 0.994 (0.996). Signal 0.966 (0.981) 0.486 (0.659) 0.910 (0.955) 0.994 (0.996). . bg0 0.905 (0.932) 0.252 (0.325) 0.716 (0.801) 0.908 (0.955). bg0 0.905 (0.932) 0.252 (0.325) 0.716 (0.801) 0.908 (0.955). . bg1 0.951 (0.955) 0.463 (0.465) 0.850 (0.845) 0.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2631
https://github.com/root-project/root/pull/2631:615,usability,user,user,615,"[TMVA] Further parallelisation of BDTG; ## Performance after further //-isation. Note these are small gains; Benchmarks are run on small sample sizes. Training takes ~50% of total run time. Before:. root -l -b -q 'TMVAClassification.C(""BDTG"")' 4.06s user 1.01s system 147% cpu 3.423 total. root -l -b -q 'TMVARegression.C(""BDTG"")' 14.52s user 2.47s system 149% cpu 11.396 total. root -l -b -q 'TMVAMulticlass.C(""BDTG"")' 16.51s user 2.81s system 181% cpu 10.645 total. After:. root -l -b -q 'TMVAClassification.C(""BDTG"")' 3.99s user 0.97s system 149% cpu 3.310 total. root -l -b -q 'TMVARegression.C(""BDTG"")' 13.60s user 2.51s system 147% cpu 10.924 total. root -l -b -q 'TMVAMulticlass.C(""BDTG"")' 19.52s user 3.87s system 242% cpu 9.631 total. ## Output comparison. Summary: output identical before and after. ```. Classification. ==============. Top line: before. Bottom line: after. DataSet MVA Signal efficiency: from test sample (from training sample). Name: Method: @B=0.01 @B=0.10 @B=0.30. ------------------------------------------------------------------------------------------. dataset BDTG : 0.206 (0.540) 0.705 (0.858) 0.902 (0.928). dataset BDTG : 0.206 (0.540) 0.705 (0.858) 0.902 (0.928). Regression. ==========. Top line: before. Bottom line: after. DataSet Name: MVA Method: <Bias> <Bias_T> RMS RMS_T | MutInf MutInf_T. dataset BDTG : 0.198 0.165 2.60 2.00 | 3.084 3.149. dataset BDTG : 0.198 0.165 2.60 2.00 | 3.084 3.149. Multiclass. ==========. Top line: before. Bottom line: after. Dataset MVA Method ROC AUC Sig eff@B=0.01 Sig eff@B=0.10 Sig eff@B=0.30. Name: / Class: test (train) test (train) test (train) test (train). After:. dataset BDTG. ------------------------------. Signal 0.966 (0.981) 0.486 (0.659) 0.910 (0.955) 0.994 (0.996). Signal 0.966 (0.981) 0.486 (0.659) 0.910 (0.955) 0.994 (0.996). . bg0 0.905 (0.932) 0.252 (0.325) 0.716 (0.801) 0.908 (0.955). bg0 0.905 (0.932) 0.252 (0.325) 0.716 (0.801) 0.908 (0.955). . bg1 0.951 (0.955) 0.463 (0.465) 0.850 (0.845) 0.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2631
https://github.com/root-project/root/pull/2631:704,usability,user,user,704,"[TMVA] Further parallelisation of BDTG; ## Performance after further //-isation. Note these are small gains; Benchmarks are run on small sample sizes. Training takes ~50% of total run time. Before:. root -l -b -q 'TMVAClassification.C(""BDTG"")' 4.06s user 1.01s system 147% cpu 3.423 total. root -l -b -q 'TMVARegression.C(""BDTG"")' 14.52s user 2.47s system 149% cpu 11.396 total. root -l -b -q 'TMVAMulticlass.C(""BDTG"")' 16.51s user 2.81s system 181% cpu 10.645 total. After:. root -l -b -q 'TMVAClassification.C(""BDTG"")' 3.99s user 0.97s system 149% cpu 3.310 total. root -l -b -q 'TMVARegression.C(""BDTG"")' 13.60s user 2.51s system 147% cpu 10.924 total. root -l -b -q 'TMVAMulticlass.C(""BDTG"")' 19.52s user 3.87s system 242% cpu 9.631 total. ## Output comparison. Summary: output identical before and after. ```. Classification. ==============. Top line: before. Bottom line: after. DataSet MVA Signal efficiency: from test sample (from training sample). Name: Method: @B=0.01 @B=0.10 @B=0.30. ------------------------------------------------------------------------------------------. dataset BDTG : 0.206 (0.540) 0.705 (0.858) 0.902 (0.928). dataset BDTG : 0.206 (0.540) 0.705 (0.858) 0.902 (0.928). Regression. ==========. Top line: before. Bottom line: after. DataSet Name: MVA Method: <Bias> <Bias_T> RMS RMS_T | MutInf MutInf_T. dataset BDTG : 0.198 0.165 2.60 2.00 | 3.084 3.149. dataset BDTG : 0.198 0.165 2.60 2.00 | 3.084 3.149. Multiclass. ==========. Top line: before. Bottom line: after. Dataset MVA Method ROC AUC Sig eff@B=0.01 Sig eff@B=0.10 Sig eff@B=0.30. Name: / Class: test (train) test (train) test (train) test (train). After:. dataset BDTG. ------------------------------. Signal 0.966 (0.981) 0.486 (0.659) 0.910 (0.955) 0.994 (0.996). Signal 0.966 (0.981) 0.486 (0.659) 0.910 (0.955) 0.994 (0.996). . bg0 0.905 (0.932) 0.252 (0.325) 0.716 (0.801) 0.908 (0.955). bg0 0.905 (0.932) 0.252 (0.325) 0.716 (0.801) 0.908 (0.955). . bg1 0.951 (0.955) 0.463 (0.465) 0.850 (0.845) 0.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2631
https://github.com/root-project/root/pull/2631:904,usability,efficien,efficiency,904,"[TMVA] Further parallelisation of BDTG; ## Performance after further //-isation. Note these are small gains; Benchmarks are run on small sample sizes. Training takes ~50% of total run time. Before:. root -l -b -q 'TMVAClassification.C(""BDTG"")' 4.06s user 1.01s system 147% cpu 3.423 total. root -l -b -q 'TMVARegression.C(""BDTG"")' 14.52s user 2.47s system 149% cpu 11.396 total. root -l -b -q 'TMVAMulticlass.C(""BDTG"")' 16.51s user 2.81s system 181% cpu 10.645 total. After:. root -l -b -q 'TMVAClassification.C(""BDTG"")' 3.99s user 0.97s system 149% cpu 3.310 total. root -l -b -q 'TMVARegression.C(""BDTG"")' 13.60s user 2.51s system 147% cpu 10.924 total. root -l -b -q 'TMVAMulticlass.C(""BDTG"")' 19.52s user 3.87s system 242% cpu 9.631 total. ## Output comparison. Summary: output identical before and after. ```. Classification. ==============. Top line: before. Bottom line: after. DataSet MVA Signal efficiency: from test sample (from training sample). Name: Method: @B=0.01 @B=0.10 @B=0.30. ------------------------------------------------------------------------------------------. dataset BDTG : 0.206 (0.540) 0.705 (0.858) 0.902 (0.928). dataset BDTG : 0.206 (0.540) 0.705 (0.858) 0.902 (0.928). Regression. ==========. Top line: before. Bottom line: after. DataSet Name: MVA Method: <Bias> <Bias_T> RMS RMS_T | MutInf MutInf_T. dataset BDTG : 0.198 0.165 2.60 2.00 | 3.084 3.149. dataset BDTG : 0.198 0.165 2.60 2.00 | 3.084 3.149. Multiclass. ==========. Top line: before. Bottom line: after. Dataset MVA Method ROC AUC Sig eff@B=0.01 Sig eff@B=0.10 Sig eff@B=0.30. Name: / Class: test (train) test (train) test (train) test (train). After:. dataset BDTG. ------------------------------. Signal 0.966 (0.981) 0.486 (0.659) 0.910 (0.955) 0.994 (0.996). Signal 0.966 (0.981) 0.486 (0.659) 0.910 (0.955) 0.994 (0.996). . bg0 0.905 (0.932) 0.252 (0.325) 0.716 (0.801) 0.908 (0.955). bg0 0.905 (0.932) 0.252 (0.325) 0.716 (0.801) 0.908 (0.955). . bg1 0.951 (0.955) 0.463 (0.465) 0.850 (0.845) 0.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2631
https://github.com/root-project/root/pull/2632:108,deployability,depend,dependencies,108,Implement built-in chunking in TThreadExecutor::Foreach and apply it in BDT-LossFunction; Also fix TMVA-IMT dependencies and improve some of the memory allocations in LossFunction.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2632
https://github.com/root-project/root/pull/2632:152,energy efficiency,alloc,allocations,152,Implement built-in chunking in TThreadExecutor::Foreach and apply it in BDT-LossFunction; Also fix TMVA-IMT dependencies and improve some of the memory allocations in LossFunction.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2632
https://github.com/root-project/root/pull/2632:108,integrability,depend,dependencies,108,Implement built-in chunking in TThreadExecutor::Foreach and apply it in BDT-LossFunction; Also fix TMVA-IMT dependencies and improve some of the memory allocations in LossFunction.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2632
https://github.com/root-project/root/pull/2632:108,modifiability,depend,dependencies,108,Implement built-in chunking in TThreadExecutor::Foreach and apply it in BDT-LossFunction; Also fix TMVA-IMT dependencies and improve some of the memory allocations in LossFunction.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2632
https://github.com/root-project/root/pull/2632:145,performance,memor,memory,145,Implement built-in chunking in TThreadExecutor::Foreach and apply it in BDT-LossFunction; Also fix TMVA-IMT dependencies and improve some of the memory allocations in LossFunction.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2632
https://github.com/root-project/root/pull/2632:108,safety,depend,dependencies,108,Implement built-in chunking in TThreadExecutor::Foreach and apply it in BDT-LossFunction; Also fix TMVA-IMT dependencies and improve some of the memory allocations in LossFunction.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2632
https://github.com/root-project/root/pull/2632:76,security,Loss,LossFunction,76,Implement built-in chunking in TThreadExecutor::Foreach and apply it in BDT-LossFunction; Also fix TMVA-IMT dependencies and improve some of the memory allocations in LossFunction.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2632
https://github.com/root-project/root/pull/2632:167,security,Loss,LossFunction,167,Implement built-in chunking in TThreadExecutor::Foreach and apply it in BDT-LossFunction; Also fix TMVA-IMT dependencies and improve some of the memory allocations in LossFunction.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2632
https://github.com/root-project/root/pull/2632:108,testability,depend,dependencies,108,Implement built-in chunking in TThreadExecutor::Foreach and apply it in BDT-LossFunction; Also fix TMVA-IMT dependencies and improve some of the memory allocations in LossFunction.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2632
https://github.com/root-project/root/pull/2632:145,usability,memor,memory,145,Implement built-in chunking in TThreadExecutor::Foreach and apply it in BDT-LossFunction; Also fix TMVA-IMT dependencies and improve some of the memory allocations in LossFunction.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2632
https://github.com/root-project/root/pull/2633:28,deployability,version,version,28,[PyROOT] Use GetLength from version 3.3 and onwards; GetSize is deprecated from 3.3. (https://docs.python.org/3/c-api/unicode.html#c.PyUnicode_GetSize) and. the new recommendation is to use PyUnicode_GetLength.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2633
https://github.com/root-project/root/pull/2633:114,deployability,api,api,114,[PyROOT] Use GetLength from version 3.3 and onwards; GetSize is deprecated from 3.3. (https://docs.python.org/3/c-api/unicode.html#c.PyUnicode_GetSize) and. the new recommendation is to use PyUnicode_GetLength.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2633
https://github.com/root-project/root/pull/2633:28,integrability,version,version,28,[PyROOT] Use GetLength from version 3.3 and onwards; GetSize is deprecated from 3.3. (https://docs.python.org/3/c-api/unicode.html#c.PyUnicode_GetSize) and. the new recommendation is to use PyUnicode_GetLength.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2633
https://github.com/root-project/root/pull/2633:114,integrability,api,api,114,[PyROOT] Use GetLength from version 3.3 and onwards; GetSize is deprecated from 3.3. (https://docs.python.org/3/c-api/unicode.html#c.PyUnicode_GetSize) and. the new recommendation is to use PyUnicode_GetLength.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2633
https://github.com/root-project/root/pull/2633:114,interoperability,api,api,114,[PyROOT] Use GetLength from version 3.3 and onwards; GetSize is deprecated from 3.3. (https://docs.python.org/3/c-api/unicode.html#c.PyUnicode_GetSize) and. the new recommendation is to use PyUnicode_GetLength.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2633
https://github.com/root-project/root/pull/2633:28,modifiability,version,version,28,[PyROOT] Use GetLength from version 3.3 and onwards; GetSize is deprecated from 3.3. (https://docs.python.org/3/c-api/unicode.html#c.PyUnicode_GetSize) and. the new recommendation is to use PyUnicode_GetLength.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2633
https://github.com/root-project/root/pull/2635:111,availability,consist,consisted,111,[TMVA] CrossValidation multiproc fix; Ensures that data is actually read from the TTree. Silly mistake that is consisted with the problems we've seen.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2635
https://github.com/root-project/root/pull/2635:111,usability,consist,consisted,111,[TMVA] CrossValidation multiproc fix; Ensures that data is actually read from the TTree. Silly mistake that is consisted with the problems we've seen.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2635
https://github.com/root-project/root/pull/2637:140,interoperability,XML,XML-CHAR-,140,Removing color output for Clang compiler; It triggers wrong behaviour of Jenkins parser together with CDash adding ANSI escape symbols: NON-XML-CHAR-0x1B][0m[NON-XML-CHAR-0x1B],MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2637
https://github.com/root-project/root/pull/2637:162,interoperability,XML,XML-CHAR-,162,Removing color output for Clang compiler; It triggers wrong behaviour of Jenkins parser together with CDash adding ANSI escape symbols: NON-XML-CHAR-0x1B][0m[NON-XML-CHAR-0x1B],MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2637
https://github.com/root-project/root/pull/2637:60,usability,behavi,behaviour,60,Removing color output for Clang compiler; It triggers wrong behaviour of Jenkins parser together with CDash adding ANSI escape symbols: NON-XML-CHAR-0x1B][0m[NON-XML-CHAR-0x1B],MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2637
https://github.com/root-project/root/pull/2640:20,integrability,Interfac,Interface,20,Use proper printing Interface in RVec.hxx; printValue is not expected to be used this way. Please use. gInterpreter->Evaluate for printing purposes.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2640
https://github.com/root-project/root/pull/2640:20,interoperability,Interfac,Interface,20,Use proper printing Interface in RVec.hxx; printValue is not expected to be used this way. Please use. gInterpreter->Evaluate for printing purposes.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2640
https://github.com/root-project/root/pull/2640:20,modifiability,Interfac,Interface,20,Use proper printing Interface in RVec.hxx; printValue is not expected to be used this way. Please use. gInterpreter->Evaluate for printing purposes.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2640
https://github.com/root-project/root/pull/2641:0,deployability,Updat,Update,0,Update builtin version of VecCore to 0.5.0;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2641
https://github.com/root-project/root/pull/2641:15,deployability,version,version,15,Update builtin version of VecCore to 0.5.0;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2641
https://github.com/root-project/root/pull/2641:15,integrability,version,version,15,Update builtin version of VecCore to 0.5.0;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2641
https://github.com/root-project/root/pull/2641:15,modifiability,version,version,15,Update builtin version of VecCore to 0.5.0;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2641
https://github.com/root-project/root/pull/2641:0,safety,Updat,Update,0,Update builtin version of VecCore to 0.5.0;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2641
https://github.com/root-project/root/pull/2641:0,security,Updat,Update,0,Update builtin version of VecCore to 0.5.0;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2641
https://github.com/root-project/root/pull/2642:5,safety,Test,Test,5,[DF] Test cast to RNode of RDF with datasource;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2642
https://github.com/root-project/root/pull/2642:5,testability,Test,Test,5,[DF] Test cast to RNode of RDF with datasource;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2642
https://github.com/root-project/root/pull/2643:49,deployability,Updat,Updating,49,Clarifying documentation about ccache and clang; Updating comment about bug in ccache 3.1.x used together with clang (https://bugzilla.samba.org/show_bug.cgi?id=8118). In commit was added the detection of version of ccache and extra check to apply flag -Qunused-argument for ccache version less then 3.2.0.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2643
https://github.com/root-project/root/pull/2643:205,deployability,version,version,205,Clarifying documentation about ccache and clang; Updating comment about bug in ccache 3.1.x used together with clang (https://bugzilla.samba.org/show_bug.cgi?id=8118). In commit was added the detection of version of ccache and extra check to apply flag -Qunused-argument for ccache version less then 3.2.0.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2643
https://github.com/root-project/root/pull/2643:282,deployability,version,version,282,Clarifying documentation about ccache and clang; Updating comment about bug in ccache 3.1.x used together with clang (https://bugzilla.samba.org/show_bug.cgi?id=8118). In commit was added the detection of version of ccache and extra check to apply flag -Qunused-argument for ccache version less then 3.2.0.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2643
https://github.com/root-project/root/pull/2643:205,integrability,version,version,205,Clarifying documentation about ccache and clang; Updating comment about bug in ccache 3.1.x used together with clang (https://bugzilla.samba.org/show_bug.cgi?id=8118). In commit was added the detection of version of ccache and extra check to apply flag -Qunused-argument for ccache version less then 3.2.0.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2643
https://github.com/root-project/root/pull/2643:282,integrability,version,version,282,Clarifying documentation about ccache and clang; Updating comment about bug in ccache 3.1.x used together with clang (https://bugzilla.samba.org/show_bug.cgi?id=8118). In commit was added the detection of version of ccache and extra check to apply flag -Qunused-argument for ccache version less then 3.2.0.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2643
https://github.com/root-project/root/pull/2643:205,modifiability,version,version,205,Clarifying documentation about ccache and clang; Updating comment about bug in ccache 3.1.x used together with clang (https://bugzilla.samba.org/show_bug.cgi?id=8118). In commit was added the detection of version of ccache and extra check to apply flag -Qunused-argument for ccache version less then 3.2.0.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2643
https://github.com/root-project/root/pull/2643:282,modifiability,version,version,282,Clarifying documentation about ccache and clang; Updating comment about bug in ccache 3.1.x used together with clang (https://bugzilla.samba.org/show_bug.cgi?id=8118). In commit was added the detection of version of ccache and extra check to apply flag -Qunused-argument for ccache version less then 3.2.0.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2643
https://github.com/root-project/root/pull/2643:49,safety,Updat,Updating,49,Clarifying documentation about ccache and clang; Updating comment about bug in ccache 3.1.x used together with clang (https://bugzilla.samba.org/show_bug.cgi?id=8118). In commit was added the detection of version of ccache and extra check to apply flag -Qunused-argument for ccache version less then 3.2.0.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2643
https://github.com/root-project/root/pull/2643:192,safety,detect,detection,192,Clarifying documentation about ccache and clang; Updating comment about bug in ccache 3.1.x used together with clang (https://bugzilla.samba.org/show_bug.cgi?id=8118). In commit was added the detection of version of ccache and extra check to apply flag -Qunused-argument for ccache version less then 3.2.0.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2643
https://github.com/root-project/root/pull/2643:49,security,Updat,Updating,49,Clarifying documentation about ccache and clang; Updating comment about bug in ccache 3.1.x used together with clang (https://bugzilla.samba.org/show_bug.cgi?id=8118). In commit was added the detection of version of ccache and extra check to apply flag -Qunused-argument for ccache version less then 3.2.0.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2643
https://github.com/root-project/root/pull/2643:192,security,detect,detection,192,Clarifying documentation about ccache and clang; Updating comment about bug in ccache 3.1.x used together with clang (https://bugzilla.samba.org/show_bug.cgi?id=8118). In commit was added the detection of version of ccache and extra check to apply flag -Qunused-argument for ccache version less then 3.2.0.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2643
https://github.com/root-project/root/pull/2643:11,usability,document,documentation,11,Clarifying documentation about ccache and clang; Updating comment about bug in ccache 3.1.x used together with clang (https://bugzilla.samba.org/show_bug.cgi?id=8118). In commit was added the detection of version of ccache and extra check to apply flag -Qunused-argument for ccache version less then 3.2.0.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2643
https://github.com/root-project/root/pull/2644:20,integrability,interfac,interface,20,Use proper printing interface in RDataFrame; gInterpreter->Evaluate is a proper interface for printing purposes.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2644
https://github.com/root-project/root/pull/2644:80,integrability,interfac,interface,80,Use proper printing interface in RDataFrame; gInterpreter->Evaluate is a proper interface for printing purposes.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2644
https://github.com/root-project/root/pull/2644:20,interoperability,interfac,interface,20,Use proper printing interface in RDataFrame; gInterpreter->Evaluate is a proper interface for printing purposes.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2644
https://github.com/root-project/root/pull/2644:80,interoperability,interfac,interface,80,Use proper printing interface in RDataFrame; gInterpreter->Evaluate is a proper interface for printing purposes.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2644
https://github.com/root-project/root/pull/2644:20,modifiability,interfac,interface,20,Use proper printing interface in RDataFrame; gInterpreter->Evaluate is a proper interface for printing purposes.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2644
https://github.com/root-project/root/pull/2644:80,modifiability,interfac,interface,80,Use proper printing interface in RDataFrame; gInterpreter->Evaluate is a proper interface for printing purposes.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2644
https://github.com/root-project/root/pull/2645:40,deployability,contain,contains,40,"[TreePlayer] ROOT-9312: Fix; If a TTree contains. a branch ""v"" with a leaf ""a"", created with t.Branch(""v"", &a, ""a/I""). a branch ""w"" containing a split struct that has a datamember ""v"" with a datamember ""a"". constructing TTreeReaderValue<int>(r, ""v.a"") attaches the reader to ""w.v.a"" instead of ""v.a"". Note that in this scenario there is no way to access ""v.a"", because constructing TTreeReaderValue<int>(r, ""v"") attaches to ""w.v""",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2645
https://github.com/root-project/root/pull/2645:132,deployability,contain,containing,132,"[TreePlayer] ROOT-9312: Fix; If a TTree contains. a branch ""v"" with a leaf ""a"", created with t.Branch(""v"", &a, ""a/I""). a branch ""w"" containing a split struct that has a datamember ""v"" with a datamember ""a"". constructing TTreeReaderValue<int>(r, ""v.a"") attaches the reader to ""w.v.a"" instead of ""v.a"". Note that in this scenario there is no way to access ""v.a"", because constructing TTreeReaderValue<int>(r, ""v"") attaches to ""w.v""",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2645
https://github.com/root-project/root/pull/2645:319,modifiability,scenario,scenario,319,"[TreePlayer] ROOT-9312: Fix; If a TTree contains. a branch ""v"" with a leaf ""a"", created with t.Branch(""v"", &a, ""a/I""). a branch ""w"" containing a split struct that has a datamember ""v"" with a datamember ""a"". constructing TTreeReaderValue<int>(r, ""v.a"") attaches the reader to ""w.v.a"" instead of ""v.a"". Note that in this scenario there is no way to access ""v.a"", because constructing TTreeReaderValue<int>(r, ""v"") attaches to ""w.v""",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2645
https://github.com/root-project/root/pull/2645:347,security,access,access,347,"[TreePlayer] ROOT-9312: Fix; If a TTree contains. a branch ""v"" with a leaf ""a"", created with t.Branch(""v"", &a, ""a/I""). a branch ""w"" containing a split struct that has a datamember ""v"" with a datamember ""a"". constructing TTreeReaderValue<int>(r, ""v.a"") attaches the reader to ""w.v.a"" instead of ""v.a"". Note that in this scenario there is no way to access ""v.a"", because constructing TTreeReaderValue<int>(r, ""v"") attaches to ""w.v""",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2645
https://github.com/root-project/root/pull/2647:7,deployability,version,version,7,root --version improvements;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2647
https://github.com/root-project/root/pull/2647:7,integrability,version,version,7,root --version improvements;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2647
https://github.com/root-project/root/pull/2647:7,modifiability,version,version,7,root --version improvements;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2647
https://github.com/root-project/root/pull/2648:44,availability,error,error,44,webgui: small fixes in CanvasPainter; * Fix error with too fast update of canvas before previous is completed. * Use better methods to work with STL containers like std::find_if or std::remove_if.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2648
https://github.com/root-project/root/pull/2648:64,deployability,updat,update,64,webgui: small fixes in CanvasPainter; * Fix error with too fast update of canvas before previous is completed. * Use better methods to work with STL containers like std::find_if or std::remove_if.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2648
https://github.com/root-project/root/pull/2648:149,deployability,contain,containers,149,webgui: small fixes in CanvasPainter; * Fix error with too fast update of canvas before previous is completed. * Use better methods to work with STL containers like std::find_if or std::remove_if.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2648
https://github.com/root-project/root/pull/2648:44,performance,error,error,44,webgui: small fixes in CanvasPainter; * Fix error with too fast update of canvas before previous is completed. * Use better methods to work with STL containers like std::find_if or std::remove_if.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2648
https://github.com/root-project/root/pull/2648:44,safety,error,error,44,webgui: small fixes in CanvasPainter; * Fix error with too fast update of canvas before previous is completed. * Use better methods to work with STL containers like std::find_if or std::remove_if.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2648
https://github.com/root-project/root/pull/2648:64,safety,updat,update,64,webgui: small fixes in CanvasPainter; * Fix error with too fast update of canvas before previous is completed. * Use better methods to work with STL containers like std::find_if or std::remove_if.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2648
https://github.com/root-project/root/pull/2648:100,safety,compl,completed,100,webgui: small fixes in CanvasPainter; * Fix error with too fast update of canvas before previous is completed. * Use better methods to work with STL containers like std::find_if or std::remove_if.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2648
https://github.com/root-project/root/pull/2648:64,security,updat,update,64,webgui: small fixes in CanvasPainter; * Fix error with too fast update of canvas before previous is completed. * Use better methods to work with STL containers like std::find_if or std::remove_if.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2648
https://github.com/root-project/root/pull/2648:100,security,compl,completed,100,webgui: small fixes in CanvasPainter; * Fix error with too fast update of canvas before previous is completed. * Use better methods to work with STL containers like std::find_if or std::remove_if.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2648
https://github.com/root-project/root/pull/2648:44,usability,error,error,44,webgui: small fixes in CanvasPainter; * Fix error with too fast update of canvas before previous is completed. * Use better methods to work with STL containers like std::find_if or std::remove_if.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2648
https://github.com/root-project/root/pull/2649:40,deployability,patch,patches,40,http: latest civetweb 1.11 plus OpenSSL patches; Should solve different compilation warnings. Use directly version from head - not waiting for next release which typically happens once a year. Version 1.11 was released 10 September 2018 (10 days ago),MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2649
https://github.com/root-project/root/pull/2649:107,deployability,version,version,107,http: latest civetweb 1.11 plus OpenSSL patches; Should solve different compilation warnings. Use directly version from head - not waiting for next release which typically happens once a year. Version 1.11 was released 10 September 2018 (10 days ago),MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2649
https://github.com/root-project/root/pull/2649:148,deployability,releas,release,148,http: latest civetweb 1.11 plus OpenSSL patches; Should solve different compilation warnings. Use directly version from head - not waiting for next release which typically happens once a year. Version 1.11 was released 10 September 2018 (10 days ago),MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2649
https://github.com/root-project/root/pull/2649:193,deployability,Version,Version,193,http: latest civetweb 1.11 plus OpenSSL patches; Should solve different compilation warnings. Use directly version from head - not waiting for next release which typically happens once a year. Version 1.11 was released 10 September 2018 (10 days ago),MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2649
https://github.com/root-project/root/pull/2649:210,deployability,releas,released,210,http: latest civetweb 1.11 plus OpenSSL patches; Should solve different compilation warnings. Use directly version from head - not waiting for next release which typically happens once a year. Version 1.11 was released 10 September 2018 (10 days ago),MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2649
https://github.com/root-project/root/pull/2649:107,integrability,version,version,107,http: latest civetweb 1.11 plus OpenSSL patches; Should solve different compilation warnings. Use directly version from head - not waiting for next release which typically happens once a year. Version 1.11 was released 10 September 2018 (10 days ago),MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2649
https://github.com/root-project/root/pull/2649:193,integrability,Version,Version,193,http: latest civetweb 1.11 plus OpenSSL patches; Should solve different compilation warnings. Use directly version from head - not waiting for next release which typically happens once a year. Version 1.11 was released 10 September 2018 (10 days ago),MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2649
https://github.com/root-project/root/pull/2649:107,modifiability,version,version,107,http: latest civetweb 1.11 plus OpenSSL patches; Should solve different compilation warnings. Use directly version from head - not waiting for next release which typically happens once a year. Version 1.11 was released 10 September 2018 (10 days ago),MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2649
https://github.com/root-project/root/pull/2649:193,modifiability,Version,Version,193,http: latest civetweb 1.11 plus OpenSSL patches; Should solve different compilation warnings. Use directly version from head - not waiting for next release which typically happens once a year. Version 1.11 was released 10 September 2018 (10 days ago),MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2649
https://github.com/root-project/root/pull/2649:40,safety,patch,patches,40,http: latest civetweb 1.11 plus OpenSSL patches; Should solve different compilation warnings. Use directly version from head - not waiting for next release which typically happens once a year. Version 1.11 was released 10 September 2018 (10 days ago),MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2649
https://github.com/root-project/root/pull/2649:40,security,patch,patches,40,http: latest civetweb 1.11 plus OpenSSL patches; Should solve different compilation warnings. Use directly version from head - not waiting for next release which typically happens once a year. Version 1.11 was released 10 September 2018 (10 days ago),MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2649
https://github.com/root-project/root/pull/2650:62,deployability,stack,stack,62,[Math] Remove memory leaks in TFormula using variables on the stack; This is a better version that #2622 which removes entirely the need to use new and delete...,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2650
https://github.com/root-project/root/pull/2650:86,deployability,version,version,86,[Math] Remove memory leaks in TFormula using variables on the stack; This is a better version that #2622 which removes entirely the need to use new and delete...,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2650
https://github.com/root-project/root/pull/2650:86,integrability,version,version,86,[Math] Remove memory leaks in TFormula using variables on the stack; This is a better version that #2622 which removes entirely the need to use new and delete...,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2650
https://github.com/root-project/root/pull/2650:45,modifiability,variab,variables,45,[Math] Remove memory leaks in TFormula using variables on the stack; This is a better version that #2622 which removes entirely the need to use new and delete...,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2650
https://github.com/root-project/root/pull/2650:86,modifiability,version,version,86,[Math] Remove memory leaks in TFormula using variables on the stack; This is a better version that #2622 which removes entirely the need to use new and delete...,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2650
https://github.com/root-project/root/pull/2650:14,performance,memor,memory,14,[Math] Remove memory leaks in TFormula using variables on the stack; This is a better version that #2622 which removes entirely the need to use new and delete...,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2650
https://github.com/root-project/root/pull/2650:14,usability,memor,memory,14,[Math] Remove memory leaks in TFormula using variables on the stack; This is a better version that #2622 which removes entirely the need to use new and delete...,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2650
https://github.com/root-project/root/pull/2651:47,integrability,compon,components,47,Remove globbing for headers&sources in ROOT IO components;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2651
https://github.com/root-project/root/pull/2651:47,interoperability,compon,components,47,Remove globbing for headers&sources in ROOT IO components;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2651
https://github.com/root-project/root/pull/2651:47,modifiability,compon,components,47,Remove globbing for headers&sources in ROOT IO components;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2651
https://github.com/root-project/root/pull/2652:223,energy efficiency,current,current,223,"Introduce template to correctly handle each kind of global function; Try to keep old interface as is - if by chance anybody uses it outside ROOT. In any case, one **MUST** fix problem with TVirtualGL.cxx. In original code (current master) not a pointer on the function but double pointer on the variable was provided - code was never working before",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2652
https://github.com/root-project/root/pull/2652:85,integrability,interfac,interface,85,"Introduce template to correctly handle each kind of global function; Try to keep old interface as is - if by chance anybody uses it outside ROOT. In any case, one **MUST** fix problem with TVirtualGL.cxx. In original code (current master) not a pointer on the function but double pointer on the variable was provided - code was never working before",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2652
https://github.com/root-project/root/pull/2652:85,interoperability,interfac,interface,85,"Introduce template to correctly handle each kind of global function; Try to keep old interface as is - if by chance anybody uses it outside ROOT. In any case, one **MUST** fix problem with TVirtualGL.cxx. In original code (current master) not a pointer on the function but double pointer on the variable was provided - code was never working before",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2652
https://github.com/root-project/root/pull/2652:85,modifiability,interfac,interface,85,"Introduce template to correctly handle each kind of global function; Try to keep old interface as is - if by chance anybody uses it outside ROOT. In any case, one **MUST** fix problem with TVirtualGL.cxx. In original code (current master) not a pointer on the function but double pointer on the variable was provided - code was never working before",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2652
https://github.com/root-project/root/pull/2652:295,modifiability,variab,variable,295,"Introduce template to correctly handle each kind of global function; Try to keep old interface as is - if by chance anybody uses it outside ROOT. In any case, one **MUST** fix problem with TVirtualGL.cxx. In original code (current master) not a pointer on the function but double pointer on the variable was provided - code was never working before",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2652
https://github.com/root-project/root/pull/2653:31,deployability,instal,install,31,[ROOT-9489] Exclude Clang from install directive when using shared LLVM; exclude clang from install directive. Related JIRA issue: https://sft.its.cern.ch/jira/browse/ROOT-9489,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2653
https://github.com/root-project/root/pull/2653:92,deployability,instal,install,92,[ROOT-9489] Exclude Clang from install directive when using shared LLVM; exclude clang from install directive. Related JIRA issue: https://sft.its.cern.ch/jira/browse/ROOT-9489,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2653
https://github.com/root-project/root/pull/2653:60,interoperability,share,shared,60,[ROOT-9489] Exclude Clang from install directive when using shared LLVM; exclude clang from install directive. Related JIRA issue: https://sft.its.cern.ch/jira/browse/ROOT-9489,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2653
https://github.com/root-project/root/pull/2655:39,deployability,updat,update,39,"webgui: simply ignore Show() in batch, update most of tutorials;",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2655
https://github.com/root-project/root/pull/2655:32,integrability,batch,batch,32,"webgui: simply ignore Show() in batch, update most of tutorials;",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2655
https://github.com/root-project/root/pull/2655:32,performance,batch,batch,32,"webgui: simply ignore Show() in batch, update most of tutorials;",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2655
https://github.com/root-project/root/pull/2655:39,safety,updat,update,39,"webgui: simply ignore Show() in batch, update most of tutorials;",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2655
https://github.com/root-project/root/pull/2655:39,security,updat,update,39,"webgui: simply ignore Show() in batch, update most of tutorials;",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2655
https://github.com/root-project/root/pull/2655:8,testability,simpl,simply,8,"webgui: simply ignore Show() in batch, update most of tutorials;",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2655
https://github.com/root-project/root/pull/2655:8,usability,simpl,simply,8,"webgui: simply ignore Show() in batch, update most of tutorials;",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2655
https://github.com/root-project/root/pull/2656:222,interoperability,prox,proxy,222,"[ROOT-9689] Pythonization for TChain::SetBranchAddress; In an analogous way to what has been implemented for TTree, pythonize the TChain::SetBranchAddress method to be able to pass as parameter the reference to the Python proxy object, in particular when calling the `(const char*, void*)` overload. This helps in cases when the C++ implementation of SetBranchAddress expects the address of a pointer, e.g. when the branch is an std::vector. Therefore, this Pythonization will allow to do:. ```python. v = ROOT.std.vector('int')(). c.SetBranchAddress(""my_vector_branch"", v). ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2656
https://github.com/root-project/root/pull/2656:184,modifiability,paramet,parameter,184,"[ROOT-9689] Pythonization for TChain::SetBranchAddress; In an analogous way to what has been implemented for TTree, pythonize the TChain::SetBranchAddress method to be able to pass as parameter the reference to the Python proxy object, in particular when calling the `(const char*, void*)` overload. This helps in cases when the C++ implementation of SetBranchAddress expects the address of a pointer, e.g. when the branch is an std::vector. Therefore, this Pythonization will allow to do:. ```python. v = ROOT.std.vector('int')(). c.SetBranchAddress(""my_vector_branch"", v). ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2656
https://github.com/root-project/root/pull/2656:305,usability,help,helps,305,"[ROOT-9689] Pythonization for TChain::SetBranchAddress; In an analogous way to what has been implemented for TTree, pythonize the TChain::SetBranchAddress method to be able to pass as parameter the reference to the Python proxy object, in particular when calling the `(const char*, void*)` overload. This helps in cases when the C++ implementation of SetBranchAddress expects the address of a pointer, e.g. when the branch is an std::vector. Therefore, this Pythonization will allow to do:. ```python. v = ROOT.std.vector('int')(). c.SetBranchAddress(""my_vector_branch"", v). ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2656
https://github.com/root-project/root/pull/2657:118,security,sign,signals,118,Fix Jira issue #ROOT-9640 Segmentation violation after starting TBrow…; …ser b. Only connect/disconnect TGHtmlBrowser signals when it is present,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2657
https://github.com/root-project/root/pull/2658:68,integrability,rout,routines,68,[DF] Support rdfslot_ and rdfentry_ as well as avoid duplication of routines to check implicit columns;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2658
https://github.com/root-project/root/pull/2658:47,safety,avoid,avoid,47,[DF] Support rdfslot_ and rdfentry_ as well as avoid duplication of routines to check implicit columns;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2658
https://github.com/root-project/root/pull/2658:5,usability,Support,Support,5,[DF] Support rdfslot_ and rdfentry_ as well as avoid duplication of routines to check implicit columns;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2658
https://github.com/root-project/root/pull/2659:48,modifiability,variab,variable,48,v6.14 fix wrong cast - instead function pointer variable pointer was provided;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2659
https://github.com/root-project/root/pull/2660:66,energy efficiency,profil,profile,66,"webgui: fix firefox problem in batch mode; One requires temporary profile for firefox. To create it, one should run firefox -CreateProfile. When doing this in batch, one also should specify -headless as argument",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2660
https://github.com/root-project/root/pull/2660:31,integrability,batch,batch,31,"webgui: fix firefox problem in batch mode; One requires temporary profile for firefox. To create it, one should run firefox -CreateProfile. When doing this in batch, one also should specify -headless as argument",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2660
https://github.com/root-project/root/pull/2660:159,integrability,batch,batch,159,"webgui: fix firefox problem in batch mode; One requires temporary profile for firefox. To create it, one should run firefox -CreateProfile. When doing this in batch, one also should specify -headless as argument",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2660
https://github.com/root-project/root/pull/2660:182,interoperability,specif,specify,182,"webgui: fix firefox problem in batch mode; One requires temporary profile for firefox. To create it, one should run firefox -CreateProfile. When doing this in batch, one also should specify -headless as argument",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2660
https://github.com/root-project/root/pull/2660:31,performance,batch,batch,31,"webgui: fix firefox problem in batch mode; One requires temporary profile for firefox. To create it, one should run firefox -CreateProfile. When doing this in batch, one also should specify -headless as argument",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2660
https://github.com/root-project/root/pull/2660:66,performance,profil,profile,66,"webgui: fix firefox problem in batch mode; One requires temporary profile for firefox. To create it, one should run firefox -CreateProfile. When doing this in batch, one also should specify -headless as argument",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2660
https://github.com/root-project/root/pull/2660:159,performance,batch,batch,159,"webgui: fix firefox problem in batch mode; One requires temporary profile for firefox. To create it, one should run firefox -CreateProfile. When doing this in batch, one also should specify -headless as argument",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2660
https://github.com/root-project/root/pull/2661:13,performance,time,time,13,"http: -Wdate-time warning appeared first in gcc5, not exists before;",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2661
https://github.com/root-project/root/pull/2662:38,safety,avoid,avoid,38,Fixed typedef names in TMCAutolock to avoid name clash with Geant4 :; - this should address the ROOT-9681 JIRA item,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2662
https://github.com/root-project/root/pull/2664:216,deployability,patch,patch,216,"Allow calling printValue() directly in ROOT; As we discussed in #2644, the nicest interface for printing is. printValue, and we should support people using this rather than going. through cling::Value::print(). This patch contains:. - Implementation of declarePrintValue. - Re-Implementation of ClingPrintValue because I changed to use Evaluate. some time ago. - removing of RVec version of printValue which wasn't used at all. - Fix test/vecops_rvec.cxx, printValue is never supposed to be called by. a normal user.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2664
https://github.com/root-project/root/pull/2664:222,deployability,contain,contains,222,"Allow calling printValue() directly in ROOT; As we discussed in #2644, the nicest interface for printing is. printValue, and we should support people using this rather than going. through cling::Value::print(). This patch contains:. - Implementation of declarePrintValue. - Re-Implementation of ClingPrintValue because I changed to use Evaluate. some time ago. - removing of RVec version of printValue which wasn't used at all. - Fix test/vecops_rvec.cxx, printValue is never supposed to be called by. a normal user.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2664
https://github.com/root-project/root/pull/2664:380,deployability,version,version,380,"Allow calling printValue() directly in ROOT; As we discussed in #2644, the nicest interface for printing is. printValue, and we should support people using this rather than going. through cling::Value::print(). This patch contains:. - Implementation of declarePrintValue. - Re-Implementation of ClingPrintValue because I changed to use Evaluate. some time ago. - removing of RVec version of printValue which wasn't used at all. - Fix test/vecops_rvec.cxx, printValue is never supposed to be called by. a normal user.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2664
https://github.com/root-project/root/pull/2664:82,integrability,interfac,interface,82,"Allow calling printValue() directly in ROOT; As we discussed in #2644, the nicest interface for printing is. printValue, and we should support people using this rather than going. through cling::Value::print(). This patch contains:. - Implementation of declarePrintValue. - Re-Implementation of ClingPrintValue because I changed to use Evaluate. some time ago. - removing of RVec version of printValue which wasn't used at all. - Fix test/vecops_rvec.cxx, printValue is never supposed to be called by. a normal user.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2664
https://github.com/root-project/root/pull/2664:380,integrability,version,version,380,"Allow calling printValue() directly in ROOT; As we discussed in #2644, the nicest interface for printing is. printValue, and we should support people using this rather than going. through cling::Value::print(). This patch contains:. - Implementation of declarePrintValue. - Re-Implementation of ClingPrintValue because I changed to use Evaluate. some time ago. - removing of RVec version of printValue which wasn't used at all. - Fix test/vecops_rvec.cxx, printValue is never supposed to be called by. a normal user.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2664
https://github.com/root-project/root/pull/2664:82,interoperability,interfac,interface,82,"Allow calling printValue() directly in ROOT; As we discussed in #2644, the nicest interface for printing is. printValue, and we should support people using this rather than going. through cling::Value::print(). This patch contains:. - Implementation of declarePrintValue. - Re-Implementation of ClingPrintValue because I changed to use Evaluate. some time ago. - removing of RVec version of printValue which wasn't used at all. - Fix test/vecops_rvec.cxx, printValue is never supposed to be called by. a normal user.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2664
https://github.com/root-project/root/pull/2664:82,modifiability,interfac,interface,82,"Allow calling printValue() directly in ROOT; As we discussed in #2644, the nicest interface for printing is. printValue, and we should support people using this rather than going. through cling::Value::print(). This patch contains:. - Implementation of declarePrintValue. - Re-Implementation of ClingPrintValue because I changed to use Evaluate. some time ago. - removing of RVec version of printValue which wasn't used at all. - Fix test/vecops_rvec.cxx, printValue is never supposed to be called by. a normal user.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2664
https://github.com/root-project/root/pull/2664:380,modifiability,version,version,380,"Allow calling printValue() directly in ROOT; As we discussed in #2644, the nicest interface for printing is. printValue, and we should support people using this rather than going. through cling::Value::print(). This patch contains:. - Implementation of declarePrintValue. - Re-Implementation of ClingPrintValue because I changed to use Evaluate. some time ago. - removing of RVec version of printValue which wasn't used at all. - Fix test/vecops_rvec.cxx, printValue is never supposed to be called by. a normal user.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2664
https://github.com/root-project/root/pull/2664:351,performance,time,time,351,"Allow calling printValue() directly in ROOT; As we discussed in #2644, the nicest interface for printing is. printValue, and we should support people using this rather than going. through cling::Value::print(). This patch contains:. - Implementation of declarePrintValue. - Re-Implementation of ClingPrintValue because I changed to use Evaluate. some time ago. - removing of RVec version of printValue which wasn't used at all. - Fix test/vecops_rvec.cxx, printValue is never supposed to be called by. a normal user.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2664
https://github.com/root-project/root/pull/2664:216,safety,patch,patch,216,"Allow calling printValue() directly in ROOT; As we discussed in #2644, the nicest interface for printing is. printValue, and we should support people using this rather than going. through cling::Value::print(). This patch contains:. - Implementation of declarePrintValue. - Re-Implementation of ClingPrintValue because I changed to use Evaluate. some time ago. - removing of RVec version of printValue which wasn't used at all. - Fix test/vecops_rvec.cxx, printValue is never supposed to be called by. a normal user.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2664
https://github.com/root-project/root/pull/2664:434,safety,test,test,434,"Allow calling printValue() directly in ROOT; As we discussed in #2644, the nicest interface for printing is. printValue, and we should support people using this rather than going. through cling::Value::print(). This patch contains:. - Implementation of declarePrintValue. - Re-Implementation of ClingPrintValue because I changed to use Evaluate. some time ago. - removing of RVec version of printValue which wasn't used at all. - Fix test/vecops_rvec.cxx, printValue is never supposed to be called by. a normal user.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2664
https://github.com/root-project/root/pull/2664:216,security,patch,patch,216,"Allow calling printValue() directly in ROOT; As we discussed in #2644, the nicest interface for printing is. printValue, and we should support people using this rather than going. through cling::Value::print(). This patch contains:. - Implementation of declarePrintValue. - Re-Implementation of ClingPrintValue because I changed to use Evaluate. some time ago. - removing of RVec version of printValue which wasn't used at all. - Fix test/vecops_rvec.cxx, printValue is never supposed to be called by. a normal user.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2664
https://github.com/root-project/root/pull/2664:434,testability,test,test,434,"Allow calling printValue() directly in ROOT; As we discussed in #2644, the nicest interface for printing is. printValue, and we should support people using this rather than going. through cling::Value::print(). This patch contains:. - Implementation of declarePrintValue. - Re-Implementation of ClingPrintValue because I changed to use Evaluate. some time ago. - removing of RVec version of printValue which wasn't used at all. - Fix test/vecops_rvec.cxx, printValue is never supposed to be called by. a normal user.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2664
https://github.com/root-project/root/pull/2664:135,usability,support,support,135,"Allow calling printValue() directly in ROOT; As we discussed in #2644, the nicest interface for printing is. printValue, and we should support people using this rather than going. through cling::Value::print(). This patch contains:. - Implementation of declarePrintValue. - Re-Implementation of ClingPrintValue because I changed to use Evaluate. some time ago. - removing of RVec version of printValue which wasn't used at all. - Fix test/vecops_rvec.cxx, printValue is never supposed to be called by. a normal user.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2664
https://github.com/root-project/root/pull/2664:511,usability,user,user,511,"Allow calling printValue() directly in ROOT; As we discussed in #2644, the nicest interface for printing is. printValue, and we should support people using this rather than going. through cling::Value::print(). This patch contains:. - Implementation of declarePrintValue. - Re-Implementation of ClingPrintValue because I changed to use Evaluate. some time ago. - removing of RVec version of printValue which wasn't used at all. - Fix test/vecops_rvec.cxx, printValue is never supposed to be called by. a normal user.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2664
https://github.com/root-project/root/pull/2665:6,deployability,Updat,Update,6,[Doc] Update readme; - Remove from build status table 5.34 and 6.10. - Add the link to zenodo records for the citation rather than the hardcoded link to 6.10,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2665
https://github.com/root-project/root/pull/2665:35,deployability,build,build,35,[Doc] Update readme; - Remove from build status table 5.34 and 6.10. - Add the link to zenodo records for the citation rather than the hardcoded link to 6.10,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2665
https://github.com/root-project/root/pull/2665:6,safety,Updat,Update,6,[Doc] Update readme; - Remove from build status table 5.34 and 6.10. - Add the link to zenodo records for the citation rather than the hardcoded link to 6.10,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2665
https://github.com/root-project/root/pull/2665:6,security,Updat,Update,6,[Doc] Update readme; - Remove from build status table 5.34 and 6.10. - Add the link to zenodo records for the citation rather than the hardcoded link to 6.10,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2665
https://github.com/root-project/root/pull/2665:135,security,hardcod,hardcoded,135,[Doc] Update readme; - Remove from build status table 5.34 and 6.10. - Add the link to zenodo records for the citation rather than the hardcoded link to 6.10,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2665
https://github.com/root-project/root/pull/2665:41,usability,statu,status,41,[Doc] Update readme; - Remove from build status table 5.34 and 6.10. - Add the link to zenodo records for the citation rather than the hardcoded link to 6.10,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2665
https://github.com/root-project/root/pull/2666:49,energy efficiency,profil,profile,49,[DF] ROOT-9476 Fix treatment of custom x axis of profile 1d models; thanks to daniel savoiu for the report and suggested fix: https://root-forum.cern.ch/t/custom-binning-with-tprofile1dmodel/30780,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2666
https://github.com/root-project/root/pull/2666:60,energy efficiency,model,models,60,[DF] ROOT-9476 Fix treatment of custom x axis of profile 1d models; thanks to daniel savoiu for the report and suggested fix: https://root-forum.cern.ch/t/custom-binning-with-tprofile1dmodel/30780,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2666
https://github.com/root-project/root/pull/2666:49,performance,profil,profile,49,[DF] ROOT-9476 Fix treatment of custom x axis of profile 1d models; thanks to daniel savoiu for the report and suggested fix: https://root-forum.cern.ch/t/custom-binning-with-tprofile1dmodel/30780,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2666
https://github.com/root-project/root/pull/2666:60,security,model,models,60,[DF] ROOT-9476 Fix treatment of custom x axis of profile 1d models; thanks to daniel savoiu for the report and suggested fix: https://root-forum.cern.ch/t/custom-binning-with-tprofile1dmodel/30780,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2666
https://github.com/root-project/root/pull/2666:32,usability,custom,custom,32,[DF] ROOT-9476 Fix treatment of custom x axis of profile 1d models; thanks to daniel savoiu for the report and suggested fix: https://root-forum.cern.ch/t/custom-binning-with-tprofile1dmodel/30780,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2666
https://github.com/root-project/root/pull/2666:155,usability,custom,custom-binning-with-,155,[DF] ROOT-9476 Fix treatment of custom x axis of profile 1d models; thanks to daniel savoiu for the report and suggested fix: https://root-forum.cern.ch/t/custom-binning-with-tprofile1dmodel/30780,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2666
https://github.com/root-project/root/pull/2667:71,energy efficiency,model,models,71,[DF] ROOT-9692: Properly consider custom binning when using TProfile2D models;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2667
