id,quality_attribute,keyword,matched_word,match_idx,sentence,source,author,repo,version,wiki,url
https://github.com/google/deepvariant/issues/586:230,performance,scalab,scalable,230,"Hi @JakeHagen . Thank you for the report, and for including the quality readout from the HTML file. One thing I want to mention is that this distribution is something that we have seen in some samples - see Figure 1 of [Accurate, scalable cohort variant calls using DeepVariant and GLnexus](https://academic.oup.com/bioinformatics/article/36/24/5582/6064144). In this figure, some of the analyzed cohorts do have bimodal GQ distributions for DeepVariant calls, while others (e.g. GIAB) do not. Supplementary Figure 3 of that paper indicates that a reasonable component of the bimodal distribution relates to sequence depth, at lower sample sequence depths, GIAB becomes more bimodal. I believe that we internally stratified calls and (though my memory is hazy) found that another factor in the bimodal distribution is whether a site is HET or HOM. Specifically, HET sites with lower depth have lower GQs, and I believe the explanation for this is that as coverage drops, it can become difficult to tell a HET site from either a REF or HOM, while HOM sites have more effective signal for them as non-REF. I don't think that the model is likely to be less confident in 100bp reads because they are not as much of the training data, but I expect the fact that 100bp reads are harder to uniquely map and will results in more variability in the coverage of high-MAPQ reads would indirectly contribute. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/586
https://github.com/google/deepvariant/issues/586:745,performance,memor,memory,745,"Hi @JakeHagen . Thank you for the report, and for including the quality readout from the HTML file. One thing I want to mention is that this distribution is something that we have seen in some samples - see Figure 1 of [Accurate, scalable cohort variant calls using DeepVariant and GLnexus](https://academic.oup.com/bioinformatics/article/36/24/5582/6064144). In this figure, some of the analyzed cohorts do have bimodal GQ distributions for DeepVariant calls, while others (e.g. GIAB) do not. Supplementary Figure 3 of that paper indicates that a reasonable component of the bimodal distribution relates to sequence depth, at lower sample sequence depths, GIAB becomes more bimodal. I believe that we internally stratified calls and (though my memory is hazy) found that another factor in the bimodal distribution is whether a site is HET or HOM. Specifically, HET sites with lower depth have lower GQs, and I believe the explanation for this is that as coverage drops, it can become difficult to tell a HET site from either a REF or HOM, while HOM sites have more effective signal for them as non-REF. I don't think that the model is likely to be less confident in 100bp reads because they are not as much of the training data, but I expect the fact that 100bp reads are harder to uniquely map and will results in more variability in the coverage of high-MAPQ reads would indirectly contribute. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/586
https://github.com/google/deepvariant/issues/586:1076,security,sign,signal,1076,"Hi @JakeHagen . Thank you for the report, and for including the quality readout from the HTML file. One thing I want to mention is that this distribution is something that we have seen in some samples - see Figure 1 of [Accurate, scalable cohort variant calls using DeepVariant and GLnexus](https://academic.oup.com/bioinformatics/article/36/24/5582/6064144). In this figure, some of the analyzed cohorts do have bimodal GQ distributions for DeepVariant calls, while others (e.g. GIAB) do not. Supplementary Figure 3 of that paper indicates that a reasonable component of the bimodal distribution relates to sequence depth, at lower sample sequence depths, GIAB becomes more bimodal. I believe that we internally stratified calls and (though my memory is hazy) found that another factor in the bimodal distribution is whether a site is HET or HOM. Specifically, HET sites with lower depth have lower GQs, and I believe the explanation for this is that as coverage drops, it can become difficult to tell a HET site from either a REF or HOM, while HOM sites have more effective signal for them as non-REF. I don't think that the model is likely to be less confident in 100bp reads because they are not as much of the training data, but I expect the fact that 100bp reads are harder to uniquely map and will results in more variability in the coverage of high-MAPQ reads would indirectly contribute. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/586
https://github.com/google/deepvariant/issues/586:1127,security,model,model,1127,"Hi @JakeHagen . Thank you for the report, and for including the quality readout from the HTML file. One thing I want to mention is that this distribution is something that we have seen in some samples - see Figure 1 of [Accurate, scalable cohort variant calls using DeepVariant and GLnexus](https://academic.oup.com/bioinformatics/article/36/24/5582/6064144). In this figure, some of the analyzed cohorts do have bimodal GQ distributions for DeepVariant calls, while others (e.g. GIAB) do not. Supplementary Figure 3 of that paper indicates that a reasonable component of the bimodal distribution relates to sequence depth, at lower sample sequence depths, GIAB becomes more bimodal. I believe that we internally stratified calls and (though my memory is hazy) found that another factor in the bimodal distribution is whether a site is HET or HOM. Specifically, HET sites with lower depth have lower GQs, and I believe the explanation for this is that as coverage drops, it can become difficult to tell a HET site from either a REF or HOM, while HOM sites have more effective signal for them as non-REF. I don't think that the model is likely to be less confident in 100bp reads because they are not as much of the training data, but I expect the fact that 100bp reads are harder to uniquely map and will results in more variability in the coverage of high-MAPQ reads would indirectly contribute. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/586
https://github.com/google/deepvariant/issues/586:955,testability,coverag,coverage,955,"Hi @JakeHagen . Thank you for the report, and for including the quality readout from the HTML file. One thing I want to mention is that this distribution is something that we have seen in some samples - see Figure 1 of [Accurate, scalable cohort variant calls using DeepVariant and GLnexus](https://academic.oup.com/bioinformatics/article/36/24/5582/6064144). In this figure, some of the analyzed cohorts do have bimodal GQ distributions for DeepVariant calls, while others (e.g. GIAB) do not. Supplementary Figure 3 of that paper indicates that a reasonable component of the bimodal distribution relates to sequence depth, at lower sample sequence depths, GIAB becomes more bimodal. I believe that we internally stratified calls and (though my memory is hazy) found that another factor in the bimodal distribution is whether a site is HET or HOM. Specifically, HET sites with lower depth have lower GQs, and I believe the explanation for this is that as coverage drops, it can become difficult to tell a HET site from either a REF or HOM, while HOM sites have more effective signal for them as non-REF. I don't think that the model is likely to be less confident in 100bp reads because they are not as much of the training data, but I expect the fact that 100bp reads are harder to uniquely map and will results in more variability in the coverage of high-MAPQ reads would indirectly contribute. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/586
https://github.com/google/deepvariant/issues/586:1340,testability,coverag,coverage,1340,"Hi @JakeHagen . Thank you for the report, and for including the quality readout from the HTML file. One thing I want to mention is that this distribution is something that we have seen in some samples - see Figure 1 of [Accurate, scalable cohort variant calls using DeepVariant and GLnexus](https://academic.oup.com/bioinformatics/article/36/24/5582/6064144). In this figure, some of the analyzed cohorts do have bimodal GQ distributions for DeepVariant calls, while others (e.g. GIAB) do not. Supplementary Figure 3 of that paper indicates that a reasonable component of the bimodal distribution relates to sequence depth, at lower sample sequence depths, GIAB becomes more bimodal. I believe that we internally stratified calls and (though my memory is hazy) found that another factor in the bimodal distribution is whether a site is HET or HOM. Specifically, HET sites with lower depth have lower GQs, and I believe the explanation for this is that as coverage drops, it can become difficult to tell a HET site from either a REF or HOM, while HOM sites have more effective signal for them as non-REF. I don't think that the model is likely to be less confident in 100bp reads because they are not as much of the training data, but I expect the fact that 100bp reads are harder to uniquely map and will results in more variability in the coverage of high-MAPQ reads would indirectly contribute. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/586
https://github.com/google/deepvariant/issues/586:531,usability,indicat,indicates,531,"Hi @JakeHagen . Thank you for the report, and for including the quality readout from the HTML file. One thing I want to mention is that this distribution is something that we have seen in some samples - see Figure 1 of [Accurate, scalable cohort variant calls using DeepVariant and GLnexus](https://academic.oup.com/bioinformatics/article/36/24/5582/6064144). In this figure, some of the analyzed cohorts do have bimodal GQ distributions for DeepVariant calls, while others (e.g. GIAB) do not. Supplementary Figure 3 of that paper indicates that a reasonable component of the bimodal distribution relates to sequence depth, at lower sample sequence depths, GIAB becomes more bimodal. I believe that we internally stratified calls and (though my memory is hazy) found that another factor in the bimodal distribution is whether a site is HET or HOM. Specifically, HET sites with lower depth have lower GQs, and I believe the explanation for this is that as coverage drops, it can become difficult to tell a HET site from either a REF or HOM, while HOM sites have more effective signal for them as non-REF. I don't think that the model is likely to be less confident in 100bp reads because they are not as much of the training data, but I expect the fact that 100bp reads are harder to uniquely map and will results in more variability in the coverage of high-MAPQ reads would indirectly contribute. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/586
https://github.com/google/deepvariant/issues/586:745,usability,memor,memory,745,"Hi @JakeHagen . Thank you for the report, and for including the quality readout from the HTML file. One thing I want to mention is that this distribution is something that we have seen in some samples - see Figure 1 of [Accurate, scalable cohort variant calls using DeepVariant and GLnexus](https://academic.oup.com/bioinformatics/article/36/24/5582/6064144). In this figure, some of the analyzed cohorts do have bimodal GQ distributions for DeepVariant calls, while others (e.g. GIAB) do not. Supplementary Figure 3 of that paper indicates that a reasonable component of the bimodal distribution relates to sequence depth, at lower sample sequence depths, GIAB becomes more bimodal. I believe that we internally stratified calls and (though my memory is hazy) found that another factor in the bimodal distribution is whether a site is HET or HOM. Specifically, HET sites with lower depth have lower GQs, and I believe the explanation for this is that as coverage drops, it can become difficult to tell a HET site from either a REF or HOM, while HOM sites have more effective signal for them as non-REF. I don't think that the model is likely to be less confident in 100bp reads because they are not as much of the training data, but I expect the fact that 100bp reads are harder to uniquely map and will results in more variability in the coverage of high-MAPQ reads would indirectly contribute. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/586
https://github.com/google/deepvariant/issues/586:1066,usability,effectiv,effective,1066,"Hi @JakeHagen . Thank you for the report, and for including the quality readout from the HTML file. One thing I want to mention is that this distribution is something that we have seen in some samples - see Figure 1 of [Accurate, scalable cohort variant calls using DeepVariant and GLnexus](https://academic.oup.com/bioinformatics/article/36/24/5582/6064144). In this figure, some of the analyzed cohorts do have bimodal GQ distributions for DeepVariant calls, while others (e.g. GIAB) do not. Supplementary Figure 3 of that paper indicates that a reasonable component of the bimodal distribution relates to sequence depth, at lower sample sequence depths, GIAB becomes more bimodal. I believe that we internally stratified calls and (though my memory is hazy) found that another factor in the bimodal distribution is whether a site is HET or HOM. Specifically, HET sites with lower depth have lower GQs, and I believe the explanation for this is that as coverage drops, it can become difficult to tell a HET site from either a REF or HOM, while HOM sites have more effective signal for them as non-REF. I don't think that the model is likely to be less confident in 100bp reads because they are not as much of the training data, but I expect the fact that 100bp reads are harder to uniquely map and will results in more variability in the coverage of high-MAPQ reads would indirectly contribute. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/586
https://github.com/google/deepvariant/issues/586:98,deployability,log,logging,98,"@JakeHagen . I have one other question, do you know what the median insert size is (e.g. from the logging information of BWA)? One other possibility is that the insert sizes for this sample are different and this is interacting with the input channel for insert length. . If this is the case, then you would expect that DeepVariant 1.3 (which does not include this channel) would have less of that bimodal distribution. If you do check this and see a difference in GQ distribution, it would be good for us to know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/586
https://github.com/google/deepvariant/issues/586:406,interoperability,distribut,distribution,406,"@JakeHagen . I have one other question, do you know what the median insert size is (e.g. from the logging information of BWA)? One other possibility is that the insert sizes for this sample are different and this is interacting with the input channel for insert length. . If this is the case, then you would expect that DeepVariant 1.3 (which does not include this channel) would have less of that bimodal distribution. If you do check this and see a difference in GQ distribution, it would be good for us to know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/586
https://github.com/google/deepvariant/issues/586:468,interoperability,distribut,distribution,468,"@JakeHagen . I have one other question, do you know what the median insert size is (e.g. from the logging information of BWA)? One other possibility is that the insert sizes for this sample are different and this is interacting with the input channel for insert length. . If this is the case, then you would expect that DeepVariant 1.3 (which does not include this channel) would have less of that bimodal distribution. If you do check this and see a difference in GQ distribution, it would be good for us to know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/586
https://github.com/google/deepvariant/issues/586:343,reliability,doe,does,343,"@JakeHagen . I have one other question, do you know what the median insert size is (e.g. from the logging information of BWA)? One other possibility is that the insert sizes for this sample are different and this is interacting with the input channel for insert length. . If this is the case, then you would expect that DeepVariant 1.3 (which does not include this channel) would have less of that bimodal distribution. If you do check this and see a difference in GQ distribution, it would be good for us to know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/586
https://github.com/google/deepvariant/issues/586:98,safety,log,logging,98,"@JakeHagen . I have one other question, do you know what the median insert size is (e.g. from the logging information of BWA)? One other possibility is that the insert sizes for this sample are different and this is interacting with the input channel for insert length. . If this is the case, then you would expect that DeepVariant 1.3 (which does not include this channel) would have less of that bimodal distribution. If you do check this and see a difference in GQ distribution, it would be good for us to know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/586
https://github.com/google/deepvariant/issues/586:237,safety,input,input,237,"@JakeHagen . I have one other question, do you know what the median insert size is (e.g. from the logging information of BWA)? One other possibility is that the insert sizes for this sample are different and this is interacting with the input channel for insert length. . If this is the case, then you would expect that DeepVariant 1.3 (which does not include this channel) would have less of that bimodal distribution. If you do check this and see a difference in GQ distribution, it would be good for us to know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/586
https://github.com/google/deepvariant/issues/586:98,security,log,logging,98,"@JakeHagen . I have one other question, do you know what the median insert size is (e.g. from the logging information of BWA)? One other possibility is that the insert sizes for this sample are different and this is interacting with the input channel for insert length. . If this is the case, then you would expect that DeepVariant 1.3 (which does not include this channel) would have less of that bimodal distribution. If you do check this and see a difference in GQ distribution, it would be good for us to know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/586
https://github.com/google/deepvariant/issues/586:98,testability,log,logging,98,"@JakeHagen . I have one other question, do you know what the median insert size is (e.g. from the logging information of BWA)? One other possibility is that the insert sizes for this sample are different and this is interacting with the input channel for insert length. . If this is the case, then you would expect that DeepVariant 1.3 (which does not include this channel) would have less of that bimodal distribution. If you do check this and see a difference in GQ distribution, it would be good for us to know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/586
https://github.com/google/deepvariant/issues/586:216,usability,interact,interacting,216,"@JakeHagen . I have one other question, do you know what the median insert size is (e.g. from the logging information of BWA)? One other possibility is that the insert sizes for this sample are different and this is interacting with the input channel for insert length. . If this is the case, then you would expect that DeepVariant 1.3 (which does not include this channel) would have less of that bimodal distribution. If you do check this and see a difference in GQ distribution, it would be good for us to know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/586
https://github.com/google/deepvariant/issues/586:237,usability,input,input,237,"@JakeHagen . I have one other question, do you know what the median insert size is (e.g. from the logging information of BWA)? One other possibility is that the insert sizes for this sample are different and this is interacting with the input channel for insert length. . If this is the case, then you would expect that DeepVariant 1.3 (which does not include this channel) would have less of that bimodal distribution. If you do check this and see a difference in GQ distribution, it would be good for us to know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/586
https://github.com/google/deepvariant/issues/586:155,interoperability,distribut,distribution,155,"Thank you very much for your responses @pichuan @AndrewCarroll . I attached the relevant plots from deepvariant 1.3's visual output. It has a very similar distribution compared to 1.4. (I also did not know these plots were zoomable). <img width=""820"" alt=""image"" src=""https://user-images.githubusercontent.com/8237552/203120271-1e04138e-c9f3-44a4-a35b-1e006ea5b58e.png"">. I also looked into insert size differences using the tlen field from the bam, which I believe is equivalent to insert size. The median is 267 and the distribution is below. A sample with a normal GQ distribution had a median of 150 (but also 75bp read length). Because of the similar plots between 1.3 and 1.4, I don't think insert size is the difference maker, but I can do a deep dive into it if nothing else comes up. ![image](https://user-images.githubusercontent.com/8237552/203121343-40eb3f61-ec92-4b97-b656-7ae34a2463e0.png). ![image](https://user-images.githubusercontent.com/8237552/203125666-0983300b-4f0e-4a56-b3ba-81a438fc1798.png). I will also investigate the relationship between depth, GT, and GQ. I think you are right that the lower GQ peak is from heterozygous samples, but I will need to look back into some previous plots. . Thanks again for your insight.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/586
https://github.com/google/deepvariant/issues/586:522,interoperability,distribut,distribution,522,"Thank you very much for your responses @pichuan @AndrewCarroll . I attached the relevant plots from deepvariant 1.3's visual output. It has a very similar distribution compared to 1.4. (I also did not know these plots were zoomable). <img width=""820"" alt=""image"" src=""https://user-images.githubusercontent.com/8237552/203120271-1e04138e-c9f3-44a4-a35b-1e006ea5b58e.png"">. I also looked into insert size differences using the tlen field from the bam, which I believe is equivalent to insert size. The median is 267 and the distribution is below. A sample with a normal GQ distribution had a median of 150 (but also 75bp read length). Because of the similar plots between 1.3 and 1.4, I don't think insert size is the difference maker, but I can do a deep dive into it if nothing else comes up. ![image](https://user-images.githubusercontent.com/8237552/203121343-40eb3f61-ec92-4b97-b656-7ae34a2463e0.png). ![image](https://user-images.githubusercontent.com/8237552/203125666-0983300b-4f0e-4a56-b3ba-81a438fc1798.png). I will also investigate the relationship between depth, GT, and GQ. I think you are right that the lower GQ peak is from heterozygous samples, but I will need to look back into some previous plots. . Thanks again for your insight.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/586
https://github.com/google/deepvariant/issues/586:571,interoperability,distribut,distribution,571,"Thank you very much for your responses @pichuan @AndrewCarroll . I attached the relevant plots from deepvariant 1.3's visual output. It has a very similar distribution compared to 1.4. (I also did not know these plots were zoomable). <img width=""820"" alt=""image"" src=""https://user-images.githubusercontent.com/8237552/203120271-1e04138e-c9f3-44a4-a35b-1e006ea5b58e.png"">. I also looked into insert size differences using the tlen field from the bam, which I believe is equivalent to insert size. The median is 267 and the distribution is below. A sample with a normal GQ distribution had a median of 150 (but also 75bp read length). Because of the similar plots between 1.3 and 1.4, I don't think insert size is the difference maker, but I can do a deep dive into it if nothing else comes up. ![image](https://user-images.githubusercontent.com/8237552/203121343-40eb3f61-ec92-4b97-b656-7ae34a2463e0.png). ![image](https://user-images.githubusercontent.com/8237552/203125666-0983300b-4f0e-4a56-b3ba-81a438fc1798.png). I will also investigate the relationship between depth, GT, and GQ. I think you are right that the lower GQ peak is from heterozygous samples, but I will need to look back into some previous plots. . Thanks again for your insight.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/586
https://github.com/google/deepvariant/issues/586:118,usability,visual,visual,118,"Thank you very much for your responses @pichuan @AndrewCarroll . I attached the relevant plots from deepvariant 1.3's visual output. It has a very similar distribution compared to 1.4. (I also did not know these plots were zoomable). <img width=""820"" alt=""image"" src=""https://user-images.githubusercontent.com/8237552/203120271-1e04138e-c9f3-44a4-a35b-1e006ea5b58e.png"">. I also looked into insert size differences using the tlen field from the bam, which I believe is equivalent to insert size. The median is 267 and the distribution is below. A sample with a normal GQ distribution had a median of 150 (but also 75bp read length). Because of the similar plots between 1.3 and 1.4, I don't think insert size is the difference maker, but I can do a deep dive into it if nothing else comes up. ![image](https://user-images.githubusercontent.com/8237552/203121343-40eb3f61-ec92-4b97-b656-7ae34a2463e0.png). ![image](https://user-images.githubusercontent.com/8237552/203125666-0983300b-4f0e-4a56-b3ba-81a438fc1798.png). I will also investigate the relationship between depth, GT, and GQ. I think you are right that the lower GQ peak is from heterozygous samples, but I will need to look back into some previous plots. . Thanks again for your insight.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/586
https://github.com/google/deepvariant/issues/586:276,usability,user,user-images,276,"Thank you very much for your responses @pichuan @AndrewCarroll . I attached the relevant plots from deepvariant 1.3's visual output. It has a very similar distribution compared to 1.4. (I also did not know these plots were zoomable). <img width=""820"" alt=""image"" src=""https://user-images.githubusercontent.com/8237552/203120271-1e04138e-c9f3-44a4-a35b-1e006ea5b58e.png"">. I also looked into insert size differences using the tlen field from the bam, which I believe is equivalent to insert size. The median is 267 and the distribution is below. A sample with a normal GQ distribution had a median of 150 (but also 75bp read length). Because of the similar plots between 1.3 and 1.4, I don't think insert size is the difference maker, but I can do a deep dive into it if nothing else comes up. ![image](https://user-images.githubusercontent.com/8237552/203121343-40eb3f61-ec92-4b97-b656-7ae34a2463e0.png). ![image](https://user-images.githubusercontent.com/8237552/203125666-0983300b-4f0e-4a56-b3ba-81a438fc1798.png). I will also investigate the relationship between depth, GT, and GQ. I think you are right that the lower GQ peak is from heterozygous samples, but I will need to look back into some previous plots. . Thanks again for your insight.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/586
https://github.com/google/deepvariant/issues/586:810,usability,user,user-images,810,"Thank you very much for your responses @pichuan @AndrewCarroll . I attached the relevant plots from deepvariant 1.3's visual output. It has a very similar distribution compared to 1.4. (I also did not know these plots were zoomable). <img width=""820"" alt=""image"" src=""https://user-images.githubusercontent.com/8237552/203120271-1e04138e-c9f3-44a4-a35b-1e006ea5b58e.png"">. I also looked into insert size differences using the tlen field from the bam, which I believe is equivalent to insert size. The median is 267 and the distribution is below. A sample with a normal GQ distribution had a median of 150 (but also 75bp read length). Because of the similar plots between 1.3 and 1.4, I don't think insert size is the difference maker, but I can do a deep dive into it if nothing else comes up. ![image](https://user-images.githubusercontent.com/8237552/203121343-40eb3f61-ec92-4b97-b656-7ae34a2463e0.png). ![image](https://user-images.githubusercontent.com/8237552/203125666-0983300b-4f0e-4a56-b3ba-81a438fc1798.png). I will also investigate the relationship between depth, GT, and GQ. I think you are right that the lower GQ peak is from heterozygous samples, but I will need to look back into some previous plots. . Thanks again for your insight.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/586
https://github.com/google/deepvariant/issues/586:922,usability,user,user-images,922,"Thank you very much for your responses @pichuan @AndrewCarroll . I attached the relevant plots from deepvariant 1.3's visual output. It has a very similar distribution compared to 1.4. (I also did not know these plots were zoomable). <img width=""820"" alt=""image"" src=""https://user-images.githubusercontent.com/8237552/203120271-1e04138e-c9f3-44a4-a35b-1e006ea5b58e.png"">. I also looked into insert size differences using the tlen field from the bam, which I believe is equivalent to insert size. The median is 267 and the distribution is below. A sample with a normal GQ distribution had a median of 150 (but also 75bp read length). Because of the similar plots between 1.3 and 1.4, I don't think insert size is the difference maker, but I can do a deep dive into it if nothing else comes up. ![image](https://user-images.githubusercontent.com/8237552/203121343-40eb3f61-ec92-4b97-b656-7ae34a2463e0.png). ![image](https://user-images.githubusercontent.com/8237552/203125666-0983300b-4f0e-4a56-b3ba-81a438fc1798.png). I will also investigate the relationship between depth, GT, and GQ. I think you are right that the lower GQ peak is from heterozygous samples, but I will need to look back into some previous plots. . Thanks again for your insight.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/586
https://github.com/google/deepvariant/issues/586:120,interoperability,distribut,distribution,120,"Hi again, it looks like this might be a read length issue. I used the first 75bp of the same sample and the GQ and QUAL distribution is what I would expect. . <img width=""509"" alt=""Screenshot 2022-11-28 at 11 12 31 AM"" src=""https://user-images.githubusercontent.com/8237552/204326815-9c04b607-742b-424f-ac5e-218663cc21aa.png"">.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/586
https://github.com/google/deepvariant/issues/586:232,usability,user,user-images,232,"Hi again, it looks like this might be a read length issue. I used the first 75bp of the same sample and the GQ and QUAL distribution is what I would expect. . <img width=""509"" alt=""Screenshot 2022-11-28 at 11 12 31 AM"" src=""https://user-images.githubusercontent.com/8237552/204326815-9c04b607-742b-424f-ac5e-218663cc21aa.png"">.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/586
https://github.com/google/deepvariant/issues/586:37,deployability,observ,observation,37,"Hi @JakeHagen . That is a surprising observation. I am going to look at this myself. I'll start with trying to reproduce the observation. I assume this sample is not shareable, but if it is, it would be great for me to start from that. Otherwise, I'll try truncating to 100bp and then to 75bp on a standard sample to see if I can reproduce this finding.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/586
https://github.com/google/deepvariant/issues/586:125,deployability,observ,observation,125,"Hi @JakeHagen . That is a surprising observation. I am going to look at this myself. I'll start with trying to reproduce the observation. I assume this sample is not shareable, but if it is, it would be great for me to start from that. Otherwise, I'll try truncating to 100bp and then to 75bp on a standard sample to see if I can reproduce this finding.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/586
https://github.com/google/deepvariant/issues/586:166,interoperability,share,shareable,166,"Hi @JakeHagen . That is a surprising observation. I am going to look at this myself. I'll start with trying to reproduce the observation. I assume this sample is not shareable, but if it is, it would be great for me to start from that. Otherwise, I'll try truncating to 100bp and then to 75bp on a standard sample to see if I can reproduce this finding.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/586
https://github.com/google/deepvariant/issues/586:298,interoperability,standard,standard,298,"Hi @JakeHagen . That is a surprising observation. I am going to look at this myself. I'll start with trying to reproduce the observation. I assume this sample is not shareable, but if it is, it would be great for me to start from that. Otherwise, I'll try truncating to 100bp and then to 75bp on a standard sample to see if I can reproduce this finding.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/586
https://github.com/google/deepvariant/issues/586:37,testability,observ,observation,37,"Hi @JakeHagen . That is a surprising observation. I am going to look at this myself. I'll start with trying to reproduce the observation. I assume this sample is not shareable, but if it is, it would be great for me to start from that. Otherwise, I'll try truncating to 100bp and then to 75bp on a standard sample to see if I can reproduce this finding.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/586
https://github.com/google/deepvariant/issues/586:125,testability,observ,observation,125,"Hi @JakeHagen . That is a surprising observation. I am going to look at this myself. I'll start with trying to reproduce the observation. I assume this sample is not shareable, but if it is, it would be great for me to start from that. Otherwise, I'll try truncating to 100bp and then to 75bp on a standard sample to see if I can reproduce this finding.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/586
https://github.com/google/deepvariant/issues/586:78,interoperability,share,share,78,"Thank you @AndrewCarroll , I appreciate you looking into this. I wish I could share this sample, but I can not (unless you have dbGaP approval?).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/586
https://github.com/google/deepvariant/issues/586:182,availability,replic,replicate,182,"Hi @JakeHagen . Although we do have access to some dbGaP datasets, I don't believe that this is one of them. Let me conduct some experiments from our benchmark data and see if I can replicate the effect.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/586
https://github.com/google/deepvariant/issues/586:36,security,access,access,36,"Hi @JakeHagen . Although we do have access to some dbGaP datasets, I don't believe that this is one of them. Let me conduct some experiments from our benchmark data and see if I can replicate the effect.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/586
https://github.com/google/deepvariant/issues/586:111,availability,consist,consisting,111,"Hi @JakeHagen . I took one of our 50x NovaSeq samples that are 150bp PE reads and trimmed the reads into 4 WGS consisting of a sample each for:. 1. first 100bp of the reads. 2. last 100bp of the reads. 3. first 75bp of the reads. 4. last 75bp of the reads. I wasn't able to replicate the effect that you see in any off the output reports. In your approach, did you trim the reads from the end (simply truncating to the first 75bp)? If so, I wasn't able to replicate the effect you see. There might be something more complicated about your sample. One possible explanation is that you have a run with lower sequencing quality and trimming to the first 75bp reads removes some lower quality parts which look suspicious to DeepVariant. If so, I wonder if your results would differ if you retained only the last 75bp reads. But I am not quite sure how to further diagnose. Here are my plots:. <img width=""1264"" alt=""HG003 novaseq 50x_only_first75bp"" src=""https://user-images.githubusercontent.com/583711/205179365-6a05941b-b6ba-47fc-b778-f970a9850651.png"">. <img width=""1266"" alt=""HG003 novaseq 50x_only_last75bp"" src=""https://user-images.githubusercontent.com/583711/205179387-7814e398-1b48-4476-9d35-6f93fafa8fbd.png"">. <img width=""1267"" alt=""HG003 novaseq 50x_only_first100bp"" src=""https://user-images.githubusercontent.com/583711/205179398-b2841a19-cb63-4ea1-9c7e-dfd79fad774d.png"">. <img width=""1286"" alt=""HG003 novaseq 50x_only_last100bp"" src=""https://user-images.githubusercontent.com/583711/205179415-a723bab5-e011-421b-a6a5-a7a750871159.png"">.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/586
https://github.com/google/deepvariant/issues/586:274,availability,replic,replicate,274,"Hi @JakeHagen . I took one of our 50x NovaSeq samples that are 150bp PE reads and trimmed the reads into 4 WGS consisting of a sample each for:. 1. first 100bp of the reads. 2. last 100bp of the reads. 3. first 75bp of the reads. 4. last 75bp of the reads. I wasn't able to replicate the effect that you see in any off the output reports. In your approach, did you trim the reads from the end (simply truncating to the first 75bp)? If so, I wasn't able to replicate the effect you see. There might be something more complicated about your sample. One possible explanation is that you have a run with lower sequencing quality and trimming to the first 75bp reads removes some lower quality parts which look suspicious to DeepVariant. If so, I wonder if your results would differ if you retained only the last 75bp reads. But I am not quite sure how to further diagnose. Here are my plots:. <img width=""1264"" alt=""HG003 novaseq 50x_only_first75bp"" src=""https://user-images.githubusercontent.com/583711/205179365-6a05941b-b6ba-47fc-b778-f970a9850651.png"">. <img width=""1266"" alt=""HG003 novaseq 50x_only_last75bp"" src=""https://user-images.githubusercontent.com/583711/205179387-7814e398-1b48-4476-9d35-6f93fafa8fbd.png"">. <img width=""1267"" alt=""HG003 novaseq 50x_only_first100bp"" src=""https://user-images.githubusercontent.com/583711/205179398-b2841a19-cb63-4ea1-9c7e-dfd79fad774d.png"">. <img width=""1286"" alt=""HG003 novaseq 50x_only_last100bp"" src=""https://user-images.githubusercontent.com/583711/205179415-a723bab5-e011-421b-a6a5-a7a750871159.png"">.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/586
https://github.com/google/deepvariant/issues/586:456,availability,replic,replicate,456,"Hi @JakeHagen . I took one of our 50x NovaSeq samples that are 150bp PE reads and trimmed the reads into 4 WGS consisting of a sample each for:. 1. first 100bp of the reads. 2. last 100bp of the reads. 3. first 75bp of the reads. 4. last 75bp of the reads. I wasn't able to replicate the effect that you see in any off the output reports. In your approach, did you trim the reads from the end (simply truncating to the first 75bp)? If so, I wasn't able to replicate the effect you see. There might be something more complicated about your sample. One possible explanation is that you have a run with lower sequencing quality and trimming to the first 75bp reads removes some lower quality parts which look suspicious to DeepVariant. If so, I wonder if your results would differ if you retained only the last 75bp reads. But I am not quite sure how to further diagnose. Here are my plots:. <img width=""1264"" alt=""HG003 novaseq 50x_only_first75bp"" src=""https://user-images.githubusercontent.com/583711/205179365-6a05941b-b6ba-47fc-b778-f970a9850651.png"">. <img width=""1266"" alt=""HG003 novaseq 50x_only_last75bp"" src=""https://user-images.githubusercontent.com/583711/205179387-7814e398-1b48-4476-9d35-6f93fafa8fbd.png"">. <img width=""1267"" alt=""HG003 novaseq 50x_only_first100bp"" src=""https://user-images.githubusercontent.com/583711/205179398-b2841a19-cb63-4ea1-9c7e-dfd79fad774d.png"">. <img width=""1286"" alt=""HG003 novaseq 50x_only_last100bp"" src=""https://user-images.githubusercontent.com/583711/205179415-a723bab5-e011-421b-a6a5-a7a750871159.png"">.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/586
https://github.com/google/deepvariant/issues/586:859,reliability,diagno,diagnose,859,"Hi @JakeHagen . I took one of our 50x NovaSeq samples that are 150bp PE reads and trimmed the reads into 4 WGS consisting of a sample each for:. 1. first 100bp of the reads. 2. last 100bp of the reads. 3. first 75bp of the reads. 4. last 75bp of the reads. I wasn't able to replicate the effect that you see in any off the output reports. In your approach, did you trim the reads from the end (simply truncating to the first 75bp)? If so, I wasn't able to replicate the effect you see. There might be something more complicated about your sample. One possible explanation is that you have a run with lower sequencing quality and trimming to the first 75bp reads removes some lower quality parts which look suspicious to DeepVariant. If so, I wonder if your results would differ if you retained only the last 75bp reads. But I am not quite sure how to further diagnose. Here are my plots:. <img width=""1264"" alt=""HG003 novaseq 50x_only_first75bp"" src=""https://user-images.githubusercontent.com/583711/205179365-6a05941b-b6ba-47fc-b778-f970a9850651.png"">. <img width=""1266"" alt=""HG003 novaseq 50x_only_last75bp"" src=""https://user-images.githubusercontent.com/583711/205179387-7814e398-1b48-4476-9d35-6f93fafa8fbd.png"">. <img width=""1267"" alt=""HG003 novaseq 50x_only_first100bp"" src=""https://user-images.githubusercontent.com/583711/205179398-b2841a19-cb63-4ea1-9c7e-dfd79fad774d.png"">. <img width=""1286"" alt=""HG003 novaseq 50x_only_last100bp"" src=""https://user-images.githubusercontent.com/583711/205179415-a723bab5-e011-421b-a6a5-a7a750871159.png"">.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/586
https://github.com/google/deepvariant/issues/586:516,safety,compl,complicated,516,"Hi @JakeHagen . I took one of our 50x NovaSeq samples that are 150bp PE reads and trimmed the reads into 4 WGS consisting of a sample each for:. 1. first 100bp of the reads. 2. last 100bp of the reads. 3. first 75bp of the reads. 4. last 75bp of the reads. I wasn't able to replicate the effect that you see in any off the output reports. In your approach, did you trim the reads from the end (simply truncating to the first 75bp)? If so, I wasn't able to replicate the effect you see. There might be something more complicated about your sample. One possible explanation is that you have a run with lower sequencing quality and trimming to the first 75bp reads removes some lower quality parts which look suspicious to DeepVariant. If so, I wonder if your results would differ if you retained only the last 75bp reads. But I am not quite sure how to further diagnose. Here are my plots:. <img width=""1264"" alt=""HG003 novaseq 50x_only_first75bp"" src=""https://user-images.githubusercontent.com/583711/205179365-6a05941b-b6ba-47fc-b778-f970a9850651.png"">. <img width=""1266"" alt=""HG003 novaseq 50x_only_last75bp"" src=""https://user-images.githubusercontent.com/583711/205179387-7814e398-1b48-4476-9d35-6f93fafa8fbd.png"">. <img width=""1267"" alt=""HG003 novaseq 50x_only_first100bp"" src=""https://user-images.githubusercontent.com/583711/205179398-b2841a19-cb63-4ea1-9c7e-dfd79fad774d.png"">. <img width=""1286"" alt=""HG003 novaseq 50x_only_last100bp"" src=""https://user-images.githubusercontent.com/583711/205179415-a723bab5-e011-421b-a6a5-a7a750871159.png"">.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/586
https://github.com/google/deepvariant/issues/586:516,security,compl,complicated,516,"Hi @JakeHagen . I took one of our 50x NovaSeq samples that are 150bp PE reads and trimmed the reads into 4 WGS consisting of a sample each for:. 1. first 100bp of the reads. 2. last 100bp of the reads. 3. first 75bp of the reads. 4. last 75bp of the reads. I wasn't able to replicate the effect that you see in any off the output reports. In your approach, did you trim the reads from the end (simply truncating to the first 75bp)? If so, I wasn't able to replicate the effect you see. There might be something more complicated about your sample. One possible explanation is that you have a run with lower sequencing quality and trimming to the first 75bp reads removes some lower quality parts which look suspicious to DeepVariant. If so, I wonder if your results would differ if you retained only the last 75bp reads. But I am not quite sure how to further diagnose. Here are my plots:. <img width=""1264"" alt=""HG003 novaseq 50x_only_first75bp"" src=""https://user-images.githubusercontent.com/583711/205179365-6a05941b-b6ba-47fc-b778-f970a9850651.png"">. <img width=""1266"" alt=""HG003 novaseq 50x_only_last75bp"" src=""https://user-images.githubusercontent.com/583711/205179387-7814e398-1b48-4476-9d35-6f93fafa8fbd.png"">. <img width=""1267"" alt=""HG003 novaseq 50x_only_first100bp"" src=""https://user-images.githubusercontent.com/583711/205179398-b2841a19-cb63-4ea1-9c7e-dfd79fad774d.png"">. <img width=""1286"" alt=""HG003 novaseq 50x_only_last100bp"" src=""https://user-images.githubusercontent.com/583711/205179415-a723bab5-e011-421b-a6a5-a7a750871159.png"">.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/586
https://github.com/google/deepvariant/issues/586:394,testability,simpl,simply,394,"Hi @JakeHagen . I took one of our 50x NovaSeq samples that are 150bp PE reads and trimmed the reads into 4 WGS consisting of a sample each for:. 1. first 100bp of the reads. 2. last 100bp of the reads. 3. first 75bp of the reads. 4. last 75bp of the reads. I wasn't able to replicate the effect that you see in any off the output reports. In your approach, did you trim the reads from the end (simply truncating to the first 75bp)? If so, I wasn't able to replicate the effect you see. There might be something more complicated about your sample. One possible explanation is that you have a run with lower sequencing quality and trimming to the first 75bp reads removes some lower quality parts which look suspicious to DeepVariant. If so, I wonder if your results would differ if you retained only the last 75bp reads. But I am not quite sure how to further diagnose. Here are my plots:. <img width=""1264"" alt=""HG003 novaseq 50x_only_first75bp"" src=""https://user-images.githubusercontent.com/583711/205179365-6a05941b-b6ba-47fc-b778-f970a9850651.png"">. <img width=""1266"" alt=""HG003 novaseq 50x_only_last75bp"" src=""https://user-images.githubusercontent.com/583711/205179387-7814e398-1b48-4476-9d35-6f93fafa8fbd.png"">. <img width=""1267"" alt=""HG003 novaseq 50x_only_first100bp"" src=""https://user-images.githubusercontent.com/583711/205179398-b2841a19-cb63-4ea1-9c7e-dfd79fad774d.png"">. <img width=""1286"" alt=""HG003 novaseq 50x_only_last100bp"" src=""https://user-images.githubusercontent.com/583711/205179415-a723bab5-e011-421b-a6a5-a7a750871159.png"">.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/586
https://github.com/google/deepvariant/issues/586:859,testability,diagno,diagnose,859,"Hi @JakeHagen . I took one of our 50x NovaSeq samples that are 150bp PE reads and trimmed the reads into 4 WGS consisting of a sample each for:. 1. first 100bp of the reads. 2. last 100bp of the reads. 3. first 75bp of the reads. 4. last 75bp of the reads. I wasn't able to replicate the effect that you see in any off the output reports. In your approach, did you trim the reads from the end (simply truncating to the first 75bp)? If so, I wasn't able to replicate the effect you see. There might be something more complicated about your sample. One possible explanation is that you have a run with lower sequencing quality and trimming to the first 75bp reads removes some lower quality parts which look suspicious to DeepVariant. If so, I wonder if your results would differ if you retained only the last 75bp reads. But I am not quite sure how to further diagnose. Here are my plots:. <img width=""1264"" alt=""HG003 novaseq 50x_only_first75bp"" src=""https://user-images.githubusercontent.com/583711/205179365-6a05941b-b6ba-47fc-b778-f970a9850651.png"">. <img width=""1266"" alt=""HG003 novaseq 50x_only_last75bp"" src=""https://user-images.githubusercontent.com/583711/205179387-7814e398-1b48-4476-9d35-6f93fafa8fbd.png"">. <img width=""1267"" alt=""HG003 novaseq 50x_only_first100bp"" src=""https://user-images.githubusercontent.com/583711/205179398-b2841a19-cb63-4ea1-9c7e-dfd79fad774d.png"">. <img width=""1286"" alt=""HG003 novaseq 50x_only_last100bp"" src=""https://user-images.githubusercontent.com/583711/205179415-a723bab5-e011-421b-a6a5-a7a750871159.png"">.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/586
https://github.com/google/deepvariant/issues/586:111,usability,consist,consisting,111,"Hi @JakeHagen . I took one of our 50x NovaSeq samples that are 150bp PE reads and trimmed the reads into 4 WGS consisting of a sample each for:. 1. first 100bp of the reads. 2. last 100bp of the reads. 3. first 75bp of the reads. 4. last 75bp of the reads. I wasn't able to replicate the effect that you see in any off the output reports. In your approach, did you trim the reads from the end (simply truncating to the first 75bp)? If so, I wasn't able to replicate the effect you see. There might be something more complicated about your sample. One possible explanation is that you have a run with lower sequencing quality and trimming to the first 75bp reads removes some lower quality parts which look suspicious to DeepVariant. If so, I wonder if your results would differ if you retained only the last 75bp reads. But I am not quite sure how to further diagnose. Here are my plots:. <img width=""1264"" alt=""HG003 novaseq 50x_only_first75bp"" src=""https://user-images.githubusercontent.com/583711/205179365-6a05941b-b6ba-47fc-b778-f970a9850651.png"">. <img width=""1266"" alt=""HG003 novaseq 50x_only_last75bp"" src=""https://user-images.githubusercontent.com/583711/205179387-7814e398-1b48-4476-9d35-6f93fafa8fbd.png"">. <img width=""1267"" alt=""HG003 novaseq 50x_only_first100bp"" src=""https://user-images.githubusercontent.com/583711/205179398-b2841a19-cb63-4ea1-9c7e-dfd79fad774d.png"">. <img width=""1286"" alt=""HG003 novaseq 50x_only_last100bp"" src=""https://user-images.githubusercontent.com/583711/205179415-a723bab5-e011-421b-a6a5-a7a750871159.png"">.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/586
https://github.com/google/deepvariant/issues/586:394,usability,simpl,simply,394,"Hi @JakeHagen . I took one of our 50x NovaSeq samples that are 150bp PE reads and trimmed the reads into 4 WGS consisting of a sample each for:. 1. first 100bp of the reads. 2. last 100bp of the reads. 3. first 75bp of the reads. 4. last 75bp of the reads. I wasn't able to replicate the effect that you see in any off the output reports. In your approach, did you trim the reads from the end (simply truncating to the first 75bp)? If so, I wasn't able to replicate the effect you see. There might be something more complicated about your sample. One possible explanation is that you have a run with lower sequencing quality and trimming to the first 75bp reads removes some lower quality parts which look suspicious to DeepVariant. If so, I wonder if your results would differ if you retained only the last 75bp reads. But I am not quite sure how to further diagnose. Here are my plots:. <img width=""1264"" alt=""HG003 novaseq 50x_only_first75bp"" src=""https://user-images.githubusercontent.com/583711/205179365-6a05941b-b6ba-47fc-b778-f970a9850651.png"">. <img width=""1266"" alt=""HG003 novaseq 50x_only_last75bp"" src=""https://user-images.githubusercontent.com/583711/205179387-7814e398-1b48-4476-9d35-6f93fafa8fbd.png"">. <img width=""1267"" alt=""HG003 novaseq 50x_only_first100bp"" src=""https://user-images.githubusercontent.com/583711/205179398-b2841a19-cb63-4ea1-9c7e-dfd79fad774d.png"">. <img width=""1286"" alt=""HG003 novaseq 50x_only_last100bp"" src=""https://user-images.githubusercontent.com/583711/205179415-a723bab5-e011-421b-a6a5-a7a750871159.png"">.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/586
https://github.com/google/deepvariant/issues/586:959,usability,user,user-images,959,"Hi @JakeHagen . I took one of our 50x NovaSeq samples that are 150bp PE reads and trimmed the reads into 4 WGS consisting of a sample each for:. 1. first 100bp of the reads. 2. last 100bp of the reads. 3. first 75bp of the reads. 4. last 75bp of the reads. I wasn't able to replicate the effect that you see in any off the output reports. In your approach, did you trim the reads from the end (simply truncating to the first 75bp)? If so, I wasn't able to replicate the effect you see. There might be something more complicated about your sample. One possible explanation is that you have a run with lower sequencing quality and trimming to the first 75bp reads removes some lower quality parts which look suspicious to DeepVariant. If so, I wonder if your results would differ if you retained only the last 75bp reads. But I am not quite sure how to further diagnose. Here are my plots:. <img width=""1264"" alt=""HG003 novaseq 50x_only_first75bp"" src=""https://user-images.githubusercontent.com/583711/205179365-6a05941b-b6ba-47fc-b778-f970a9850651.png"">. <img width=""1266"" alt=""HG003 novaseq 50x_only_last75bp"" src=""https://user-images.githubusercontent.com/583711/205179387-7814e398-1b48-4476-9d35-6f93fafa8fbd.png"">. <img width=""1267"" alt=""HG003 novaseq 50x_only_first100bp"" src=""https://user-images.githubusercontent.com/583711/205179398-b2841a19-cb63-4ea1-9c7e-dfd79fad774d.png"">. <img width=""1286"" alt=""HG003 novaseq 50x_only_last100bp"" src=""https://user-images.githubusercontent.com/583711/205179415-a723bab5-e011-421b-a6a5-a7a750871159.png"">.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/586
https://github.com/google/deepvariant/issues/586:1123,usability,user,user-images,1123,"Hi @JakeHagen . I took one of our 50x NovaSeq samples that are 150bp PE reads and trimmed the reads into 4 WGS consisting of a sample each for:. 1. first 100bp of the reads. 2. last 100bp of the reads. 3. first 75bp of the reads. 4. last 75bp of the reads. I wasn't able to replicate the effect that you see in any off the output reports. In your approach, did you trim the reads from the end (simply truncating to the first 75bp)? If so, I wasn't able to replicate the effect you see. There might be something more complicated about your sample. One possible explanation is that you have a run with lower sequencing quality and trimming to the first 75bp reads removes some lower quality parts which look suspicious to DeepVariant. If so, I wonder if your results would differ if you retained only the last 75bp reads. But I am not quite sure how to further diagnose. Here are my plots:. <img width=""1264"" alt=""HG003 novaseq 50x_only_first75bp"" src=""https://user-images.githubusercontent.com/583711/205179365-6a05941b-b6ba-47fc-b778-f970a9850651.png"">. <img width=""1266"" alt=""HG003 novaseq 50x_only_last75bp"" src=""https://user-images.githubusercontent.com/583711/205179387-7814e398-1b48-4476-9d35-6f93fafa8fbd.png"">. <img width=""1267"" alt=""HG003 novaseq 50x_only_first100bp"" src=""https://user-images.githubusercontent.com/583711/205179398-b2841a19-cb63-4ea1-9c7e-dfd79fad774d.png"">. <img width=""1286"" alt=""HG003 novaseq 50x_only_last100bp"" src=""https://user-images.githubusercontent.com/583711/205179415-a723bab5-e011-421b-a6a5-a7a750871159.png"">.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/586
https://github.com/google/deepvariant/issues/586:1289,usability,user,user-images,1289,"Hi @JakeHagen . I took one of our 50x NovaSeq samples that are 150bp PE reads and trimmed the reads into 4 WGS consisting of a sample each for:. 1. first 100bp of the reads. 2. last 100bp of the reads. 3. first 75bp of the reads. 4. last 75bp of the reads. I wasn't able to replicate the effect that you see in any off the output reports. In your approach, did you trim the reads from the end (simply truncating to the first 75bp)? If so, I wasn't able to replicate the effect you see. There might be something more complicated about your sample. One possible explanation is that you have a run with lower sequencing quality and trimming to the first 75bp reads removes some lower quality parts which look suspicious to DeepVariant. If so, I wonder if your results would differ if you retained only the last 75bp reads. But I am not quite sure how to further diagnose. Here are my plots:. <img width=""1264"" alt=""HG003 novaseq 50x_only_first75bp"" src=""https://user-images.githubusercontent.com/583711/205179365-6a05941b-b6ba-47fc-b778-f970a9850651.png"">. <img width=""1266"" alt=""HG003 novaseq 50x_only_last75bp"" src=""https://user-images.githubusercontent.com/583711/205179387-7814e398-1b48-4476-9d35-6f93fafa8fbd.png"">. <img width=""1267"" alt=""HG003 novaseq 50x_only_first100bp"" src=""https://user-images.githubusercontent.com/583711/205179398-b2841a19-cb63-4ea1-9c7e-dfd79fad774d.png"">. <img width=""1286"" alt=""HG003 novaseq 50x_only_last100bp"" src=""https://user-images.githubusercontent.com/583711/205179415-a723bab5-e011-421b-a6a5-a7a750871159.png"">.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/586
https://github.com/google/deepvariant/issues/586:1454,usability,user,user-images,1454,"Hi @JakeHagen . I took one of our 50x NovaSeq samples that are 150bp PE reads and trimmed the reads into 4 WGS consisting of a sample each for:. 1. first 100bp of the reads. 2. last 100bp of the reads. 3. first 75bp of the reads. 4. last 75bp of the reads. I wasn't able to replicate the effect that you see in any off the output reports. In your approach, did you trim the reads from the end (simply truncating to the first 75bp)? If so, I wasn't able to replicate the effect you see. There might be something more complicated about your sample. One possible explanation is that you have a run with lower sequencing quality and trimming to the first 75bp reads removes some lower quality parts which look suspicious to DeepVariant. If so, I wonder if your results would differ if you retained only the last 75bp reads. But I am not quite sure how to further diagnose. Here are my plots:. <img width=""1264"" alt=""HG003 novaseq 50x_only_first75bp"" src=""https://user-images.githubusercontent.com/583711/205179365-6a05941b-b6ba-47fc-b778-f970a9850651.png"">. <img width=""1266"" alt=""HG003 novaseq 50x_only_last75bp"" src=""https://user-images.githubusercontent.com/583711/205179387-7814e398-1b48-4476-9d35-6f93fafa8fbd.png"">. <img width=""1267"" alt=""HG003 novaseq 50x_only_first100bp"" src=""https://user-images.githubusercontent.com/583711/205179398-b2841a19-cb63-4ea1-9c7e-dfd79fad774d.png"">. <img width=""1286"" alt=""HG003 novaseq 50x_only_last100bp"" src=""https://user-images.githubusercontent.com/583711/205179415-a723bab5-e011-421b-a6a5-a7a750871159.png"">.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/586
https://github.com/google/deepvariant/issues/586:410,usability,help,help,410,"Hi @AndrewCarroll . I was hoping you would be able to reproduce this, but Im not surprised you couldn't. . I clipped the last 26bp from the fastq and then realigned etc. . The base quality scores are very good through the length of the read but I will try to clip the first 26bp to see if that makes a difference. Also, it probably wouldn't have an effect but my data is WES not WGS. Thanks again for all your help.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/586
https://github.com/google/deepvariant/issues/586:104,usability,user,user-images,104,"This is clipping the first 26bp. I am really stumped by this. <img width=""783"" alt=""image"" src=""https://user-images.githubusercontent.com/8237552/205371600-e240d27c-04a8-47eb-9790-bd06b10c40cd.png"">.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/586
https://github.com/google/deepvariant/issues/586:159,energy efficiency,reduc,reduces,159,"Hi @JakeHagen . I will take a look at running a similar analysis on our exome samples. I suppose one remaining possibility is that the truncation of the reads reduces how far beyond the capture region the sequencing is getting. The edges of the capture region tend to both have less coverage and it's harder to sample both alleles. That's just a guess, I don't have a clear answer and will still try to collect more data. When you run DeepVariant for the exome, do you restrict to the capture regions only and do you add any padding to those?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/586
https://github.com/google/deepvariant/issues/586:283,testability,coverag,coverage,283,"Hi @JakeHagen . I will take a look at running a similar analysis on our exome samples. I suppose one remaining possibility is that the truncation of the reads reduces how far beyond the capture region the sequencing is getting. The edges of the capture region tend to both have less coverage and it's harder to sample both alleles. That's just a guess, I don't have a clear answer and will still try to collect more data. When you run DeepVariant for the exome, do you restrict to the capture regions only and do you add any padding to those?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/586
https://github.com/google/deepvariant/issues/586:368,usability,clear,clear,368,"Hi @JakeHagen . I will take a look at running a similar analysis on our exome samples. I suppose one remaining possibility is that the truncation of the reads reduces how far beyond the capture region the sequencing is getting. The edges of the capture region tend to both have less coverage and it's harder to sample both alleles. That's just a guess, I don't have a clear answer and will still try to collect more data. When you run DeepVariant for the exome, do you restrict to the capture regions only and do you add any padding to those?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/586
https://github.com/google/deepvariant/issues/586:103,availability,Down,Downloaded,103,"Hi @AndrewCarroll . I believe I have recreated this issue with HG002 from GIAB. This is what I did:. - Downloaded ftp://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/data/AshkenazimTrio/HG002_NA24385_son/OsloUniversityHospital_Exome/151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bam. - Converted the bam to fastq. - Made a 100bp and 75bp fastq. - Aligned all three fastqs (original-127bp, 100bp, 75bp) to https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/GCA_000001405.15_GRCh38_no_alt_plus_hs38d1_analysis_set.fna.gz. - Marked dups etc. - Ran deep variant 1.4 on 100bp bam, 75bp bam, and original bam using WES model. - Use bed file consisting of regions from the 100bp bam with depth over 10 intersected with CDS regions. . [over10.cds.bed.txt](https://github.com/google/deepvariant/files/10231884/over10.cds.bed.txt). This is how the distributions look. Original-127bp. <img width=""786"" alt=""original_127bp"" src=""https://user-images.githubusercontent.com/8237552/207720422-ea64ddad-1290-4cc3-819d-4b9829e56981.png"">. 100bp. <img width=""786"" alt=""100bp"" src=""https://user-images.githubusercontent.com/8237552/207713090-c669cdbf-544c-4190-ac6b-6c091123c551.png"">. 75bp. <img width=""786"" alt=""75bp"" src=""https://user-images.githubusercontent.com/8237552/207713065-cdb71b28-8d69-45df-81e3-53b2f1d3aecf.png"">. This is what my command looked like. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ -B /share/terra docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type WES \. --ref /share/terra/rsrc/hg38/ref/GCA_000001405.15_GRCh38_no_alt_plus_hs38d1_analysis_set.fna.gz \. --reads ./HG002.proc.bam \. --output_vcf HG002.over10dp.vcf.gz \. --output_gvcf HG002.over10dp.gvcf.gz \. --num_shards 8 \. --intermediate_results_dir ./dv_int_results \. --regions ../100bp/over10.cds.bed. ```. Thanks again for looking into this",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/586
https://github.com/google/deepvariant/issues/586:753,availability,consist,consisting,753,"Hi @AndrewCarroll . I believe I have recreated this issue with HG002 from GIAB. This is what I did:. - Downloaded ftp://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/data/AshkenazimTrio/HG002_NA24385_son/OsloUniversityHospital_Exome/151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bam. - Converted the bam to fastq. - Made a 100bp and 75bp fastq. - Aligned all three fastqs (original-127bp, 100bp, 75bp) to https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/GCA_000001405.15_GRCh38_no_alt_plus_hs38d1_analysis_set.fna.gz. - Marked dups etc. - Ran deep variant 1.4 on 100bp bam, 75bp bam, and original bam using WES model. - Use bed file consisting of regions from the 100bp bam with depth over 10 intersected with CDS regions. . [over10.cds.bed.txt](https://github.com/google/deepvariant/files/10231884/over10.cds.bed.txt). This is how the distributions look. Original-127bp. <img width=""786"" alt=""original_127bp"" src=""https://user-images.githubusercontent.com/8237552/207720422-ea64ddad-1290-4cc3-819d-4b9829e56981.png"">. 100bp. <img width=""786"" alt=""100bp"" src=""https://user-images.githubusercontent.com/8237552/207713090-c669cdbf-544c-4190-ac6b-6c091123c551.png"">. 75bp. <img width=""786"" alt=""75bp"" src=""https://user-images.githubusercontent.com/8237552/207713065-cdb71b28-8d69-45df-81e3-53b2f1d3aecf.png"">. This is what my command looked like. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ -B /share/terra docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type WES \. --ref /share/terra/rsrc/hg38/ref/GCA_000001405.15_GRCh38_no_alt_plus_hs38d1_analysis_set.fna.gz \. --reads ./HG002.proc.bam \. --output_vcf HG002.over10dp.vcf.gz \. --output_gvcf HG002.over10dp.gvcf.gz \. --num_shards 8 \. --intermediate_results_dir ./dv_int_results \. --regions ../100bp/over10.cds.bed. ```. Thanks again for looking into this",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/586
https://github.com/google/deepvariant/issues/586:731,energy efficiency,model,model,731,"Hi @AndrewCarroll . I believe I have recreated this issue with HG002 from GIAB. This is what I did:. - Downloaded ftp://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/data/AshkenazimTrio/HG002_NA24385_son/OsloUniversityHospital_Exome/151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bam. - Converted the bam to fastq. - Made a 100bp and 75bp fastq. - Aligned all three fastqs (original-127bp, 100bp, 75bp) to https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/GCA_000001405.15_GRCh38_no_alt_plus_hs38d1_analysis_set.fna.gz. - Marked dups etc. - Ran deep variant 1.4 on 100bp bam, 75bp bam, and original bam using WES model. - Use bed file consisting of regions from the 100bp bam with depth over 10 intersected with CDS regions. . [over10.cds.bed.txt](https://github.com/google/deepvariant/files/10231884/over10.cds.bed.txt). This is how the distributions look. Original-127bp. <img width=""786"" alt=""original_127bp"" src=""https://user-images.githubusercontent.com/8237552/207720422-ea64ddad-1290-4cc3-819d-4b9829e56981.png"">. 100bp. <img width=""786"" alt=""100bp"" src=""https://user-images.githubusercontent.com/8237552/207713090-c669cdbf-544c-4190-ac6b-6c091123c551.png"">. 75bp. <img width=""786"" alt=""75bp"" src=""https://user-images.githubusercontent.com/8237552/207713065-cdb71b28-8d69-45df-81e3-53b2f1d3aecf.png"">. This is what my command looked like. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ -B /share/terra docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type WES \. --ref /share/terra/rsrc/hg38/ref/GCA_000001405.15_GRCh38_no_alt_plus_hs38d1_analysis_set.fna.gz \. --reads ./HG002.proc.bam \. --output_vcf HG002.over10dp.vcf.gz \. --output_gvcf HG002.over10dp.gvcf.gz \. --num_shards 8 \. --intermediate_results_dir ./dv_int_results \. --regions ../100bp/over10.cds.bed. ```. Thanks again for looking into this",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/586
https://github.com/google/deepvariant/issues/586:956,interoperability,distribut,distributions,956,"Hi @AndrewCarroll . I believe I have recreated this issue with HG002 from GIAB. This is what I did:. - Downloaded ftp://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/data/AshkenazimTrio/HG002_NA24385_son/OsloUniversityHospital_Exome/151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bam. - Converted the bam to fastq. - Made a 100bp and 75bp fastq. - Aligned all three fastqs (original-127bp, 100bp, 75bp) to https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/GCA_000001405.15_GRCh38_no_alt_plus_hs38d1_analysis_set.fna.gz. - Marked dups etc. - Ran deep variant 1.4 on 100bp bam, 75bp bam, and original bam using WES model. - Use bed file consisting of regions from the 100bp bam with depth over 10 intersected with CDS regions. . [over10.cds.bed.txt](https://github.com/google/deepvariant/files/10231884/over10.cds.bed.txt). This is how the distributions look. Original-127bp. <img width=""786"" alt=""original_127bp"" src=""https://user-images.githubusercontent.com/8237552/207720422-ea64ddad-1290-4cc3-819d-4b9829e56981.png"">. 100bp. <img width=""786"" alt=""100bp"" src=""https://user-images.githubusercontent.com/8237552/207713090-c669cdbf-544c-4190-ac6b-6c091123c551.png"">. 75bp. <img width=""786"" alt=""75bp"" src=""https://user-images.githubusercontent.com/8237552/207713065-cdb71b28-8d69-45df-81e3-53b2f1d3aecf.png"">. This is what my command looked like. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ -B /share/terra docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type WES \. --ref /share/terra/rsrc/hg38/ref/GCA_000001405.15_GRCh38_no_alt_plus_hs38d1_analysis_set.fna.gz \. --reads ./HG002.proc.bam \. --output_vcf HG002.over10dp.vcf.gz \. --output_gvcf HG002.over10dp.gvcf.gz \. --num_shards 8 \. --intermediate_results_dir ./dv_int_results \. --regions ../100bp/over10.cds.bed. ```. Thanks again for looking into this",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/586
https://github.com/google/deepvariant/issues/586:1526,interoperability,share,share,1526,"Hi @AndrewCarroll . I believe I have recreated this issue with HG002 from GIAB. This is what I did:. - Downloaded ftp://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/data/AshkenazimTrio/HG002_NA24385_son/OsloUniversityHospital_Exome/151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bam. - Converted the bam to fastq. - Made a 100bp and 75bp fastq. - Aligned all three fastqs (original-127bp, 100bp, 75bp) to https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/GCA_000001405.15_GRCh38_no_alt_plus_hs38d1_analysis_set.fna.gz. - Marked dups etc. - Ran deep variant 1.4 on 100bp bam, 75bp bam, and original bam using WES model. - Use bed file consisting of regions from the 100bp bam with depth over 10 intersected with CDS regions. . [over10.cds.bed.txt](https://github.com/google/deepvariant/files/10231884/over10.cds.bed.txt). This is how the distributions look. Original-127bp. <img width=""786"" alt=""original_127bp"" src=""https://user-images.githubusercontent.com/8237552/207720422-ea64ddad-1290-4cc3-819d-4b9829e56981.png"">. 100bp. <img width=""786"" alt=""100bp"" src=""https://user-images.githubusercontent.com/8237552/207713090-c669cdbf-544c-4190-ac6b-6c091123c551.png"">. 75bp. <img width=""786"" alt=""75bp"" src=""https://user-images.githubusercontent.com/8237552/207713065-cdb71b28-8d69-45df-81e3-53b2f1d3aecf.png"">. This is what my command looked like. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ -B /share/terra docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type WES \. --ref /share/terra/rsrc/hg38/ref/GCA_000001405.15_GRCh38_no_alt_plus_hs38d1_analysis_set.fna.gz \. --reads ./HG002.proc.bam \. --output_vcf HG002.over10dp.vcf.gz \. --output_gvcf HG002.over10dp.gvcf.gz \. --num_shards 8 \. --intermediate_results_dir ./dv_int_results \. --regions ../100bp/over10.cds.bed. ```. Thanks again for looking into this",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/586
https://github.com/google/deepvariant/issues/586:1642,interoperability,share,share,1642,"Hi @AndrewCarroll . I believe I have recreated this issue with HG002 from GIAB. This is what I did:. - Downloaded ftp://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/data/AshkenazimTrio/HG002_NA24385_son/OsloUniversityHospital_Exome/151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bam. - Converted the bam to fastq. - Made a 100bp and 75bp fastq. - Aligned all three fastqs (original-127bp, 100bp, 75bp) to https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/GCA_000001405.15_GRCh38_no_alt_plus_hs38d1_analysis_set.fna.gz. - Marked dups etc. - Ran deep variant 1.4 on 100bp bam, 75bp bam, and original bam using WES model. - Use bed file consisting of regions from the 100bp bam with depth over 10 intersected with CDS regions. . [over10.cds.bed.txt](https://github.com/google/deepvariant/files/10231884/over10.cds.bed.txt). This is how the distributions look. Original-127bp. <img width=""786"" alt=""original_127bp"" src=""https://user-images.githubusercontent.com/8237552/207720422-ea64ddad-1290-4cc3-819d-4b9829e56981.png"">. 100bp. <img width=""786"" alt=""100bp"" src=""https://user-images.githubusercontent.com/8237552/207713090-c669cdbf-544c-4190-ac6b-6c091123c551.png"">. 75bp. <img width=""786"" alt=""75bp"" src=""https://user-images.githubusercontent.com/8237552/207713065-cdb71b28-8d69-45df-81e3-53b2f1d3aecf.png"">. This is what my command looked like. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ -B /share/terra docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type WES \. --ref /share/terra/rsrc/hg38/ref/GCA_000001405.15_GRCh38_no_alt_plus_hs38d1_analysis_set.fna.gz \. --reads ./HG002.proc.bam \. --output_vcf HG002.over10dp.vcf.gz \. --output_gvcf HG002.over10dp.gvcf.gz \. --num_shards 8 \. --intermediate_results_dir ./dv_int_results \. --regions ../100bp/over10.cds.bed. ```. Thanks again for looking into this",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/586
https://github.com/google/deepvariant/issues/586:731,security,model,model,731,"Hi @AndrewCarroll . I believe I have recreated this issue with HG002 from GIAB. This is what I did:. - Downloaded ftp://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/data/AshkenazimTrio/HG002_NA24385_son/OsloUniversityHospital_Exome/151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bam. - Converted the bam to fastq. - Made a 100bp and 75bp fastq. - Aligned all three fastqs (original-127bp, 100bp, 75bp) to https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/GCA_000001405.15_GRCh38_no_alt_plus_hs38d1_analysis_set.fna.gz. - Marked dups etc. - Ran deep variant 1.4 on 100bp bam, 75bp bam, and original bam using WES model. - Use bed file consisting of regions from the 100bp bam with depth over 10 intersected with CDS regions. . [over10.cds.bed.txt](https://github.com/google/deepvariant/files/10231884/over10.cds.bed.txt). This is how the distributions look. Original-127bp. <img width=""786"" alt=""original_127bp"" src=""https://user-images.githubusercontent.com/8237552/207720422-ea64ddad-1290-4cc3-819d-4b9829e56981.png"">. 100bp. <img width=""786"" alt=""100bp"" src=""https://user-images.githubusercontent.com/8237552/207713090-c669cdbf-544c-4190-ac6b-6c091123c551.png"">. 75bp. <img width=""786"" alt=""75bp"" src=""https://user-images.githubusercontent.com/8237552/207713065-cdb71b28-8d69-45df-81e3-53b2f1d3aecf.png"">. This is what my command looked like. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ -B /share/terra docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type WES \. --ref /share/terra/rsrc/hg38/ref/GCA_000001405.15_GRCh38_no_alt_plus_hs38d1_analysis_set.fna.gz \. --reads ./HG002.proc.bam \. --output_vcf HG002.over10dp.vcf.gz \. --output_gvcf HG002.over10dp.gvcf.gz \. --num_shards 8 \. --intermediate_results_dir ./dv_int_results \. --regions ../100bp/over10.cds.bed. ```. Thanks again for looking into this",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/586
https://github.com/google/deepvariant/issues/586:124,testability,trace,trace,124,"Hi @AndrewCarroll . I believe I have recreated this issue with HG002 from GIAB. This is what I did:. - Downloaded ftp://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/data/AshkenazimTrio/HG002_NA24385_son/OsloUniversityHospital_Exome/151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bam. - Converted the bam to fastq. - Made a 100bp and 75bp fastq. - Aligned all three fastqs (original-127bp, 100bp, 75bp) to https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/GCA_000001405.15_GRCh38_no_alt_plus_hs38d1_analysis_set.fna.gz. - Marked dups etc. - Ran deep variant 1.4 on 100bp bam, 75bp bam, and original bam using WES model. - Use bed file consisting of regions from the 100bp bam with depth over 10 intersected with CDS regions. . [over10.cds.bed.txt](https://github.com/google/deepvariant/files/10231884/over10.cds.bed.txt). This is how the distributions look. Original-127bp. <img width=""786"" alt=""original_127bp"" src=""https://user-images.githubusercontent.com/8237552/207720422-ea64ddad-1290-4cc3-819d-4b9829e56981.png"">. 100bp. <img width=""786"" alt=""100bp"" src=""https://user-images.githubusercontent.com/8237552/207713090-c669cdbf-544c-4190-ac6b-6c091123c551.png"">. 75bp. <img width=""786"" alt=""75bp"" src=""https://user-images.githubusercontent.com/8237552/207713065-cdb71b28-8d69-45df-81e3-53b2f1d3aecf.png"">. This is what my command looked like. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ -B /share/terra docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type WES \. --ref /share/terra/rsrc/hg38/ref/GCA_000001405.15_GRCh38_no_alt_plus_hs38d1_analysis_set.fna.gz \. --reads ./HG002.proc.bam \. --output_vcf HG002.over10dp.vcf.gz \. --output_gvcf HG002.over10dp.gvcf.gz \. --num_shards 8 \. --intermediate_results_dir ./dv_int_results \. --regions ../100bp/over10.cds.bed. ```. Thanks again for looking into this",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/586
https://github.com/google/deepvariant/issues/586:753,usability,consist,consisting,753,"Hi @AndrewCarroll . I believe I have recreated this issue with HG002 from GIAB. This is what I did:. - Downloaded ftp://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/data/AshkenazimTrio/HG002_NA24385_son/OsloUniversityHospital_Exome/151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bam. - Converted the bam to fastq. - Made a 100bp and 75bp fastq. - Aligned all three fastqs (original-127bp, 100bp, 75bp) to https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/GCA_000001405.15_GRCh38_no_alt_plus_hs38d1_analysis_set.fna.gz. - Marked dups etc. - Ran deep variant 1.4 on 100bp bam, 75bp bam, and original bam using WES model. - Use bed file consisting of regions from the 100bp bam with depth over 10 intersected with CDS regions. . [over10.cds.bed.txt](https://github.com/google/deepvariant/files/10231884/over10.cds.bed.txt). This is how the distributions look. Original-127bp. <img width=""786"" alt=""original_127bp"" src=""https://user-images.githubusercontent.com/8237552/207720422-ea64ddad-1290-4cc3-819d-4b9829e56981.png"">. 100bp. <img width=""786"" alt=""100bp"" src=""https://user-images.githubusercontent.com/8237552/207713090-c669cdbf-544c-4190-ac6b-6c091123c551.png"">. 75bp. <img width=""786"" alt=""75bp"" src=""https://user-images.githubusercontent.com/8237552/207713065-cdb71b28-8d69-45df-81e3-53b2f1d3aecf.png"">. This is what my command looked like. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ -B /share/terra docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type WES \. --ref /share/terra/rsrc/hg38/ref/GCA_000001405.15_GRCh38_no_alt_plus_hs38d1_analysis_set.fna.gz \. --reads ./HG002.proc.bam \. --output_vcf HG002.over10dp.vcf.gz \. --output_gvcf HG002.over10dp.gvcf.gz \. --num_shards 8 \. --intermediate_results_dir ./dv_int_results \. --regions ../100bp/over10.cds.bed. ```. Thanks again for looking into this",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/586
https://github.com/google/deepvariant/issues/586:1043,usability,user,user-images,1043,"Hi @AndrewCarroll . I believe I have recreated this issue with HG002 from GIAB. This is what I did:. - Downloaded ftp://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/data/AshkenazimTrio/HG002_NA24385_son/OsloUniversityHospital_Exome/151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bam. - Converted the bam to fastq. - Made a 100bp and 75bp fastq. - Aligned all three fastqs (original-127bp, 100bp, 75bp) to https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/GCA_000001405.15_GRCh38_no_alt_plus_hs38d1_analysis_set.fna.gz. - Marked dups etc. - Ran deep variant 1.4 on 100bp bam, 75bp bam, and original bam using WES model. - Use bed file consisting of regions from the 100bp bam with depth over 10 intersected with CDS regions. . [over10.cds.bed.txt](https://github.com/google/deepvariant/files/10231884/over10.cds.bed.txt). This is how the distributions look. Original-127bp. <img width=""786"" alt=""original_127bp"" src=""https://user-images.githubusercontent.com/8237552/207720422-ea64ddad-1290-4cc3-819d-4b9829e56981.png"">. 100bp. <img width=""786"" alt=""100bp"" src=""https://user-images.githubusercontent.com/8237552/207713090-c669cdbf-544c-4190-ac6b-6c091123c551.png"">. 75bp. <img width=""786"" alt=""75bp"" src=""https://user-images.githubusercontent.com/8237552/207713065-cdb71b28-8d69-45df-81e3-53b2f1d3aecf.png"">. This is what my command looked like. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ -B /share/terra docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type WES \. --ref /share/terra/rsrc/hg38/ref/GCA_000001405.15_GRCh38_no_alt_plus_hs38d1_analysis_set.fna.gz \. --reads ./HG002.proc.bam \. --output_vcf HG002.over10dp.vcf.gz \. --output_gvcf HG002.over10dp.gvcf.gz \. --num_shards 8 \. --intermediate_results_dir ./dv_int_results \. --regions ../100bp/over10.cds.bed. ```. Thanks again for looking into this",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/586
https://github.com/google/deepvariant/issues/586:1188,usability,user,user-images,1188,"Hi @AndrewCarroll . I believe I have recreated this issue with HG002 from GIAB. This is what I did:. - Downloaded ftp://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/data/AshkenazimTrio/HG002_NA24385_son/OsloUniversityHospital_Exome/151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bam. - Converted the bam to fastq. - Made a 100bp and 75bp fastq. - Aligned all three fastqs (original-127bp, 100bp, 75bp) to https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/GCA_000001405.15_GRCh38_no_alt_plus_hs38d1_analysis_set.fna.gz. - Marked dups etc. - Ran deep variant 1.4 on 100bp bam, 75bp bam, and original bam using WES model. - Use bed file consisting of regions from the 100bp bam with depth over 10 intersected with CDS regions. . [over10.cds.bed.txt](https://github.com/google/deepvariant/files/10231884/over10.cds.bed.txt). This is how the distributions look. Original-127bp. <img width=""786"" alt=""original_127bp"" src=""https://user-images.githubusercontent.com/8237552/207720422-ea64ddad-1290-4cc3-819d-4b9829e56981.png"">. 100bp. <img width=""786"" alt=""100bp"" src=""https://user-images.githubusercontent.com/8237552/207713090-c669cdbf-544c-4190-ac6b-6c091123c551.png"">. 75bp. <img width=""786"" alt=""75bp"" src=""https://user-images.githubusercontent.com/8237552/207713065-cdb71b28-8d69-45df-81e3-53b2f1d3aecf.png"">. This is what my command looked like. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ -B /share/terra docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type WES \. --ref /share/terra/rsrc/hg38/ref/GCA_000001405.15_GRCh38_no_alt_plus_hs38d1_analysis_set.fna.gz \. --reads ./HG002.proc.bam \. --output_vcf HG002.over10dp.vcf.gz \. --output_gvcf HG002.over10dp.gvcf.gz \. --num_shards 8 \. --intermediate_results_dir ./dv_int_results \. --regions ../100bp/over10.cds.bed. ```. Thanks again for looking into this",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/586
https://github.com/google/deepvariant/issues/586:1331,usability,user,user-images,1331,"Hi @AndrewCarroll . I believe I have recreated this issue with HG002 from GIAB. This is what I did:. - Downloaded ftp://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/data/AshkenazimTrio/HG002_NA24385_son/OsloUniversityHospital_Exome/151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bam. - Converted the bam to fastq. - Made a 100bp and 75bp fastq. - Aligned all three fastqs (original-127bp, 100bp, 75bp) to https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/GCA_000001405.15_GRCh38_no_alt_plus_hs38d1_analysis_set.fna.gz. - Marked dups etc. - Ran deep variant 1.4 on 100bp bam, 75bp bam, and original bam using WES model. - Use bed file consisting of regions from the 100bp bam with depth over 10 intersected with CDS regions. . [over10.cds.bed.txt](https://github.com/google/deepvariant/files/10231884/over10.cds.bed.txt). This is how the distributions look. Original-127bp. <img width=""786"" alt=""original_127bp"" src=""https://user-images.githubusercontent.com/8237552/207720422-ea64ddad-1290-4cc3-819d-4b9829e56981.png"">. 100bp. <img width=""786"" alt=""100bp"" src=""https://user-images.githubusercontent.com/8237552/207713090-c669cdbf-544c-4190-ac6b-6c091123c551.png"">. 75bp. <img width=""786"" alt=""75bp"" src=""https://user-images.githubusercontent.com/8237552/207713065-cdb71b28-8d69-45df-81e3-53b2f1d3aecf.png"">. This is what my command looked like. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ -B /share/terra docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type WES \. --ref /share/terra/rsrc/hg38/ref/GCA_000001405.15_GRCh38_no_alt_plus_hs38d1_analysis_set.fna.gz \. --reads ./HG002.proc.bam \. --output_vcf HG002.over10dp.vcf.gz \. --output_gvcf HG002.over10dp.gvcf.gz \. --num_shards 8 \. --intermediate_results_dir ./dv_int_results \. --regions ../100bp/over10.cds.bed. ```. Thanks again for looking into this",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/586
https://github.com/google/deepvariant/issues/586:1443,usability,command,command,1443,"Hi @AndrewCarroll . I believe I have recreated this issue with HG002 from GIAB. This is what I did:. - Downloaded ftp://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/data/AshkenazimTrio/HG002_NA24385_son/OsloUniversityHospital_Exome/151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bam. - Converted the bam to fastq. - Made a 100bp and 75bp fastq. - Aligned all three fastqs (original-127bp, 100bp, 75bp) to https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/GCA_000001405.15_GRCh38_no_alt_plus_hs38d1_analysis_set.fna.gz. - Marked dups etc. - Ran deep variant 1.4 on 100bp bam, 75bp bam, and original bam using WES model. - Use bed file consisting of regions from the 100bp bam with depth over 10 intersected with CDS regions. . [over10.cds.bed.txt](https://github.com/google/deepvariant/files/10231884/over10.cds.bed.txt). This is how the distributions look. Original-127bp. <img width=""786"" alt=""original_127bp"" src=""https://user-images.githubusercontent.com/8237552/207720422-ea64ddad-1290-4cc3-819d-4b9829e56981.png"">. 100bp. <img width=""786"" alt=""100bp"" src=""https://user-images.githubusercontent.com/8237552/207713090-c669cdbf-544c-4190-ac6b-6c091123c551.png"">. 75bp. <img width=""786"" alt=""75bp"" src=""https://user-images.githubusercontent.com/8237552/207713065-cdb71b28-8d69-45df-81e3-53b2f1d3aecf.png"">. This is what my command looked like. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ -B /share/terra docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type WES \. --ref /share/terra/rsrc/hg38/ref/GCA_000001405.15_GRCh38_no_alt_plus_hs38d1_analysis_set.fna.gz \. --reads ./HG002.proc.bam \. --output_vcf HG002.over10dp.vcf.gz \. --output_gvcf HG002.over10dp.gvcf.gz \. --num_shards 8 \. --intermediate_results_dir ./dv_int_results \. --regions ../100bp/over10.cds.bed. ```. Thanks again for looking into this",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/586
https://github.com/google/deepvariant/issues/586:210,availability,replic,replicates,210,"Hi @JakeHagen . Thank you for this analysis. This is an interesting observation. I have been some progress on doing the same truncation for the broader exome data we have. It will be interesting to see if that replicates as well. Either way, the fact that you have generated this effect on public data will be very useful. It will be informative to see what factors we can do to isolate or mitigate the effect. We're going to do some experiments here. Thanks again,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/586
https://github.com/google/deepvariant/issues/586:68,deployability,observ,observation,68,"Hi @JakeHagen . Thank you for this analysis. This is an interesting observation. I have been some progress on doing the same truncation for the broader exome data we have. It will be interesting to see if that replicates as well. Either way, the fact that you have generated this effect on public data will be very useful. It will be informative to see what factors we can do to isolate or mitigate the effect. We're going to do some experiments here. Thanks again,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/586
https://github.com/google/deepvariant/issues/586:290,integrability,pub,public,290,"Hi @JakeHagen . Thank you for this analysis. This is an interesting observation. I have been some progress on doing the same truncation for the broader exome data we have. It will be interesting to see if that replicates as well. Either way, the fact that you have generated this effect on public data will be very useful. It will be informative to see what factors we can do to isolate or mitigate the effect. We're going to do some experiments here. Thanks again,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/586
https://github.com/google/deepvariant/issues/586:379,safety,isol,isolate,379,"Hi @JakeHagen . Thank you for this analysis. This is an interesting observation. I have been some progress on doing the same truncation for the broader exome data we have. It will be interesting to see if that replicates as well. Either way, the fact that you have generated this effect on public data will be very useful. It will be informative to see what factors we can do to isolate or mitigate the effect. We're going to do some experiments here. Thanks again,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/586
https://github.com/google/deepvariant/issues/586:379,security,iso,isolate,379,"Hi @JakeHagen . Thank you for this analysis. This is an interesting observation. I have been some progress on doing the same truncation for the broader exome data we have. It will be interesting to see if that replicates as well. Either way, the fact that you have generated this effect on public data will be very useful. It will be informative to see what factors we can do to isolate or mitigate the effect. We're going to do some experiments here. Thanks again,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/586
https://github.com/google/deepvariant/issues/586:68,testability,observ,observation,68,"Hi @JakeHagen . Thank you for this analysis. This is an interesting observation. I have been some progress on doing the same truncation for the broader exome data we have. It will be interesting to see if that replicates as well. Either way, the fact that you have generated this effect on public data will be very useful. It will be informative to see what factors we can do to isolate or mitigate the effect. We're going to do some experiments here. Thanks again,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/586
https://github.com/google/deepvariant/issues/586:379,testability,isol,isolate,379,"Hi @JakeHagen . Thank you for this analysis. This is an interesting observation. I have been some progress on doing the same truncation for the broader exome data we have. It will be interesting to see if that replicates as well. Either way, the fact that you have generated this effect on public data will be very useful. It will be informative to see what factors we can do to isolate or mitigate the effect. We're going to do some experiments here. Thanks again,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/586
https://github.com/google/deepvariant/issues/586:98,usability,progress,progress,98,"Hi @JakeHagen . Thank you for this analysis. This is an interesting observation. I have been some progress on doing the same truncation for the broader exome data we have. It will be interesting to see if that replicates as well. Either way, the fact that you have generated this effect on public data will be very useful. It will be informative to see what factors we can do to isolate or mitigate the effect. We're going to do some experiments here. Thanks again,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/586
https://github.com/google/deepvariant/issues/586:162,availability,replic,replicate,162,"Hi @JakeHagen . We may have identified an issue which could have affected very specifically exome runs with 100bp length (but not WGS). We have been able to both replicate your findings and train a model which seems to eliminate the effect on our replication. Would you be interested to run a with this custom model that we generated to confirm that it fixes your issue? If so, can you email awcarroll@google.com and I can send you both the model and instructions to run it. If this does seem to correct the issue and we can validate the fix, we will plan to push this out as a part of next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/586
https://github.com/google/deepvariant/issues/586:247,availability,replic,replication,247,"Hi @JakeHagen . We may have identified an issue which could have affected very specifically exome runs with 100bp length (but not WGS). We have been able to both replicate your findings and train a model which seems to eliminate the effect on our replication. Would you be interested to run a with this custom model that we generated to confirm that it fixes your issue? If so, can you email awcarroll@google.com and I can send you both the model and instructions to run it. If this does seem to correct the issue and we can validate the fix, we will plan to push this out as a part of next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/586
https://github.com/google/deepvariant/issues/586:591,deployability,releas,release,591,"Hi @JakeHagen . We may have identified an issue which could have affected very specifically exome runs with 100bp length (but not WGS). We have been able to both replicate your findings and train a model which seems to eliminate the effect on our replication. Would you be interested to run a with this custom model that we generated to confirm that it fixes your issue? If so, can you email awcarroll@google.com and I can send you both the model and instructions to run it. If this does seem to correct the issue and we can validate the fix, we will plan to push this out as a part of next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/586
https://github.com/google/deepvariant/issues/586:198,energy efficiency,model,model,198,"Hi @JakeHagen . We may have identified an issue which could have affected very specifically exome runs with 100bp length (but not WGS). We have been able to both replicate your findings and train a model which seems to eliminate the effect on our replication. Would you be interested to run a with this custom model that we generated to confirm that it fixes your issue? If so, can you email awcarroll@google.com and I can send you both the model and instructions to run it. If this does seem to correct the issue and we can validate the fix, we will plan to push this out as a part of next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/586
https://github.com/google/deepvariant/issues/586:310,energy efficiency,model,model,310,"Hi @JakeHagen . We may have identified an issue which could have affected very specifically exome runs with 100bp length (but not WGS). We have been able to both replicate your findings and train a model which seems to eliminate the effect on our replication. Would you be interested to run a with this custom model that we generated to confirm that it fixes your issue? If so, can you email awcarroll@google.com and I can send you both the model and instructions to run it. If this does seem to correct the issue and we can validate the fix, we will plan to push this out as a part of next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/586
https://github.com/google/deepvariant/issues/586:441,energy efficiency,model,model,441,"Hi @JakeHagen . We may have identified an issue which could have affected very specifically exome runs with 100bp length (but not WGS). We have been able to both replicate your findings and train a model which seems to eliminate the effect on our replication. Would you be interested to run a with this custom model that we generated to confirm that it fixes your issue? If so, can you email awcarroll@google.com and I can send you both the model and instructions to run it. If this does seem to correct the issue and we can validate the fix, we will plan to push this out as a part of next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/586
https://github.com/google/deepvariant/issues/586:79,interoperability,specif,specifically,79,"Hi @JakeHagen . We may have identified an issue which could have affected very specifically exome runs with 100bp length (but not WGS). We have been able to both replicate your findings and train a model which seems to eliminate the effect on our replication. Would you be interested to run a with this custom model that we generated to confirm that it fixes your issue? If so, can you email awcarroll@google.com and I can send you both the model and instructions to run it. If this does seem to correct the issue and we can validate the fix, we will plan to push this out as a part of next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/586
https://github.com/google/deepvariant/issues/586:483,reliability,doe,does,483,"Hi @JakeHagen . We may have identified an issue which could have affected very specifically exome runs with 100bp length (but not WGS). We have been able to both replicate your findings and train a model which seems to eliminate the effect on our replication. Would you be interested to run a with this custom model that we generated to confirm that it fixes your issue? If so, can you email awcarroll@google.com and I can send you both the model and instructions to run it. If this does seem to correct the issue and we can validate the fix, we will plan to push this out as a part of next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/586
https://github.com/google/deepvariant/issues/586:525,safety,valid,validate,525,"Hi @JakeHagen . We may have identified an issue which could have affected very specifically exome runs with 100bp length (but not WGS). We have been able to both replicate your findings and train a model which seems to eliminate the effect on our replication. Would you be interested to run a with this custom model that we generated to confirm that it fixes your issue? If so, can you email awcarroll@google.com and I can send you both the model and instructions to run it. If this does seem to correct the issue and we can validate the fix, we will plan to push this out as a part of next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/586
https://github.com/google/deepvariant/issues/586:28,security,ident,identified,28,"Hi @JakeHagen . We may have identified an issue which could have affected very specifically exome runs with 100bp length (but not WGS). We have been able to both replicate your findings and train a model which seems to eliminate the effect on our replication. Would you be interested to run a with this custom model that we generated to confirm that it fixes your issue? If so, can you email awcarroll@google.com and I can send you both the model and instructions to run it. If this does seem to correct the issue and we can validate the fix, we will plan to push this out as a part of next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/586
https://github.com/google/deepvariant/issues/586:198,security,model,model,198,"Hi @JakeHagen . We may have identified an issue which could have affected very specifically exome runs with 100bp length (but not WGS). We have been able to both replicate your findings and train a model which seems to eliminate the effect on our replication. Would you be interested to run a with this custom model that we generated to confirm that it fixes your issue? If so, can you email awcarroll@google.com and I can send you both the model and instructions to run it. If this does seem to correct the issue and we can validate the fix, we will plan to push this out as a part of next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/586
https://github.com/google/deepvariant/issues/586:310,security,model,model,310,"Hi @JakeHagen . We may have identified an issue which could have affected very specifically exome runs with 100bp length (but not WGS). We have been able to both replicate your findings and train a model which seems to eliminate the effect on our replication. Would you be interested to run a with this custom model that we generated to confirm that it fixes your issue? If so, can you email awcarroll@google.com and I can send you both the model and instructions to run it. If this does seem to correct the issue and we can validate the fix, we will plan to push this out as a part of next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/586
https://github.com/google/deepvariant/issues/586:441,security,model,model,441,"Hi @JakeHagen . We may have identified an issue which could have affected very specifically exome runs with 100bp length (but not WGS). We have been able to both replicate your findings and train a model which seems to eliminate the effect on our replication. Would you be interested to run a with this custom model that we generated to confirm that it fixes your issue? If so, can you email awcarroll@google.com and I can send you both the model and instructions to run it. If this does seem to correct the issue and we can validate the fix, we will plan to push this out as a part of next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/586
https://github.com/google/deepvariant/issues/586:525,security,validat,validate,525,"Hi @JakeHagen . We may have identified an issue which could have affected very specifically exome runs with 100bp length (but not WGS). We have been able to both replicate your findings and train a model which seems to eliminate the effect on our replication. Would you be interested to run a with this custom model that we generated to confirm that it fixes your issue? If so, can you email awcarroll@google.com and I can send you both the model and instructions to run it. If this does seem to correct the issue and we can validate the fix, we will plan to push this out as a part of next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/586
https://github.com/google/deepvariant/issues/586:551,testability,plan,plan,551,"Hi @JakeHagen . We may have identified an issue which could have affected very specifically exome runs with 100bp length (but not WGS). We have been able to both replicate your findings and train a model which seems to eliminate the effect on our replication. Would you be interested to run a with this custom model that we generated to confirm that it fixes your issue? If so, can you email awcarroll@google.com and I can send you both the model and instructions to run it. If this does seem to correct the issue and we can validate the fix, we will plan to push this out as a part of next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/586
https://github.com/google/deepvariant/issues/586:303,usability,custom,custom,303,"Hi @JakeHagen . We may have identified an issue which could have affected very specifically exome runs with 100bp length (but not WGS). We have been able to both replicate your findings and train a model which seems to eliminate the effect on our replication. Would you be interested to run a with this custom model that we generated to confirm that it fixes your issue? If so, can you email awcarroll@google.com and I can send you both the model and instructions to run it. If this does seem to correct the issue and we can validate the fix, we will plan to push this out as a part of next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/586
https://github.com/google/deepvariant/issues/586:337,usability,confirm,confirm,337,"Hi @JakeHagen . We may have identified an issue which could have affected very specifically exome runs with 100bp length (but not WGS). We have been able to both replicate your findings and train a model which seems to eliminate the effect on our replication. Would you be interested to run a with this custom model that we generated to confirm that it fixes your issue? If so, can you email awcarroll@google.com and I can send you both the model and instructions to run it. If this does seem to correct the issue and we can validate the fix, we will plan to push this out as a part of next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/586
https://github.com/google/deepvariant/issues/586:29,energy efficiency,model,model,29,That's great. I will try the model for sure. I will reach out shortly by email. Thanks!,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/586
https://github.com/google/deepvariant/issues/586:29,security,model,model,29,That's great. I will try the model for sure. I will reach out shortly by email. Thanks!,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/586
https://github.com/google/deepvariant/issues/586:87,deployability,releas,releases,87,"Hi @JakeHagen ,. this problem should be fixed in https://github.com/google/deepvariant/releases/tag/v1.5.0. And, starting in this release, we added the VCF stats plots to https://github.com/google/deepvariant/blob/r1.5/docs/metrics.md and https://github.com/google/deepvariant/blob/r1.5/docs/metrics-deeptrio.md. We're glad to see that @MariaNattestad 's VCF stats tool was useful for you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/586
https://github.com/google/deepvariant/issues/586:130,deployability,releas,release,130,"Hi @JakeHagen ,. this problem should be fixed in https://github.com/google/deepvariant/releases/tag/v1.5.0. And, starting in this release, we added the VCF stats plots to https://github.com/google/deepvariant/blob/r1.5/docs/metrics.md and https://github.com/google/deepvariant/blob/r1.5/docs/metrics-deeptrio.md. We're glad to see that @MariaNattestad 's VCF stats tool was useful for you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/586
https://github.com/google/deepvariant/issues/586:365,usability,tool,tool,365,"Hi @JakeHagen ,. this problem should be fixed in https://github.com/google/deepvariant/releases/tag/v1.5.0. And, starting in this release, we added the VCF stats plots to https://github.com/google/deepvariant/blob/r1.5/docs/metrics.md and https://github.com/google/deepvariant/blob/r1.5/docs/metrics-deeptrio.md. We're glad to see that @MariaNattestad 's VCF stats tool was useful for you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/586
https://github.com/google/deepvariant/issues/587:376,availability,down,down,376,"Hi @Sh1von ,. Thanks for reporting this. Currently that's how our pipeline is designed unfortunately. We don't automatically resume. However, if make_examples and call_variants has completed successfully, you can manually run postprocess_variants separately. The way to figure out the commands to use : You can run your original command with `--dry_run`, and then it'll break down the sequence of commands you need. Then, you can manually rerun the postprocess_variants command. I understand that being able to resume can be a useful feature. To automatically detect existing data robustly will be challenging, because we don't know if there might be corrupted data. But, I think it'll be possible to add a flag for users to skip steps. I will think about it a bit more. For now, please try manually re-run postprocess_variants, and see if that works for you. Sorry for the inconvenience!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/587
https://github.com/google/deepvariant/issues/587:581,availability,robust,robustly,581,"Hi @Sh1von ,. Thanks for reporting this. Currently that's how our pipeline is designed unfortunately. We don't automatically resume. However, if make_examples and call_variants has completed successfully, you can manually run postprocess_variants separately. The way to figure out the commands to use : You can run your original command with `--dry_run`, and then it'll break down the sequence of commands you need. Then, you can manually rerun the postprocess_variants command. I understand that being able to resume can be a useful feature. To automatically detect existing data robustly will be challenging, because we don't know if there might be corrupted data. But, I think it'll be possible to add a flag for users to skip steps. I will think about it a bit more. For now, please try manually re-run postprocess_variants, and see if that works for you. Sorry for the inconvenience!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/587
https://github.com/google/deepvariant/issues/587:66,deployability,pipelin,pipeline,66,"Hi @Sh1von ,. Thanks for reporting this. Currently that's how our pipeline is designed unfortunately. We don't automatically resume. However, if make_examples and call_variants has completed successfully, you can manually run postprocess_variants separately. The way to figure out the commands to use : You can run your original command with `--dry_run`, and then it'll break down the sequence of commands you need. Then, you can manually rerun the postprocess_variants command. I understand that being able to resume can be a useful feature. To automatically detect existing data robustly will be challenging, because we don't know if there might be corrupted data. But, I think it'll be possible to add a flag for users to skip steps. I will think about it a bit more. For now, please try manually re-run postprocess_variants, and see if that works for you. Sorry for the inconvenience!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/587
https://github.com/google/deepvariant/issues/587:111,deployability,automat,automatically,111,"Hi @Sh1von ,. Thanks for reporting this. Currently that's how our pipeline is designed unfortunately. We don't automatically resume. However, if make_examples and call_variants has completed successfully, you can manually run postprocess_variants separately. The way to figure out the commands to use : You can run your original command with `--dry_run`, and then it'll break down the sequence of commands you need. Then, you can manually rerun the postprocess_variants command. I understand that being able to resume can be a useful feature. To automatically detect existing data robustly will be challenging, because we don't know if there might be corrupted data. But, I think it'll be possible to add a flag for users to skip steps. I will think about it a bit more. For now, please try manually re-run postprocess_variants, and see if that works for you. Sorry for the inconvenience!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/587
https://github.com/google/deepvariant/issues/587:546,deployability,automat,automatically,546,"Hi @Sh1von ,. Thanks for reporting this. Currently that's how our pipeline is designed unfortunately. We don't automatically resume. However, if make_examples and call_variants has completed successfully, you can manually run postprocess_variants separately. The way to figure out the commands to use : You can run your original command with `--dry_run`, and then it'll break down the sequence of commands you need. Then, you can manually rerun the postprocess_variants command. I understand that being able to resume can be a useful feature. To automatically detect existing data robustly will be challenging, because we don't know if there might be corrupted data. But, I think it'll be possible to add a flag for users to skip steps. I will think about it a bit more. For now, please try manually re-run postprocess_variants, and see if that works for you. Sorry for the inconvenience!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/587
https://github.com/google/deepvariant/issues/587:41,energy efficiency,Current,Currently,41,"Hi @Sh1von ,. Thanks for reporting this. Currently that's how our pipeline is designed unfortunately. We don't automatically resume. However, if make_examples and call_variants has completed successfully, you can manually run postprocess_variants separately. The way to figure out the commands to use : You can run your original command with `--dry_run`, and then it'll break down the sequence of commands you need. Then, you can manually rerun the postprocess_variants command. I understand that being able to resume can be a useful feature. To automatically detect existing data robustly will be challenging, because we don't know if there might be corrupted data. But, I think it'll be possible to add a flag for users to skip steps. I will think about it a bit more. For now, please try manually re-run postprocess_variants, and see if that works for you. Sorry for the inconvenience!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/587
https://github.com/google/deepvariant/issues/587:66,integrability,pipelin,pipeline,66,"Hi @Sh1von ,. Thanks for reporting this. Currently that's how our pipeline is designed unfortunately. We don't automatically resume. However, if make_examples and call_variants has completed successfully, you can manually run postprocess_variants separately. The way to figure out the commands to use : You can run your original command with `--dry_run`, and then it'll break down the sequence of commands you need. Then, you can manually rerun the postprocess_variants command. I understand that being able to resume can be a useful feature. To automatically detect existing data robustly will be challenging, because we don't know if there might be corrupted data. But, I think it'll be possible to add a flag for users to skip steps. I will think about it a bit more. For now, please try manually re-run postprocess_variants, and see if that works for you. Sorry for the inconvenience!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/587
https://github.com/google/deepvariant/issues/587:581,reliability,robust,robustly,581,"Hi @Sh1von ,. Thanks for reporting this. Currently that's how our pipeline is designed unfortunately. We don't automatically resume. However, if make_examples and call_variants has completed successfully, you can manually run postprocess_variants separately. The way to figure out the commands to use : You can run your original command with `--dry_run`, and then it'll break down the sequence of commands you need. Then, you can manually rerun the postprocess_variants command. I understand that being able to resume can be a useful feature. To automatically detect existing data robustly will be challenging, because we don't know if there might be corrupted data. But, I think it'll be possible to add a flag for users to skip steps. I will think about it a bit more. For now, please try manually re-run postprocess_variants, and see if that works for you. Sorry for the inconvenience!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/587
https://github.com/google/deepvariant/issues/587:181,safety,compl,completed,181,"Hi @Sh1von ,. Thanks for reporting this. Currently that's how our pipeline is designed unfortunately. We don't automatically resume. However, if make_examples and call_variants has completed successfully, you can manually run postprocess_variants separately. The way to figure out the commands to use : You can run your original command with `--dry_run`, and then it'll break down the sequence of commands you need. Then, you can manually rerun the postprocess_variants command. I understand that being able to resume can be a useful feature. To automatically detect existing data robustly will be challenging, because we don't know if there might be corrupted data. But, I think it'll be possible to add a flag for users to skip steps. I will think about it a bit more. For now, please try manually re-run postprocess_variants, and see if that works for you. Sorry for the inconvenience!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/587
https://github.com/google/deepvariant/issues/587:560,safety,detect,detect,560,"Hi @Sh1von ,. Thanks for reporting this. Currently that's how our pipeline is designed unfortunately. We don't automatically resume. However, if make_examples and call_variants has completed successfully, you can manually run postprocess_variants separately. The way to figure out the commands to use : You can run your original command with `--dry_run`, and then it'll break down the sequence of commands you need. Then, you can manually rerun the postprocess_variants command. I understand that being able to resume can be a useful feature. To automatically detect existing data robustly will be challenging, because we don't know if there might be corrupted data. But, I think it'll be possible to add a flag for users to skip steps. I will think about it a bit more. For now, please try manually re-run postprocess_variants, and see if that works for you. Sorry for the inconvenience!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/587
https://github.com/google/deepvariant/issues/587:581,safety,robust,robustly,581,"Hi @Sh1von ,. Thanks for reporting this. Currently that's how our pipeline is designed unfortunately. We don't automatically resume. However, if make_examples and call_variants has completed successfully, you can manually run postprocess_variants separately. The way to figure out the commands to use : You can run your original command with `--dry_run`, and then it'll break down the sequence of commands you need. Then, you can manually rerun the postprocess_variants command. I understand that being able to resume can be a useful feature. To automatically detect existing data robustly will be challenging, because we don't know if there might be corrupted data. But, I think it'll be possible to add a flag for users to skip steps. I will think about it a bit more. For now, please try manually re-run postprocess_variants, and see if that works for you. Sorry for the inconvenience!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/587
https://github.com/google/deepvariant/issues/587:181,security,compl,completed,181,"Hi @Sh1von ,. Thanks for reporting this. Currently that's how our pipeline is designed unfortunately. We don't automatically resume. However, if make_examples and call_variants has completed successfully, you can manually run postprocess_variants separately. The way to figure out the commands to use : You can run your original command with `--dry_run`, and then it'll break down the sequence of commands you need. Then, you can manually rerun the postprocess_variants command. I understand that being able to resume can be a useful feature. To automatically detect existing data robustly will be challenging, because we don't know if there might be corrupted data. But, I think it'll be possible to add a flag for users to skip steps. I will think about it a bit more. For now, please try manually re-run postprocess_variants, and see if that works for you. Sorry for the inconvenience!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/587
https://github.com/google/deepvariant/issues/587:560,security,detect,detect,560,"Hi @Sh1von ,. Thanks for reporting this. Currently that's how our pipeline is designed unfortunately. We don't automatically resume. However, if make_examples and call_variants has completed successfully, you can manually run postprocess_variants separately. The way to figure out the commands to use : You can run your original command with `--dry_run`, and then it'll break down the sequence of commands you need. Then, you can manually rerun the postprocess_variants command. I understand that being able to resume can be a useful feature. To automatically detect existing data robustly will be challenging, because we don't know if there might be corrupted data. But, I think it'll be possible to add a flag for users to skip steps. I will think about it a bit more. For now, please try manually re-run postprocess_variants, and see if that works for you. Sorry for the inconvenience!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/587
https://github.com/google/deepvariant/issues/587:111,testability,automat,automatically,111,"Hi @Sh1von ,. Thanks for reporting this. Currently that's how our pipeline is designed unfortunately. We don't automatically resume. However, if make_examples and call_variants has completed successfully, you can manually run postprocess_variants separately. The way to figure out the commands to use : You can run your original command with `--dry_run`, and then it'll break down the sequence of commands you need. Then, you can manually rerun the postprocess_variants command. I understand that being able to resume can be a useful feature. To automatically detect existing data robustly will be challenging, because we don't know if there might be corrupted data. But, I think it'll be possible to add a flag for users to skip steps. I will think about it a bit more. For now, please try manually re-run postprocess_variants, and see if that works for you. Sorry for the inconvenience!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/587
https://github.com/google/deepvariant/issues/587:481,testability,understand,understand,481,"Hi @Sh1von ,. Thanks for reporting this. Currently that's how our pipeline is designed unfortunately. We don't automatically resume. However, if make_examples and call_variants has completed successfully, you can manually run postprocess_variants separately. The way to figure out the commands to use : You can run your original command with `--dry_run`, and then it'll break down the sequence of commands you need. Then, you can manually rerun the postprocess_variants command. I understand that being able to resume can be a useful feature. To automatically detect existing data robustly will be challenging, because we don't know if there might be corrupted data. But, I think it'll be possible to add a flag for users to skip steps. I will think about it a bit more. For now, please try manually re-run postprocess_variants, and see if that works for you. Sorry for the inconvenience!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/587
https://github.com/google/deepvariant/issues/587:546,testability,automat,automatically,546,"Hi @Sh1von ,. Thanks for reporting this. Currently that's how our pipeline is designed unfortunately. We don't automatically resume. However, if make_examples and call_variants has completed successfully, you can manually run postprocess_variants separately. The way to figure out the commands to use : You can run your original command with `--dry_run`, and then it'll break down the sequence of commands you need. Then, you can manually rerun the postprocess_variants command. I understand that being able to resume can be a useful feature. To automatically detect existing data robustly will be challenging, because we don't know if there might be corrupted data. But, I think it'll be possible to add a flag for users to skip steps. I will think about it a bit more. For now, please try manually re-run postprocess_variants, and see if that works for you. Sorry for the inconvenience!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/587
https://github.com/google/deepvariant/issues/587:125,usability,resum,resume,125,"Hi @Sh1von ,. Thanks for reporting this. Currently that's how our pipeline is designed unfortunately. We don't automatically resume. However, if make_examples and call_variants has completed successfully, you can manually run postprocess_variants separately. The way to figure out the commands to use : You can run your original command with `--dry_run`, and then it'll break down the sequence of commands you need. Then, you can manually rerun the postprocess_variants command. I understand that being able to resume can be a useful feature. To automatically detect existing data robustly will be challenging, because we don't know if there might be corrupted data. But, I think it'll be possible to add a flag for users to skip steps. I will think about it a bit more. For now, please try manually re-run postprocess_variants, and see if that works for you. Sorry for the inconvenience!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/587
https://github.com/google/deepvariant/issues/587:285,usability,command,commands,285,"Hi @Sh1von ,. Thanks for reporting this. Currently that's how our pipeline is designed unfortunately. We don't automatically resume. However, if make_examples and call_variants has completed successfully, you can manually run postprocess_variants separately. The way to figure out the commands to use : You can run your original command with `--dry_run`, and then it'll break down the sequence of commands you need. Then, you can manually rerun the postprocess_variants command. I understand that being able to resume can be a useful feature. To automatically detect existing data robustly will be challenging, because we don't know if there might be corrupted data. But, I think it'll be possible to add a flag for users to skip steps. I will think about it a bit more. For now, please try manually re-run postprocess_variants, and see if that works for you. Sorry for the inconvenience!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/587
https://github.com/google/deepvariant/issues/587:329,usability,command,command,329,"Hi @Sh1von ,. Thanks for reporting this. Currently that's how our pipeline is designed unfortunately. We don't automatically resume. However, if make_examples and call_variants has completed successfully, you can manually run postprocess_variants separately. The way to figure out the commands to use : You can run your original command with `--dry_run`, and then it'll break down the sequence of commands you need. Then, you can manually rerun the postprocess_variants command. I understand that being able to resume can be a useful feature. To automatically detect existing data robustly will be challenging, because we don't know if there might be corrupted data. But, I think it'll be possible to add a flag for users to skip steps. I will think about it a bit more. For now, please try manually re-run postprocess_variants, and see if that works for you. Sorry for the inconvenience!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/587
https://github.com/google/deepvariant/issues/587:397,usability,command,commands,397,"Hi @Sh1von ,. Thanks for reporting this. Currently that's how our pipeline is designed unfortunately. We don't automatically resume. However, if make_examples and call_variants has completed successfully, you can manually run postprocess_variants separately. The way to figure out the commands to use : You can run your original command with `--dry_run`, and then it'll break down the sequence of commands you need. Then, you can manually rerun the postprocess_variants command. I understand that being able to resume can be a useful feature. To automatically detect existing data robustly will be challenging, because we don't know if there might be corrupted data. But, I think it'll be possible to add a flag for users to skip steps. I will think about it a bit more. For now, please try manually re-run postprocess_variants, and see if that works for you. Sorry for the inconvenience!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/587
https://github.com/google/deepvariant/issues/587:470,usability,command,command,470,"Hi @Sh1von ,. Thanks for reporting this. Currently that's how our pipeline is designed unfortunately. We don't automatically resume. However, if make_examples and call_variants has completed successfully, you can manually run postprocess_variants separately. The way to figure out the commands to use : You can run your original command with `--dry_run`, and then it'll break down the sequence of commands you need. Then, you can manually rerun the postprocess_variants command. I understand that being able to resume can be a useful feature. To automatically detect existing data robustly will be challenging, because we don't know if there might be corrupted data. But, I think it'll be possible to add a flag for users to skip steps. I will think about it a bit more. For now, please try manually re-run postprocess_variants, and see if that works for you. Sorry for the inconvenience!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/587
https://github.com/google/deepvariant/issues/587:511,usability,resum,resume,511,"Hi @Sh1von ,. Thanks for reporting this. Currently that's how our pipeline is designed unfortunately. We don't automatically resume. However, if make_examples and call_variants has completed successfully, you can manually run postprocess_variants separately. The way to figure out the commands to use : You can run your original command with `--dry_run`, and then it'll break down the sequence of commands you need. Then, you can manually rerun the postprocess_variants command. I understand that being able to resume can be a useful feature. To automatically detect existing data robustly will be challenging, because we don't know if there might be corrupted data. But, I think it'll be possible to add a flag for users to skip steps. I will think about it a bit more. For now, please try manually re-run postprocess_variants, and see if that works for you. Sorry for the inconvenience!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/587
https://github.com/google/deepvariant/issues/587:716,usability,user,users,716,"Hi @Sh1von ,. Thanks for reporting this. Currently that's how our pipeline is designed unfortunately. We don't automatically resume. However, if make_examples and call_variants has completed successfully, you can manually run postprocess_variants separately. The way to figure out the commands to use : You can run your original command with `--dry_run`, and then it'll break down the sequence of commands you need. Then, you can manually rerun the postprocess_variants command. I understand that being able to resume can be a useful feature. To automatically detect existing data robustly will be challenging, because we don't know if there might be corrupted data. But, I think it'll be possible to add a flag for users to skip steps. I will think about it a bit more. For now, please try manually re-run postprocess_variants, and see if that works for you. Sorry for the inconvenience!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/587
https://github.com/google/deepvariant/issues/588:16,availability,error,error,16,"@zivlang ,. The error says: `ValueError: NOT_FOUND: Could not open /input/1115492_23181_0_0.cram`. Can you please do `ls /input/1115492_23181_0_0.cram` and see if the file exists in this file path?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/588
https://github.com/google/deepvariant/issues/588:16,performance,error,error,16,"@zivlang ,. The error says: `ValueError: NOT_FOUND: Could not open /input/1115492_23181_0_0.cram`. Can you please do `ls /input/1115492_23181_0_0.cram` and see if the file exists in this file path?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/588
https://github.com/google/deepvariant/issues/588:16,safety,error,error,16,"@zivlang ,. The error says: `ValueError: NOT_FOUND: Could not open /input/1115492_23181_0_0.cram`. Can you please do `ls /input/1115492_23181_0_0.cram` and see if the file exists in this file path?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/588
https://github.com/google/deepvariant/issues/588:68,safety,input,input,68,"@zivlang ,. The error says: `ValueError: NOT_FOUND: Could not open /input/1115492_23181_0_0.cram`. Can you please do `ls /input/1115492_23181_0_0.cram` and see if the file exists in this file path?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/588
https://github.com/google/deepvariant/issues/588:122,safety,input,input,122,"@zivlang ,. The error says: `ValueError: NOT_FOUND: Could not open /input/1115492_23181_0_0.cram`. Can you please do `ls /input/1115492_23181_0_0.cram` and see if the file exists in this file path?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/588
https://github.com/google/deepvariant/issues/588:16,usability,error,error,16,"@zivlang ,. The error says: `ValueError: NOT_FOUND: Could not open /input/1115492_23181_0_0.cram`. Can you please do `ls /input/1115492_23181_0_0.cram` and see if the file exists in this file path?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/588
https://github.com/google/deepvariant/issues/588:68,usability,input,input,68,"@zivlang ,. The error says: `ValueError: NOT_FOUND: Could not open /input/1115492_23181_0_0.cram`. Can you please do `ls /input/1115492_23181_0_0.cram` and see if the file exists in this file path?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/588
https://github.com/google/deepvariant/issues/588:122,usability,input,input,122,"@zivlang ,. The error says: `ValueError: NOT_FOUND: Could not open /input/1115492_23181_0_0.cram`. Can you please do `ls /input/1115492_23181_0_0.cram` and see if the file exists in this file path?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/588
https://github.com/google/deepvariant/issues/588:3,reliability,doe,does,3,It does:. `sudo ls -l input/`. `total 22167604`. `-rw------- 1 root root 22699626496 Nov 21 07:08 1115492_23181_0_0.cram`,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/588
https://github.com/google/deepvariant/issues/588:22,safety,input,input,22,It does:. `sudo ls -l input/`. `total 22167604`. `-rw------- 1 root root 22699626496 Nov 21 07:08 1115492_23181_0_0.cram`,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/588
https://github.com/google/deepvariant/issues/588:22,usability,input,input,22,It does:. `sudo ls -l input/`. `total 22167604`. `-rw------- 1 root root 22699626496 Nov 21 07:08 1115492_23181_0_0.cram`,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/588
https://github.com/google/deepvariant/issues/588:227,interoperability,bind,bindings,227,"@zivlang , please change these two lines in your command:. ```. -v ""input"":""/input"" \. -v ""output"":""/output"" \. ```. to:. ```. -v ""$PWD/input"":""/input"" \. -v ""$PWD/output"":""/output"" \. ```. You are missing a `$PWD/` and docker bindings require absolute path not relative path, which is causing this issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/588
https://github.com/google/deepvariant/issues/588:227,modifiability,bind,bindings,227,"@zivlang , please change these two lines in your command:. ```. -v ""input"":""/input"" \. -v ""output"":""/output"" \. ```. to:. ```. -v ""$PWD/input"":""/input"" \. -v ""$PWD/output"":""/output"" \. ```. You are missing a `$PWD/` and docker bindings require absolute path not relative path, which is causing this issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/588
https://github.com/google/deepvariant/issues/588:68,safety,input,input,68,"@zivlang , please change these two lines in your command:. ```. -v ""input"":""/input"" \. -v ""output"":""/output"" \. ```. to:. ```. -v ""$PWD/input"":""/input"" \. -v ""$PWD/output"":""/output"" \. ```. You are missing a `$PWD/` and docker bindings require absolute path not relative path, which is causing this issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/588
https://github.com/google/deepvariant/issues/588:77,safety,input,input,77,"@zivlang , please change these two lines in your command:. ```. -v ""input"":""/input"" \. -v ""output"":""/output"" \. ```. to:. ```. -v ""$PWD/input"":""/input"" \. -v ""$PWD/output"":""/output"" \. ```. You are missing a `$PWD/` and docker bindings require absolute path not relative path, which is causing this issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/588
https://github.com/google/deepvariant/issues/588:136,safety,input,input,136,"@zivlang , please change these two lines in your command:. ```. -v ""input"":""/input"" \. -v ""output"":""/output"" \. ```. to:. ```. -v ""$PWD/input"":""/input"" \. -v ""$PWD/output"":""/output"" \. ```. You are missing a `$PWD/` and docker bindings require absolute path not relative path, which is causing this issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/588
https://github.com/google/deepvariant/issues/588:145,safety,input,input,145,"@zivlang , please change these two lines in your command:. ```. -v ""input"":""/input"" \. -v ""output"":""/output"" \. ```. to:. ```. -v ""$PWD/input"":""/input"" \. -v ""$PWD/output"":""/output"" \. ```. You are missing a `$PWD/` and docker bindings require absolute path not relative path, which is causing this issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/588
https://github.com/google/deepvariant/issues/588:49,usability,command,command,49,"@zivlang , please change these two lines in your command:. ```. -v ""input"":""/input"" \. -v ""output"":""/output"" \. ```. to:. ```. -v ""$PWD/input"":""/input"" \. -v ""$PWD/output"":""/output"" \. ```. You are missing a `$PWD/` and docker bindings require absolute path not relative path, which is causing this issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/588
https://github.com/google/deepvariant/issues/588:68,usability,input,input,68,"@zivlang , please change these two lines in your command:. ```. -v ""input"":""/input"" \. -v ""output"":""/output"" \. ```. to:. ```. -v ""$PWD/input"":""/input"" \. -v ""$PWD/output"":""/output"" \. ```. You are missing a `$PWD/` and docker bindings require absolute path not relative path, which is causing this issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/588
https://github.com/google/deepvariant/issues/588:77,usability,input,input,77,"@zivlang , please change these two lines in your command:. ```. -v ""input"":""/input"" \. -v ""output"":""/output"" \. ```. to:. ```. -v ""$PWD/input"":""/input"" \. -v ""$PWD/output"":""/output"" \. ```. You are missing a `$PWD/` and docker bindings require absolute path not relative path, which is causing this issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/588
https://github.com/google/deepvariant/issues/588:136,usability,input,input,136,"@zivlang , please change these two lines in your command:. ```. -v ""input"":""/input"" \. -v ""output"":""/output"" \. ```. to:. ```. -v ""$PWD/input"":""/input"" \. -v ""$PWD/output"":""/output"" \. ```. You are missing a `$PWD/` and docker bindings require absolute path not relative path, which is causing this issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/588
https://github.com/google/deepvariant/issues/588:145,usability,input,input,145,"@zivlang , please change these two lines in your command:. ```. -v ""input"":""/input"" \. -v ""output"":""/output"" \. ```. to:. ```. -v ""$PWD/input"":""/input"" \. -v ""$PWD/output"":""/output"" \. ```. You are missing a `$PWD/` and docker bindings require absolute path not relative path, which is causing this issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/588
https://github.com/google/deepvariant/issues/588:45,availability,error,error,45,"Thanks. That helped. Now I'm facing the next error:. For every header in the reference file it prints. > ""header name"" is n bp and IS MISSING,. and then. > Please make sure the --ref input matches the build used for the input in --reads. That means that the reference genome that was used for creating the input I used in the variant calling pipe line command is different than the reference genome I used in that command? in which case, is there a way to find which reference genome was used for the creation of the input I used?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/588
https://github.com/google/deepvariant/issues/588:201,deployability,build,build,201,"Thanks. That helped. Now I'm facing the next error:. For every header in the reference file it prints. > ""header name"" is n bp and IS MISSING,. and then. > Please make sure the --ref input matches the build used for the input in --reads. That means that the reference genome that was used for creating the input I used in the variant calling pipe line command is different than the reference genome I used in that command? in which case, is there a way to find which reference genome was used for the creation of the input I used?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/588
https://github.com/google/deepvariant/issues/588:45,performance,error,error,45,"Thanks. That helped. Now I'm facing the next error:. For every header in the reference file it prints. > ""header name"" is n bp and IS MISSING,. and then. > Please make sure the --ref input matches the build used for the input in --reads. That means that the reference genome that was used for creating the input I used in the variant calling pipe line command is different than the reference genome I used in that command? in which case, is there a way to find which reference genome was used for the creation of the input I used?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/588
https://github.com/google/deepvariant/issues/588:45,safety,error,error,45,"Thanks. That helped. Now I'm facing the next error:. For every header in the reference file it prints. > ""header name"" is n bp and IS MISSING,. and then. > Please make sure the --ref input matches the build used for the input in --reads. That means that the reference genome that was used for creating the input I used in the variant calling pipe line command is different than the reference genome I used in that command? in which case, is there a way to find which reference genome was used for the creation of the input I used?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/588
https://github.com/google/deepvariant/issues/588:183,safety,input,input,183,"Thanks. That helped. Now I'm facing the next error:. For every header in the reference file it prints. > ""header name"" is n bp and IS MISSING,. and then. > Please make sure the --ref input matches the build used for the input in --reads. That means that the reference genome that was used for creating the input I used in the variant calling pipe line command is different than the reference genome I used in that command? in which case, is there a way to find which reference genome was used for the creation of the input I used?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/588
https://github.com/google/deepvariant/issues/588:220,safety,input,input,220,"Thanks. That helped. Now I'm facing the next error:. For every header in the reference file it prints. > ""header name"" is n bp and IS MISSING,. and then. > Please make sure the --ref input matches the build used for the input in --reads. That means that the reference genome that was used for creating the input I used in the variant calling pipe line command is different than the reference genome I used in that command? in which case, is there a way to find which reference genome was used for the creation of the input I used?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/588
https://github.com/google/deepvariant/issues/588:306,safety,input,input,306,"Thanks. That helped. Now I'm facing the next error:. For every header in the reference file it prints. > ""header name"" is n bp and IS MISSING,. and then. > Please make sure the --ref input matches the build used for the input in --reads. That means that the reference genome that was used for creating the input I used in the variant calling pipe line command is different than the reference genome I used in that command? in which case, is there a way to find which reference genome was used for the creation of the input I used?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/588
https://github.com/google/deepvariant/issues/588:517,safety,input,input,517,"Thanks. That helped. Now I'm facing the next error:. For every header in the reference file it prints. > ""header name"" is n bp and IS MISSING,. and then. > Please make sure the --ref input matches the build used for the input in --reads. That means that the reference genome that was used for creating the input I used in the variant calling pipe line command is different than the reference genome I used in that command? in which case, is there a way to find which reference genome was used for the creation of the input I used?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/588
https://github.com/google/deepvariant/issues/588:13,usability,help,helped,13,"Thanks. That helped. Now I'm facing the next error:. For every header in the reference file it prints. > ""header name"" is n bp and IS MISSING,. and then. > Please make sure the --ref input matches the build used for the input in --reads. That means that the reference genome that was used for creating the input I used in the variant calling pipe line command is different than the reference genome I used in that command? in which case, is there a way to find which reference genome was used for the creation of the input I used?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/588
https://github.com/google/deepvariant/issues/588:45,usability,error,error,45,"Thanks. That helped. Now I'm facing the next error:. For every header in the reference file it prints. > ""header name"" is n bp and IS MISSING,. and then. > Please make sure the --ref input matches the build used for the input in --reads. That means that the reference genome that was used for creating the input I used in the variant calling pipe line command is different than the reference genome I used in that command? in which case, is there a way to find which reference genome was used for the creation of the input I used?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/588
https://github.com/google/deepvariant/issues/588:183,usability,input,input,183,"Thanks. That helped. Now I'm facing the next error:. For every header in the reference file it prints. > ""header name"" is n bp and IS MISSING,. and then. > Please make sure the --ref input matches the build used for the input in --reads. That means that the reference genome that was used for creating the input I used in the variant calling pipe line command is different than the reference genome I used in that command? in which case, is there a way to find which reference genome was used for the creation of the input I used?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/588
https://github.com/google/deepvariant/issues/588:220,usability,input,input,220,"Thanks. That helped. Now I'm facing the next error:. For every header in the reference file it prints. > ""header name"" is n bp and IS MISSING,. and then. > Please make sure the --ref input matches the build used for the input in --reads. That means that the reference genome that was used for creating the input I used in the variant calling pipe line command is different than the reference genome I used in that command? in which case, is there a way to find which reference genome was used for the creation of the input I used?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/588
https://github.com/google/deepvariant/issues/588:306,usability,input,input,306,"Thanks. That helped. Now I'm facing the next error:. For every header in the reference file it prints. > ""header name"" is n bp and IS MISSING,. and then. > Please make sure the --ref input matches the build used for the input in --reads. That means that the reference genome that was used for creating the input I used in the variant calling pipe line command is different than the reference genome I used in that command? in which case, is there a way to find which reference genome was used for the creation of the input I used?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/588
https://github.com/google/deepvariant/issues/588:352,usability,command,command,352,"Thanks. That helped. Now I'm facing the next error:. For every header in the reference file it prints. > ""header name"" is n bp and IS MISSING,. and then. > Please make sure the --ref input matches the build used for the input in --reads. That means that the reference genome that was used for creating the input I used in the variant calling pipe line command is different than the reference genome I used in that command? in which case, is there a way to find which reference genome was used for the creation of the input I used?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/588
https://github.com/google/deepvariant/issues/588:414,usability,command,command,414,"Thanks. That helped. Now I'm facing the next error:. For every header in the reference file it prints. > ""header name"" is n bp and IS MISSING,. and then. > Please make sure the --ref input matches the build used for the input in --reads. That means that the reference genome that was used for creating the input I used in the variant calling pipe line command is different than the reference genome I used in that command? in which case, is there a way to find which reference genome was used for the creation of the input I used?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/588
https://github.com/google/deepvariant/issues/588:517,usability,input,input,517,"Thanks. That helped. Now I'm facing the next error:. For every header in the reference file it prints. > ""header name"" is n bp and IS MISSING,. and then. > Please make sure the --ref input matches the build used for the input in --reads. That means that the reference genome that was used for creating the input I used in the variant calling pipe line command is different than the reference genome I used in that command? in which case, is there a way to find which reference genome was used for the creation of the input I used?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/588
https://github.com/google/deepvariant/issues/588:152,usability,command,command,152,"@zivlang ,. You need to use the same reference you used to generate the CRAM file using a mapping software like bwa-mem or minimap2. You can see if the command was stored in the header of the cram file. If you want to see if DeepVariant works on your machine, please follow the steps provided in the quick-start here: https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-quick-start.md",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/588
https://github.com/google/deepvariant/issues/590:110,deployability,version,versions,110,"Hi @zivlang ,. In https://github.com/google/deepvariant/blob/r0.6/docs/deepvariant-quick-start.md and earlier versions, we show how you can use the pre-built binaries. The main thing is that you need to run `run-prereq.sh` first. Some details might have changed since v0.6, but the steps should be similar. Please take a look at the older document first, and I'll plan to do a step-by-step documentation here later today.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/590
https://github.com/google/deepvariant/issues/590:110,integrability,version,versions,110,"Hi @zivlang ,. In https://github.com/google/deepvariant/blob/r0.6/docs/deepvariant-quick-start.md and earlier versions, we show how you can use the pre-built binaries. The main thing is that you need to run `run-prereq.sh` first. Some details might have changed since v0.6, but the steps should be similar. Please take a look at the older document first, and I'll plan to do a step-by-step documentation here later today.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/590
https://github.com/google/deepvariant/issues/590:110,modifiability,version,versions,110,"Hi @zivlang ,. In https://github.com/google/deepvariant/blob/r0.6/docs/deepvariant-quick-start.md and earlier versions, we show how you can use the pre-built binaries. The main thing is that you need to run `run-prereq.sh` first. Some details might have changed since v0.6, but the steps should be similar. Please take a look at the older document first, and I'll plan to do a step-by-step documentation here later today.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/590
https://github.com/google/deepvariant/issues/590:364,testability,plan,plan,364,"Hi @zivlang ,. In https://github.com/google/deepvariant/blob/r0.6/docs/deepvariant-quick-start.md and earlier versions, we show how you can use the pre-built binaries. The main thing is that you need to run `run-prereq.sh` first. Some details might have changed since v0.6, but the steps should be similar. Please take a look at the older document first, and I'll plan to do a step-by-step documentation here later today.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/590
https://github.com/google/deepvariant/issues/590:339,usability,document,document,339,"Hi @zivlang ,. In https://github.com/google/deepvariant/blob/r0.6/docs/deepvariant-quick-start.md and earlier versions, we show how you can use the pre-built binaries. The main thing is that you need to run `run-prereq.sh` first. Some details might have changed since v0.6, but the steps should be similar. Please take a look at the older document first, and I'll plan to do a step-by-step documentation here later today.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/590
https://github.com/google/deepvariant/issues/590:390,usability,document,documentation,390,"Hi @zivlang ,. In https://github.com/google/deepvariant/blob/r0.6/docs/deepvariant-quick-start.md and earlier versions, we show how you can use the pre-built binaries. The main thing is that you need to run `run-prereq.sh` first. Some details might have changed since v0.6, but the steps should be similar. Please take a look at the older document first, and I'll plan to do a step-by-step documentation here later today.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/590
https://github.com/google/deepvariant/issues/590:1086,availability,Down,Download,1086,"ou try Singularity: https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-quick-start.md#notes-on-singularity. If you don't have root permission, you won't be able to install necessary things before running the binaries either. ---. Here is what I did:. Get a machine. (Not required to run on GCP. I just use this to get a machine to test). `gcloud compute instances create ""${USER}-cpu"" --scopes ""compute-rw,storage-full,cloud-platform"" --image-family ""ubuntu-2004-lts"" --image-project ""ubuntu-os-cloud"" --machine-type ""custom-64-131072"" --boot-disk-size ""300"" --zone ""us-west2-b"" --min-cpu-platform ""Intel Skylake""`. ssh into the machine:. ```. gcloud compute ssh pichuan-cpu --zone us-west2-b. ```. Get the binaries and models:. ```. BUCKET=""gs://deepvariant"". BIN_VERSION=""1.4.0"". MODEL_VERSION=""1.4.0"". BIN_BUCKET=""${BUCKET}/binaries/DeepVariant/${BIN_VERSION}/DeepVariant-${BIN_VERSION}"". MODEL_NAME=""DeepVariant-inception_v3-${MODEL_VERSION}+data-wgs_standard"". mkdir -p bin. # Download the DeepVariant binaries. gsutil -m cp ""${BIN_BUCKET}/*"" bin/. chmod a+x bin/*. ```. Then, I ran:. ```. cd bin; bash run-prereq.sh; cd -. ```. The `run-prereq.sh` tends to be the most tricky one - it will require root permission, and it'll install a bunch of stuff on your machine. If you can't use Docker because of root permissions, you likely won't be able to run this as well. Download test data:. ```. INPUT_DIR=""${PWD}/quickstart-testdata"". DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". mkdir -p ${INPUT_DIR}. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/590
https://github.com/google/deepvariant/issues/590:1476,availability,Down,Download,1476,"ER}-cpu"" --scopes ""compute-rw,storage-full,cloud-platform"" --image-family ""ubuntu-2004-lts"" --image-project ""ubuntu-os-cloud"" --machine-type ""custom-64-131072"" --boot-disk-size ""300"" --zone ""us-west2-b"" --min-cpu-platform ""Intel Skylake""`. ssh into the machine:. ```. gcloud compute ssh pichuan-cpu --zone us-west2-b. ```. Get the binaries and models:. ```. BUCKET=""gs://deepvariant"". BIN_VERSION=""1.4.0"". MODEL_VERSION=""1.4.0"". BIN_BUCKET=""${BUCKET}/binaries/DeepVariant/${BIN_VERSION}/DeepVariant-${BIN_VERSION}"". MODEL_NAME=""DeepVariant-inception_v3-${MODEL_VERSION}+data-wgs_standard"". mkdir -p bin. # Download the DeepVariant binaries. gsutil -m cp ""${BIN_BUCKET}/*"" bin/. chmod a+x bin/*. ```. Then, I ran:. ```. cd bin; bash run-prereq.sh; cd -. ```. The `run-prereq.sh` tends to be the most tricky one - it will require root permission, and it'll install a bunch of stuff on your machine. If you can't use Docker because of root permissions, you likely won't be able to run this as well. Download test data:. ```. INPUT_DIR=""${PWD}/quickstart-testdata"". DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". mkdir -p ${INPUT_DIR}. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. ```. Run make_examples:. ```. OUTPUT_DIR=""${PWD}/quickst",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/590
https://github.com/google/deepvariant/issues/590:3534,availability,mainten,maintenance,3534,"hr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. ```. Run make_examples:. ```. OUTPUT_DIR=""${PWD}/quickstart-output"". mkdir -p ""${OUTPUT_DIR}"". ```. ```. python bin/make_examples.zip \. --mode calling \. --ref ""${INPUT_DIR}/ucsc.hg19.chr20.unittest.fasta"" \. --reads ""${INPUT_DIR}/NA12878_S1.chr20.10_10p1mb.bam"" \. --regions ""chr20:10,000,000-10,010,000"" \. --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"" \. --channels ""insert_size"". ```. (To figure out which flags you need to add for each model, you can read https://github.com/google/deepvariant/blob/r1.4/scripts/run_deepvariant.py#L236-L253 . Sorry that we don't have better documentation than that right now). For how to run this with multiple shards, and how to run the rest of the commands, please read https://github.com/google/deepvariant/blob/r0.6/docs/deepvariant-quick-start.md. I just tested the steps above and confirmed that it worked for me on v1.4.0, at least for the make_examples step. If you encounter more issues with other steps, please feel free to ask again. I'd be happy to help. Note that I don't plan to put this into an official documentation page now, because that adds to our maintenance burden to keep it up to date. Given that we have the Docker/Singularity solution that works generally well for our users, I don't expect many of our users to need to use pre-built binaries. @zivlang thank you for your question so I have a chance to test it again and document it here. Hopefully this is helpful for you. Happy to answer more questions if you encounter more problems.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/590
https://github.com/google/deepvariant/issues/590:268,deployability,instal,install,268,"Before you proceed, if you can't use Docker because of root permission, I recommend that you try Singularity: https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-quick-start.md#notes-on-singularity. If you don't have root permission, you won't be able to install necessary things before running the binaries either. ---. Here is what I did:. Get a machine. (Not required to run on GCP. I just use this to get a machine to test). `gcloud compute instances create ""${USER}-cpu"" --scopes ""compute-rw,storage-full,cloud-platform"" --image-family ""ubuntu-2004-lts"" --image-project ""ubuntu-os-cloud"" --machine-type ""custom-64-131072"" --boot-disk-size ""300"" --zone ""us-west2-b"" --min-cpu-platform ""Intel Skylake""`. ssh into the machine:. ```. gcloud compute ssh pichuan-cpu --zone us-west2-b. ```. Get the binaries and models:. ```. BUCKET=""gs://deepvariant"". BIN_VERSION=""1.4.0"". MODEL_VERSION=""1.4.0"". BIN_BUCKET=""${BUCKET}/binaries/DeepVariant/${BIN_VERSION}/DeepVariant-${BIN_VERSION}"". MODEL_NAME=""DeepVariant-inception_v3-${MODEL_VERSION}+data-wgs_standard"". mkdir -p bin. # Download the DeepVariant binaries. gsutil -m cp ""${BIN_BUCKET}/*"" bin/. chmod a+x bin/*. ```. Then, I ran:. ```. cd bin; bash run-prereq.sh; cd -. ```. The `run-prereq.sh` tends to be the most tricky one - it will require root permission, and it'll install a bunch of stuff on your machine. If you can't use Docker because of root permissions, you likely won't be able to run this as well. Download test data:. ```. INPUT_DIR=""${PWD}/quickstart-testdata"". DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". mkdir -p ${INPUT_DIR}. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/590
https://github.com/google/deepvariant/issues/590:1335,deployability,instal,install,1335,"is what I did:. Get a machine. (Not required to run on GCP. I just use this to get a machine to test). `gcloud compute instances create ""${USER}-cpu"" --scopes ""compute-rw,storage-full,cloud-platform"" --image-family ""ubuntu-2004-lts"" --image-project ""ubuntu-os-cloud"" --machine-type ""custom-64-131072"" --boot-disk-size ""300"" --zone ""us-west2-b"" --min-cpu-platform ""Intel Skylake""`. ssh into the machine:. ```. gcloud compute ssh pichuan-cpu --zone us-west2-b. ```. Get the binaries and models:. ```. BUCKET=""gs://deepvariant"". BIN_VERSION=""1.4.0"". MODEL_VERSION=""1.4.0"". BIN_BUCKET=""${BUCKET}/binaries/DeepVariant/${BIN_VERSION}/DeepVariant-${BIN_VERSION}"". MODEL_NAME=""DeepVariant-inception_v3-${MODEL_VERSION}+data-wgs_standard"". mkdir -p bin. # Download the DeepVariant binaries. gsutil -m cp ""${BIN_BUCKET}/*"" bin/. chmod a+x bin/*. ```. Then, I ran:. ```. cd bin; bash run-prereq.sh; cd -. ```. The `run-prereq.sh` tends to be the most tricky one - it will require root permission, and it'll install a bunch of stuff on your machine. If you can't use Docker because of root permissions, you likely won't be able to run this as well. Download test data:. ```. INPUT_DIR=""${PWD}/quickstart-testdata"". DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". mkdir -p ${INPUT_DIR}. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/590
https://github.com/google/deepvariant/issues/590:484,energy efficiency,cpu,cpu,484,"Before you proceed, if you can't use Docker because of root permission, I recommend that you try Singularity: https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-quick-start.md#notes-on-singularity. If you don't have root permission, you won't be able to install necessary things before running the binaries either. ---. Here is what I did:. Get a machine. (Not required to run on GCP. I just use this to get a machine to test). `gcloud compute instances create ""${USER}-cpu"" --scopes ""compute-rw,storage-full,cloud-platform"" --image-family ""ubuntu-2004-lts"" --image-project ""ubuntu-os-cloud"" --machine-type ""custom-64-131072"" --boot-disk-size ""300"" --zone ""us-west2-b"" --min-cpu-platform ""Intel Skylake""`. ssh into the machine:. ```. gcloud compute ssh pichuan-cpu --zone us-west2-b. ```. Get the binaries and models:. ```. BUCKET=""gs://deepvariant"". BIN_VERSION=""1.4.0"". MODEL_VERSION=""1.4.0"". BIN_BUCKET=""${BUCKET}/binaries/DeepVariant/${BIN_VERSION}/DeepVariant-${BIN_VERSION}"". MODEL_NAME=""DeepVariant-inception_v3-${MODEL_VERSION}+data-wgs_standard"". mkdir -p bin. # Download the DeepVariant binaries. gsutil -m cp ""${BIN_BUCKET}/*"" bin/. chmod a+x bin/*. ```. Then, I ran:. ```. cd bin; bash run-prereq.sh; cd -. ```. The `run-prereq.sh` tends to be the most tricky one - it will require root permission, and it'll install a bunch of stuff on your machine. If you can't use Docker because of root permissions, you likely won't be able to run this as well. Download test data:. ```. INPUT_DIR=""${PWD}/quickstart-testdata"". DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". mkdir -p ${INPUT_DIR}. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/590
https://github.com/google/deepvariant/issues/590:523,energy efficiency,cloud,cloud-platform,523,"Before you proceed, if you can't use Docker because of root permission, I recommend that you try Singularity: https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-quick-start.md#notes-on-singularity. If you don't have root permission, you won't be able to install necessary things before running the binaries either. ---. Here is what I did:. Get a machine. (Not required to run on GCP. I just use this to get a machine to test). `gcloud compute instances create ""${USER}-cpu"" --scopes ""compute-rw,storage-full,cloud-platform"" --image-family ""ubuntu-2004-lts"" --image-project ""ubuntu-os-cloud"" --machine-type ""custom-64-131072"" --boot-disk-size ""300"" --zone ""us-west2-b"" --min-cpu-platform ""Intel Skylake""`. ssh into the machine:. ```. gcloud compute ssh pichuan-cpu --zone us-west2-b. ```. Get the binaries and models:. ```. BUCKET=""gs://deepvariant"". BIN_VERSION=""1.4.0"". MODEL_VERSION=""1.4.0"". BIN_BUCKET=""${BUCKET}/binaries/DeepVariant/${BIN_VERSION}/DeepVariant-${BIN_VERSION}"". MODEL_NAME=""DeepVariant-inception_v3-${MODEL_VERSION}+data-wgs_standard"". mkdir -p bin. # Download the DeepVariant binaries. gsutil -m cp ""${BIN_BUCKET}/*"" bin/. chmod a+x bin/*. ```. Then, I ran:. ```. cd bin; bash run-prereq.sh; cd -. ```. The `run-prereq.sh` tends to be the most tricky one - it will require root permission, and it'll install a bunch of stuff on your machine. If you can't use Docker because of root permissions, you likely won't be able to run this as well. Download test data:. ```. INPUT_DIR=""${PWD}/quickstart-testdata"". DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". mkdir -p ${INPUT_DIR}. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/590
https://github.com/google/deepvariant/issues/590:599,energy efficiency,cloud,cloud,599,"Before you proceed, if you can't use Docker because of root permission, I recommend that you try Singularity: https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-quick-start.md#notes-on-singularity. If you don't have root permission, you won't be able to install necessary things before running the binaries either. ---. Here is what I did:. Get a machine. (Not required to run on GCP. I just use this to get a machine to test). `gcloud compute instances create ""${USER}-cpu"" --scopes ""compute-rw,storage-full,cloud-platform"" --image-family ""ubuntu-2004-lts"" --image-project ""ubuntu-os-cloud"" --machine-type ""custom-64-131072"" --boot-disk-size ""300"" --zone ""us-west2-b"" --min-cpu-platform ""Intel Skylake""`. ssh into the machine:. ```. gcloud compute ssh pichuan-cpu --zone us-west2-b. ```. Get the binaries and models:. ```. BUCKET=""gs://deepvariant"". BIN_VERSION=""1.4.0"". MODEL_VERSION=""1.4.0"". BIN_BUCKET=""${BUCKET}/binaries/DeepVariant/${BIN_VERSION}/DeepVariant-${BIN_VERSION}"". MODEL_NAME=""DeepVariant-inception_v3-${MODEL_VERSION}+data-wgs_standard"". mkdir -p bin. # Download the DeepVariant binaries. gsutil -m cp ""${BIN_BUCKET}/*"" bin/. chmod a+x bin/*. ```. Then, I ran:. ```. cd bin; bash run-prereq.sh; cd -. ```. The `run-prereq.sh` tends to be the most tricky one - it will require root permission, and it'll install a bunch of stuff on your machine. If you can't use Docker because of root permissions, you likely won't be able to run this as well. Download test data:. ```. INPUT_DIR=""${PWD}/quickstart-testdata"". DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". mkdir -p ${INPUT_DIR}. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/590
https://github.com/google/deepvariant/issues/590:689,energy efficiency,cpu,cpu-platform,689,"Before you proceed, if you can't use Docker because of root permission, I recommend that you try Singularity: https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-quick-start.md#notes-on-singularity. If you don't have root permission, you won't be able to install necessary things before running the binaries either. ---. Here is what I did:. Get a machine. (Not required to run on GCP. I just use this to get a machine to test). `gcloud compute instances create ""${USER}-cpu"" --scopes ""compute-rw,storage-full,cloud-platform"" --image-family ""ubuntu-2004-lts"" --image-project ""ubuntu-os-cloud"" --machine-type ""custom-64-131072"" --boot-disk-size ""300"" --zone ""us-west2-b"" --min-cpu-platform ""Intel Skylake""`. ssh into the machine:. ```. gcloud compute ssh pichuan-cpu --zone us-west2-b. ```. Get the binaries and models:. ```. BUCKET=""gs://deepvariant"". BIN_VERSION=""1.4.0"". MODEL_VERSION=""1.4.0"". BIN_BUCKET=""${BUCKET}/binaries/DeepVariant/${BIN_VERSION}/DeepVariant-${BIN_VERSION}"". MODEL_NAME=""DeepVariant-inception_v3-${MODEL_VERSION}+data-wgs_standard"". mkdir -p bin. # Download the DeepVariant binaries. gsutil -m cp ""${BIN_BUCKET}/*"" bin/. chmod a+x bin/*. ```. Then, I ran:. ```. cd bin; bash run-prereq.sh; cd -. ```. The `run-prereq.sh` tends to be the most tricky one - it will require root permission, and it'll install a bunch of stuff on your machine. If you can't use Docker because of root permissions, you likely won't be able to run this as well. Download test data:. ```. INPUT_DIR=""${PWD}/quickstart-testdata"". DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". mkdir -p ${INPUT_DIR}. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/590
https://github.com/google/deepvariant/issues/590:775,energy efficiency,cpu,cpu,775,"Before you proceed, if you can't use Docker because of root permission, I recommend that you try Singularity: https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-quick-start.md#notes-on-singularity. If you don't have root permission, you won't be able to install necessary things before running the binaries either. ---. Here is what I did:. Get a machine. (Not required to run on GCP. I just use this to get a machine to test). `gcloud compute instances create ""${USER}-cpu"" --scopes ""compute-rw,storage-full,cloud-platform"" --image-family ""ubuntu-2004-lts"" --image-project ""ubuntu-os-cloud"" --machine-type ""custom-64-131072"" --boot-disk-size ""300"" --zone ""us-west2-b"" --min-cpu-platform ""Intel Skylake""`. ssh into the machine:. ```. gcloud compute ssh pichuan-cpu --zone us-west2-b. ```. Get the binaries and models:. ```. BUCKET=""gs://deepvariant"". BIN_VERSION=""1.4.0"". MODEL_VERSION=""1.4.0"". BIN_BUCKET=""${BUCKET}/binaries/DeepVariant/${BIN_VERSION}/DeepVariant-${BIN_VERSION}"". MODEL_NAME=""DeepVariant-inception_v3-${MODEL_VERSION}+data-wgs_standard"". mkdir -p bin. # Download the DeepVariant binaries. gsutil -m cp ""${BIN_BUCKET}/*"" bin/. chmod a+x bin/*. ```. Then, I ran:. ```. cd bin; bash run-prereq.sh; cd -. ```. The `run-prereq.sh` tends to be the most tricky one - it will require root permission, and it'll install a bunch of stuff on your machine. If you can't use Docker because of root permissions, you likely won't be able to run this as well. Download test data:. ```. INPUT_DIR=""${PWD}/quickstart-testdata"". DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". mkdir -p ${INPUT_DIR}. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/590
https://github.com/google/deepvariant/issues/590:824,energy efficiency,model,models,824,"Before you proceed, if you can't use Docker because of root permission, I recommend that you try Singularity: https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-quick-start.md#notes-on-singularity. If you don't have root permission, you won't be able to install necessary things before running the binaries either. ---. Here is what I did:. Get a machine. (Not required to run on GCP. I just use this to get a machine to test). `gcloud compute instances create ""${USER}-cpu"" --scopes ""compute-rw,storage-full,cloud-platform"" --image-family ""ubuntu-2004-lts"" --image-project ""ubuntu-os-cloud"" --machine-type ""custom-64-131072"" --boot-disk-size ""300"" --zone ""us-west2-b"" --min-cpu-platform ""Intel Skylake""`. ssh into the machine:. ```. gcloud compute ssh pichuan-cpu --zone us-west2-b. ```. Get the binaries and models:. ```. BUCKET=""gs://deepvariant"". BIN_VERSION=""1.4.0"". MODEL_VERSION=""1.4.0"". BIN_BUCKET=""${BUCKET}/binaries/DeepVariant/${BIN_VERSION}/DeepVariant-${BIN_VERSION}"". MODEL_NAME=""DeepVariant-inception_v3-${MODEL_VERSION}+data-wgs_standard"". mkdir -p bin. # Download the DeepVariant binaries. gsutil -m cp ""${BIN_BUCKET}/*"" bin/. chmod a+x bin/*. ```. Then, I ran:. ```. cd bin; bash run-prereq.sh; cd -. ```. The `run-prereq.sh` tends to be the most tricky one - it will require root permission, and it'll install a bunch of stuff on your machine. If you can't use Docker because of root permissions, you likely won't be able to run this as well. Download test data:. ```. INPUT_DIR=""${PWD}/quickstart-testdata"". DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". mkdir -p ${INPUT_DIR}. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/590
https://github.com/google/deepvariant/issues/590:2868,energy efficiency,model,model,2868,". wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. ```. Run make_examples:. ```. OUTPUT_DIR=""${PWD}/quickstart-output"". mkdir -p ""${OUTPUT_DIR}"". ```. ```. python bin/make_examples.zip \. --mode calling \. --ref ""${INPUT_DIR}/ucsc.hg19.chr20.unittest.fasta"" \. --reads ""${INPUT_DIR}/NA12878_S1.chr20.10_10p1mb.bam"" \. --regions ""chr20:10,000,000-10,010,000"" \. --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"" \. --channels ""insert_size"". ```. (To figure out which flags you need to add for each model, you can read https://github.com/google/deepvariant/blob/r1.4/scripts/run_deepvariant.py#L236-L253 . Sorry that we don't have better documentation than that right now). For how to run this with multiple shards, and how to run the rest of the commands, please read https://github.com/google/deepvariant/blob/r0.6/docs/deepvariant-quick-start.md. I just tested the steps above and confirmed that it worked for me on v1.4.0, at least for the make_examples step. If you encounter more issues with other steps, please feel free to ask again. I'd be happy to help. Note that I don't plan to put this into an official documentation page now, because that adds to our maintenance burden to keep it up to date. Given that we have the Docker/Singularity solution that works generally well for our users, I don't expect many of our users to need to use pre-built binaries. @zivlang thank you for your question so I have a chance to test it again and document it here. Hopefully this is helpful for you. Happy",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/590
https://github.com/google/deepvariant/issues/590:529,interoperability,platform,platform,529,"Before you proceed, if you can't use Docker because of root permission, I recommend that you try Singularity: https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-quick-start.md#notes-on-singularity. If you don't have root permission, you won't be able to install necessary things before running the binaries either. ---. Here is what I did:. Get a machine. (Not required to run on GCP. I just use this to get a machine to test). `gcloud compute instances create ""${USER}-cpu"" --scopes ""compute-rw,storage-full,cloud-platform"" --image-family ""ubuntu-2004-lts"" --image-project ""ubuntu-os-cloud"" --machine-type ""custom-64-131072"" --boot-disk-size ""300"" --zone ""us-west2-b"" --min-cpu-platform ""Intel Skylake""`. ssh into the machine:. ```. gcloud compute ssh pichuan-cpu --zone us-west2-b. ```. Get the binaries and models:. ```. BUCKET=""gs://deepvariant"". BIN_VERSION=""1.4.0"". MODEL_VERSION=""1.4.0"". BIN_BUCKET=""${BUCKET}/binaries/DeepVariant/${BIN_VERSION}/DeepVariant-${BIN_VERSION}"". MODEL_NAME=""DeepVariant-inception_v3-${MODEL_VERSION}+data-wgs_standard"". mkdir -p bin. # Download the DeepVariant binaries. gsutil -m cp ""${BIN_BUCKET}/*"" bin/. chmod a+x bin/*. ```. Then, I ran:. ```. cd bin; bash run-prereq.sh; cd -. ```. The `run-prereq.sh` tends to be the most tricky one - it will require root permission, and it'll install a bunch of stuff on your machine. If you can't use Docker because of root permissions, you likely won't be able to run this as well. Download test data:. ```. INPUT_DIR=""${PWD}/quickstart-testdata"". DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". mkdir -p ${INPUT_DIR}. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/590
https://github.com/google/deepvariant/issues/590:693,interoperability,platform,platform,693,"Before you proceed, if you can't use Docker because of root permission, I recommend that you try Singularity: https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-quick-start.md#notes-on-singularity. If you don't have root permission, you won't be able to install necessary things before running the binaries either. ---. Here is what I did:. Get a machine. (Not required to run on GCP. I just use this to get a machine to test). `gcloud compute instances create ""${USER}-cpu"" --scopes ""compute-rw,storage-full,cloud-platform"" --image-family ""ubuntu-2004-lts"" --image-project ""ubuntu-os-cloud"" --machine-type ""custom-64-131072"" --boot-disk-size ""300"" --zone ""us-west2-b"" --min-cpu-platform ""Intel Skylake""`. ssh into the machine:. ```. gcloud compute ssh pichuan-cpu --zone us-west2-b. ```. Get the binaries and models:. ```. BUCKET=""gs://deepvariant"". BIN_VERSION=""1.4.0"". MODEL_VERSION=""1.4.0"". BIN_BUCKET=""${BUCKET}/binaries/DeepVariant/${BIN_VERSION}/DeepVariant-${BIN_VERSION}"". MODEL_NAME=""DeepVariant-inception_v3-${MODEL_VERSION}+data-wgs_standard"". mkdir -p bin. # Download the DeepVariant binaries. gsutil -m cp ""${BIN_BUCKET}/*"" bin/. chmod a+x bin/*. ```. Then, I ran:. ```. cd bin; bash run-prereq.sh; cd -. ```. The `run-prereq.sh` tends to be the most tricky one - it will require root permission, and it'll install a bunch of stuff on your machine. If you can't use Docker because of root permissions, you likely won't be able to run this as well. Download test data:. ```. INPUT_DIR=""${PWD}/quickstart-testdata"". DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". mkdir -p ${INPUT_DIR}. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/590
https://github.com/google/deepvariant/issues/590:484,performance,cpu,cpu,484,"Before you proceed, if you can't use Docker because of root permission, I recommend that you try Singularity: https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-quick-start.md#notes-on-singularity. If you don't have root permission, you won't be able to install necessary things before running the binaries either. ---. Here is what I did:. Get a machine. (Not required to run on GCP. I just use this to get a machine to test). `gcloud compute instances create ""${USER}-cpu"" --scopes ""compute-rw,storage-full,cloud-platform"" --image-family ""ubuntu-2004-lts"" --image-project ""ubuntu-os-cloud"" --machine-type ""custom-64-131072"" --boot-disk-size ""300"" --zone ""us-west2-b"" --min-cpu-platform ""Intel Skylake""`. ssh into the machine:. ```. gcloud compute ssh pichuan-cpu --zone us-west2-b. ```. Get the binaries and models:. ```. BUCKET=""gs://deepvariant"". BIN_VERSION=""1.4.0"". MODEL_VERSION=""1.4.0"". BIN_BUCKET=""${BUCKET}/binaries/DeepVariant/${BIN_VERSION}/DeepVariant-${BIN_VERSION}"". MODEL_NAME=""DeepVariant-inception_v3-${MODEL_VERSION}+data-wgs_standard"". mkdir -p bin. # Download the DeepVariant binaries. gsutil -m cp ""${BIN_BUCKET}/*"" bin/. chmod a+x bin/*. ```. Then, I ran:. ```. cd bin; bash run-prereq.sh; cd -. ```. The `run-prereq.sh` tends to be the most tricky one - it will require root permission, and it'll install a bunch of stuff on your machine. If you can't use Docker because of root permissions, you likely won't be able to run this as well. Download test data:. ```. INPUT_DIR=""${PWD}/quickstart-testdata"". DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". mkdir -p ${INPUT_DIR}. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/590
https://github.com/google/deepvariant/issues/590:647,performance,disk,disk-size,647,"Before you proceed, if you can't use Docker because of root permission, I recommend that you try Singularity: https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-quick-start.md#notes-on-singularity. If you don't have root permission, you won't be able to install necessary things before running the binaries either. ---. Here is what I did:. Get a machine. (Not required to run on GCP. I just use this to get a machine to test). `gcloud compute instances create ""${USER}-cpu"" --scopes ""compute-rw,storage-full,cloud-platform"" --image-family ""ubuntu-2004-lts"" --image-project ""ubuntu-os-cloud"" --machine-type ""custom-64-131072"" --boot-disk-size ""300"" --zone ""us-west2-b"" --min-cpu-platform ""Intel Skylake""`. ssh into the machine:. ```. gcloud compute ssh pichuan-cpu --zone us-west2-b. ```. Get the binaries and models:. ```. BUCKET=""gs://deepvariant"". BIN_VERSION=""1.4.0"". MODEL_VERSION=""1.4.0"". BIN_BUCKET=""${BUCKET}/binaries/DeepVariant/${BIN_VERSION}/DeepVariant-${BIN_VERSION}"". MODEL_NAME=""DeepVariant-inception_v3-${MODEL_VERSION}+data-wgs_standard"". mkdir -p bin. # Download the DeepVariant binaries. gsutil -m cp ""${BIN_BUCKET}/*"" bin/. chmod a+x bin/*. ```. Then, I ran:. ```. cd bin; bash run-prereq.sh; cd -. ```. The `run-prereq.sh` tends to be the most tricky one - it will require root permission, and it'll install a bunch of stuff on your machine. If you can't use Docker because of root permissions, you likely won't be able to run this as well. Download test data:. ```. INPUT_DIR=""${PWD}/quickstart-testdata"". DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". mkdir -p ${INPUT_DIR}. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/590
https://github.com/google/deepvariant/issues/590:689,performance,cpu,cpu-platform,689,"Before you proceed, if you can't use Docker because of root permission, I recommend that you try Singularity: https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-quick-start.md#notes-on-singularity. If you don't have root permission, you won't be able to install necessary things before running the binaries either. ---. Here is what I did:. Get a machine. (Not required to run on GCP. I just use this to get a machine to test). `gcloud compute instances create ""${USER}-cpu"" --scopes ""compute-rw,storage-full,cloud-platform"" --image-family ""ubuntu-2004-lts"" --image-project ""ubuntu-os-cloud"" --machine-type ""custom-64-131072"" --boot-disk-size ""300"" --zone ""us-west2-b"" --min-cpu-platform ""Intel Skylake""`. ssh into the machine:. ```. gcloud compute ssh pichuan-cpu --zone us-west2-b. ```. Get the binaries and models:. ```. BUCKET=""gs://deepvariant"". BIN_VERSION=""1.4.0"". MODEL_VERSION=""1.4.0"". BIN_BUCKET=""${BUCKET}/binaries/DeepVariant/${BIN_VERSION}/DeepVariant-${BIN_VERSION}"". MODEL_NAME=""DeepVariant-inception_v3-${MODEL_VERSION}+data-wgs_standard"". mkdir -p bin. # Download the DeepVariant binaries. gsutil -m cp ""${BIN_BUCKET}/*"" bin/. chmod a+x bin/*. ```. Then, I ran:. ```. cd bin; bash run-prereq.sh; cd -. ```. The `run-prereq.sh` tends to be the most tricky one - it will require root permission, and it'll install a bunch of stuff on your machine. If you can't use Docker because of root permissions, you likely won't be able to run this as well. Download test data:. ```. INPUT_DIR=""${PWD}/quickstart-testdata"". DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". mkdir -p ${INPUT_DIR}. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/590
https://github.com/google/deepvariant/issues/590:775,performance,cpu,cpu,775,"Before you proceed, if you can't use Docker because of root permission, I recommend that you try Singularity: https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-quick-start.md#notes-on-singularity. If you don't have root permission, you won't be able to install necessary things before running the binaries either. ---. Here is what I did:. Get a machine. (Not required to run on GCP. I just use this to get a machine to test). `gcloud compute instances create ""${USER}-cpu"" --scopes ""compute-rw,storage-full,cloud-platform"" --image-family ""ubuntu-2004-lts"" --image-project ""ubuntu-os-cloud"" --machine-type ""custom-64-131072"" --boot-disk-size ""300"" --zone ""us-west2-b"" --min-cpu-platform ""Intel Skylake""`. ssh into the machine:. ```. gcloud compute ssh pichuan-cpu --zone us-west2-b. ```. Get the binaries and models:. ```. BUCKET=""gs://deepvariant"". BIN_VERSION=""1.4.0"". MODEL_VERSION=""1.4.0"". BIN_BUCKET=""${BUCKET}/binaries/DeepVariant/${BIN_VERSION}/DeepVariant-${BIN_VERSION}"". MODEL_NAME=""DeepVariant-inception_v3-${MODEL_VERSION}+data-wgs_standard"". mkdir -p bin. # Download the DeepVariant binaries. gsutil -m cp ""${BIN_BUCKET}/*"" bin/. chmod a+x bin/*. ```. Then, I ran:. ```. cd bin; bash run-prereq.sh; cd -. ```. The `run-prereq.sh` tends to be the most tricky one - it will require root permission, and it'll install a bunch of stuff on your machine. If you can't use Docker because of root permissions, you likely won't be able to run this as well. Download test data:. ```. INPUT_DIR=""${PWD}/quickstart-testdata"". DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". mkdir -p ${INPUT_DIR}. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/590
https://github.com/google/deepvariant/issues/590:3534,reliability,mainten,maintenance,3534,"hr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. ```. Run make_examples:. ```. OUTPUT_DIR=""${PWD}/quickstart-output"". mkdir -p ""${OUTPUT_DIR}"". ```. ```. python bin/make_examples.zip \. --mode calling \. --ref ""${INPUT_DIR}/ucsc.hg19.chr20.unittest.fasta"" \. --reads ""${INPUT_DIR}/NA12878_S1.chr20.10_10p1mb.bam"" \. --regions ""chr20:10,000,000-10,010,000"" \. --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"" \. --channels ""insert_size"". ```. (To figure out which flags you need to add for each model, you can read https://github.com/google/deepvariant/blob/r1.4/scripts/run_deepvariant.py#L236-L253 . Sorry that we don't have better documentation than that right now). For how to run this with multiple shards, and how to run the rest of the commands, please read https://github.com/google/deepvariant/blob/r0.6/docs/deepvariant-quick-start.md. I just tested the steps above and confirmed that it worked for me on v1.4.0, at least for the make_examples step. If you encounter more issues with other steps, please feel free to ask again. I'd be happy to help. Note that I don't plan to put this into an official documentation page now, because that adds to our maintenance burden to keep it up to date. Given that we have the Docker/Singularity solution that works generally well for our users, I don't expect many of our users to need to use pre-built binaries. @zivlang thank you for your question so I have a chance to test it again and document it here. Hopefully this is helpful for you. Happy to answer more questions if you encounter more problems.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/590
https://github.com/google/deepvariant/issues/590:60,safety,permiss,permission,60,"Before you proceed, if you can't use Docker because of root permission, I recommend that you try Singularity: https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-quick-start.md#notes-on-singularity. If you don't have root permission, you won't be able to install necessary things before running the binaries either. ---. Here is what I did:. Get a machine. (Not required to run on GCP. I just use this to get a machine to test). `gcloud compute instances create ""${USER}-cpu"" --scopes ""compute-rw,storage-full,cloud-platform"" --image-family ""ubuntu-2004-lts"" --image-project ""ubuntu-os-cloud"" --machine-type ""custom-64-131072"" --boot-disk-size ""300"" --zone ""us-west2-b"" --min-cpu-platform ""Intel Skylake""`. ssh into the machine:. ```. gcloud compute ssh pichuan-cpu --zone us-west2-b. ```. Get the binaries and models:. ```. BUCKET=""gs://deepvariant"". BIN_VERSION=""1.4.0"". MODEL_VERSION=""1.4.0"". BIN_BUCKET=""${BUCKET}/binaries/DeepVariant/${BIN_VERSION}/DeepVariant-${BIN_VERSION}"". MODEL_NAME=""DeepVariant-inception_v3-${MODEL_VERSION}+data-wgs_standard"". mkdir -p bin. # Download the DeepVariant binaries. gsutil -m cp ""${BIN_BUCKET}/*"" bin/. chmod a+x bin/*. ```. Then, I ran:. ```. cd bin; bash run-prereq.sh; cd -. ```. The `run-prereq.sh` tends to be the most tricky one - it will require root permission, and it'll install a bunch of stuff on your machine. If you can't use Docker because of root permissions, you likely won't be able to run this as well. Download test data:. ```. INPUT_DIR=""${PWD}/quickstart-testdata"". DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". mkdir -p ${INPUT_DIR}. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/590
https://github.com/google/deepvariant/issues/590:235,safety,permiss,permission,235,"Before you proceed, if you can't use Docker because of root permission, I recommend that you try Singularity: https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-quick-start.md#notes-on-singularity. If you don't have root permission, you won't be able to install necessary things before running the binaries either. ---. Here is what I did:. Get a machine. (Not required to run on GCP. I just use this to get a machine to test). `gcloud compute instances create ""${USER}-cpu"" --scopes ""compute-rw,storage-full,cloud-platform"" --image-family ""ubuntu-2004-lts"" --image-project ""ubuntu-os-cloud"" --machine-type ""custom-64-131072"" --boot-disk-size ""300"" --zone ""us-west2-b"" --min-cpu-platform ""Intel Skylake""`. ssh into the machine:. ```. gcloud compute ssh pichuan-cpu --zone us-west2-b. ```. Get the binaries and models:. ```. BUCKET=""gs://deepvariant"". BIN_VERSION=""1.4.0"". MODEL_VERSION=""1.4.0"". BIN_BUCKET=""${BUCKET}/binaries/DeepVariant/${BIN_VERSION}/DeepVariant-${BIN_VERSION}"". MODEL_NAME=""DeepVariant-inception_v3-${MODEL_VERSION}+data-wgs_standard"". mkdir -p bin. # Download the DeepVariant binaries. gsutil -m cp ""${BIN_BUCKET}/*"" bin/. chmod a+x bin/*. ```. Then, I ran:. ```. cd bin; bash run-prereq.sh; cd -. ```. The `run-prereq.sh` tends to be the most tricky one - it will require root permission, and it'll install a bunch of stuff on your machine. If you can't use Docker because of root permissions, you likely won't be able to run this as well. Download test data:. ```. INPUT_DIR=""${PWD}/quickstart-testdata"". DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". mkdir -p ${INPUT_DIR}. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/590
https://github.com/google/deepvariant/issues/590:435,safety,test,test,435,"Before you proceed, if you can't use Docker because of root permission, I recommend that you try Singularity: https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-quick-start.md#notes-on-singularity. If you don't have root permission, you won't be able to install necessary things before running the binaries either. ---. Here is what I did:. Get a machine. (Not required to run on GCP. I just use this to get a machine to test). `gcloud compute instances create ""${USER}-cpu"" --scopes ""compute-rw,storage-full,cloud-platform"" --image-family ""ubuntu-2004-lts"" --image-project ""ubuntu-os-cloud"" --machine-type ""custom-64-131072"" --boot-disk-size ""300"" --zone ""us-west2-b"" --min-cpu-platform ""Intel Skylake""`. ssh into the machine:. ```. gcloud compute ssh pichuan-cpu --zone us-west2-b. ```. Get the binaries and models:. ```. BUCKET=""gs://deepvariant"". BIN_VERSION=""1.4.0"". MODEL_VERSION=""1.4.0"". BIN_BUCKET=""${BUCKET}/binaries/DeepVariant/${BIN_VERSION}/DeepVariant-${BIN_VERSION}"". MODEL_NAME=""DeepVariant-inception_v3-${MODEL_VERSION}+data-wgs_standard"". mkdir -p bin. # Download the DeepVariant binaries. gsutil -m cp ""${BIN_BUCKET}/*"" bin/. chmod a+x bin/*. ```. Then, I ran:. ```. cd bin; bash run-prereq.sh; cd -. ```. The `run-prereq.sh` tends to be the most tricky one - it will require root permission, and it'll install a bunch of stuff on your machine. If you can't use Docker because of root permissions, you likely won't be able to run this as well. Download test data:. ```. INPUT_DIR=""${PWD}/quickstart-testdata"". DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". mkdir -p ${INPUT_DIR}. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/590
https://github.com/google/deepvariant/issues/590:1313,safety,permiss,permission,1313,"es either. ---. Here is what I did:. Get a machine. (Not required to run on GCP. I just use this to get a machine to test). `gcloud compute instances create ""${USER}-cpu"" --scopes ""compute-rw,storage-full,cloud-platform"" --image-family ""ubuntu-2004-lts"" --image-project ""ubuntu-os-cloud"" --machine-type ""custom-64-131072"" --boot-disk-size ""300"" --zone ""us-west2-b"" --min-cpu-platform ""Intel Skylake""`. ssh into the machine:. ```. gcloud compute ssh pichuan-cpu --zone us-west2-b. ```. Get the binaries and models:. ```. BUCKET=""gs://deepvariant"". BIN_VERSION=""1.4.0"". MODEL_VERSION=""1.4.0"". BIN_BUCKET=""${BUCKET}/binaries/DeepVariant/${BIN_VERSION}/DeepVariant-${BIN_VERSION}"". MODEL_NAME=""DeepVariant-inception_v3-${MODEL_VERSION}+data-wgs_standard"". mkdir -p bin. # Download the DeepVariant binaries. gsutil -m cp ""${BIN_BUCKET}/*"" bin/. chmod a+x bin/*. ```. Then, I ran:. ```. cd bin; bash run-prereq.sh; cd -. ```. The `run-prereq.sh` tends to be the most tricky one - it will require root permission, and it'll install a bunch of stuff on your machine. If you can't use Docker because of root permissions, you likely won't be able to run this as well. Download test data:. ```. INPUT_DIR=""${PWD}/quickstart-testdata"". DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". mkdir -p ${INPUT_DIR}. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.ch",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/590
https://github.com/google/deepvariant/issues/590:1417,safety,permiss,permissions,1417," machine to test). `gcloud compute instances create ""${USER}-cpu"" --scopes ""compute-rw,storage-full,cloud-platform"" --image-family ""ubuntu-2004-lts"" --image-project ""ubuntu-os-cloud"" --machine-type ""custom-64-131072"" --boot-disk-size ""300"" --zone ""us-west2-b"" --min-cpu-platform ""Intel Skylake""`. ssh into the machine:. ```. gcloud compute ssh pichuan-cpu --zone us-west2-b. ```. Get the binaries and models:. ```. BUCKET=""gs://deepvariant"". BIN_VERSION=""1.4.0"". MODEL_VERSION=""1.4.0"". BIN_BUCKET=""${BUCKET}/binaries/DeepVariant/${BIN_VERSION}/DeepVariant-${BIN_VERSION}"". MODEL_NAME=""DeepVariant-inception_v3-${MODEL_VERSION}+data-wgs_standard"". mkdir -p bin. # Download the DeepVariant binaries. gsutil -m cp ""${BIN_BUCKET}/*"" bin/. chmod a+x bin/*. ```. Then, I ran:. ```. cd bin; bash run-prereq.sh; cd -. ```. The `run-prereq.sh` tends to be the most tricky one - it will require root permission, and it'll install a bunch of stuff on your machine. If you can't use Docker because of root permissions, you likely won't be able to run this as well. Download test data:. ```. INPUT_DIR=""${PWD}/quickstart-testdata"". DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". mkdir -p ${INPUT_DIR}. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/590
https://github.com/google/deepvariant/issues/590:1485,safety,test,test,1485,""" --scopes ""compute-rw,storage-full,cloud-platform"" --image-family ""ubuntu-2004-lts"" --image-project ""ubuntu-os-cloud"" --machine-type ""custom-64-131072"" --boot-disk-size ""300"" --zone ""us-west2-b"" --min-cpu-platform ""Intel Skylake""`. ssh into the machine:. ```. gcloud compute ssh pichuan-cpu --zone us-west2-b. ```. Get the binaries and models:. ```. BUCKET=""gs://deepvariant"". BIN_VERSION=""1.4.0"". MODEL_VERSION=""1.4.0"". BIN_BUCKET=""${BUCKET}/binaries/DeepVariant/${BIN_VERSION}/DeepVariant-${BIN_VERSION}"". MODEL_NAME=""DeepVariant-inception_v3-${MODEL_VERSION}+data-wgs_standard"". mkdir -p bin. # Download the DeepVariant binaries. gsutil -m cp ""${BIN_BUCKET}/*"" bin/. chmod a+x bin/*. ```. Then, I ran:. ```. cd bin; bash run-prereq.sh; cd -. ```. The `run-prereq.sh` tends to be the most tricky one - it will require root permission, and it'll install a bunch of stuff on your machine. If you can't use Docker because of root permissions, you likely won't be able to run this as well. Download test data:. ```. INPUT_DIR=""${PWD}/quickstart-testdata"". DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". mkdir -p ${INPUT_DIR}. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. ```. Run make_examples:. ```. OUTPUT_DIR=""${PWD}/quickstart-out",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/590
https://github.com/google/deepvariant/issues/590:1531,safety,test,testdata,1531,"rm"" --image-family ""ubuntu-2004-lts"" --image-project ""ubuntu-os-cloud"" --machine-type ""custom-64-131072"" --boot-disk-size ""300"" --zone ""us-west2-b"" --min-cpu-platform ""Intel Skylake""`. ssh into the machine:. ```. gcloud compute ssh pichuan-cpu --zone us-west2-b. ```. Get the binaries and models:. ```. BUCKET=""gs://deepvariant"". BIN_VERSION=""1.4.0"". MODEL_VERSION=""1.4.0"". BIN_BUCKET=""${BUCKET}/binaries/DeepVariant/${BIN_VERSION}/DeepVariant-${BIN_VERSION}"". MODEL_NAME=""DeepVariant-inception_v3-${MODEL_VERSION}+data-wgs_standard"". mkdir -p bin. # Download the DeepVariant binaries. gsutil -m cp ""${BIN_BUCKET}/*"" bin/. chmod a+x bin/*. ```. Then, I ran:. ```. cd bin; bash run-prereq.sh; cd -. ```. The `run-prereq.sh` tends to be the most tricky one - it will require root permission, and it'll install a bunch of stuff on your machine. If you can't use Docker because of root permissions, you likely won't be able to run this as well. Download test data:. ```. INPUT_DIR=""${PWD}/quickstart-testdata"". DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". mkdir -p ${INPUT_DIR}. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. ```. Run make_examples:. ```. OUTPUT_DIR=""${PWD}/quickstart-output"". mkdir -p ""${OUTPUT_DIR}"". ```. ```. python",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/590
https://github.com/google/deepvariant/issues/590:1611,safety,test,testdata,1611,"-type ""custom-64-131072"" --boot-disk-size ""300"" --zone ""us-west2-b"" --min-cpu-platform ""Intel Skylake""`. ssh into the machine:. ```. gcloud compute ssh pichuan-cpu --zone us-west2-b. ```. Get the binaries and models:. ```. BUCKET=""gs://deepvariant"". BIN_VERSION=""1.4.0"". MODEL_VERSION=""1.4.0"". BIN_BUCKET=""${BUCKET}/binaries/DeepVariant/${BIN_VERSION}/DeepVariant-${BIN_VERSION}"". MODEL_NAME=""DeepVariant-inception_v3-${MODEL_VERSION}+data-wgs_standard"". mkdir -p bin. # Download the DeepVariant binaries. gsutil -m cp ""${BIN_BUCKET}/*"" bin/. chmod a+x bin/*. ```. Then, I ran:. ```. cd bin; bash run-prereq.sh; cd -. ```. The `run-prereq.sh` tends to be the most tricky one - it will require root permission, and it'll install a bunch of stuff on your machine. If you can't use Docker because of root permissions, you likely won't be able to run this as well. Download test data:. ```. INPUT_DIR=""${PWD}/quickstart-testdata"". DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". mkdir -p ${INPUT_DIR}. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. ```. Run make_examples:. ```. OUTPUT_DIR=""${PWD}/quickstart-output"". mkdir -p ""${OUTPUT_DIR}"". ```. ```. python bin/make_examples.zip \. --mode calling \. --ref ""${INPUT_DIR}/ucsc.hg19.chr20.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/590
https://github.com/google/deepvariant/issues/590:3226,safety,test,tested,3226,"hr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. ```. Run make_examples:. ```. OUTPUT_DIR=""${PWD}/quickstart-output"". mkdir -p ""${OUTPUT_DIR}"". ```. ```. python bin/make_examples.zip \. --mode calling \. --ref ""${INPUT_DIR}/ucsc.hg19.chr20.unittest.fasta"" \. --reads ""${INPUT_DIR}/NA12878_S1.chr20.10_10p1mb.bam"" \. --regions ""chr20:10,000,000-10,010,000"" \. --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"" \. --channels ""insert_size"". ```. (To figure out which flags you need to add for each model, you can read https://github.com/google/deepvariant/blob/r1.4/scripts/run_deepvariant.py#L236-L253 . Sorry that we don't have better documentation than that right now). For how to run this with multiple shards, and how to run the rest of the commands, please read https://github.com/google/deepvariant/blob/r0.6/docs/deepvariant-quick-start.md. I just tested the steps above and confirmed that it worked for me on v1.4.0, at least for the make_examples step. If you encounter more issues with other steps, please feel free to ask again. I'd be happy to help. Note that I don't plan to put this into an official documentation page now, because that adds to our maintenance burden to keep it up to date. Given that we have the Docker/Singularity solution that works generally well for our users, I don't expect many of our users to need to use pre-built binaries. @zivlang thank you for your question so I have a chance to test it again and document it here. Hopefully this is helpful for you. Happy to answer more questions if you encounter more problems.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/590
https://github.com/google/deepvariant/issues/590:3795,safety,test,test,3795,"hr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. ```. Run make_examples:. ```. OUTPUT_DIR=""${PWD}/quickstart-output"". mkdir -p ""${OUTPUT_DIR}"". ```. ```. python bin/make_examples.zip \. --mode calling \. --ref ""${INPUT_DIR}/ucsc.hg19.chr20.unittest.fasta"" \. --reads ""${INPUT_DIR}/NA12878_S1.chr20.10_10p1mb.bam"" \. --regions ""chr20:10,000,000-10,010,000"" \. --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"" \. --channels ""insert_size"". ```. (To figure out which flags you need to add for each model, you can read https://github.com/google/deepvariant/blob/r1.4/scripts/run_deepvariant.py#L236-L253 . Sorry that we don't have better documentation than that right now). For how to run this with multiple shards, and how to run the rest of the commands, please read https://github.com/google/deepvariant/blob/r0.6/docs/deepvariant-quick-start.md. I just tested the steps above and confirmed that it worked for me on v1.4.0, at least for the make_examples step. If you encounter more issues with other steps, please feel free to ask again. I'd be happy to help. Note that I don't plan to put this into an official documentation page now, because that adds to our maintenance burden to keep it up to date. Given that we have the Docker/Singularity solution that works generally well for our users, I don't expect many of our users to need to use pre-built binaries. @zivlang thank you for your question so I have a chance to test it again and document it here. Hopefully this is helpful for you. Happy to answer more questions if you encounter more problems.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/590
https://github.com/google/deepvariant/issues/590:720,security,ssh,ssh,720,"Before you proceed, if you can't use Docker because of root permission, I recommend that you try Singularity: https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-quick-start.md#notes-on-singularity. If you don't have root permission, you won't be able to install necessary things before running the binaries either. ---. Here is what I did:. Get a machine. (Not required to run on GCP. I just use this to get a machine to test). `gcloud compute instances create ""${USER}-cpu"" --scopes ""compute-rw,storage-full,cloud-platform"" --image-family ""ubuntu-2004-lts"" --image-project ""ubuntu-os-cloud"" --machine-type ""custom-64-131072"" --boot-disk-size ""300"" --zone ""us-west2-b"" --min-cpu-platform ""Intel Skylake""`. ssh into the machine:. ```. gcloud compute ssh pichuan-cpu --zone us-west2-b. ```. Get the binaries and models:. ```. BUCKET=""gs://deepvariant"". BIN_VERSION=""1.4.0"". MODEL_VERSION=""1.4.0"". BIN_BUCKET=""${BUCKET}/binaries/DeepVariant/${BIN_VERSION}/DeepVariant-${BIN_VERSION}"". MODEL_NAME=""DeepVariant-inception_v3-${MODEL_VERSION}+data-wgs_standard"". mkdir -p bin. # Download the DeepVariant binaries. gsutil -m cp ""${BIN_BUCKET}/*"" bin/. chmod a+x bin/*. ```. Then, I ran:. ```. cd bin; bash run-prereq.sh; cd -. ```. The `run-prereq.sh` tends to be the most tricky one - it will require root permission, and it'll install a bunch of stuff on your machine. If you can't use Docker because of root permissions, you likely won't be able to run this as well. Download test data:. ```. INPUT_DIR=""${PWD}/quickstart-testdata"". DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". mkdir -p ${INPUT_DIR}. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/590
https://github.com/google/deepvariant/issues/590:763,security,ssh,ssh,763,"Before you proceed, if you can't use Docker because of root permission, I recommend that you try Singularity: https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-quick-start.md#notes-on-singularity. If you don't have root permission, you won't be able to install necessary things before running the binaries either. ---. Here is what I did:. Get a machine. (Not required to run on GCP. I just use this to get a machine to test). `gcloud compute instances create ""${USER}-cpu"" --scopes ""compute-rw,storage-full,cloud-platform"" --image-family ""ubuntu-2004-lts"" --image-project ""ubuntu-os-cloud"" --machine-type ""custom-64-131072"" --boot-disk-size ""300"" --zone ""us-west2-b"" --min-cpu-platform ""Intel Skylake""`. ssh into the machine:. ```. gcloud compute ssh pichuan-cpu --zone us-west2-b. ```. Get the binaries and models:. ```. BUCKET=""gs://deepvariant"". BIN_VERSION=""1.4.0"". MODEL_VERSION=""1.4.0"". BIN_BUCKET=""${BUCKET}/binaries/DeepVariant/${BIN_VERSION}/DeepVariant-${BIN_VERSION}"". MODEL_NAME=""DeepVariant-inception_v3-${MODEL_VERSION}+data-wgs_standard"". mkdir -p bin. # Download the DeepVariant binaries. gsutil -m cp ""${BIN_BUCKET}/*"" bin/. chmod a+x bin/*. ```. Then, I ran:. ```. cd bin; bash run-prereq.sh; cd -. ```. The `run-prereq.sh` tends to be the most tricky one - it will require root permission, and it'll install a bunch of stuff on your machine. If you can't use Docker because of root permissions, you likely won't be able to run this as well. Download test data:. ```. INPUT_DIR=""${PWD}/quickstart-testdata"". DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". mkdir -p ${INPUT_DIR}. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/590
https://github.com/google/deepvariant/issues/590:824,security,model,models,824,"Before you proceed, if you can't use Docker because of root permission, I recommend that you try Singularity: https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-quick-start.md#notes-on-singularity. If you don't have root permission, you won't be able to install necessary things before running the binaries either. ---. Here is what I did:. Get a machine. (Not required to run on GCP. I just use this to get a machine to test). `gcloud compute instances create ""${USER}-cpu"" --scopes ""compute-rw,storage-full,cloud-platform"" --image-family ""ubuntu-2004-lts"" --image-project ""ubuntu-os-cloud"" --machine-type ""custom-64-131072"" --boot-disk-size ""300"" --zone ""us-west2-b"" --min-cpu-platform ""Intel Skylake""`. ssh into the machine:. ```. gcloud compute ssh pichuan-cpu --zone us-west2-b. ```. Get the binaries and models:. ```. BUCKET=""gs://deepvariant"". BIN_VERSION=""1.4.0"". MODEL_VERSION=""1.4.0"". BIN_BUCKET=""${BUCKET}/binaries/DeepVariant/${BIN_VERSION}/DeepVariant-${BIN_VERSION}"". MODEL_NAME=""DeepVariant-inception_v3-${MODEL_VERSION}+data-wgs_standard"". mkdir -p bin. # Download the DeepVariant binaries. gsutil -m cp ""${BIN_BUCKET}/*"" bin/. chmod a+x bin/*. ```. Then, I ran:. ```. cd bin; bash run-prereq.sh; cd -. ```. The `run-prereq.sh` tends to be the most tricky one - it will require root permission, and it'll install a bunch of stuff on your machine. If you can't use Docker because of root permissions, you likely won't be able to run this as well. Download test data:. ```. INPUT_DIR=""${PWD}/quickstart-testdata"". DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". mkdir -p ${INPUT_DIR}. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/590
https://github.com/google/deepvariant/issues/590:2868,security,model,model,2868,". wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. ```. Run make_examples:. ```. OUTPUT_DIR=""${PWD}/quickstart-output"". mkdir -p ""${OUTPUT_DIR}"". ```. ```. python bin/make_examples.zip \. --mode calling \. --ref ""${INPUT_DIR}/ucsc.hg19.chr20.unittest.fasta"" \. --reads ""${INPUT_DIR}/NA12878_S1.chr20.10_10p1mb.bam"" \. --regions ""chr20:10,000,000-10,010,000"" \. --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"" \. --channels ""insert_size"". ```. (To figure out which flags you need to add for each model, you can read https://github.com/google/deepvariant/blob/r1.4/scripts/run_deepvariant.py#L236-L253 . Sorry that we don't have better documentation than that right now). For how to run this with multiple shards, and how to run the rest of the commands, please read https://github.com/google/deepvariant/blob/r0.6/docs/deepvariant-quick-start.md. I just tested the steps above and confirmed that it worked for me on v1.4.0, at least for the make_examples step. If you encounter more issues with other steps, please feel free to ask again. I'd be happy to help. Note that I don't plan to put this into an official documentation page now, because that adds to our maintenance burden to keep it up to date. Given that we have the Docker/Singularity solution that works generally well for our users, I don't expect many of our users to need to use pre-built binaries. @zivlang thank you for your question so I have a chance to test it again and document it here. Hopefully this is helpful for you. Happy",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/590
https://github.com/google/deepvariant/issues/590:435,testability,test,test,435,"Before you proceed, if you can't use Docker because of root permission, I recommend that you try Singularity: https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-quick-start.md#notes-on-singularity. If you don't have root permission, you won't be able to install necessary things before running the binaries either. ---. Here is what I did:. Get a machine. (Not required to run on GCP. I just use this to get a machine to test). `gcloud compute instances create ""${USER}-cpu"" --scopes ""compute-rw,storage-full,cloud-platform"" --image-family ""ubuntu-2004-lts"" --image-project ""ubuntu-os-cloud"" --machine-type ""custom-64-131072"" --boot-disk-size ""300"" --zone ""us-west2-b"" --min-cpu-platform ""Intel Skylake""`. ssh into the machine:. ```. gcloud compute ssh pichuan-cpu --zone us-west2-b. ```. Get the binaries and models:. ```. BUCKET=""gs://deepvariant"". BIN_VERSION=""1.4.0"". MODEL_VERSION=""1.4.0"". BIN_BUCKET=""${BUCKET}/binaries/DeepVariant/${BIN_VERSION}/DeepVariant-${BIN_VERSION}"". MODEL_NAME=""DeepVariant-inception_v3-${MODEL_VERSION}+data-wgs_standard"". mkdir -p bin. # Download the DeepVariant binaries. gsutil -m cp ""${BIN_BUCKET}/*"" bin/. chmod a+x bin/*. ```. Then, I ran:. ```. cd bin; bash run-prereq.sh; cd -. ```. The `run-prereq.sh` tends to be the most tricky one - it will require root permission, and it'll install a bunch of stuff on your machine. If you can't use Docker because of root permissions, you likely won't be able to run this as well. Download test data:. ```. INPUT_DIR=""${PWD}/quickstart-testdata"". DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". mkdir -p ${INPUT_DIR}. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/590
https://github.com/google/deepvariant/issues/590:1485,testability,test,test,1485,""" --scopes ""compute-rw,storage-full,cloud-platform"" --image-family ""ubuntu-2004-lts"" --image-project ""ubuntu-os-cloud"" --machine-type ""custom-64-131072"" --boot-disk-size ""300"" --zone ""us-west2-b"" --min-cpu-platform ""Intel Skylake""`. ssh into the machine:. ```. gcloud compute ssh pichuan-cpu --zone us-west2-b. ```. Get the binaries and models:. ```. BUCKET=""gs://deepvariant"". BIN_VERSION=""1.4.0"". MODEL_VERSION=""1.4.0"". BIN_BUCKET=""${BUCKET}/binaries/DeepVariant/${BIN_VERSION}/DeepVariant-${BIN_VERSION}"". MODEL_NAME=""DeepVariant-inception_v3-${MODEL_VERSION}+data-wgs_standard"". mkdir -p bin. # Download the DeepVariant binaries. gsutil -m cp ""${BIN_BUCKET}/*"" bin/. chmod a+x bin/*. ```. Then, I ran:. ```. cd bin; bash run-prereq.sh; cd -. ```. The `run-prereq.sh` tends to be the most tricky one - it will require root permission, and it'll install a bunch of stuff on your machine. If you can't use Docker because of root permissions, you likely won't be able to run this as well. Download test data:. ```. INPUT_DIR=""${PWD}/quickstart-testdata"". DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". mkdir -p ${INPUT_DIR}. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. ```. Run make_examples:. ```. OUTPUT_DIR=""${PWD}/quickstart-out",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/590
https://github.com/google/deepvariant/issues/590:1531,testability,test,testdata,1531,"rm"" --image-family ""ubuntu-2004-lts"" --image-project ""ubuntu-os-cloud"" --machine-type ""custom-64-131072"" --boot-disk-size ""300"" --zone ""us-west2-b"" --min-cpu-platform ""Intel Skylake""`. ssh into the machine:. ```. gcloud compute ssh pichuan-cpu --zone us-west2-b. ```. Get the binaries and models:. ```. BUCKET=""gs://deepvariant"". BIN_VERSION=""1.4.0"". MODEL_VERSION=""1.4.0"". BIN_BUCKET=""${BUCKET}/binaries/DeepVariant/${BIN_VERSION}/DeepVariant-${BIN_VERSION}"". MODEL_NAME=""DeepVariant-inception_v3-${MODEL_VERSION}+data-wgs_standard"". mkdir -p bin. # Download the DeepVariant binaries. gsutil -m cp ""${BIN_BUCKET}/*"" bin/. chmod a+x bin/*. ```. Then, I ran:. ```. cd bin; bash run-prereq.sh; cd -. ```. The `run-prereq.sh` tends to be the most tricky one - it will require root permission, and it'll install a bunch of stuff on your machine. If you can't use Docker because of root permissions, you likely won't be able to run this as well. Download test data:. ```. INPUT_DIR=""${PWD}/quickstart-testdata"". DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". mkdir -p ${INPUT_DIR}. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. ```. Run make_examples:. ```. OUTPUT_DIR=""${PWD}/quickstart-output"". mkdir -p ""${OUTPUT_DIR}"". ```. ```. python",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/590
https://github.com/google/deepvariant/issues/590:1611,testability,test,testdata,1611,"-type ""custom-64-131072"" --boot-disk-size ""300"" --zone ""us-west2-b"" --min-cpu-platform ""Intel Skylake""`. ssh into the machine:. ```. gcloud compute ssh pichuan-cpu --zone us-west2-b. ```. Get the binaries and models:. ```. BUCKET=""gs://deepvariant"". BIN_VERSION=""1.4.0"". MODEL_VERSION=""1.4.0"". BIN_BUCKET=""${BUCKET}/binaries/DeepVariant/${BIN_VERSION}/DeepVariant-${BIN_VERSION}"". MODEL_NAME=""DeepVariant-inception_v3-${MODEL_VERSION}+data-wgs_standard"". mkdir -p bin. # Download the DeepVariant binaries. gsutil -m cp ""${BIN_BUCKET}/*"" bin/. chmod a+x bin/*. ```. Then, I ran:. ```. cd bin; bash run-prereq.sh; cd -. ```. The `run-prereq.sh` tends to be the most tricky one - it will require root permission, and it'll install a bunch of stuff on your machine. If you can't use Docker because of root permissions, you likely won't be able to run this as well. Download test data:. ```. INPUT_DIR=""${PWD}/quickstart-testdata"". DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". mkdir -p ${INPUT_DIR}. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. ```. Run make_examples:. ```. OUTPUT_DIR=""${PWD}/quickstart-output"". mkdir -p ""${OUTPUT_DIR}"". ```. ```. python bin/make_examples.zip \. --mode calling \. --ref ""${INPUT_DIR}/ucsc.hg19.chr20.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/590
https://github.com/google/deepvariant/issues/590:2099,testability,unit,unittest,2099,"Variant binaries. gsutil -m cp ""${BIN_BUCKET}/*"" bin/. chmod a+x bin/*. ```. Then, I ran:. ```. cd bin; bash run-prereq.sh; cd -. ```. The `run-prereq.sh` tends to be the most tricky one - it will require root permission, and it'll install a bunch of stuff on your machine. If you can't use Docker because of root permissions, you likely won't be able to run this as well. Download test data:. ```. INPUT_DIR=""${PWD}/quickstart-testdata"". DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". mkdir -p ${INPUT_DIR}. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. ```. Run make_examples:. ```. OUTPUT_DIR=""${PWD}/quickstart-output"". mkdir -p ""${OUTPUT_DIR}"". ```. ```. python bin/make_examples.zip \. --mode calling \. --ref ""${INPUT_DIR}/ucsc.hg19.chr20.unittest.fasta"" \. --reads ""${INPUT_DIR}/NA12878_S1.chr20.10_10p1mb.bam"" \. --regions ""chr20:10,000,000-10,010,000"" \. --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"" \. --channels ""insert_size"". ```. (To figure out which flags you need to add for each model, you can read https://github.com/google/deepvariant/blob/r1.4/scripts/run_deepvariant.py#L236-L253 . Sorry that we don't have better documentation than that right now). For how to run this with multiple shards, and how to run the",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/590
https://github.com/google/deepvariant/issues/590:2171,testability,unit,unittest,2171,"```. Then, I ran:. ```. cd bin; bash run-prereq.sh; cd -. ```. The `run-prereq.sh` tends to be the most tricky one - it will require root permission, and it'll install a bunch of stuff on your machine. If you can't use Docker because of root permissions, you likely won't be able to run this as well. Download test data:. ```. INPUT_DIR=""${PWD}/quickstart-testdata"". DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". mkdir -p ${INPUT_DIR}. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. ```. Run make_examples:. ```. OUTPUT_DIR=""${PWD}/quickstart-output"". mkdir -p ""${OUTPUT_DIR}"". ```. ```. python bin/make_examples.zip \. --mode calling \. --ref ""${INPUT_DIR}/ucsc.hg19.chr20.unittest.fasta"" \. --reads ""${INPUT_DIR}/NA12878_S1.chr20.10_10p1mb.bam"" \. --regions ""chr20:10,000,000-10,010,000"" \. --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"" \. --channels ""insert_size"". ```. (To figure out which flags you need to add for each model, you can read https://github.com/google/deepvariant/blob/r1.4/scripts/run_deepvariant.py#L236-L253 . Sorry that we don't have better documentation than that right now). For how to run this with multiple shards, and how to run the rest of the commands, please read https://github.com/google/deepvariant",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/590
https://github.com/google/deepvariant/issues/590:2247,testability,unit,unittest,2247,"eq.sh` tends to be the most tricky one - it will require root permission, and it'll install a bunch of stuff on your machine. If you can't use Docker because of root permissions, you likely won't be able to run this as well. Download test data:. ```. INPUT_DIR=""${PWD}/quickstart-testdata"". DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". mkdir -p ${INPUT_DIR}. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. ```. Run make_examples:. ```. OUTPUT_DIR=""${PWD}/quickstart-output"". mkdir -p ""${OUTPUT_DIR}"". ```. ```. python bin/make_examples.zip \. --mode calling \. --ref ""${INPUT_DIR}/ucsc.hg19.chr20.unittest.fasta"" \. --reads ""${INPUT_DIR}/NA12878_S1.chr20.10_10p1mb.bam"" \. --regions ""chr20:10,000,000-10,010,000"" \. --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"" \. --channels ""insert_size"". ```. (To figure out which flags you need to add for each model, you can read https://github.com/google/deepvariant/blob/r1.4/scripts/run_deepvariant.py#L236-L253 . Sorry that we don't have better documentation than that right now). For how to run this with multiple shards, and how to run the rest of the commands, please read https://github.com/google/deepvariant/blob/r0.6/docs/deepvariant-quick-start.md. I just tested the steps above an",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/590
https://github.com/google/deepvariant/issues/590:2322,testability,unit,unittest,2322,"nd it'll install a bunch of stuff on your machine. If you can't use Docker because of root permissions, you likely won't be able to run this as well. Download test data:. ```. INPUT_DIR=""${PWD}/quickstart-testdata"". DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". mkdir -p ${INPUT_DIR}. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. ```. Run make_examples:. ```. OUTPUT_DIR=""${PWD}/quickstart-output"". mkdir -p ""${OUTPUT_DIR}"". ```. ```. python bin/make_examples.zip \. --mode calling \. --ref ""${INPUT_DIR}/ucsc.hg19.chr20.unittest.fasta"" \. --reads ""${INPUT_DIR}/NA12878_S1.chr20.10_10p1mb.bam"" \. --regions ""chr20:10,000,000-10,010,000"" \. --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"" \. --channels ""insert_size"". ```. (To figure out which flags you need to add for each model, you can read https://github.com/google/deepvariant/blob/r1.4/scripts/run_deepvariant.py#L236-L253 . Sorry that we don't have better documentation than that right now). For how to run this with multiple shards, and how to run the rest of the commands, please read https://github.com/google/deepvariant/blob/r0.6/docs/deepvariant-quick-start.md. I just tested the steps above and confirmed that it worked for me on v1.4.0, at least for the make_examples",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/590
https://github.com/google/deepvariant/issues/590:2401,testability,unit,unittest,2401,"use of root permissions, you likely won't be able to run this as well. Download test data:. ```. INPUT_DIR=""${PWD}/quickstart-testdata"". DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". mkdir -p ${INPUT_DIR}. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. ```. Run make_examples:. ```. OUTPUT_DIR=""${PWD}/quickstart-output"". mkdir -p ""${OUTPUT_DIR}"". ```. ```. python bin/make_examples.zip \. --mode calling \. --ref ""${INPUT_DIR}/ucsc.hg19.chr20.unittest.fasta"" \. --reads ""${INPUT_DIR}/NA12878_S1.chr20.10_10p1mb.bam"" \. --regions ""chr20:10,000,000-10,010,000"" \. --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"" \. --channels ""insert_size"". ```. (To figure out which flags you need to add for each model, you can read https://github.com/google/deepvariant/blob/r1.4/scripts/run_deepvariant.py#L236-L253 . Sorry that we don't have better documentation than that right now). For how to run this with multiple shards, and how to run the rest of the commands, please read https://github.com/google/deepvariant/blob/r0.6/docs/deepvariant-quick-start.md. I just tested the steps above and confirmed that it worked for me on v1.4.0, at least for the make_examples step. If you encounter more issues with other steps, please feel free to ask a",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/590
https://github.com/google/deepvariant/issues/590:2615,testability,unit,unittest,2615,""". mkdir -p ${INPUT_DIR}. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. ```. Run make_examples:. ```. OUTPUT_DIR=""${PWD}/quickstart-output"". mkdir -p ""${OUTPUT_DIR}"". ```. ```. python bin/make_examples.zip \. --mode calling \. --ref ""${INPUT_DIR}/ucsc.hg19.chr20.unittest.fasta"" \. --reads ""${INPUT_DIR}/NA12878_S1.chr20.10_10p1mb.bam"" \. --regions ""chr20:10,000,000-10,010,000"" \. --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"" \. --channels ""insert_size"". ```. (To figure out which flags you need to add for each model, you can read https://github.com/google/deepvariant/blob/r1.4/scripts/run_deepvariant.py#L236-L253 . Sorry that we don't have better documentation than that right now). For how to run this with multiple shards, and how to run the rest of the commands, please read https://github.com/google/deepvariant/blob/r0.6/docs/deepvariant-quick-start.md. I just tested the steps above and confirmed that it worked for me on v1.4.0, at least for the make_examples step. If you encounter more issues with other steps, please feel free to ask again. I'd be happy to help. Note that I don't plan to put this into an official documentation page now, because that adds to our maintenance burden to keep it up to date. Given that we have the Docker/Singularity s",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/590
https://github.com/google/deepvariant/issues/590:3226,testability,test,tested,3226,"hr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. ```. Run make_examples:. ```. OUTPUT_DIR=""${PWD}/quickstart-output"". mkdir -p ""${OUTPUT_DIR}"". ```. ```. python bin/make_examples.zip \. --mode calling \. --ref ""${INPUT_DIR}/ucsc.hg19.chr20.unittest.fasta"" \. --reads ""${INPUT_DIR}/NA12878_S1.chr20.10_10p1mb.bam"" \. --regions ""chr20:10,000,000-10,010,000"" \. --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"" \. --channels ""insert_size"". ```. (To figure out which flags you need to add for each model, you can read https://github.com/google/deepvariant/blob/r1.4/scripts/run_deepvariant.py#L236-L253 . Sorry that we don't have better documentation than that right now). For how to run this with multiple shards, and how to run the rest of the commands, please read https://github.com/google/deepvariant/blob/r0.6/docs/deepvariant-quick-start.md. I just tested the steps above and confirmed that it worked for me on v1.4.0, at least for the make_examples step. If you encounter more issues with other steps, please feel free to ask again. I'd be happy to help. Note that I don't plan to put this into an official documentation page now, because that adds to our maintenance burden to keep it up to date. Given that we have the Docker/Singularity solution that works generally well for our users, I don't expect many of our users to need to use pre-built binaries. @zivlang thank you for your question so I have a chance to test it again and document it here. Hopefully this is helpful for you. Happy to answer more questions if you encounter more problems.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/590
https://github.com/google/deepvariant/issues/590:3451,testability,plan,plan,3451,"hr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. ```. Run make_examples:. ```. OUTPUT_DIR=""${PWD}/quickstart-output"". mkdir -p ""${OUTPUT_DIR}"". ```. ```. python bin/make_examples.zip \. --mode calling \. --ref ""${INPUT_DIR}/ucsc.hg19.chr20.unittest.fasta"" \. --reads ""${INPUT_DIR}/NA12878_S1.chr20.10_10p1mb.bam"" \. --regions ""chr20:10,000,000-10,010,000"" \. --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"" \. --channels ""insert_size"". ```. (To figure out which flags you need to add for each model, you can read https://github.com/google/deepvariant/blob/r1.4/scripts/run_deepvariant.py#L236-L253 . Sorry that we don't have better documentation than that right now). For how to run this with multiple shards, and how to run the rest of the commands, please read https://github.com/google/deepvariant/blob/r0.6/docs/deepvariant-quick-start.md. I just tested the steps above and confirmed that it worked for me on v1.4.0, at least for the make_examples step. If you encounter more issues with other steps, please feel free to ask again. I'd be happy to help. Note that I don't plan to put this into an official documentation page now, because that adds to our maintenance burden to keep it up to date. Given that we have the Docker/Singularity solution that works generally well for our users, I don't expect many of our users to need to use pre-built binaries. @zivlang thank you for your question so I have a chance to test it again and document it here. Hopefully this is helpful for you. Happy to answer more questions if you encounter more problems.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/590
https://github.com/google/deepvariant/issues/590:3795,testability,test,test,3795,"hr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. ```. Run make_examples:. ```. OUTPUT_DIR=""${PWD}/quickstart-output"". mkdir -p ""${OUTPUT_DIR}"". ```. ```. python bin/make_examples.zip \. --mode calling \. --ref ""${INPUT_DIR}/ucsc.hg19.chr20.unittest.fasta"" \. --reads ""${INPUT_DIR}/NA12878_S1.chr20.10_10p1mb.bam"" \. --regions ""chr20:10,000,000-10,010,000"" \. --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"" \. --channels ""insert_size"". ```. (To figure out which flags you need to add for each model, you can read https://github.com/google/deepvariant/blob/r1.4/scripts/run_deepvariant.py#L236-L253 . Sorry that we don't have better documentation than that right now). For how to run this with multiple shards, and how to run the rest of the commands, please read https://github.com/google/deepvariant/blob/r0.6/docs/deepvariant-quick-start.md. I just tested the steps above and confirmed that it worked for me on v1.4.0, at least for the make_examples step. If you encounter more issues with other steps, please feel free to ask again. I'd be happy to help. Note that I don't plan to put this into an official documentation page now, because that adds to our maintenance burden to keep it up to date. Given that we have the Docker/Singularity solution that works generally well for our users, I don't expect many of our users to need to use pre-built binaries. @zivlang thank you for your question so I have a chance to test it again and document it here. Hopefully this is helpful for you. Happy to answer more questions if you encounter more problems.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/590
https://github.com/google/deepvariant/issues/590:478,usability,USER,USER,478,"Before you proceed, if you can't use Docker because of root permission, I recommend that you try Singularity: https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-quick-start.md#notes-on-singularity. If you don't have root permission, you won't be able to install necessary things before running the binaries either. ---. Here is what I did:. Get a machine. (Not required to run on GCP. I just use this to get a machine to test). `gcloud compute instances create ""${USER}-cpu"" --scopes ""compute-rw,storage-full,cloud-platform"" --image-family ""ubuntu-2004-lts"" --image-project ""ubuntu-os-cloud"" --machine-type ""custom-64-131072"" --boot-disk-size ""300"" --zone ""us-west2-b"" --min-cpu-platform ""Intel Skylake""`. ssh into the machine:. ```. gcloud compute ssh pichuan-cpu --zone us-west2-b. ```. Get the binaries and models:. ```. BUCKET=""gs://deepvariant"". BIN_VERSION=""1.4.0"". MODEL_VERSION=""1.4.0"". BIN_BUCKET=""${BUCKET}/binaries/DeepVariant/${BIN_VERSION}/DeepVariant-${BIN_VERSION}"". MODEL_NAME=""DeepVariant-inception_v3-${MODEL_VERSION}+data-wgs_standard"". mkdir -p bin. # Download the DeepVariant binaries. gsutil -m cp ""${BIN_BUCKET}/*"" bin/. chmod a+x bin/*. ```. Then, I ran:. ```. cd bin; bash run-prereq.sh; cd -. ```. The `run-prereq.sh` tends to be the most tricky one - it will require root permission, and it'll install a bunch of stuff on your machine. If you can't use Docker because of root permissions, you likely won't be able to run this as well. Download test data:. ```. INPUT_DIR=""${PWD}/quickstart-testdata"". DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". mkdir -p ${INPUT_DIR}. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/590
https://github.com/google/deepvariant/issues/590:622,usability,custom,custom-,622,"Before you proceed, if you can't use Docker because of root permission, I recommend that you try Singularity: https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-quick-start.md#notes-on-singularity. If you don't have root permission, you won't be able to install necessary things before running the binaries either. ---. Here is what I did:. Get a machine. (Not required to run on GCP. I just use this to get a machine to test). `gcloud compute instances create ""${USER}-cpu"" --scopes ""compute-rw,storage-full,cloud-platform"" --image-family ""ubuntu-2004-lts"" --image-project ""ubuntu-os-cloud"" --machine-type ""custom-64-131072"" --boot-disk-size ""300"" --zone ""us-west2-b"" --min-cpu-platform ""Intel Skylake""`. ssh into the machine:. ```. gcloud compute ssh pichuan-cpu --zone us-west2-b. ```. Get the binaries and models:. ```. BUCKET=""gs://deepvariant"". BIN_VERSION=""1.4.0"". MODEL_VERSION=""1.4.0"". BIN_BUCKET=""${BUCKET}/binaries/DeepVariant/${BIN_VERSION}/DeepVariant-${BIN_VERSION}"". MODEL_NAME=""DeepVariant-inception_v3-${MODEL_VERSION}+data-wgs_standard"". mkdir -p bin. # Download the DeepVariant binaries. gsutil -m cp ""${BIN_BUCKET}/*"" bin/. chmod a+x bin/*. ```. Then, I ran:. ```. cd bin; bash run-prereq.sh; cd -. ```. The `run-prereq.sh` tends to be the most tricky one - it will require root permission, and it'll install a bunch of stuff on your machine. If you can't use Docker because of root permissions, you likely won't be able to run this as well. Download test data:. ```. INPUT_DIR=""${PWD}/quickstart-testdata"". DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". mkdir -p ${INPUT_DIR}. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/590
https://github.com/google/deepvariant/issues/590:3007,usability,document,documentation,3007,"hr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. ```. Run make_examples:. ```. OUTPUT_DIR=""${PWD}/quickstart-output"". mkdir -p ""${OUTPUT_DIR}"". ```. ```. python bin/make_examples.zip \. --mode calling \. --ref ""${INPUT_DIR}/ucsc.hg19.chr20.unittest.fasta"" \. --reads ""${INPUT_DIR}/NA12878_S1.chr20.10_10p1mb.bam"" \. --regions ""chr20:10,000,000-10,010,000"" \. --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"" \. --channels ""insert_size"". ```. (To figure out which flags you need to add for each model, you can read https://github.com/google/deepvariant/blob/r1.4/scripts/run_deepvariant.py#L236-L253 . Sorry that we don't have better documentation than that right now). For how to run this with multiple shards, and how to run the rest of the commands, please read https://github.com/google/deepvariant/blob/r0.6/docs/deepvariant-quick-start.md. I just tested the steps above and confirmed that it worked for me on v1.4.0, at least for the make_examples step. If you encounter more issues with other steps, please feel free to ask again. I'd be happy to help. Note that I don't plan to put this into an official documentation page now, because that adds to our maintenance burden to keep it up to date. Given that we have the Docker/Singularity solution that works generally well for our users, I don't expect many of our users to need to use pre-built binaries. @zivlang thank you for your question so I have a chance to test it again and document it here. Hopefully this is helpful for you. Happy to answer more questions if you encounter more problems.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/590
https://github.com/google/deepvariant/issues/590:3116,usability,command,commands,3116,"hr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. ```. Run make_examples:. ```. OUTPUT_DIR=""${PWD}/quickstart-output"". mkdir -p ""${OUTPUT_DIR}"". ```. ```. python bin/make_examples.zip \. --mode calling \. --ref ""${INPUT_DIR}/ucsc.hg19.chr20.unittest.fasta"" \. --reads ""${INPUT_DIR}/NA12878_S1.chr20.10_10p1mb.bam"" \. --regions ""chr20:10,000,000-10,010,000"" \. --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"" \. --channels ""insert_size"". ```. (To figure out which flags you need to add for each model, you can read https://github.com/google/deepvariant/blob/r1.4/scripts/run_deepvariant.py#L236-L253 . Sorry that we don't have better documentation than that right now). For how to run this with multiple shards, and how to run the rest of the commands, please read https://github.com/google/deepvariant/blob/r0.6/docs/deepvariant-quick-start.md. I just tested the steps above and confirmed that it worked for me on v1.4.0, at least for the make_examples step. If you encounter more issues with other steps, please feel free to ask again. I'd be happy to help. Note that I don't plan to put this into an official documentation page now, because that adds to our maintenance burden to keep it up to date. Given that we have the Docker/Singularity solution that works generally well for our users, I don't expect many of our users to need to use pre-built binaries. @zivlang thank you for your question so I have a chance to test it again and document it here. Hopefully this is helpful for you. Happy to answer more questions if you encounter more problems.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/590
https://github.com/google/deepvariant/issues/590:3253,usability,confirm,confirmed,3253,"hr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. ```. Run make_examples:. ```. OUTPUT_DIR=""${PWD}/quickstart-output"". mkdir -p ""${OUTPUT_DIR}"". ```. ```. python bin/make_examples.zip \. --mode calling \. --ref ""${INPUT_DIR}/ucsc.hg19.chr20.unittest.fasta"" \. --reads ""${INPUT_DIR}/NA12878_S1.chr20.10_10p1mb.bam"" \. --regions ""chr20:10,000,000-10,010,000"" \. --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"" \. --channels ""insert_size"". ```. (To figure out which flags you need to add for each model, you can read https://github.com/google/deepvariant/blob/r1.4/scripts/run_deepvariant.py#L236-L253 . Sorry that we don't have better documentation than that right now). For how to run this with multiple shards, and how to run the rest of the commands, please read https://github.com/google/deepvariant/blob/r0.6/docs/deepvariant-quick-start.md. I just tested the steps above and confirmed that it worked for me on v1.4.0, at least for the make_examples step. If you encounter more issues with other steps, please feel free to ask again. I'd be happy to help. Note that I don't plan to put this into an official documentation page now, because that adds to our maintenance burden to keep it up to date. Given that we have the Docker/Singularity solution that works generally well for our users, I don't expect many of our users to need to use pre-built binaries. @zivlang thank you for your question so I have a chance to test it again and document it here. Hopefully this is helpful for you. Happy to answer more questions if you encounter more problems.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/590
https://github.com/google/deepvariant/issues/590:3427,usability,help,help,3427,"hr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. ```. Run make_examples:. ```. OUTPUT_DIR=""${PWD}/quickstart-output"". mkdir -p ""${OUTPUT_DIR}"". ```. ```. python bin/make_examples.zip \. --mode calling \. --ref ""${INPUT_DIR}/ucsc.hg19.chr20.unittest.fasta"" \. --reads ""${INPUT_DIR}/NA12878_S1.chr20.10_10p1mb.bam"" \. --regions ""chr20:10,000,000-10,010,000"" \. --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"" \. --channels ""insert_size"". ```. (To figure out which flags you need to add for each model, you can read https://github.com/google/deepvariant/blob/r1.4/scripts/run_deepvariant.py#L236-L253 . Sorry that we don't have better documentation than that right now). For how to run this with multiple shards, and how to run the rest of the commands, please read https://github.com/google/deepvariant/blob/r0.6/docs/deepvariant-quick-start.md. I just tested the steps above and confirmed that it worked for me on v1.4.0, at least for the make_examples step. If you encounter more issues with other steps, please feel free to ask again. I'd be happy to help. Note that I don't plan to put this into an official documentation page now, because that adds to our maintenance burden to keep it up to date. Given that we have the Docker/Singularity solution that works generally well for our users, I don't expect many of our users to need to use pre-built binaries. @zivlang thank you for your question so I have a chance to test it again and document it here. Hopefully this is helpful for you. Happy to answer more questions if you encounter more problems.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/590
https://github.com/google/deepvariant/issues/590:3485,usability,document,documentation,3485,"hr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. ```. Run make_examples:. ```. OUTPUT_DIR=""${PWD}/quickstart-output"". mkdir -p ""${OUTPUT_DIR}"". ```. ```. python bin/make_examples.zip \. --mode calling \. --ref ""${INPUT_DIR}/ucsc.hg19.chr20.unittest.fasta"" \. --reads ""${INPUT_DIR}/NA12878_S1.chr20.10_10p1mb.bam"" \. --regions ""chr20:10,000,000-10,010,000"" \. --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"" \. --channels ""insert_size"". ```. (To figure out which flags you need to add for each model, you can read https://github.com/google/deepvariant/blob/r1.4/scripts/run_deepvariant.py#L236-L253 . Sorry that we don't have better documentation than that right now). For how to run this with multiple shards, and how to run the rest of the commands, please read https://github.com/google/deepvariant/blob/r0.6/docs/deepvariant-quick-start.md. I just tested the steps above and confirmed that it worked for me on v1.4.0, at least for the make_examples step. If you encounter more issues with other steps, please feel free to ask again. I'd be happy to help. Note that I don't plan to put this into an official documentation page now, because that adds to our maintenance burden to keep it up to date. Given that we have the Docker/Singularity solution that works generally well for our users, I don't expect many of our users to need to use pre-built binaries. @zivlang thank you for your question so I have a chance to test it again and document it here. Hopefully this is helpful for you. Happy to answer more questions if you encounter more problems.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/590
https://github.com/google/deepvariant/issues/590:3661,usability,user,users,3661,"hr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. ```. Run make_examples:. ```. OUTPUT_DIR=""${PWD}/quickstart-output"". mkdir -p ""${OUTPUT_DIR}"". ```. ```. python bin/make_examples.zip \. --mode calling \. --ref ""${INPUT_DIR}/ucsc.hg19.chr20.unittest.fasta"" \. --reads ""${INPUT_DIR}/NA12878_S1.chr20.10_10p1mb.bam"" \. --regions ""chr20:10,000,000-10,010,000"" \. --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"" \. --channels ""insert_size"". ```. (To figure out which flags you need to add for each model, you can read https://github.com/google/deepvariant/blob/r1.4/scripts/run_deepvariant.py#L236-L253 . Sorry that we don't have better documentation than that right now). For how to run this with multiple shards, and how to run the rest of the commands, please read https://github.com/google/deepvariant/blob/r0.6/docs/deepvariant-quick-start.md. I just tested the steps above and confirmed that it worked for me on v1.4.0, at least for the make_examples step. If you encounter more issues with other steps, please feel free to ask again. I'd be happy to help. Note that I don't plan to put this into an official documentation page now, because that adds to our maintenance burden to keep it up to date. Given that we have the Docker/Singularity solution that works generally well for our users, I don't expect many of our users to need to use pre-built binaries. @zivlang thank you for your question so I have a chance to test it again and document it here. Hopefully this is helpful for you. Happy to answer more questions if you encounter more problems.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/590
https://github.com/google/deepvariant/issues/590:3695,usability,user,users,3695,"hr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. ```. Run make_examples:. ```. OUTPUT_DIR=""${PWD}/quickstart-output"". mkdir -p ""${OUTPUT_DIR}"". ```. ```. python bin/make_examples.zip \. --mode calling \. --ref ""${INPUT_DIR}/ucsc.hg19.chr20.unittest.fasta"" \. --reads ""${INPUT_DIR}/NA12878_S1.chr20.10_10p1mb.bam"" \. --regions ""chr20:10,000,000-10,010,000"" \. --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"" \. --channels ""insert_size"". ```. (To figure out which flags you need to add for each model, you can read https://github.com/google/deepvariant/blob/r1.4/scripts/run_deepvariant.py#L236-L253 . Sorry that we don't have better documentation than that right now). For how to run this with multiple shards, and how to run the rest of the commands, please read https://github.com/google/deepvariant/blob/r0.6/docs/deepvariant-quick-start.md. I just tested the steps above and confirmed that it worked for me on v1.4.0, at least for the make_examples step. If you encounter more issues with other steps, please feel free to ask again. I'd be happy to help. Note that I don't plan to put this into an official documentation page now, because that adds to our maintenance burden to keep it up to date. Given that we have the Docker/Singularity solution that works generally well for our users, I don't expect many of our users to need to use pre-built binaries. @zivlang thank you for your question so I have a chance to test it again and document it here. Hopefully this is helpful for you. Happy to answer more questions if you encounter more problems.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/590
https://github.com/google/deepvariant/issues/590:3813,usability,document,document,3813,"hr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. ```. Run make_examples:. ```. OUTPUT_DIR=""${PWD}/quickstart-output"". mkdir -p ""${OUTPUT_DIR}"". ```. ```. python bin/make_examples.zip \. --mode calling \. --ref ""${INPUT_DIR}/ucsc.hg19.chr20.unittest.fasta"" \. --reads ""${INPUT_DIR}/NA12878_S1.chr20.10_10p1mb.bam"" \. --regions ""chr20:10,000,000-10,010,000"" \. --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"" \. --channels ""insert_size"". ```. (To figure out which flags you need to add for each model, you can read https://github.com/google/deepvariant/blob/r1.4/scripts/run_deepvariant.py#L236-L253 . Sorry that we don't have better documentation than that right now). For how to run this with multiple shards, and how to run the rest of the commands, please read https://github.com/google/deepvariant/blob/r0.6/docs/deepvariant-quick-start.md. I just tested the steps above and confirmed that it worked for me on v1.4.0, at least for the make_examples step. If you encounter more issues with other steps, please feel free to ask again. I'd be happy to help. Note that I don't plan to put this into an official documentation page now, because that adds to our maintenance burden to keep it up to date. Given that we have the Docker/Singularity solution that works generally well for our users, I don't expect many of our users to need to use pre-built binaries. @zivlang thank you for your question so I have a chance to test it again and document it here. Hopefully this is helpful for you. Happy to answer more questions if you encounter more problems.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/590
https://github.com/google/deepvariant/issues/590:3849,usability,help,helpful,3849,"hr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. ```. Run make_examples:. ```. OUTPUT_DIR=""${PWD}/quickstart-output"". mkdir -p ""${OUTPUT_DIR}"". ```. ```. python bin/make_examples.zip \. --mode calling \. --ref ""${INPUT_DIR}/ucsc.hg19.chr20.unittest.fasta"" \. --reads ""${INPUT_DIR}/NA12878_S1.chr20.10_10p1mb.bam"" \. --regions ""chr20:10,000,000-10,010,000"" \. --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"" \. --channels ""insert_size"". ```. (To figure out which flags you need to add for each model, you can read https://github.com/google/deepvariant/blob/r1.4/scripts/run_deepvariant.py#L236-L253 . Sorry that we don't have better documentation than that right now). For how to run this with multiple shards, and how to run the rest of the commands, please read https://github.com/google/deepvariant/blob/r0.6/docs/deepvariant-quick-start.md. I just tested the steps above and confirmed that it worked for me on v1.4.0, at least for the make_examples step. If you encounter more issues with other steps, please feel free to ask again. I'd be happy to help. Note that I don't plan to put this into an official documentation page now, because that adds to our maintenance burden to keep it up to date. Given that we have the Docker/Singularity solution that works generally well for our users, I don't expect many of our users to need to use pre-built binaries. @zivlang thank you for your question so I have a chance to test it again and document it here. Hopefully this is helpful for you. Happy to answer more questions if you encounter more problems.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/590
https://github.com/google/deepvariant/issues/590:1265,availability,Down,Download,1265,"ck-start.md#notes-on-singularity. >. > If you don't have root permission, you won't be able to install necessary. > things before running the binaries either. > ------------------------------. >. > Here is what I did:. >. > Get a machine. (Not required to run on GCP. I just use this to get a. > machine to test). >. > gcloud compute instances create ""${USER}-cpu"" --scopes. > ""compute-rw,storage-full,cloud-platform"" --image-family ""ubuntu-2004-lts"". > --image-project ""ubuntu-os-cloud"" --machine-type ""custom-64-131072"". > --boot-disk-size ""300"" --zone ""us-west2-b"" --min-cpu-platform ""Intel. > Skylake"". >. > ssh into the machine:. >. > gcloud compute ssh pichuan-cpu --zone us-west2-b. >. > Get the binaries and models:. >. > BUCKET=""gs://deepvariant"". > BIN_VERSION=""1.4.0"". > MODEL_VERSION=""1.4.0"". >. > BIN_BUCKET=""${BUCKET}/binaries/DeepVariant/${BIN_VERSION}/DeepVariant-${BIN_VERSION}"". > MODEL_NAME=""DeepVariant-inception_v3-${MODEL_VERSION}+data-wgs_standard"". >. > mkdir -p bin. > # Download the DeepVariant binaries. > gsutil -m cp ""${BIN_BUCKET}/*"" bin/. > chmod a+x bin/*. >. > Then, I ran:. >. > cd bin; bash run-prereq.sh; cd -. >. > The run-prereq.sh tends to be the most tricky one - it will require root. > permission, and it'll install a bunch of stuff on your machine. If you. > can't use Docker because of root permissions, you likely won't be able to. > run this as well. >. > Download test data:. >. > INPUT_DIR=""${PWD}/quickstart-testdata"". > DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". >. > mkdir -p ${INPUT_DIR}. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. > wget -P ${IN",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/590
https://github.com/google/deepvariant/issues/590:1671,availability,Down,Download,1671,"d-platform"" --image-family ""ubuntu-2004-lts"". > --image-project ""ubuntu-os-cloud"" --machine-type ""custom-64-131072"". > --boot-disk-size ""300"" --zone ""us-west2-b"" --min-cpu-platform ""Intel. > Skylake"". >. > ssh into the machine:. >. > gcloud compute ssh pichuan-cpu --zone us-west2-b. >. > Get the binaries and models:. >. > BUCKET=""gs://deepvariant"". > BIN_VERSION=""1.4.0"". > MODEL_VERSION=""1.4.0"". >. > BIN_BUCKET=""${BUCKET}/binaries/DeepVariant/${BIN_VERSION}/DeepVariant-${BIN_VERSION}"". > MODEL_NAME=""DeepVariant-inception_v3-${MODEL_VERSION}+data-wgs_standard"". >. > mkdir -p bin. > # Download the DeepVariant binaries. > gsutil -m cp ""${BIN_BUCKET}/*"" bin/. > chmod a+x bin/*. >. > Then, I ran:. >. > cd bin; bash run-prereq.sh; cd -. >. > The run-prereq.sh tends to be the most tricky one - it will require root. > permission, and it'll install a bunch of stuff on your machine. If you. > can't use Docker because of root permissions, you likely won't be able to. > run this as well. >. > Download test data:. >. > INPUT_DIR=""${PWD}/quickstart-testdata"". > DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". >. > mkdir -p ${INPUT_DIR}. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. >. > Run make_examples:. >. >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/590
https://github.com/google/deepvariant/issues/590:3803,availability,mainten,maintenance,3803,"ATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. >. > Run make_examples:. >. > OUTPUT_DIR=""${PWD}/quickstart-output"". > mkdir -p ""${OUTPUT_DIR}"". >. > python bin/make_examples.zip \. > --mode calling \. > --ref ""${INPUT_DIR}/ucsc.hg19.chr20.unittest.fasta"" \. > --reads ""${INPUT_DIR}/NA12878_S1.chr20.10_10p1mb.bam"" \. > --regions ""chr20:10,000,000-10,010,000"" \. > --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"" \. > --channels ""insert_size"". >. > (To figure out which flags you need to add for each model, you can read. > https://github.com/google/deepvariant/blob/r1.4/scripts/run_deepvariant.py#L236-L253. > . Sorry that we don't have better documentation than that right now). >. > For how to run this with multiple shards, and how to run the rest of the. > commands, please read. > https://github.com/google/deepvariant/blob/r0.6/docs/deepvariant-quick-start.md. >. > I just tested the steps above and confirmed that it worked for me on. > v1.4.0, at least for the make_examples step. > If you encounter more issues with other steps, please feel free to ask. > again. I'd be happy to help. >. > Note that I don't plan to put this into an official documentation page. > now, because that adds to our maintenance burden to keep it up to date. > Given that we have the Docker/Singularity solution that works generally. > well for our users, I don't expect many of our users to need to use. > pre-built binaries. @zivlang <https://github.com/zivlang> thank you for. > your question so I have a chance to test it again and document it here. > Hopefully this is helpful for you. Happy to answer more questions if you. > encounter more problems. >. > —. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/590#issuecomment-1322695241>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/ALBF75YWC2I4SZFXBKRL4KTWJPVBNANCNFSM6AAAAAASFZYTTI>. > . > You are receiving this because you were mentioned.Message ID:. > ***@***.***>. >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/590
https://github.com/google/deepvariant/issues/590:364,deployability,instal,install,364,"Thanks! בתאריך יום ב׳, 21 בנוב׳ 2022, 23:50, מאת Pi-Chuan Chang ‏<. ***@***.***>:. > Before you proceed, if you can't use Docker because of root permission, I. > recommend that you try Singularity:. > https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-quick-start.md#notes-on-singularity. >. > If you don't have root permission, you won't be able to install necessary. > things before running the binaries either. > ------------------------------. >. > Here is what I did:. >. > Get a machine. (Not required to run on GCP. I just use this to get a. > machine to test). >. > gcloud compute instances create ""${USER}-cpu"" --scopes. > ""compute-rw,storage-full,cloud-platform"" --image-family ""ubuntu-2004-lts"". > --image-project ""ubuntu-os-cloud"" --machine-type ""custom-64-131072"". > --boot-disk-size ""300"" --zone ""us-west2-b"" --min-cpu-platform ""Intel. > Skylake"". >. > ssh into the machine:. >. > gcloud compute ssh pichuan-cpu --zone us-west2-b. >. > Get the binaries and models:. >. > BUCKET=""gs://deepvariant"". > BIN_VERSION=""1.4.0"". > MODEL_VERSION=""1.4.0"". >. > BIN_BUCKET=""${BUCKET}/binaries/DeepVariant/${BIN_VERSION}/DeepVariant-${BIN_VERSION}"". > MODEL_NAME=""DeepVariant-inception_v3-${MODEL_VERSION}+data-wgs_standard"". >. > mkdir -p bin. > # Download the DeepVariant binaries. > gsutil -m cp ""${BIN_BUCKET}/*"" bin/. > chmod a+x bin/*. >. > Then, I ran:. >. > cd bin; bash run-prereq.sh; cd -. >. > The run-prereq.sh tends to be the most tricky one - it will require root. > permission, and it'll install a bunch of stuff on your machine. If you. > can't use Docker because of root permissions, you likely won't be able to. > run this as well. >. > Download test data:. >. > INPUT_DIR=""${PWD}/quickstart-testdata"". > DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". >. > mkdir -p ${INPUT_DIR}. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/590
https://github.com/google/deepvariant/issues/590:1519,deployability,instal,install,1519,"o run on GCP. I just use this to get a. > machine to test). >. > gcloud compute instances create ""${USER}-cpu"" --scopes. > ""compute-rw,storage-full,cloud-platform"" --image-family ""ubuntu-2004-lts"". > --image-project ""ubuntu-os-cloud"" --machine-type ""custom-64-131072"". > --boot-disk-size ""300"" --zone ""us-west2-b"" --min-cpu-platform ""Intel. > Skylake"". >. > ssh into the machine:. >. > gcloud compute ssh pichuan-cpu --zone us-west2-b. >. > Get the binaries and models:. >. > BUCKET=""gs://deepvariant"". > BIN_VERSION=""1.4.0"". > MODEL_VERSION=""1.4.0"". >. > BIN_BUCKET=""${BUCKET}/binaries/DeepVariant/${BIN_VERSION}/DeepVariant-${BIN_VERSION}"". > MODEL_NAME=""DeepVariant-inception_v3-${MODEL_VERSION}+data-wgs_standard"". >. > mkdir -p bin. > # Download the DeepVariant binaries. > gsutil -m cp ""${BIN_BUCKET}/*"" bin/. > chmod a+x bin/*. >. > Then, I ran:. >. > cd bin; bash run-prereq.sh; cd -. >. > The run-prereq.sh tends to be the most tricky one - it will require root. > permission, and it'll install a bunch of stuff on your machine. If you. > can't use Docker because of root permissions, you likely won't be able to. > run this as well. >. > Download test data:. >. > INPUT_DIR=""${PWD}/quickstart-testdata"". > DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". >. > mkdir -p ${INPUT_DIR}. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/590
https://github.com/google/deepvariant/issues/590:629,energy efficiency,cpu,cpu,629,"Thanks! בתאריך יום ב׳, 21 בנוב׳ 2022, 23:50, מאת Pi-Chuan Chang ‏<. ***@***.***>:. > Before you proceed, if you can't use Docker because of root permission, I. > recommend that you try Singularity:. > https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-quick-start.md#notes-on-singularity. >. > If you don't have root permission, you won't be able to install necessary. > things before running the binaries either. > ------------------------------. >. > Here is what I did:. >. > Get a machine. (Not required to run on GCP. I just use this to get a. > machine to test). >. > gcloud compute instances create ""${USER}-cpu"" --scopes. > ""compute-rw,storage-full,cloud-platform"" --image-family ""ubuntu-2004-lts"". > --image-project ""ubuntu-os-cloud"" --machine-type ""custom-64-131072"". > --boot-disk-size ""300"" --zone ""us-west2-b"" --min-cpu-platform ""Intel. > Skylake"". >. > ssh into the machine:. >. > gcloud compute ssh pichuan-cpu --zone us-west2-b. >. > Get the binaries and models:. >. > BUCKET=""gs://deepvariant"". > BIN_VERSION=""1.4.0"". > MODEL_VERSION=""1.4.0"". >. > BIN_BUCKET=""${BUCKET}/binaries/DeepVariant/${BIN_VERSION}/DeepVariant-${BIN_VERSION}"". > MODEL_NAME=""DeepVariant-inception_v3-${MODEL_VERSION}+data-wgs_standard"". >. > mkdir -p bin. > # Download the DeepVariant binaries. > gsutil -m cp ""${BIN_BUCKET}/*"" bin/. > chmod a+x bin/*. >. > Then, I ran:. >. > cd bin; bash run-prereq.sh; cd -. >. > The run-prereq.sh tends to be the most tricky one - it will require root. > permission, and it'll install a bunch of stuff on your machine. If you. > can't use Docker because of root permissions, you likely won't be able to. > run this as well. >. > Download test data:. >. > INPUT_DIR=""${PWD}/quickstart-testdata"". > DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". >. > mkdir -p ${INPUT_DIR}. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/590
https://github.com/google/deepvariant/issues/590:671,energy efficiency,cloud,cloud-platform,671,"Thanks! בתאריך יום ב׳, 21 בנוב׳ 2022, 23:50, מאת Pi-Chuan Chang ‏<. ***@***.***>:. > Before you proceed, if you can't use Docker because of root permission, I. > recommend that you try Singularity:. > https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-quick-start.md#notes-on-singularity. >. > If you don't have root permission, you won't be able to install necessary. > things before running the binaries either. > ------------------------------. >. > Here is what I did:. >. > Get a machine. (Not required to run on GCP. I just use this to get a. > machine to test). >. > gcloud compute instances create ""${USER}-cpu"" --scopes. > ""compute-rw,storage-full,cloud-platform"" --image-family ""ubuntu-2004-lts"". > --image-project ""ubuntu-os-cloud"" --machine-type ""custom-64-131072"". > --boot-disk-size ""300"" --zone ""us-west2-b"" --min-cpu-platform ""Intel. > Skylake"". >. > ssh into the machine:. >. > gcloud compute ssh pichuan-cpu --zone us-west2-b. >. > Get the binaries and models:. >. > BUCKET=""gs://deepvariant"". > BIN_VERSION=""1.4.0"". > MODEL_VERSION=""1.4.0"". >. > BIN_BUCKET=""${BUCKET}/binaries/DeepVariant/${BIN_VERSION}/DeepVariant-${BIN_VERSION}"". > MODEL_NAME=""DeepVariant-inception_v3-${MODEL_VERSION}+data-wgs_standard"". >. > mkdir -p bin. > # Download the DeepVariant binaries. > gsutil -m cp ""${BIN_BUCKET}/*"" bin/. > chmod a+x bin/*. >. > Then, I ran:. >. > cd bin; bash run-prereq.sh; cd -. >. > The run-prereq.sh tends to be the most tricky one - it will require root. > permission, and it'll install a bunch of stuff on your machine. If you. > can't use Docker because of root permissions, you likely won't be able to. > run this as well. >. > Download test data:. >. > INPUT_DIR=""${PWD}/quickstart-testdata"". > DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". >. > mkdir -p ${INPUT_DIR}. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/590
https://github.com/google/deepvariant/issues/590:750,energy efficiency,cloud,cloud,750,"Thanks! בתאריך יום ב׳, 21 בנוב׳ 2022, 23:50, מאת Pi-Chuan Chang ‏<. ***@***.***>:. > Before you proceed, if you can't use Docker because of root permission, I. > recommend that you try Singularity:. > https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-quick-start.md#notes-on-singularity. >. > If you don't have root permission, you won't be able to install necessary. > things before running the binaries either. > ------------------------------. >. > Here is what I did:. >. > Get a machine. (Not required to run on GCP. I just use this to get a. > machine to test). >. > gcloud compute instances create ""${USER}-cpu"" --scopes. > ""compute-rw,storage-full,cloud-platform"" --image-family ""ubuntu-2004-lts"". > --image-project ""ubuntu-os-cloud"" --machine-type ""custom-64-131072"". > --boot-disk-size ""300"" --zone ""us-west2-b"" --min-cpu-platform ""Intel. > Skylake"". >. > ssh into the machine:. >. > gcloud compute ssh pichuan-cpu --zone us-west2-b. >. > Get the binaries and models:. >. > BUCKET=""gs://deepvariant"". > BIN_VERSION=""1.4.0"". > MODEL_VERSION=""1.4.0"". >. > BIN_BUCKET=""${BUCKET}/binaries/DeepVariant/${BIN_VERSION}/DeepVariant-${BIN_VERSION}"". > MODEL_NAME=""DeepVariant-inception_v3-${MODEL_VERSION}+data-wgs_standard"". >. > mkdir -p bin. > # Download the DeepVariant binaries. > gsutil -m cp ""${BIN_BUCKET}/*"" bin/. > chmod a+x bin/*. >. > Then, I ran:. >. > cd bin; bash run-prereq.sh; cd -. >. > The run-prereq.sh tends to be the most tricky one - it will require root. > permission, and it'll install a bunch of stuff on your machine. If you. > can't use Docker because of root permissions, you likely won't be able to. > run this as well. >. > Download test data:. >. > INPUT_DIR=""${PWD}/quickstart-testdata"". > DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". >. > mkdir -p ${INPUT_DIR}. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/590
https://github.com/google/deepvariant/issues/590:843,energy efficiency,cpu,cpu-platform,843,"Thanks! בתאריך יום ב׳, 21 בנוב׳ 2022, 23:50, מאת Pi-Chuan Chang ‏<. ***@***.***>:. > Before you proceed, if you can't use Docker because of root permission, I. > recommend that you try Singularity:. > https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-quick-start.md#notes-on-singularity. >. > If you don't have root permission, you won't be able to install necessary. > things before running the binaries either. > ------------------------------. >. > Here is what I did:. >. > Get a machine. (Not required to run on GCP. I just use this to get a. > machine to test). >. > gcloud compute instances create ""${USER}-cpu"" --scopes. > ""compute-rw,storage-full,cloud-platform"" --image-family ""ubuntu-2004-lts"". > --image-project ""ubuntu-os-cloud"" --machine-type ""custom-64-131072"". > --boot-disk-size ""300"" --zone ""us-west2-b"" --min-cpu-platform ""Intel. > Skylake"". >. > ssh into the machine:. >. > gcloud compute ssh pichuan-cpu --zone us-west2-b. >. > Get the binaries and models:. >. > BUCKET=""gs://deepvariant"". > BIN_VERSION=""1.4.0"". > MODEL_VERSION=""1.4.0"". >. > BIN_BUCKET=""${BUCKET}/binaries/DeepVariant/${BIN_VERSION}/DeepVariant-${BIN_VERSION}"". > MODEL_NAME=""DeepVariant-inception_v3-${MODEL_VERSION}+data-wgs_standard"". >. > mkdir -p bin. > # Download the DeepVariant binaries. > gsutil -m cp ""${BIN_BUCKET}/*"" bin/. > chmod a+x bin/*. >. > Then, I ran:. >. > cd bin; bash run-prereq.sh; cd -. >. > The run-prereq.sh tends to be the most tricky one - it will require root. > permission, and it'll install a bunch of stuff on your machine. If you. > can't use Docker because of root permissions, you likely won't be able to. > run this as well. >. > Download test data:. >. > INPUT_DIR=""${PWD}/quickstart-testdata"". > DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". >. > mkdir -p ${INPUT_DIR}. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/590
https://github.com/google/deepvariant/issues/590:936,energy efficiency,cpu,cpu,936,"Thanks! בתאריך יום ב׳, 21 בנוב׳ 2022, 23:50, מאת Pi-Chuan Chang ‏<. ***@***.***>:. > Before you proceed, if you can't use Docker because of root permission, I. > recommend that you try Singularity:. > https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-quick-start.md#notes-on-singularity. >. > If you don't have root permission, you won't be able to install necessary. > things before running the binaries either. > ------------------------------. >. > Here is what I did:. >. > Get a machine. (Not required to run on GCP. I just use this to get a. > machine to test). >. > gcloud compute instances create ""${USER}-cpu"" --scopes. > ""compute-rw,storage-full,cloud-platform"" --image-family ""ubuntu-2004-lts"". > --image-project ""ubuntu-os-cloud"" --machine-type ""custom-64-131072"". > --boot-disk-size ""300"" --zone ""us-west2-b"" --min-cpu-platform ""Intel. > Skylake"". >. > ssh into the machine:. >. > gcloud compute ssh pichuan-cpu --zone us-west2-b. >. > Get the binaries and models:. >. > BUCKET=""gs://deepvariant"". > BIN_VERSION=""1.4.0"". > MODEL_VERSION=""1.4.0"". >. > BIN_BUCKET=""${BUCKET}/binaries/DeepVariant/${BIN_VERSION}/DeepVariant-${BIN_VERSION}"". > MODEL_NAME=""DeepVariant-inception_v3-${MODEL_VERSION}+data-wgs_standard"". >. > mkdir -p bin. > # Download the DeepVariant binaries. > gsutil -m cp ""${BIN_BUCKET}/*"" bin/. > chmod a+x bin/*. >. > Then, I ran:. >. > cd bin; bash run-prereq.sh; cd -. >. > The run-prereq.sh tends to be the most tricky one - it will require root. > permission, and it'll install a bunch of stuff on your machine. If you. > can't use Docker because of root permissions, you likely won't be able to. > run this as well. >. > Download test data:. >. > INPUT_DIR=""${PWD}/quickstart-testdata"". > DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". >. > mkdir -p ${INPUT_DIR}. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/590
https://github.com/google/deepvariant/issues/590:985,energy efficiency,model,models,985,"Thanks! בתאריך יום ב׳, 21 בנוב׳ 2022, 23:50, מאת Pi-Chuan Chang ‏<. ***@***.***>:. > Before you proceed, if you can't use Docker because of root permission, I. > recommend that you try Singularity:. > https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-quick-start.md#notes-on-singularity. >. > If you don't have root permission, you won't be able to install necessary. > things before running the binaries either. > ------------------------------. >. > Here is what I did:. >. > Get a machine. (Not required to run on GCP. I just use this to get a. > machine to test). >. > gcloud compute instances create ""${USER}-cpu"" --scopes. > ""compute-rw,storage-full,cloud-platform"" --image-family ""ubuntu-2004-lts"". > --image-project ""ubuntu-os-cloud"" --machine-type ""custom-64-131072"". > --boot-disk-size ""300"" --zone ""us-west2-b"" --min-cpu-platform ""Intel. > Skylake"". >. > ssh into the machine:. >. > gcloud compute ssh pichuan-cpu --zone us-west2-b. >. > Get the binaries and models:. >. > BUCKET=""gs://deepvariant"". > BIN_VERSION=""1.4.0"". > MODEL_VERSION=""1.4.0"". >. > BIN_BUCKET=""${BUCKET}/binaries/DeepVariant/${BIN_VERSION}/DeepVariant-${BIN_VERSION}"". > MODEL_NAME=""DeepVariant-inception_v3-${MODEL_VERSION}+data-wgs_standard"". >. > mkdir -p bin. > # Download the DeepVariant binaries. > gsutil -m cp ""${BIN_BUCKET}/*"" bin/. > chmod a+x bin/*. >. > Then, I ran:. >. > cd bin; bash run-prereq.sh; cd -. >. > The run-prereq.sh tends to be the most tricky one - it will require root. > permission, and it'll install a bunch of stuff on your machine. If you. > can't use Docker because of root permissions, you likely won't be able to. > run this as well. >. > Download test data:. >. > INPUT_DIR=""${PWD}/quickstart-testdata"". > DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". >. > mkdir -p ${INPUT_DIR}. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/590
https://github.com/google/deepvariant/issues/590:3099,energy efficiency,model,model,3099,"} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. >. > Run make_examples:. >. > OUTPUT_DIR=""${PWD}/quickstart-output"". > mkdir -p ""${OUTPUT_DIR}"". >. > python bin/make_examples.zip \. > --mode calling \. > --ref ""${INPUT_DIR}/ucsc.hg19.chr20.unittest.fasta"" \. > --reads ""${INPUT_DIR}/NA12878_S1.chr20.10_10p1mb.bam"" \. > --regions ""chr20:10,000,000-10,010,000"" \. > --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"" \. > --channels ""insert_size"". >. > (To figure out which flags you need to add for each model, you can read. > https://github.com/google/deepvariant/blob/r1.4/scripts/run_deepvariant.py#L236-L253. > . Sorry that we don't have better documentation than that right now). >. > For how to run this with multiple shards, and how to run the rest of the. > commands, please read. > https://github.com/google/deepvariant/blob/r0.6/docs/deepvariant-quick-start.md. >. > I just tested the steps above and confirmed that it worked for me on. > v1.4.0, at least for the make_examples step. > If you encounter more issues with other steps, please feel free to ask. > again. I'd be happy to help. >. > Note that I don't plan to put this into an official documentation page. > now, because that adds to our maintenance burden to keep it up to date. > Given that we have the Docker/Singularity solution that works generally. > well for our users, I don't expect many of our users to need to use. > pre-built binaries. @zivlang <https://github.com/zivlang> thank you for. > your question so I have a chance t",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/590
https://github.com/google/deepvariant/issues/590:4561,integrability,Messag,Message,4561,"ATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. >. > Run make_examples:. >. > OUTPUT_DIR=""${PWD}/quickstart-output"". > mkdir -p ""${OUTPUT_DIR}"". >. > python bin/make_examples.zip \. > --mode calling \. > --ref ""${INPUT_DIR}/ucsc.hg19.chr20.unittest.fasta"" \. > --reads ""${INPUT_DIR}/NA12878_S1.chr20.10_10p1mb.bam"" \. > --regions ""chr20:10,000,000-10,010,000"" \. > --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"" \. > --channels ""insert_size"". >. > (To figure out which flags you need to add for each model, you can read. > https://github.com/google/deepvariant/blob/r1.4/scripts/run_deepvariant.py#L236-L253. > . Sorry that we don't have better documentation than that right now). >. > For how to run this with multiple shards, and how to run the rest of the. > commands, please read. > https://github.com/google/deepvariant/blob/r0.6/docs/deepvariant-quick-start.md. >. > I just tested the steps above and confirmed that it worked for me on. > v1.4.0, at least for the make_examples step. > If you encounter more issues with other steps, please feel free to ask. > again. I'd be happy to help. >. > Note that I don't plan to put this into an official documentation page. > now, because that adds to our maintenance burden to keep it up to date. > Given that we have the Docker/Singularity solution that works generally. > well for our users, I don't expect many of our users to need to use. > pre-built binaries. @zivlang <https://github.com/zivlang> thank you for. > your question so I have a chance to test it again and document it here. > Hopefully this is helpful for you. Happy to answer more questions if you. > encounter more problems. >. > —. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/590#issuecomment-1322695241>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/ALBF75YWC2I4SZFXBKRL4KTWJPVBNANCNFSM6AAAAAASFZYTTI>. > . > You are receiving this because you were mentioned.Message ID:. > ***@***.***>. >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/590
https://github.com/google/deepvariant/issues/590:677,interoperability,platform,platform,677,"Thanks! בתאריך יום ב׳, 21 בנוב׳ 2022, 23:50, מאת Pi-Chuan Chang ‏<. ***@***.***>:. > Before you proceed, if you can't use Docker because of root permission, I. > recommend that you try Singularity:. > https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-quick-start.md#notes-on-singularity. >. > If you don't have root permission, you won't be able to install necessary. > things before running the binaries either. > ------------------------------. >. > Here is what I did:. >. > Get a machine. (Not required to run on GCP. I just use this to get a. > machine to test). >. > gcloud compute instances create ""${USER}-cpu"" --scopes. > ""compute-rw,storage-full,cloud-platform"" --image-family ""ubuntu-2004-lts"". > --image-project ""ubuntu-os-cloud"" --machine-type ""custom-64-131072"". > --boot-disk-size ""300"" --zone ""us-west2-b"" --min-cpu-platform ""Intel. > Skylake"". >. > ssh into the machine:. >. > gcloud compute ssh pichuan-cpu --zone us-west2-b. >. > Get the binaries and models:. >. > BUCKET=""gs://deepvariant"". > BIN_VERSION=""1.4.0"". > MODEL_VERSION=""1.4.0"". >. > BIN_BUCKET=""${BUCKET}/binaries/DeepVariant/${BIN_VERSION}/DeepVariant-${BIN_VERSION}"". > MODEL_NAME=""DeepVariant-inception_v3-${MODEL_VERSION}+data-wgs_standard"". >. > mkdir -p bin. > # Download the DeepVariant binaries. > gsutil -m cp ""${BIN_BUCKET}/*"" bin/. > chmod a+x bin/*. >. > Then, I ran:. >. > cd bin; bash run-prereq.sh; cd -. >. > The run-prereq.sh tends to be the most tricky one - it will require root. > permission, and it'll install a bunch of stuff on your machine. If you. > can't use Docker because of root permissions, you likely won't be able to. > run this as well. >. > Download test data:. >. > INPUT_DIR=""${PWD}/quickstart-testdata"". > DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". >. > mkdir -p ${INPUT_DIR}. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/590
https://github.com/google/deepvariant/issues/590:847,interoperability,platform,platform,847,"Thanks! בתאריך יום ב׳, 21 בנוב׳ 2022, 23:50, מאת Pi-Chuan Chang ‏<. ***@***.***>:. > Before you proceed, if you can't use Docker because of root permission, I. > recommend that you try Singularity:. > https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-quick-start.md#notes-on-singularity. >. > If you don't have root permission, you won't be able to install necessary. > things before running the binaries either. > ------------------------------. >. > Here is what I did:. >. > Get a machine. (Not required to run on GCP. I just use this to get a. > machine to test). >. > gcloud compute instances create ""${USER}-cpu"" --scopes. > ""compute-rw,storage-full,cloud-platform"" --image-family ""ubuntu-2004-lts"". > --image-project ""ubuntu-os-cloud"" --machine-type ""custom-64-131072"". > --boot-disk-size ""300"" --zone ""us-west2-b"" --min-cpu-platform ""Intel. > Skylake"". >. > ssh into the machine:. >. > gcloud compute ssh pichuan-cpu --zone us-west2-b. >. > Get the binaries and models:. >. > BUCKET=""gs://deepvariant"". > BIN_VERSION=""1.4.0"". > MODEL_VERSION=""1.4.0"". >. > BIN_BUCKET=""${BUCKET}/binaries/DeepVariant/${BIN_VERSION}/DeepVariant-${BIN_VERSION}"". > MODEL_NAME=""DeepVariant-inception_v3-${MODEL_VERSION}+data-wgs_standard"". >. > mkdir -p bin. > # Download the DeepVariant binaries. > gsutil -m cp ""${BIN_BUCKET}/*"" bin/. > chmod a+x bin/*. >. > Then, I ran:. >. > cd bin; bash run-prereq.sh; cd -. >. > The run-prereq.sh tends to be the most tricky one - it will require root. > permission, and it'll install a bunch of stuff on your machine. If you. > can't use Docker because of root permissions, you likely won't be able to. > run this as well. >. > Download test data:. >. > INPUT_DIR=""${PWD}/quickstart-testdata"". > DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". >. > mkdir -p ${INPUT_DIR}. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/590
https://github.com/google/deepvariant/issues/590:4561,interoperability,Messag,Message,4561,"ATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. >. > Run make_examples:. >. > OUTPUT_DIR=""${PWD}/quickstart-output"". > mkdir -p ""${OUTPUT_DIR}"". >. > python bin/make_examples.zip \. > --mode calling \. > --ref ""${INPUT_DIR}/ucsc.hg19.chr20.unittest.fasta"" \. > --reads ""${INPUT_DIR}/NA12878_S1.chr20.10_10p1mb.bam"" \. > --regions ""chr20:10,000,000-10,010,000"" \. > --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"" \. > --channels ""insert_size"". >. > (To figure out which flags you need to add for each model, you can read. > https://github.com/google/deepvariant/blob/r1.4/scripts/run_deepvariant.py#L236-L253. > . Sorry that we don't have better documentation than that right now). >. > For how to run this with multiple shards, and how to run the rest of the. > commands, please read. > https://github.com/google/deepvariant/blob/r0.6/docs/deepvariant-quick-start.md. >. > I just tested the steps above and confirmed that it worked for me on. > v1.4.0, at least for the make_examples step. > If you encounter more issues with other steps, please feel free to ask. > again. I'd be happy to help. >. > Note that I don't plan to put this into an official documentation page. > now, because that adds to our maintenance burden to keep it up to date. > Given that we have the Docker/Singularity solution that works generally. > well for our users, I don't expect many of our users to need to use. > pre-built binaries. @zivlang <https://github.com/zivlang> thank you for. > your question so I have a chance to test it again and document it here. > Hopefully this is helpful for you. Happy to answer more questions if you. > encounter more problems. >. > —. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/590#issuecomment-1322695241>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/ALBF75YWC2I4SZFXBKRL4KTWJPVBNANCNFSM6AAAAAASFZYTTI>. > . > You are receiving this because you were mentioned.Message ID:. > ***@***.***>. >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/590
https://github.com/google/deepvariant/issues/590:629,performance,cpu,cpu,629,"Thanks! בתאריך יום ב׳, 21 בנוב׳ 2022, 23:50, מאת Pi-Chuan Chang ‏<. ***@***.***>:. > Before you proceed, if you can't use Docker because of root permission, I. > recommend that you try Singularity:. > https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-quick-start.md#notes-on-singularity. >. > If you don't have root permission, you won't be able to install necessary. > things before running the binaries either. > ------------------------------. >. > Here is what I did:. >. > Get a machine. (Not required to run on GCP. I just use this to get a. > machine to test). >. > gcloud compute instances create ""${USER}-cpu"" --scopes. > ""compute-rw,storage-full,cloud-platform"" --image-family ""ubuntu-2004-lts"". > --image-project ""ubuntu-os-cloud"" --machine-type ""custom-64-131072"". > --boot-disk-size ""300"" --zone ""us-west2-b"" --min-cpu-platform ""Intel. > Skylake"". >. > ssh into the machine:. >. > gcloud compute ssh pichuan-cpu --zone us-west2-b. >. > Get the binaries and models:. >. > BUCKET=""gs://deepvariant"". > BIN_VERSION=""1.4.0"". > MODEL_VERSION=""1.4.0"". >. > BIN_BUCKET=""${BUCKET}/binaries/DeepVariant/${BIN_VERSION}/DeepVariant-${BIN_VERSION}"". > MODEL_NAME=""DeepVariant-inception_v3-${MODEL_VERSION}+data-wgs_standard"". >. > mkdir -p bin. > # Download the DeepVariant binaries. > gsutil -m cp ""${BIN_BUCKET}/*"" bin/. > chmod a+x bin/*. >. > Then, I ran:. >. > cd bin; bash run-prereq.sh; cd -. >. > The run-prereq.sh tends to be the most tricky one - it will require root. > permission, and it'll install a bunch of stuff on your machine. If you. > can't use Docker because of root permissions, you likely won't be able to. > run this as well. >. > Download test data:. >. > INPUT_DIR=""${PWD}/quickstart-testdata"". > DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". >. > mkdir -p ${INPUT_DIR}. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/590
https://github.com/google/deepvariant/issues/590:801,performance,disk,disk-size,801,"Thanks! בתאריך יום ב׳, 21 בנוב׳ 2022, 23:50, מאת Pi-Chuan Chang ‏<. ***@***.***>:. > Before you proceed, if you can't use Docker because of root permission, I. > recommend that you try Singularity:. > https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-quick-start.md#notes-on-singularity. >. > If you don't have root permission, you won't be able to install necessary. > things before running the binaries either. > ------------------------------. >. > Here is what I did:. >. > Get a machine. (Not required to run on GCP. I just use this to get a. > machine to test). >. > gcloud compute instances create ""${USER}-cpu"" --scopes. > ""compute-rw,storage-full,cloud-platform"" --image-family ""ubuntu-2004-lts"". > --image-project ""ubuntu-os-cloud"" --machine-type ""custom-64-131072"". > --boot-disk-size ""300"" --zone ""us-west2-b"" --min-cpu-platform ""Intel. > Skylake"". >. > ssh into the machine:. >. > gcloud compute ssh pichuan-cpu --zone us-west2-b. >. > Get the binaries and models:. >. > BUCKET=""gs://deepvariant"". > BIN_VERSION=""1.4.0"". > MODEL_VERSION=""1.4.0"". >. > BIN_BUCKET=""${BUCKET}/binaries/DeepVariant/${BIN_VERSION}/DeepVariant-${BIN_VERSION}"". > MODEL_NAME=""DeepVariant-inception_v3-${MODEL_VERSION}+data-wgs_standard"". >. > mkdir -p bin. > # Download the DeepVariant binaries. > gsutil -m cp ""${BIN_BUCKET}/*"" bin/. > chmod a+x bin/*. >. > Then, I ran:. >. > cd bin; bash run-prereq.sh; cd -. >. > The run-prereq.sh tends to be the most tricky one - it will require root. > permission, and it'll install a bunch of stuff on your machine. If you. > can't use Docker because of root permissions, you likely won't be able to. > run this as well. >. > Download test data:. >. > INPUT_DIR=""${PWD}/quickstart-testdata"". > DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". >. > mkdir -p ${INPUT_DIR}. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/590
https://github.com/google/deepvariant/issues/590:843,performance,cpu,cpu-platform,843,"Thanks! בתאריך יום ב׳, 21 בנוב׳ 2022, 23:50, מאת Pi-Chuan Chang ‏<. ***@***.***>:. > Before you proceed, if you can't use Docker because of root permission, I. > recommend that you try Singularity:. > https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-quick-start.md#notes-on-singularity. >. > If you don't have root permission, you won't be able to install necessary. > things before running the binaries either. > ------------------------------. >. > Here is what I did:. >. > Get a machine. (Not required to run on GCP. I just use this to get a. > machine to test). >. > gcloud compute instances create ""${USER}-cpu"" --scopes. > ""compute-rw,storage-full,cloud-platform"" --image-family ""ubuntu-2004-lts"". > --image-project ""ubuntu-os-cloud"" --machine-type ""custom-64-131072"". > --boot-disk-size ""300"" --zone ""us-west2-b"" --min-cpu-platform ""Intel. > Skylake"". >. > ssh into the machine:. >. > gcloud compute ssh pichuan-cpu --zone us-west2-b. >. > Get the binaries and models:. >. > BUCKET=""gs://deepvariant"". > BIN_VERSION=""1.4.0"". > MODEL_VERSION=""1.4.0"". >. > BIN_BUCKET=""${BUCKET}/binaries/DeepVariant/${BIN_VERSION}/DeepVariant-${BIN_VERSION}"". > MODEL_NAME=""DeepVariant-inception_v3-${MODEL_VERSION}+data-wgs_standard"". >. > mkdir -p bin. > # Download the DeepVariant binaries. > gsutil -m cp ""${BIN_BUCKET}/*"" bin/. > chmod a+x bin/*. >. > Then, I ran:. >. > cd bin; bash run-prereq.sh; cd -. >. > The run-prereq.sh tends to be the most tricky one - it will require root. > permission, and it'll install a bunch of stuff on your machine. If you. > can't use Docker because of root permissions, you likely won't be able to. > run this as well. >. > Download test data:. >. > INPUT_DIR=""${PWD}/quickstart-testdata"". > DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". >. > mkdir -p ${INPUT_DIR}. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/590
https://github.com/google/deepvariant/issues/590:936,performance,cpu,cpu,936,"Thanks! בתאריך יום ב׳, 21 בנוב׳ 2022, 23:50, מאת Pi-Chuan Chang ‏<. ***@***.***>:. > Before you proceed, if you can't use Docker because of root permission, I. > recommend that you try Singularity:. > https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-quick-start.md#notes-on-singularity. >. > If you don't have root permission, you won't be able to install necessary. > things before running the binaries either. > ------------------------------. >. > Here is what I did:. >. > Get a machine. (Not required to run on GCP. I just use this to get a. > machine to test). >. > gcloud compute instances create ""${USER}-cpu"" --scopes. > ""compute-rw,storage-full,cloud-platform"" --image-family ""ubuntu-2004-lts"". > --image-project ""ubuntu-os-cloud"" --machine-type ""custom-64-131072"". > --boot-disk-size ""300"" --zone ""us-west2-b"" --min-cpu-platform ""Intel. > Skylake"". >. > ssh into the machine:. >. > gcloud compute ssh pichuan-cpu --zone us-west2-b. >. > Get the binaries and models:. >. > BUCKET=""gs://deepvariant"". > BIN_VERSION=""1.4.0"". > MODEL_VERSION=""1.4.0"". >. > BIN_BUCKET=""${BUCKET}/binaries/DeepVariant/${BIN_VERSION}/DeepVariant-${BIN_VERSION}"". > MODEL_NAME=""DeepVariant-inception_v3-${MODEL_VERSION}+data-wgs_standard"". >. > mkdir -p bin. > # Download the DeepVariant binaries. > gsutil -m cp ""${BIN_BUCKET}/*"" bin/. > chmod a+x bin/*. >. > Then, I ran:. >. > cd bin; bash run-prereq.sh; cd -. >. > The run-prereq.sh tends to be the most tricky one - it will require root. > permission, and it'll install a bunch of stuff on your machine. If you. > can't use Docker because of root permissions, you likely won't be able to. > run this as well. >. > Download test data:. >. > INPUT_DIR=""${PWD}/quickstart-testdata"". > DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". >. > mkdir -p ${INPUT_DIR}. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/590
https://github.com/google/deepvariant/issues/590:3803,reliability,mainten,maintenance,3803,"ATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. >. > Run make_examples:. >. > OUTPUT_DIR=""${PWD}/quickstart-output"". > mkdir -p ""${OUTPUT_DIR}"". >. > python bin/make_examples.zip \. > --mode calling \. > --ref ""${INPUT_DIR}/ucsc.hg19.chr20.unittest.fasta"" \. > --reads ""${INPUT_DIR}/NA12878_S1.chr20.10_10p1mb.bam"" \. > --regions ""chr20:10,000,000-10,010,000"" \. > --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"" \. > --channels ""insert_size"". >. > (To figure out which flags you need to add for each model, you can read. > https://github.com/google/deepvariant/blob/r1.4/scripts/run_deepvariant.py#L236-L253. > . Sorry that we don't have better documentation than that right now). >. > For how to run this with multiple shards, and how to run the rest of the. > commands, please read. > https://github.com/google/deepvariant/blob/r0.6/docs/deepvariant-quick-start.md. >. > I just tested the steps above and confirmed that it worked for me on. > v1.4.0, at least for the make_examples step. > If you encounter more issues with other steps, please feel free to ask. > again. I'd be happy to help. >. > Note that I don't plan to put this into an official documentation page. > now, because that adds to our maintenance burden to keep it up to date. > Given that we have the Docker/Singularity solution that works generally. > well for our users, I don't expect many of our users to need to use. > pre-built binaries. @zivlang <https://github.com/zivlang> thank you for. > your question so I have a chance to test it again and document it here. > Hopefully this is helpful for you. Happy to answer more questions if you. > encounter more problems. >. > —. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/590#issuecomment-1322695241>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/ALBF75YWC2I4SZFXBKRL4KTWJPVBNANCNFSM6AAAAAASFZYTTI>. > . > You are receiving this because you were mentioned.Message ID:. > ***@***.***>. >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/590
https://github.com/google/deepvariant/issues/590:145,safety,permiss,permission,145,"Thanks! בתאריך יום ב׳, 21 בנוב׳ 2022, 23:50, מאת Pi-Chuan Chang ‏<. ***@***.***>:. > Before you proceed, if you can't use Docker because of root permission, I. > recommend that you try Singularity:. > https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-quick-start.md#notes-on-singularity. >. > If you don't have root permission, you won't be able to install necessary. > things before running the binaries either. > ------------------------------. >. > Here is what I did:. >. > Get a machine. (Not required to run on GCP. I just use this to get a. > machine to test). >. > gcloud compute instances create ""${USER}-cpu"" --scopes. > ""compute-rw,storage-full,cloud-platform"" --image-family ""ubuntu-2004-lts"". > --image-project ""ubuntu-os-cloud"" --machine-type ""custom-64-131072"". > --boot-disk-size ""300"" --zone ""us-west2-b"" --min-cpu-platform ""Intel. > Skylake"". >. > ssh into the machine:. >. > gcloud compute ssh pichuan-cpu --zone us-west2-b. >. > Get the binaries and models:. >. > BUCKET=""gs://deepvariant"". > BIN_VERSION=""1.4.0"". > MODEL_VERSION=""1.4.0"". >. > BIN_BUCKET=""${BUCKET}/binaries/DeepVariant/${BIN_VERSION}/DeepVariant-${BIN_VERSION}"". > MODEL_NAME=""DeepVariant-inception_v3-${MODEL_VERSION}+data-wgs_standard"". >. > mkdir -p bin. > # Download the DeepVariant binaries. > gsutil -m cp ""${BIN_BUCKET}/*"" bin/. > chmod a+x bin/*. >. > Then, I ran:. >. > cd bin; bash run-prereq.sh; cd -. >. > The run-prereq.sh tends to be the most tricky one - it will require root. > permission, and it'll install a bunch of stuff on your machine. If you. > can't use Docker because of root permissions, you likely won't be able to. > run this as well. >. > Download test data:. >. > INPUT_DIR=""${PWD}/quickstart-testdata"". > DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". >. > mkdir -p ${INPUT_DIR}. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/590
https://github.com/google/deepvariant/issues/590:331,safety,permiss,permission,331,"Thanks! בתאריך יום ב׳, 21 בנוב׳ 2022, 23:50, מאת Pi-Chuan Chang ‏<. ***@***.***>:. > Before you proceed, if you can't use Docker because of root permission, I. > recommend that you try Singularity:. > https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-quick-start.md#notes-on-singularity. >. > If you don't have root permission, you won't be able to install necessary. > things before running the binaries either. > ------------------------------. >. > Here is what I did:. >. > Get a machine. (Not required to run on GCP. I just use this to get a. > machine to test). >. > gcloud compute instances create ""${USER}-cpu"" --scopes. > ""compute-rw,storage-full,cloud-platform"" --image-family ""ubuntu-2004-lts"". > --image-project ""ubuntu-os-cloud"" --machine-type ""custom-64-131072"". > --boot-disk-size ""300"" --zone ""us-west2-b"" --min-cpu-platform ""Intel. > Skylake"". >. > ssh into the machine:. >. > gcloud compute ssh pichuan-cpu --zone us-west2-b. >. > Get the binaries and models:. >. > BUCKET=""gs://deepvariant"". > BIN_VERSION=""1.4.0"". > MODEL_VERSION=""1.4.0"". >. > BIN_BUCKET=""${BUCKET}/binaries/DeepVariant/${BIN_VERSION}/DeepVariant-${BIN_VERSION}"". > MODEL_NAME=""DeepVariant-inception_v3-${MODEL_VERSION}+data-wgs_standard"". >. > mkdir -p bin. > # Download the DeepVariant binaries. > gsutil -m cp ""${BIN_BUCKET}/*"" bin/. > chmod a+x bin/*. >. > Then, I ran:. >. > cd bin; bash run-prereq.sh; cd -. >. > The run-prereq.sh tends to be the most tricky one - it will require root. > permission, and it'll install a bunch of stuff on your machine. If you. > can't use Docker because of root permissions, you likely won't be able to. > run this as well. >. > Download test data:. >. > INPUT_DIR=""${PWD}/quickstart-testdata"". > DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". >. > mkdir -p ${INPUT_DIR}. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/590
https://github.com/google/deepvariant/issues/590:576,safety,test,test,576,"Thanks! בתאריך יום ב׳, 21 בנוב׳ 2022, 23:50, מאת Pi-Chuan Chang ‏<. ***@***.***>:. > Before you proceed, if you can't use Docker because of root permission, I. > recommend that you try Singularity:. > https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-quick-start.md#notes-on-singularity. >. > If you don't have root permission, you won't be able to install necessary. > things before running the binaries either. > ------------------------------. >. > Here is what I did:. >. > Get a machine. (Not required to run on GCP. I just use this to get a. > machine to test). >. > gcloud compute instances create ""${USER}-cpu"" --scopes. > ""compute-rw,storage-full,cloud-platform"" --image-family ""ubuntu-2004-lts"". > --image-project ""ubuntu-os-cloud"" --machine-type ""custom-64-131072"". > --boot-disk-size ""300"" --zone ""us-west2-b"" --min-cpu-platform ""Intel. > Skylake"". >. > ssh into the machine:. >. > gcloud compute ssh pichuan-cpu --zone us-west2-b. >. > Get the binaries and models:. >. > BUCKET=""gs://deepvariant"". > BIN_VERSION=""1.4.0"". > MODEL_VERSION=""1.4.0"". >. > BIN_BUCKET=""${BUCKET}/binaries/DeepVariant/${BIN_VERSION}/DeepVariant-${BIN_VERSION}"". > MODEL_NAME=""DeepVariant-inception_v3-${MODEL_VERSION}+data-wgs_standard"". >. > mkdir -p bin. > # Download the DeepVariant binaries. > gsutil -m cp ""${BIN_BUCKET}/*"" bin/. > chmod a+x bin/*. >. > Then, I ran:. >. > cd bin; bash run-prereq.sh; cd -. >. > The run-prereq.sh tends to be the most tricky one - it will require root. > permission, and it'll install a bunch of stuff on your machine. If you. > can't use Docker because of root permissions, you likely won't be able to. > run this as well. >. > Download test data:. >. > INPUT_DIR=""${PWD}/quickstart-testdata"". > DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". >. > mkdir -p ${INPUT_DIR}. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/590
https://github.com/google/deepvariant/issues/590:1497,safety,permiss,permission,1497,"hine. (Not required to run on GCP. I just use this to get a. > machine to test). >. > gcloud compute instances create ""${USER}-cpu"" --scopes. > ""compute-rw,storage-full,cloud-platform"" --image-family ""ubuntu-2004-lts"". > --image-project ""ubuntu-os-cloud"" --machine-type ""custom-64-131072"". > --boot-disk-size ""300"" --zone ""us-west2-b"" --min-cpu-platform ""Intel. > Skylake"". >. > ssh into the machine:. >. > gcloud compute ssh pichuan-cpu --zone us-west2-b. >. > Get the binaries and models:. >. > BUCKET=""gs://deepvariant"". > BIN_VERSION=""1.4.0"". > MODEL_VERSION=""1.4.0"". >. > BIN_BUCKET=""${BUCKET}/binaries/DeepVariant/${BIN_VERSION}/DeepVariant-${BIN_VERSION}"". > MODEL_NAME=""DeepVariant-inception_v3-${MODEL_VERSION}+data-wgs_standard"". >. > mkdir -p bin. > # Download the DeepVariant binaries. > gsutil -m cp ""${BIN_BUCKET}/*"" bin/. > chmod a+x bin/*. >. > Then, I ran:. >. > cd bin; bash run-prereq.sh; cd -. >. > The run-prereq.sh tends to be the most tricky one - it will require root. > permission, and it'll install a bunch of stuff on your machine. If you. > can't use Docker because of root permissions, you likely won't be able to. > run this as well. >. > Download test data:. >. > INPUT_DIR=""${PWD}/quickstart-testdata"". > DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". >. > mkdir -p ${INPUT_DIR}. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. > wget -P ${INPUT_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/590
https://github.com/google/deepvariant/issues/590:1604,safety,permiss,permissions,1604,"es create ""${USER}-cpu"" --scopes. > ""compute-rw,storage-full,cloud-platform"" --image-family ""ubuntu-2004-lts"". > --image-project ""ubuntu-os-cloud"" --machine-type ""custom-64-131072"". > --boot-disk-size ""300"" --zone ""us-west2-b"" --min-cpu-platform ""Intel. > Skylake"". >. > ssh into the machine:. >. > gcloud compute ssh pichuan-cpu --zone us-west2-b. >. > Get the binaries and models:. >. > BUCKET=""gs://deepvariant"". > BIN_VERSION=""1.4.0"". > MODEL_VERSION=""1.4.0"". >. > BIN_BUCKET=""${BUCKET}/binaries/DeepVariant/${BIN_VERSION}/DeepVariant-${BIN_VERSION}"". > MODEL_NAME=""DeepVariant-inception_v3-${MODEL_VERSION}+data-wgs_standard"". >. > mkdir -p bin. > # Download the DeepVariant binaries. > gsutil -m cp ""${BIN_BUCKET}/*"" bin/. > chmod a+x bin/*. >. > Then, I ran:. >. > cd bin; bash run-prereq.sh; cd -. >. > The run-prereq.sh tends to be the most tricky one - it will require root. > permission, and it'll install a bunch of stuff on your machine. If you. > can't use Docker because of root permissions, you likely won't be able to. > run this as well. >. > Download test data:. >. > INPUT_DIR=""${PWD}/quickstart-testdata"". > DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". >. > mkdir -p ${INPUT_DIR}. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucs",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/590
https://github.com/google/deepvariant/issues/590:1680,safety,test,test,1680,"orm"" --image-family ""ubuntu-2004-lts"". > --image-project ""ubuntu-os-cloud"" --machine-type ""custom-64-131072"". > --boot-disk-size ""300"" --zone ""us-west2-b"" --min-cpu-platform ""Intel. > Skylake"". >. > ssh into the machine:. >. > gcloud compute ssh pichuan-cpu --zone us-west2-b. >. > Get the binaries and models:. >. > BUCKET=""gs://deepvariant"". > BIN_VERSION=""1.4.0"". > MODEL_VERSION=""1.4.0"". >. > BIN_BUCKET=""${BUCKET}/binaries/DeepVariant/${BIN_VERSION}/DeepVariant-${BIN_VERSION}"". > MODEL_NAME=""DeepVariant-inception_v3-${MODEL_VERSION}+data-wgs_standard"". >. > mkdir -p bin. > # Download the DeepVariant binaries. > gsutil -m cp ""${BIN_BUCKET}/*"" bin/. > chmod a+x bin/*. >. > Then, I ran:. >. > cd bin; bash run-prereq.sh; cd -. >. > The run-prereq.sh tends to be the most tricky one - it will require root. > permission, and it'll install a bunch of stuff on your machine. If you. > can't use Docker because of root permissions, you likely won't be able to. > run this as well. >. > Download test data:. >. > INPUT_DIR=""${PWD}/quickstart-testdata"". > DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". >. > mkdir -p ${INPUT_DIR}. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. >. > Run make_examples:. >. > OUTPUT",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/590
https://github.com/google/deepvariant/issues/590:1726,safety,test,testdata,1726,"-project ""ubuntu-os-cloud"" --machine-type ""custom-64-131072"". > --boot-disk-size ""300"" --zone ""us-west2-b"" --min-cpu-platform ""Intel. > Skylake"". >. > ssh into the machine:. >. > gcloud compute ssh pichuan-cpu --zone us-west2-b. >. > Get the binaries and models:. >. > BUCKET=""gs://deepvariant"". > BIN_VERSION=""1.4.0"". > MODEL_VERSION=""1.4.0"". >. > BIN_BUCKET=""${BUCKET}/binaries/DeepVariant/${BIN_VERSION}/DeepVariant-${BIN_VERSION}"". > MODEL_NAME=""DeepVariant-inception_v3-${MODEL_VERSION}+data-wgs_standard"". >. > mkdir -p bin. > # Download the DeepVariant binaries. > gsutil -m cp ""${BIN_BUCKET}/*"" bin/. > chmod a+x bin/*. >. > Then, I ran:. >. > cd bin; bash run-prereq.sh; cd -. >. > The run-prereq.sh tends to be the most tricky one - it will require root. > permission, and it'll install a bunch of stuff on your machine. If you. > can't use Docker because of root permissions, you likely won't be able to. > run this as well. >. > Download test data:. >. > INPUT_DIR=""${PWD}/quickstart-testdata"". > DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". >. > mkdir -p ${INPUT_DIR}. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. >. > Run make_examples:. >. > OUTPUT_DIR=""${PWD}/quickstart-output"". > mkdir -p ""${O",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/590
https://github.com/google/deepvariant/issues/590:1808,safety,test,testdata,1808,"300"" --zone ""us-west2-b"" --min-cpu-platform ""Intel. > Skylake"". >. > ssh into the machine:. >. > gcloud compute ssh pichuan-cpu --zone us-west2-b. >. > Get the binaries and models:. >. > BUCKET=""gs://deepvariant"". > BIN_VERSION=""1.4.0"". > MODEL_VERSION=""1.4.0"". >. > BIN_BUCKET=""${BUCKET}/binaries/DeepVariant/${BIN_VERSION}/DeepVariant-${BIN_VERSION}"". > MODEL_NAME=""DeepVariant-inception_v3-${MODEL_VERSION}+data-wgs_standard"". >. > mkdir -p bin. > # Download the DeepVariant binaries. > gsutil -m cp ""${BIN_BUCKET}/*"" bin/. > chmod a+x bin/*. >. > Then, I ran:. >. > cd bin; bash run-prereq.sh; cd -. >. > The run-prereq.sh tends to be the most tricky one - it will require root. > permission, and it'll install a bunch of stuff on your machine. If you. > can't use Docker because of root permissions, you likely won't be able to. > run this as well. >. > Download test data:. >. > INPUT_DIR=""${PWD}/quickstart-testdata"". > DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". >. > mkdir -p ${INPUT_DIR}. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. >. > Run make_examples:. >. > OUTPUT_DIR=""${PWD}/quickstart-output"". > mkdir -p ""${OUTPUT_DIR}"". >. > python bin/make_examples.zip \. > --mode calling \. > --ref ""${I",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/590
https://github.com/google/deepvariant/issues/590:3479,safety,test,tested,3479,". > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. >. > Run make_examples:. >. > OUTPUT_DIR=""${PWD}/quickstart-output"". > mkdir -p ""${OUTPUT_DIR}"". >. > python bin/make_examples.zip \. > --mode calling \. > --ref ""${INPUT_DIR}/ucsc.hg19.chr20.unittest.fasta"" \. > --reads ""${INPUT_DIR}/NA12878_S1.chr20.10_10p1mb.bam"" \. > --regions ""chr20:10,000,000-10,010,000"" \. > --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"" \. > --channels ""insert_size"". >. > (To figure out which flags you need to add for each model, you can read. > https://github.com/google/deepvariant/blob/r1.4/scripts/run_deepvariant.py#L236-L253. > . Sorry that we don't have better documentation than that right now). >. > For how to run this with multiple shards, and how to run the rest of the. > commands, please read. > https://github.com/google/deepvariant/blob/r0.6/docs/deepvariant-quick-start.md. >. > I just tested the steps above and confirmed that it worked for me on. > v1.4.0, at least for the make_examples step. > If you encounter more issues with other steps, please feel free to ask. > again. I'd be happy to help. >. > Note that I don't plan to put this into an official documentation page. > now, because that adds to our maintenance burden to keep it up to date. > Given that we have the Docker/Singularity solution that works generally. > well for our users, I don't expect many of our users to need to use. > pre-built binaries. @zivlang <https://github.com/zivlang> thank you for. > your question so I have a chance to test it again and document it here. > Hopefully this is helpful for you. Happy to answer more questions if you. > encounter more problems. >. > —. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/590#issuecomment-1322695241>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/ALBF75YWC2I4SZFXBKRL4KTWJPVBNA",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/590
https://github.com/google/deepvariant/issues/590:4104,safety,test,test,4104,"ATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. >. > Run make_examples:. >. > OUTPUT_DIR=""${PWD}/quickstart-output"". > mkdir -p ""${OUTPUT_DIR}"". >. > python bin/make_examples.zip \. > --mode calling \. > --ref ""${INPUT_DIR}/ucsc.hg19.chr20.unittest.fasta"" \. > --reads ""${INPUT_DIR}/NA12878_S1.chr20.10_10p1mb.bam"" \. > --regions ""chr20:10,000,000-10,010,000"" \. > --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"" \. > --channels ""insert_size"". >. > (To figure out which flags you need to add for each model, you can read. > https://github.com/google/deepvariant/blob/r1.4/scripts/run_deepvariant.py#L236-L253. > . Sorry that we don't have better documentation than that right now). >. > For how to run this with multiple shards, and how to run the rest of the. > commands, please read. > https://github.com/google/deepvariant/blob/r0.6/docs/deepvariant-quick-start.md. >. > I just tested the steps above and confirmed that it worked for me on. > v1.4.0, at least for the make_examples step. > If you encounter more issues with other steps, please feel free to ask. > again. I'd be happy to help. >. > Note that I don't plan to put this into an official documentation page. > now, because that adds to our maintenance burden to keep it up to date. > Given that we have the Docker/Singularity solution that works generally. > well for our users, I don't expect many of our users to need to use. > pre-built binaries. @zivlang <https://github.com/zivlang> thank you for. > your question so I have a chance to test it again and document it here. > Hopefully this is helpful for you. Happy to answer more questions if you. > encounter more problems. >. > —. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/590#issuecomment-1322695241>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/ALBF75YWC2I4SZFXBKRL4KTWJPVBNANCNFSM6AAAAAASFZYTTI>. > . > You are receiving this because you were mentioned.Message ID:. > ***@***.***>. >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/590
https://github.com/google/deepvariant/issues/590:881,security,ssh,ssh,881,"Thanks! בתאריך יום ב׳, 21 בנוב׳ 2022, 23:50, מאת Pi-Chuan Chang ‏<. ***@***.***>:. > Before you proceed, if you can't use Docker because of root permission, I. > recommend that you try Singularity:. > https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-quick-start.md#notes-on-singularity. >. > If you don't have root permission, you won't be able to install necessary. > things before running the binaries either. > ------------------------------. >. > Here is what I did:. >. > Get a machine. (Not required to run on GCP. I just use this to get a. > machine to test). >. > gcloud compute instances create ""${USER}-cpu"" --scopes. > ""compute-rw,storage-full,cloud-platform"" --image-family ""ubuntu-2004-lts"". > --image-project ""ubuntu-os-cloud"" --machine-type ""custom-64-131072"". > --boot-disk-size ""300"" --zone ""us-west2-b"" --min-cpu-platform ""Intel. > Skylake"". >. > ssh into the machine:. >. > gcloud compute ssh pichuan-cpu --zone us-west2-b. >. > Get the binaries and models:. >. > BUCKET=""gs://deepvariant"". > BIN_VERSION=""1.4.0"". > MODEL_VERSION=""1.4.0"". >. > BIN_BUCKET=""${BUCKET}/binaries/DeepVariant/${BIN_VERSION}/DeepVariant-${BIN_VERSION}"". > MODEL_NAME=""DeepVariant-inception_v3-${MODEL_VERSION}+data-wgs_standard"". >. > mkdir -p bin. > # Download the DeepVariant binaries. > gsutil -m cp ""${BIN_BUCKET}/*"" bin/. > chmod a+x bin/*. >. > Then, I ran:. >. > cd bin; bash run-prereq.sh; cd -. >. > The run-prereq.sh tends to be the most tricky one - it will require root. > permission, and it'll install a bunch of stuff on your machine. If you. > can't use Docker because of root permissions, you likely won't be able to. > run this as well. >. > Download test data:. >. > INPUT_DIR=""${PWD}/quickstart-testdata"". > DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". >. > mkdir -p ${INPUT_DIR}. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/590
https://github.com/google/deepvariant/issues/590:924,security,ssh,ssh,924,"Thanks! בתאריך יום ב׳, 21 בנוב׳ 2022, 23:50, מאת Pi-Chuan Chang ‏<. ***@***.***>:. > Before you proceed, if you can't use Docker because of root permission, I. > recommend that you try Singularity:. > https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-quick-start.md#notes-on-singularity. >. > If you don't have root permission, you won't be able to install necessary. > things before running the binaries either. > ------------------------------. >. > Here is what I did:. >. > Get a machine. (Not required to run on GCP. I just use this to get a. > machine to test). >. > gcloud compute instances create ""${USER}-cpu"" --scopes. > ""compute-rw,storage-full,cloud-platform"" --image-family ""ubuntu-2004-lts"". > --image-project ""ubuntu-os-cloud"" --machine-type ""custom-64-131072"". > --boot-disk-size ""300"" --zone ""us-west2-b"" --min-cpu-platform ""Intel. > Skylake"". >. > ssh into the machine:. >. > gcloud compute ssh pichuan-cpu --zone us-west2-b. >. > Get the binaries and models:. >. > BUCKET=""gs://deepvariant"". > BIN_VERSION=""1.4.0"". > MODEL_VERSION=""1.4.0"". >. > BIN_BUCKET=""${BUCKET}/binaries/DeepVariant/${BIN_VERSION}/DeepVariant-${BIN_VERSION}"". > MODEL_NAME=""DeepVariant-inception_v3-${MODEL_VERSION}+data-wgs_standard"". >. > mkdir -p bin. > # Download the DeepVariant binaries. > gsutil -m cp ""${BIN_BUCKET}/*"" bin/. > chmod a+x bin/*. >. > Then, I ran:. >. > cd bin; bash run-prereq.sh; cd -. >. > The run-prereq.sh tends to be the most tricky one - it will require root. > permission, and it'll install a bunch of stuff on your machine. If you. > can't use Docker because of root permissions, you likely won't be able to. > run this as well. >. > Download test data:. >. > INPUT_DIR=""${PWD}/quickstart-testdata"". > DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". >. > mkdir -p ${INPUT_DIR}. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/590
https://github.com/google/deepvariant/issues/590:985,security,model,models,985,"Thanks! בתאריך יום ב׳, 21 בנוב׳ 2022, 23:50, מאת Pi-Chuan Chang ‏<. ***@***.***>:. > Before you proceed, if you can't use Docker because of root permission, I. > recommend that you try Singularity:. > https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-quick-start.md#notes-on-singularity. >. > If you don't have root permission, you won't be able to install necessary. > things before running the binaries either. > ------------------------------. >. > Here is what I did:. >. > Get a machine. (Not required to run on GCP. I just use this to get a. > machine to test). >. > gcloud compute instances create ""${USER}-cpu"" --scopes. > ""compute-rw,storage-full,cloud-platform"" --image-family ""ubuntu-2004-lts"". > --image-project ""ubuntu-os-cloud"" --machine-type ""custom-64-131072"". > --boot-disk-size ""300"" --zone ""us-west2-b"" --min-cpu-platform ""Intel. > Skylake"". >. > ssh into the machine:. >. > gcloud compute ssh pichuan-cpu --zone us-west2-b. >. > Get the binaries and models:. >. > BUCKET=""gs://deepvariant"". > BIN_VERSION=""1.4.0"". > MODEL_VERSION=""1.4.0"". >. > BIN_BUCKET=""${BUCKET}/binaries/DeepVariant/${BIN_VERSION}/DeepVariant-${BIN_VERSION}"". > MODEL_NAME=""DeepVariant-inception_v3-${MODEL_VERSION}+data-wgs_standard"". >. > mkdir -p bin. > # Download the DeepVariant binaries. > gsutil -m cp ""${BIN_BUCKET}/*"" bin/. > chmod a+x bin/*. >. > Then, I ran:. >. > cd bin; bash run-prereq.sh; cd -. >. > The run-prereq.sh tends to be the most tricky one - it will require root. > permission, and it'll install a bunch of stuff on your machine. If you. > can't use Docker because of root permissions, you likely won't be able to. > run this as well. >. > Download test data:. >. > INPUT_DIR=""${PWD}/quickstart-testdata"". > DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". >. > mkdir -p ${INPUT_DIR}. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/590
https://github.com/google/deepvariant/issues/590:3099,security,model,model,3099,"} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. >. > Run make_examples:. >. > OUTPUT_DIR=""${PWD}/quickstart-output"". > mkdir -p ""${OUTPUT_DIR}"". >. > python bin/make_examples.zip \. > --mode calling \. > --ref ""${INPUT_DIR}/ucsc.hg19.chr20.unittest.fasta"" \. > --reads ""${INPUT_DIR}/NA12878_S1.chr20.10_10p1mb.bam"" \. > --regions ""chr20:10,000,000-10,010,000"" \. > --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"" \. > --channels ""insert_size"". >. > (To figure out which flags you need to add for each model, you can read. > https://github.com/google/deepvariant/blob/r1.4/scripts/run_deepvariant.py#L236-L253. > . Sorry that we don't have better documentation than that right now). >. > For how to run this with multiple shards, and how to run the rest of the. > commands, please read. > https://github.com/google/deepvariant/blob/r0.6/docs/deepvariant-quick-start.md. >. > I just tested the steps above and confirmed that it worked for me on. > v1.4.0, at least for the make_examples step. > If you encounter more issues with other steps, please feel free to ask. > again. I'd be happy to help. >. > Note that I don't plan to put this into an official documentation page. > now, because that adds to our maintenance burden to keep it up to date. > Given that we have the Docker/Singularity solution that works generally. > well for our users, I don't expect many of our users to need to use. > pre-built binaries. @zivlang <https://github.com/zivlang> thank you for. > your question so I have a chance t",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/590
https://github.com/google/deepvariant/issues/590:4447,security,auth,auth,4447,"ATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. >. > Run make_examples:. >. > OUTPUT_DIR=""${PWD}/quickstart-output"". > mkdir -p ""${OUTPUT_DIR}"". >. > python bin/make_examples.zip \. > --mode calling \. > --ref ""${INPUT_DIR}/ucsc.hg19.chr20.unittest.fasta"" \. > --reads ""${INPUT_DIR}/NA12878_S1.chr20.10_10p1mb.bam"" \. > --regions ""chr20:10,000,000-10,010,000"" \. > --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"" \. > --channels ""insert_size"". >. > (To figure out which flags you need to add for each model, you can read. > https://github.com/google/deepvariant/blob/r1.4/scripts/run_deepvariant.py#L236-L253. > . Sorry that we don't have better documentation than that right now). >. > For how to run this with multiple shards, and how to run the rest of the. > commands, please read. > https://github.com/google/deepvariant/blob/r0.6/docs/deepvariant-quick-start.md. >. > I just tested the steps above and confirmed that it worked for me on. > v1.4.0, at least for the make_examples step. > If you encounter more issues with other steps, please feel free to ask. > again. I'd be happy to help. >. > Note that I don't plan to put this into an official documentation page. > now, because that adds to our maintenance burden to keep it up to date. > Given that we have the Docker/Singularity solution that works generally. > well for our users, I don't expect many of our users to need to use. > pre-built binaries. @zivlang <https://github.com/zivlang> thank you for. > your question so I have a chance to test it again and document it here. > Hopefully this is helpful for you. Happy to answer more questions if you. > encounter more problems. >. > —. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/590#issuecomment-1322695241>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/ALBF75YWC2I4SZFXBKRL4KTWJPVBNANCNFSM6AAAAAASFZYTTI>. > . > You are receiving this because you were mentioned.Message ID:. > ***@***.***>. >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/590
https://github.com/google/deepvariant/issues/590:576,testability,test,test,576,"Thanks! בתאריך יום ב׳, 21 בנוב׳ 2022, 23:50, מאת Pi-Chuan Chang ‏<. ***@***.***>:. > Before you proceed, if you can't use Docker because of root permission, I. > recommend that you try Singularity:. > https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-quick-start.md#notes-on-singularity. >. > If you don't have root permission, you won't be able to install necessary. > things before running the binaries either. > ------------------------------. >. > Here is what I did:. >. > Get a machine. (Not required to run on GCP. I just use this to get a. > machine to test). >. > gcloud compute instances create ""${USER}-cpu"" --scopes. > ""compute-rw,storage-full,cloud-platform"" --image-family ""ubuntu-2004-lts"". > --image-project ""ubuntu-os-cloud"" --machine-type ""custom-64-131072"". > --boot-disk-size ""300"" --zone ""us-west2-b"" --min-cpu-platform ""Intel. > Skylake"". >. > ssh into the machine:. >. > gcloud compute ssh pichuan-cpu --zone us-west2-b. >. > Get the binaries and models:. >. > BUCKET=""gs://deepvariant"". > BIN_VERSION=""1.4.0"". > MODEL_VERSION=""1.4.0"". >. > BIN_BUCKET=""${BUCKET}/binaries/DeepVariant/${BIN_VERSION}/DeepVariant-${BIN_VERSION}"". > MODEL_NAME=""DeepVariant-inception_v3-${MODEL_VERSION}+data-wgs_standard"". >. > mkdir -p bin. > # Download the DeepVariant binaries. > gsutil -m cp ""${BIN_BUCKET}/*"" bin/. > chmod a+x bin/*. >. > Then, I ran:. >. > cd bin; bash run-prereq.sh; cd -. >. > The run-prereq.sh tends to be the most tricky one - it will require root. > permission, and it'll install a bunch of stuff on your machine. If you. > can't use Docker because of root permissions, you likely won't be able to. > run this as well. >. > Download test data:. >. > INPUT_DIR=""${PWD}/quickstart-testdata"". > DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". >. > mkdir -p ${INPUT_DIR}. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/590
https://github.com/google/deepvariant/issues/590:1680,testability,test,test,1680,"orm"" --image-family ""ubuntu-2004-lts"". > --image-project ""ubuntu-os-cloud"" --machine-type ""custom-64-131072"". > --boot-disk-size ""300"" --zone ""us-west2-b"" --min-cpu-platform ""Intel. > Skylake"". >. > ssh into the machine:. >. > gcloud compute ssh pichuan-cpu --zone us-west2-b. >. > Get the binaries and models:. >. > BUCKET=""gs://deepvariant"". > BIN_VERSION=""1.4.0"". > MODEL_VERSION=""1.4.0"". >. > BIN_BUCKET=""${BUCKET}/binaries/DeepVariant/${BIN_VERSION}/DeepVariant-${BIN_VERSION}"". > MODEL_NAME=""DeepVariant-inception_v3-${MODEL_VERSION}+data-wgs_standard"". >. > mkdir -p bin. > # Download the DeepVariant binaries. > gsutil -m cp ""${BIN_BUCKET}/*"" bin/. > chmod a+x bin/*. >. > Then, I ran:. >. > cd bin; bash run-prereq.sh; cd -. >. > The run-prereq.sh tends to be the most tricky one - it will require root. > permission, and it'll install a bunch of stuff on your machine. If you. > can't use Docker because of root permissions, you likely won't be able to. > run this as well. >. > Download test data:. >. > INPUT_DIR=""${PWD}/quickstart-testdata"". > DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". >. > mkdir -p ${INPUT_DIR}. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. >. > Run make_examples:. >. > OUTPUT",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/590
https://github.com/google/deepvariant/issues/590:1726,testability,test,testdata,1726,"-project ""ubuntu-os-cloud"" --machine-type ""custom-64-131072"". > --boot-disk-size ""300"" --zone ""us-west2-b"" --min-cpu-platform ""Intel. > Skylake"". >. > ssh into the machine:. >. > gcloud compute ssh pichuan-cpu --zone us-west2-b. >. > Get the binaries and models:. >. > BUCKET=""gs://deepvariant"". > BIN_VERSION=""1.4.0"". > MODEL_VERSION=""1.4.0"". >. > BIN_BUCKET=""${BUCKET}/binaries/DeepVariant/${BIN_VERSION}/DeepVariant-${BIN_VERSION}"". > MODEL_NAME=""DeepVariant-inception_v3-${MODEL_VERSION}+data-wgs_standard"". >. > mkdir -p bin. > # Download the DeepVariant binaries. > gsutil -m cp ""${BIN_BUCKET}/*"" bin/. > chmod a+x bin/*. >. > Then, I ran:. >. > cd bin; bash run-prereq.sh; cd -. >. > The run-prereq.sh tends to be the most tricky one - it will require root. > permission, and it'll install a bunch of stuff on your machine. If you. > can't use Docker because of root permissions, you likely won't be able to. > run this as well. >. > Download test data:. >. > INPUT_DIR=""${PWD}/quickstart-testdata"". > DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". >. > mkdir -p ${INPUT_DIR}. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. >. > Run make_examples:. >. > OUTPUT_DIR=""${PWD}/quickstart-output"". > mkdir -p ""${O",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/590
https://github.com/google/deepvariant/issues/590:1808,testability,test,testdata,1808,"300"" --zone ""us-west2-b"" --min-cpu-platform ""Intel. > Skylake"". >. > ssh into the machine:. >. > gcloud compute ssh pichuan-cpu --zone us-west2-b. >. > Get the binaries and models:. >. > BUCKET=""gs://deepvariant"". > BIN_VERSION=""1.4.0"". > MODEL_VERSION=""1.4.0"". >. > BIN_BUCKET=""${BUCKET}/binaries/DeepVariant/${BIN_VERSION}/DeepVariant-${BIN_VERSION}"". > MODEL_NAME=""DeepVariant-inception_v3-${MODEL_VERSION}+data-wgs_standard"". >. > mkdir -p bin. > # Download the DeepVariant binaries. > gsutil -m cp ""${BIN_BUCKET}/*"" bin/. > chmod a+x bin/*. >. > Then, I ran:. >. > cd bin; bash run-prereq.sh; cd -. >. > The run-prereq.sh tends to be the most tricky one - it will require root. > permission, and it'll install a bunch of stuff on your machine. If you. > can't use Docker because of root permissions, you likely won't be able to. > run this as well. >. > Download test data:. >. > INPUT_DIR=""${PWD}/quickstart-testdata"". > DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". >. > mkdir -p ${INPUT_DIR}. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. >. > Run make_examples:. >. > OUTPUT_DIR=""${PWD}/quickstart-output"". > mkdir -p ""${OUTPUT_DIR}"". >. > python bin/make_examples.zip \. > --mode calling \. > --ref ""${I",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/590
https://github.com/google/deepvariant/issues/590:2313,testability,unit,unittest,2313,"{BIN_BUCKET}/*"" bin/. > chmod a+x bin/*. >. > Then, I ran:. >. > cd bin; bash run-prereq.sh; cd -. >. > The run-prereq.sh tends to be the most tricky one - it will require root. > permission, and it'll install a bunch of stuff on your machine. If you. > can't use Docker because of root permissions, you likely won't be able to. > run this as well. >. > Download test data:. >. > INPUT_DIR=""${PWD}/quickstart-testdata"". > DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". >. > mkdir -p ${INPUT_DIR}. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. >. > Run make_examples:. >. > OUTPUT_DIR=""${PWD}/quickstart-output"". > mkdir -p ""${OUTPUT_DIR}"". >. > python bin/make_examples.zip \. > --mode calling \. > --ref ""${INPUT_DIR}/ucsc.hg19.chr20.unittest.fasta"" \. > --reads ""${INPUT_DIR}/NA12878_S1.chr20.10_10p1mb.bam"" \. > --regions ""chr20:10,000,000-10,010,000"" \. > --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"" \. > --channels ""insert_size"". >. > (To figure out which flags you need to add for each model, you can read. > https://github.com/google/deepvariant/blob/r1.4/scripts/run_deepvariant.py#L236-L253. > . Sorry that we don't have better documentation than that right now). >. > For how to run this with multipl",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/590
https://github.com/google/deepvariant/issues/590:2387,testability,unit,unittest,2387,"ash run-prereq.sh; cd -. >. > The run-prereq.sh tends to be the most tricky one - it will require root. > permission, and it'll install a bunch of stuff on your machine. If you. > can't use Docker because of root permissions, you likely won't be able to. > run this as well. >. > Download test data:. >. > INPUT_DIR=""${PWD}/quickstart-testdata"". > DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". >. > mkdir -p ${INPUT_DIR}. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. >. > Run make_examples:. >. > OUTPUT_DIR=""${PWD}/quickstart-output"". > mkdir -p ""${OUTPUT_DIR}"". >. > python bin/make_examples.zip \. > --mode calling \. > --ref ""${INPUT_DIR}/ucsc.hg19.chr20.unittest.fasta"" \. > --reads ""${INPUT_DIR}/NA12878_S1.chr20.10_10p1mb.bam"" \. > --regions ""chr20:10,000,000-10,010,000"" \. > --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"" \. > --channels ""insert_size"". >. > (To figure out which flags you need to add for each model, you can read. > https://github.com/google/deepvariant/blob/r1.4/scripts/run_deepvariant.py#L236-L253. > . Sorry that we don't have better documentation than that right now). >. > For how to run this with multiple shards, and how to run the rest of the. > commands, please read. > https",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/590
https://github.com/google/deepvariant/issues/590:2465,testability,unit,unittest,2465,"e - it will require root. > permission, and it'll install a bunch of stuff on your machine. If you. > can't use Docker because of root permissions, you likely won't be able to. > run this as well. >. > Download test data:. >. > INPUT_DIR=""${PWD}/quickstart-testdata"". > DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". >. > mkdir -p ${INPUT_DIR}. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. >. > Run make_examples:. >. > OUTPUT_DIR=""${PWD}/quickstart-output"". > mkdir -p ""${OUTPUT_DIR}"". >. > python bin/make_examples.zip \. > --mode calling \. > --ref ""${INPUT_DIR}/ucsc.hg19.chr20.unittest.fasta"" \. > --reads ""${INPUT_DIR}/NA12878_S1.chr20.10_10p1mb.bam"" \. > --regions ""chr20:10,000,000-10,010,000"" \. > --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"" \. > --channels ""insert_size"". >. > (To figure out which flags you need to add for each model, you can read. > https://github.com/google/deepvariant/blob/r1.4/scripts/run_deepvariant.py#L236-L253. > . Sorry that we don't have better documentation than that right now). >. > For how to run this with multiple shards, and how to run the rest of the. > commands, please read. > https://github.com/google/deepvariant/blob/r0.6/docs/deepvariant-quick-start.md. >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/590
https://github.com/google/deepvariant/issues/590:2542,testability,unit,unittest,2542," your machine. If you. > can't use Docker because of root permissions, you likely won't be able to. > run this as well. >. > Download test data:. >. > INPUT_DIR=""${PWD}/quickstart-testdata"". > DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". >. > mkdir -p ${INPUT_DIR}. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. >. > Run make_examples:. >. > OUTPUT_DIR=""${PWD}/quickstart-output"". > mkdir -p ""${OUTPUT_DIR}"". >. > python bin/make_examples.zip \. > --mode calling \. > --ref ""${INPUT_DIR}/ucsc.hg19.chr20.unittest.fasta"" \. > --reads ""${INPUT_DIR}/NA12878_S1.chr20.10_10p1mb.bam"" \. > --regions ""chr20:10,000,000-10,010,000"" \. > --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"" \. > --channels ""insert_size"". >. > (To figure out which flags you need to add for each model, you can read. > https://github.com/google/deepvariant/blob/r1.4/scripts/run_deepvariant.py#L236-L253. > . Sorry that we don't have better documentation than that right now). >. > For how to run this with multiple shards, and how to run the rest of the. > commands, please read. > https://github.com/google/deepvariant/blob/r0.6/docs/deepvariant-quick-start.md. >. > I just tested the steps above and confirmed that it worked for me on. > v1",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/590
https://github.com/google/deepvariant/issues/590:2623,testability,unit,unittest,2623," won't be able to. > run this as well. >. > Download test data:. >. > INPUT_DIR=""${PWD}/quickstart-testdata"". > DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". >. > mkdir -p ${INPUT_DIR}. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. >. > Run make_examples:. >. > OUTPUT_DIR=""${PWD}/quickstart-output"". > mkdir -p ""${OUTPUT_DIR}"". >. > python bin/make_examples.zip \. > --mode calling \. > --ref ""${INPUT_DIR}/ucsc.hg19.chr20.unittest.fasta"" \. > --reads ""${INPUT_DIR}/NA12878_S1.chr20.10_10p1mb.bam"" \. > --regions ""chr20:10,000,000-10,010,000"" \. > --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"" \. > --channels ""insert_size"". >. > (To figure out which flags you need to add for each model, you can read. > https://github.com/google/deepvariant/blob/r1.4/scripts/run_deepvariant.py#L236-L253. > . Sorry that we don't have better documentation than that right now). >. > For how to run this with multiple shards, and how to run the rest of the. > commands, please read. > https://github.com/google/deepvariant/blob/r0.6/docs/deepvariant-quick-start.md. >. > I just tested the steps above and confirmed that it worked for me on. > v1.4.0, at least for the make_examples step. > If you encounter more issues with ot",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/590
https://github.com/google/deepvariant/issues/590:2838,testability,unit,unittest,2838,"IR}. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. >. > Run make_examples:. >. > OUTPUT_DIR=""${PWD}/quickstart-output"". > mkdir -p ""${OUTPUT_DIR}"". >. > python bin/make_examples.zip \. > --mode calling \. > --ref ""${INPUT_DIR}/ucsc.hg19.chr20.unittest.fasta"" \. > --reads ""${INPUT_DIR}/NA12878_S1.chr20.10_10p1mb.bam"" \. > --regions ""chr20:10,000,000-10,010,000"" \. > --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"" \. > --channels ""insert_size"". >. > (To figure out which flags you need to add for each model, you can read. > https://github.com/google/deepvariant/blob/r1.4/scripts/run_deepvariant.py#L236-L253. > . Sorry that we don't have better documentation than that right now). >. > For how to run this with multiple shards, and how to run the rest of the. > commands, please read. > https://github.com/google/deepvariant/blob/r0.6/docs/deepvariant-quick-start.md. >. > I just tested the steps above and confirmed that it worked for me on. > v1.4.0, at least for the make_examples step. > If you encounter more issues with other steps, please feel free to ask. > again. I'd be happy to help. >. > Note that I don't plan to put this into an official documentation page. > now, because that adds to our maintenance burden to keep it up to dat",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/590
https://github.com/google/deepvariant/issues/590:3479,testability,test,tested,3479,". > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. >. > Run make_examples:. >. > OUTPUT_DIR=""${PWD}/quickstart-output"". > mkdir -p ""${OUTPUT_DIR}"". >. > python bin/make_examples.zip \. > --mode calling \. > --ref ""${INPUT_DIR}/ucsc.hg19.chr20.unittest.fasta"" \. > --reads ""${INPUT_DIR}/NA12878_S1.chr20.10_10p1mb.bam"" \. > --regions ""chr20:10,000,000-10,010,000"" \. > --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"" \. > --channels ""insert_size"". >. > (To figure out which flags you need to add for each model, you can read. > https://github.com/google/deepvariant/blob/r1.4/scripts/run_deepvariant.py#L236-L253. > . Sorry that we don't have better documentation than that right now). >. > For how to run this with multiple shards, and how to run the rest of the. > commands, please read. > https://github.com/google/deepvariant/blob/r0.6/docs/deepvariant-quick-start.md. >. > I just tested the steps above and confirmed that it worked for me on. > v1.4.0, at least for the make_examples step. > If you encounter more issues with other steps, please feel free to ask. > again. I'd be happy to help. >. > Note that I don't plan to put this into an official documentation page. > now, because that adds to our maintenance burden to keep it up to date. > Given that we have the Docker/Singularity solution that works generally. > well for our users, I don't expect many of our users to need to use. > pre-built binaries. @zivlang <https://github.com/zivlang> thank you for. > your question so I have a chance to test it again and document it here. > Hopefully this is helpful for you. Happy to answer more questions if you. > encounter more problems. >. > —. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/590#issuecomment-1322695241>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/ALBF75YWC2I4SZFXBKRL4KTWJPVBNA",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/590
https://github.com/google/deepvariant/issues/590:3717,testability,plan,plan,3717,"ATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. >. > Run make_examples:. >. > OUTPUT_DIR=""${PWD}/quickstart-output"". > mkdir -p ""${OUTPUT_DIR}"". >. > python bin/make_examples.zip \. > --mode calling \. > --ref ""${INPUT_DIR}/ucsc.hg19.chr20.unittest.fasta"" \. > --reads ""${INPUT_DIR}/NA12878_S1.chr20.10_10p1mb.bam"" \. > --regions ""chr20:10,000,000-10,010,000"" \. > --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"" \. > --channels ""insert_size"". >. > (To figure out which flags you need to add for each model, you can read. > https://github.com/google/deepvariant/blob/r1.4/scripts/run_deepvariant.py#L236-L253. > . Sorry that we don't have better documentation than that right now). >. > For how to run this with multiple shards, and how to run the rest of the. > commands, please read. > https://github.com/google/deepvariant/blob/r0.6/docs/deepvariant-quick-start.md. >. > I just tested the steps above and confirmed that it worked for me on. > v1.4.0, at least for the make_examples step. > If you encounter more issues with other steps, please feel free to ask. > again. I'd be happy to help. >. > Note that I don't plan to put this into an official documentation page. > now, because that adds to our maintenance burden to keep it up to date. > Given that we have the Docker/Singularity solution that works generally. > well for our users, I don't expect many of our users to need to use. > pre-built binaries. @zivlang <https://github.com/zivlang> thank you for. > your question so I have a chance to test it again and document it here. > Hopefully this is helpful for you. Happy to answer more questions if you. > encounter more problems. >. > —. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/590#issuecomment-1322695241>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/ALBF75YWC2I4SZFXBKRL4KTWJPVBNANCNFSM6AAAAAASFZYTTI>. > . > You are receiving this because you were mentioned.Message ID:. > ***@***.***>. >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/590
https://github.com/google/deepvariant/issues/590:4104,testability,test,test,4104,"ATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. >. > Run make_examples:. >. > OUTPUT_DIR=""${PWD}/quickstart-output"". > mkdir -p ""${OUTPUT_DIR}"". >. > python bin/make_examples.zip \. > --mode calling \. > --ref ""${INPUT_DIR}/ucsc.hg19.chr20.unittest.fasta"" \. > --reads ""${INPUT_DIR}/NA12878_S1.chr20.10_10p1mb.bam"" \. > --regions ""chr20:10,000,000-10,010,000"" \. > --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"" \. > --channels ""insert_size"". >. > (To figure out which flags you need to add for each model, you can read. > https://github.com/google/deepvariant/blob/r1.4/scripts/run_deepvariant.py#L236-L253. > . Sorry that we don't have better documentation than that right now). >. > For how to run this with multiple shards, and how to run the rest of the. > commands, please read. > https://github.com/google/deepvariant/blob/r0.6/docs/deepvariant-quick-start.md. >. > I just tested the steps above and confirmed that it worked for me on. > v1.4.0, at least for the make_examples step. > If you encounter more issues with other steps, please feel free to ask. > again. I'd be happy to help. >. > Note that I don't plan to put this into an official documentation page. > now, because that adds to our maintenance burden to keep it up to date. > Given that we have the Docker/Singularity solution that works generally. > well for our users, I don't expect many of our users to need to use. > pre-built binaries. @zivlang <https://github.com/zivlang> thank you for. > your question so I have a chance to test it again and document it here. > Hopefully this is helpful for you. Happy to answer more questions if you. > encounter more problems. >. > —. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/590#issuecomment-1322695241>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/ALBF75YWC2I4SZFXBKRL4KTWJPVBNANCNFSM6AAAAAASFZYTTI>. > . > You are receiving this because you were mentioned.Message ID:. > ***@***.***>. >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/590
https://github.com/google/deepvariant/issues/590:623,usability,USER,USER,623,"Thanks! בתאריך יום ב׳, 21 בנוב׳ 2022, 23:50, מאת Pi-Chuan Chang ‏<. ***@***.***>:. > Before you proceed, if you can't use Docker because of root permission, I. > recommend that you try Singularity:. > https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-quick-start.md#notes-on-singularity. >. > If you don't have root permission, you won't be able to install necessary. > things before running the binaries either. > ------------------------------. >. > Here is what I did:. >. > Get a machine. (Not required to run on GCP. I just use this to get a. > machine to test). >. > gcloud compute instances create ""${USER}-cpu"" --scopes. > ""compute-rw,storage-full,cloud-platform"" --image-family ""ubuntu-2004-lts"". > --image-project ""ubuntu-os-cloud"" --machine-type ""custom-64-131072"". > --boot-disk-size ""300"" --zone ""us-west2-b"" --min-cpu-platform ""Intel. > Skylake"". >. > ssh into the machine:. >. > gcloud compute ssh pichuan-cpu --zone us-west2-b. >. > Get the binaries and models:. >. > BUCKET=""gs://deepvariant"". > BIN_VERSION=""1.4.0"". > MODEL_VERSION=""1.4.0"". >. > BIN_BUCKET=""${BUCKET}/binaries/DeepVariant/${BIN_VERSION}/DeepVariant-${BIN_VERSION}"". > MODEL_NAME=""DeepVariant-inception_v3-${MODEL_VERSION}+data-wgs_standard"". >. > mkdir -p bin. > # Download the DeepVariant binaries. > gsutil -m cp ""${BIN_BUCKET}/*"" bin/. > chmod a+x bin/*. >. > Then, I ran:. >. > cd bin; bash run-prereq.sh; cd -. >. > The run-prereq.sh tends to be the most tricky one - it will require root. > permission, and it'll install a bunch of stuff on your machine. If you. > can't use Docker because of root permissions, you likely won't be able to. > run this as well. >. > Download test data:. >. > INPUT_DIR=""${PWD}/quickstart-testdata"". > DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". >. > mkdir -p ${INPUT_DIR}. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/590
https://github.com/google/deepvariant/issues/590:773,usability,custom,custom-,773,"Thanks! בתאריך יום ב׳, 21 בנוב׳ 2022, 23:50, מאת Pi-Chuan Chang ‏<. ***@***.***>:. > Before you proceed, if you can't use Docker because of root permission, I. > recommend that you try Singularity:. > https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-quick-start.md#notes-on-singularity. >. > If you don't have root permission, you won't be able to install necessary. > things before running the binaries either. > ------------------------------. >. > Here is what I did:. >. > Get a machine. (Not required to run on GCP. I just use this to get a. > machine to test). >. > gcloud compute instances create ""${USER}-cpu"" --scopes. > ""compute-rw,storage-full,cloud-platform"" --image-family ""ubuntu-2004-lts"". > --image-project ""ubuntu-os-cloud"" --machine-type ""custom-64-131072"". > --boot-disk-size ""300"" --zone ""us-west2-b"" --min-cpu-platform ""Intel. > Skylake"". >. > ssh into the machine:. >. > gcloud compute ssh pichuan-cpu --zone us-west2-b. >. > Get the binaries and models:. >. > BUCKET=""gs://deepvariant"". > BIN_VERSION=""1.4.0"". > MODEL_VERSION=""1.4.0"". >. > BIN_BUCKET=""${BUCKET}/binaries/DeepVariant/${BIN_VERSION}/DeepVariant-${BIN_VERSION}"". > MODEL_NAME=""DeepVariant-inception_v3-${MODEL_VERSION}+data-wgs_standard"". >. > mkdir -p bin. > # Download the DeepVariant binaries. > gsutil -m cp ""${BIN_BUCKET}/*"" bin/. > chmod a+x bin/*. >. > Then, I ran:. >. > cd bin; bash run-prereq.sh; cd -. >. > The run-prereq.sh tends to be the most tricky one - it will require root. > permission, and it'll install a bunch of stuff on your machine. If you. > can't use Docker because of root permissions, you likely won't be able to. > run this as well. >. > Download test data:. >. > INPUT_DIR=""${PWD}/quickstart-testdata"". > DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". >. > mkdir -p ${INPUT_DIR}. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/590
https://github.com/google/deepvariant/issues/590:3244,usability,document,documentation,3244,"bi. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. >. > Run make_examples:. >. > OUTPUT_DIR=""${PWD}/quickstart-output"". > mkdir -p ""${OUTPUT_DIR}"". >. > python bin/make_examples.zip \. > --mode calling \. > --ref ""${INPUT_DIR}/ucsc.hg19.chr20.unittest.fasta"" \. > --reads ""${INPUT_DIR}/NA12878_S1.chr20.10_10p1mb.bam"" \. > --regions ""chr20:10,000,000-10,010,000"" \. > --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"" \. > --channels ""insert_size"". >. > (To figure out which flags you need to add for each model, you can read. > https://github.com/google/deepvariant/blob/r1.4/scripts/run_deepvariant.py#L236-L253. > . Sorry that we don't have better documentation than that right now). >. > For how to run this with multiple shards, and how to run the rest of the. > commands, please read. > https://github.com/google/deepvariant/blob/r0.6/docs/deepvariant-quick-start.md. >. > I just tested the steps above and confirmed that it worked for me on. > v1.4.0, at least for the make_examples step. > If you encounter more issues with other steps, please feel free to ask. > again. I'd be happy to help. >. > Note that I don't plan to put this into an official documentation page. > now, because that adds to our maintenance burden to keep it up to date. > Given that we have the Docker/Singularity solution that works generally. > well for our users, I don't expect many of our users to need to use. > pre-built binaries. @zivlang <https://github.com/zivlang> thank you for. > your question so I have a chance to test it again and document it here. > Hopefully this is helpful for you. Happy to answer more questions if you. > encounter more problems. >. > —. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/590
https://github.com/google/deepvariant/issues/590:3361,usability,command,commands,3361,"DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. >. > Run make_examples:. >. > OUTPUT_DIR=""${PWD}/quickstart-output"". > mkdir -p ""${OUTPUT_DIR}"". >. > python bin/make_examples.zip \. > --mode calling \. > --ref ""${INPUT_DIR}/ucsc.hg19.chr20.unittest.fasta"" \. > --reads ""${INPUT_DIR}/NA12878_S1.chr20.10_10p1mb.bam"" \. > --regions ""chr20:10,000,000-10,010,000"" \. > --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"" \. > --channels ""insert_size"". >. > (To figure out which flags you need to add for each model, you can read. > https://github.com/google/deepvariant/blob/r1.4/scripts/run_deepvariant.py#L236-L253. > . Sorry that we don't have better documentation than that right now). >. > For how to run this with multiple shards, and how to run the rest of the. > commands, please read. > https://github.com/google/deepvariant/blob/r0.6/docs/deepvariant-quick-start.md. >. > I just tested the steps above and confirmed that it worked for me on. > v1.4.0, at least for the make_examples step. > If you encounter more issues with other steps, please feel free to ask. > again. I'd be happy to help. >. > Note that I don't plan to put this into an official documentation page. > now, because that adds to our maintenance burden to keep it up to date. > Given that we have the Docker/Singularity solution that works generally. > well for our users, I don't expect many of our users to need to use. > pre-built binaries. @zivlang <https://github.com/zivlang> thank you for. > your question so I have a chance to test it again and document it here. > Hopefully this is helpful for you. Happy to answer more questions if you. > encounter more problems. >. > —. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/590#issuecommen",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/590
https://github.com/google/deepvariant/issues/590:3506,usability,confirm,confirmed,3506,"ATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. >. > Run make_examples:. >. > OUTPUT_DIR=""${PWD}/quickstart-output"". > mkdir -p ""${OUTPUT_DIR}"". >. > python bin/make_examples.zip \. > --mode calling \. > --ref ""${INPUT_DIR}/ucsc.hg19.chr20.unittest.fasta"" \. > --reads ""${INPUT_DIR}/NA12878_S1.chr20.10_10p1mb.bam"" \. > --regions ""chr20:10,000,000-10,010,000"" \. > --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"" \. > --channels ""insert_size"". >. > (To figure out which flags you need to add for each model, you can read. > https://github.com/google/deepvariant/blob/r1.4/scripts/run_deepvariant.py#L236-L253. > . Sorry that we don't have better documentation than that right now). >. > For how to run this with multiple shards, and how to run the rest of the. > commands, please read. > https://github.com/google/deepvariant/blob/r0.6/docs/deepvariant-quick-start.md. >. > I just tested the steps above and confirmed that it worked for me on. > v1.4.0, at least for the make_examples step. > If you encounter more issues with other steps, please feel free to ask. > again. I'd be happy to help. >. > Note that I don't plan to put this into an official documentation page. > now, because that adds to our maintenance burden to keep it up to date. > Given that we have the Docker/Singularity solution that works generally. > well for our users, I don't expect many of our users to need to use. > pre-built binaries. @zivlang <https://github.com/zivlang> thank you for. > your question so I have a chance to test it again and document it here. > Hopefully this is helpful for you. Happy to answer more questions if you. > encounter more problems. >. > —. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/590#issuecomment-1322695241>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/ALBF75YWC2I4SZFXBKRL4KTWJPVBNANCNFSM6AAAAAASFZYTTI>. > . > ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/590
https://github.com/google/deepvariant/issues/590:3688,usability,help,help,3688,"ATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. >. > Run make_examples:. >. > OUTPUT_DIR=""${PWD}/quickstart-output"". > mkdir -p ""${OUTPUT_DIR}"". >. > python bin/make_examples.zip \. > --mode calling \. > --ref ""${INPUT_DIR}/ucsc.hg19.chr20.unittest.fasta"" \. > --reads ""${INPUT_DIR}/NA12878_S1.chr20.10_10p1mb.bam"" \. > --regions ""chr20:10,000,000-10,010,000"" \. > --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"" \. > --channels ""insert_size"". >. > (To figure out which flags you need to add for each model, you can read. > https://github.com/google/deepvariant/blob/r1.4/scripts/run_deepvariant.py#L236-L253. > . Sorry that we don't have better documentation than that right now). >. > For how to run this with multiple shards, and how to run the rest of the. > commands, please read. > https://github.com/google/deepvariant/blob/r0.6/docs/deepvariant-quick-start.md. >. > I just tested the steps above and confirmed that it worked for me on. > v1.4.0, at least for the make_examples step. > If you encounter more issues with other steps, please feel free to ask. > again. I'd be happy to help. >. > Note that I don't plan to put this into an official documentation page. > now, because that adds to our maintenance burden to keep it up to date. > Given that we have the Docker/Singularity solution that works generally. > well for our users, I don't expect many of our users to need to use. > pre-built binaries. @zivlang <https://github.com/zivlang> thank you for. > your question so I have a chance to test it again and document it here. > Hopefully this is helpful for you. Happy to answer more questions if you. > encounter more problems. >. > —. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/590#issuecomment-1322695241>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/ALBF75YWC2I4SZFXBKRL4KTWJPVBNANCNFSM6AAAAAASFZYTTI>. > . > You are receiving this because you were mentioned.Message ID:. > ***@***.***>. >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/590
https://github.com/google/deepvariant/issues/590:3751,usability,document,documentation,3751,"ATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. >. > Run make_examples:. >. > OUTPUT_DIR=""${PWD}/quickstart-output"". > mkdir -p ""${OUTPUT_DIR}"". >. > python bin/make_examples.zip \. > --mode calling \. > --ref ""${INPUT_DIR}/ucsc.hg19.chr20.unittest.fasta"" \. > --reads ""${INPUT_DIR}/NA12878_S1.chr20.10_10p1mb.bam"" \. > --regions ""chr20:10,000,000-10,010,000"" \. > --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"" \. > --channels ""insert_size"". >. > (To figure out which flags you need to add for each model, you can read. > https://github.com/google/deepvariant/blob/r1.4/scripts/run_deepvariant.py#L236-L253. > . Sorry that we don't have better documentation than that right now). >. > For how to run this with multiple shards, and how to run the rest of the. > commands, please read. > https://github.com/google/deepvariant/blob/r0.6/docs/deepvariant-quick-start.md. >. > I just tested the steps above and confirmed that it worked for me on. > v1.4.0, at least for the make_examples step. > If you encounter more issues with other steps, please feel free to ask. > again. I'd be happy to help. >. > Note that I don't plan to put this into an official documentation page. > now, because that adds to our maintenance burden to keep it up to date. > Given that we have the Docker/Singularity solution that works generally. > well for our users, I don't expect many of our users to need to use. > pre-built binaries. @zivlang <https://github.com/zivlang> thank you for. > your question so I have a chance to test it again and document it here. > Hopefully this is helpful for you. Happy to answer more questions if you. > encounter more problems. >. > —. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/590#issuecomment-1322695241>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/ALBF75YWC2I4SZFXBKRL4KTWJPVBNANCNFSM6AAAAAASFZYTTI>. > . > You are receiving this because you were mentioned.Message ID:. > ***@***.***>. >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/590
https://github.com/google/deepvariant/issues/590:3935,usability,user,users,3935,"ATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. >. > Run make_examples:. >. > OUTPUT_DIR=""${PWD}/quickstart-output"". > mkdir -p ""${OUTPUT_DIR}"". >. > python bin/make_examples.zip \. > --mode calling \. > --ref ""${INPUT_DIR}/ucsc.hg19.chr20.unittest.fasta"" \. > --reads ""${INPUT_DIR}/NA12878_S1.chr20.10_10p1mb.bam"" \. > --regions ""chr20:10,000,000-10,010,000"" \. > --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"" \. > --channels ""insert_size"". >. > (To figure out which flags you need to add for each model, you can read. > https://github.com/google/deepvariant/blob/r1.4/scripts/run_deepvariant.py#L236-L253. > . Sorry that we don't have better documentation than that right now). >. > For how to run this with multiple shards, and how to run the rest of the. > commands, please read. > https://github.com/google/deepvariant/blob/r0.6/docs/deepvariant-quick-start.md. >. > I just tested the steps above and confirmed that it worked for me on. > v1.4.0, at least for the make_examples step. > If you encounter more issues with other steps, please feel free to ask. > again. I'd be happy to help. >. > Note that I don't plan to put this into an official documentation page. > now, because that adds to our maintenance burden to keep it up to date. > Given that we have the Docker/Singularity solution that works generally. > well for our users, I don't expect many of our users to need to use. > pre-built binaries. @zivlang <https://github.com/zivlang> thank you for. > your question so I have a chance to test it again and document it here. > Hopefully this is helpful for you. Happy to answer more questions if you. > encounter more problems. >. > —. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/590#issuecomment-1322695241>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/ALBF75YWC2I4SZFXBKRL4KTWJPVBNANCNFSM6AAAAAASFZYTTI>. > . > You are receiving this because you were mentioned.Message ID:. > ***@***.***>. >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/590
https://github.com/google/deepvariant/issues/590:3969,usability,user,users,3969,"ATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. >. > Run make_examples:. >. > OUTPUT_DIR=""${PWD}/quickstart-output"". > mkdir -p ""${OUTPUT_DIR}"". >. > python bin/make_examples.zip \. > --mode calling \. > --ref ""${INPUT_DIR}/ucsc.hg19.chr20.unittest.fasta"" \. > --reads ""${INPUT_DIR}/NA12878_S1.chr20.10_10p1mb.bam"" \. > --regions ""chr20:10,000,000-10,010,000"" \. > --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"" \. > --channels ""insert_size"". >. > (To figure out which flags you need to add for each model, you can read. > https://github.com/google/deepvariant/blob/r1.4/scripts/run_deepvariant.py#L236-L253. > . Sorry that we don't have better documentation than that right now). >. > For how to run this with multiple shards, and how to run the rest of the. > commands, please read. > https://github.com/google/deepvariant/blob/r0.6/docs/deepvariant-quick-start.md. >. > I just tested the steps above and confirmed that it worked for me on. > v1.4.0, at least for the make_examples step. > If you encounter more issues with other steps, please feel free to ask. > again. I'd be happy to help. >. > Note that I don't plan to put this into an official documentation page. > now, because that adds to our maintenance burden to keep it up to date. > Given that we have the Docker/Singularity solution that works generally. > well for our users, I don't expect many of our users to need to use. > pre-built binaries. @zivlang <https://github.com/zivlang> thank you for. > your question so I have a chance to test it again and document it here. > Hopefully this is helpful for you. Happy to answer more questions if you. > encounter more problems. >. > —. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/590#issuecomment-1322695241>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/ALBF75YWC2I4SZFXBKRL4KTWJPVBNANCNFSM6AAAAAASFZYTTI>. > . > You are receiving this because you were mentioned.Message ID:. > ***@***.***>. >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/590
https://github.com/google/deepvariant/issues/590:4122,usability,document,document,4122,"ATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. >. > Run make_examples:. >. > OUTPUT_DIR=""${PWD}/quickstart-output"". > mkdir -p ""${OUTPUT_DIR}"". >. > python bin/make_examples.zip \. > --mode calling \. > --ref ""${INPUT_DIR}/ucsc.hg19.chr20.unittest.fasta"" \. > --reads ""${INPUT_DIR}/NA12878_S1.chr20.10_10p1mb.bam"" \. > --regions ""chr20:10,000,000-10,010,000"" \. > --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"" \. > --channels ""insert_size"". >. > (To figure out which flags you need to add for each model, you can read. > https://github.com/google/deepvariant/blob/r1.4/scripts/run_deepvariant.py#L236-L253. > . Sorry that we don't have better documentation than that right now). >. > For how to run this with multiple shards, and how to run the rest of the. > commands, please read. > https://github.com/google/deepvariant/blob/r0.6/docs/deepvariant-quick-start.md. >. > I just tested the steps above and confirmed that it worked for me on. > v1.4.0, at least for the make_examples step. > If you encounter more issues with other steps, please feel free to ask. > again. I'd be happy to help. >. > Note that I don't plan to put this into an official documentation page. > now, because that adds to our maintenance burden to keep it up to date. > Given that we have the Docker/Singularity solution that works generally. > well for our users, I don't expect many of our users to need to use. > pre-built binaries. @zivlang <https://github.com/zivlang> thank you for. > your question so I have a chance to test it again and document it here. > Hopefully this is helpful for you. Happy to answer more questions if you. > encounter more problems. >. > —. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/590#issuecomment-1322695241>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/ALBF75YWC2I4SZFXBKRL4KTWJPVBNANCNFSM6AAAAAASFZYTTI>. > . > You are receiving this because you were mentioned.Message ID:. > ***@***.***>. >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/590
https://github.com/google/deepvariant/issues/590:4160,usability,help,helpful,4160,"ATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. >. > Run make_examples:. >. > OUTPUT_DIR=""${PWD}/quickstart-output"". > mkdir -p ""${OUTPUT_DIR}"". >. > python bin/make_examples.zip \. > --mode calling \. > --ref ""${INPUT_DIR}/ucsc.hg19.chr20.unittest.fasta"" \. > --reads ""${INPUT_DIR}/NA12878_S1.chr20.10_10p1mb.bam"" \. > --regions ""chr20:10,000,000-10,010,000"" \. > --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"" \. > --channels ""insert_size"". >. > (To figure out which flags you need to add for each model, you can read. > https://github.com/google/deepvariant/blob/r1.4/scripts/run_deepvariant.py#L236-L253. > . Sorry that we don't have better documentation than that right now). >. > For how to run this with multiple shards, and how to run the rest of the. > commands, please read. > https://github.com/google/deepvariant/blob/r0.6/docs/deepvariant-quick-start.md. >. > I just tested the steps above and confirmed that it worked for me on. > v1.4.0, at least for the make_examples step. > If you encounter more issues with other steps, please feel free to ask. > again. I'd be happy to help. >. > Note that I don't plan to put this into an official documentation page. > now, because that adds to our maintenance burden to keep it up to date. > Given that we have the Docker/Singularity solution that works generally. > well for our users, I don't expect many of our users to need to use. > pre-built binaries. @zivlang <https://github.com/zivlang> thank you for. > your question so I have a chance to test it again and document it here. > Hopefully this is helpful for you. Happy to answer more questions if you. > encounter more problems. >. > —. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/590#issuecomment-1322695241>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/ALBF75YWC2I4SZFXBKRL4KTWJPVBNANCNFSM6AAAAAASFZYTTI>. > . > You are receiving this because you were mentioned.Message ID:. > ***@***.***>. >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/590
https://github.com/google/deepvariant/issues/590:20,usability,efficien,efficient,20,@George-du The most efficient pre-built binaries would be the Docker/Singularity approach.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/590
https://github.com/google/deepvariant/issues/591:36,deployability,instal,install,36,@leedchou is it possible for you to install Docker and/or singularity and run DeepVariant using that? . This may be an easier route than enabling builds on Ubuntu16.04. Currently we only build/test on Ubuntu 20.04.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/591
https://github.com/google/deepvariant/issues/591:146,deployability,build,builds,146,@leedchou is it possible for you to install Docker and/or singularity and run DeepVariant using that? . This may be an easier route than enabling builds on Ubuntu16.04. Currently we only build/test on Ubuntu 20.04.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/591
https://github.com/google/deepvariant/issues/591:187,deployability,build,build,187,@leedchou is it possible for you to install Docker and/or singularity and run DeepVariant using that? . This may be an easier route than enabling builds on Ubuntu16.04. Currently we only build/test on Ubuntu 20.04.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/591
https://github.com/google/deepvariant/issues/591:169,energy efficiency,Current,Currently,169,@leedchou is it possible for you to install Docker and/or singularity and run DeepVariant using that? . This may be an easier route than enabling builds on Ubuntu16.04. Currently we only build/test on Ubuntu 20.04.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/591
https://github.com/google/deepvariant/issues/591:126,integrability,rout,route,126,@leedchou is it possible for you to install Docker and/or singularity and run DeepVariant using that? . This may be an easier route than enabling builds on Ubuntu16.04. Currently we only build/test on Ubuntu 20.04.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/591
https://github.com/google/deepvariant/issues/591:193,safety,test,test,193,@leedchou is it possible for you to install Docker and/or singularity and run DeepVariant using that? . This may be an easier route than enabling builds on Ubuntu16.04. Currently we only build/test on Ubuntu 20.04.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/591
https://github.com/google/deepvariant/issues/591:193,testability,test,test,193,@leedchou is it possible for you to install Docker and/or singularity and run DeepVariant using that? . This may be an easier route than enabling builds on Ubuntu16.04. Currently we only build/test on Ubuntu 20.04.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/591
https://github.com/google/deepvariant/issues/591:38,deployability,instal,install,38,"> @leedchou is it possible for you to install Docker and/or singularity and run DeepVariant using that? > . > This may be an easier route than enabling builds on Ubuntu16.04. Currently we only build/test on Ubuntu 20.04. thanks@danielecook. Yes, I am able to run DV using Docker, which did work. I need to debug some parts of this project to better understand it, so I have to build it from source. Do you mean there is a route to build DV using Docker or Singularity? .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/591
https://github.com/google/deepvariant/issues/591:152,deployability,build,builds,152,"> @leedchou is it possible for you to install Docker and/or singularity and run DeepVariant using that? > . > This may be an easier route than enabling builds on Ubuntu16.04. Currently we only build/test on Ubuntu 20.04. thanks@danielecook. Yes, I am able to run DV using Docker, which did work. I need to debug some parts of this project to better understand it, so I have to build it from source. Do you mean there is a route to build DV using Docker or Singularity? .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/591
https://github.com/google/deepvariant/issues/591:193,deployability,build,build,193,"> @leedchou is it possible for you to install Docker and/or singularity and run DeepVariant using that? > . > This may be an easier route than enabling builds on Ubuntu16.04. Currently we only build/test on Ubuntu 20.04. thanks@danielecook. Yes, I am able to run DV using Docker, which did work. I need to debug some parts of this project to better understand it, so I have to build it from source. Do you mean there is a route to build DV using Docker or Singularity? .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/591
https://github.com/google/deepvariant/issues/591:377,deployability,build,build,377,"> @leedchou is it possible for you to install Docker and/or singularity and run DeepVariant using that? > . > This may be an easier route than enabling builds on Ubuntu16.04. Currently we only build/test on Ubuntu 20.04. thanks@danielecook. Yes, I am able to run DV using Docker, which did work. I need to debug some parts of this project to better understand it, so I have to build it from source. Do you mean there is a route to build DV using Docker or Singularity? .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/591
https://github.com/google/deepvariant/issues/591:431,deployability,build,build,431,"> @leedchou is it possible for you to install Docker and/or singularity and run DeepVariant using that? > . > This may be an easier route than enabling builds on Ubuntu16.04. Currently we only build/test on Ubuntu 20.04. thanks@danielecook. Yes, I am able to run DV using Docker, which did work. I need to debug some parts of this project to better understand it, so I have to build it from source. Do you mean there is a route to build DV using Docker or Singularity? .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/591
https://github.com/google/deepvariant/issues/591:175,energy efficiency,Current,Currently,175,"> @leedchou is it possible for you to install Docker and/or singularity and run DeepVariant using that? > . > This may be an easier route than enabling builds on Ubuntu16.04. Currently we only build/test on Ubuntu 20.04. thanks@danielecook. Yes, I am able to run DV using Docker, which did work. I need to debug some parts of this project to better understand it, so I have to build it from source. Do you mean there is a route to build DV using Docker or Singularity? .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/591
https://github.com/google/deepvariant/issues/591:132,integrability,rout,route,132,"> @leedchou is it possible for you to install Docker and/or singularity and run DeepVariant using that? > . > This may be an easier route than enabling builds on Ubuntu16.04. Currently we only build/test on Ubuntu 20.04. thanks@danielecook. Yes, I am able to run DV using Docker, which did work. I need to debug some parts of this project to better understand it, so I have to build it from source. Do you mean there is a route to build DV using Docker or Singularity? .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/591
https://github.com/google/deepvariant/issues/591:422,integrability,rout,route,422,"> @leedchou is it possible for you to install Docker and/or singularity and run DeepVariant using that? > . > This may be an easier route than enabling builds on Ubuntu16.04. Currently we only build/test on Ubuntu 20.04. thanks@danielecook. Yes, I am able to run DV using Docker, which did work. I need to debug some parts of this project to better understand it, so I have to build it from source. Do you mean there is a route to build DV using Docker or Singularity? .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/591
https://github.com/google/deepvariant/issues/591:199,safety,test,test,199,"> @leedchou is it possible for you to install Docker and/or singularity and run DeepVariant using that? > . > This may be an easier route than enabling builds on Ubuntu16.04. Currently we only build/test on Ubuntu 20.04. thanks@danielecook. Yes, I am able to run DV using Docker, which did work. I need to debug some parts of this project to better understand it, so I have to build it from source. Do you mean there is a route to build DV using Docker or Singularity? .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/591
https://github.com/google/deepvariant/issues/591:199,testability,test,test,199,"> @leedchou is it possible for you to install Docker and/or singularity and run DeepVariant using that? > . > This may be an easier route than enabling builds on Ubuntu16.04. Currently we only build/test on Ubuntu 20.04. thanks@danielecook. Yes, I am able to run DV using Docker, which did work. I need to debug some parts of this project to better understand it, so I have to build it from source. Do you mean there is a route to build DV using Docker or Singularity? .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/591
https://github.com/google/deepvariant/issues/591:349,testability,understand,understand,349,"> @leedchou is it possible for you to install Docker and/or singularity and run DeepVariant using that? > . > This may be an easier route than enabling builds on Ubuntu16.04. Currently we only build/test on Ubuntu 20.04. thanks@danielecook. Yes, I am able to run DV using Docker, which did work. I need to debug some parts of this project to better understand it, so I have to build it from source. Do you mean there is a route to build DV using Docker or Singularity? .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/591
https://github.com/google/deepvariant/issues/591:401,availability,operat,operating,401,"> @leedchou is it possible for you to install Docker and/or singularity and run DeepVariant using that? > . > This may be an easier route than enabling builds on Ubuntu16.04. Currently we only build/test on Ubuntu 20.04. Sorry, I am a beginner of Docker. I've learned something about docker technology in the past two days, figuring out what you mean. Now I can build an iamge that does not depend on operating system of my device. Thank you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/591
https://github.com/google/deepvariant/issues/591:38,deployability,instal,install,38,"> @leedchou is it possible for you to install Docker and/or singularity and run DeepVariant using that? > . > This may be an easier route than enabling builds on Ubuntu16.04. Currently we only build/test on Ubuntu 20.04. Sorry, I am a beginner of Docker. I've learned something about docker technology in the past two days, figuring out what you mean. Now I can build an iamge that does not depend on operating system of my device. Thank you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/591
https://github.com/google/deepvariant/issues/591:152,deployability,build,builds,152,"> @leedchou is it possible for you to install Docker and/or singularity and run DeepVariant using that? > . > This may be an easier route than enabling builds on Ubuntu16.04. Currently we only build/test on Ubuntu 20.04. Sorry, I am a beginner of Docker. I've learned something about docker technology in the past two days, figuring out what you mean. Now I can build an iamge that does not depend on operating system of my device. Thank you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/591
https://github.com/google/deepvariant/issues/591:193,deployability,build,build,193,"> @leedchou is it possible for you to install Docker and/or singularity and run DeepVariant using that? > . > This may be an easier route than enabling builds on Ubuntu16.04. Currently we only build/test on Ubuntu 20.04. Sorry, I am a beginner of Docker. I've learned something about docker technology in the past two days, figuring out what you mean. Now I can build an iamge that does not depend on operating system of my device. Thank you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/591
https://github.com/google/deepvariant/issues/591:362,deployability,build,build,362,"> @leedchou is it possible for you to install Docker and/or singularity and run DeepVariant using that? > . > This may be an easier route than enabling builds on Ubuntu16.04. Currently we only build/test on Ubuntu 20.04. Sorry, I am a beginner of Docker. I've learned something about docker technology in the past two days, figuring out what you mean. Now I can build an iamge that does not depend on operating system of my device. Thank you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/591
https://github.com/google/deepvariant/issues/591:391,deployability,depend,depend,391,"> @leedchou is it possible for you to install Docker and/or singularity and run DeepVariant using that? > . > This may be an easier route than enabling builds on Ubuntu16.04. Currently we only build/test on Ubuntu 20.04. Sorry, I am a beginner of Docker. I've learned something about docker technology in the past two days, figuring out what you mean. Now I can build an iamge that does not depend on operating system of my device. Thank you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/591
https://github.com/google/deepvariant/issues/591:175,energy efficiency,Current,Currently,175,"> @leedchou is it possible for you to install Docker and/or singularity and run DeepVariant using that? > . > This may be an easier route than enabling builds on Ubuntu16.04. Currently we only build/test on Ubuntu 20.04. Sorry, I am a beginner of Docker. I've learned something about docker technology in the past two days, figuring out what you mean. Now I can build an iamge that does not depend on operating system of my device. Thank you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/591
https://github.com/google/deepvariant/issues/591:132,integrability,rout,route,132,"> @leedchou is it possible for you to install Docker and/or singularity and run DeepVariant using that? > . > This may be an easier route than enabling builds on Ubuntu16.04. Currently we only build/test on Ubuntu 20.04. Sorry, I am a beginner of Docker. I've learned something about docker technology in the past two days, figuring out what you mean. Now I can build an iamge that does not depend on operating system of my device. Thank you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/591
https://github.com/google/deepvariant/issues/591:391,integrability,depend,depend,391,"> @leedchou is it possible for you to install Docker and/or singularity and run DeepVariant using that? > . > This may be an easier route than enabling builds on Ubuntu16.04. Currently we only build/test on Ubuntu 20.04. Sorry, I am a beginner of Docker. I've learned something about docker technology in the past two days, figuring out what you mean. Now I can build an iamge that does not depend on operating system of my device. Thank you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/591
https://github.com/google/deepvariant/issues/591:391,modifiability,depend,depend,391,"> @leedchou is it possible for you to install Docker and/or singularity and run DeepVariant using that? > . > This may be an easier route than enabling builds on Ubuntu16.04. Currently we only build/test on Ubuntu 20.04. Sorry, I am a beginner of Docker. I've learned something about docker technology in the past two days, figuring out what you mean. Now I can build an iamge that does not depend on operating system of my device. Thank you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/591
https://github.com/google/deepvariant/issues/591:382,reliability,doe,does,382,"> @leedchou is it possible for you to install Docker and/or singularity and run DeepVariant using that? > . > This may be an easier route than enabling builds on Ubuntu16.04. Currently we only build/test on Ubuntu 20.04. Sorry, I am a beginner of Docker. I've learned something about docker technology in the past two days, figuring out what you mean. Now I can build an iamge that does not depend on operating system of my device. Thank you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/591
https://github.com/google/deepvariant/issues/591:199,safety,test,test,199,"> @leedchou is it possible for you to install Docker and/or singularity and run DeepVariant using that? > . > This may be an easier route than enabling builds on Ubuntu16.04. Currently we only build/test on Ubuntu 20.04. Sorry, I am a beginner of Docker. I've learned something about docker technology in the past two days, figuring out what you mean. Now I can build an iamge that does not depend on operating system of my device. Thank you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/591
https://github.com/google/deepvariant/issues/591:391,safety,depend,depend,391,"> @leedchou is it possible for you to install Docker and/or singularity and run DeepVariant using that? > . > This may be an easier route than enabling builds on Ubuntu16.04. Currently we only build/test on Ubuntu 20.04. Sorry, I am a beginner of Docker. I've learned something about docker technology in the past two days, figuring out what you mean. Now I can build an iamge that does not depend on operating system of my device. Thank you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/591
https://github.com/google/deepvariant/issues/591:199,testability,test,test,199,"> @leedchou is it possible for you to install Docker and/or singularity and run DeepVariant using that? > . > This may be an easier route than enabling builds on Ubuntu16.04. Currently we only build/test on Ubuntu 20.04. Sorry, I am a beginner of Docker. I've learned something about docker technology in the past two days, figuring out what you mean. Now I can build an iamge that does not depend on operating system of my device. Thank you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/591
https://github.com/google/deepvariant/issues/591:391,testability,depend,depend,391,"> @leedchou is it possible for you to install Docker and/or singularity and run DeepVariant using that? > . > This may be an easier route than enabling builds on Ubuntu16.04. Currently we only build/test on Ubuntu 20.04. Sorry, I am a beginner of Docker. I've learned something about docker technology in the past two days, figuring out what you mean. Now I can build an iamge that does not depend on operating system of my device. Thank you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/591
https://github.com/google/deepvariant/issues/591:260,usability,learn,learned,260,"> @leedchou is it possible for you to install Docker and/or singularity and run DeepVariant using that? > . > This may be an easier route than enabling builds on Ubuntu16.04. Currently we only build/test on Ubuntu 20.04. Sorry, I am a beginner of Docker. I've learned something about docker technology in the past two days, figuring out what you mean. Now I can build an iamge that does not depend on operating system of my device. Thank you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/591
https://github.com/google/deepvariant/issues/592:490,deployability,version,version,490,"Hi @li1ba . Thank you for the report, and for the VCF lines and IGV screenshot which are very informative. there are a few items to discuss:. 1) **Difference in genotype between runs**. The GT from the two runs indicates that the neural network assesses the probability of 0/1 and 1/1 calls to be very similar (in the first entry they are rounded identically in the PL field). Your collaborator calls do have a small lean toward 1/1. DeepVariant should give identical results when the same version is run on the same hardware platform (e.g. CPU-CPU). However, there can be floating point differences with very minor (almost never at the level of the variant call, but mostly at the GQ level) between compute platforms. Can you confirm that you and your collaborator ran the exact same version of DeepVariant on the same compute platform, or if there might be a difference (e.g. CPU vs Parabricks GPU). 2) **Why is the call here 0/1 given the pileup**. The IGV screenshot you've attached certainly looks 1/1, and all experts will assess it as a 1/1 from what is shown. We have observed that in rare circumstances, DeepVariant will occasionally call such positions as 0/1 or 0/0 or to decrease the confidence in the calls of certain positions. The signature for this seems to be when DeepVariant assesses a region to be likely to be a segmental duplication or structural variant. Signatures for that often involve a haplotype with dense variants while another haplotype is almost entirely reference, or a high amount of discordant read mapping or low MAPQ. Although your pileup does have a discordantly mapped read present, those patterns generally aren't present. For some reason, in both your and your collaborator's run, DeepVariant seems to think that this region is difficult to call. . In these cases, I'm always interested to see whether this could highlight a bug in DeepVariant, or if it reflects some learning of the model. Would there be any chance for you to share the small window of the B",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/592
https://github.com/google/deepvariant/issues/592:785,deployability,version,version,785,"Hi @li1ba . Thank you for the report, and for the VCF lines and IGV screenshot which are very informative. there are a few items to discuss:. 1) **Difference in genotype between runs**. The GT from the two runs indicates that the neural network assesses the probability of 0/1 and 1/1 calls to be very similar (in the first entry they are rounded identically in the PL field). Your collaborator calls do have a small lean toward 1/1. DeepVariant should give identical results when the same version is run on the same hardware platform (e.g. CPU-CPU). However, there can be floating point differences with very minor (almost never at the level of the variant call, but mostly at the GQ level) between compute platforms. Can you confirm that you and your collaborator ran the exact same version of DeepVariant on the same compute platform, or if there might be a difference (e.g. CPU vs Parabricks GPU). 2) **Why is the call here 0/1 given the pileup**. The IGV screenshot you've attached certainly looks 1/1, and all experts will assess it as a 1/1 from what is shown. We have observed that in rare circumstances, DeepVariant will occasionally call such positions as 0/1 or 0/0 or to decrease the confidence in the calls of certain positions. The signature for this seems to be when DeepVariant assesses a region to be likely to be a segmental duplication or structural variant. Signatures for that often involve a haplotype with dense variants while another haplotype is almost entirely reference, or a high amount of discordant read mapping or low MAPQ. Although your pileup does have a discordantly mapped read present, those patterns generally aren't present. For some reason, in both your and your collaborator's run, DeepVariant seems to think that this region is difficult to call. . In these cases, I'm always interested to see whether this could highlight a bug in DeepVariant, or if it reflects some learning of the model. Would there be any chance for you to share the small window of the B",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/592
https://github.com/google/deepvariant/issues/592:1076,deployability,observ,observed,1076,"hich are very informative. there are a few items to discuss:. 1) **Difference in genotype between runs**. The GT from the two runs indicates that the neural network assesses the probability of 0/1 and 1/1 calls to be very similar (in the first entry they are rounded identically in the PL field). Your collaborator calls do have a small lean toward 1/1. DeepVariant should give identical results when the same version is run on the same hardware platform (e.g. CPU-CPU). However, there can be floating point differences with very minor (almost never at the level of the variant call, but mostly at the GQ level) between compute platforms. Can you confirm that you and your collaborator ran the exact same version of DeepVariant on the same compute platform, or if there might be a difference (e.g. CPU vs Parabricks GPU). 2) **Why is the call here 0/1 given the pileup**. The IGV screenshot you've attached certainly looks 1/1, and all experts will assess it as a 1/1 from what is shown. We have observed that in rare circumstances, DeepVariant will occasionally call such positions as 0/1 or 0/0 or to decrease the confidence in the calls of certain positions. The signature for this seems to be when DeepVariant assesses a region to be likely to be a segmental duplication or structural variant. Signatures for that often involve a haplotype with dense variants while another haplotype is almost entirely reference, or a high amount of discordant read mapping or low MAPQ. Although your pileup does have a discordantly mapped read present, those patterns generally aren't present. For some reason, in both your and your collaborator's run, DeepVariant seems to think that this region is difficult to call. . In these cases, I'm always interested to see whether this could highlight a bug in DeepVariant, or if it reflects some learning of the model. Would there be any chance for you to share the small window of the BAM file here (maybe +/- 500 bp from the position). People in the team would be i",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/592
https://github.com/google/deepvariant/issues/592:541,energy efficiency,CPU,CPU-CPU,541,"Hi @li1ba . Thank you for the report, and for the VCF lines and IGV screenshot which are very informative. there are a few items to discuss:. 1) **Difference in genotype between runs**. The GT from the two runs indicates that the neural network assesses the probability of 0/1 and 1/1 calls to be very similar (in the first entry they are rounded identically in the PL field). Your collaborator calls do have a small lean toward 1/1. DeepVariant should give identical results when the same version is run on the same hardware platform (e.g. CPU-CPU). However, there can be floating point differences with very minor (almost never at the level of the variant call, but mostly at the GQ level) between compute platforms. Can you confirm that you and your collaborator ran the exact same version of DeepVariant on the same compute platform, or if there might be a difference (e.g. CPU vs Parabricks GPU). 2) **Why is the call here 0/1 given the pileup**. The IGV screenshot you've attached certainly looks 1/1, and all experts will assess it as a 1/1 from what is shown. We have observed that in rare circumstances, DeepVariant will occasionally call such positions as 0/1 or 0/0 or to decrease the confidence in the calls of certain positions. The signature for this seems to be when DeepVariant assesses a region to be likely to be a segmental duplication or structural variant. Signatures for that often involve a haplotype with dense variants while another haplotype is almost entirely reference, or a high amount of discordant read mapping or low MAPQ. Although your pileup does have a discordantly mapped read present, those patterns generally aren't present. For some reason, in both your and your collaborator's run, DeepVariant seems to think that this region is difficult to call. . In these cases, I'm always interested to see whether this could highlight a bug in DeepVariant, or if it reflects some learning of the model. Would there be any chance for you to share the small window of the B",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/592
https://github.com/google/deepvariant/issues/592:878,energy efficiency,CPU,CPU,878,"Hi @li1ba . Thank you for the report, and for the VCF lines and IGV screenshot which are very informative. there are a few items to discuss:. 1) **Difference in genotype between runs**. The GT from the two runs indicates that the neural network assesses the probability of 0/1 and 1/1 calls to be very similar (in the first entry they are rounded identically in the PL field). Your collaborator calls do have a small lean toward 1/1. DeepVariant should give identical results when the same version is run on the same hardware platform (e.g. CPU-CPU). However, there can be floating point differences with very minor (almost never at the level of the variant call, but mostly at the GQ level) between compute platforms. Can you confirm that you and your collaborator ran the exact same version of DeepVariant on the same compute platform, or if there might be a difference (e.g. CPU vs Parabricks GPU). 2) **Why is the call here 0/1 given the pileup**. The IGV screenshot you've attached certainly looks 1/1, and all experts will assess it as a 1/1 from what is shown. We have observed that in rare circumstances, DeepVariant will occasionally call such positions as 0/1 or 0/0 or to decrease the confidence in the calls of certain positions. The signature for this seems to be when DeepVariant assesses a region to be likely to be a segmental duplication or structural variant. Signatures for that often involve a haplotype with dense variants while another haplotype is almost entirely reference, or a high amount of discordant read mapping or low MAPQ. Although your pileup does have a discordantly mapped read present, those patterns generally aren't present. For some reason, in both your and your collaborator's run, DeepVariant seems to think that this region is difficult to call. . In these cases, I'm always interested to see whether this could highlight a bug in DeepVariant, or if it reflects some learning of the model. Would there be any chance for you to share the small window of the B",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/592
https://github.com/google/deepvariant/issues/592:896,energy efficiency,GPU,GPU,896,"Hi @li1ba . Thank you for the report, and for the VCF lines and IGV screenshot which are very informative. there are a few items to discuss:. 1) **Difference in genotype between runs**. The GT from the two runs indicates that the neural network assesses the probability of 0/1 and 1/1 calls to be very similar (in the first entry they are rounded identically in the PL field). Your collaborator calls do have a small lean toward 1/1. DeepVariant should give identical results when the same version is run on the same hardware platform (e.g. CPU-CPU). However, there can be floating point differences with very minor (almost never at the level of the variant call, but mostly at the GQ level) between compute platforms. Can you confirm that you and your collaborator ran the exact same version of DeepVariant on the same compute platform, or if there might be a difference (e.g. CPU vs Parabricks GPU). 2) **Why is the call here 0/1 given the pileup**. The IGV screenshot you've attached certainly looks 1/1, and all experts will assess it as a 1/1 from what is shown. We have observed that in rare circumstances, DeepVariant will occasionally call such positions as 0/1 or 0/0 or to decrease the confidence in the calls of certain positions. The signature for this seems to be when DeepVariant assesses a region to be likely to be a segmental duplication or structural variant. Signatures for that often involve a haplotype with dense variants while another haplotype is almost entirely reference, or a high amount of discordant read mapping or low MAPQ. Although your pileup does have a discordantly mapped read present, those patterns generally aren't present. For some reason, in both your and your collaborator's run, DeepVariant seems to think that this region is difficult to call. . In these cases, I'm always interested to see whether this could highlight a bug in DeepVariant, or if it reflects some learning of the model. Would there be any chance for you to share the small window of the B",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/592
https://github.com/google/deepvariant/issues/592:1925,energy efficiency,model,model,1925,"GT from the two runs indicates that the neural network assesses the probability of 0/1 and 1/1 calls to be very similar (in the first entry they are rounded identically in the PL field). Your collaborator calls do have a small lean toward 1/1. DeepVariant should give identical results when the same version is run on the same hardware platform (e.g. CPU-CPU). However, there can be floating point differences with very minor (almost never at the level of the variant call, but mostly at the GQ level) between compute platforms. Can you confirm that you and your collaborator ran the exact same version of DeepVariant on the same compute platform, or if there might be a difference (e.g. CPU vs Parabricks GPU). 2) **Why is the call here 0/1 given the pileup**. The IGV screenshot you've attached certainly looks 1/1, and all experts will assess it as a 1/1 from what is shown. We have observed that in rare circumstances, DeepVariant will occasionally call such positions as 0/1 or 0/0 or to decrease the confidence in the calls of certain positions. The signature for this seems to be when DeepVariant assesses a region to be likely to be a segmental duplication or structural variant. Signatures for that often involve a haplotype with dense variants while another haplotype is almost entirely reference, or a high amount of discordant read mapping or low MAPQ. Although your pileup does have a discordantly mapped read present, those patterns generally aren't present. For some reason, in both your and your collaborator's run, DeepVariant seems to think that this region is difficult to call. . In these cases, I'm always interested to see whether this could highlight a bug in DeepVariant, or if it reflects some learning of the model. Would there be any chance for you to share the small window of the BAM file here (maybe +/- 500 bp from the position). People in the team would be interested in whether this could detect any sort of edge case DeepVariant isn't handling well. Thanks,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/592
https://github.com/google/deepvariant/issues/592:490,integrability,version,version,490,"Hi @li1ba . Thank you for the report, and for the VCF lines and IGV screenshot which are very informative. there are a few items to discuss:. 1) **Difference in genotype between runs**. The GT from the two runs indicates that the neural network assesses the probability of 0/1 and 1/1 calls to be very similar (in the first entry they are rounded identically in the PL field). Your collaborator calls do have a small lean toward 1/1. DeepVariant should give identical results when the same version is run on the same hardware platform (e.g. CPU-CPU). However, there can be floating point differences with very minor (almost never at the level of the variant call, but mostly at the GQ level) between compute platforms. Can you confirm that you and your collaborator ran the exact same version of DeepVariant on the same compute platform, or if there might be a difference (e.g. CPU vs Parabricks GPU). 2) **Why is the call here 0/1 given the pileup**. The IGV screenshot you've attached certainly looks 1/1, and all experts will assess it as a 1/1 from what is shown. We have observed that in rare circumstances, DeepVariant will occasionally call such positions as 0/1 or 0/0 or to decrease the confidence in the calls of certain positions. The signature for this seems to be when DeepVariant assesses a region to be likely to be a segmental duplication or structural variant. Signatures for that often involve a haplotype with dense variants while another haplotype is almost entirely reference, or a high amount of discordant read mapping or low MAPQ. Although your pileup does have a discordantly mapped read present, those patterns generally aren't present. For some reason, in both your and your collaborator's run, DeepVariant seems to think that this region is difficult to call. . In these cases, I'm always interested to see whether this could highlight a bug in DeepVariant, or if it reflects some learning of the model. Would there be any chance for you to share the small window of the B",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/592
https://github.com/google/deepvariant/issues/592:785,integrability,version,version,785,"Hi @li1ba . Thank you for the report, and for the VCF lines and IGV screenshot which are very informative. there are a few items to discuss:. 1) **Difference in genotype between runs**. The GT from the two runs indicates that the neural network assesses the probability of 0/1 and 1/1 calls to be very similar (in the first entry they are rounded identically in the PL field). Your collaborator calls do have a small lean toward 1/1. DeepVariant should give identical results when the same version is run on the same hardware platform (e.g. CPU-CPU). However, there can be floating point differences with very minor (almost never at the level of the variant call, but mostly at the GQ level) between compute platforms. Can you confirm that you and your collaborator ran the exact same version of DeepVariant on the same compute platform, or if there might be a difference (e.g. CPU vs Parabricks GPU). 2) **Why is the call here 0/1 given the pileup**. The IGV screenshot you've attached certainly looks 1/1, and all experts will assess it as a 1/1 from what is shown. We have observed that in rare circumstances, DeepVariant will occasionally call such positions as 0/1 or 0/0 or to decrease the confidence in the calls of certain positions. The signature for this seems to be when DeepVariant assesses a region to be likely to be a segmental duplication or structural variant. Signatures for that often involve a haplotype with dense variants while another haplotype is almost entirely reference, or a high amount of discordant read mapping or low MAPQ. Although your pileup does have a discordantly mapped read present, those patterns generally aren't present. For some reason, in both your and your collaborator's run, DeepVariant seems to think that this region is difficult to call. . In these cases, I'm always interested to see whether this could highlight a bug in DeepVariant, or if it reflects some learning of the model. Would there be any chance for you to share the small window of the B",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/592
https://github.com/google/deepvariant/issues/592:526,interoperability,platform,platform,526,"Hi @li1ba . Thank you for the report, and for the VCF lines and IGV screenshot which are very informative. there are a few items to discuss:. 1) **Difference in genotype between runs**. The GT from the two runs indicates that the neural network assesses the probability of 0/1 and 1/1 calls to be very similar (in the first entry they are rounded identically in the PL field). Your collaborator calls do have a small lean toward 1/1. DeepVariant should give identical results when the same version is run on the same hardware platform (e.g. CPU-CPU). However, there can be floating point differences with very minor (almost never at the level of the variant call, but mostly at the GQ level) between compute platforms. Can you confirm that you and your collaborator ran the exact same version of DeepVariant on the same compute platform, or if there might be a difference (e.g. CPU vs Parabricks GPU). 2) **Why is the call here 0/1 given the pileup**. The IGV screenshot you've attached certainly looks 1/1, and all experts will assess it as a 1/1 from what is shown. We have observed that in rare circumstances, DeepVariant will occasionally call such positions as 0/1 or 0/0 or to decrease the confidence in the calls of certain positions. The signature for this seems to be when DeepVariant assesses a region to be likely to be a segmental duplication or structural variant. Signatures for that often involve a haplotype with dense variants while another haplotype is almost entirely reference, or a high amount of discordant read mapping or low MAPQ. Although your pileup does have a discordantly mapped read present, those patterns generally aren't present. For some reason, in both your and your collaborator's run, DeepVariant seems to think that this region is difficult to call. . In these cases, I'm always interested to see whether this could highlight a bug in DeepVariant, or if it reflects some learning of the model. Would there be any chance for you to share the small window of the B",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/592
https://github.com/google/deepvariant/issues/592:708,interoperability,platform,platforms,708,"Hi @li1ba . Thank you for the report, and for the VCF lines and IGV screenshot which are very informative. there are a few items to discuss:. 1) **Difference in genotype between runs**. The GT from the two runs indicates that the neural network assesses the probability of 0/1 and 1/1 calls to be very similar (in the first entry they are rounded identically in the PL field). Your collaborator calls do have a small lean toward 1/1. DeepVariant should give identical results when the same version is run on the same hardware platform (e.g. CPU-CPU). However, there can be floating point differences with very minor (almost never at the level of the variant call, but mostly at the GQ level) between compute platforms. Can you confirm that you and your collaborator ran the exact same version of DeepVariant on the same compute platform, or if there might be a difference (e.g. CPU vs Parabricks GPU). 2) **Why is the call here 0/1 given the pileup**. The IGV screenshot you've attached certainly looks 1/1, and all experts will assess it as a 1/1 from what is shown. We have observed that in rare circumstances, DeepVariant will occasionally call such positions as 0/1 or 0/0 or to decrease the confidence in the calls of certain positions. The signature for this seems to be when DeepVariant assesses a region to be likely to be a segmental duplication or structural variant. Signatures for that often involve a haplotype with dense variants while another haplotype is almost entirely reference, or a high amount of discordant read mapping or low MAPQ. Although your pileup does have a discordantly mapped read present, those patterns generally aren't present. For some reason, in both your and your collaborator's run, DeepVariant seems to think that this region is difficult to call. . In these cases, I'm always interested to see whether this could highlight a bug in DeepVariant, or if it reflects some learning of the model. Would there be any chance for you to share the small window of the B",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/592
https://github.com/google/deepvariant/issues/592:828,interoperability,platform,platform,828,"Hi @li1ba . Thank you for the report, and for the VCF lines and IGV screenshot which are very informative. there are a few items to discuss:. 1) **Difference in genotype between runs**. The GT from the two runs indicates that the neural network assesses the probability of 0/1 and 1/1 calls to be very similar (in the first entry they are rounded identically in the PL field). Your collaborator calls do have a small lean toward 1/1. DeepVariant should give identical results when the same version is run on the same hardware platform (e.g. CPU-CPU). However, there can be floating point differences with very minor (almost never at the level of the variant call, but mostly at the GQ level) between compute platforms. Can you confirm that you and your collaborator ran the exact same version of DeepVariant on the same compute platform, or if there might be a difference (e.g. CPU vs Parabricks GPU). 2) **Why is the call here 0/1 given the pileup**. The IGV screenshot you've attached certainly looks 1/1, and all experts will assess it as a 1/1 from what is shown. We have observed that in rare circumstances, DeepVariant will occasionally call such positions as 0/1 or 0/0 or to decrease the confidence in the calls of certain positions. The signature for this seems to be when DeepVariant assesses a region to be likely to be a segmental duplication or structural variant. Signatures for that often involve a haplotype with dense variants while another haplotype is almost entirely reference, or a high amount of discordant read mapping or low MAPQ. Although your pileup does have a discordantly mapped read present, those patterns generally aren't present. For some reason, in both your and your collaborator's run, DeepVariant seems to think that this region is difficult to call. . In these cases, I'm always interested to see whether this could highlight a bug in DeepVariant, or if it reflects some learning of the model. Would there be any chance for you to share the small window of the B",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/592
https://github.com/google/deepvariant/issues/592:1969,interoperability,share,share,1969,"GT from the two runs indicates that the neural network assesses the probability of 0/1 and 1/1 calls to be very similar (in the first entry they are rounded identically in the PL field). Your collaborator calls do have a small lean toward 1/1. DeepVariant should give identical results when the same version is run on the same hardware platform (e.g. CPU-CPU). However, there can be floating point differences with very minor (almost never at the level of the variant call, but mostly at the GQ level) between compute platforms. Can you confirm that you and your collaborator ran the exact same version of DeepVariant on the same compute platform, or if there might be a difference (e.g. CPU vs Parabricks GPU). 2) **Why is the call here 0/1 given the pileup**. The IGV screenshot you've attached certainly looks 1/1, and all experts will assess it as a 1/1 from what is shown. We have observed that in rare circumstances, DeepVariant will occasionally call such positions as 0/1 or 0/0 or to decrease the confidence in the calls of certain positions. The signature for this seems to be when DeepVariant assesses a region to be likely to be a segmental duplication or structural variant. Signatures for that often involve a haplotype with dense variants while another haplotype is almost entirely reference, or a high amount of discordant read mapping or low MAPQ. Although your pileup does have a discordantly mapped read present, those patterns generally aren't present. For some reason, in both your and your collaborator's run, DeepVariant seems to think that this region is difficult to call. . In these cases, I'm always interested to see whether this could highlight a bug in DeepVariant, or if it reflects some learning of the model. Would there be any chance for you to share the small window of the BAM file here (maybe +/- 500 bp from the position). People in the team would be interested in whether this could detect any sort of edge case DeepVariant isn't handling well. Thanks,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/592
https://github.com/google/deepvariant/issues/592:490,modifiability,version,version,490,"Hi @li1ba . Thank you for the report, and for the VCF lines and IGV screenshot which are very informative. there are a few items to discuss:. 1) **Difference in genotype between runs**. The GT from the two runs indicates that the neural network assesses the probability of 0/1 and 1/1 calls to be very similar (in the first entry they are rounded identically in the PL field). Your collaborator calls do have a small lean toward 1/1. DeepVariant should give identical results when the same version is run on the same hardware platform (e.g. CPU-CPU). However, there can be floating point differences with very minor (almost never at the level of the variant call, but mostly at the GQ level) between compute platforms. Can you confirm that you and your collaborator ran the exact same version of DeepVariant on the same compute platform, or if there might be a difference (e.g. CPU vs Parabricks GPU). 2) **Why is the call here 0/1 given the pileup**. The IGV screenshot you've attached certainly looks 1/1, and all experts will assess it as a 1/1 from what is shown. We have observed that in rare circumstances, DeepVariant will occasionally call such positions as 0/1 or 0/0 or to decrease the confidence in the calls of certain positions. The signature for this seems to be when DeepVariant assesses a region to be likely to be a segmental duplication or structural variant. Signatures for that often involve a haplotype with dense variants while another haplotype is almost entirely reference, or a high amount of discordant read mapping or low MAPQ. Although your pileup does have a discordantly mapped read present, those patterns generally aren't present. For some reason, in both your and your collaborator's run, DeepVariant seems to think that this region is difficult to call. . In these cases, I'm always interested to see whether this could highlight a bug in DeepVariant, or if it reflects some learning of the model. Would there be any chance for you to share the small window of the B",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/592
https://github.com/google/deepvariant/issues/592:785,modifiability,version,version,785,"Hi @li1ba . Thank you for the report, and for the VCF lines and IGV screenshot which are very informative. there are a few items to discuss:. 1) **Difference in genotype between runs**. The GT from the two runs indicates that the neural network assesses the probability of 0/1 and 1/1 calls to be very similar (in the first entry they are rounded identically in the PL field). Your collaborator calls do have a small lean toward 1/1. DeepVariant should give identical results when the same version is run on the same hardware platform (e.g. CPU-CPU). However, there can be floating point differences with very minor (almost never at the level of the variant call, but mostly at the GQ level) between compute platforms. Can you confirm that you and your collaborator ran the exact same version of DeepVariant on the same compute platform, or if there might be a difference (e.g. CPU vs Parabricks GPU). 2) **Why is the call here 0/1 given the pileup**. The IGV screenshot you've attached certainly looks 1/1, and all experts will assess it as a 1/1 from what is shown. We have observed that in rare circumstances, DeepVariant will occasionally call such positions as 0/1 or 0/0 or to decrease the confidence in the calls of certain positions. The signature for this seems to be when DeepVariant assesses a region to be likely to be a segmental duplication or structural variant. Signatures for that often involve a haplotype with dense variants while another haplotype is almost entirely reference, or a high amount of discordant read mapping or low MAPQ. Although your pileup does have a discordantly mapped read present, those patterns generally aren't present. For some reason, in both your and your collaborator's run, DeepVariant seems to think that this region is difficult to call. . In these cases, I'm always interested to see whether this could highlight a bug in DeepVariant, or if it reflects some learning of the model. Would there be any chance for you to share the small window of the B",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/592
https://github.com/google/deepvariant/issues/592:237,performance,network,network,237,"Hi @li1ba . Thank you for the report, and for the VCF lines and IGV screenshot which are very informative. there are a few items to discuss:. 1) **Difference in genotype between runs**. The GT from the two runs indicates that the neural network assesses the probability of 0/1 and 1/1 calls to be very similar (in the first entry they are rounded identically in the PL field). Your collaborator calls do have a small lean toward 1/1. DeepVariant should give identical results when the same version is run on the same hardware platform (e.g. CPU-CPU). However, there can be floating point differences with very minor (almost never at the level of the variant call, but mostly at the GQ level) between compute platforms. Can you confirm that you and your collaborator ran the exact same version of DeepVariant on the same compute platform, or if there might be a difference (e.g. CPU vs Parabricks GPU). 2) **Why is the call here 0/1 given the pileup**. The IGV screenshot you've attached certainly looks 1/1, and all experts will assess it as a 1/1 from what is shown. We have observed that in rare circumstances, DeepVariant will occasionally call such positions as 0/1 or 0/0 or to decrease the confidence in the calls of certain positions. The signature for this seems to be when DeepVariant assesses a region to be likely to be a segmental duplication or structural variant. Signatures for that often involve a haplotype with dense variants while another haplotype is almost entirely reference, or a high amount of discordant read mapping or low MAPQ. Although your pileup does have a discordantly mapped read present, those patterns generally aren't present. For some reason, in both your and your collaborator's run, DeepVariant seems to think that this region is difficult to call. . In these cases, I'm always interested to see whether this could highlight a bug in DeepVariant, or if it reflects some learning of the model. Would there be any chance for you to share the small window of the B",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/592
https://github.com/google/deepvariant/issues/592:541,performance,CPU,CPU-CPU,541,"Hi @li1ba . Thank you for the report, and for the VCF lines and IGV screenshot which are very informative. there are a few items to discuss:. 1) **Difference in genotype between runs**. The GT from the two runs indicates that the neural network assesses the probability of 0/1 and 1/1 calls to be very similar (in the first entry they are rounded identically in the PL field). Your collaborator calls do have a small lean toward 1/1. DeepVariant should give identical results when the same version is run on the same hardware platform (e.g. CPU-CPU). However, there can be floating point differences with very minor (almost never at the level of the variant call, but mostly at the GQ level) between compute platforms. Can you confirm that you and your collaborator ran the exact same version of DeepVariant on the same compute platform, or if there might be a difference (e.g. CPU vs Parabricks GPU). 2) **Why is the call here 0/1 given the pileup**. The IGV screenshot you've attached certainly looks 1/1, and all experts will assess it as a 1/1 from what is shown. We have observed that in rare circumstances, DeepVariant will occasionally call such positions as 0/1 or 0/0 or to decrease the confidence in the calls of certain positions. The signature for this seems to be when DeepVariant assesses a region to be likely to be a segmental duplication or structural variant. Signatures for that often involve a haplotype with dense variants while another haplotype is almost entirely reference, or a high amount of discordant read mapping or low MAPQ. Although your pileup does have a discordantly mapped read present, those patterns generally aren't present. For some reason, in both your and your collaborator's run, DeepVariant seems to think that this region is difficult to call. . In these cases, I'm always interested to see whether this could highlight a bug in DeepVariant, or if it reflects some learning of the model. Would there be any chance for you to share the small window of the B",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/592
https://github.com/google/deepvariant/issues/592:878,performance,CPU,CPU,878,"Hi @li1ba . Thank you for the report, and for the VCF lines and IGV screenshot which are very informative. there are a few items to discuss:. 1) **Difference in genotype between runs**. The GT from the two runs indicates that the neural network assesses the probability of 0/1 and 1/1 calls to be very similar (in the first entry they are rounded identically in the PL field). Your collaborator calls do have a small lean toward 1/1. DeepVariant should give identical results when the same version is run on the same hardware platform (e.g. CPU-CPU). However, there can be floating point differences with very minor (almost never at the level of the variant call, but mostly at the GQ level) between compute platforms. Can you confirm that you and your collaborator ran the exact same version of DeepVariant on the same compute platform, or if there might be a difference (e.g. CPU vs Parabricks GPU). 2) **Why is the call here 0/1 given the pileup**. The IGV screenshot you've attached certainly looks 1/1, and all experts will assess it as a 1/1 from what is shown. We have observed that in rare circumstances, DeepVariant will occasionally call such positions as 0/1 or 0/0 or to decrease the confidence in the calls of certain positions. The signature for this seems to be when DeepVariant assesses a region to be likely to be a segmental duplication or structural variant. Signatures for that often involve a haplotype with dense variants while another haplotype is almost entirely reference, or a high amount of discordant read mapping or low MAPQ. Although your pileup does have a discordantly mapped read present, those patterns generally aren't present. For some reason, in both your and your collaborator's run, DeepVariant seems to think that this region is difficult to call. . In these cases, I'm always interested to see whether this could highlight a bug in DeepVariant, or if it reflects some learning of the model. Would there be any chance for you to share the small window of the B",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/592
https://github.com/google/deepvariant/issues/592:896,performance,GPU,GPU,896,"Hi @li1ba . Thank you for the report, and for the VCF lines and IGV screenshot which are very informative. there are a few items to discuss:. 1) **Difference in genotype between runs**. The GT from the two runs indicates that the neural network assesses the probability of 0/1 and 1/1 calls to be very similar (in the first entry they are rounded identically in the PL field). Your collaborator calls do have a small lean toward 1/1. DeepVariant should give identical results when the same version is run on the same hardware platform (e.g. CPU-CPU). However, there can be floating point differences with very minor (almost never at the level of the variant call, but mostly at the GQ level) between compute platforms. Can you confirm that you and your collaborator ran the exact same version of DeepVariant on the same compute platform, or if there might be a difference (e.g. CPU vs Parabricks GPU). 2) **Why is the call here 0/1 given the pileup**. The IGV screenshot you've attached certainly looks 1/1, and all experts will assess it as a 1/1 from what is shown. We have observed that in rare circumstances, DeepVariant will occasionally call such positions as 0/1 or 0/0 or to decrease the confidence in the calls of certain positions. The signature for this seems to be when DeepVariant assesses a region to be likely to be a segmental duplication or structural variant. Signatures for that often involve a haplotype with dense variants while another haplotype is almost entirely reference, or a high amount of discordant read mapping or low MAPQ. Although your pileup does have a discordantly mapped read present, those patterns generally aren't present. For some reason, in both your and your collaborator's run, DeepVariant seems to think that this region is difficult to call. . In these cases, I'm always interested to see whether this could highlight a bug in DeepVariant, or if it reflects some learning of the model. Would there be any chance for you to share the small window of the B",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/592
https://github.com/google/deepvariant/issues/592:1576,reliability,doe,does,1576,"GT from the two runs indicates that the neural network assesses the probability of 0/1 and 1/1 calls to be very similar (in the first entry they are rounded identically in the PL field). Your collaborator calls do have a small lean toward 1/1. DeepVariant should give identical results when the same version is run on the same hardware platform (e.g. CPU-CPU). However, there can be floating point differences with very minor (almost never at the level of the variant call, but mostly at the GQ level) between compute platforms. Can you confirm that you and your collaborator ran the exact same version of DeepVariant on the same compute platform, or if there might be a difference (e.g. CPU vs Parabricks GPU). 2) **Why is the call here 0/1 given the pileup**. The IGV screenshot you've attached certainly looks 1/1, and all experts will assess it as a 1/1 from what is shown. We have observed that in rare circumstances, DeepVariant will occasionally call such positions as 0/1 or 0/0 or to decrease the confidence in the calls of certain positions. The signature for this seems to be when DeepVariant assesses a region to be likely to be a segmental duplication or structural variant. Signatures for that often involve a haplotype with dense variants while another haplotype is almost entirely reference, or a high amount of discordant read mapping or low MAPQ. Although your pileup does have a discordantly mapped read present, those patterns generally aren't present. For some reason, in both your and your collaborator's run, DeepVariant seems to think that this region is difficult to call. . In these cases, I'm always interested to see whether this could highlight a bug in DeepVariant, or if it reflects some learning of the model. Would there be any chance for you to share the small window of the BAM file here (maybe +/- 500 bp from the position). People in the team would be interested in whether this could detect any sort of edge case DeepVariant isn't handling well. Thanks,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/592
https://github.com/google/deepvariant/issues/592:2112,safety,detect,detect,2112,"GT from the two runs indicates that the neural network assesses the probability of 0/1 and 1/1 calls to be very similar (in the first entry they are rounded identically in the PL field). Your collaborator calls do have a small lean toward 1/1. DeepVariant should give identical results when the same version is run on the same hardware platform (e.g. CPU-CPU). However, there can be floating point differences with very minor (almost never at the level of the variant call, but mostly at the GQ level) between compute platforms. Can you confirm that you and your collaborator ran the exact same version of DeepVariant on the same compute platform, or if there might be a difference (e.g. CPU vs Parabricks GPU). 2) **Why is the call here 0/1 given the pileup**. The IGV screenshot you've attached certainly looks 1/1, and all experts will assess it as a 1/1 from what is shown. We have observed that in rare circumstances, DeepVariant will occasionally call such positions as 0/1 or 0/0 or to decrease the confidence in the calls of certain positions. The signature for this seems to be when DeepVariant assesses a region to be likely to be a segmental duplication or structural variant. Signatures for that often involve a haplotype with dense variants while another haplotype is almost entirely reference, or a high amount of discordant read mapping or low MAPQ. Although your pileup does have a discordantly mapped read present, those patterns generally aren't present. For some reason, in both your and your collaborator's run, DeepVariant seems to think that this region is difficult to call. . In these cases, I'm always interested to see whether this could highlight a bug in DeepVariant, or if it reflects some learning of the model. Would there be any chance for you to share the small window of the BAM file here (maybe +/- 500 bp from the position). People in the team would be interested in whether this could detect any sort of edge case DeepVariant isn't handling well. Thanks,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/592
https://github.com/google/deepvariant/issues/592:237,security,network,network,237,"Hi @li1ba . Thank you for the report, and for the VCF lines and IGV screenshot which are very informative. there are a few items to discuss:. 1) **Difference in genotype between runs**. The GT from the two runs indicates that the neural network assesses the probability of 0/1 and 1/1 calls to be very similar (in the first entry they are rounded identically in the PL field). Your collaborator calls do have a small lean toward 1/1. DeepVariant should give identical results when the same version is run on the same hardware platform (e.g. CPU-CPU). However, there can be floating point differences with very minor (almost never at the level of the variant call, but mostly at the GQ level) between compute platforms. Can you confirm that you and your collaborator ran the exact same version of DeepVariant on the same compute platform, or if there might be a difference (e.g. CPU vs Parabricks GPU). 2) **Why is the call here 0/1 given the pileup**. The IGV screenshot you've attached certainly looks 1/1, and all experts will assess it as a 1/1 from what is shown. We have observed that in rare circumstances, DeepVariant will occasionally call such positions as 0/1 or 0/0 or to decrease the confidence in the calls of certain positions. The signature for this seems to be when DeepVariant assesses a region to be likely to be a segmental duplication or structural variant. Signatures for that often involve a haplotype with dense variants while another haplotype is almost entirely reference, or a high amount of discordant read mapping or low MAPQ. Although your pileup does have a discordantly mapped read present, those patterns generally aren't present. For some reason, in both your and your collaborator's run, DeepVariant seems to think that this region is difficult to call. . In these cases, I'm always interested to see whether this could highlight a bug in DeepVariant, or if it reflects some learning of the model. Would there be any chance for you to share the small window of the B",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/592
https://github.com/google/deepvariant/issues/592:245,security,assess,assesses,245,"Hi @li1ba . Thank you for the report, and for the VCF lines and IGV screenshot which are very informative. there are a few items to discuss:. 1) **Difference in genotype between runs**. The GT from the two runs indicates that the neural network assesses the probability of 0/1 and 1/1 calls to be very similar (in the first entry they are rounded identically in the PL field). Your collaborator calls do have a small lean toward 1/1. DeepVariant should give identical results when the same version is run on the same hardware platform (e.g. CPU-CPU). However, there can be floating point differences with very minor (almost never at the level of the variant call, but mostly at the GQ level) between compute platforms. Can you confirm that you and your collaborator ran the exact same version of DeepVariant on the same compute platform, or if there might be a difference (e.g. CPU vs Parabricks GPU). 2) **Why is the call here 0/1 given the pileup**. The IGV screenshot you've attached certainly looks 1/1, and all experts will assess it as a 1/1 from what is shown. We have observed that in rare circumstances, DeepVariant will occasionally call such positions as 0/1 or 0/0 or to decrease the confidence in the calls of certain positions. The signature for this seems to be when DeepVariant assesses a region to be likely to be a segmental duplication or structural variant. Signatures for that often involve a haplotype with dense variants while another haplotype is almost entirely reference, or a high amount of discordant read mapping or low MAPQ. Although your pileup does have a discordantly mapped read present, those patterns generally aren't present. For some reason, in both your and your collaborator's run, DeepVariant seems to think that this region is difficult to call. . In these cases, I'm always interested to see whether this could highlight a bug in DeepVariant, or if it reflects some learning of the model. Would there be any chance for you to share the small window of the B",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/592
https://github.com/google/deepvariant/issues/592:347,security,ident,identically,347,"Hi @li1ba . Thank you for the report, and for the VCF lines and IGV screenshot which are very informative. there are a few items to discuss:. 1) **Difference in genotype between runs**. The GT from the two runs indicates that the neural network assesses the probability of 0/1 and 1/1 calls to be very similar (in the first entry they are rounded identically in the PL field). Your collaborator calls do have a small lean toward 1/1. DeepVariant should give identical results when the same version is run on the same hardware platform (e.g. CPU-CPU). However, there can be floating point differences with very minor (almost never at the level of the variant call, but mostly at the GQ level) between compute platforms. Can you confirm that you and your collaborator ran the exact same version of DeepVariant on the same compute platform, or if there might be a difference (e.g. CPU vs Parabricks GPU). 2) **Why is the call here 0/1 given the pileup**. The IGV screenshot you've attached certainly looks 1/1, and all experts will assess it as a 1/1 from what is shown. We have observed that in rare circumstances, DeepVariant will occasionally call such positions as 0/1 or 0/0 or to decrease the confidence in the calls of certain positions. The signature for this seems to be when DeepVariant assesses a region to be likely to be a segmental duplication or structural variant. Signatures for that often involve a haplotype with dense variants while another haplotype is almost entirely reference, or a high amount of discordant read mapping or low MAPQ. Although your pileup does have a discordantly mapped read present, those patterns generally aren't present. For some reason, in both your and your collaborator's run, DeepVariant seems to think that this region is difficult to call. . In these cases, I'm always interested to see whether this could highlight a bug in DeepVariant, or if it reflects some learning of the model. Would there be any chance for you to share the small window of the B",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/592
https://github.com/google/deepvariant/issues/592:458,security,ident,identical,458,"Hi @li1ba . Thank you for the report, and for the VCF lines and IGV screenshot which are very informative. there are a few items to discuss:. 1) **Difference in genotype between runs**. The GT from the two runs indicates that the neural network assesses the probability of 0/1 and 1/1 calls to be very similar (in the first entry they are rounded identically in the PL field). Your collaborator calls do have a small lean toward 1/1. DeepVariant should give identical results when the same version is run on the same hardware platform (e.g. CPU-CPU). However, there can be floating point differences with very minor (almost never at the level of the variant call, but mostly at the GQ level) between compute platforms. Can you confirm that you and your collaborator ran the exact same version of DeepVariant on the same compute platform, or if there might be a difference (e.g. CPU vs Parabricks GPU). 2) **Why is the call here 0/1 given the pileup**. The IGV screenshot you've attached certainly looks 1/1, and all experts will assess it as a 1/1 from what is shown. We have observed that in rare circumstances, DeepVariant will occasionally call such positions as 0/1 or 0/0 or to decrease the confidence in the calls of certain positions. The signature for this seems to be when DeepVariant assesses a region to be likely to be a segmental duplication or structural variant. Signatures for that often involve a haplotype with dense variants while another haplotype is almost entirely reference, or a high amount of discordant read mapping or low MAPQ. Although your pileup does have a discordantly mapped read present, those patterns generally aren't present. For some reason, in both your and your collaborator's run, DeepVariant seems to think that this region is difficult to call. . In these cases, I'm always interested to see whether this could highlight a bug in DeepVariant, or if it reflects some learning of the model. Would there be any chance for you to share the small window of the B",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/592
https://github.com/google/deepvariant/issues/592:1029,security,assess,assess,1029,"port, and for the VCF lines and IGV screenshot which are very informative. there are a few items to discuss:. 1) **Difference in genotype between runs**. The GT from the two runs indicates that the neural network assesses the probability of 0/1 and 1/1 calls to be very similar (in the first entry they are rounded identically in the PL field). Your collaborator calls do have a small lean toward 1/1. DeepVariant should give identical results when the same version is run on the same hardware platform (e.g. CPU-CPU). However, there can be floating point differences with very minor (almost never at the level of the variant call, but mostly at the GQ level) between compute platforms. Can you confirm that you and your collaborator ran the exact same version of DeepVariant on the same compute platform, or if there might be a difference (e.g. CPU vs Parabricks GPU). 2) **Why is the call here 0/1 given the pileup**. The IGV screenshot you've attached certainly looks 1/1, and all experts will assess it as a 1/1 from what is shown. We have observed that in rare circumstances, DeepVariant will occasionally call such positions as 0/1 or 0/0 or to decrease the confidence in the calls of certain positions. The signature for this seems to be when DeepVariant assesses a region to be likely to be a segmental duplication or structural variant. Signatures for that often involve a haplotype with dense variants while another haplotype is almost entirely reference, or a high amount of discordant read mapping or low MAPQ. Although your pileup does have a discordantly mapped read present, those patterns generally aren't present. For some reason, in both your and your collaborator's run, DeepVariant seems to think that this region is difficult to call. . In these cases, I'm always interested to see whether this could highlight a bug in DeepVariant, or if it reflects some learning of the model. Would there be any chance for you to share the small window of the BAM file here (maybe +/- 500 bp f",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/592
https://github.com/google/deepvariant/issues/592:1246,security,sign,signature,1246,"GT from the two runs indicates that the neural network assesses the probability of 0/1 and 1/1 calls to be very similar (in the first entry they are rounded identically in the PL field). Your collaborator calls do have a small lean toward 1/1. DeepVariant should give identical results when the same version is run on the same hardware platform (e.g. CPU-CPU). However, there can be floating point differences with very minor (almost never at the level of the variant call, but mostly at the GQ level) between compute platforms. Can you confirm that you and your collaborator ran the exact same version of DeepVariant on the same compute platform, or if there might be a difference (e.g. CPU vs Parabricks GPU). 2) **Why is the call here 0/1 given the pileup**. The IGV screenshot you've attached certainly looks 1/1, and all experts will assess it as a 1/1 from what is shown. We have observed that in rare circumstances, DeepVariant will occasionally call such positions as 0/1 or 0/0 or to decrease the confidence in the calls of certain positions. The signature for this seems to be when DeepVariant assesses a region to be likely to be a segmental duplication or structural variant. Signatures for that often involve a haplotype with dense variants while another haplotype is almost entirely reference, or a high amount of discordant read mapping or low MAPQ. Although your pileup does have a discordantly mapped read present, those patterns generally aren't present. For some reason, in both your and your collaborator's run, DeepVariant seems to think that this region is difficult to call. . In these cases, I'm always interested to see whether this could highlight a bug in DeepVariant, or if it reflects some learning of the model. Would there be any chance for you to share the small window of the BAM file here (maybe +/- 500 bp from the position). People in the team would be interested in whether this could detect any sort of edge case DeepVariant isn't handling well. Thanks,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/592
https://github.com/google/deepvariant/issues/592:1294,security,assess,assesses,1294,"GT from the two runs indicates that the neural network assesses the probability of 0/1 and 1/1 calls to be very similar (in the first entry they are rounded identically in the PL field). Your collaborator calls do have a small lean toward 1/1. DeepVariant should give identical results when the same version is run on the same hardware platform (e.g. CPU-CPU). However, there can be floating point differences with very minor (almost never at the level of the variant call, but mostly at the GQ level) between compute platforms. Can you confirm that you and your collaborator ran the exact same version of DeepVariant on the same compute platform, or if there might be a difference (e.g. CPU vs Parabricks GPU). 2) **Why is the call here 0/1 given the pileup**. The IGV screenshot you've attached certainly looks 1/1, and all experts will assess it as a 1/1 from what is shown. We have observed that in rare circumstances, DeepVariant will occasionally call such positions as 0/1 or 0/0 or to decrease the confidence in the calls of certain positions. The signature for this seems to be when DeepVariant assesses a region to be likely to be a segmental duplication or structural variant. Signatures for that often involve a haplotype with dense variants while another haplotype is almost entirely reference, or a high amount of discordant read mapping or low MAPQ. Although your pileup does have a discordantly mapped read present, those patterns generally aren't present. For some reason, in both your and your collaborator's run, DeepVariant seems to think that this region is difficult to call. . In these cases, I'm always interested to see whether this could highlight a bug in DeepVariant, or if it reflects some learning of the model. Would there be any chance for you to share the small window of the BAM file here (maybe +/- 500 bp from the position). People in the team would be interested in whether this could detect any sort of edge case DeepVariant isn't handling well. Thanks,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/592
https://github.com/google/deepvariant/issues/592:1378,security,Sign,Signatures,1378,"GT from the two runs indicates that the neural network assesses the probability of 0/1 and 1/1 calls to be very similar (in the first entry they are rounded identically in the PL field). Your collaborator calls do have a small lean toward 1/1. DeepVariant should give identical results when the same version is run on the same hardware platform (e.g. CPU-CPU). However, there can be floating point differences with very minor (almost never at the level of the variant call, but mostly at the GQ level) between compute platforms. Can you confirm that you and your collaborator ran the exact same version of DeepVariant on the same compute platform, or if there might be a difference (e.g. CPU vs Parabricks GPU). 2) **Why is the call here 0/1 given the pileup**. The IGV screenshot you've attached certainly looks 1/1, and all experts will assess it as a 1/1 from what is shown. We have observed that in rare circumstances, DeepVariant will occasionally call such positions as 0/1 or 0/0 or to decrease the confidence in the calls of certain positions. The signature for this seems to be when DeepVariant assesses a region to be likely to be a segmental duplication or structural variant. Signatures for that often involve a haplotype with dense variants while another haplotype is almost entirely reference, or a high amount of discordant read mapping or low MAPQ. Although your pileup does have a discordantly mapped read present, those patterns generally aren't present. For some reason, in both your and your collaborator's run, DeepVariant seems to think that this region is difficult to call. . In these cases, I'm always interested to see whether this could highlight a bug in DeepVariant, or if it reflects some learning of the model. Would there be any chance for you to share the small window of the BAM file here (maybe +/- 500 bp from the position). People in the team would be interested in whether this could detect any sort of edge case DeepVariant isn't handling well. Thanks,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/592
https://github.com/google/deepvariant/issues/592:1925,security,model,model,1925,"GT from the two runs indicates that the neural network assesses the probability of 0/1 and 1/1 calls to be very similar (in the first entry they are rounded identically in the PL field). Your collaborator calls do have a small lean toward 1/1. DeepVariant should give identical results when the same version is run on the same hardware platform (e.g. CPU-CPU). However, there can be floating point differences with very minor (almost never at the level of the variant call, but mostly at the GQ level) between compute platforms. Can you confirm that you and your collaborator ran the exact same version of DeepVariant on the same compute platform, or if there might be a difference (e.g. CPU vs Parabricks GPU). 2) **Why is the call here 0/1 given the pileup**. The IGV screenshot you've attached certainly looks 1/1, and all experts will assess it as a 1/1 from what is shown. We have observed that in rare circumstances, DeepVariant will occasionally call such positions as 0/1 or 0/0 or to decrease the confidence in the calls of certain positions. The signature for this seems to be when DeepVariant assesses a region to be likely to be a segmental duplication or structural variant. Signatures for that often involve a haplotype with dense variants while another haplotype is almost entirely reference, or a high amount of discordant read mapping or low MAPQ. Although your pileup does have a discordantly mapped read present, those patterns generally aren't present. For some reason, in both your and your collaborator's run, DeepVariant seems to think that this region is difficult to call. . In these cases, I'm always interested to see whether this could highlight a bug in DeepVariant, or if it reflects some learning of the model. Would there be any chance for you to share the small window of the BAM file here (maybe +/- 500 bp from the position). People in the team would be interested in whether this could detect any sort of edge case DeepVariant isn't handling well. Thanks,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/592
https://github.com/google/deepvariant/issues/592:2065,security,team,team,2065,"GT from the two runs indicates that the neural network assesses the probability of 0/1 and 1/1 calls to be very similar (in the first entry they are rounded identically in the PL field). Your collaborator calls do have a small lean toward 1/1. DeepVariant should give identical results when the same version is run on the same hardware platform (e.g. CPU-CPU). However, there can be floating point differences with very minor (almost never at the level of the variant call, but mostly at the GQ level) between compute platforms. Can you confirm that you and your collaborator ran the exact same version of DeepVariant on the same compute platform, or if there might be a difference (e.g. CPU vs Parabricks GPU). 2) **Why is the call here 0/1 given the pileup**. The IGV screenshot you've attached certainly looks 1/1, and all experts will assess it as a 1/1 from what is shown. We have observed that in rare circumstances, DeepVariant will occasionally call such positions as 0/1 or 0/0 or to decrease the confidence in the calls of certain positions. The signature for this seems to be when DeepVariant assesses a region to be likely to be a segmental duplication or structural variant. Signatures for that often involve a haplotype with dense variants while another haplotype is almost entirely reference, or a high amount of discordant read mapping or low MAPQ. Although your pileup does have a discordantly mapped read present, those patterns generally aren't present. For some reason, in both your and your collaborator's run, DeepVariant seems to think that this region is difficult to call. . In these cases, I'm always interested to see whether this could highlight a bug in DeepVariant, or if it reflects some learning of the model. Would there be any chance for you to share the small window of the BAM file here (maybe +/- 500 bp from the position). People in the team would be interested in whether this could detect any sort of edge case DeepVariant isn't handling well. Thanks,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/592
https://github.com/google/deepvariant/issues/592:2112,security,detect,detect,2112,"GT from the two runs indicates that the neural network assesses the probability of 0/1 and 1/1 calls to be very similar (in the first entry they are rounded identically in the PL field). Your collaborator calls do have a small lean toward 1/1. DeepVariant should give identical results when the same version is run on the same hardware platform (e.g. CPU-CPU). However, there can be floating point differences with very minor (almost never at the level of the variant call, but mostly at the GQ level) between compute platforms. Can you confirm that you and your collaborator ran the exact same version of DeepVariant on the same compute platform, or if there might be a difference (e.g. CPU vs Parabricks GPU). 2) **Why is the call here 0/1 given the pileup**. The IGV screenshot you've attached certainly looks 1/1, and all experts will assess it as a 1/1 from what is shown. We have observed that in rare circumstances, DeepVariant will occasionally call such positions as 0/1 or 0/0 or to decrease the confidence in the calls of certain positions. The signature for this seems to be when DeepVariant assesses a region to be likely to be a segmental duplication or structural variant. Signatures for that often involve a haplotype with dense variants while another haplotype is almost entirely reference, or a high amount of discordant read mapping or low MAPQ. Although your pileup does have a discordantly mapped read present, those patterns generally aren't present. For some reason, in both your and your collaborator's run, DeepVariant seems to think that this region is difficult to call. . In these cases, I'm always interested to see whether this could highlight a bug in DeepVariant, or if it reflects some learning of the model. Would there be any chance for you to share the small window of the BAM file here (maybe +/- 500 bp from the position). People in the team would be interested in whether this could detect any sort of edge case DeepVariant isn't handling well. Thanks,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/592
https://github.com/google/deepvariant/issues/592:1076,testability,observ,observed,1076,"hich are very informative. there are a few items to discuss:. 1) **Difference in genotype between runs**. The GT from the two runs indicates that the neural network assesses the probability of 0/1 and 1/1 calls to be very similar (in the first entry they are rounded identically in the PL field). Your collaborator calls do have a small lean toward 1/1. DeepVariant should give identical results when the same version is run on the same hardware platform (e.g. CPU-CPU). However, there can be floating point differences with very minor (almost never at the level of the variant call, but mostly at the GQ level) between compute platforms. Can you confirm that you and your collaborator ran the exact same version of DeepVariant on the same compute platform, or if there might be a difference (e.g. CPU vs Parabricks GPU). 2) **Why is the call here 0/1 given the pileup**. The IGV screenshot you've attached certainly looks 1/1, and all experts will assess it as a 1/1 from what is shown. We have observed that in rare circumstances, DeepVariant will occasionally call such positions as 0/1 or 0/0 or to decrease the confidence in the calls of certain positions. The signature for this seems to be when DeepVariant assesses a region to be likely to be a segmental duplication or structural variant. Signatures for that often involve a haplotype with dense variants while another haplotype is almost entirely reference, or a high amount of discordant read mapping or low MAPQ. Although your pileup does have a discordantly mapped read present, those patterns generally aren't present. For some reason, in both your and your collaborator's run, DeepVariant seems to think that this region is difficult to call. . In these cases, I'm always interested to see whether this could highlight a bug in DeepVariant, or if it reflects some learning of the model. Would there be any chance for you to share the small window of the BAM file here (maybe +/- 500 bp from the position). People in the team would be i",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/592
https://github.com/google/deepvariant/issues/592:211,usability,indicat,indicates,211,"Hi @li1ba . Thank you for the report, and for the VCF lines and IGV screenshot which are very informative. there are a few items to discuss:. 1) **Difference in genotype between runs**. The GT from the two runs indicates that the neural network assesses the probability of 0/1 and 1/1 calls to be very similar (in the first entry they are rounded identically in the PL field). Your collaborator calls do have a small lean toward 1/1. DeepVariant should give identical results when the same version is run on the same hardware platform (e.g. CPU-CPU). However, there can be floating point differences with very minor (almost never at the level of the variant call, but mostly at the GQ level) between compute platforms. Can you confirm that you and your collaborator ran the exact same version of DeepVariant on the same compute platform, or if there might be a difference (e.g. CPU vs Parabricks GPU). 2) **Why is the call here 0/1 given the pileup**. The IGV screenshot you've attached certainly looks 1/1, and all experts will assess it as a 1/1 from what is shown. We have observed that in rare circumstances, DeepVariant will occasionally call such positions as 0/1 or 0/0 or to decrease the confidence in the calls of certain positions. The signature for this seems to be when DeepVariant assesses a region to be likely to be a segmental duplication or structural variant. Signatures for that often involve a haplotype with dense variants while another haplotype is almost entirely reference, or a high amount of discordant read mapping or low MAPQ. Although your pileup does have a discordantly mapped read present, those patterns generally aren't present. For some reason, in both your and your collaborator's run, DeepVariant seems to think that this region is difficult to call. . In these cases, I'm always interested to see whether this could highlight a bug in DeepVariant, or if it reflects some learning of the model. Would there be any chance for you to share the small window of the B",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/592
https://github.com/google/deepvariant/issues/592:727,usability,confirm,confirm,727,"Hi @li1ba . Thank you for the report, and for the VCF lines and IGV screenshot which are very informative. there are a few items to discuss:. 1) **Difference in genotype between runs**. The GT from the two runs indicates that the neural network assesses the probability of 0/1 and 1/1 calls to be very similar (in the first entry they are rounded identically in the PL field). Your collaborator calls do have a small lean toward 1/1. DeepVariant should give identical results when the same version is run on the same hardware platform (e.g. CPU-CPU). However, there can be floating point differences with very minor (almost never at the level of the variant call, but mostly at the GQ level) between compute platforms. Can you confirm that you and your collaborator ran the exact same version of DeepVariant on the same compute platform, or if there might be a difference (e.g. CPU vs Parabricks GPU). 2) **Why is the call here 0/1 given the pileup**. The IGV screenshot you've attached certainly looks 1/1, and all experts will assess it as a 1/1 from what is shown. We have observed that in rare circumstances, DeepVariant will occasionally call such positions as 0/1 or 0/0 or to decrease the confidence in the calls of certain positions. The signature for this seems to be when DeepVariant assesses a region to be likely to be a segmental duplication or structural variant. Signatures for that often involve a haplotype with dense variants while another haplotype is almost entirely reference, or a high amount of discordant read mapping or low MAPQ. Although your pileup does have a discordantly mapped read present, those patterns generally aren't present. For some reason, in both your and your collaborator's run, DeepVariant seems to think that this region is difficult to call. . In these cases, I'm always interested to see whether this could highlight a bug in DeepVariant, or if it reflects some learning of the model. Would there be any chance for you to share the small window of the B",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/592
https://github.com/google/deepvariant/issues/592:1909,usability,learn,learning,1909,"GT from the two runs indicates that the neural network assesses the probability of 0/1 and 1/1 calls to be very similar (in the first entry they are rounded identically in the PL field). Your collaborator calls do have a small lean toward 1/1. DeepVariant should give identical results when the same version is run on the same hardware platform (e.g. CPU-CPU). However, there can be floating point differences with very minor (almost never at the level of the variant call, but mostly at the GQ level) between compute platforms. Can you confirm that you and your collaborator ran the exact same version of DeepVariant on the same compute platform, or if there might be a difference (e.g. CPU vs Parabricks GPU). 2) **Why is the call here 0/1 given the pileup**. The IGV screenshot you've attached certainly looks 1/1, and all experts will assess it as a 1/1 from what is shown. We have observed that in rare circumstances, DeepVariant will occasionally call such positions as 0/1 or 0/0 or to decrease the confidence in the calls of certain positions. The signature for this seems to be when DeepVariant assesses a region to be likely to be a segmental duplication or structural variant. Signatures for that often involve a haplotype with dense variants while another haplotype is almost entirely reference, or a high amount of discordant read mapping or low MAPQ. Although your pileup does have a discordantly mapped read present, those patterns generally aren't present. For some reason, in both your and your collaborator's run, DeepVariant seems to think that this region is difficult to call. . In these cases, I'm always interested to see whether this could highlight a bug in DeepVariant, or if it reflects some learning of the model. Would there be any chance for you to share the small window of the BAM file here (maybe +/- 500 bp from the position). People in the team would be interested in whether this could detect any sort of edge case DeepVariant isn't handling well. Thanks,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/592
https://github.com/google/deepvariant/issues/592:231,availability,sli,slightly,231,"Hi Andrew! Thanks for looking into this! 1. Our collaborators use Ubuntu18.04, CPU and the same DeepVariant version (1.2.0) with Docker. The results might not be the same because they didn't use the same .bam file, and they have a slightly different mapping command. 2. I have attached a small window of the BAM file, as you asked. I could also share the whole BAM file per e-mail if you would be interested, as we saw multiple such variants with VAF=1 and GT=0/1. [sample.zip](https://github.com/google/deepvariant/files/10132460/sample.zip).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/592
https://github.com/google/deepvariant/issues/592:108,deployability,version,version,108,"Hi Andrew! Thanks for looking into this! 1. Our collaborators use Ubuntu18.04, CPU and the same DeepVariant version (1.2.0) with Docker. The results might not be the same because they didn't use the same .bam file, and they have a slightly different mapping command. 2. I have attached a small window of the BAM file, as you asked. I could also share the whole BAM file per e-mail if you would be interested, as we saw multiple such variants with VAF=1 and GT=0/1. [sample.zip](https://github.com/google/deepvariant/files/10132460/sample.zip).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/592
https://github.com/google/deepvariant/issues/592:79,energy efficiency,CPU,CPU,79,"Hi Andrew! Thanks for looking into this! 1. Our collaborators use Ubuntu18.04, CPU and the same DeepVariant version (1.2.0) with Docker. The results might not be the same because they didn't use the same .bam file, and they have a slightly different mapping command. 2. I have attached a small window of the BAM file, as you asked. I could also share the whole BAM file per e-mail if you would be interested, as we saw multiple such variants with VAF=1 and GT=0/1. [sample.zip](https://github.com/google/deepvariant/files/10132460/sample.zip).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/592
https://github.com/google/deepvariant/issues/592:108,integrability,version,version,108,"Hi Andrew! Thanks for looking into this! 1. Our collaborators use Ubuntu18.04, CPU and the same DeepVariant version (1.2.0) with Docker. The results might not be the same because they didn't use the same .bam file, and they have a slightly different mapping command. 2. I have attached a small window of the BAM file, as you asked. I could also share the whole BAM file per e-mail if you would be interested, as we saw multiple such variants with VAF=1 and GT=0/1. [sample.zip](https://github.com/google/deepvariant/files/10132460/sample.zip).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/592
https://github.com/google/deepvariant/issues/592:345,interoperability,share,share,345,"Hi Andrew! Thanks for looking into this! 1. Our collaborators use Ubuntu18.04, CPU and the same DeepVariant version (1.2.0) with Docker. The results might not be the same because they didn't use the same .bam file, and they have a slightly different mapping command. 2. I have attached a small window of the BAM file, as you asked. I could also share the whole BAM file per e-mail if you would be interested, as we saw multiple such variants with VAF=1 and GT=0/1. [sample.zip](https://github.com/google/deepvariant/files/10132460/sample.zip).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/592
https://github.com/google/deepvariant/issues/592:108,modifiability,version,version,108,"Hi Andrew! Thanks for looking into this! 1. Our collaborators use Ubuntu18.04, CPU and the same DeepVariant version (1.2.0) with Docker. The results might not be the same because they didn't use the same .bam file, and they have a slightly different mapping command. 2. I have attached a small window of the BAM file, as you asked. I could also share the whole BAM file per e-mail if you would be interested, as we saw multiple such variants with VAF=1 and GT=0/1. [sample.zip](https://github.com/google/deepvariant/files/10132460/sample.zip).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/592
https://github.com/google/deepvariant/issues/592:79,performance,CPU,CPU,79,"Hi Andrew! Thanks for looking into this! 1. Our collaborators use Ubuntu18.04, CPU and the same DeepVariant version (1.2.0) with Docker. The results might not be the same because they didn't use the same .bam file, and they have a slightly different mapping command. 2. I have attached a small window of the BAM file, as you asked. I could also share the whole BAM file per e-mail if you would be interested, as we saw multiple such variants with VAF=1 and GT=0/1. [sample.zip](https://github.com/google/deepvariant/files/10132460/sample.zip).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/592
https://github.com/google/deepvariant/issues/592:231,reliability,sli,slightly,231,"Hi Andrew! Thanks for looking into this! 1. Our collaborators use Ubuntu18.04, CPU and the same DeepVariant version (1.2.0) with Docker. The results might not be the same because they didn't use the same .bam file, and they have a slightly different mapping command. 2. I have attached a small window of the BAM file, as you asked. I could also share the whole BAM file per e-mail if you would be interested, as we saw multiple such variants with VAF=1 and GT=0/1. [sample.zip](https://github.com/google/deepvariant/files/10132460/sample.zip).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/592
https://github.com/google/deepvariant/issues/592:258,usability,command,command,258,"Hi Andrew! Thanks for looking into this! 1. Our collaborators use Ubuntu18.04, CPU and the same DeepVariant version (1.2.0) with Docker. The results might not be the same because they didn't use the same .bam file, and they have a slightly different mapping command. 2. I have attached a small window of the BAM file, as you asked. I could also share the whole BAM file per e-mail if you would be interested, as we saw multiple such variants with VAF=1 and GT=0/1. [sample.zip](https://github.com/google/deepvariant/files/10132460/sample.zip).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/592
https://github.com/google/deepvariant/issues/592:58,availability,down,download,58,"Hi @li1ba . Thank you for the BAM file, I've been able to download it and look at the region in IGV. To my eye, I can't see the reason DeepVariant is calling 0/1 based on the pileup I see. We're going to run a few experiments with this and see if we can identify either something DeepVariant sees that we don't or if this is highlighting some sort of edge case or bug in the code.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/592
https://github.com/google/deepvariant/issues/592:254,security,ident,identify,254,"Hi @li1ba . Thank you for the BAM file, I've been able to download it and look at the region in IGV. To my eye, I can't see the reason DeepVariant is calling 0/1 based on the pileup I see. We're going to run a few experiments with this and see if we can identify either something DeepVariant sees that we don't or if this is highlighting some sort of edge case or bug in the code.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/592
https://github.com/google/deepvariant/issues/592:642,availability,replic,replicate,642,"Hi @li1ba . For the question about why does the model have a difficult time calling this 1/1 versus 0/1, we have done further investigation. First, it doesn't look like this is a bug in pre-processing or how the data is represented. It seems to be a property of the model. . Second, the property of the model seems to reflect something learned about exome sequencing as opposed to WGS. To determine that, we ran your snippet through both the WGS and WES models. The WGS model is able to confidently call this site as 1/1:. ```. chr2 24146804 . C T 34.1 PASS . GT:GQ:DP:AD:VAF:PL 1/1:34:162:0,162:1:34,45,0. ```. When we run the WES model, we replicate your finding (this is with the most recent DeepVariant v1.4, so there is a small difference in the GQ values. . ```. chr2 24146804 . C T 29.3 PASS . GT:GQ:DP:AD:VAF:PL 0/1:4:162:0,162:1:26,0,1. ```. This suggests that there is some aspect of exome sequencing that the DeepVariant WES model has learned makes this variant difficult to genotype, possibly because there is some signal that indicates only one allele is present. The reason for this might be some factor which isn't understood (at least by me). This is an interesting observation, but really understanding the reason for a 0/1 call at this position would probably need more investigation (for example, by going into the GIAB training labels for exome sequencing and seeing how often positions that look like this are 0/1 versus 1/1 and trying to understand why). With respect to why your collaborator has different results from you, it's very likely that the difference in mappers has a small effect on which reads are present and how many map discordantly. This small difference pushes the output for the variant on the edge of probabilities between 0/1 an 1/1 to the other side.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/592
https://github.com/google/deepvariant/issues/592:1182,deployability,observ,observation,1182,"Hi @li1ba . For the question about why does the model have a difficult time calling this 1/1 versus 0/1, we have done further investigation. First, it doesn't look like this is a bug in pre-processing or how the data is represented. It seems to be a property of the model. . Second, the property of the model seems to reflect something learned about exome sequencing as opposed to WGS. To determine that, we ran your snippet through both the WGS and WES models. The WGS model is able to confidently call this site as 1/1:. ```. chr2 24146804 . C T 34.1 PASS . GT:GQ:DP:AD:VAF:PL 1/1:34:162:0,162:1:34,45,0. ```. When we run the WES model, we replicate your finding (this is with the most recent DeepVariant v1.4, so there is a small difference in the GQ values. . ```. chr2 24146804 . C T 29.3 PASS . GT:GQ:DP:AD:VAF:PL 0/1:4:162:0,162:1:26,0,1. ```. This suggests that there is some aspect of exome sequencing that the DeepVariant WES model has learned makes this variant difficult to genotype, possibly because there is some signal that indicates only one allele is present. The reason for this might be some factor which isn't understood (at least by me). This is an interesting observation, but really understanding the reason for a 0/1 call at this position would probably need more investigation (for example, by going into the GIAB training labels for exome sequencing and seeing how often positions that look like this are 0/1 versus 1/1 and trying to understand why). With respect to why your collaborator has different results from you, it's very likely that the difference in mappers has a small effect on which reads are present and how many map discordantly. This small difference pushes the output for the variant on the edge of probabilities between 0/1 an 1/1 to the other side.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/592
https://github.com/google/deepvariant/issues/592:48,energy efficiency,model,model,48,"Hi @li1ba . For the question about why does the model have a difficult time calling this 1/1 versus 0/1, we have done further investigation. First, it doesn't look like this is a bug in pre-processing or how the data is represented. It seems to be a property of the model. . Second, the property of the model seems to reflect something learned about exome sequencing as opposed to WGS. To determine that, we ran your snippet through both the WGS and WES models. The WGS model is able to confidently call this site as 1/1:. ```. chr2 24146804 . C T 34.1 PASS . GT:GQ:DP:AD:VAF:PL 1/1:34:162:0,162:1:34,45,0. ```. When we run the WES model, we replicate your finding (this is with the most recent DeepVariant v1.4, so there is a small difference in the GQ values. . ```. chr2 24146804 . C T 29.3 PASS . GT:GQ:DP:AD:VAF:PL 0/1:4:162:0,162:1:26,0,1. ```. This suggests that there is some aspect of exome sequencing that the DeepVariant WES model has learned makes this variant difficult to genotype, possibly because there is some signal that indicates only one allele is present. The reason for this might be some factor which isn't understood (at least by me). This is an interesting observation, but really understanding the reason for a 0/1 call at this position would probably need more investigation (for example, by going into the GIAB training labels for exome sequencing and seeing how often positions that look like this are 0/1 versus 1/1 and trying to understand why). With respect to why your collaborator has different results from you, it's very likely that the difference in mappers has a small effect on which reads are present and how many map discordantly. This small difference pushes the output for the variant on the edge of probabilities between 0/1 an 1/1 to the other side.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/592
https://github.com/google/deepvariant/issues/592:266,energy efficiency,model,model,266,"Hi @li1ba . For the question about why does the model have a difficult time calling this 1/1 versus 0/1, we have done further investigation. First, it doesn't look like this is a bug in pre-processing or how the data is represented. It seems to be a property of the model. . Second, the property of the model seems to reflect something learned about exome sequencing as opposed to WGS. To determine that, we ran your snippet through both the WGS and WES models. The WGS model is able to confidently call this site as 1/1:. ```. chr2 24146804 . C T 34.1 PASS . GT:GQ:DP:AD:VAF:PL 1/1:34:162:0,162:1:34,45,0. ```. When we run the WES model, we replicate your finding (this is with the most recent DeepVariant v1.4, so there is a small difference in the GQ values. . ```. chr2 24146804 . C T 29.3 PASS . GT:GQ:DP:AD:VAF:PL 0/1:4:162:0,162:1:26,0,1. ```. This suggests that there is some aspect of exome sequencing that the DeepVariant WES model has learned makes this variant difficult to genotype, possibly because there is some signal that indicates only one allele is present. The reason for this might be some factor which isn't understood (at least by me). This is an interesting observation, but really understanding the reason for a 0/1 call at this position would probably need more investigation (for example, by going into the GIAB training labels for exome sequencing and seeing how often positions that look like this are 0/1 versus 1/1 and trying to understand why). With respect to why your collaborator has different results from you, it's very likely that the difference in mappers has a small effect on which reads are present and how many map discordantly. This small difference pushes the output for the variant on the edge of probabilities between 0/1 an 1/1 to the other side.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/592
https://github.com/google/deepvariant/issues/592:303,energy efficiency,model,model,303,"Hi @li1ba . For the question about why does the model have a difficult time calling this 1/1 versus 0/1, we have done further investigation. First, it doesn't look like this is a bug in pre-processing or how the data is represented. It seems to be a property of the model. . Second, the property of the model seems to reflect something learned about exome sequencing as opposed to WGS. To determine that, we ran your snippet through both the WGS and WES models. The WGS model is able to confidently call this site as 1/1:. ```. chr2 24146804 . C T 34.1 PASS . GT:GQ:DP:AD:VAF:PL 1/1:34:162:0,162:1:34,45,0. ```. When we run the WES model, we replicate your finding (this is with the most recent DeepVariant v1.4, so there is a small difference in the GQ values. . ```. chr2 24146804 . C T 29.3 PASS . GT:GQ:DP:AD:VAF:PL 0/1:4:162:0,162:1:26,0,1. ```. This suggests that there is some aspect of exome sequencing that the DeepVariant WES model has learned makes this variant difficult to genotype, possibly because there is some signal that indicates only one allele is present. The reason for this might be some factor which isn't understood (at least by me). This is an interesting observation, but really understanding the reason for a 0/1 call at this position would probably need more investigation (for example, by going into the GIAB training labels for exome sequencing and seeing how often positions that look like this are 0/1 versus 1/1 and trying to understand why). With respect to why your collaborator has different results from you, it's very likely that the difference in mappers has a small effect on which reads are present and how many map discordantly. This small difference pushes the output for the variant on the edge of probabilities between 0/1 an 1/1 to the other side.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/592
https://github.com/google/deepvariant/issues/592:454,energy efficiency,model,models,454,"Hi @li1ba . For the question about why does the model have a difficult time calling this 1/1 versus 0/1, we have done further investigation. First, it doesn't look like this is a bug in pre-processing or how the data is represented. It seems to be a property of the model. . Second, the property of the model seems to reflect something learned about exome sequencing as opposed to WGS. To determine that, we ran your snippet through both the WGS and WES models. The WGS model is able to confidently call this site as 1/1:. ```. chr2 24146804 . C T 34.1 PASS . GT:GQ:DP:AD:VAF:PL 1/1:34:162:0,162:1:34,45,0. ```. When we run the WES model, we replicate your finding (this is with the most recent DeepVariant v1.4, so there is a small difference in the GQ values. . ```. chr2 24146804 . C T 29.3 PASS . GT:GQ:DP:AD:VAF:PL 0/1:4:162:0,162:1:26,0,1. ```. This suggests that there is some aspect of exome sequencing that the DeepVariant WES model has learned makes this variant difficult to genotype, possibly because there is some signal that indicates only one allele is present. The reason for this might be some factor which isn't understood (at least by me). This is an interesting observation, but really understanding the reason for a 0/1 call at this position would probably need more investigation (for example, by going into the GIAB training labels for exome sequencing and seeing how often positions that look like this are 0/1 versus 1/1 and trying to understand why). With respect to why your collaborator has different results from you, it's very likely that the difference in mappers has a small effect on which reads are present and how many map discordantly. This small difference pushes the output for the variant on the edge of probabilities between 0/1 an 1/1 to the other side.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/592
https://github.com/google/deepvariant/issues/592:470,energy efficiency,model,model,470,"Hi @li1ba . For the question about why does the model have a difficult time calling this 1/1 versus 0/1, we have done further investigation. First, it doesn't look like this is a bug in pre-processing or how the data is represented. It seems to be a property of the model. . Second, the property of the model seems to reflect something learned about exome sequencing as opposed to WGS. To determine that, we ran your snippet through both the WGS and WES models. The WGS model is able to confidently call this site as 1/1:. ```. chr2 24146804 . C T 34.1 PASS . GT:GQ:DP:AD:VAF:PL 1/1:34:162:0,162:1:34,45,0. ```. When we run the WES model, we replicate your finding (this is with the most recent DeepVariant v1.4, so there is a small difference in the GQ values. . ```. chr2 24146804 . C T 29.3 PASS . GT:GQ:DP:AD:VAF:PL 0/1:4:162:0,162:1:26,0,1. ```. This suggests that there is some aspect of exome sequencing that the DeepVariant WES model has learned makes this variant difficult to genotype, possibly because there is some signal that indicates only one allele is present. The reason for this might be some factor which isn't understood (at least by me). This is an interesting observation, but really understanding the reason for a 0/1 call at this position would probably need more investigation (for example, by going into the GIAB training labels for exome sequencing and seeing how often positions that look like this are 0/1 versus 1/1 and trying to understand why). With respect to why your collaborator has different results from you, it's very likely that the difference in mappers has a small effect on which reads are present and how many map discordantly. This small difference pushes the output for the variant on the edge of probabilities between 0/1 an 1/1 to the other side.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/592
https://github.com/google/deepvariant/issues/592:632,energy efficiency,model,model,632,"Hi @li1ba . For the question about why does the model have a difficult time calling this 1/1 versus 0/1, we have done further investigation. First, it doesn't look like this is a bug in pre-processing or how the data is represented. It seems to be a property of the model. . Second, the property of the model seems to reflect something learned about exome sequencing as opposed to WGS. To determine that, we ran your snippet through both the WGS and WES models. The WGS model is able to confidently call this site as 1/1:. ```. chr2 24146804 . C T 34.1 PASS . GT:GQ:DP:AD:VAF:PL 1/1:34:162:0,162:1:34,45,0. ```. When we run the WES model, we replicate your finding (this is with the most recent DeepVariant v1.4, so there is a small difference in the GQ values. . ```. chr2 24146804 . C T 29.3 PASS . GT:GQ:DP:AD:VAF:PL 0/1:4:162:0,162:1:26,0,1. ```. This suggests that there is some aspect of exome sequencing that the DeepVariant WES model has learned makes this variant difficult to genotype, possibly because there is some signal that indicates only one allele is present. The reason for this might be some factor which isn't understood (at least by me). This is an interesting observation, but really understanding the reason for a 0/1 call at this position would probably need more investigation (for example, by going into the GIAB training labels for exome sequencing and seeing how often positions that look like this are 0/1 versus 1/1 and trying to understand why). With respect to why your collaborator has different results from you, it's very likely that the difference in mappers has a small effect on which reads are present and how many map discordantly. This small difference pushes the output for the variant on the edge of probabilities between 0/1 an 1/1 to the other side.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/592
https://github.com/google/deepvariant/issues/592:936,energy efficiency,model,model,936,"Hi @li1ba . For the question about why does the model have a difficult time calling this 1/1 versus 0/1, we have done further investigation. First, it doesn't look like this is a bug in pre-processing or how the data is represented. It seems to be a property of the model. . Second, the property of the model seems to reflect something learned about exome sequencing as opposed to WGS. To determine that, we ran your snippet through both the WGS and WES models. The WGS model is able to confidently call this site as 1/1:. ```. chr2 24146804 . C T 34.1 PASS . GT:GQ:DP:AD:VAF:PL 1/1:34:162:0,162:1:34,45,0. ```. When we run the WES model, we replicate your finding (this is with the most recent DeepVariant v1.4, so there is a small difference in the GQ values. . ```. chr2 24146804 . C T 29.3 PASS . GT:GQ:DP:AD:VAF:PL 0/1:4:162:0,162:1:26,0,1. ```. This suggests that there is some aspect of exome sequencing that the DeepVariant WES model has learned makes this variant difficult to genotype, possibly because there is some signal that indicates only one allele is present. The reason for this might be some factor which isn't understood (at least by me). This is an interesting observation, but really understanding the reason for a 0/1 call at this position would probably need more investigation (for example, by going into the GIAB training labels for exome sequencing and seeing how often positions that look like this are 0/1 versus 1/1 and trying to understand why). With respect to why your collaborator has different results from you, it's very likely that the difference in mappers has a small effect on which reads are present and how many map discordantly. This small difference pushes the output for the variant on the edge of probabilities between 0/1 an 1/1 to the other side.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/592
https://github.com/google/deepvariant/issues/592:71,performance,time,time,71,"Hi @li1ba . For the question about why does the model have a difficult time calling this 1/1 versus 0/1, we have done further investigation. First, it doesn't look like this is a bug in pre-processing or how the data is represented. It seems to be a property of the model. . Second, the property of the model seems to reflect something learned about exome sequencing as opposed to WGS. To determine that, we ran your snippet through both the WGS and WES models. The WGS model is able to confidently call this site as 1/1:. ```. chr2 24146804 . C T 34.1 PASS . GT:GQ:DP:AD:VAF:PL 1/1:34:162:0,162:1:34,45,0. ```. When we run the WES model, we replicate your finding (this is with the most recent DeepVariant v1.4, so there is a small difference in the GQ values. . ```. chr2 24146804 . C T 29.3 PASS . GT:GQ:DP:AD:VAF:PL 0/1:4:162:0,162:1:26,0,1. ```. This suggests that there is some aspect of exome sequencing that the DeepVariant WES model has learned makes this variant difficult to genotype, possibly because there is some signal that indicates only one allele is present. The reason for this might be some factor which isn't understood (at least by me). This is an interesting observation, but really understanding the reason for a 0/1 call at this position would probably need more investigation (for example, by going into the GIAB training labels for exome sequencing and seeing how often positions that look like this are 0/1 versus 1/1 and trying to understand why). With respect to why your collaborator has different results from you, it's very likely that the difference in mappers has a small effect on which reads are present and how many map discordantly. This small difference pushes the output for the variant on the edge of probabilities between 0/1 an 1/1 to the other side.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/592
https://github.com/google/deepvariant/issues/592:39,reliability,doe,does,39,"Hi @li1ba . For the question about why does the model have a difficult time calling this 1/1 versus 0/1, we have done further investigation. First, it doesn't look like this is a bug in pre-processing or how the data is represented. It seems to be a property of the model. . Second, the property of the model seems to reflect something learned about exome sequencing as opposed to WGS. To determine that, we ran your snippet through both the WGS and WES models. The WGS model is able to confidently call this site as 1/1:. ```. chr2 24146804 . C T 34.1 PASS . GT:GQ:DP:AD:VAF:PL 1/1:34:162:0,162:1:34,45,0. ```. When we run the WES model, we replicate your finding (this is with the most recent DeepVariant v1.4, so there is a small difference in the GQ values. . ```. chr2 24146804 . C T 29.3 PASS . GT:GQ:DP:AD:VAF:PL 0/1:4:162:0,162:1:26,0,1. ```. This suggests that there is some aspect of exome sequencing that the DeepVariant WES model has learned makes this variant difficult to genotype, possibly because there is some signal that indicates only one allele is present. The reason for this might be some factor which isn't understood (at least by me). This is an interesting observation, but really understanding the reason for a 0/1 call at this position would probably need more investigation (for example, by going into the GIAB training labels for exome sequencing and seeing how often positions that look like this are 0/1 versus 1/1 and trying to understand why). With respect to why your collaborator has different results from you, it's very likely that the difference in mappers has a small effect on which reads are present and how many map discordantly. This small difference pushes the output for the variant on the edge of probabilities between 0/1 an 1/1 to the other side.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/592
https://github.com/google/deepvariant/issues/592:151,reliability,doe,doesn,151,"Hi @li1ba . For the question about why does the model have a difficult time calling this 1/1 versus 0/1, we have done further investigation. First, it doesn't look like this is a bug in pre-processing or how the data is represented. It seems to be a property of the model. . Second, the property of the model seems to reflect something learned about exome sequencing as opposed to WGS. To determine that, we ran your snippet through both the WGS and WES models. The WGS model is able to confidently call this site as 1/1:. ```. chr2 24146804 . C T 34.1 PASS . GT:GQ:DP:AD:VAF:PL 1/1:34:162:0,162:1:34,45,0. ```. When we run the WES model, we replicate your finding (this is with the most recent DeepVariant v1.4, so there is a small difference in the GQ values. . ```. chr2 24146804 . C T 29.3 PASS . GT:GQ:DP:AD:VAF:PL 0/1:4:162:0,162:1:26,0,1. ```. This suggests that there is some aspect of exome sequencing that the DeepVariant WES model has learned makes this variant difficult to genotype, possibly because there is some signal that indicates only one allele is present. The reason for this might be some factor which isn't understood (at least by me). This is an interesting observation, but really understanding the reason for a 0/1 call at this position would probably need more investigation (for example, by going into the GIAB training labels for exome sequencing and seeing how often positions that look like this are 0/1 versus 1/1 and trying to understand why). With respect to why your collaborator has different results from you, it's very likely that the difference in mappers has a small effect on which reads are present and how many map discordantly. This small difference pushes the output for the variant on the edge of probabilities between 0/1 an 1/1 to the other side.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/592
https://github.com/google/deepvariant/issues/592:48,security,model,model,48,"Hi @li1ba . For the question about why does the model have a difficult time calling this 1/1 versus 0/1, we have done further investigation. First, it doesn't look like this is a bug in pre-processing or how the data is represented. It seems to be a property of the model. . Second, the property of the model seems to reflect something learned about exome sequencing as opposed to WGS. To determine that, we ran your snippet through both the WGS and WES models. The WGS model is able to confidently call this site as 1/1:. ```. chr2 24146804 . C T 34.1 PASS . GT:GQ:DP:AD:VAF:PL 1/1:34:162:0,162:1:34,45,0. ```. When we run the WES model, we replicate your finding (this is with the most recent DeepVariant v1.4, so there is a small difference in the GQ values. . ```. chr2 24146804 . C T 29.3 PASS . GT:GQ:DP:AD:VAF:PL 0/1:4:162:0,162:1:26,0,1. ```. This suggests that there is some aspect of exome sequencing that the DeepVariant WES model has learned makes this variant difficult to genotype, possibly because there is some signal that indicates only one allele is present. The reason for this might be some factor which isn't understood (at least by me). This is an interesting observation, but really understanding the reason for a 0/1 call at this position would probably need more investigation (for example, by going into the GIAB training labels for exome sequencing and seeing how often positions that look like this are 0/1 versus 1/1 and trying to understand why). With respect to why your collaborator has different results from you, it's very likely that the difference in mappers has a small effect on which reads are present and how many map discordantly. This small difference pushes the output for the variant on the edge of probabilities between 0/1 an 1/1 to the other side.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/592
https://github.com/google/deepvariant/issues/592:266,security,model,model,266,"Hi @li1ba . For the question about why does the model have a difficult time calling this 1/1 versus 0/1, we have done further investigation. First, it doesn't look like this is a bug in pre-processing or how the data is represented. It seems to be a property of the model. . Second, the property of the model seems to reflect something learned about exome sequencing as opposed to WGS. To determine that, we ran your snippet through both the WGS and WES models. The WGS model is able to confidently call this site as 1/1:. ```. chr2 24146804 . C T 34.1 PASS . GT:GQ:DP:AD:VAF:PL 1/1:34:162:0,162:1:34,45,0. ```. When we run the WES model, we replicate your finding (this is with the most recent DeepVariant v1.4, so there is a small difference in the GQ values. . ```. chr2 24146804 . C T 29.3 PASS . GT:GQ:DP:AD:VAF:PL 0/1:4:162:0,162:1:26,0,1. ```. This suggests that there is some aspect of exome sequencing that the DeepVariant WES model has learned makes this variant difficult to genotype, possibly because there is some signal that indicates only one allele is present. The reason for this might be some factor which isn't understood (at least by me). This is an interesting observation, but really understanding the reason for a 0/1 call at this position would probably need more investigation (for example, by going into the GIAB training labels for exome sequencing and seeing how often positions that look like this are 0/1 versus 1/1 and trying to understand why). With respect to why your collaborator has different results from you, it's very likely that the difference in mappers has a small effect on which reads are present and how many map discordantly. This small difference pushes the output for the variant on the edge of probabilities between 0/1 an 1/1 to the other side.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/592
https://github.com/google/deepvariant/issues/592:303,security,model,model,303,"Hi @li1ba . For the question about why does the model have a difficult time calling this 1/1 versus 0/1, we have done further investigation. First, it doesn't look like this is a bug in pre-processing or how the data is represented. It seems to be a property of the model. . Second, the property of the model seems to reflect something learned about exome sequencing as opposed to WGS. To determine that, we ran your snippet through both the WGS and WES models. The WGS model is able to confidently call this site as 1/1:. ```. chr2 24146804 . C T 34.1 PASS . GT:GQ:DP:AD:VAF:PL 1/1:34:162:0,162:1:34,45,0. ```. When we run the WES model, we replicate your finding (this is with the most recent DeepVariant v1.4, so there is a small difference in the GQ values. . ```. chr2 24146804 . C T 29.3 PASS . GT:GQ:DP:AD:VAF:PL 0/1:4:162:0,162:1:26,0,1. ```. This suggests that there is some aspect of exome sequencing that the DeepVariant WES model has learned makes this variant difficult to genotype, possibly because there is some signal that indicates only one allele is present. The reason for this might be some factor which isn't understood (at least by me). This is an interesting observation, but really understanding the reason for a 0/1 call at this position would probably need more investigation (for example, by going into the GIAB training labels for exome sequencing and seeing how often positions that look like this are 0/1 versus 1/1 and trying to understand why). With respect to why your collaborator has different results from you, it's very likely that the difference in mappers has a small effect on which reads are present and how many map discordantly. This small difference pushes the output for the variant on the edge of probabilities between 0/1 an 1/1 to the other side.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/592
https://github.com/google/deepvariant/issues/592:454,security,model,models,454,"Hi @li1ba . For the question about why does the model have a difficult time calling this 1/1 versus 0/1, we have done further investigation. First, it doesn't look like this is a bug in pre-processing or how the data is represented. It seems to be a property of the model. . Second, the property of the model seems to reflect something learned about exome sequencing as opposed to WGS. To determine that, we ran your snippet through both the WGS and WES models. The WGS model is able to confidently call this site as 1/1:. ```. chr2 24146804 . C T 34.1 PASS . GT:GQ:DP:AD:VAF:PL 1/1:34:162:0,162:1:34,45,0. ```. When we run the WES model, we replicate your finding (this is with the most recent DeepVariant v1.4, so there is a small difference in the GQ values. . ```. chr2 24146804 . C T 29.3 PASS . GT:GQ:DP:AD:VAF:PL 0/1:4:162:0,162:1:26,0,1. ```. This suggests that there is some aspect of exome sequencing that the DeepVariant WES model has learned makes this variant difficult to genotype, possibly because there is some signal that indicates only one allele is present. The reason for this might be some factor which isn't understood (at least by me). This is an interesting observation, but really understanding the reason for a 0/1 call at this position would probably need more investigation (for example, by going into the GIAB training labels for exome sequencing and seeing how often positions that look like this are 0/1 versus 1/1 and trying to understand why). With respect to why your collaborator has different results from you, it's very likely that the difference in mappers has a small effect on which reads are present and how many map discordantly. This small difference pushes the output for the variant on the edge of probabilities between 0/1 an 1/1 to the other side.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/592
https://github.com/google/deepvariant/issues/592:470,security,model,model,470,"Hi @li1ba . For the question about why does the model have a difficult time calling this 1/1 versus 0/1, we have done further investigation. First, it doesn't look like this is a bug in pre-processing or how the data is represented. It seems to be a property of the model. . Second, the property of the model seems to reflect something learned about exome sequencing as opposed to WGS. To determine that, we ran your snippet through both the WGS and WES models. The WGS model is able to confidently call this site as 1/1:. ```. chr2 24146804 . C T 34.1 PASS . GT:GQ:DP:AD:VAF:PL 1/1:34:162:0,162:1:34,45,0. ```. When we run the WES model, we replicate your finding (this is with the most recent DeepVariant v1.4, so there is a small difference in the GQ values. . ```. chr2 24146804 . C T 29.3 PASS . GT:GQ:DP:AD:VAF:PL 0/1:4:162:0,162:1:26,0,1. ```. This suggests that there is some aspect of exome sequencing that the DeepVariant WES model has learned makes this variant difficult to genotype, possibly because there is some signal that indicates only one allele is present. The reason for this might be some factor which isn't understood (at least by me). This is an interesting observation, but really understanding the reason for a 0/1 call at this position would probably need more investigation (for example, by going into the GIAB training labels for exome sequencing and seeing how often positions that look like this are 0/1 versus 1/1 and trying to understand why). With respect to why your collaborator has different results from you, it's very likely that the difference in mappers has a small effect on which reads are present and how many map discordantly. This small difference pushes the output for the variant on the edge of probabilities between 0/1 an 1/1 to the other side.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/592
https://github.com/google/deepvariant/issues/592:632,security,model,model,632,"Hi @li1ba . For the question about why does the model have a difficult time calling this 1/1 versus 0/1, we have done further investigation. First, it doesn't look like this is a bug in pre-processing or how the data is represented. It seems to be a property of the model. . Second, the property of the model seems to reflect something learned about exome sequencing as opposed to WGS. To determine that, we ran your snippet through both the WGS and WES models. The WGS model is able to confidently call this site as 1/1:. ```. chr2 24146804 . C T 34.1 PASS . GT:GQ:DP:AD:VAF:PL 1/1:34:162:0,162:1:34,45,0. ```. When we run the WES model, we replicate your finding (this is with the most recent DeepVariant v1.4, so there is a small difference in the GQ values. . ```. chr2 24146804 . C T 29.3 PASS . GT:GQ:DP:AD:VAF:PL 0/1:4:162:0,162:1:26,0,1. ```. This suggests that there is some aspect of exome sequencing that the DeepVariant WES model has learned makes this variant difficult to genotype, possibly because there is some signal that indicates only one allele is present. The reason for this might be some factor which isn't understood (at least by me). This is an interesting observation, but really understanding the reason for a 0/1 call at this position would probably need more investigation (for example, by going into the GIAB training labels for exome sequencing and seeing how often positions that look like this are 0/1 versus 1/1 and trying to understand why). With respect to why your collaborator has different results from you, it's very likely that the difference in mappers has a small effect on which reads are present and how many map discordantly. This small difference pushes the output for the variant on the edge of probabilities between 0/1 an 1/1 to the other side.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/592
https://github.com/google/deepvariant/issues/592:936,security,model,model,936,"Hi @li1ba . For the question about why does the model have a difficult time calling this 1/1 versus 0/1, we have done further investigation. First, it doesn't look like this is a bug in pre-processing or how the data is represented. It seems to be a property of the model. . Second, the property of the model seems to reflect something learned about exome sequencing as opposed to WGS. To determine that, we ran your snippet through both the WGS and WES models. The WGS model is able to confidently call this site as 1/1:. ```. chr2 24146804 . C T 34.1 PASS . GT:GQ:DP:AD:VAF:PL 1/1:34:162:0,162:1:34,45,0. ```. When we run the WES model, we replicate your finding (this is with the most recent DeepVariant v1.4, so there is a small difference in the GQ values. . ```. chr2 24146804 . C T 29.3 PASS . GT:GQ:DP:AD:VAF:PL 0/1:4:162:0,162:1:26,0,1. ```. This suggests that there is some aspect of exome sequencing that the DeepVariant WES model has learned makes this variant difficult to genotype, possibly because there is some signal that indicates only one allele is present. The reason for this might be some factor which isn't understood (at least by me). This is an interesting observation, but really understanding the reason for a 0/1 call at this position would probably need more investigation (for example, by going into the GIAB training labels for exome sequencing and seeing how often positions that look like this are 0/1 versus 1/1 and trying to understand why). With respect to why your collaborator has different results from you, it's very likely that the difference in mappers has a small effect on which reads are present and how many map discordantly. This small difference pushes the output for the variant on the edge of probabilities between 0/1 an 1/1 to the other side.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/592
https://github.com/google/deepvariant/issues/592:1027,security,sign,signal,1027,"Hi @li1ba . For the question about why does the model have a difficult time calling this 1/1 versus 0/1, we have done further investigation. First, it doesn't look like this is a bug in pre-processing or how the data is represented. It seems to be a property of the model. . Second, the property of the model seems to reflect something learned about exome sequencing as opposed to WGS. To determine that, we ran your snippet through both the WGS and WES models. The WGS model is able to confidently call this site as 1/1:. ```. chr2 24146804 . C T 34.1 PASS . GT:GQ:DP:AD:VAF:PL 1/1:34:162:0,162:1:34,45,0. ```. When we run the WES model, we replicate your finding (this is with the most recent DeepVariant v1.4, so there is a small difference in the GQ values. . ```. chr2 24146804 . C T 29.3 PASS . GT:GQ:DP:AD:VAF:PL 0/1:4:162:0,162:1:26,0,1. ```. This suggests that there is some aspect of exome sequencing that the DeepVariant WES model has learned makes this variant difficult to genotype, possibly because there is some signal that indicates only one allele is present. The reason for this might be some factor which isn't understood (at least by me). This is an interesting observation, but really understanding the reason for a 0/1 call at this position would probably need more investigation (for example, by going into the GIAB training labels for exome sequencing and seeing how often positions that look like this are 0/1 versus 1/1 and trying to understand why). With respect to why your collaborator has different results from you, it's very likely that the difference in mappers has a small effect on which reads are present and how many map discordantly. This small difference pushes the output for the variant on the edge of probabilities between 0/1 an 1/1 to the other side.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/592
https://github.com/google/deepvariant/issues/592:1182,testability,observ,observation,1182,"Hi @li1ba . For the question about why does the model have a difficult time calling this 1/1 versus 0/1, we have done further investigation. First, it doesn't look like this is a bug in pre-processing or how the data is represented. It seems to be a property of the model. . Second, the property of the model seems to reflect something learned about exome sequencing as opposed to WGS. To determine that, we ran your snippet through both the WGS and WES models. The WGS model is able to confidently call this site as 1/1:. ```. chr2 24146804 . C T 34.1 PASS . GT:GQ:DP:AD:VAF:PL 1/1:34:162:0,162:1:34,45,0. ```. When we run the WES model, we replicate your finding (this is with the most recent DeepVariant v1.4, so there is a small difference in the GQ values. . ```. chr2 24146804 . C T 29.3 PASS . GT:GQ:DP:AD:VAF:PL 0/1:4:162:0,162:1:26,0,1. ```. This suggests that there is some aspect of exome sequencing that the DeepVariant WES model has learned makes this variant difficult to genotype, possibly because there is some signal that indicates only one allele is present. The reason for this might be some factor which isn't understood (at least by me). This is an interesting observation, but really understanding the reason for a 0/1 call at this position would probably need more investigation (for example, by going into the GIAB training labels for exome sequencing and seeing how often positions that look like this are 0/1 versus 1/1 and trying to understand why). With respect to why your collaborator has different results from you, it's very likely that the difference in mappers has a small effect on which reads are present and how many map discordantly. This small difference pushes the output for the variant on the edge of probabilities between 0/1 an 1/1 to the other side.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/592
https://github.com/google/deepvariant/issues/592:1206,testability,understand,understanding,1206,"Hi @li1ba . For the question about why does the model have a difficult time calling this 1/1 versus 0/1, we have done further investigation. First, it doesn't look like this is a bug in pre-processing or how the data is represented. It seems to be a property of the model. . Second, the property of the model seems to reflect something learned about exome sequencing as opposed to WGS. To determine that, we ran your snippet through both the WGS and WES models. The WGS model is able to confidently call this site as 1/1:. ```. chr2 24146804 . C T 34.1 PASS . GT:GQ:DP:AD:VAF:PL 1/1:34:162:0,162:1:34,45,0. ```. When we run the WES model, we replicate your finding (this is with the most recent DeepVariant v1.4, so there is a small difference in the GQ values. . ```. chr2 24146804 . C T 29.3 PASS . GT:GQ:DP:AD:VAF:PL 0/1:4:162:0,162:1:26,0,1. ```. This suggests that there is some aspect of exome sequencing that the DeepVariant WES model has learned makes this variant difficult to genotype, possibly because there is some signal that indicates only one allele is present. The reason for this might be some factor which isn't understood (at least by me). This is an interesting observation, but really understanding the reason for a 0/1 call at this position would probably need more investigation (for example, by going into the GIAB training labels for exome sequencing and seeing how often positions that look like this are 0/1 versus 1/1 and trying to understand why). With respect to why your collaborator has different results from you, it's very likely that the difference in mappers has a small effect on which reads are present and how many map discordantly. This small difference pushes the output for the variant on the edge of probabilities between 0/1 an 1/1 to the other side.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/592
https://github.com/google/deepvariant/issues/592:1460,testability,understand,understand,1460,"Hi @li1ba . For the question about why does the model have a difficult time calling this 1/1 versus 0/1, we have done further investigation. First, it doesn't look like this is a bug in pre-processing or how the data is represented. It seems to be a property of the model. . Second, the property of the model seems to reflect something learned about exome sequencing as opposed to WGS. To determine that, we ran your snippet through both the WGS and WES models. The WGS model is able to confidently call this site as 1/1:. ```. chr2 24146804 . C T 34.1 PASS . GT:GQ:DP:AD:VAF:PL 1/1:34:162:0,162:1:34,45,0. ```. When we run the WES model, we replicate your finding (this is with the most recent DeepVariant v1.4, so there is a small difference in the GQ values. . ```. chr2 24146804 . C T 29.3 PASS . GT:GQ:DP:AD:VAF:PL 0/1:4:162:0,162:1:26,0,1. ```. This suggests that there is some aspect of exome sequencing that the DeepVariant WES model has learned makes this variant difficult to genotype, possibly because there is some signal that indicates only one allele is present. The reason for this might be some factor which isn't understood (at least by me). This is an interesting observation, but really understanding the reason for a 0/1 call at this position would probably need more investigation (for example, by going into the GIAB training labels for exome sequencing and seeing how often positions that look like this are 0/1 versus 1/1 and trying to understand why). With respect to why your collaborator has different results from you, it's very likely that the difference in mappers has a small effect on which reads are present and how many map discordantly. This small difference pushes the output for the variant on the edge of probabilities between 0/1 an 1/1 to the other side.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/592
https://github.com/google/deepvariant/issues/592:336,usability,learn,learned,336,"Hi @li1ba . For the question about why does the model have a difficult time calling this 1/1 versus 0/1, we have done further investigation. First, it doesn't look like this is a bug in pre-processing or how the data is represented. It seems to be a property of the model. . Second, the property of the model seems to reflect something learned about exome sequencing as opposed to WGS. To determine that, we ran your snippet through both the WGS and WES models. The WGS model is able to confidently call this site as 1/1:. ```. chr2 24146804 . C T 34.1 PASS . GT:GQ:DP:AD:VAF:PL 1/1:34:162:0,162:1:34,45,0. ```. When we run the WES model, we replicate your finding (this is with the most recent DeepVariant v1.4, so there is a small difference in the GQ values. . ```. chr2 24146804 . C T 29.3 PASS . GT:GQ:DP:AD:VAF:PL 0/1:4:162:0,162:1:26,0,1. ```. This suggests that there is some aspect of exome sequencing that the DeepVariant WES model has learned makes this variant difficult to genotype, possibly because there is some signal that indicates only one allele is present. The reason for this might be some factor which isn't understood (at least by me). This is an interesting observation, but really understanding the reason for a 0/1 call at this position would probably need more investigation (for example, by going into the GIAB training labels for exome sequencing and seeing how often positions that look like this are 0/1 versus 1/1 and trying to understand why). With respect to why your collaborator has different results from you, it's very likely that the difference in mappers has a small effect on which reads are present and how many map discordantly. This small difference pushes the output for the variant on the edge of probabilities between 0/1 an 1/1 to the other side.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/592
https://github.com/google/deepvariant/issues/592:946,usability,learn,learned,946,"Hi @li1ba . For the question about why does the model have a difficult time calling this 1/1 versus 0/1, we have done further investigation. First, it doesn't look like this is a bug in pre-processing or how the data is represented. It seems to be a property of the model. . Second, the property of the model seems to reflect something learned about exome sequencing as opposed to WGS. To determine that, we ran your snippet through both the WGS and WES models. The WGS model is able to confidently call this site as 1/1:. ```. chr2 24146804 . C T 34.1 PASS . GT:GQ:DP:AD:VAF:PL 1/1:34:162:0,162:1:34,45,0. ```. When we run the WES model, we replicate your finding (this is with the most recent DeepVariant v1.4, so there is a small difference in the GQ values. . ```. chr2 24146804 . C T 29.3 PASS . GT:GQ:DP:AD:VAF:PL 0/1:4:162:0,162:1:26,0,1. ```. This suggests that there is some aspect of exome sequencing that the DeepVariant WES model has learned makes this variant difficult to genotype, possibly because there is some signal that indicates only one allele is present. The reason for this might be some factor which isn't understood (at least by me). This is an interesting observation, but really understanding the reason for a 0/1 call at this position would probably need more investigation (for example, by going into the GIAB training labels for exome sequencing and seeing how often positions that look like this are 0/1 versus 1/1 and trying to understand why). With respect to why your collaborator has different results from you, it's very likely that the difference in mappers has a small effect on which reads are present and how many map discordantly. This small difference pushes the output for the variant on the edge of probabilities between 0/1 an 1/1 to the other side.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/592
https://github.com/google/deepvariant/issues/592:1039,usability,indicat,indicates,1039,"Hi @li1ba . For the question about why does the model have a difficult time calling this 1/1 versus 0/1, we have done further investigation. First, it doesn't look like this is a bug in pre-processing or how the data is represented. It seems to be a property of the model. . Second, the property of the model seems to reflect something learned about exome sequencing as opposed to WGS. To determine that, we ran your snippet through both the WGS and WES models. The WGS model is able to confidently call this site as 1/1:. ```. chr2 24146804 . C T 34.1 PASS . GT:GQ:DP:AD:VAF:PL 1/1:34:162:0,162:1:34,45,0. ```. When we run the WES model, we replicate your finding (this is with the most recent DeepVariant v1.4, so there is a small difference in the GQ values. . ```. chr2 24146804 . C T 29.3 PASS . GT:GQ:DP:AD:VAF:PL 0/1:4:162:0,162:1:26,0,1. ```. This suggests that there is some aspect of exome sequencing that the DeepVariant WES model has learned makes this variant difficult to genotype, possibly because there is some signal that indicates only one allele is present. The reason for this might be some factor which isn't understood (at least by me). This is an interesting observation, but really understanding the reason for a 0/1 call at this position would probably need more investigation (for example, by going into the GIAB training labels for exome sequencing and seeing how often positions that look like this are 0/1 versus 1/1 and trying to understand why). With respect to why your collaborator has different results from you, it's very likely that the difference in mappers has a small effect on which reads are present and how many map discordantly. This small difference pushes the output for the variant on the edge of probabilities between 0/1 an 1/1 to the other side.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/592
https://github.com/google/deepvariant/issues/592:222,energy efficiency,model,model,222,"Hi @li1ba . I have a follow-up question on this issue. It looks like these reads are 100bp exome reads. We may have identified an issue which could cause genotype calls of this nature in this specific data type. We have a model which we think will address this issue. If you are interested to try running that model, can you email awcarroll@google.com and I can send you the model and instructions on how to run it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/592
https://github.com/google/deepvariant/issues/592:310,energy efficiency,model,model,310,"Hi @li1ba . I have a follow-up question on this issue. It looks like these reads are 100bp exome reads. We may have identified an issue which could cause genotype calls of this nature in this specific data type. We have a model which we think will address this issue. If you are interested to try running that model, can you email awcarroll@google.com and I can send you the model and instructions on how to run it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/592
https://github.com/google/deepvariant/issues/592:375,energy efficiency,model,model,375,"Hi @li1ba . I have a follow-up question on this issue. It looks like these reads are 100bp exome reads. We may have identified an issue which could cause genotype calls of this nature in this specific data type. We have a model which we think will address this issue. If you are interested to try running that model, can you email awcarroll@google.com and I can send you the model and instructions on how to run it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/592
https://github.com/google/deepvariant/issues/592:192,interoperability,specif,specific,192,"Hi @li1ba . I have a follow-up question on this issue. It looks like these reads are 100bp exome reads. We may have identified an issue which could cause genotype calls of this nature in this specific data type. We have a model which we think will address this issue. If you are interested to try running that model, can you email awcarroll@google.com and I can send you the model and instructions on how to run it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/592
https://github.com/google/deepvariant/issues/592:116,security,ident,identified,116,"Hi @li1ba . I have a follow-up question on this issue. It looks like these reads are 100bp exome reads. We may have identified an issue which could cause genotype calls of this nature in this specific data type. We have a model which we think will address this issue. If you are interested to try running that model, can you email awcarroll@google.com and I can send you the model and instructions on how to run it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/592
https://github.com/google/deepvariant/issues/592:222,security,model,model,222,"Hi @li1ba . I have a follow-up question on this issue. It looks like these reads are 100bp exome reads. We may have identified an issue which could cause genotype calls of this nature in this specific data type. We have a model which we think will address this issue. If you are interested to try running that model, can you email awcarroll@google.com and I can send you the model and instructions on how to run it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/592
https://github.com/google/deepvariant/issues/592:310,security,model,model,310,"Hi @li1ba . I have a follow-up question on this issue. It looks like these reads are 100bp exome reads. We may have identified an issue which could cause genotype calls of this nature in this specific data type. We have a model which we think will address this issue. If you are interested to try running that model, can you email awcarroll@google.com and I can send you the model and instructions on how to run it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/592
https://github.com/google/deepvariant/issues/592:375,security,model,model,375,"Hi @li1ba . I have a follow-up question on this issue. It looks like these reads are 100bp exome reads. We may have identified an issue which could cause genotype calls of this nature in this specific data type. We have a model which we think will address this issue. If you are interested to try running that model, can you email awcarroll@google.com and I can send you the model and instructions on how to run it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/592
https://github.com/google/deepvariant/issues/592:55,deployability,releas,release,55,"Hi @li1ba , this problem should be fixed in the latest release: https://github.com/google/deepvariant/releases/tag/v1.5.0. Let us know if it works. I'll close this issue for now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/592
https://github.com/google/deepvariant/issues/592:102,deployability,releas,releases,102,"Hi @li1ba , this problem should be fixed in the latest release: https://github.com/google/deepvariant/releases/tag/v1.5.0. Let us know if it works. I'll close this issue for now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/592
https://github.com/google/deepvariant/issues/592:153,usability,close,close,153,"Hi @li1ba , this problem should be fixed in the latest release: https://github.com/google/deepvariant/releases/tag/v1.5.0. Let us know if it works. I'll close this issue for now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/592
https://github.com/google/deepvariant/issues/593:75,availability,error,error,75,@sivianil it looks to me like an issue with the location of your data. The error is saying that the reference file does not exist. Can you double check the following:. 1. Did you define `${INPUT_DIR}`? 2. Do you have the `ucsc.hg19.chr20.unittest.fasta` in the input directory?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/593
https://github.com/google/deepvariant/issues/593:75,performance,error,error,75,@sivianil it looks to me like an issue with the location of your data. The error is saying that the reference file does not exist. Can you double check the following:. 1. Did you define `${INPUT_DIR}`? 2. Do you have the `ucsc.hg19.chr20.unittest.fasta` in the input directory?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/593
https://github.com/google/deepvariant/issues/593:115,reliability,doe,does,115,@sivianil it looks to me like an issue with the location of your data. The error is saying that the reference file does not exist. Can you double check the following:. 1. Did you define `${INPUT_DIR}`? 2. Do you have the `ucsc.hg19.chr20.unittest.fasta` in the input directory?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/593
https://github.com/google/deepvariant/issues/593:75,safety,error,error,75,@sivianil it looks to me like an issue with the location of your data. The error is saying that the reference file does not exist. Can you double check the following:. 1. Did you define `${INPUT_DIR}`? 2. Do you have the `ucsc.hg19.chr20.unittest.fasta` in the input directory?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/593
https://github.com/google/deepvariant/issues/593:261,safety,input,input,261,@sivianil it looks to me like an issue with the location of your data. The error is saying that the reference file does not exist. Can you double check the following:. 1. Did you define `${INPUT_DIR}`? 2. Do you have the `ucsc.hg19.chr20.unittest.fasta` in the input directory?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/593
https://github.com/google/deepvariant/issues/593:238,testability,unit,unittest,238,@sivianil it looks to me like an issue with the location of your data. The error is saying that the reference file does not exist. Can you double check the following:. 1. Did you define `${INPUT_DIR}`? 2. Do you have the `ucsc.hg19.chr20.unittest.fasta` in the input directory?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/593
https://github.com/google/deepvariant/issues/593:75,usability,error,error,75,@sivianil it looks to me like an issue with the location of your data. The error is saying that the reference file does not exist. Can you double check the following:. 1. Did you define `${INPUT_DIR}`? 2. Do you have the `ucsc.hg19.chr20.unittest.fasta` in the input directory?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/593
https://github.com/google/deepvariant/issues/593:261,usability,input,input,261,@sivianil it looks to me like an issue with the location of your data. The error is saying that the reference file does not exist. Can you double check the following:. 1. Did you define `${INPUT_DIR}`? 2. Do you have the `ucsc.hg19.chr20.unittest.fasta` in the input directory?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/593
https://github.com/google/deepvariant/issues/593:149,usability,user,user-images,149,"Yeah I defined INPUT_DIR and I have all the required files in my INPUT_DIR. . <img width=""1011"" alt=""Screenshot 2022-12-01 at 15 16 55"" src=""https://user-images.githubusercontent.com/75676816/205075692-189e08af-4ea8-44b7-ab1a-c812f170fa8e.png"">.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/593
https://github.com/google/deepvariant/issues/593:135,safety,input,input,135,"Try running the image interactively and verifying that the mounting works as expected. ```bash. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /bin/bash. ```. If you run the command this way you should be able to type:. ```bash. ls /input. ```. and see the `${INPUT_DIR}` listing of files. You can also run the command directly from there:. ```bash. /opt/deepvariant/bin/run_deepvariant ... ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/593
https://github.com/google/deepvariant/issues/593:306,safety,input,input,306,"Try running the image interactively and verifying that the mounting works as expected. ```bash. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /bin/bash. ```. If you run the command this way you should be able to type:. ```bash. ls /input. ```. and see the `${INPUT_DIR}` listing of files. You can also run the command directly from there:. ```bash. /opt/deepvariant/bin/run_deepvariant ... ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/593
https://github.com/google/deepvariant/issues/593:40,testability,verif,verifying,40,"Try running the image interactively and verifying that the mounting works as expected. ```bash. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /bin/bash. ```. If you run the command this way you should be able to type:. ```bash. ls /input. ```. and see the `${INPUT_DIR}` listing of files. You can also run the command directly from there:. ```bash. /opt/deepvariant/bin/run_deepvariant ... ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/593
https://github.com/google/deepvariant/issues/593:22,usability,interact,interactively,22,"Try running the image interactively and verifying that the mounting works as expected. ```bash. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /bin/bash. ```. If you run the command this way you should be able to type:. ```bash. ls /input. ```. and see the `${INPUT_DIR}` listing of files. You can also run the command directly from there:. ```bash. /opt/deepvariant/bin/run_deepvariant ... ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/593
https://github.com/google/deepvariant/issues/593:135,usability,input,input,135,"Try running the image interactively and verifying that the mounting works as expected. ```bash. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /bin/bash. ```. If you run the command this way you should be able to type:. ```bash. ls /input. ```. and see the `${INPUT_DIR}` listing of files. You can also run the command directly from there:. ```bash. /opt/deepvariant/bin/run_deepvariant ... ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/593
https://github.com/google/deepvariant/issues/593:247,usability,command,command,247,"Try running the image interactively and verifying that the mounting works as expected. ```bash. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /bin/bash. ```. If you run the command this way you should be able to type:. ```bash. ls /input. ```. and see the `${INPUT_DIR}` listing of files. You can also run the command directly from there:. ```bash. /opt/deepvariant/bin/run_deepvariant ... ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/593
https://github.com/google/deepvariant/issues/593:306,usability,input,input,306,"Try running the image interactively and verifying that the mounting works as expected. ```bash. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /bin/bash. ```. If you run the command this way you should be able to type:. ```bash. ls /input. ```. and see the `${INPUT_DIR}` listing of files. You can also run the command directly from there:. ```bash. /opt/deepvariant/bin/run_deepvariant ... ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/593
https://github.com/google/deepvariant/issues/593:384,usability,command,command,384,"Try running the image interactively and verifying that the mounting works as expected. ```bash. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /bin/bash. ```. If you run the command this way you should be able to type:. ```bash. ls /input. ```. and see the `${INPUT_DIR}` listing of files. You can also run the command directly from there:. ```bash. /opt/deepvariant/bin/run_deepvariant ... ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/593
https://github.com/google/deepvariant/issues/594:224,energy efficiency,model,model,224,"Hi @Solyris83 . Although it is in theory possible to train a 3-generation trio, I'm not aware of a truth dataset with sufficient representation of grandparents-parents-child (n=4,n=2,n=1) that would allow us to train such a model. The platinum genomes pedigree around NA12878 is the closes, but I think that is missing high enough quality labels of the children and not all of the grandparents.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/594
https://github.com/google/deepvariant/issues/594:224,security,model,model,224,"Hi @Solyris83 . Although it is in theory possible to train a 3-generation trio, I'm not aware of a truth dataset with sufficient representation of grandparents-parents-child (n=4,n=2,n=1) that would allow us to train such a model. The platinum genomes pedigree around NA12878 is the closes, but I think that is missing high enough quality labels of the children and not all of the grandparents.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/594
https://github.com/google/deepvariant/issues/594:283,usability,close,closes,283,"Hi @Solyris83 . Although it is in theory possible to train a 3-generation trio, I'm not aware of a truth dataset with sufficient representation of grandparents-parents-child (n=4,n=2,n=1) that would allow us to train such a model. The platinum genomes pedigree around NA12878 is the closes, but I think that is missing high enough quality labels of the children and not all of the grandparents.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/594
https://github.com/google/deepvariant/issues/594:62,usability,help,helpful,62,"Hi @Solyris83 ,. hopefully @AndrewCarroll 's answer have been helpful. I'm going to close this issue now. Feel free to let us know if you have more questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/594
https://github.com/google/deepvariant/issues/594:84,usability,close,close,84,"Hi @Solyris83 ,. hopefully @AndrewCarroll 's answer have been helpful. I'm going to close this issue now. Feel free to let us know if you have more questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/594
https://github.com/google/deepvariant/issues/596:21,availability,avail,available,21,nproc command is not available on your machine. You may replace $(nproc) with the number of cores on your computer.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/596
https://github.com/google/deepvariant/issues/596:92,energy efficiency,core,cores,92,nproc command is not available on your machine. You may replace $(nproc) with the number of cores on your computer.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/596
https://github.com/google/deepvariant/issues/596:21,reliability,availab,available,21,nproc command is not available on your machine. You may replace $(nproc) with the number of cores on your computer.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/596
https://github.com/google/deepvariant/issues/596:21,safety,avail,available,21,nproc command is not available on your machine. You may replace $(nproc) with the number of cores on your computer.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/596
https://github.com/google/deepvariant/issues/596:21,security,availab,available,21,nproc command is not available on your machine. You may replace $(nproc) with the number of cores on your computer.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/596
https://github.com/google/deepvariant/issues/596:6,usability,command,command,6,nproc command is not available on your machine. You may replace $(nproc) with the number of cores on your computer.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/596
https://github.com/google/deepvariant/issues/597:113,deployability,version,versions,113,"Hi, it seems like you're using the openvino flag. Please remove that flag and try again. For context - in recent versions, we haven't been building in openvino because we haven't been able to see extra speedup. In the next version, I'll try to make this more clear and show the proper message a bit earlier (or, just remove the flag). Please let me know if it works after you remove the openvino flag.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/597
https://github.com/google/deepvariant/issues/597:139,deployability,build,building,139,"Hi, it seems like you're using the openvino flag. Please remove that flag and try again. For context - in recent versions, we haven't been building in openvino because we haven't been able to see extra speedup. In the next version, I'll try to make this more clear and show the proper message a bit earlier (or, just remove the flag). Please let me know if it works after you remove the openvino flag.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/597
https://github.com/google/deepvariant/issues/597:223,deployability,version,version,223,"Hi, it seems like you're using the openvino flag. Please remove that flag and try again. For context - in recent versions, we haven't been building in openvino because we haven't been able to see extra speedup. In the next version, I'll try to make this more clear and show the proper message a bit earlier (or, just remove the flag). Please let me know if it works after you remove the openvino flag.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/597
https://github.com/google/deepvariant/issues/597:113,integrability,version,versions,113,"Hi, it seems like you're using the openvino flag. Please remove that flag and try again. For context - in recent versions, we haven't been building in openvino because we haven't been able to see extra speedup. In the next version, I'll try to make this more clear and show the proper message a bit earlier (or, just remove the flag). Please let me know if it works after you remove the openvino flag.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/597
https://github.com/google/deepvariant/issues/597:223,integrability,version,version,223,"Hi, it seems like you're using the openvino flag. Please remove that flag and try again. For context - in recent versions, we haven't been building in openvino because we haven't been able to see extra speedup. In the next version, I'll try to make this more clear and show the proper message a bit earlier (or, just remove the flag). Please let me know if it works after you remove the openvino flag.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/597
https://github.com/google/deepvariant/issues/597:285,integrability,messag,message,285,"Hi, it seems like you're using the openvino flag. Please remove that flag and try again. For context - in recent versions, we haven't been building in openvino because we haven't been able to see extra speedup. In the next version, I'll try to make this more clear and show the proper message a bit earlier (or, just remove the flag). Please let me know if it works after you remove the openvino flag.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/597
https://github.com/google/deepvariant/issues/597:285,interoperability,messag,message,285,"Hi, it seems like you're using the openvino flag. Please remove that flag and try again. For context - in recent versions, we haven't been building in openvino because we haven't been able to see extra speedup. In the next version, I'll try to make this more clear and show the proper message a bit earlier (or, just remove the flag). Please let me know if it works after you remove the openvino flag.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/597
https://github.com/google/deepvariant/issues/597:113,modifiability,version,versions,113,"Hi, it seems like you're using the openvino flag. Please remove that flag and try again. For context - in recent versions, we haven't been building in openvino because we haven't been able to see extra speedup. In the next version, I'll try to make this more clear and show the proper message a bit earlier (or, just remove the flag). Please let me know if it works after you remove the openvino flag.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/597
https://github.com/google/deepvariant/issues/597:223,modifiability,version,version,223,"Hi, it seems like you're using the openvino flag. Please remove that flag and try again. For context - in recent versions, we haven't been building in openvino because we haven't been able to see extra speedup. In the next version, I'll try to make this more clear and show the proper message a bit earlier (or, just remove the flag). Please let me know if it works after you remove the openvino flag.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/597
https://github.com/google/deepvariant/issues/597:93,testability,context,context,93,"Hi, it seems like you're using the openvino flag. Please remove that flag and try again. For context - in recent versions, we haven't been building in openvino because we haven't been able to see extra speedup. In the next version, I'll try to make this more clear and show the proper message a bit earlier (or, just remove the flag). Please let me know if it works after you remove the openvino flag.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/597
https://github.com/google/deepvariant/issues/597:259,usability,clear,clear,259,"Hi, it seems like you're using the openvino flag. Please remove that flag and try again. For context - in recent versions, we haven't been building in openvino because we haven't been able to see extra speedup. In the next version, I'll try to make this more clear and show the proper message a bit earlier (or, just remove the flag). Please let me know if it works after you remove the openvino flag.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/597
https://github.com/google/deepvariant/issues/597:731,availability,state,state,731,"Thanks for the feedback, I’m using a published docker image 1.4.0 which also latest at the time of this posting.How do I disable openvino?On Dec 13, 2022, at 11:51 PM, Pi-Chuan Chang ***@***.***> wrote:﻿. Hi, it seems like you're using the openvino flag. Please remove that flag and try again. For context - in recent versions, we haven't been building in openvino because we haven't been able to see extra speedup. In the next version, I'll try to make this more clear and show the proper message a bit earlier (or, just remove the flag). Please let me know if it works after you remove the openvino flag. —Reply to this email directly, view it on GitHub, or unsubscribe.You are receiving this because you modified the open/close state.Message ID: ***@***.***>",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/597
https://github.com/google/deepvariant/issues/597:318,deployability,version,versions,318,"Thanks for the feedback, I’m using a published docker image 1.4.0 which also latest at the time of this posting.How do I disable openvino?On Dec 13, 2022, at 11:51 PM, Pi-Chuan Chang ***@***.***> wrote:﻿. Hi, it seems like you're using the openvino flag. Please remove that flag and try again. For context - in recent versions, we haven't been building in openvino because we haven't been able to see extra speedup. In the next version, I'll try to make this more clear and show the proper message a bit earlier (or, just remove the flag). Please let me know if it works after you remove the openvino flag. —Reply to this email directly, view it on GitHub, or unsubscribe.You are receiving this because you modified the open/close state.Message ID: ***@***.***>",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/597
https://github.com/google/deepvariant/issues/597:344,deployability,build,building,344,"Thanks for the feedback, I’m using a published docker image 1.4.0 which also latest at the time of this posting.How do I disable openvino?On Dec 13, 2022, at 11:51 PM, Pi-Chuan Chang ***@***.***> wrote:﻿. Hi, it seems like you're using the openvino flag. Please remove that flag and try again. For context - in recent versions, we haven't been building in openvino because we haven't been able to see extra speedup. In the next version, I'll try to make this more clear and show the proper message a bit earlier (or, just remove the flag). Please let me know if it works after you remove the openvino flag. —Reply to this email directly, view it on GitHub, or unsubscribe.You are receiving this because you modified the open/close state.Message ID: ***@***.***>",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/597
https://github.com/google/deepvariant/issues/597:428,deployability,version,version,428,"Thanks for the feedback, I’m using a published docker image 1.4.0 which also latest at the time of this posting.How do I disable openvino?On Dec 13, 2022, at 11:51 PM, Pi-Chuan Chang ***@***.***> wrote:﻿. Hi, it seems like you're using the openvino flag. Please remove that flag and try again. For context - in recent versions, we haven't been building in openvino because we haven't been able to see extra speedup. In the next version, I'll try to make this more clear and show the proper message a bit earlier (or, just remove the flag). Please let me know if it works after you remove the openvino flag. —Reply to this email directly, view it on GitHub, or unsubscribe.You are receiving this because you modified the open/close state.Message ID: ***@***.***>",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/597
https://github.com/google/deepvariant/issues/597:37,integrability,pub,published,37,"Thanks for the feedback, I’m using a published docker image 1.4.0 which also latest at the time of this posting.How do I disable openvino?On Dec 13, 2022, at 11:51 PM, Pi-Chuan Chang ***@***.***> wrote:﻿. Hi, it seems like you're using the openvino flag. Please remove that flag and try again. For context - in recent versions, we haven't been building in openvino because we haven't been able to see extra speedup. In the next version, I'll try to make this more clear and show the proper message a bit earlier (or, just remove the flag). Please let me know if it works after you remove the openvino flag. —Reply to this email directly, view it on GitHub, or unsubscribe.You are receiving this because you modified the open/close state.Message ID: ***@***.***>",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/597
https://github.com/google/deepvariant/issues/597:318,integrability,version,versions,318,"Thanks for the feedback, I’m using a published docker image 1.4.0 which also latest at the time of this posting.How do I disable openvino?On Dec 13, 2022, at 11:51 PM, Pi-Chuan Chang ***@***.***> wrote:﻿. Hi, it seems like you're using the openvino flag. Please remove that flag and try again. For context - in recent versions, we haven't been building in openvino because we haven't been able to see extra speedup. In the next version, I'll try to make this more clear and show the proper message a bit earlier (or, just remove the flag). Please let me know if it works after you remove the openvino flag. —Reply to this email directly, view it on GitHub, or unsubscribe.You are receiving this because you modified the open/close state.Message ID: ***@***.***>",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/597
https://github.com/google/deepvariant/issues/597:428,integrability,version,version,428,"Thanks for the feedback, I’m using a published docker image 1.4.0 which also latest at the time of this posting.How do I disable openvino?On Dec 13, 2022, at 11:51 PM, Pi-Chuan Chang ***@***.***> wrote:﻿. Hi, it seems like you're using the openvino flag. Please remove that flag and try again. For context - in recent versions, we haven't been building in openvino because we haven't been able to see extra speedup. In the next version, I'll try to make this more clear and show the proper message a bit earlier (or, just remove the flag). Please let me know if it works after you remove the openvino flag. —Reply to this email directly, view it on GitHub, or unsubscribe.You are receiving this because you modified the open/close state.Message ID: ***@***.***>",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/597
https://github.com/google/deepvariant/issues/597:490,integrability,messag,message,490,"Thanks for the feedback, I’m using a published docker image 1.4.0 which also latest at the time of this posting.How do I disable openvino?On Dec 13, 2022, at 11:51 PM, Pi-Chuan Chang ***@***.***> wrote:﻿. Hi, it seems like you're using the openvino flag. Please remove that flag and try again. For context - in recent versions, we haven't been building in openvino because we haven't been able to see extra speedup. In the next version, I'll try to make this more clear and show the proper message a bit earlier (or, just remove the flag). Please let me know if it works after you remove the openvino flag. —Reply to this email directly, view it on GitHub, or unsubscribe.You are receiving this because you modified the open/close state.Message ID: ***@***.***>",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/597
https://github.com/google/deepvariant/issues/597:731,integrability,state,state,731,"Thanks for the feedback, I’m using a published docker image 1.4.0 which also latest at the time of this posting.How do I disable openvino?On Dec 13, 2022, at 11:51 PM, Pi-Chuan Chang ***@***.***> wrote:﻿. Hi, it seems like you're using the openvino flag. Please remove that flag and try again. For context - in recent versions, we haven't been building in openvino because we haven't been able to see extra speedup. In the next version, I'll try to make this more clear and show the proper message a bit earlier (or, just remove the flag). Please let me know if it works after you remove the openvino flag. —Reply to this email directly, view it on GitHub, or unsubscribe.You are receiving this because you modified the open/close state.Message ID: ***@***.***>",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/597
https://github.com/google/deepvariant/issues/597:737,integrability,Messag,Message,737,"Thanks for the feedback, I’m using a published docker image 1.4.0 which also latest at the time of this posting.How do I disable openvino?On Dec 13, 2022, at 11:51 PM, Pi-Chuan Chang ***@***.***> wrote:﻿. Hi, it seems like you're using the openvino flag. Please remove that flag and try again. For context - in recent versions, we haven't been building in openvino because we haven't been able to see extra speedup. In the next version, I'll try to make this more clear and show the proper message a bit earlier (or, just remove the flag). Please let me know if it works after you remove the openvino flag. —Reply to this email directly, view it on GitHub, or unsubscribe.You are receiving this because you modified the open/close state.Message ID: ***@***.***>",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/597
https://github.com/google/deepvariant/issues/597:490,interoperability,messag,message,490,"Thanks for the feedback, I’m using a published docker image 1.4.0 which also latest at the time of this posting.How do I disable openvino?On Dec 13, 2022, at 11:51 PM, Pi-Chuan Chang ***@***.***> wrote:﻿. Hi, it seems like you're using the openvino flag. Please remove that flag and try again. For context - in recent versions, we haven't been building in openvino because we haven't been able to see extra speedup. In the next version, I'll try to make this more clear and show the proper message a bit earlier (or, just remove the flag). Please let me know if it works after you remove the openvino flag. —Reply to this email directly, view it on GitHub, or unsubscribe.You are receiving this because you modified the open/close state.Message ID: ***@***.***>",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/597
https://github.com/google/deepvariant/issues/597:737,interoperability,Messag,Message,737,"Thanks for the feedback, I’m using a published docker image 1.4.0 which also latest at the time of this posting.How do I disable openvino?On Dec 13, 2022, at 11:51 PM, Pi-Chuan Chang ***@***.***> wrote:﻿. Hi, it seems like you're using the openvino flag. Please remove that flag and try again. For context - in recent versions, we haven't been building in openvino because we haven't been able to see extra speedup. In the next version, I'll try to make this more clear and show the proper message a bit earlier (or, just remove the flag). Please let me know if it works after you remove the openvino flag. —Reply to this email directly, view it on GitHub, or unsubscribe.You are receiving this because you modified the open/close state.Message ID: ***@***.***>",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/597
https://github.com/google/deepvariant/issues/597:318,modifiability,version,versions,318,"Thanks for the feedback, I’m using a published docker image 1.4.0 which also latest at the time of this posting.How do I disable openvino?On Dec 13, 2022, at 11:51 PM, Pi-Chuan Chang ***@***.***> wrote:﻿. Hi, it seems like you're using the openvino flag. Please remove that flag and try again. For context - in recent versions, we haven't been building in openvino because we haven't been able to see extra speedup. In the next version, I'll try to make this more clear and show the proper message a bit earlier (or, just remove the flag). Please let me know if it works after you remove the openvino flag. —Reply to this email directly, view it on GitHub, or unsubscribe.You are receiving this because you modified the open/close state.Message ID: ***@***.***>",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/597
https://github.com/google/deepvariant/issues/597:428,modifiability,version,version,428,"Thanks for the feedback, I’m using a published docker image 1.4.0 which also latest at the time of this posting.How do I disable openvino?On Dec 13, 2022, at 11:51 PM, Pi-Chuan Chang ***@***.***> wrote:﻿. Hi, it seems like you're using the openvino flag. Please remove that flag and try again. For context - in recent versions, we haven't been building in openvino because we haven't been able to see extra speedup. In the next version, I'll try to make this more clear and show the proper message a bit earlier (or, just remove the flag). Please let me know if it works after you remove the openvino flag. —Reply to this email directly, view it on GitHub, or unsubscribe.You are receiving this because you modified the open/close state.Message ID: ***@***.***>",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/597
https://github.com/google/deepvariant/issues/597:91,performance,time,time,91,"Thanks for the feedback, I’m using a published docker image 1.4.0 which also latest at the time of this posting.How do I disable openvino?On Dec 13, 2022, at 11:51 PM, Pi-Chuan Chang ***@***.***> wrote:﻿. Hi, it seems like you're using the openvino flag. Please remove that flag and try again. For context - in recent versions, we haven't been building in openvino because we haven't been able to see extra speedup. In the next version, I'll try to make this more clear and show the proper message a bit earlier (or, just remove the flag). Please let me know if it works after you remove the openvino flag. —Reply to this email directly, view it on GitHub, or unsubscribe.You are receiving this because you modified the open/close state.Message ID: ***@***.***>",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/597
https://github.com/google/deepvariant/issues/597:707,security,modif,modified,707,"Thanks for the feedback, I’m using a published docker image 1.4.0 which also latest at the time of this posting.How do I disable openvino?On Dec 13, 2022, at 11:51 PM, Pi-Chuan Chang ***@***.***> wrote:﻿. Hi, it seems like you're using the openvino flag. Please remove that flag and try again. For context - in recent versions, we haven't been building in openvino because we haven't been able to see extra speedup. In the next version, I'll try to make this more clear and show the proper message a bit earlier (or, just remove the flag). Please let me know if it works after you remove the openvino flag. —Reply to this email directly, view it on GitHub, or unsubscribe.You are receiving this because you modified the open/close state.Message ID: ***@***.***>",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/597
https://github.com/google/deepvariant/issues/597:298,testability,context,context,298,"Thanks for the feedback, I’m using a published docker image 1.4.0 which also latest at the time of this posting.How do I disable openvino?On Dec 13, 2022, at 11:51 PM, Pi-Chuan Chang ***@***.***> wrote:﻿. Hi, it seems like you're using the openvino flag. Please remove that flag and try again. For context - in recent versions, we haven't been building in openvino because we haven't been able to see extra speedup. In the next version, I'll try to make this more clear and show the proper message a bit earlier (or, just remove the flag). Please let me know if it works after you remove the openvino flag. —Reply to this email directly, view it on GitHub, or unsubscribe.You are receiving this because you modified the open/close state.Message ID: ***@***.***>",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/597
https://github.com/google/deepvariant/issues/597:15,usability,feedback,feedback,15,"Thanks for the feedback, I’m using a published docker image 1.4.0 which also latest at the time of this posting.How do I disable openvino?On Dec 13, 2022, at 11:51 PM, Pi-Chuan Chang ***@***.***> wrote:﻿. Hi, it seems like you're using the openvino flag. Please remove that flag and try again. For context - in recent versions, we haven't been building in openvino because we haven't been able to see extra speedup. In the next version, I'll try to make this more clear and show the proper message a bit earlier (or, just remove the flag). Please let me know if it works after you remove the openvino flag. —Reply to this email directly, view it on GitHub, or unsubscribe.You are receiving this because you modified the open/close state.Message ID: ***@***.***>",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/597
https://github.com/google/deepvariant/issues/597:464,usability,clear,clear,464,"Thanks for the feedback, I’m using a published docker image 1.4.0 which also latest at the time of this posting.How do I disable openvino?On Dec 13, 2022, at 11:51 PM, Pi-Chuan Chang ***@***.***> wrote:﻿. Hi, it seems like you're using the openvino flag. Please remove that flag and try again. For context - in recent versions, we haven't been building in openvino because we haven't been able to see extra speedup. In the next version, I'll try to make this more clear and show the proper message a bit earlier (or, just remove the flag). Please let me know if it works after you remove the openvino flag. —Reply to this email directly, view it on GitHub, or unsubscribe.You are receiving this because you modified the open/close state.Message ID: ***@***.***>",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/597
https://github.com/google/deepvariant/issues/597:725,usability,close,close,725,"Thanks for the feedback, I’m using a published docker image 1.4.0 which also latest at the time of this posting.How do I disable openvino?On Dec 13, 2022, at 11:51 PM, Pi-Chuan Chang ***@***.***> wrote:﻿. Hi, it seems like you're using the openvino flag. Please remove that flag and try again. For context - in recent versions, we haven't been building in openvino because we haven't been able to see extra speedup. In the next version, I'll try to make this more clear and show the proper message a bit earlier (or, just remove the flag). Please let me know if it works after you remove the openvino flag. —Reply to this email directly, view it on GitHub, or unsubscribe.You are receiving this because you modified the open/close state.Message ID: ***@***.***>",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/597
https://github.com/google/deepvariant/issues/597:8,interoperability,share,share,8,Can you share the command you run? OpenVINO is off by default in v1.4. So you likely are adding a flag for it.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/597
https://github.com/google/deepvariant/issues/597:18,usability,command,command,18,Can you share the command you run? OpenVINO is off by default in v1.4. So you likely are adding a flag for it.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/597
https://github.com/google/deepvariant/issues/597:15,interoperability,share,share,15,"Sorry, you did share the command in the first post. It's this one - please just remove it `--call_variants_extra_args=use_openvino=true`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/597
https://github.com/google/deepvariant/issues/597:25,usability,command,command,25,"Sorry, you did share the command in the first post. It's this one - please just remove it `--call_variants_extra_args=use_openvino=true`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/597
https://github.com/google/deepvariant/issues/597:49,deployability,updat,update,49,I'll close this one for now. Please feel free to update and let me know if it worked or not.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/597
https://github.com/google/deepvariant/issues/597:49,safety,updat,update,49,I'll close this one for now. Please feel free to update and let me know if it worked or not.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/597
https://github.com/google/deepvariant/issues/597:49,security,updat,update,49,I'll close this one for now. Please feel free to update and let me know if it worked or not.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/597
https://github.com/google/deepvariant/issues/597:5,usability,close,close,5,I'll close this one for now. Please feel free to update and let me know if it worked or not.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/597
https://github.com/google/deepvariant/issues/597:28,deployability,updat,update,28,"Hi @serverchief , . a quick update for you:. In the next release, we'll make sure that if you accidentally run with `--call_variants_extra_args=use_openvino=true`, the program will exit earlier (before it starts make_examples) and warn you that openvino is no longer built in.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/597
https://github.com/google/deepvariant/issues/597:57,deployability,releas,release,57,"Hi @serverchief , . a quick update for you:. In the next release, we'll make sure that if you accidentally run with `--call_variants_extra_args=use_openvino=true`, the program will exit earlier (before it starts make_examples) and warn you that openvino is no longer built in.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/597
https://github.com/google/deepvariant/issues/597:28,safety,updat,update,28,"Hi @serverchief , . a quick update for you:. In the next release, we'll make sure that if you accidentally run with `--call_variants_extra_args=use_openvino=true`, the program will exit earlier (before it starts make_examples) and warn you that openvino is no longer built in.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/597
https://github.com/google/deepvariant/issues/597:94,safety,accid,accidentally,94,"Hi @serverchief , . a quick update for you:. In the next release, we'll make sure that if you accidentally run with `--call_variants_extra_args=use_openvino=true`, the program will exit earlier (before it starts make_examples) and warn you that openvino is no longer built in.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/597
https://github.com/google/deepvariant/issues/597:28,security,updat,update,28,"Hi @serverchief , . a quick update for you:. In the next release, we'll make sure that if you accidentally run with `--call_variants_extra_args=use_openvino=true`, the program will exit earlier (before it starts make_examples) and warn you that openvino is no longer built in.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/597
https://github.com/google/deepvariant/issues/598:150,deployability,instal,install,150,Hi @Zero-Sun . `dv_make_examples.py` isn't a file that our GitHub repo provided. Can you tell me a bit more about what's in that file and how did you install DeepVariant?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598
https://github.com/google/deepvariant/issues/598:23,deployability,instal,install,23,"Sorry, I used mamba to install it. And then I get `dv_make_examples.py`,`dv_call_variants.py`,`dv_postprocess_variants.py`. ```. $ mamba search deepvariant. Name Version Build Channel. deepvariant 1.3.0 py36hf3e76ba_0 bioconda. deepvariant 1.4.0 py36hf3e76ba_0 bioconda. $ mamba install deepvariant. ```. When I run `dv_make_examples.py`, this is the directory where my `python` executes `make_examples.zip`, the same directory as the file you provide at this site. [https://console.cloud.google.com/storage/browser/deepvariant/binaries/DeepVariant/1.4.0/DeepVariant-1.4.0](url). ```. $cd /path/deepvariant-1.4.0-0/binaries/DeepVariant/1.4.0/DeepVariant-1.4.0/. $ls. call_variants_keras.zip freeze_graph.zip model_eval.zip postprocess_variants.zip settings.sh. call_variants.zip licenses.zip model_train.zip run-prereq.sh show_examples.zip. deeptrio make_examples.zip multisample_make_examples.zip runtime_by_region_vis.zip vcf_stats_report.zip. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598
https://github.com/google/deepvariant/issues/598:162,deployability,Version,Version,162,"Sorry, I used mamba to install it. And then I get `dv_make_examples.py`,`dv_call_variants.py`,`dv_postprocess_variants.py`. ```. $ mamba search deepvariant. Name Version Build Channel. deepvariant 1.3.0 py36hf3e76ba_0 bioconda. deepvariant 1.4.0 py36hf3e76ba_0 bioconda. $ mamba install deepvariant. ```. When I run `dv_make_examples.py`, this is the directory where my `python` executes `make_examples.zip`, the same directory as the file you provide at this site. [https://console.cloud.google.com/storage/browser/deepvariant/binaries/DeepVariant/1.4.0/DeepVariant-1.4.0](url). ```. $cd /path/deepvariant-1.4.0-0/binaries/DeepVariant/1.4.0/DeepVariant-1.4.0/. $ls. call_variants_keras.zip freeze_graph.zip model_eval.zip postprocess_variants.zip settings.sh. call_variants.zip licenses.zip model_train.zip run-prereq.sh show_examples.zip. deeptrio make_examples.zip multisample_make_examples.zip runtime_by_region_vis.zip vcf_stats_report.zip. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598
https://github.com/google/deepvariant/issues/598:170,deployability,Build,Build,170,"Sorry, I used mamba to install it. And then I get `dv_make_examples.py`,`dv_call_variants.py`,`dv_postprocess_variants.py`. ```. $ mamba search deepvariant. Name Version Build Channel. deepvariant 1.3.0 py36hf3e76ba_0 bioconda. deepvariant 1.4.0 py36hf3e76ba_0 bioconda. $ mamba install deepvariant. ```. When I run `dv_make_examples.py`, this is the directory where my `python` executes `make_examples.zip`, the same directory as the file you provide at this site. [https://console.cloud.google.com/storage/browser/deepvariant/binaries/DeepVariant/1.4.0/DeepVariant-1.4.0](url). ```. $cd /path/deepvariant-1.4.0-0/binaries/DeepVariant/1.4.0/DeepVariant-1.4.0/. $ls. call_variants_keras.zip freeze_graph.zip model_eval.zip postprocess_variants.zip settings.sh. call_variants.zip licenses.zip model_train.zip run-prereq.sh show_examples.zip. deeptrio make_examples.zip multisample_make_examples.zip runtime_by_region_vis.zip vcf_stats_report.zip. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598
https://github.com/google/deepvariant/issues/598:279,deployability,instal,install,279,"Sorry, I used mamba to install it. And then I get `dv_make_examples.py`,`dv_call_variants.py`,`dv_postprocess_variants.py`. ```. $ mamba search deepvariant. Name Version Build Channel. deepvariant 1.3.0 py36hf3e76ba_0 bioconda. deepvariant 1.4.0 py36hf3e76ba_0 bioconda. $ mamba install deepvariant. ```. When I run `dv_make_examples.py`, this is the directory where my `python` executes `make_examples.zip`, the same directory as the file you provide at this site. [https://console.cloud.google.com/storage/browser/deepvariant/binaries/DeepVariant/1.4.0/DeepVariant-1.4.0](url). ```. $cd /path/deepvariant-1.4.0-0/binaries/DeepVariant/1.4.0/DeepVariant-1.4.0/. $ls. call_variants_keras.zip freeze_graph.zip model_eval.zip postprocess_variants.zip settings.sh. call_variants.zip licenses.zip model_train.zip run-prereq.sh show_examples.zip. deeptrio make_examples.zip multisample_make_examples.zip runtime_by_region_vis.zip vcf_stats_report.zip. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598
https://github.com/google/deepvariant/issues/598:483,energy efficiency,cloud,cloud,483,"Sorry, I used mamba to install it. And then I get `dv_make_examples.py`,`dv_call_variants.py`,`dv_postprocess_variants.py`. ```. $ mamba search deepvariant. Name Version Build Channel. deepvariant 1.3.0 py36hf3e76ba_0 bioconda. deepvariant 1.4.0 py36hf3e76ba_0 bioconda. $ mamba install deepvariant. ```. When I run `dv_make_examples.py`, this is the directory where my `python` executes `make_examples.zip`, the same directory as the file you provide at this site. [https://console.cloud.google.com/storage/browser/deepvariant/binaries/DeepVariant/1.4.0/DeepVariant-1.4.0](url). ```. $cd /path/deepvariant-1.4.0-0/binaries/DeepVariant/1.4.0/DeepVariant-1.4.0/. $ls. call_variants_keras.zip freeze_graph.zip model_eval.zip postprocess_variants.zip settings.sh. call_variants.zip licenses.zip model_train.zip run-prereq.sh show_examples.zip. deeptrio make_examples.zip multisample_make_examples.zip runtime_by_region_vis.zip vcf_stats_report.zip. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598
https://github.com/google/deepvariant/issues/598:162,integrability,Version,Version,162,"Sorry, I used mamba to install it. And then I get `dv_make_examples.py`,`dv_call_variants.py`,`dv_postprocess_variants.py`. ```. $ mamba search deepvariant. Name Version Build Channel. deepvariant 1.3.0 py36hf3e76ba_0 bioconda. deepvariant 1.4.0 py36hf3e76ba_0 bioconda. $ mamba install deepvariant. ```. When I run `dv_make_examples.py`, this is the directory where my `python` executes `make_examples.zip`, the same directory as the file you provide at this site. [https://console.cloud.google.com/storage/browser/deepvariant/binaries/DeepVariant/1.4.0/DeepVariant-1.4.0](url). ```. $cd /path/deepvariant-1.4.0-0/binaries/DeepVariant/1.4.0/DeepVariant-1.4.0/. $ls. call_variants_keras.zip freeze_graph.zip model_eval.zip postprocess_variants.zip settings.sh. call_variants.zip licenses.zip model_train.zip run-prereq.sh show_examples.zip. deeptrio make_examples.zip multisample_make_examples.zip runtime_by_region_vis.zip vcf_stats_report.zip. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598
https://github.com/google/deepvariant/issues/598:162,modifiability,Version,Version,162,"Sorry, I used mamba to install it. And then I get `dv_make_examples.py`,`dv_call_variants.py`,`dv_postprocess_variants.py`. ```. $ mamba search deepvariant. Name Version Build Channel. deepvariant 1.3.0 py36hf3e76ba_0 bioconda. deepvariant 1.4.0 py36hf3e76ba_0 bioconda. $ mamba install deepvariant. ```. When I run `dv_make_examples.py`, this is the directory where my `python` executes `make_examples.zip`, the same directory as the file you provide at this site. [https://console.cloud.google.com/storage/browser/deepvariant/binaries/DeepVariant/1.4.0/DeepVariant-1.4.0](url). ```. $cd /path/deepvariant-1.4.0-0/binaries/DeepVariant/1.4.0/DeepVariant-1.4.0/. $ls. call_variants_keras.zip freeze_graph.zip model_eval.zip postprocess_variants.zip settings.sh. call_variants.zip licenses.zip model_train.zip run-prereq.sh show_examples.zip. deeptrio make_examples.zip multisample_make_examples.zip runtime_by_region_vis.zip vcf_stats_report.zip. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598
https://github.com/google/deepvariant/issues/598:185,availability,error,error,185,"Thank you for your reply! For the following code, I don't have permission to modify the path `/usr/bin/`, and I didn't find `/usr/bin/python3` from `subprocess.py`, so I didn't fix the error either. As a study student, I am working hard to learn to solve these mistakes. It hasn't been resolved yet. ```. File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/python3.6/subprocess.py"", line 1364, in _execute_child. raise child_exception_type(errno_num, err_msg, err_filename). FileNotFoundError: [Errno 2] No such file or directory: '/usr/bin/python3': '/usr/bin/python3'. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598
https://github.com/google/deepvariant/issues/598:149,integrability,sub,subprocess,149,"Thank you for your reply! For the following code, I don't have permission to modify the path `/usr/bin/`, and I didn't find `/usr/bin/python3` from `subprocess.py`, so I didn't fix the error either. As a study student, I am working hard to learn to solve these mistakes. It hasn't been resolved yet. ```. File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/python3.6/subprocess.py"", line 1364, in _execute_child. raise child_exception_type(errno_num, err_msg, err_filename). FileNotFoundError: [Errno 2] No such file or directory: '/usr/bin/python3': '/usr/bin/python3'. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598
https://github.com/google/deepvariant/issues/598:360,integrability,sub,subprocess,360,"Thank you for your reply! For the following code, I don't have permission to modify the path `/usr/bin/`, and I didn't find `/usr/bin/python3` from `subprocess.py`, so I didn't fix the error either. As a study student, I am working hard to learn to solve these mistakes. It hasn't been resolved yet. ```. File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/python3.6/subprocess.py"", line 1364, in _execute_child. raise child_exception_type(errno_num, err_msg, err_filename). FileNotFoundError: [Errno 2] No such file or directory: '/usr/bin/python3': '/usr/bin/python3'. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598
https://github.com/google/deepvariant/issues/598:185,performance,error,error,185,"Thank you for your reply! For the following code, I don't have permission to modify the path `/usr/bin/`, and I didn't find `/usr/bin/python3` from `subprocess.py`, so I didn't fix the error either. As a study student, I am working hard to learn to solve these mistakes. It hasn't been resolved yet. ```. File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/python3.6/subprocess.py"", line 1364, in _execute_child. raise child_exception_type(errno_num, err_msg, err_filename). FileNotFoundError: [Errno 2] No such file or directory: '/usr/bin/python3': '/usr/bin/python3'. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598
https://github.com/google/deepvariant/issues/598:63,safety,permiss,permission,63,"Thank you for your reply! For the following code, I don't have permission to modify the path `/usr/bin/`, and I didn't find `/usr/bin/python3` from `subprocess.py`, so I didn't fix the error either. As a study student, I am working hard to learn to solve these mistakes. It hasn't been resolved yet. ```. File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/python3.6/subprocess.py"", line 1364, in _execute_child. raise child_exception_type(errno_num, err_msg, err_filename). FileNotFoundError: [Errno 2] No such file or directory: '/usr/bin/python3': '/usr/bin/python3'. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598
https://github.com/google/deepvariant/issues/598:185,safety,error,error,185,"Thank you for your reply! For the following code, I don't have permission to modify the path `/usr/bin/`, and I didn't find `/usr/bin/python3` from `subprocess.py`, so I didn't fix the error either. As a study student, I am working hard to learn to solve these mistakes. It hasn't been resolved yet. ```. File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/python3.6/subprocess.py"", line 1364, in _execute_child. raise child_exception_type(errno_num, err_msg, err_filename). FileNotFoundError: [Errno 2] No such file or directory: '/usr/bin/python3': '/usr/bin/python3'. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598
https://github.com/google/deepvariant/issues/598:77,security,modif,modify,77,"Thank you for your reply! For the following code, I don't have permission to modify the path `/usr/bin/`, and I didn't find `/usr/bin/python3` from `subprocess.py`, so I didn't fix the error either. As a study student, I am working hard to learn to solve these mistakes. It hasn't been resolved yet. ```. File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/python3.6/subprocess.py"", line 1364, in _execute_child. raise child_exception_type(errno_num, err_msg, err_filename). FileNotFoundError: [Errno 2] No such file or directory: '/usr/bin/python3': '/usr/bin/python3'. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598
https://github.com/google/deepvariant/issues/598:185,usability,error,error,185,"Thank you for your reply! For the following code, I don't have permission to modify the path `/usr/bin/`, and I didn't find `/usr/bin/python3` from `subprocess.py`, so I didn't fix the error either. As a study student, I am working hard to learn to solve these mistakes. It hasn't been resolved yet. ```. File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/python3.6/subprocess.py"", line 1364, in _execute_child. raise child_exception_type(errno_num, err_msg, err_filename). FileNotFoundError: [Errno 2] No such file or directory: '/usr/bin/python3': '/usr/bin/python3'. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598
https://github.com/google/deepvariant/issues/598:240,usability,learn,learn,240,"Thank you for your reply! For the following code, I don't have permission to modify the path `/usr/bin/`, and I didn't find `/usr/bin/python3` from `subprocess.py`, so I didn't fix the error either. As a study student, I am working hard to learn to solve these mistakes. It hasn't been resolved yet. ```. File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/python3.6/subprocess.py"", line 1364, in _execute_child. raise child_exception_type(errno_num, err_msg, err_filename). FileNotFoundError: [Errno 2] No such file or directory: '/usr/bin/python3': '/usr/bin/python3'. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598
https://github.com/google/deepvariant/issues/598:3,reliability,Doe,Does,3,"1. Does your machine have `/usr/bin/python3`? If not, I'm not exactly sure how this would work. 2. Does your machine have Singularity? If so, you can consider that as an alternative. An example here https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-quick-start.md#notes-on-singularity",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598
https://github.com/google/deepvariant/issues/598:99,reliability,Doe,Does,99,"1. Does your machine have `/usr/bin/python3`? If not, I'm not exactly sure how this would work. 2. Does your machine have Singularity? If so, you can consider that as an alternative. An example here https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-quick-start.md#notes-on-singularity",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598
https://github.com/google/deepvariant/issues/598:306,availability,error,error,306,"Thank you for your reply! 1、My machine doesn't have `/usr/bin/python3`.I don't have permission to change `/usr/bin/`.I tried to modify the source code, but I couldn't locate the code. 2、I tried to install Singularity. I use the parameter `--without -- seccomp --without -- conmon` because of the following error. `./mconfig --without-suid --without-seccomp --without-conmon --prefix=/path/singularity && make -C ./builddir &&make -C ./builddir install`. ```. seccomp headers are required to build Singularity with seccomp support. To disable seccomp support run mconfig using '--without-seccomp'. Cannot build conmon for OCI support without libseccomp headers. Use --without-conmon to disable build and use conmon on PATH if present. ```. Then I try to run it. ```. singularity run -B /path/locale/:/path/locale/ \. > docker://google/deepvariant:""1.4.0"" \. > /path/dpv_singu \. > --model_type=PACBIO \. > --ref=/path/ref_fasta/QJref.fa \. > --reads=/path/bam_files/F1N_sorted.merged.addg.uniq.rmdup.bam \. > --output_vcf=/path/output.vcf.gz \. > --output_gvcf=/path/output.g.vcf.gz \. > --intermediate_results_dir /path/intermediate_results_dir. ```. The error information is as follows. ```. INFO: Using cached SIF image. INFO: Converting SIF file to temporary sandbox... FATAL: while handling /home/my_user_name/.singularity/cache/oci-tmp/sha256.83ce0d6bbe3695bcbaa348b73c48737bdbfaeaea2272b0105dd4bdfa7a804f18: while extracting image: root filesystem extraction failed: extract command failed: ERROR : Failed to create user namespace: user namespace disabled. : exit status 1. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598
https://github.com/google/deepvariant/issues/598:1155,availability,error,error,1155,"Thank you for your reply! 1、My machine doesn't have `/usr/bin/python3`.I don't have permission to change `/usr/bin/`.I tried to modify the source code, but I couldn't locate the code. 2、I tried to install Singularity. I use the parameter `--without -- seccomp --without -- conmon` because of the following error. `./mconfig --without-suid --without-seccomp --without-conmon --prefix=/path/singularity && make -C ./builddir &&make -C ./builddir install`. ```. seccomp headers are required to build Singularity with seccomp support. To disable seccomp support run mconfig using '--without-seccomp'. Cannot build conmon for OCI support without libseccomp headers. Use --without-conmon to disable build and use conmon on PATH if present. ```. Then I try to run it. ```. singularity run -B /path/locale/:/path/locale/ \. > docker://google/deepvariant:""1.4.0"" \. > /path/dpv_singu \. > --model_type=PACBIO \. > --ref=/path/ref_fasta/QJref.fa \. > --reads=/path/bam_files/F1N_sorted.merged.addg.uniq.rmdup.bam \. > --output_vcf=/path/output.vcf.gz \. > --output_gvcf=/path/output.g.vcf.gz \. > --intermediate_results_dir /path/intermediate_results_dir. ```. The error information is as follows. ```. INFO: Using cached SIF image. INFO: Converting SIF file to temporary sandbox... FATAL: while handling /home/my_user_name/.singularity/cache/oci-tmp/sha256.83ce0d6bbe3695bcbaa348b73c48737bdbfaeaea2272b0105dd4bdfa7a804f18: while extracting image: root filesystem extraction failed: extract command failed: ERROR : Failed to create user namespace: user namespace disabled. : exit status 1. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598
https://github.com/google/deepvariant/issues/598:1497,availability,ERROR,ERROR,1497,"Thank you for your reply! 1、My machine doesn't have `/usr/bin/python3`.I don't have permission to change `/usr/bin/`.I tried to modify the source code, but I couldn't locate the code. 2、I tried to install Singularity. I use the parameter `--without -- seccomp --without -- conmon` because of the following error. `./mconfig --without-suid --without-seccomp --without-conmon --prefix=/path/singularity && make -C ./builddir &&make -C ./builddir install`. ```. seccomp headers are required to build Singularity with seccomp support. To disable seccomp support run mconfig using '--without-seccomp'. Cannot build conmon for OCI support without libseccomp headers. Use --without-conmon to disable build and use conmon on PATH if present. ```. Then I try to run it. ```. singularity run -B /path/locale/:/path/locale/ \. > docker://google/deepvariant:""1.4.0"" \. > /path/dpv_singu \. > --model_type=PACBIO \. > --ref=/path/ref_fasta/QJref.fa \. > --reads=/path/bam_files/F1N_sorted.merged.addg.uniq.rmdup.bam \. > --output_vcf=/path/output.vcf.gz \. > --output_gvcf=/path/output.g.vcf.gz \. > --intermediate_results_dir /path/intermediate_results_dir. ```. The error information is as follows. ```. INFO: Using cached SIF image. INFO: Converting SIF file to temporary sandbox... FATAL: while handling /home/my_user_name/.singularity/cache/oci-tmp/sha256.83ce0d6bbe3695bcbaa348b73c48737bdbfaeaea2272b0105dd4bdfa7a804f18: while extracting image: root filesystem extraction failed: extract command failed: ERROR : Failed to create user namespace: user namespace disabled. : exit status 1. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598
https://github.com/google/deepvariant/issues/598:197,deployability,instal,install,197,"Thank you for your reply! 1、My machine doesn't have `/usr/bin/python3`.I don't have permission to change `/usr/bin/`.I tried to modify the source code, but I couldn't locate the code. 2、I tried to install Singularity. I use the parameter `--without -- seccomp --without -- conmon` because of the following error. `./mconfig --without-suid --without-seccomp --without-conmon --prefix=/path/singularity && make -C ./builddir &&make -C ./builddir install`. ```. seccomp headers are required to build Singularity with seccomp support. To disable seccomp support run mconfig using '--without-seccomp'. Cannot build conmon for OCI support without libseccomp headers. Use --without-conmon to disable build and use conmon on PATH if present. ```. Then I try to run it. ```. singularity run -B /path/locale/:/path/locale/ \. > docker://google/deepvariant:""1.4.0"" \. > /path/dpv_singu \. > --model_type=PACBIO \. > --ref=/path/ref_fasta/QJref.fa \. > --reads=/path/bam_files/F1N_sorted.merged.addg.uniq.rmdup.bam \. > --output_vcf=/path/output.vcf.gz \. > --output_gvcf=/path/output.g.vcf.gz \. > --intermediate_results_dir /path/intermediate_results_dir. ```. The error information is as follows. ```. INFO: Using cached SIF image. INFO: Converting SIF file to temporary sandbox... FATAL: while handling /home/my_user_name/.singularity/cache/oci-tmp/sha256.83ce0d6bbe3695bcbaa348b73c48737bdbfaeaea2272b0105dd4bdfa7a804f18: while extracting image: root filesystem extraction failed: extract command failed: ERROR : Failed to create user namespace: user namespace disabled. : exit status 1. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598
https://github.com/google/deepvariant/issues/598:414,deployability,build,builddir,414,"Thank you for your reply! 1、My machine doesn't have `/usr/bin/python3`.I don't have permission to change `/usr/bin/`.I tried to modify the source code, but I couldn't locate the code. 2、I tried to install Singularity. I use the parameter `--without -- seccomp --without -- conmon` because of the following error. `./mconfig --without-suid --without-seccomp --without-conmon --prefix=/path/singularity && make -C ./builddir &&make -C ./builddir install`. ```. seccomp headers are required to build Singularity with seccomp support. To disable seccomp support run mconfig using '--without-seccomp'. Cannot build conmon for OCI support without libseccomp headers. Use --without-conmon to disable build and use conmon on PATH if present. ```. Then I try to run it. ```. singularity run -B /path/locale/:/path/locale/ \. > docker://google/deepvariant:""1.4.0"" \. > /path/dpv_singu \. > --model_type=PACBIO \. > --ref=/path/ref_fasta/QJref.fa \. > --reads=/path/bam_files/F1N_sorted.merged.addg.uniq.rmdup.bam \. > --output_vcf=/path/output.vcf.gz \. > --output_gvcf=/path/output.g.vcf.gz \. > --intermediate_results_dir /path/intermediate_results_dir. ```. The error information is as follows. ```. INFO: Using cached SIF image. INFO: Converting SIF file to temporary sandbox... FATAL: while handling /home/my_user_name/.singularity/cache/oci-tmp/sha256.83ce0d6bbe3695bcbaa348b73c48737bdbfaeaea2272b0105dd4bdfa7a804f18: while extracting image: root filesystem extraction failed: extract command failed: ERROR : Failed to create user namespace: user namespace disabled. : exit status 1. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598
https://github.com/google/deepvariant/issues/598:435,deployability,build,builddir,435,"Thank you for your reply! 1、My machine doesn't have `/usr/bin/python3`.I don't have permission to change `/usr/bin/`.I tried to modify the source code, but I couldn't locate the code. 2、I tried to install Singularity. I use the parameter `--without -- seccomp --without -- conmon` because of the following error. `./mconfig --without-suid --without-seccomp --without-conmon --prefix=/path/singularity && make -C ./builddir &&make -C ./builddir install`. ```. seccomp headers are required to build Singularity with seccomp support. To disable seccomp support run mconfig using '--without-seccomp'. Cannot build conmon for OCI support without libseccomp headers. Use --without-conmon to disable build and use conmon on PATH if present. ```. Then I try to run it. ```. singularity run -B /path/locale/:/path/locale/ \. > docker://google/deepvariant:""1.4.0"" \. > /path/dpv_singu \. > --model_type=PACBIO \. > --ref=/path/ref_fasta/QJref.fa \. > --reads=/path/bam_files/F1N_sorted.merged.addg.uniq.rmdup.bam \. > --output_vcf=/path/output.vcf.gz \. > --output_gvcf=/path/output.g.vcf.gz \. > --intermediate_results_dir /path/intermediate_results_dir. ```. The error information is as follows. ```. INFO: Using cached SIF image. INFO: Converting SIF file to temporary sandbox... FATAL: while handling /home/my_user_name/.singularity/cache/oci-tmp/sha256.83ce0d6bbe3695bcbaa348b73c48737bdbfaeaea2272b0105dd4bdfa7a804f18: while extracting image: root filesystem extraction failed: extract command failed: ERROR : Failed to create user namespace: user namespace disabled. : exit status 1. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598
https://github.com/google/deepvariant/issues/598:444,deployability,instal,install,444,"Thank you for your reply! 1、My machine doesn't have `/usr/bin/python3`.I don't have permission to change `/usr/bin/`.I tried to modify the source code, but I couldn't locate the code. 2、I tried to install Singularity. I use the parameter `--without -- seccomp --without -- conmon` because of the following error. `./mconfig --without-suid --without-seccomp --without-conmon --prefix=/path/singularity && make -C ./builddir &&make -C ./builddir install`. ```. seccomp headers are required to build Singularity with seccomp support. To disable seccomp support run mconfig using '--without-seccomp'. Cannot build conmon for OCI support without libseccomp headers. Use --without-conmon to disable build and use conmon on PATH if present. ```. Then I try to run it. ```. singularity run -B /path/locale/:/path/locale/ \. > docker://google/deepvariant:""1.4.0"" \. > /path/dpv_singu \. > --model_type=PACBIO \. > --ref=/path/ref_fasta/QJref.fa \. > --reads=/path/bam_files/F1N_sorted.merged.addg.uniq.rmdup.bam \. > --output_vcf=/path/output.vcf.gz \. > --output_gvcf=/path/output.g.vcf.gz \. > --intermediate_results_dir /path/intermediate_results_dir. ```. The error information is as follows. ```. INFO: Using cached SIF image. INFO: Converting SIF file to temporary sandbox... FATAL: while handling /home/my_user_name/.singularity/cache/oci-tmp/sha256.83ce0d6bbe3695bcbaa348b73c48737bdbfaeaea2272b0105dd4bdfa7a804f18: while extracting image: root filesystem extraction failed: extract command failed: ERROR : Failed to create user namespace: user namespace disabled. : exit status 1. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598
https://github.com/google/deepvariant/issues/598:491,deployability,build,build,491,"Thank you for your reply! 1、My machine doesn't have `/usr/bin/python3`.I don't have permission to change `/usr/bin/`.I tried to modify the source code, but I couldn't locate the code. 2、I tried to install Singularity. I use the parameter `--without -- seccomp --without -- conmon` because of the following error. `./mconfig --without-suid --without-seccomp --without-conmon --prefix=/path/singularity && make -C ./builddir &&make -C ./builddir install`. ```. seccomp headers are required to build Singularity with seccomp support. To disable seccomp support run mconfig using '--without-seccomp'. Cannot build conmon for OCI support without libseccomp headers. Use --without-conmon to disable build and use conmon on PATH if present. ```. Then I try to run it. ```. singularity run -B /path/locale/:/path/locale/ \. > docker://google/deepvariant:""1.4.0"" \. > /path/dpv_singu \. > --model_type=PACBIO \. > --ref=/path/ref_fasta/QJref.fa \. > --reads=/path/bam_files/F1N_sorted.merged.addg.uniq.rmdup.bam \. > --output_vcf=/path/output.vcf.gz \. > --output_gvcf=/path/output.g.vcf.gz \. > --intermediate_results_dir /path/intermediate_results_dir. ```. The error information is as follows. ```. INFO: Using cached SIF image. INFO: Converting SIF file to temporary sandbox... FATAL: while handling /home/my_user_name/.singularity/cache/oci-tmp/sha256.83ce0d6bbe3695bcbaa348b73c48737bdbfaeaea2272b0105dd4bdfa7a804f18: while extracting image: root filesystem extraction failed: extract command failed: ERROR : Failed to create user namespace: user namespace disabled. : exit status 1. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598
https://github.com/google/deepvariant/issues/598:604,deployability,build,build,604,"Thank you for your reply! 1、My machine doesn't have `/usr/bin/python3`.I don't have permission to change `/usr/bin/`.I tried to modify the source code, but I couldn't locate the code. 2、I tried to install Singularity. I use the parameter `--without -- seccomp --without -- conmon` because of the following error. `./mconfig --without-suid --without-seccomp --without-conmon --prefix=/path/singularity && make -C ./builddir &&make -C ./builddir install`. ```. seccomp headers are required to build Singularity with seccomp support. To disable seccomp support run mconfig using '--without-seccomp'. Cannot build conmon for OCI support without libseccomp headers. Use --without-conmon to disable build and use conmon on PATH if present. ```. Then I try to run it. ```. singularity run -B /path/locale/:/path/locale/ \. > docker://google/deepvariant:""1.4.0"" \. > /path/dpv_singu \. > --model_type=PACBIO \. > --ref=/path/ref_fasta/QJref.fa \. > --reads=/path/bam_files/F1N_sorted.merged.addg.uniq.rmdup.bam \. > --output_vcf=/path/output.vcf.gz \. > --output_gvcf=/path/output.g.vcf.gz \. > --intermediate_results_dir /path/intermediate_results_dir. ```. The error information is as follows. ```. INFO: Using cached SIF image. INFO: Converting SIF file to temporary sandbox... FATAL: while handling /home/my_user_name/.singularity/cache/oci-tmp/sha256.83ce0d6bbe3695bcbaa348b73c48737bdbfaeaea2272b0105dd4bdfa7a804f18: while extracting image: root filesystem extraction failed: extract command failed: ERROR : Failed to create user namespace: user namespace disabled. : exit status 1. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598
https://github.com/google/deepvariant/issues/598:693,deployability,build,build,693,"Thank you for your reply! 1、My machine doesn't have `/usr/bin/python3`.I don't have permission to change `/usr/bin/`.I tried to modify the source code, but I couldn't locate the code. 2、I tried to install Singularity. I use the parameter `--without -- seccomp --without -- conmon` because of the following error. `./mconfig --without-suid --without-seccomp --without-conmon --prefix=/path/singularity && make -C ./builddir &&make -C ./builddir install`. ```. seccomp headers are required to build Singularity with seccomp support. To disable seccomp support run mconfig using '--without-seccomp'. Cannot build conmon for OCI support without libseccomp headers. Use --without-conmon to disable build and use conmon on PATH if present. ```. Then I try to run it. ```. singularity run -B /path/locale/:/path/locale/ \. > docker://google/deepvariant:""1.4.0"" \. > /path/dpv_singu \. > --model_type=PACBIO \. > --ref=/path/ref_fasta/QJref.fa \. > --reads=/path/bam_files/F1N_sorted.merged.addg.uniq.rmdup.bam \. > --output_vcf=/path/output.vcf.gz \. > --output_gvcf=/path/output.g.vcf.gz \. > --intermediate_results_dir /path/intermediate_results_dir. ```. The error information is as follows. ```. INFO: Using cached SIF image. INFO: Converting SIF file to temporary sandbox... FATAL: while handling /home/my_user_name/.singularity/cache/oci-tmp/sha256.83ce0d6bbe3695bcbaa348b73c48737bdbfaeaea2272b0105dd4bdfa7a804f18: while extracting image: root filesystem extraction failed: extract command failed: ERROR : Failed to create user namespace: user namespace disabled. : exit status 1. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598
https://github.com/google/deepvariant/issues/598:1465,deployability,fail,failed,1465,"Thank you for your reply! 1、My machine doesn't have `/usr/bin/python3`.I don't have permission to change `/usr/bin/`.I tried to modify the source code, but I couldn't locate the code. 2、I tried to install Singularity. I use the parameter `--without -- seccomp --without -- conmon` because of the following error. `./mconfig --without-suid --without-seccomp --without-conmon --prefix=/path/singularity && make -C ./builddir &&make -C ./builddir install`. ```. seccomp headers are required to build Singularity with seccomp support. To disable seccomp support run mconfig using '--without-seccomp'. Cannot build conmon for OCI support without libseccomp headers. Use --without-conmon to disable build and use conmon on PATH if present. ```. Then I try to run it. ```. singularity run -B /path/locale/:/path/locale/ \. > docker://google/deepvariant:""1.4.0"" \. > /path/dpv_singu \. > --model_type=PACBIO \. > --ref=/path/ref_fasta/QJref.fa \. > --reads=/path/bam_files/F1N_sorted.merged.addg.uniq.rmdup.bam \. > --output_vcf=/path/output.vcf.gz \. > --output_gvcf=/path/output.g.vcf.gz \. > --intermediate_results_dir /path/intermediate_results_dir. ```. The error information is as follows. ```. INFO: Using cached SIF image. INFO: Converting SIF file to temporary sandbox... FATAL: while handling /home/my_user_name/.singularity/cache/oci-tmp/sha256.83ce0d6bbe3695bcbaa348b73c48737bdbfaeaea2272b0105dd4bdfa7a804f18: while extracting image: root filesystem extraction failed: extract command failed: ERROR : Failed to create user namespace: user namespace disabled. : exit status 1. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598
https://github.com/google/deepvariant/issues/598:1489,deployability,fail,failed,1489,"Thank you for your reply! 1、My machine doesn't have `/usr/bin/python3`.I don't have permission to change `/usr/bin/`.I tried to modify the source code, but I couldn't locate the code. 2、I tried to install Singularity. I use the parameter `--without -- seccomp --without -- conmon` because of the following error. `./mconfig --without-suid --without-seccomp --without-conmon --prefix=/path/singularity && make -C ./builddir &&make -C ./builddir install`. ```. seccomp headers are required to build Singularity with seccomp support. To disable seccomp support run mconfig using '--without-seccomp'. Cannot build conmon for OCI support without libseccomp headers. Use --without-conmon to disable build and use conmon on PATH if present. ```. Then I try to run it. ```. singularity run -B /path/locale/:/path/locale/ \. > docker://google/deepvariant:""1.4.0"" \. > /path/dpv_singu \. > --model_type=PACBIO \. > --ref=/path/ref_fasta/QJref.fa \. > --reads=/path/bam_files/F1N_sorted.merged.addg.uniq.rmdup.bam \. > --output_vcf=/path/output.vcf.gz \. > --output_gvcf=/path/output.g.vcf.gz \. > --intermediate_results_dir /path/intermediate_results_dir. ```. The error information is as follows. ```. INFO: Using cached SIF image. INFO: Converting SIF file to temporary sandbox... FATAL: while handling /home/my_user_name/.singularity/cache/oci-tmp/sha256.83ce0d6bbe3695bcbaa348b73c48737bdbfaeaea2272b0105dd4bdfa7a804f18: while extracting image: root filesystem extraction failed: extract command failed: ERROR : Failed to create user namespace: user namespace disabled. : exit status 1. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598
https://github.com/google/deepvariant/issues/598:1505,deployability,Fail,Failed,1505,"Thank you for your reply! 1、My machine doesn't have `/usr/bin/python3`.I don't have permission to change `/usr/bin/`.I tried to modify the source code, but I couldn't locate the code. 2、I tried to install Singularity. I use the parameter `--without -- seccomp --without -- conmon` because of the following error. `./mconfig --without-suid --without-seccomp --without-conmon --prefix=/path/singularity && make -C ./builddir &&make -C ./builddir install`. ```. seccomp headers are required to build Singularity with seccomp support. To disable seccomp support run mconfig using '--without-seccomp'. Cannot build conmon for OCI support without libseccomp headers. Use --without-conmon to disable build and use conmon on PATH if present. ```. Then I try to run it. ```. singularity run -B /path/locale/:/path/locale/ \. > docker://google/deepvariant:""1.4.0"" \. > /path/dpv_singu \. > --model_type=PACBIO \. > --ref=/path/ref_fasta/QJref.fa \. > --reads=/path/bam_files/F1N_sorted.merged.addg.uniq.rmdup.bam \. > --output_vcf=/path/output.vcf.gz \. > --output_gvcf=/path/output.g.vcf.gz \. > --intermediate_results_dir /path/intermediate_results_dir. ```. The error information is as follows. ```. INFO: Using cached SIF image. INFO: Converting SIF file to temporary sandbox... FATAL: while handling /home/my_user_name/.singularity/cache/oci-tmp/sha256.83ce0d6bbe3695bcbaa348b73c48737bdbfaeaea2272b0105dd4bdfa7a804f18: while extracting image: root filesystem extraction failed: extract command failed: ERROR : Failed to create user namespace: user namespace disabled. : exit status 1. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598
https://github.com/google/deepvariant/issues/598:228,modifiability,paramet,parameter,228,"Thank you for your reply! 1、My machine doesn't have `/usr/bin/python3`.I don't have permission to change `/usr/bin/`.I tried to modify the source code, but I couldn't locate the code. 2、I tried to install Singularity. I use the parameter `--without -- seccomp --without -- conmon` because of the following error. `./mconfig --without-suid --without-seccomp --without-conmon --prefix=/path/singularity && make -C ./builddir &&make -C ./builddir install`. ```. seccomp headers are required to build Singularity with seccomp support. To disable seccomp support run mconfig using '--without-seccomp'. Cannot build conmon for OCI support without libseccomp headers. Use --without-conmon to disable build and use conmon on PATH if present. ```. Then I try to run it. ```. singularity run -B /path/locale/:/path/locale/ \. > docker://google/deepvariant:""1.4.0"" \. > /path/dpv_singu \. > --model_type=PACBIO \. > --ref=/path/ref_fasta/QJref.fa \. > --reads=/path/bam_files/F1N_sorted.merged.addg.uniq.rmdup.bam \. > --output_vcf=/path/output.vcf.gz \. > --output_gvcf=/path/output.g.vcf.gz \. > --intermediate_results_dir /path/intermediate_results_dir. ```. The error information is as follows. ```. INFO: Using cached SIF image. INFO: Converting SIF file to temporary sandbox... FATAL: while handling /home/my_user_name/.singularity/cache/oci-tmp/sha256.83ce0d6bbe3695bcbaa348b73c48737bdbfaeaea2272b0105dd4bdfa7a804f18: while extracting image: root filesystem extraction failed: extract command failed: ERROR : Failed to create user namespace: user namespace disabled. : exit status 1. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598
https://github.com/google/deepvariant/issues/598:893,modifiability,PAC,PACBIO,893,"Thank you for your reply! 1、My machine doesn't have `/usr/bin/python3`.I don't have permission to change `/usr/bin/`.I tried to modify the source code, but I couldn't locate the code. 2、I tried to install Singularity. I use the parameter `--without -- seccomp --without -- conmon` because of the following error. `./mconfig --without-suid --without-seccomp --without-conmon --prefix=/path/singularity && make -C ./builddir &&make -C ./builddir install`. ```. seccomp headers are required to build Singularity with seccomp support. To disable seccomp support run mconfig using '--without-seccomp'. Cannot build conmon for OCI support without libseccomp headers. Use --without-conmon to disable build and use conmon on PATH if present. ```. Then I try to run it. ```. singularity run -B /path/locale/:/path/locale/ \. > docker://google/deepvariant:""1.4.0"" \. > /path/dpv_singu \. > --model_type=PACBIO \. > --ref=/path/ref_fasta/QJref.fa \. > --reads=/path/bam_files/F1N_sorted.merged.addg.uniq.rmdup.bam \. > --output_vcf=/path/output.vcf.gz \. > --output_gvcf=/path/output.g.vcf.gz \. > --intermediate_results_dir /path/intermediate_results_dir. ```. The error information is as follows. ```. INFO: Using cached SIF image. INFO: Converting SIF file to temporary sandbox... FATAL: while handling /home/my_user_name/.singularity/cache/oci-tmp/sha256.83ce0d6bbe3695bcbaa348b73c48737bdbfaeaea2272b0105dd4bdfa7a804f18: while extracting image: root filesystem extraction failed: extract command failed: ERROR : Failed to create user namespace: user namespace disabled. : exit status 1. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598
https://github.com/google/deepvariant/issues/598:306,performance,error,error,306,"Thank you for your reply! 1、My machine doesn't have `/usr/bin/python3`.I don't have permission to change `/usr/bin/`.I tried to modify the source code, but I couldn't locate the code. 2、I tried to install Singularity. I use the parameter `--without -- seccomp --without -- conmon` because of the following error. `./mconfig --without-suid --without-seccomp --without-conmon --prefix=/path/singularity && make -C ./builddir &&make -C ./builddir install`. ```. seccomp headers are required to build Singularity with seccomp support. To disable seccomp support run mconfig using '--without-seccomp'. Cannot build conmon for OCI support without libseccomp headers. Use --without-conmon to disable build and use conmon on PATH if present. ```. Then I try to run it. ```. singularity run -B /path/locale/:/path/locale/ \. > docker://google/deepvariant:""1.4.0"" \. > /path/dpv_singu \. > --model_type=PACBIO \. > --ref=/path/ref_fasta/QJref.fa \. > --reads=/path/bam_files/F1N_sorted.merged.addg.uniq.rmdup.bam \. > --output_vcf=/path/output.vcf.gz \. > --output_gvcf=/path/output.g.vcf.gz \. > --intermediate_results_dir /path/intermediate_results_dir. ```. The error information is as follows. ```. INFO: Using cached SIF image. INFO: Converting SIF file to temporary sandbox... FATAL: while handling /home/my_user_name/.singularity/cache/oci-tmp/sha256.83ce0d6bbe3695bcbaa348b73c48737bdbfaeaea2272b0105dd4bdfa7a804f18: while extracting image: root filesystem extraction failed: extract command failed: ERROR : Failed to create user namespace: user namespace disabled. : exit status 1. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598
https://github.com/google/deepvariant/issues/598:1155,performance,error,error,1155,"Thank you for your reply! 1、My machine doesn't have `/usr/bin/python3`.I don't have permission to change `/usr/bin/`.I tried to modify the source code, but I couldn't locate the code. 2、I tried to install Singularity. I use the parameter `--without -- seccomp --without -- conmon` because of the following error. `./mconfig --without-suid --without-seccomp --without-conmon --prefix=/path/singularity && make -C ./builddir &&make -C ./builddir install`. ```. seccomp headers are required to build Singularity with seccomp support. To disable seccomp support run mconfig using '--without-seccomp'. Cannot build conmon for OCI support without libseccomp headers. Use --without-conmon to disable build and use conmon on PATH if present. ```. Then I try to run it. ```. singularity run -B /path/locale/:/path/locale/ \. > docker://google/deepvariant:""1.4.0"" \. > /path/dpv_singu \. > --model_type=PACBIO \. > --ref=/path/ref_fasta/QJref.fa \. > --reads=/path/bam_files/F1N_sorted.merged.addg.uniq.rmdup.bam \. > --output_vcf=/path/output.vcf.gz \. > --output_gvcf=/path/output.g.vcf.gz \. > --intermediate_results_dir /path/intermediate_results_dir. ```. The error information is as follows. ```. INFO: Using cached SIF image. INFO: Converting SIF file to temporary sandbox... FATAL: while handling /home/my_user_name/.singularity/cache/oci-tmp/sha256.83ce0d6bbe3695bcbaa348b73c48737bdbfaeaea2272b0105dd4bdfa7a804f18: while extracting image: root filesystem extraction failed: extract command failed: ERROR : Failed to create user namespace: user namespace disabled. : exit status 1. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598
https://github.com/google/deepvariant/issues/598:1205,performance,cach,cached,1205,"Thank you for your reply! 1、My machine doesn't have `/usr/bin/python3`.I don't have permission to change `/usr/bin/`.I tried to modify the source code, but I couldn't locate the code. 2、I tried to install Singularity. I use the parameter `--without -- seccomp --without -- conmon` because of the following error. `./mconfig --without-suid --without-seccomp --without-conmon --prefix=/path/singularity && make -C ./builddir &&make -C ./builddir install`. ```. seccomp headers are required to build Singularity with seccomp support. To disable seccomp support run mconfig using '--without-seccomp'. Cannot build conmon for OCI support without libseccomp headers. Use --without-conmon to disable build and use conmon on PATH if present. ```. Then I try to run it. ```. singularity run -B /path/locale/:/path/locale/ \. > docker://google/deepvariant:""1.4.0"" \. > /path/dpv_singu \. > --model_type=PACBIO \. > --ref=/path/ref_fasta/QJref.fa \. > --reads=/path/bam_files/F1N_sorted.merged.addg.uniq.rmdup.bam \. > --output_vcf=/path/output.vcf.gz \. > --output_gvcf=/path/output.g.vcf.gz \. > --intermediate_results_dir /path/intermediate_results_dir. ```. The error information is as follows. ```. INFO: Using cached SIF image. INFO: Converting SIF file to temporary sandbox... FATAL: while handling /home/my_user_name/.singularity/cache/oci-tmp/sha256.83ce0d6bbe3695bcbaa348b73c48737bdbfaeaea2272b0105dd4bdfa7a804f18: while extracting image: root filesystem extraction failed: extract command failed: ERROR : Failed to create user namespace: user namespace disabled. : exit status 1. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598
https://github.com/google/deepvariant/issues/598:1327,performance,cach,cache,1327,"Thank you for your reply! 1、My machine doesn't have `/usr/bin/python3`.I don't have permission to change `/usr/bin/`.I tried to modify the source code, but I couldn't locate the code. 2、I tried to install Singularity. I use the parameter `--without -- seccomp --without -- conmon` because of the following error. `./mconfig --without-suid --without-seccomp --without-conmon --prefix=/path/singularity && make -C ./builddir &&make -C ./builddir install`. ```. seccomp headers are required to build Singularity with seccomp support. To disable seccomp support run mconfig using '--without-seccomp'. Cannot build conmon for OCI support without libseccomp headers. Use --without-conmon to disable build and use conmon on PATH if present. ```. Then I try to run it. ```. singularity run -B /path/locale/:/path/locale/ \. > docker://google/deepvariant:""1.4.0"" \. > /path/dpv_singu \. > --model_type=PACBIO \. > --ref=/path/ref_fasta/QJref.fa \. > --reads=/path/bam_files/F1N_sorted.merged.addg.uniq.rmdup.bam \. > --output_vcf=/path/output.vcf.gz \. > --output_gvcf=/path/output.g.vcf.gz \. > --intermediate_results_dir /path/intermediate_results_dir. ```. The error information is as follows. ```. INFO: Using cached SIF image. INFO: Converting SIF file to temporary sandbox... FATAL: while handling /home/my_user_name/.singularity/cache/oci-tmp/sha256.83ce0d6bbe3695bcbaa348b73c48737bdbfaeaea2272b0105dd4bdfa7a804f18: while extracting image: root filesystem extraction failed: extract command failed: ERROR : Failed to create user namespace: user namespace disabled. : exit status 1. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598
https://github.com/google/deepvariant/issues/598:1497,performance,ERROR,ERROR,1497,"Thank you for your reply! 1、My machine doesn't have `/usr/bin/python3`.I don't have permission to change `/usr/bin/`.I tried to modify the source code, but I couldn't locate the code. 2、I tried to install Singularity. I use the parameter `--without -- seccomp --without -- conmon` because of the following error. `./mconfig --without-suid --without-seccomp --without-conmon --prefix=/path/singularity && make -C ./builddir &&make -C ./builddir install`. ```. seccomp headers are required to build Singularity with seccomp support. To disable seccomp support run mconfig using '--without-seccomp'. Cannot build conmon for OCI support without libseccomp headers. Use --without-conmon to disable build and use conmon on PATH if present. ```. Then I try to run it. ```. singularity run -B /path/locale/:/path/locale/ \. > docker://google/deepvariant:""1.4.0"" \. > /path/dpv_singu \. > --model_type=PACBIO \. > --ref=/path/ref_fasta/QJref.fa \. > --reads=/path/bam_files/F1N_sorted.merged.addg.uniq.rmdup.bam \. > --output_vcf=/path/output.vcf.gz \. > --output_gvcf=/path/output.g.vcf.gz \. > --intermediate_results_dir /path/intermediate_results_dir. ```. The error information is as follows. ```. INFO: Using cached SIF image. INFO: Converting SIF file to temporary sandbox... FATAL: while handling /home/my_user_name/.singularity/cache/oci-tmp/sha256.83ce0d6bbe3695bcbaa348b73c48737bdbfaeaea2272b0105dd4bdfa7a804f18: while extracting image: root filesystem extraction failed: extract command failed: ERROR : Failed to create user namespace: user namespace disabled. : exit status 1. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598
https://github.com/google/deepvariant/issues/598:39,reliability,doe,doesn,39,"Thank you for your reply! 1、My machine doesn't have `/usr/bin/python3`.I don't have permission to change `/usr/bin/`.I tried to modify the source code, but I couldn't locate the code. 2、I tried to install Singularity. I use the parameter `--without -- seccomp --without -- conmon` because of the following error. `./mconfig --without-suid --without-seccomp --without-conmon --prefix=/path/singularity && make -C ./builddir &&make -C ./builddir install`. ```. seccomp headers are required to build Singularity with seccomp support. To disable seccomp support run mconfig using '--without-seccomp'. Cannot build conmon for OCI support without libseccomp headers. Use --without-conmon to disable build and use conmon on PATH if present. ```. Then I try to run it. ```. singularity run -B /path/locale/:/path/locale/ \. > docker://google/deepvariant:""1.4.0"" \. > /path/dpv_singu \. > --model_type=PACBIO \. > --ref=/path/ref_fasta/QJref.fa \. > --reads=/path/bam_files/F1N_sorted.merged.addg.uniq.rmdup.bam \. > --output_vcf=/path/output.vcf.gz \. > --output_gvcf=/path/output.g.vcf.gz \. > --intermediate_results_dir /path/intermediate_results_dir. ```. The error information is as follows. ```. INFO: Using cached SIF image. INFO: Converting SIF file to temporary sandbox... FATAL: while handling /home/my_user_name/.singularity/cache/oci-tmp/sha256.83ce0d6bbe3695bcbaa348b73c48737bdbfaeaea2272b0105dd4bdfa7a804f18: while extracting image: root filesystem extraction failed: extract command failed: ERROR : Failed to create user namespace: user namespace disabled. : exit status 1. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598
https://github.com/google/deepvariant/issues/598:1465,reliability,fail,failed,1465,"Thank you for your reply! 1、My machine doesn't have `/usr/bin/python3`.I don't have permission to change `/usr/bin/`.I tried to modify the source code, but I couldn't locate the code. 2、I tried to install Singularity. I use the parameter `--without -- seccomp --without -- conmon` because of the following error. `./mconfig --without-suid --without-seccomp --without-conmon --prefix=/path/singularity && make -C ./builddir &&make -C ./builddir install`. ```. seccomp headers are required to build Singularity with seccomp support. To disable seccomp support run mconfig using '--without-seccomp'. Cannot build conmon for OCI support without libseccomp headers. Use --without-conmon to disable build and use conmon on PATH if present. ```. Then I try to run it. ```. singularity run -B /path/locale/:/path/locale/ \. > docker://google/deepvariant:""1.4.0"" \. > /path/dpv_singu \. > --model_type=PACBIO \. > --ref=/path/ref_fasta/QJref.fa \. > --reads=/path/bam_files/F1N_sorted.merged.addg.uniq.rmdup.bam \. > --output_vcf=/path/output.vcf.gz \. > --output_gvcf=/path/output.g.vcf.gz \. > --intermediate_results_dir /path/intermediate_results_dir. ```. The error information is as follows. ```. INFO: Using cached SIF image. INFO: Converting SIF file to temporary sandbox... FATAL: while handling /home/my_user_name/.singularity/cache/oci-tmp/sha256.83ce0d6bbe3695bcbaa348b73c48737bdbfaeaea2272b0105dd4bdfa7a804f18: while extracting image: root filesystem extraction failed: extract command failed: ERROR : Failed to create user namespace: user namespace disabled. : exit status 1. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598
https://github.com/google/deepvariant/issues/598:1489,reliability,fail,failed,1489,"Thank you for your reply! 1、My machine doesn't have `/usr/bin/python3`.I don't have permission to change `/usr/bin/`.I tried to modify the source code, but I couldn't locate the code. 2、I tried to install Singularity. I use the parameter `--without -- seccomp --without -- conmon` because of the following error. `./mconfig --without-suid --without-seccomp --without-conmon --prefix=/path/singularity && make -C ./builddir &&make -C ./builddir install`. ```. seccomp headers are required to build Singularity with seccomp support. To disable seccomp support run mconfig using '--without-seccomp'. Cannot build conmon for OCI support without libseccomp headers. Use --without-conmon to disable build and use conmon on PATH if present. ```. Then I try to run it. ```. singularity run -B /path/locale/:/path/locale/ \. > docker://google/deepvariant:""1.4.0"" \. > /path/dpv_singu \. > --model_type=PACBIO \. > --ref=/path/ref_fasta/QJref.fa \. > --reads=/path/bam_files/F1N_sorted.merged.addg.uniq.rmdup.bam \. > --output_vcf=/path/output.vcf.gz \. > --output_gvcf=/path/output.g.vcf.gz \. > --intermediate_results_dir /path/intermediate_results_dir. ```. The error information is as follows. ```. INFO: Using cached SIF image. INFO: Converting SIF file to temporary sandbox... FATAL: while handling /home/my_user_name/.singularity/cache/oci-tmp/sha256.83ce0d6bbe3695bcbaa348b73c48737bdbfaeaea2272b0105dd4bdfa7a804f18: while extracting image: root filesystem extraction failed: extract command failed: ERROR : Failed to create user namespace: user namespace disabled. : exit status 1. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598
https://github.com/google/deepvariant/issues/598:1505,reliability,Fail,Failed,1505,"Thank you for your reply! 1、My machine doesn't have `/usr/bin/python3`.I don't have permission to change `/usr/bin/`.I tried to modify the source code, but I couldn't locate the code. 2、I tried to install Singularity. I use the parameter `--without -- seccomp --without -- conmon` because of the following error. `./mconfig --without-suid --without-seccomp --without-conmon --prefix=/path/singularity && make -C ./builddir &&make -C ./builddir install`. ```. seccomp headers are required to build Singularity with seccomp support. To disable seccomp support run mconfig using '--without-seccomp'. Cannot build conmon for OCI support without libseccomp headers. Use --without-conmon to disable build and use conmon on PATH if present. ```. Then I try to run it. ```. singularity run -B /path/locale/:/path/locale/ \. > docker://google/deepvariant:""1.4.0"" \. > /path/dpv_singu \. > --model_type=PACBIO \. > --ref=/path/ref_fasta/QJref.fa \. > --reads=/path/bam_files/F1N_sorted.merged.addg.uniq.rmdup.bam \. > --output_vcf=/path/output.vcf.gz \. > --output_gvcf=/path/output.g.vcf.gz \. > --intermediate_results_dir /path/intermediate_results_dir. ```. The error information is as follows. ```. INFO: Using cached SIF image. INFO: Converting SIF file to temporary sandbox... FATAL: while handling /home/my_user_name/.singularity/cache/oci-tmp/sha256.83ce0d6bbe3695bcbaa348b73c48737bdbfaeaea2272b0105dd4bdfa7a804f18: while extracting image: root filesystem extraction failed: extract command failed: ERROR : Failed to create user namespace: user namespace disabled. : exit status 1. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598
https://github.com/google/deepvariant/issues/598:84,safety,permiss,permission,84,"Thank you for your reply! 1、My machine doesn't have `/usr/bin/python3`.I don't have permission to change `/usr/bin/`.I tried to modify the source code, but I couldn't locate the code. 2、I tried to install Singularity. I use the parameter `--without -- seccomp --without -- conmon` because of the following error. `./mconfig --without-suid --without-seccomp --without-conmon --prefix=/path/singularity && make -C ./builddir &&make -C ./builddir install`. ```. seccomp headers are required to build Singularity with seccomp support. To disable seccomp support run mconfig using '--without-seccomp'. Cannot build conmon for OCI support without libseccomp headers. Use --without-conmon to disable build and use conmon on PATH if present. ```. Then I try to run it. ```. singularity run -B /path/locale/:/path/locale/ \. > docker://google/deepvariant:""1.4.0"" \. > /path/dpv_singu \. > --model_type=PACBIO \. > --ref=/path/ref_fasta/QJref.fa \. > --reads=/path/bam_files/F1N_sorted.merged.addg.uniq.rmdup.bam \. > --output_vcf=/path/output.vcf.gz \. > --output_gvcf=/path/output.g.vcf.gz \. > --intermediate_results_dir /path/intermediate_results_dir. ```. The error information is as follows. ```. INFO: Using cached SIF image. INFO: Converting SIF file to temporary sandbox... FATAL: while handling /home/my_user_name/.singularity/cache/oci-tmp/sha256.83ce0d6bbe3695bcbaa348b73c48737bdbfaeaea2272b0105dd4bdfa7a804f18: while extracting image: root filesystem extraction failed: extract command failed: ERROR : Failed to create user namespace: user namespace disabled. : exit status 1. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598
https://github.com/google/deepvariant/issues/598:306,safety,error,error,306,"Thank you for your reply! 1、My machine doesn't have `/usr/bin/python3`.I don't have permission to change `/usr/bin/`.I tried to modify the source code, but I couldn't locate the code. 2、I tried to install Singularity. I use the parameter `--without -- seccomp --without -- conmon` because of the following error. `./mconfig --without-suid --without-seccomp --without-conmon --prefix=/path/singularity && make -C ./builddir &&make -C ./builddir install`. ```. seccomp headers are required to build Singularity with seccomp support. To disable seccomp support run mconfig using '--without-seccomp'. Cannot build conmon for OCI support without libseccomp headers. Use --without-conmon to disable build and use conmon on PATH if present. ```. Then I try to run it. ```. singularity run -B /path/locale/:/path/locale/ \. > docker://google/deepvariant:""1.4.0"" \. > /path/dpv_singu \. > --model_type=PACBIO \. > --ref=/path/ref_fasta/QJref.fa \. > --reads=/path/bam_files/F1N_sorted.merged.addg.uniq.rmdup.bam \. > --output_vcf=/path/output.vcf.gz \. > --output_gvcf=/path/output.g.vcf.gz \. > --intermediate_results_dir /path/intermediate_results_dir. ```. The error information is as follows. ```. INFO: Using cached SIF image. INFO: Converting SIF file to temporary sandbox... FATAL: while handling /home/my_user_name/.singularity/cache/oci-tmp/sha256.83ce0d6bbe3695bcbaa348b73c48737bdbfaeaea2272b0105dd4bdfa7a804f18: while extracting image: root filesystem extraction failed: extract command failed: ERROR : Failed to create user namespace: user namespace disabled. : exit status 1. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598
https://github.com/google/deepvariant/issues/598:1155,safety,error,error,1155,"Thank you for your reply! 1、My machine doesn't have `/usr/bin/python3`.I don't have permission to change `/usr/bin/`.I tried to modify the source code, but I couldn't locate the code. 2、I tried to install Singularity. I use the parameter `--without -- seccomp --without -- conmon` because of the following error. `./mconfig --without-suid --without-seccomp --without-conmon --prefix=/path/singularity && make -C ./builddir &&make -C ./builddir install`. ```. seccomp headers are required to build Singularity with seccomp support. To disable seccomp support run mconfig using '--without-seccomp'. Cannot build conmon for OCI support without libseccomp headers. Use --without-conmon to disable build and use conmon on PATH if present. ```. Then I try to run it. ```. singularity run -B /path/locale/:/path/locale/ \. > docker://google/deepvariant:""1.4.0"" \. > /path/dpv_singu \. > --model_type=PACBIO \. > --ref=/path/ref_fasta/QJref.fa \. > --reads=/path/bam_files/F1N_sorted.merged.addg.uniq.rmdup.bam \. > --output_vcf=/path/output.vcf.gz \. > --output_gvcf=/path/output.g.vcf.gz \. > --intermediate_results_dir /path/intermediate_results_dir. ```. The error information is as follows. ```. INFO: Using cached SIF image. INFO: Converting SIF file to temporary sandbox... FATAL: while handling /home/my_user_name/.singularity/cache/oci-tmp/sha256.83ce0d6bbe3695bcbaa348b73c48737bdbfaeaea2272b0105dd4bdfa7a804f18: while extracting image: root filesystem extraction failed: extract command failed: ERROR : Failed to create user namespace: user namespace disabled. : exit status 1. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598
https://github.com/google/deepvariant/issues/598:1497,safety,ERROR,ERROR,1497,"Thank you for your reply! 1、My machine doesn't have `/usr/bin/python3`.I don't have permission to change `/usr/bin/`.I tried to modify the source code, but I couldn't locate the code. 2、I tried to install Singularity. I use the parameter `--without -- seccomp --without -- conmon` because of the following error. `./mconfig --without-suid --without-seccomp --without-conmon --prefix=/path/singularity && make -C ./builddir &&make -C ./builddir install`. ```. seccomp headers are required to build Singularity with seccomp support. To disable seccomp support run mconfig using '--without-seccomp'. Cannot build conmon for OCI support without libseccomp headers. Use --without-conmon to disable build and use conmon on PATH if present. ```. Then I try to run it. ```. singularity run -B /path/locale/:/path/locale/ \. > docker://google/deepvariant:""1.4.0"" \. > /path/dpv_singu \. > --model_type=PACBIO \. > --ref=/path/ref_fasta/QJref.fa \. > --reads=/path/bam_files/F1N_sorted.merged.addg.uniq.rmdup.bam \. > --output_vcf=/path/output.vcf.gz \. > --output_gvcf=/path/output.g.vcf.gz \. > --intermediate_results_dir /path/intermediate_results_dir. ```. The error information is as follows. ```. INFO: Using cached SIF image. INFO: Converting SIF file to temporary sandbox... FATAL: while handling /home/my_user_name/.singularity/cache/oci-tmp/sha256.83ce0d6bbe3695bcbaa348b73c48737bdbfaeaea2272b0105dd4bdfa7a804f18: while extracting image: root filesystem extraction failed: extract command failed: ERROR : Failed to create user namespace: user namespace disabled. : exit status 1. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598
https://github.com/google/deepvariant/issues/598:128,security,modif,modify,128,"Thank you for your reply! 1、My machine doesn't have `/usr/bin/python3`.I don't have permission to change `/usr/bin/`.I tried to modify the source code, but I couldn't locate the code. 2、I tried to install Singularity. I use the parameter `--without -- seccomp --without -- conmon` because of the following error. `./mconfig --without-suid --without-seccomp --without-conmon --prefix=/path/singularity && make -C ./builddir &&make -C ./builddir install`. ```. seccomp headers are required to build Singularity with seccomp support. To disable seccomp support run mconfig using '--without-seccomp'. Cannot build conmon for OCI support without libseccomp headers. Use --without-conmon to disable build and use conmon on PATH if present. ```. Then I try to run it. ```. singularity run -B /path/locale/:/path/locale/ \. > docker://google/deepvariant:""1.4.0"" \. > /path/dpv_singu \. > --model_type=PACBIO \. > --ref=/path/ref_fasta/QJref.fa \. > --reads=/path/bam_files/F1N_sorted.merged.addg.uniq.rmdup.bam \. > --output_vcf=/path/output.vcf.gz \. > --output_gvcf=/path/output.g.vcf.gz \. > --intermediate_results_dir /path/intermediate_results_dir. ```. The error information is as follows. ```. INFO: Using cached SIF image. INFO: Converting SIF file to temporary sandbox... FATAL: while handling /home/my_user_name/.singularity/cache/oci-tmp/sha256.83ce0d6bbe3695bcbaa348b73c48737bdbfaeaea2272b0105dd4bdfa7a804f18: while extracting image: root filesystem extraction failed: extract command failed: ERROR : Failed to create user namespace: user namespace disabled. : exit status 1. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598
https://github.com/google/deepvariant/issues/598:1262,security,sandbox,sandbox,1262,"Thank you for your reply! 1、My machine doesn't have `/usr/bin/python3`.I don't have permission to change `/usr/bin/`.I tried to modify the source code, but I couldn't locate the code. 2、I tried to install Singularity. I use the parameter `--without -- seccomp --without -- conmon` because of the following error. `./mconfig --without-suid --without-seccomp --without-conmon --prefix=/path/singularity && make -C ./builddir &&make -C ./builddir install`. ```. seccomp headers are required to build Singularity with seccomp support. To disable seccomp support run mconfig using '--without-seccomp'. Cannot build conmon for OCI support without libseccomp headers. Use --without-conmon to disable build and use conmon on PATH if present. ```. Then I try to run it. ```. singularity run -B /path/locale/:/path/locale/ \. > docker://google/deepvariant:""1.4.0"" \. > /path/dpv_singu \. > --model_type=PACBIO \. > --ref=/path/ref_fasta/QJref.fa \. > --reads=/path/bam_files/F1N_sorted.merged.addg.uniq.rmdup.bam \. > --output_vcf=/path/output.vcf.gz \. > --output_gvcf=/path/output.g.vcf.gz \. > --intermediate_results_dir /path/intermediate_results_dir. ```. The error information is as follows. ```. INFO: Using cached SIF image. INFO: Converting SIF file to temporary sandbox... FATAL: while handling /home/my_user_name/.singularity/cache/oci-tmp/sha256.83ce0d6bbe3695bcbaa348b73c48737bdbfaeaea2272b0105dd4bdfa7a804f18: while extracting image: root filesystem extraction failed: extract command failed: ERROR : Failed to create user namespace: user namespace disabled. : exit status 1. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598
https://github.com/google/deepvariant/issues/598:306,usability,error,error,306,"Thank you for your reply! 1、My machine doesn't have `/usr/bin/python3`.I don't have permission to change `/usr/bin/`.I tried to modify the source code, but I couldn't locate the code. 2、I tried to install Singularity. I use the parameter `--without -- seccomp --without -- conmon` because of the following error. `./mconfig --without-suid --without-seccomp --without-conmon --prefix=/path/singularity && make -C ./builddir &&make -C ./builddir install`. ```. seccomp headers are required to build Singularity with seccomp support. To disable seccomp support run mconfig using '--without-seccomp'. Cannot build conmon for OCI support without libseccomp headers. Use --without-conmon to disable build and use conmon on PATH if present. ```. Then I try to run it. ```. singularity run -B /path/locale/:/path/locale/ \. > docker://google/deepvariant:""1.4.0"" \. > /path/dpv_singu \. > --model_type=PACBIO \. > --ref=/path/ref_fasta/QJref.fa \. > --reads=/path/bam_files/F1N_sorted.merged.addg.uniq.rmdup.bam \. > --output_vcf=/path/output.vcf.gz \. > --output_gvcf=/path/output.g.vcf.gz \. > --intermediate_results_dir /path/intermediate_results_dir. ```. The error information is as follows. ```. INFO: Using cached SIF image. INFO: Converting SIF file to temporary sandbox... FATAL: while handling /home/my_user_name/.singularity/cache/oci-tmp/sha256.83ce0d6bbe3695bcbaa348b73c48737bdbfaeaea2272b0105dd4bdfa7a804f18: while extracting image: root filesystem extraction failed: extract command failed: ERROR : Failed to create user namespace: user namespace disabled. : exit status 1. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598
https://github.com/google/deepvariant/issues/598:522,usability,support,support,522,"Thank you for your reply! 1、My machine doesn't have `/usr/bin/python3`.I don't have permission to change `/usr/bin/`.I tried to modify the source code, but I couldn't locate the code. 2、I tried to install Singularity. I use the parameter `--without -- seccomp --without -- conmon` because of the following error. `./mconfig --without-suid --without-seccomp --without-conmon --prefix=/path/singularity && make -C ./builddir &&make -C ./builddir install`. ```. seccomp headers are required to build Singularity with seccomp support. To disable seccomp support run mconfig using '--without-seccomp'. Cannot build conmon for OCI support without libseccomp headers. Use --without-conmon to disable build and use conmon on PATH if present. ```. Then I try to run it. ```. singularity run -B /path/locale/:/path/locale/ \. > docker://google/deepvariant:""1.4.0"" \. > /path/dpv_singu \. > --model_type=PACBIO \. > --ref=/path/ref_fasta/QJref.fa \. > --reads=/path/bam_files/F1N_sorted.merged.addg.uniq.rmdup.bam \. > --output_vcf=/path/output.vcf.gz \. > --output_gvcf=/path/output.g.vcf.gz \. > --intermediate_results_dir /path/intermediate_results_dir. ```. The error information is as follows. ```. INFO: Using cached SIF image. INFO: Converting SIF file to temporary sandbox... FATAL: while handling /home/my_user_name/.singularity/cache/oci-tmp/sha256.83ce0d6bbe3695bcbaa348b73c48737bdbfaeaea2272b0105dd4bdfa7a804f18: while extracting image: root filesystem extraction failed: extract command failed: ERROR : Failed to create user namespace: user namespace disabled. : exit status 1. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598
https://github.com/google/deepvariant/issues/598:550,usability,support,support,550,"Thank you for your reply! 1、My machine doesn't have `/usr/bin/python3`.I don't have permission to change `/usr/bin/`.I tried to modify the source code, but I couldn't locate the code. 2、I tried to install Singularity. I use the parameter `--without -- seccomp --without -- conmon` because of the following error. `./mconfig --without-suid --without-seccomp --without-conmon --prefix=/path/singularity && make -C ./builddir &&make -C ./builddir install`. ```. seccomp headers are required to build Singularity with seccomp support. To disable seccomp support run mconfig using '--without-seccomp'. Cannot build conmon for OCI support without libseccomp headers. Use --without-conmon to disable build and use conmon on PATH if present. ```. Then I try to run it. ```. singularity run -B /path/locale/:/path/locale/ \. > docker://google/deepvariant:""1.4.0"" \. > /path/dpv_singu \. > --model_type=PACBIO \. > --ref=/path/ref_fasta/QJref.fa \. > --reads=/path/bam_files/F1N_sorted.merged.addg.uniq.rmdup.bam \. > --output_vcf=/path/output.vcf.gz \. > --output_gvcf=/path/output.g.vcf.gz \. > --intermediate_results_dir /path/intermediate_results_dir. ```. The error information is as follows. ```. INFO: Using cached SIF image. INFO: Converting SIF file to temporary sandbox... FATAL: while handling /home/my_user_name/.singularity/cache/oci-tmp/sha256.83ce0d6bbe3695bcbaa348b73c48737bdbfaeaea2272b0105dd4bdfa7a804f18: while extracting image: root filesystem extraction failed: extract command failed: ERROR : Failed to create user namespace: user namespace disabled. : exit status 1. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598
https://github.com/google/deepvariant/issues/598:625,usability,support,support,625,"Thank you for your reply! 1、My machine doesn't have `/usr/bin/python3`.I don't have permission to change `/usr/bin/`.I tried to modify the source code, but I couldn't locate the code. 2、I tried to install Singularity. I use the parameter `--without -- seccomp --without -- conmon` because of the following error. `./mconfig --without-suid --without-seccomp --without-conmon --prefix=/path/singularity && make -C ./builddir &&make -C ./builddir install`. ```. seccomp headers are required to build Singularity with seccomp support. To disable seccomp support run mconfig using '--without-seccomp'. Cannot build conmon for OCI support without libseccomp headers. Use --without-conmon to disable build and use conmon on PATH if present. ```. Then I try to run it. ```. singularity run -B /path/locale/:/path/locale/ \. > docker://google/deepvariant:""1.4.0"" \. > /path/dpv_singu \. > --model_type=PACBIO \. > --ref=/path/ref_fasta/QJref.fa \. > --reads=/path/bam_files/F1N_sorted.merged.addg.uniq.rmdup.bam \. > --output_vcf=/path/output.vcf.gz \. > --output_gvcf=/path/output.g.vcf.gz \. > --intermediate_results_dir /path/intermediate_results_dir. ```. The error information is as follows. ```. INFO: Using cached SIF image. INFO: Converting SIF file to temporary sandbox... FATAL: while handling /home/my_user_name/.singularity/cache/oci-tmp/sha256.83ce0d6bbe3695bcbaa348b73c48737bdbfaeaea2272b0105dd4bdfa7a804f18: while extracting image: root filesystem extraction failed: extract command failed: ERROR : Failed to create user namespace: user namespace disabled. : exit status 1. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598
https://github.com/google/deepvariant/issues/598:1155,usability,error,error,1155,"Thank you for your reply! 1、My machine doesn't have `/usr/bin/python3`.I don't have permission to change `/usr/bin/`.I tried to modify the source code, but I couldn't locate the code. 2、I tried to install Singularity. I use the parameter `--without -- seccomp --without -- conmon` because of the following error. `./mconfig --without-suid --without-seccomp --without-conmon --prefix=/path/singularity && make -C ./builddir &&make -C ./builddir install`. ```. seccomp headers are required to build Singularity with seccomp support. To disable seccomp support run mconfig using '--without-seccomp'. Cannot build conmon for OCI support without libseccomp headers. Use --without-conmon to disable build and use conmon on PATH if present. ```. Then I try to run it. ```. singularity run -B /path/locale/:/path/locale/ \. > docker://google/deepvariant:""1.4.0"" \. > /path/dpv_singu \. > --model_type=PACBIO \. > --ref=/path/ref_fasta/QJref.fa \. > --reads=/path/bam_files/F1N_sorted.merged.addg.uniq.rmdup.bam \. > --output_vcf=/path/output.vcf.gz \. > --output_gvcf=/path/output.g.vcf.gz \. > --intermediate_results_dir /path/intermediate_results_dir. ```. The error information is as follows. ```. INFO: Using cached SIF image. INFO: Converting SIF file to temporary sandbox... FATAL: while handling /home/my_user_name/.singularity/cache/oci-tmp/sha256.83ce0d6bbe3695bcbaa348b73c48737bdbfaeaea2272b0105dd4bdfa7a804f18: while extracting image: root filesystem extraction failed: extract command failed: ERROR : Failed to create user namespace: user namespace disabled. : exit status 1. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598
https://github.com/google/deepvariant/issues/598:1481,usability,command,command,1481,"Thank you for your reply! 1、My machine doesn't have `/usr/bin/python3`.I don't have permission to change `/usr/bin/`.I tried to modify the source code, but I couldn't locate the code. 2、I tried to install Singularity. I use the parameter `--without -- seccomp --without -- conmon` because of the following error. `./mconfig --without-suid --without-seccomp --without-conmon --prefix=/path/singularity && make -C ./builddir &&make -C ./builddir install`. ```. seccomp headers are required to build Singularity with seccomp support. To disable seccomp support run mconfig using '--without-seccomp'. Cannot build conmon for OCI support without libseccomp headers. Use --without-conmon to disable build and use conmon on PATH if present. ```. Then I try to run it. ```. singularity run -B /path/locale/:/path/locale/ \. > docker://google/deepvariant:""1.4.0"" \. > /path/dpv_singu \. > --model_type=PACBIO \. > --ref=/path/ref_fasta/QJref.fa \. > --reads=/path/bam_files/F1N_sorted.merged.addg.uniq.rmdup.bam \. > --output_vcf=/path/output.vcf.gz \. > --output_gvcf=/path/output.g.vcf.gz \. > --intermediate_results_dir /path/intermediate_results_dir. ```. The error information is as follows. ```. INFO: Using cached SIF image. INFO: Converting SIF file to temporary sandbox... FATAL: while handling /home/my_user_name/.singularity/cache/oci-tmp/sha256.83ce0d6bbe3695bcbaa348b73c48737bdbfaeaea2272b0105dd4bdfa7a804f18: while extracting image: root filesystem extraction failed: extract command failed: ERROR : Failed to create user namespace: user namespace disabled. : exit status 1. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598
https://github.com/google/deepvariant/issues/598:1497,usability,ERROR,ERROR,1497,"Thank you for your reply! 1、My machine doesn't have `/usr/bin/python3`.I don't have permission to change `/usr/bin/`.I tried to modify the source code, but I couldn't locate the code. 2、I tried to install Singularity. I use the parameter `--without -- seccomp --without -- conmon` because of the following error. `./mconfig --without-suid --without-seccomp --without-conmon --prefix=/path/singularity && make -C ./builddir &&make -C ./builddir install`. ```. seccomp headers are required to build Singularity with seccomp support. To disable seccomp support run mconfig using '--without-seccomp'. Cannot build conmon for OCI support without libseccomp headers. Use --without-conmon to disable build and use conmon on PATH if present. ```. Then I try to run it. ```. singularity run -B /path/locale/:/path/locale/ \. > docker://google/deepvariant:""1.4.0"" \. > /path/dpv_singu \. > --model_type=PACBIO \. > --ref=/path/ref_fasta/QJref.fa \. > --reads=/path/bam_files/F1N_sorted.merged.addg.uniq.rmdup.bam \. > --output_vcf=/path/output.vcf.gz \. > --output_gvcf=/path/output.g.vcf.gz \. > --intermediate_results_dir /path/intermediate_results_dir. ```. The error information is as follows. ```. INFO: Using cached SIF image. INFO: Converting SIF file to temporary sandbox... FATAL: while handling /home/my_user_name/.singularity/cache/oci-tmp/sha256.83ce0d6bbe3695bcbaa348b73c48737bdbfaeaea2272b0105dd4bdfa7a804f18: while extracting image: root filesystem extraction failed: extract command failed: ERROR : Failed to create user namespace: user namespace disabled. : exit status 1. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598
https://github.com/google/deepvariant/issues/598:1522,usability,user,user,1522,"Thank you for your reply! 1、My machine doesn't have `/usr/bin/python3`.I don't have permission to change `/usr/bin/`.I tried to modify the source code, but I couldn't locate the code. 2、I tried to install Singularity. I use the parameter `--without -- seccomp --without -- conmon` because of the following error. `./mconfig --without-suid --without-seccomp --without-conmon --prefix=/path/singularity && make -C ./builddir &&make -C ./builddir install`. ```. seccomp headers are required to build Singularity with seccomp support. To disable seccomp support run mconfig using '--without-seccomp'. Cannot build conmon for OCI support without libseccomp headers. Use --without-conmon to disable build and use conmon on PATH if present. ```. Then I try to run it. ```. singularity run -B /path/locale/:/path/locale/ \. > docker://google/deepvariant:""1.4.0"" \. > /path/dpv_singu \. > --model_type=PACBIO \. > --ref=/path/ref_fasta/QJref.fa \. > --reads=/path/bam_files/F1N_sorted.merged.addg.uniq.rmdup.bam \. > --output_vcf=/path/output.vcf.gz \. > --output_gvcf=/path/output.g.vcf.gz \. > --intermediate_results_dir /path/intermediate_results_dir. ```. The error information is as follows. ```. INFO: Using cached SIF image. INFO: Converting SIF file to temporary sandbox... FATAL: while handling /home/my_user_name/.singularity/cache/oci-tmp/sha256.83ce0d6bbe3695bcbaa348b73c48737bdbfaeaea2272b0105dd4bdfa7a804f18: while extracting image: root filesystem extraction failed: extract command failed: ERROR : Failed to create user namespace: user namespace disabled. : exit status 1. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598
https://github.com/google/deepvariant/issues/598:1538,usability,user,user,1538,"Thank you for your reply! 1、My machine doesn't have `/usr/bin/python3`.I don't have permission to change `/usr/bin/`.I tried to modify the source code, but I couldn't locate the code. 2、I tried to install Singularity. I use the parameter `--without -- seccomp --without -- conmon` because of the following error. `./mconfig --without-suid --without-seccomp --without-conmon --prefix=/path/singularity && make -C ./builddir &&make -C ./builddir install`. ```. seccomp headers are required to build Singularity with seccomp support. To disable seccomp support run mconfig using '--without-seccomp'. Cannot build conmon for OCI support without libseccomp headers. Use --without-conmon to disable build and use conmon on PATH if present. ```. Then I try to run it. ```. singularity run -B /path/locale/:/path/locale/ \. > docker://google/deepvariant:""1.4.0"" \. > /path/dpv_singu \. > --model_type=PACBIO \. > --ref=/path/ref_fasta/QJref.fa \. > --reads=/path/bam_files/F1N_sorted.merged.addg.uniq.rmdup.bam \. > --output_vcf=/path/output.vcf.gz \. > --output_gvcf=/path/output.g.vcf.gz \. > --intermediate_results_dir /path/intermediate_results_dir. ```. The error information is as follows. ```. INFO: Using cached SIF image. INFO: Converting SIF file to temporary sandbox... FATAL: while handling /home/my_user_name/.singularity/cache/oci-tmp/sha256.83ce0d6bbe3695bcbaa348b73c48737bdbfaeaea2272b0105dd4bdfa7a804f18: while extracting image: root filesystem extraction failed: extract command failed: ERROR : Failed to create user namespace: user namespace disabled. : exit status 1. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598
https://github.com/google/deepvariant/issues/598:1570,usability,statu,status,1570,"Thank you for your reply! 1、My machine doesn't have `/usr/bin/python3`.I don't have permission to change `/usr/bin/`.I tried to modify the source code, but I couldn't locate the code. 2、I tried to install Singularity. I use the parameter `--without -- seccomp --without -- conmon` because of the following error. `./mconfig --without-suid --without-seccomp --without-conmon --prefix=/path/singularity && make -C ./builddir &&make -C ./builddir install`. ```. seccomp headers are required to build Singularity with seccomp support. To disable seccomp support run mconfig using '--without-seccomp'. Cannot build conmon for OCI support without libseccomp headers. Use --without-conmon to disable build and use conmon on PATH if present. ```. Then I try to run it. ```. singularity run -B /path/locale/:/path/locale/ \. > docker://google/deepvariant:""1.4.0"" \. > /path/dpv_singu \. > --model_type=PACBIO \. > --ref=/path/ref_fasta/QJref.fa \. > --reads=/path/bam_files/F1N_sorted.merged.addg.uniq.rmdup.bam \. > --output_vcf=/path/output.vcf.gz \. > --output_gvcf=/path/output.g.vcf.gz \. > --intermediate_results_dir /path/intermediate_results_dir. ```. The error information is as follows. ```. INFO: Using cached SIF image. INFO: Converting SIF file to temporary sandbox... FATAL: while handling /home/my_user_name/.singularity/cache/oci-tmp/sha256.83ce0d6bbe3695bcbaa348b73c48737bdbfaeaea2272b0105dd4bdfa7a804f18: while extracting image: root filesystem extraction failed: extract command failed: ERROR : Failed to create user namespace: user namespace disabled. : exit status 1. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598
https://github.com/google/deepvariant/issues/598:66,modifiability,paramet,parameter,66,"@Zero-Sun ,. Can you please try the same command with `--no-home` parameter?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598
https://github.com/google/deepvariant/issues/598:41,usability,command,command,41,"@Zero-Sun ,. Can you please try the same command with `--no-home` parameter?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598
https://github.com/google/deepvariant/issues/599:190,performance,time,time,190,"@robertzeibich ,. Yes, please run your previous command with `--dry_run` and you will see all the commands that are run. Then copy the the `post_process` command and run it like this:. ```. time docker run -it -u `id -u`:`id -g` -v /data:/data \. google/deepvariant:1.4.0 \. **YOUR_COMMAND**. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/599
https://github.com/google/deepvariant/issues/599:48,usability,command,command,48,"@robertzeibich ,. Yes, please run your previous command with `--dry_run` and you will see all the commands that are run. Then copy the the `post_process` command and run it like this:. ```. time docker run -it -u `id -u`:`id -g` -v /data:/data \. google/deepvariant:1.4.0 \. **YOUR_COMMAND**. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/599
https://github.com/google/deepvariant/issues/599:98,usability,command,commands,98,"@robertzeibich ,. Yes, please run your previous command with `--dry_run` and you will see all the commands that are run. Then copy the the `post_process` command and run it like this:. ```. time docker run -it -u `id -u`:`id -g` -v /data:/data \. google/deepvariant:1.4.0 \. **YOUR_COMMAND**. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/599
https://github.com/google/deepvariant/issues/599:154,usability,command,command,154,"@robertzeibich ,. Yes, please run your previous command with `--dry_run` and you will see all the commands that are run. Then copy the the `post_process` command and run it like this:. ```. time docker run -it -u `id -u`:`id -g` -v /data:/data \. google/deepvariant:1.4.0 \. **YOUR_COMMAND**. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/599
https://github.com/google/deepvariant/issues/599:69,availability,error,error,69,"Thanks. I just tried to execute the command, but I get the following error message `ValueError: Cannot find matching files with the pattern ""/tmp/tmphskm66vr/call_variants_output.tfrecord.gz""`. The files in the tmp directory also seem to be gone.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/599
https://github.com/google/deepvariant/issues/599:75,integrability,messag,message,75,"Thanks. I just tried to execute the command, but I get the following error message `ValueError: Cannot find matching files with the pattern ""/tmp/tmphskm66vr/call_variants_output.tfrecord.gz""`. The files in the tmp directory also seem to be gone.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/599
https://github.com/google/deepvariant/issues/599:75,interoperability,messag,message,75,"Thanks. I just tried to execute the command, but I get the following error message `ValueError: Cannot find matching files with the pattern ""/tmp/tmphskm66vr/call_variants_output.tfrecord.gz""`. The files in the tmp directory also seem to be gone.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/599
https://github.com/google/deepvariant/issues/599:69,performance,error,error,69,"Thanks. I just tried to execute the command, but I get the following error message `ValueError: Cannot find matching files with the pattern ""/tmp/tmphskm66vr/call_variants_output.tfrecord.gz""`. The files in the tmp directory also seem to be gone.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/599
https://github.com/google/deepvariant/issues/599:69,safety,error,error,69,"Thanks. I just tried to execute the command, but I get the following error message `ValueError: Cannot find matching files with the pattern ""/tmp/tmphskm66vr/call_variants_output.tfrecord.gz""`. The files in the tmp directory also seem to be gone.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/599
https://github.com/google/deepvariant/issues/599:36,usability,command,command,36,"Thanks. I just tried to execute the command, but I get the following error message `ValueError: Cannot find matching files with the pattern ""/tmp/tmphskm66vr/call_variants_output.tfrecord.gz""`. The files in the tmp directory also seem to be gone.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/599
https://github.com/google/deepvariant/issues/599:69,usability,error,error,69,"Thanks. I just tried to execute the command, but I get the following error message `ValueError: Cannot find matching files with the pattern ""/tmp/tmphskm66vr/call_variants_output.tfrecord.gz""`. The files in the tmp directory also seem to be gone.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/599
https://github.com/google/deepvariant/issues/599:59,modifiability,interm,intermediate,59,"@robertzeibich ,. Yes, unfortunately, you need to have the intermediate directory saved to have this working. The intermediate directory can be saved outside temp by setting `--intermediate_results_dir` parameter.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/599
https://github.com/google/deepvariant/issues/599:114,modifiability,interm,intermediate,114,"@robertzeibich ,. Yes, unfortunately, you need to have the intermediate directory saved to have this working. The intermediate directory can be saved outside temp by setting `--intermediate_results_dir` parameter.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/599
https://github.com/google/deepvariant/issues/599:203,modifiability,paramet,parameter,203,"@robertzeibich ,. Yes, unfortunately, you need to have the intermediate directory saved to have this working. The intermediate directory can be saved outside temp by setting `--intermediate_results_dir` parameter.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/599
https://github.com/google/deepvariant/issues/599:105,modifiability,interm,intermediate,105,"Hi @robertzeibich ,. Hopefully our answer is helpful . In the future if you want to capture and keep the intermediate results, please add that flag. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/599
https://github.com/google/deepvariant/issues/599:45,usability,help,helpful,45,"Hi @robertzeibich ,. Hopefully our answer is helpful . In the future if you want to capture and keep the intermediate results, please add that flag. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/599
https://github.com/google/deepvariant/issues/600:80,integrability,coupl,couple,80,"It looks to me like a large number of candidate variants are being generated. A couple reasons that come to mind:. 1. You are not restricting calling to a set of regions (e.g. exons). 2. Is this an FFPE sample? That could increase the number of candidates considerably. . I recommend using a bed file to restrict calling to regions of interest. If that does not work we might increase thresholds for considering candidates, but it would be worth looking in IGV to see what the sequence data looks like and where candidates are being called.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/600
https://github.com/google/deepvariant/issues/600:80,modifiability,coupl,couple,80,"It looks to me like a large number of candidate variants are being generated. A couple reasons that come to mind:. 1. You are not restricting calling to a set of regions (e.g. exons). 2. Is this an FFPE sample? That could increase the number of candidates considerably. . I recommend using a bed file to restrict calling to regions of interest. If that does not work we might increase thresholds for considering candidates, but it would be worth looking in IGV to see what the sequence data looks like and where candidates are being called.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/600
https://github.com/google/deepvariant/issues/600:353,reliability,doe,does,353,"It looks to me like a large number of candidate variants are being generated. A couple reasons that come to mind:. 1. You are not restricting calling to a set of regions (e.g. exons). 2. Is this an FFPE sample? That could increase the number of candidates considerably. . I recommend using a bed file to restrict calling to regions of interest. If that does not work we might increase thresholds for considering candidates, but it would be worth looking in IGV to see what the sequence data looks like and where candidates are being called.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/600
https://github.com/google/deepvariant/issues/600:80,testability,coupl,couple,80,"It looks to me like a large number of candidate variants are being generated. A couple reasons that come to mind:. 1. You are not restricting calling to a set of regions (e.g. exons). 2. Is this an FFPE sample? That could increase the number of candidates considerably. . I recommend using a bed file to restrict calling to regions of interest. If that does not work we might increase thresholds for considering candidates, but it would be worth looking in IGV to see what the sequence data looks like and where candidates are being called.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/600
https://github.com/google/deepvariant/issues/600:69,usability,help,helpful,69,"Hi @Rofidagamal , . hopefully @danielecook 's answer from before was helpful. I'm closing this issue. Please let us know if you have more questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/600
https://github.com/google/deepvariant/issues/601:30,interoperability,bind,bind,30,figured it out by adding more bind path for the files I need to access to,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/601
https://github.com/google/deepvariant/issues/601:30,modifiability,bind,bind,30,figured it out by adding more bind path for the files I need to access to,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/601
https://github.com/google/deepvariant/issues/601:64,security,access,access,64,figured it out by adding more bind path for the files I need to access to,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/601
https://github.com/google/deepvariant/issues/602:269,availability,error,error,269,"Hi kmarianski,. This line is suspicious. . ```. Re-using the directory for intermediate results in /tmp/kmarians_4189323/tmpxrz5rqbp. ```. Each DeepVariant job needs a separate `intermediate_directory`. Could you verify that each job uses different temp directory? The error in the log comes from TensorFlow library. Unfortunately, we are unable to help with TensorFlow (which is a third party tool).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:282,deployability,log,log,282,"Hi kmarianski,. This line is suspicious. . ```. Re-using the directory for intermediate results in /tmp/kmarians_4189323/tmpxrz5rqbp. ```. Each DeepVariant job needs a separate `intermediate_directory`. Could you verify that each job uses different temp directory? The error in the log comes from TensorFlow library. Unfortunately, we are unable to help with TensorFlow (which is a third party tool).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:75,modifiability,interm,intermediate,75,"Hi kmarianski,. This line is suspicious. . ```. Re-using the directory for intermediate results in /tmp/kmarians_4189323/tmpxrz5rqbp. ```. Each DeepVariant job needs a separate `intermediate_directory`. Could you verify that each job uses different temp directory? The error in the log comes from TensorFlow library. Unfortunately, we are unable to help with TensorFlow (which is a third party tool).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:269,performance,error,error,269,"Hi kmarianski,. This line is suspicious. . ```. Re-using the directory for intermediate results in /tmp/kmarians_4189323/tmpxrz5rqbp. ```. Each DeepVariant job needs a separate `intermediate_directory`. Could you verify that each job uses different temp directory? The error in the log comes from TensorFlow library. Unfortunately, we are unable to help with TensorFlow (which is a third party tool).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:269,safety,error,error,269,"Hi kmarianski,. This line is suspicious. . ```. Re-using the directory for intermediate results in /tmp/kmarians_4189323/tmpxrz5rqbp. ```. Each DeepVariant job needs a separate `intermediate_directory`. Could you verify that each job uses different temp directory? The error in the log comes from TensorFlow library. Unfortunately, we are unable to help with TensorFlow (which is a third party tool).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:282,safety,log,log,282,"Hi kmarianski,. This line is suspicious. . ```. Re-using the directory for intermediate results in /tmp/kmarians_4189323/tmpxrz5rqbp. ```. Each DeepVariant job needs a separate `intermediate_directory`. Could you verify that each job uses different temp directory? The error in the log comes from TensorFlow library. Unfortunately, we are unable to help with TensorFlow (which is a third party tool).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:282,security,log,log,282,"Hi kmarianski,. This line is suspicious. . ```. Re-using the directory for intermediate results in /tmp/kmarians_4189323/tmpxrz5rqbp. ```. Each DeepVariant job needs a separate `intermediate_directory`. Could you verify that each job uses different temp directory? The error in the log comes from TensorFlow library. Unfortunately, we are unable to help with TensorFlow (which is a third party tool).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:213,testability,verif,verify,213,"Hi kmarianski,. This line is suspicious. . ```. Re-using the directory for intermediate results in /tmp/kmarians_4189323/tmpxrz5rqbp. ```. Each DeepVariant job needs a separate `intermediate_directory`. Could you verify that each job uses different temp directory? The error in the log comes from TensorFlow library. Unfortunately, we are unable to help with TensorFlow (which is a third party tool).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:282,testability,log,log,282,"Hi kmarianski,. This line is suspicious. . ```. Re-using the directory for intermediate results in /tmp/kmarians_4189323/tmpxrz5rqbp. ```. Each DeepVariant job needs a separate `intermediate_directory`. Could you verify that each job uses different temp directory? The error in the log comes from TensorFlow library. Unfortunately, we are unable to help with TensorFlow (which is a third party tool).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:269,usability,error,error,269,"Hi kmarianski,. This line is suspicious. . ```. Re-using the directory for intermediate results in /tmp/kmarians_4189323/tmpxrz5rqbp. ```. Each DeepVariant job needs a separate `intermediate_directory`. Could you verify that each job uses different temp directory? The error in the log comes from TensorFlow library. Unfortunately, we are unable to help with TensorFlow (which is a third party tool).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:349,usability,help,help,349,"Hi kmarianski,. This line is suspicious. . ```. Re-using the directory for intermediate results in /tmp/kmarians_4189323/tmpxrz5rqbp. ```. Each DeepVariant job needs a separate `intermediate_directory`. Could you verify that each job uses different temp directory? The error in the log comes from TensorFlow library. Unfortunately, we are unable to help with TensorFlow (which is a third party tool).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:394,usability,tool,tool,394,"Hi kmarianski,. This line is suspicious. . ```. Re-using the directory for intermediate results in /tmp/kmarians_4189323/tmpxrz5rqbp. ```. Each DeepVariant job needs a separate `intermediate_directory`. Could you verify that each job uses different temp directory? The error in the log comes from TensorFlow library. Unfortunately, we are unable to help with TensorFlow (which is a third party tool).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/603:97,deployability,contain,contains,97,"Hi Peter,. Just to check that I understand your issue here:. Are you saying that: your reference contains information for chrEBV, your BAM file also has reads from chrEBV, and then your run completed without crashing. And you used the same BAM file for all these runs. And the only difference was just that the run where you specified `--regions ""chrEBV""` gives you the sample name as ""default"", while all other runs with the same BAM file (but different `--regions`) gave the correct sample name? Another question, if the common contigs didn't list chrEBV, were you actually able to get any calls for chrEBV? Another question for you: Are these files publicly shareable (or if you have a publicly shareable version that can help us reproduce similar issues?). Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/603
https://github.com/google/deepvariant/issues/603:708,deployability,version,version,708,"Hi Peter,. Just to check that I understand your issue here:. Are you saying that: your reference contains information for chrEBV, your BAM file also has reads from chrEBV, and then your run completed without crashing. And you used the same BAM file for all these runs. And the only difference was just that the run where you specified `--regions ""chrEBV""` gives you the sample name as ""default"", while all other runs with the same BAM file (but different `--regions`) gave the correct sample name? Another question, if the common contigs didn't list chrEBV, were you actually able to get any calls for chrEBV? Another question for you: Are these files publicly shareable (or if you have a publicly shareable version that can help us reproduce similar issues?). Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/603
https://github.com/google/deepvariant/issues/603:652,integrability,pub,publicly,652,"Hi Peter,. Just to check that I understand your issue here:. Are you saying that: your reference contains information for chrEBV, your BAM file also has reads from chrEBV, and then your run completed without crashing. And you used the same BAM file for all these runs. And the only difference was just that the run where you specified `--regions ""chrEBV""` gives you the sample name as ""default"", while all other runs with the same BAM file (but different `--regions`) gave the correct sample name? Another question, if the common contigs didn't list chrEBV, were you actually able to get any calls for chrEBV? Another question for you: Are these files publicly shareable (or if you have a publicly shareable version that can help us reproduce similar issues?). Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/603
https://github.com/google/deepvariant/issues/603:689,integrability,pub,publicly,689,"Hi Peter,. Just to check that I understand your issue here:. Are you saying that: your reference contains information for chrEBV, your BAM file also has reads from chrEBV, and then your run completed without crashing. And you used the same BAM file for all these runs. And the only difference was just that the run where you specified `--regions ""chrEBV""` gives you the sample name as ""default"", while all other runs with the same BAM file (but different `--regions`) gave the correct sample name? Another question, if the common contigs didn't list chrEBV, were you actually able to get any calls for chrEBV? Another question for you: Are these files publicly shareable (or if you have a publicly shareable version that can help us reproduce similar issues?). Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/603
https://github.com/google/deepvariant/issues/603:708,integrability,version,version,708,"Hi Peter,. Just to check that I understand your issue here:. Are you saying that: your reference contains information for chrEBV, your BAM file also has reads from chrEBV, and then your run completed without crashing. And you used the same BAM file for all these runs. And the only difference was just that the run where you specified `--regions ""chrEBV""` gives you the sample name as ""default"", while all other runs with the same BAM file (but different `--regions`) gave the correct sample name? Another question, if the common contigs didn't list chrEBV, were you actually able to get any calls for chrEBV? Another question for you: Are these files publicly shareable (or if you have a publicly shareable version that can help us reproduce similar issues?). Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/603
https://github.com/google/deepvariant/issues/603:325,interoperability,specif,specified,325,"Hi Peter,. Just to check that I understand your issue here:. Are you saying that: your reference contains information for chrEBV, your BAM file also has reads from chrEBV, and then your run completed without crashing. And you used the same BAM file for all these runs. And the only difference was just that the run where you specified `--regions ""chrEBV""` gives you the sample name as ""default"", while all other runs with the same BAM file (but different `--regions`) gave the correct sample name? Another question, if the common contigs didn't list chrEBV, were you actually able to get any calls for chrEBV? Another question for you: Are these files publicly shareable (or if you have a publicly shareable version that can help us reproduce similar issues?). Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/603
https://github.com/google/deepvariant/issues/603:661,interoperability,share,shareable,661,"Hi Peter,. Just to check that I understand your issue here:. Are you saying that: your reference contains information for chrEBV, your BAM file also has reads from chrEBV, and then your run completed without crashing. And you used the same BAM file for all these runs. And the only difference was just that the run where you specified `--regions ""chrEBV""` gives you the sample name as ""default"", while all other runs with the same BAM file (but different `--regions`) gave the correct sample name? Another question, if the common contigs didn't list chrEBV, were you actually able to get any calls for chrEBV? Another question for you: Are these files publicly shareable (or if you have a publicly shareable version that can help us reproduce similar issues?). Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/603
https://github.com/google/deepvariant/issues/603:698,interoperability,share,shareable,698,"Hi Peter,. Just to check that I understand your issue here:. Are you saying that: your reference contains information for chrEBV, your BAM file also has reads from chrEBV, and then your run completed without crashing. And you used the same BAM file for all these runs. And the only difference was just that the run where you specified `--regions ""chrEBV""` gives you the sample name as ""default"", while all other runs with the same BAM file (but different `--regions`) gave the correct sample name? Another question, if the common contigs didn't list chrEBV, were you actually able to get any calls for chrEBV? Another question for you: Are these files publicly shareable (or if you have a publicly shareable version that can help us reproduce similar issues?). Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/603
https://github.com/google/deepvariant/issues/603:708,modifiability,version,version,708,"Hi Peter,. Just to check that I understand your issue here:. Are you saying that: your reference contains information for chrEBV, your BAM file also has reads from chrEBV, and then your run completed without crashing. And you used the same BAM file for all these runs. And the only difference was just that the run where you specified `--regions ""chrEBV""` gives you the sample name as ""default"", while all other runs with the same BAM file (but different `--regions`) gave the correct sample name? Another question, if the common contigs didn't list chrEBV, were you actually able to get any calls for chrEBV? Another question for you: Are these files publicly shareable (or if you have a publicly shareable version that can help us reproduce similar issues?). Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/603
https://github.com/google/deepvariant/issues/603:190,safety,compl,completed,190,"Hi Peter,. Just to check that I understand your issue here:. Are you saying that: your reference contains information for chrEBV, your BAM file also has reads from chrEBV, and then your run completed without crashing. And you used the same BAM file for all these runs. And the only difference was just that the run where you specified `--regions ""chrEBV""` gives you the sample name as ""default"", while all other runs with the same BAM file (but different `--regions`) gave the correct sample name? Another question, if the common contigs didn't list chrEBV, were you actually able to get any calls for chrEBV? Another question for you: Are these files publicly shareable (or if you have a publicly shareable version that can help us reproduce similar issues?). Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/603
https://github.com/google/deepvariant/issues/603:190,security,compl,completed,190,"Hi Peter,. Just to check that I understand your issue here:. Are you saying that: your reference contains information for chrEBV, your BAM file also has reads from chrEBV, and then your run completed without crashing. And you used the same BAM file for all these runs. And the only difference was just that the run where you specified `--regions ""chrEBV""` gives you the sample name as ""default"", while all other runs with the same BAM file (but different `--regions`) gave the correct sample name? Another question, if the common contigs didn't list chrEBV, were you actually able to get any calls for chrEBV? Another question for you: Are these files publicly shareable (or if you have a publicly shareable version that can help us reproduce similar issues?). Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/603
https://github.com/google/deepvariant/issues/603:32,testability,understand,understand,32,"Hi Peter,. Just to check that I understand your issue here:. Are you saying that: your reference contains information for chrEBV, your BAM file also has reads from chrEBV, and then your run completed without crashing. And you used the same BAM file for all these runs. And the only difference was just that the run where you specified `--regions ""chrEBV""` gives you the sample name as ""default"", while all other runs with the same BAM file (but different `--regions`) gave the correct sample name? Another question, if the common contigs didn't list chrEBV, were you actually able to get any calls for chrEBV? Another question for you: Are these files publicly shareable (or if you have a publicly shareable version that can help us reproduce similar issues?). Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/603
https://github.com/google/deepvariant/issues/603:725,usability,help,help,725,"Hi Peter,. Just to check that I understand your issue here:. Are you saying that: your reference contains information for chrEBV, your BAM file also has reads from chrEBV, and then your run completed without crashing. And you used the same BAM file for all these runs. And the only difference was just that the run where you specified `--regions ""chrEBV""` gives you the sample name as ""default"", while all other runs with the same BAM file (but different `--regions`) gave the correct sample name? Another question, if the common contigs didn't list chrEBV, were you actually able to get any calls for chrEBV? Another question for you: Are these files publicly shareable (or if you have a publicly shareable version that can help us reproduce similar issues?). Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/603
https://github.com/google/deepvariant/issues/603:353,interoperability,share,share,353,"Hi Pi-Chuan,. thanks for looking into this - to answer your questions:. 1. yes, exactly as you summarized it. 2. no, there are no calls in the `chrEBV` output VCF (just as comparison: the `chrM` VCF looks just like the others with correct sample name and has two calls). 3. I am running this analysis as a favor for a collaboration partner, so I cannot share ad hoc. I'll reach out to them, but this is very much WIP, so I have my doubts. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/603
https://github.com/google/deepvariant/issues/603:119,deployability,version,version,119,"Ha, ignore that - I executed the run again to be sure, and the ""default"" sample name does only show up in the filtered version of the VCF, not in the raw output by DeepVariant (sorry, I just realize now that I had looked at wrong file...). So it's likely a bcftools issue... not your problem, that is! Sorry for the noise.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/603
https://github.com/google/deepvariant/issues/603:110,integrability,filter,filtered,110,"Ha, ignore that - I executed the run again to be sure, and the ""default"" sample name does only show up in the filtered version of the VCF, not in the raw output by DeepVariant (sorry, I just realize now that I had looked at wrong file...). So it's likely a bcftools issue... not your problem, that is! Sorry for the noise.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/603
https://github.com/google/deepvariant/issues/603:119,integrability,version,version,119,"Ha, ignore that - I executed the run again to be sure, and the ""default"" sample name does only show up in the filtered version of the VCF, not in the raw output by DeepVariant (sorry, I just realize now that I had looked at wrong file...). So it's likely a bcftools issue... not your problem, that is! Sorry for the noise.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/603
https://github.com/google/deepvariant/issues/603:119,modifiability,version,version,119,"Ha, ignore that - I executed the run again to be sure, and the ""default"" sample name does only show up in the filtered version of the VCF, not in the raw output by DeepVariant (sorry, I just realize now that I had looked at wrong file...). So it's likely a bcftools issue... not your problem, that is! Sorry for the noise.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/603
https://github.com/google/deepvariant/issues/603:85,reliability,doe,does,85,"Ha, ignore that - I executed the run again to be sure, and the ""default"" sample name does only show up in the filtered version of the VCF, not in the raw output by DeepVariant (sorry, I just realize now that I had looked at wrong file...). So it's likely a bcftools issue... not your problem, that is! Sorry for the noise.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/603
https://github.com/google/deepvariant/issues/605:43,energy efficiency,model,model,43,"Actually, we only published the GTEx-based model given that it outperformed the GIAB trained model. The GTEx model is the only model we published and is what is used in the case study. Let me know if you have any further questions. Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/605
https://github.com/google/deepvariant/issues/605:93,energy efficiency,model,model,93,"Actually, we only published the GTEx-based model given that it outperformed the GIAB trained model. The GTEx model is the only model we published and is what is used in the case study. Let me know if you have any further questions. Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/605
https://github.com/google/deepvariant/issues/605:109,energy efficiency,model,model,109,"Actually, we only published the GTEx-based model given that it outperformed the GIAB trained model. The GTEx model is the only model we published and is what is used in the case study. Let me know if you have any further questions. Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/605
https://github.com/google/deepvariant/issues/605:127,energy efficiency,model,model,127,"Actually, we only published the GTEx-based model given that it outperformed the GIAB trained model. The GTEx model is the only model we published and is what is used in the case study. Let me know if you have any further questions. Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/605
https://github.com/google/deepvariant/issues/605:18,integrability,pub,published,18,"Actually, we only published the GTEx-based model given that it outperformed the GIAB trained model. The GTEx model is the only model we published and is what is used in the case study. Let me know if you have any further questions. Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/605
https://github.com/google/deepvariant/issues/605:136,integrability,pub,published,136,"Actually, we only published the GTEx-based model given that it outperformed the GIAB trained model. The GTEx model is the only model we published and is what is used in the case study. Let me know if you have any further questions. Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/605
https://github.com/google/deepvariant/issues/605:43,security,model,model,43,"Actually, we only published the GTEx-based model given that it outperformed the GIAB trained model. The GTEx model is the only model we published and is what is used in the case study. Let me know if you have any further questions. Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/605
https://github.com/google/deepvariant/issues/605:93,security,model,model,93,"Actually, we only published the GTEx-based model given that it outperformed the GIAB trained model. The GTEx model is the only model we published and is what is used in the case study. Let me know if you have any further questions. Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/605
https://github.com/google/deepvariant/issues/605:109,security,model,model,109,"Actually, we only published the GTEx-based model given that it outperformed the GIAB trained model. The GTEx model is the only model we published and is what is used in the case study. Let me know if you have any further questions. Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/605
https://github.com/google/deepvariant/issues/605:127,security,model,model,127,"Actually, we only published the GTEx-based model given that it outperformed the GIAB trained model. The GTEx model is the only model we published and is what is used in the case study. Let me know if you have any further questions. Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/605
https://github.com/google/deepvariant/issues/606:98,interoperability,coordinat,coordinate,98,"Hi @sivianil . Are you able to get the VCF lines from DeepVariant and from the truth VCF for this coordinate position? If there is a line from DeepVariant, does the line have a G or a C there? If there is a variant entry from the Truth set, does the line have a G or a C there. If there isn't an entry at this position from DeepVariant which differs from the reference, then the issue is likely in the agreement with the truth set and the reference.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/606
https://github.com/google/deepvariant/issues/606:156,reliability,doe,does,156,"Hi @sivianil . Are you able to get the VCF lines from DeepVariant and from the truth VCF for this coordinate position? If there is a line from DeepVariant, does the line have a G or a C there? If there is a variant entry from the Truth set, does the line have a G or a C there. If there isn't an entry at this position from DeepVariant which differs from the reference, then the issue is likely in the agreement with the truth set and the reference.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/606
https://github.com/google/deepvariant/issues/606:241,reliability,doe,does,241,"Hi @sivianil . Are you able to get the VCF lines from DeepVariant and from the truth VCF for this coordinate position? If there is a line from DeepVariant, does the line have a G or a C there? If there is a variant entry from the Truth set, does the line have a G or a C there. If there isn't an entry at this position from DeepVariant which differs from the reference, then the issue is likely in the agreement with the truth set and the reference.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/606
https://github.com/google/deepvariant/issues/606:415,availability,error,error,415,"Hello Andrew,. . I got the VCF line from only DeepVariant generated file for this specific position. From the DeepVariant REF column have G and the ALT column have GCTCT,GCTCTCT. Two different alternate alleles of both insertion types. So I can say this comes under Multiallelic Insertion category. . From the truth VCF file, there is no line for this coordinate position. When I evaluated using hap.py, it returns error as I commented earlier in the above post. . Does the model consider both Multiallelic variant type and Biallelic or only Biallelic during evaluation? Is there any way I can filter the VCF lines having Multiallelic variant types?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/606
https://github.com/google/deepvariant/issues/606:474,energy efficiency,model,model,474,"Hello Andrew,. . I got the VCF line from only DeepVariant generated file for this specific position. From the DeepVariant REF column have G and the ALT column have GCTCT,GCTCTCT. Two different alternate alleles of both insertion types. So I can say this comes under Multiallelic Insertion category. . From the truth VCF file, there is no line for this coordinate position. When I evaluated using hap.py, it returns error as I commented earlier in the above post. . Does the model consider both Multiallelic variant type and Biallelic or only Biallelic during evaluation? Is there any way I can filter the VCF lines having Multiallelic variant types?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/606
https://github.com/google/deepvariant/issues/606:594,integrability,filter,filter,594,"Hello Andrew,. . I got the VCF line from only DeepVariant generated file for this specific position. From the DeepVariant REF column have G and the ALT column have GCTCT,GCTCTCT. Two different alternate alleles of both insertion types. So I can say this comes under Multiallelic Insertion category. . From the truth VCF file, there is no line for this coordinate position. When I evaluated using hap.py, it returns error as I commented earlier in the above post. . Does the model consider both Multiallelic variant type and Biallelic or only Biallelic during evaluation? Is there any way I can filter the VCF lines having Multiallelic variant types?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/606
https://github.com/google/deepvariant/issues/606:82,interoperability,specif,specific,82,"Hello Andrew,. . I got the VCF line from only DeepVariant generated file for this specific position. From the DeepVariant REF column have G and the ALT column have GCTCT,GCTCTCT. Two different alternate alleles of both insertion types. So I can say this comes under Multiallelic Insertion category. . From the truth VCF file, there is no line for this coordinate position. When I evaluated using hap.py, it returns error as I commented earlier in the above post. . Does the model consider both Multiallelic variant type and Biallelic or only Biallelic during evaluation? Is there any way I can filter the VCF lines having Multiallelic variant types?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/606
https://github.com/google/deepvariant/issues/606:352,interoperability,coordinat,coordinate,352,"Hello Andrew,. . I got the VCF line from only DeepVariant generated file for this specific position. From the DeepVariant REF column have G and the ALT column have GCTCT,GCTCTCT. Two different alternate alleles of both insertion types. So I can say this comes under Multiallelic Insertion category. . From the truth VCF file, there is no line for this coordinate position. When I evaluated using hap.py, it returns error as I commented earlier in the above post. . Does the model consider both Multiallelic variant type and Biallelic or only Biallelic during evaluation? Is there any way I can filter the VCF lines having Multiallelic variant types?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/606
https://github.com/google/deepvariant/issues/606:415,performance,error,error,415,"Hello Andrew,. . I got the VCF line from only DeepVariant generated file for this specific position. From the DeepVariant REF column have G and the ALT column have GCTCT,GCTCTCT. Two different alternate alleles of both insertion types. So I can say this comes under Multiallelic Insertion category. . From the truth VCF file, there is no line for this coordinate position. When I evaluated using hap.py, it returns error as I commented earlier in the above post. . Does the model consider both Multiallelic variant type and Biallelic or only Biallelic during evaluation? Is there any way I can filter the VCF lines having Multiallelic variant types?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/606
https://github.com/google/deepvariant/issues/606:465,reliability,Doe,Does,465,"Hello Andrew,. . I got the VCF line from only DeepVariant generated file for this specific position. From the DeepVariant REF column have G and the ALT column have GCTCT,GCTCTCT. Two different alternate alleles of both insertion types. So I can say this comes under Multiallelic Insertion category. . From the truth VCF file, there is no line for this coordinate position. When I evaluated using hap.py, it returns error as I commented earlier in the above post. . Does the model consider both Multiallelic variant type and Biallelic or only Biallelic during evaluation? Is there any way I can filter the VCF lines having Multiallelic variant types?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/606
https://github.com/google/deepvariant/issues/606:415,safety,error,error,415,"Hello Andrew,. . I got the VCF line from only DeepVariant generated file for this specific position. From the DeepVariant REF column have G and the ALT column have GCTCT,GCTCTCT. Two different alternate alleles of both insertion types. So I can say this comes under Multiallelic Insertion category. . From the truth VCF file, there is no line for this coordinate position. When I evaluated using hap.py, it returns error as I commented earlier in the above post. . Does the model consider both Multiallelic variant type and Biallelic or only Biallelic during evaluation? Is there any way I can filter the VCF lines having Multiallelic variant types?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/606
https://github.com/google/deepvariant/issues/606:474,security,model,model,474,"Hello Andrew,. . I got the VCF line from only DeepVariant generated file for this specific position. From the DeepVariant REF column have G and the ALT column have GCTCT,GCTCTCT. Two different alternate alleles of both insertion types. So I can say this comes under Multiallelic Insertion category. . From the truth VCF file, there is no line for this coordinate position. When I evaluated using hap.py, it returns error as I commented earlier in the above post. . Does the model consider both Multiallelic variant type and Biallelic or only Biallelic during evaluation? Is there any way I can filter the VCF lines having Multiallelic variant types?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/606
https://github.com/google/deepvariant/issues/606:415,usability,error,error,415,"Hello Andrew,. . I got the VCF line from only DeepVariant generated file for this specific position. From the DeepVariant REF column have G and the ALT column have GCTCT,GCTCTCT. Two different alternate alleles of both insertion types. So I can say this comes under Multiallelic Insertion category. . From the truth VCF file, there is no line for this coordinate position. When I evaluated using hap.py, it returns error as I commented earlier in the above post. . Does the model consider both Multiallelic variant type and Biallelic or only Biallelic during evaluation? Is there any way I can filter the VCF lines having Multiallelic variant types?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/606
https://github.com/google/deepvariant/issues/606:60,availability,error,error,60,"Hi @sivianil . That is interesting. I don't think that this error message has something to do with the allele being multiallelic. We generate multiallelic outputs for human samples and hap.py works for those. . If you want to filter to only bi-allelic calls, you can post-process the VCF with bcftools . `bcftools view -m2 -M2`. From reading the error, it seems that the most likely explanation is that the reference genome used in the hap.py evaluation does not exactly match the sequence of the reference genome used to map the reads in the BAM file used in DeepVariant. Are you certain those two files are exactly the same? Thanks,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/606
https://github.com/google/deepvariant/issues/606:346,availability,error,error,346,"Hi @sivianil . That is interesting. I don't think that this error message has something to do with the allele being multiallelic. We generate multiallelic outputs for human samples and hap.py works for those. . If you want to filter to only bi-allelic calls, you can post-process the VCF with bcftools . `bcftools view -m2 -M2`. From reading the error, it seems that the most likely explanation is that the reference genome used in the hap.py evaluation does not exactly match the sequence of the reference genome used to map the reads in the BAM file used in DeepVariant. Are you certain those two files are exactly the same? Thanks,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/606
https://github.com/google/deepvariant/issues/606:66,integrability,messag,message,66,"Hi @sivianil . That is interesting. I don't think that this error message has something to do with the allele being multiallelic. We generate multiallelic outputs for human samples and hap.py works for those. . If you want to filter to only bi-allelic calls, you can post-process the VCF with bcftools . `bcftools view -m2 -M2`. From reading the error, it seems that the most likely explanation is that the reference genome used in the hap.py evaluation does not exactly match the sequence of the reference genome used to map the reads in the BAM file used in DeepVariant. Are you certain those two files are exactly the same? Thanks,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/606
https://github.com/google/deepvariant/issues/606:226,integrability,filter,filter,226,"Hi @sivianil . That is interesting. I don't think that this error message has something to do with the allele being multiallelic. We generate multiallelic outputs for human samples and hap.py works for those. . If you want to filter to only bi-allelic calls, you can post-process the VCF with bcftools . `bcftools view -m2 -M2`. From reading the error, it seems that the most likely explanation is that the reference genome used in the hap.py evaluation does not exactly match the sequence of the reference genome used to map the reads in the BAM file used in DeepVariant. Are you certain those two files are exactly the same? Thanks,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/606
https://github.com/google/deepvariant/issues/606:66,interoperability,messag,message,66,"Hi @sivianil . That is interesting. I don't think that this error message has something to do with the allele being multiallelic. We generate multiallelic outputs for human samples and hap.py works for those. . If you want to filter to only bi-allelic calls, you can post-process the VCF with bcftools . `bcftools view -m2 -M2`. From reading the error, it seems that the most likely explanation is that the reference genome used in the hap.py evaluation does not exactly match the sequence of the reference genome used to map the reads in the BAM file used in DeepVariant. Are you certain those two files are exactly the same? Thanks,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/606
https://github.com/google/deepvariant/issues/606:60,performance,error,error,60,"Hi @sivianil . That is interesting. I don't think that this error message has something to do with the allele being multiallelic. We generate multiallelic outputs for human samples and hap.py works for those. . If you want to filter to only bi-allelic calls, you can post-process the VCF with bcftools . `bcftools view -m2 -M2`. From reading the error, it seems that the most likely explanation is that the reference genome used in the hap.py evaluation does not exactly match the sequence of the reference genome used to map the reads in the BAM file used in DeepVariant. Are you certain those two files are exactly the same? Thanks,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/606
https://github.com/google/deepvariant/issues/606:346,performance,error,error,346,"Hi @sivianil . That is interesting. I don't think that this error message has something to do with the allele being multiallelic. We generate multiallelic outputs for human samples and hap.py works for those. . If you want to filter to only bi-allelic calls, you can post-process the VCF with bcftools . `bcftools view -m2 -M2`. From reading the error, it seems that the most likely explanation is that the reference genome used in the hap.py evaluation does not exactly match the sequence of the reference genome used to map the reads in the BAM file used in DeepVariant. Are you certain those two files are exactly the same? Thanks,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/606
https://github.com/google/deepvariant/issues/606:454,reliability,doe,does,454,"Hi @sivianil . That is interesting. I don't think that this error message has something to do with the allele being multiallelic. We generate multiallelic outputs for human samples and hap.py works for those. . If you want to filter to only bi-allelic calls, you can post-process the VCF with bcftools . `bcftools view -m2 -M2`. From reading the error, it seems that the most likely explanation is that the reference genome used in the hap.py evaluation does not exactly match the sequence of the reference genome used to map the reads in the BAM file used in DeepVariant. Are you certain those two files are exactly the same? Thanks,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/606
https://github.com/google/deepvariant/issues/606:60,safety,error,error,60,"Hi @sivianil . That is interesting. I don't think that this error message has something to do with the allele being multiallelic. We generate multiallelic outputs for human samples and hap.py works for those. . If you want to filter to only bi-allelic calls, you can post-process the VCF with bcftools . `bcftools view -m2 -M2`. From reading the error, it seems that the most likely explanation is that the reference genome used in the hap.py evaluation does not exactly match the sequence of the reference genome used to map the reads in the BAM file used in DeepVariant. Are you certain those two files are exactly the same? Thanks,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/606
https://github.com/google/deepvariant/issues/606:346,safety,error,error,346,"Hi @sivianil . That is interesting. I don't think that this error message has something to do with the allele being multiallelic. We generate multiallelic outputs for human samples and hap.py works for those. . If you want to filter to only bi-allelic calls, you can post-process the VCF with bcftools . `bcftools view -m2 -M2`. From reading the error, it seems that the most likely explanation is that the reference genome used in the hap.py evaluation does not exactly match the sequence of the reference genome used to map the reads in the BAM file used in DeepVariant. Are you certain those two files are exactly the same? Thanks,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/606
https://github.com/google/deepvariant/issues/606:60,usability,error,error,60,"Hi @sivianil . That is interesting. I don't think that this error message has something to do with the allele being multiallelic. We generate multiallelic outputs for human samples and hap.py works for those. . If you want to filter to only bi-allelic calls, you can post-process the VCF with bcftools . `bcftools view -m2 -M2`. From reading the error, it seems that the most likely explanation is that the reference genome used in the hap.py evaluation does not exactly match the sequence of the reference genome used to map the reads in the BAM file used in DeepVariant. Are you certain those two files are exactly the same? Thanks,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/606
https://github.com/google/deepvariant/issues/606:346,usability,error,error,346,"Hi @sivianil . That is interesting. I don't think that this error message has something to do with the allele being multiallelic. We generate multiallelic outputs for human samples and hap.py works for those. . If you want to filter to only bi-allelic calls, you can post-process the VCF with bcftools . `bcftools view -m2 -M2`. From reading the error, it seems that the most likely explanation is that the reference genome used in the hap.py evaluation does not exactly match the sequence of the reference genome used to map the reads in the BAM file used in DeepVariant. Are you certain those two files are exactly the same? Thanks,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/606
https://github.com/google/deepvariant/issues/606:269,energy efficiency,model,model,269,"Hi Andrew,. Yes, I'm sure the same reference fasta file used for both mapping the reads in the BAM file and during the hap.py evaluation. . From the result I got when evaluated for chr1:8700-20000, there are no true positives at all. The positions at which DeepVariant model generated in output.vcf are not at all existed in Gold Standard Truth set variant file. I'm not sure the reason behind this. Is it becoz, the model trained on human data? I attached both DeepVariant model generated vcf file and GS vcf file. Have a look at it! . [output.vcf.gz](https://github.com/google/deepvariant/files/10518129/output.vcf.gz). [intersection-801.vcf.gz](https://github.com/google/deepvariant/files/10518132/intersection-801.vcf.gz). Did anyone tried to run previously on Arabidopsis Thalia samples and encountered the same issue I'm facing?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/606
https://github.com/google/deepvariant/issues/606:417,energy efficiency,model,model,417,"Hi Andrew,. Yes, I'm sure the same reference fasta file used for both mapping the reads in the BAM file and during the hap.py evaluation. . From the result I got when evaluated for chr1:8700-20000, there are no true positives at all. The positions at which DeepVariant model generated in output.vcf are not at all existed in Gold Standard Truth set variant file. I'm not sure the reason behind this. Is it becoz, the model trained on human data? I attached both DeepVariant model generated vcf file and GS vcf file. Have a look at it! . [output.vcf.gz](https://github.com/google/deepvariant/files/10518129/output.vcf.gz). [intersection-801.vcf.gz](https://github.com/google/deepvariant/files/10518132/intersection-801.vcf.gz). Did anyone tried to run previously on Arabidopsis Thalia samples and encountered the same issue I'm facing?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/606
https://github.com/google/deepvariant/issues/606:474,energy efficiency,model,model,474,"Hi Andrew,. Yes, I'm sure the same reference fasta file used for both mapping the reads in the BAM file and during the hap.py evaluation. . From the result I got when evaluated for chr1:8700-20000, there are no true positives at all. The positions at which DeepVariant model generated in output.vcf are not at all existed in Gold Standard Truth set variant file. I'm not sure the reason behind this. Is it becoz, the model trained on human data? I attached both DeepVariant model generated vcf file and GS vcf file. Have a look at it! . [output.vcf.gz](https://github.com/google/deepvariant/files/10518129/output.vcf.gz). [intersection-801.vcf.gz](https://github.com/google/deepvariant/files/10518132/intersection-801.vcf.gz). Did anyone tried to run previously on Arabidopsis Thalia samples and encountered the same issue I'm facing?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/606
https://github.com/google/deepvariant/issues/606:330,interoperability,Standard,Standard,330,"Hi Andrew,. Yes, I'm sure the same reference fasta file used for both mapping the reads in the BAM file and during the hap.py evaluation. . From the result I got when evaluated for chr1:8700-20000, there are no true positives at all. The positions at which DeepVariant model generated in output.vcf are not at all existed in Gold Standard Truth set variant file. I'm not sure the reason behind this. Is it becoz, the model trained on human data? I attached both DeepVariant model generated vcf file and GS vcf file. Have a look at it! . [output.vcf.gz](https://github.com/google/deepvariant/files/10518129/output.vcf.gz). [intersection-801.vcf.gz](https://github.com/google/deepvariant/files/10518132/intersection-801.vcf.gz). Did anyone tried to run previously on Arabidopsis Thalia samples and encountered the same issue I'm facing?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/606
https://github.com/google/deepvariant/issues/606:269,security,model,model,269,"Hi Andrew,. Yes, I'm sure the same reference fasta file used for both mapping the reads in the BAM file and during the hap.py evaluation. . From the result I got when evaluated for chr1:8700-20000, there are no true positives at all. The positions at which DeepVariant model generated in output.vcf are not at all existed in Gold Standard Truth set variant file. I'm not sure the reason behind this. Is it becoz, the model trained on human data? I attached both DeepVariant model generated vcf file and GS vcf file. Have a look at it! . [output.vcf.gz](https://github.com/google/deepvariant/files/10518129/output.vcf.gz). [intersection-801.vcf.gz](https://github.com/google/deepvariant/files/10518132/intersection-801.vcf.gz). Did anyone tried to run previously on Arabidopsis Thalia samples and encountered the same issue I'm facing?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/606
https://github.com/google/deepvariant/issues/606:417,security,model,model,417,"Hi Andrew,. Yes, I'm sure the same reference fasta file used for both mapping the reads in the BAM file and during the hap.py evaluation. . From the result I got when evaluated for chr1:8700-20000, there are no true positives at all. The positions at which DeepVariant model generated in output.vcf are not at all existed in Gold Standard Truth set variant file. I'm not sure the reason behind this. Is it becoz, the model trained on human data? I attached both DeepVariant model generated vcf file and GS vcf file. Have a look at it! . [output.vcf.gz](https://github.com/google/deepvariant/files/10518129/output.vcf.gz). [intersection-801.vcf.gz](https://github.com/google/deepvariant/files/10518132/intersection-801.vcf.gz). Did anyone tried to run previously on Arabidopsis Thalia samples and encountered the same issue I'm facing?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/606
https://github.com/google/deepvariant/issues/606:474,security,model,model,474,"Hi Andrew,. Yes, I'm sure the same reference fasta file used for both mapping the reads in the BAM file and during the hap.py evaluation. . From the result I got when evaluated for chr1:8700-20000, there are no true positives at all. The positions at which DeepVariant model generated in output.vcf are not at all existed in Gold Standard Truth set variant file. I'm not sure the reason behind this. Is it becoz, the model trained on human data? I attached both DeepVariant model generated vcf file and GS vcf file. Have a look at it! . [output.vcf.gz](https://github.com/google/deepvariant/files/10518129/output.vcf.gz). [intersection-801.vcf.gz](https://github.com/google/deepvariant/files/10518132/intersection-801.vcf.gz). Did anyone tried to run previously on Arabidopsis Thalia samples and encountered the same issue I'm facing?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/606
https://github.com/google/deepvariant/issues/606:148,availability,error,error,148,"@sivianil if the BAM file and reference FASTA files are publicly sharable as well, can you share them? It'll be good if we can try to reproduce the error. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/606
https://github.com/google/deepvariant/issues/606:56,integrability,pub,publicly,56,"@sivianil if the BAM file and reference FASTA files are publicly sharable as well, can you share them? It'll be good if we can try to reproduce the error. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/606
https://github.com/google/deepvariant/issues/606:91,interoperability,share,share,91,"@sivianil if the BAM file and reference FASTA files are publicly sharable as well, can you share them? It'll be good if we can try to reproduce the error. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/606
https://github.com/google/deepvariant/issues/606:148,performance,error,error,148,"@sivianil if the BAM file and reference FASTA files are publicly sharable as well, can you share them? It'll be good if we can try to reproduce the error. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/606
https://github.com/google/deepvariant/issues/606:148,safety,error,error,148,"@sivianil if the BAM file and reference FASTA files are publicly sharable as well, can you share them? It'll be good if we can try to reproduce the error. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/606
https://github.com/google/deepvariant/issues/606:148,usability,error,error,148,"@sivianil if the BAM file and reference FASTA files are publicly sharable as well, can you share them? It'll be good if we can try to reproduce the error. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/606
https://github.com/google/deepvariant/issues/606:190,deployability,releas,releases,190,"Well, I generated the BAM file using bam and samtools from fastq files. The file is too large to share. I'll add the one drive link to access the file. [https://1001genomes.org/data/GMI-MPI/releases/v3.1/pseudogenomes/fasta/pseudo801.fasta.gz](url). [https://unipotsdamde-my.sharepoint.com/:u:/g/personal/anil_kumar_boddapati_uni-potsdam_de/EYyX5a4xEqBBrH4aXAID8KcB9__Q3s34vU3cTfav0J-VrA?e=owCGZI](url). Best,. Anil.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/606
https://github.com/google/deepvariant/issues/606:97,interoperability,share,share,97,"Well, I generated the BAM file using bam and samtools from fastq files. The file is too large to share. I'll add the one drive link to access the file. [https://1001genomes.org/data/GMI-MPI/releases/v3.1/pseudogenomes/fasta/pseudo801.fasta.gz](url). [https://unipotsdamde-my.sharepoint.com/:u:/g/personal/anil_kumar_boddapati_uni-potsdam_de/EYyX5a4xEqBBrH4aXAID8KcB9__Q3s34vU3cTfav0J-VrA?e=owCGZI](url). Best,. Anil.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/606
https://github.com/google/deepvariant/issues/606:275,interoperability,share,sharepoint,275,"Well, I generated the BAM file using bam and samtools from fastq files. The file is too large to share. I'll add the one drive link to access the file. [https://1001genomes.org/data/GMI-MPI/releases/v3.1/pseudogenomes/fasta/pseudo801.fasta.gz](url). [https://unipotsdamde-my.sharepoint.com/:u:/g/personal/anil_kumar_boddapati_uni-potsdam_de/EYyX5a4xEqBBrH4aXAID8KcB9__Q3s34vU3cTfav0J-VrA?e=owCGZI](url). Best,. Anil.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/606
https://github.com/google/deepvariant/issues/606:135,security,access,access,135,"Well, I generated the BAM file using bam and samtools from fastq files. The file is too large to share. I'll add the one drive link to access the file. [https://1001genomes.org/data/GMI-MPI/releases/v3.1/pseudogenomes/fasta/pseudo801.fasta.gz](url). [https://unipotsdamde-my.sharepoint.com/:u:/g/personal/anil_kumar_boddapati_uni-potsdam_de/EYyX5a4xEqBBrH4aXAID8KcB9__Q3s34vU3cTfav0J-VrA?e=owCGZI](url). Best,. Anil.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/606
https://github.com/google/deepvariant/issues/606:296,usability,person,personal,296,"Well, I generated the BAM file using bam and samtools from fastq files. The file is too large to share. I'll add the one drive link to access the file. [https://1001genomes.org/data/GMI-MPI/releases/v3.1/pseudogenomes/fasta/pseudo801.fasta.gz](url). [https://unipotsdamde-my.sharepoint.com/:u:/g/personal/anil_kumar_boddapati_uni-potsdam_de/EYyX5a4xEqBBrH4aXAID8KcB9__Q3s34vU3cTfav0J-VrA?e=owCGZI](url). Best,. Anil.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/606
https://github.com/google/deepvariant/issues/606:677,availability,error,error,677,"Hi @sivianil . Thank you for the files. With them, I was able to get a better idea of the problem. The problem is that the file that you are providing as your truth set file (801_snp_short_indel_with_quality_reference.vcf.gz) has an entry on chr1 position 8669 which disagrees in the REF field with the file that you are providing as your reference (pseudo801.fasta.gz). According to *pseudo801.fasta.gz* the reference base is *G*. The line in *801_snp_short_indel_with_quality_reference.vcf.gz* has an entry on this field as:. ```. 1 8669 . C . 38 PASS DP=16 GT:GQ:DP 0|0:38:16. ```. According to this VCF, the reference base listed at this position is *C*. You will get this error with hap.py regardless the content of the DeepVariant file. This error does not relate to DeepVariant at all. If you want to do any comparison with 801_snp_short_indel_with_quality_reference.vcf.gz you will need to reconcile the difference between the reference and the contents of the VCF. . Does it really make sense to apply hap.py to this dataset? Hap.py was developed for comparison with highly curated truth datasets made with multiple technologies and/or corroborated by extensive pedigree information (e.g. as is available in Genome in a Bottle or Platinum Genomes). I'm not sure that the data present in the 1001 Arabidopsis Genomes projects would sufficiently allow such a comparison.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/606
https://github.com/google/deepvariant/issues/606:748,availability,error,error,748,"Hi @sivianil . Thank you for the files. With them, I was able to get a better idea of the problem. The problem is that the file that you are providing as your truth set file (801_snp_short_indel_with_quality_reference.vcf.gz) has an entry on chr1 position 8669 which disagrees in the REF field with the file that you are providing as your reference (pseudo801.fasta.gz). According to *pseudo801.fasta.gz* the reference base is *G*. The line in *801_snp_short_indel_with_quality_reference.vcf.gz* has an entry on this field as:. ```. 1 8669 . C . 38 PASS DP=16 GT:GQ:DP 0|0:38:16. ```. According to this VCF, the reference base listed at this position is *C*. You will get this error with hap.py regardless the content of the DeepVariant file. This error does not relate to DeepVariant at all. If you want to do any comparison with 801_snp_short_indel_with_quality_reference.vcf.gz you will need to reconcile the difference between the reference and the contents of the VCF. . Does it really make sense to apply hap.py to this dataset? Hap.py was developed for comparison with highly curated truth datasets made with multiple technologies and/or corroborated by extensive pedigree information (e.g. as is available in Genome in a Bottle or Platinum Genomes). I'm not sure that the data present in the 1001 Arabidopsis Genomes projects would sufficiently allow such a comparison.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/606
https://github.com/google/deepvariant/issues/606:1204,availability,avail,available,1204,"Hi @sivianil . Thank you for the files. With them, I was able to get a better idea of the problem. The problem is that the file that you are providing as your truth set file (801_snp_short_indel_with_quality_reference.vcf.gz) has an entry on chr1 position 8669 which disagrees in the REF field with the file that you are providing as your reference (pseudo801.fasta.gz). According to *pseudo801.fasta.gz* the reference base is *G*. The line in *801_snp_short_indel_with_quality_reference.vcf.gz* has an entry on this field as:. ```. 1 8669 . C . 38 PASS DP=16 GT:GQ:DP 0|0:38:16. ```. According to this VCF, the reference base listed at this position is *C*. You will get this error with hap.py regardless the content of the DeepVariant file. This error does not relate to DeepVariant at all. If you want to do any comparison with 801_snp_short_indel_with_quality_reference.vcf.gz you will need to reconcile the difference between the reference and the contents of the VCF. . Does it really make sense to apply hap.py to this dataset? Hap.py was developed for comparison with highly curated truth datasets made with multiple technologies and/or corroborated by extensive pedigree information (e.g. as is available in Genome in a Bottle or Platinum Genomes). I'm not sure that the data present in the 1001 Arabidopsis Genomes projects would sufficiently allow such a comparison.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/606
https://github.com/google/deepvariant/issues/606:1161,modifiability,extens,extensive,1161,"Hi @sivianil . Thank you for the files. With them, I was able to get a better idea of the problem. The problem is that the file that you are providing as your truth set file (801_snp_short_indel_with_quality_reference.vcf.gz) has an entry on chr1 position 8669 which disagrees in the REF field with the file that you are providing as your reference (pseudo801.fasta.gz). According to *pseudo801.fasta.gz* the reference base is *G*. The line in *801_snp_short_indel_with_quality_reference.vcf.gz* has an entry on this field as:. ```. 1 8669 . C . 38 PASS DP=16 GT:GQ:DP 0|0:38:16. ```. According to this VCF, the reference base listed at this position is *C*. You will get this error with hap.py regardless the content of the DeepVariant file. This error does not relate to DeepVariant at all. If you want to do any comparison with 801_snp_short_indel_with_quality_reference.vcf.gz you will need to reconcile the difference between the reference and the contents of the VCF. . Does it really make sense to apply hap.py to this dataset? Hap.py was developed for comparison with highly curated truth datasets made with multiple technologies and/or corroborated by extensive pedigree information (e.g. as is available in Genome in a Bottle or Platinum Genomes). I'm not sure that the data present in the 1001 Arabidopsis Genomes projects would sufficiently allow such a comparison.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/606
https://github.com/google/deepvariant/issues/606:677,performance,error,error,677,"Hi @sivianil . Thank you for the files. With them, I was able to get a better idea of the problem. The problem is that the file that you are providing as your truth set file (801_snp_short_indel_with_quality_reference.vcf.gz) has an entry on chr1 position 8669 which disagrees in the REF field with the file that you are providing as your reference (pseudo801.fasta.gz). According to *pseudo801.fasta.gz* the reference base is *G*. The line in *801_snp_short_indel_with_quality_reference.vcf.gz* has an entry on this field as:. ```. 1 8669 . C . 38 PASS DP=16 GT:GQ:DP 0|0:38:16. ```. According to this VCF, the reference base listed at this position is *C*. You will get this error with hap.py regardless the content of the DeepVariant file. This error does not relate to DeepVariant at all. If you want to do any comparison with 801_snp_short_indel_with_quality_reference.vcf.gz you will need to reconcile the difference between the reference and the contents of the VCF. . Does it really make sense to apply hap.py to this dataset? Hap.py was developed for comparison with highly curated truth datasets made with multiple technologies and/or corroborated by extensive pedigree information (e.g. as is available in Genome in a Bottle or Platinum Genomes). I'm not sure that the data present in the 1001 Arabidopsis Genomes projects would sufficiently allow such a comparison.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/606
https://github.com/google/deepvariant/issues/606:710,performance,content,content,710,"Hi @sivianil . Thank you for the files. With them, I was able to get a better idea of the problem. The problem is that the file that you are providing as your truth set file (801_snp_short_indel_with_quality_reference.vcf.gz) has an entry on chr1 position 8669 which disagrees in the REF field with the file that you are providing as your reference (pseudo801.fasta.gz). According to *pseudo801.fasta.gz* the reference base is *G*. The line in *801_snp_short_indel_with_quality_reference.vcf.gz* has an entry on this field as:. ```. 1 8669 . C . 38 PASS DP=16 GT:GQ:DP 0|0:38:16. ```. According to this VCF, the reference base listed at this position is *C*. You will get this error with hap.py regardless the content of the DeepVariant file. This error does not relate to DeepVariant at all. If you want to do any comparison with 801_snp_short_indel_with_quality_reference.vcf.gz you will need to reconcile the difference between the reference and the contents of the VCF. . Does it really make sense to apply hap.py to this dataset? Hap.py was developed for comparison with highly curated truth datasets made with multiple technologies and/or corroborated by extensive pedigree information (e.g. as is available in Genome in a Bottle or Platinum Genomes). I'm not sure that the data present in the 1001 Arabidopsis Genomes projects would sufficiently allow such a comparison.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/606
https://github.com/google/deepvariant/issues/606:748,performance,error,error,748,"Hi @sivianil . Thank you for the files. With them, I was able to get a better idea of the problem. The problem is that the file that you are providing as your truth set file (801_snp_short_indel_with_quality_reference.vcf.gz) has an entry on chr1 position 8669 which disagrees in the REF field with the file that you are providing as your reference (pseudo801.fasta.gz). According to *pseudo801.fasta.gz* the reference base is *G*. The line in *801_snp_short_indel_with_quality_reference.vcf.gz* has an entry on this field as:. ```. 1 8669 . C . 38 PASS DP=16 GT:GQ:DP 0|0:38:16. ```. According to this VCF, the reference base listed at this position is *C*. You will get this error with hap.py regardless the content of the DeepVariant file. This error does not relate to DeepVariant at all. If you want to do any comparison with 801_snp_short_indel_with_quality_reference.vcf.gz you will need to reconcile the difference between the reference and the contents of the VCF. . Does it really make sense to apply hap.py to this dataset? Hap.py was developed for comparison with highly curated truth datasets made with multiple technologies and/or corroborated by extensive pedigree information (e.g. as is available in Genome in a Bottle or Platinum Genomes). I'm not sure that the data present in the 1001 Arabidopsis Genomes projects would sufficiently allow such a comparison.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/606
https://github.com/google/deepvariant/issues/606:953,performance,content,contents,953,"Hi @sivianil . Thank you for the files. With them, I was able to get a better idea of the problem. The problem is that the file that you are providing as your truth set file (801_snp_short_indel_with_quality_reference.vcf.gz) has an entry on chr1 position 8669 which disagrees in the REF field with the file that you are providing as your reference (pseudo801.fasta.gz). According to *pseudo801.fasta.gz* the reference base is *G*. The line in *801_snp_short_indel_with_quality_reference.vcf.gz* has an entry on this field as:. ```. 1 8669 . C . 38 PASS DP=16 GT:GQ:DP 0|0:38:16. ```. According to this VCF, the reference base listed at this position is *C*. You will get this error with hap.py regardless the content of the DeepVariant file. This error does not relate to DeepVariant at all. If you want to do any comparison with 801_snp_short_indel_with_quality_reference.vcf.gz you will need to reconcile the difference between the reference and the contents of the VCF. . Does it really make sense to apply hap.py to this dataset? Hap.py was developed for comparison with highly curated truth datasets made with multiple technologies and/or corroborated by extensive pedigree information (e.g. as is available in Genome in a Bottle or Platinum Genomes). I'm not sure that the data present in the 1001 Arabidopsis Genomes projects would sufficiently allow such a comparison.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/606
https://github.com/google/deepvariant/issues/606:754,reliability,doe,does,754,"Hi @sivianil . Thank you for the files. With them, I was able to get a better idea of the problem. The problem is that the file that you are providing as your truth set file (801_snp_short_indel_with_quality_reference.vcf.gz) has an entry on chr1 position 8669 which disagrees in the REF field with the file that you are providing as your reference (pseudo801.fasta.gz). According to *pseudo801.fasta.gz* the reference base is *G*. The line in *801_snp_short_indel_with_quality_reference.vcf.gz* has an entry on this field as:. ```. 1 8669 . C . 38 PASS DP=16 GT:GQ:DP 0|0:38:16. ```. According to this VCF, the reference base listed at this position is *C*. You will get this error with hap.py regardless the content of the DeepVariant file. This error does not relate to DeepVariant at all. If you want to do any comparison with 801_snp_short_indel_with_quality_reference.vcf.gz you will need to reconcile the difference between the reference and the contents of the VCF. . Does it really make sense to apply hap.py to this dataset? Hap.py was developed for comparison with highly curated truth datasets made with multiple technologies and/or corroborated by extensive pedigree information (e.g. as is available in Genome in a Bottle or Platinum Genomes). I'm not sure that the data present in the 1001 Arabidopsis Genomes projects would sufficiently allow such a comparison.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/606
https://github.com/google/deepvariant/issues/606:976,reliability,Doe,Does,976,"Hi @sivianil . Thank you for the files. With them, I was able to get a better idea of the problem. The problem is that the file that you are providing as your truth set file (801_snp_short_indel_with_quality_reference.vcf.gz) has an entry on chr1 position 8669 which disagrees in the REF field with the file that you are providing as your reference (pseudo801.fasta.gz). According to *pseudo801.fasta.gz* the reference base is *G*. The line in *801_snp_short_indel_with_quality_reference.vcf.gz* has an entry on this field as:. ```. 1 8669 . C . 38 PASS DP=16 GT:GQ:DP 0|0:38:16. ```. According to this VCF, the reference base listed at this position is *C*. You will get this error with hap.py regardless the content of the DeepVariant file. This error does not relate to DeepVariant at all. If you want to do any comparison with 801_snp_short_indel_with_quality_reference.vcf.gz you will need to reconcile the difference between the reference and the contents of the VCF. . Does it really make sense to apply hap.py to this dataset? Hap.py was developed for comparison with highly curated truth datasets made with multiple technologies and/or corroborated by extensive pedigree information (e.g. as is available in Genome in a Bottle or Platinum Genomes). I'm not sure that the data present in the 1001 Arabidopsis Genomes projects would sufficiently allow such a comparison.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/606
https://github.com/google/deepvariant/issues/606:1204,reliability,availab,available,1204,"Hi @sivianil . Thank you for the files. With them, I was able to get a better idea of the problem. The problem is that the file that you are providing as your truth set file (801_snp_short_indel_with_quality_reference.vcf.gz) has an entry on chr1 position 8669 which disagrees in the REF field with the file that you are providing as your reference (pseudo801.fasta.gz). According to *pseudo801.fasta.gz* the reference base is *G*. The line in *801_snp_short_indel_with_quality_reference.vcf.gz* has an entry on this field as:. ```. 1 8669 . C . 38 PASS DP=16 GT:GQ:DP 0|0:38:16. ```. According to this VCF, the reference base listed at this position is *C*. You will get this error with hap.py regardless the content of the DeepVariant file. This error does not relate to DeepVariant at all. If you want to do any comparison with 801_snp_short_indel_with_quality_reference.vcf.gz you will need to reconcile the difference between the reference and the contents of the VCF. . Does it really make sense to apply hap.py to this dataset? Hap.py was developed for comparison with highly curated truth datasets made with multiple technologies and/or corroborated by extensive pedigree information (e.g. as is available in Genome in a Bottle or Platinum Genomes). I'm not sure that the data present in the 1001 Arabidopsis Genomes projects would sufficiently allow such a comparison.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/606
https://github.com/google/deepvariant/issues/606:677,safety,error,error,677,"Hi @sivianil . Thank you for the files. With them, I was able to get a better idea of the problem. The problem is that the file that you are providing as your truth set file (801_snp_short_indel_with_quality_reference.vcf.gz) has an entry on chr1 position 8669 which disagrees in the REF field with the file that you are providing as your reference (pseudo801.fasta.gz). According to *pseudo801.fasta.gz* the reference base is *G*. The line in *801_snp_short_indel_with_quality_reference.vcf.gz* has an entry on this field as:. ```. 1 8669 . C . 38 PASS DP=16 GT:GQ:DP 0|0:38:16. ```. According to this VCF, the reference base listed at this position is *C*. You will get this error with hap.py regardless the content of the DeepVariant file. This error does not relate to DeepVariant at all. If you want to do any comparison with 801_snp_short_indel_with_quality_reference.vcf.gz you will need to reconcile the difference between the reference and the contents of the VCF. . Does it really make sense to apply hap.py to this dataset? Hap.py was developed for comparison with highly curated truth datasets made with multiple technologies and/or corroborated by extensive pedigree information (e.g. as is available in Genome in a Bottle or Platinum Genomes). I'm not sure that the data present in the 1001 Arabidopsis Genomes projects would sufficiently allow such a comparison.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/606
https://github.com/google/deepvariant/issues/606:748,safety,error,error,748,"Hi @sivianil . Thank you for the files. With them, I was able to get a better idea of the problem. The problem is that the file that you are providing as your truth set file (801_snp_short_indel_with_quality_reference.vcf.gz) has an entry on chr1 position 8669 which disagrees in the REF field with the file that you are providing as your reference (pseudo801.fasta.gz). According to *pseudo801.fasta.gz* the reference base is *G*. The line in *801_snp_short_indel_with_quality_reference.vcf.gz* has an entry on this field as:. ```. 1 8669 . C . 38 PASS DP=16 GT:GQ:DP 0|0:38:16. ```. According to this VCF, the reference base listed at this position is *C*. You will get this error with hap.py regardless the content of the DeepVariant file. This error does not relate to DeepVariant at all. If you want to do any comparison with 801_snp_short_indel_with_quality_reference.vcf.gz you will need to reconcile the difference between the reference and the contents of the VCF. . Does it really make sense to apply hap.py to this dataset? Hap.py was developed for comparison with highly curated truth datasets made with multiple technologies and/or corroborated by extensive pedigree information (e.g. as is available in Genome in a Bottle or Platinum Genomes). I'm not sure that the data present in the 1001 Arabidopsis Genomes projects would sufficiently allow such a comparison.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/606
https://github.com/google/deepvariant/issues/606:1204,safety,avail,available,1204,"Hi @sivianil . Thank you for the files. With them, I was able to get a better idea of the problem. The problem is that the file that you are providing as your truth set file (801_snp_short_indel_with_quality_reference.vcf.gz) has an entry on chr1 position 8669 which disagrees in the REF field with the file that you are providing as your reference (pseudo801.fasta.gz). According to *pseudo801.fasta.gz* the reference base is *G*. The line in *801_snp_short_indel_with_quality_reference.vcf.gz* has an entry on this field as:. ```. 1 8669 . C . 38 PASS DP=16 GT:GQ:DP 0|0:38:16. ```. According to this VCF, the reference base listed at this position is *C*. You will get this error with hap.py regardless the content of the DeepVariant file. This error does not relate to DeepVariant at all. If you want to do any comparison with 801_snp_short_indel_with_quality_reference.vcf.gz you will need to reconcile the difference between the reference and the contents of the VCF. . Does it really make sense to apply hap.py to this dataset? Hap.py was developed for comparison with highly curated truth datasets made with multiple technologies and/or corroborated by extensive pedigree information (e.g. as is available in Genome in a Bottle or Platinum Genomes). I'm not sure that the data present in the 1001 Arabidopsis Genomes projects would sufficiently allow such a comparison.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/606
https://github.com/google/deepvariant/issues/606:1204,security,availab,available,1204,"Hi @sivianil . Thank you for the files. With them, I was able to get a better idea of the problem. The problem is that the file that you are providing as your truth set file (801_snp_short_indel_with_quality_reference.vcf.gz) has an entry on chr1 position 8669 which disagrees in the REF field with the file that you are providing as your reference (pseudo801.fasta.gz). According to *pseudo801.fasta.gz* the reference base is *G*. The line in *801_snp_short_indel_with_quality_reference.vcf.gz* has an entry on this field as:. ```. 1 8669 . C . 38 PASS DP=16 GT:GQ:DP 0|0:38:16. ```. According to this VCF, the reference base listed at this position is *C*. You will get this error with hap.py regardless the content of the DeepVariant file. This error does not relate to DeepVariant at all. If you want to do any comparison with 801_snp_short_indel_with_quality_reference.vcf.gz you will need to reconcile the difference between the reference and the contents of the VCF. . Does it really make sense to apply hap.py to this dataset? Hap.py was developed for comparison with highly curated truth datasets made with multiple technologies and/or corroborated by extensive pedigree information (e.g. as is available in Genome in a Bottle or Platinum Genomes). I'm not sure that the data present in the 1001 Arabidopsis Genomes projects would sufficiently allow such a comparison.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/606
https://github.com/google/deepvariant/issues/606:677,usability,error,error,677,"Hi @sivianil . Thank you for the files. With them, I was able to get a better idea of the problem. The problem is that the file that you are providing as your truth set file (801_snp_short_indel_with_quality_reference.vcf.gz) has an entry on chr1 position 8669 which disagrees in the REF field with the file that you are providing as your reference (pseudo801.fasta.gz). According to *pseudo801.fasta.gz* the reference base is *G*. The line in *801_snp_short_indel_with_quality_reference.vcf.gz* has an entry on this field as:. ```. 1 8669 . C . 38 PASS DP=16 GT:GQ:DP 0|0:38:16. ```. According to this VCF, the reference base listed at this position is *C*. You will get this error with hap.py regardless the content of the DeepVariant file. This error does not relate to DeepVariant at all. If you want to do any comparison with 801_snp_short_indel_with_quality_reference.vcf.gz you will need to reconcile the difference between the reference and the contents of the VCF. . Does it really make sense to apply hap.py to this dataset? Hap.py was developed for comparison with highly curated truth datasets made with multiple technologies and/or corroborated by extensive pedigree information (e.g. as is available in Genome in a Bottle or Platinum Genomes). I'm not sure that the data present in the 1001 Arabidopsis Genomes projects would sufficiently allow such a comparison.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/606
https://github.com/google/deepvariant/issues/606:748,usability,error,error,748,"Hi @sivianil . Thank you for the files. With them, I was able to get a better idea of the problem. The problem is that the file that you are providing as your truth set file (801_snp_short_indel_with_quality_reference.vcf.gz) has an entry on chr1 position 8669 which disagrees in the REF field with the file that you are providing as your reference (pseudo801.fasta.gz). According to *pseudo801.fasta.gz* the reference base is *G*. The line in *801_snp_short_indel_with_quality_reference.vcf.gz* has an entry on this field as:. ```. 1 8669 . C . 38 PASS DP=16 GT:GQ:DP 0|0:38:16. ```. According to this VCF, the reference base listed at this position is *C*. You will get this error with hap.py regardless the content of the DeepVariant file. This error does not relate to DeepVariant at all. If you want to do any comparison with 801_snp_short_indel_with_quality_reference.vcf.gz you will need to reconcile the difference between the reference and the contents of the VCF. . Does it really make sense to apply hap.py to this dataset? Hap.py was developed for comparison with highly curated truth datasets made with multiple technologies and/or corroborated by extensive pedigree information (e.g. as is available in Genome in a Bottle or Platinum Genomes). I'm not sure that the data present in the 1001 Arabidopsis Genomes projects would sufficiently allow such a comparison.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/606
https://github.com/google/deepvariant/issues/606:726,availability,error,error,726,"Hi Andrew,. The Gold standard vcf file you mentioned above is only based on SHORE pipeline. Yes, you were absolutely correct! The reference bases acc to fasta file and VCF file doesn't match. Excluding that, when I tried to run the model on subset of positions for chr1:100-249000 [chr1:250951-250951 disagree on what the reference bases should be! (A != T) ] , I can't get any true positives in the result. The result contains only FN, so the model didn't make call for single variant position which are present in Truth VCF file. Please look at the screenshot I attached. I will ask my supervisor, if there is any possibility to reconcile the difference b/w the reference and bases of the VCF. Thanks for finding the actual error. . Best,. Anil. <img width=""1505"" alt=""Screenshot 2023-01-30 at 15 04 23"" src=""https://user-images.githubusercontent.com/75676816/215498753-e4340ba2-0573-4b13-a10d-33fb593cfd9f.png"">.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/606
https://github.com/google/deepvariant/issues/606:82,deployability,pipelin,pipeline,82,"Hi Andrew,. The Gold standard vcf file you mentioned above is only based on SHORE pipeline. Yes, you were absolutely correct! The reference bases acc to fasta file and VCF file doesn't match. Excluding that, when I tried to run the model on subset of positions for chr1:100-249000 [chr1:250951-250951 disagree on what the reference bases should be! (A != T) ] , I can't get any true positives in the result. The result contains only FN, so the model didn't make call for single variant position which are present in Truth VCF file. Please look at the screenshot I attached. I will ask my supervisor, if there is any possibility to reconcile the difference b/w the reference and bases of the VCF. Thanks for finding the actual error. . Best,. Anil. <img width=""1505"" alt=""Screenshot 2023-01-30 at 15 04 23"" src=""https://user-images.githubusercontent.com/75676816/215498753-e4340ba2-0573-4b13-a10d-33fb593cfd9f.png"">.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/606
https://github.com/google/deepvariant/issues/606:419,deployability,contain,contains,419,"Hi Andrew,. The Gold standard vcf file you mentioned above is only based on SHORE pipeline. Yes, you were absolutely correct! The reference bases acc to fasta file and VCF file doesn't match. Excluding that, when I tried to run the model on subset of positions for chr1:100-249000 [chr1:250951-250951 disagree on what the reference bases should be! (A != T) ] , I can't get any true positives in the result. The result contains only FN, so the model didn't make call for single variant position which are present in Truth VCF file. Please look at the screenshot I attached. I will ask my supervisor, if there is any possibility to reconcile the difference b/w the reference and bases of the VCF. Thanks for finding the actual error. . Best,. Anil. <img width=""1505"" alt=""Screenshot 2023-01-30 at 15 04 23"" src=""https://user-images.githubusercontent.com/75676816/215498753-e4340ba2-0573-4b13-a10d-33fb593cfd9f.png"">.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/606
https://github.com/google/deepvariant/issues/606:232,energy efficiency,model,model,232,"Hi Andrew,. The Gold standard vcf file you mentioned above is only based on SHORE pipeline. Yes, you were absolutely correct! The reference bases acc to fasta file and VCF file doesn't match. Excluding that, when I tried to run the model on subset of positions for chr1:100-249000 [chr1:250951-250951 disagree on what the reference bases should be! (A != T) ] , I can't get any true positives in the result. The result contains only FN, so the model didn't make call for single variant position which are present in Truth VCF file. Please look at the screenshot I attached. I will ask my supervisor, if there is any possibility to reconcile the difference b/w the reference and bases of the VCF. Thanks for finding the actual error. . Best,. Anil. <img width=""1505"" alt=""Screenshot 2023-01-30 at 15 04 23"" src=""https://user-images.githubusercontent.com/75676816/215498753-e4340ba2-0573-4b13-a10d-33fb593cfd9f.png"">.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/606
https://github.com/google/deepvariant/issues/606:444,energy efficiency,model,model,444,"Hi Andrew,. The Gold standard vcf file you mentioned above is only based on SHORE pipeline. Yes, you were absolutely correct! The reference bases acc to fasta file and VCF file doesn't match. Excluding that, when I tried to run the model on subset of positions for chr1:100-249000 [chr1:250951-250951 disagree on what the reference bases should be! (A != T) ] , I can't get any true positives in the result. The result contains only FN, so the model didn't make call for single variant position which are present in Truth VCF file. Please look at the screenshot I attached. I will ask my supervisor, if there is any possibility to reconcile the difference b/w the reference and bases of the VCF. Thanks for finding the actual error. . Best,. Anil. <img width=""1505"" alt=""Screenshot 2023-01-30 at 15 04 23"" src=""https://user-images.githubusercontent.com/75676816/215498753-e4340ba2-0573-4b13-a10d-33fb593cfd9f.png"">.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/606
https://github.com/google/deepvariant/issues/606:82,integrability,pipelin,pipeline,82,"Hi Andrew,. The Gold standard vcf file you mentioned above is only based on SHORE pipeline. Yes, you were absolutely correct! The reference bases acc to fasta file and VCF file doesn't match. Excluding that, when I tried to run the model on subset of positions for chr1:100-249000 [chr1:250951-250951 disagree on what the reference bases should be! (A != T) ] , I can't get any true positives in the result. The result contains only FN, so the model didn't make call for single variant position which are present in Truth VCF file. Please look at the screenshot I attached. I will ask my supervisor, if there is any possibility to reconcile the difference b/w the reference and bases of the VCF. Thanks for finding the actual error. . Best,. Anil. <img width=""1505"" alt=""Screenshot 2023-01-30 at 15 04 23"" src=""https://user-images.githubusercontent.com/75676816/215498753-e4340ba2-0573-4b13-a10d-33fb593cfd9f.png"">.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/606
https://github.com/google/deepvariant/issues/606:241,integrability,sub,subset,241,"Hi Andrew,. The Gold standard vcf file you mentioned above is only based on SHORE pipeline. Yes, you were absolutely correct! The reference bases acc to fasta file and VCF file doesn't match. Excluding that, when I tried to run the model on subset of positions for chr1:100-249000 [chr1:250951-250951 disagree on what the reference bases should be! (A != T) ] , I can't get any true positives in the result. The result contains only FN, so the model didn't make call for single variant position which are present in Truth VCF file. Please look at the screenshot I attached. I will ask my supervisor, if there is any possibility to reconcile the difference b/w the reference and bases of the VCF. Thanks for finding the actual error. . Best,. Anil. <img width=""1505"" alt=""Screenshot 2023-01-30 at 15 04 23"" src=""https://user-images.githubusercontent.com/75676816/215498753-e4340ba2-0573-4b13-a10d-33fb593cfd9f.png"">.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/606
https://github.com/google/deepvariant/issues/606:21,interoperability,standard,standard,21,"Hi Andrew,. The Gold standard vcf file you mentioned above is only based on SHORE pipeline. Yes, you were absolutely correct! The reference bases acc to fasta file and VCF file doesn't match. Excluding that, when I tried to run the model on subset of positions for chr1:100-249000 [chr1:250951-250951 disagree on what the reference bases should be! (A != T) ] , I can't get any true positives in the result. The result contains only FN, so the model didn't make call for single variant position which are present in Truth VCF file. Please look at the screenshot I attached. I will ask my supervisor, if there is any possibility to reconcile the difference b/w the reference and bases of the VCF. Thanks for finding the actual error. . Best,. Anil. <img width=""1505"" alt=""Screenshot 2023-01-30 at 15 04 23"" src=""https://user-images.githubusercontent.com/75676816/215498753-e4340ba2-0573-4b13-a10d-33fb593cfd9f.png"">.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/606
https://github.com/google/deepvariant/issues/606:726,performance,error,error,726,"Hi Andrew,. The Gold standard vcf file you mentioned above is only based on SHORE pipeline. Yes, you were absolutely correct! The reference bases acc to fasta file and VCF file doesn't match. Excluding that, when I tried to run the model on subset of positions for chr1:100-249000 [chr1:250951-250951 disagree on what the reference bases should be! (A != T) ] , I can't get any true positives in the result. The result contains only FN, so the model didn't make call for single variant position which are present in Truth VCF file. Please look at the screenshot I attached. I will ask my supervisor, if there is any possibility to reconcile the difference b/w the reference and bases of the VCF. Thanks for finding the actual error. . Best,. Anil. <img width=""1505"" alt=""Screenshot 2023-01-30 at 15 04 23"" src=""https://user-images.githubusercontent.com/75676816/215498753-e4340ba2-0573-4b13-a10d-33fb593cfd9f.png"">.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/606
https://github.com/google/deepvariant/issues/606:177,reliability,doe,doesn,177,"Hi Andrew,. The Gold standard vcf file you mentioned above is only based on SHORE pipeline. Yes, you were absolutely correct! The reference bases acc to fasta file and VCF file doesn't match. Excluding that, when I tried to run the model on subset of positions for chr1:100-249000 [chr1:250951-250951 disagree on what the reference bases should be! (A != T) ] , I can't get any true positives in the result. The result contains only FN, so the model didn't make call for single variant position which are present in Truth VCF file. Please look at the screenshot I attached. I will ask my supervisor, if there is any possibility to reconcile the difference b/w the reference and bases of the VCF. Thanks for finding the actual error. . Best,. Anil. <img width=""1505"" alt=""Screenshot 2023-01-30 at 15 04 23"" src=""https://user-images.githubusercontent.com/75676816/215498753-e4340ba2-0573-4b13-a10d-33fb593cfd9f.png"">.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/606
https://github.com/google/deepvariant/issues/606:726,safety,error,error,726,"Hi Andrew,. The Gold standard vcf file you mentioned above is only based on SHORE pipeline. Yes, you were absolutely correct! The reference bases acc to fasta file and VCF file doesn't match. Excluding that, when I tried to run the model on subset of positions for chr1:100-249000 [chr1:250951-250951 disagree on what the reference bases should be! (A != T) ] , I can't get any true positives in the result. The result contains only FN, so the model didn't make call for single variant position which are present in Truth VCF file. Please look at the screenshot I attached. I will ask my supervisor, if there is any possibility to reconcile the difference b/w the reference and bases of the VCF. Thanks for finding the actual error. . Best,. Anil. <img width=""1505"" alt=""Screenshot 2023-01-30 at 15 04 23"" src=""https://user-images.githubusercontent.com/75676816/215498753-e4340ba2-0573-4b13-a10d-33fb593cfd9f.png"">.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/606
https://github.com/google/deepvariant/issues/606:232,security,model,model,232,"Hi Andrew,. The Gold standard vcf file you mentioned above is only based on SHORE pipeline. Yes, you were absolutely correct! The reference bases acc to fasta file and VCF file doesn't match. Excluding that, when I tried to run the model on subset of positions for chr1:100-249000 [chr1:250951-250951 disagree on what the reference bases should be! (A != T) ] , I can't get any true positives in the result. The result contains only FN, so the model didn't make call for single variant position which are present in Truth VCF file. Please look at the screenshot I attached. I will ask my supervisor, if there is any possibility to reconcile the difference b/w the reference and bases of the VCF. Thanks for finding the actual error. . Best,. Anil. <img width=""1505"" alt=""Screenshot 2023-01-30 at 15 04 23"" src=""https://user-images.githubusercontent.com/75676816/215498753-e4340ba2-0573-4b13-a10d-33fb593cfd9f.png"">.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/606
https://github.com/google/deepvariant/issues/606:444,security,model,model,444,"Hi Andrew,. The Gold standard vcf file you mentioned above is only based on SHORE pipeline. Yes, you were absolutely correct! The reference bases acc to fasta file and VCF file doesn't match. Excluding that, when I tried to run the model on subset of positions for chr1:100-249000 [chr1:250951-250951 disagree on what the reference bases should be! (A != T) ] , I can't get any true positives in the result. The result contains only FN, so the model didn't make call for single variant position which are present in Truth VCF file. Please look at the screenshot I attached. I will ask my supervisor, if there is any possibility to reconcile the difference b/w the reference and bases of the VCF. Thanks for finding the actual error. . Best,. Anil. <img width=""1505"" alt=""Screenshot 2023-01-30 at 15 04 23"" src=""https://user-images.githubusercontent.com/75676816/215498753-e4340ba2-0573-4b13-a10d-33fb593cfd9f.png"">.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/606
https://github.com/google/deepvariant/issues/606:726,usability,error,error,726,"Hi Andrew,. The Gold standard vcf file you mentioned above is only based on SHORE pipeline. Yes, you were absolutely correct! The reference bases acc to fasta file and VCF file doesn't match. Excluding that, when I tried to run the model on subset of positions for chr1:100-249000 [chr1:250951-250951 disagree on what the reference bases should be! (A != T) ] , I can't get any true positives in the result. The result contains only FN, so the model didn't make call for single variant position which are present in Truth VCF file. Please look at the screenshot I attached. I will ask my supervisor, if there is any possibility to reconcile the difference b/w the reference and bases of the VCF. Thanks for finding the actual error. . Best,. Anil. <img width=""1505"" alt=""Screenshot 2023-01-30 at 15 04 23"" src=""https://user-images.githubusercontent.com/75676816/215498753-e4340ba2-0573-4b13-a10d-33fb593cfd9f.png"">.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/606
https://github.com/google/deepvariant/issues/606:819,usability,user,user-images,819,"Hi Andrew,. The Gold standard vcf file you mentioned above is only based on SHORE pipeline. Yes, you were absolutely correct! The reference bases acc to fasta file and VCF file doesn't match. Excluding that, when I tried to run the model on subset of positions for chr1:100-249000 [chr1:250951-250951 disagree on what the reference bases should be! (A != T) ] , I can't get any true positives in the result. The result contains only FN, so the model didn't make call for single variant position which are present in Truth VCF file. Please look at the screenshot I attached. I will ask my supervisor, if there is any possibility to reconcile the difference b/w the reference and bases of the VCF. Thanks for finding the actual error. . Best,. Anil. <img width=""1505"" alt=""Screenshot 2023-01-30 at 15 04 23"" src=""https://user-images.githubusercontent.com/75676816/215498753-e4340ba2-0573-4b13-a10d-33fb593cfd9f.png"">.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/606
https://github.com/google/deepvariant/issues/606:288,reliability,doe,doesn,288,"Hi @sivianil . I don't think that the comparison you posted is working. I note that your Truth file lists the contig names as ""1, 2, 3,..."" while the DeepVariant VCF and Reference genome name the chromosomes as ""chr1, chr2, chr3, ..."". It seems almost certain that your hap.py comparison doesn't see any overlap in the contigs present in your truth set and the contigs present in the Reference genome.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/606
https://github.com/google/deepvariant/issues/606:62,usability,close,close,62,@sivianil Feel free to re-open if you want to follow up. I'll close this issue for now.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/606
https://github.com/google/deepvariant/issues/607:21,availability,error,errors,21,"Hello Saurabh,. What errors do you get? Could you paste the output?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/607
https://github.com/google/deepvariant/issues/607:21,performance,error,errors,21,"Hello Saurabh,. What errors do you get? Could you paste the output?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/607
https://github.com/google/deepvariant/issues/607:21,safety,error,errors,21,"Hello Saurabh,. What errors do you get? Could you paste the output?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/607
https://github.com/google/deepvariant/issues/607:21,usability,error,errors,21,"Hello Saurabh,. What errors do you get? Could you paste the output?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/607
https://github.com/google/deepvariant/issues/607:0,deployability,Updat,Updating,0,"Updating TF version might not be straightforward. We're planning to update to TF 2.11 in the next release, which should be out in the next few months.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/607
https://github.com/google/deepvariant/issues/607:12,deployability,version,version,12,"Updating TF version might not be straightforward. We're planning to update to TF 2.11 in the next release, which should be out in the next few months.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/607
https://github.com/google/deepvariant/issues/607:68,deployability,updat,update,68,"Updating TF version might not be straightforward. We're planning to update to TF 2.11 in the next release, which should be out in the next few months.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/607
https://github.com/google/deepvariant/issues/607:98,deployability,releas,release,98,"Updating TF version might not be straightforward. We're planning to update to TF 2.11 in the next release, which should be out in the next few months.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/607
https://github.com/google/deepvariant/issues/607:12,integrability,version,version,12,"Updating TF version might not be straightforward. We're planning to update to TF 2.11 in the next release, which should be out in the next few months.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/607
https://github.com/google/deepvariant/issues/607:12,modifiability,version,version,12,"Updating TF version might not be straightforward. We're planning to update to TF 2.11 in the next release, which should be out in the next few months.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/607
https://github.com/google/deepvariant/issues/607:0,safety,Updat,Updating,0,"Updating TF version might not be straightforward. We're planning to update to TF 2.11 in the next release, which should be out in the next few months.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/607
https://github.com/google/deepvariant/issues/607:68,safety,updat,update,68,"Updating TF version might not be straightforward. We're planning to update to TF 2.11 in the next release, which should be out in the next few months.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/607
https://github.com/google/deepvariant/issues/607:0,security,Updat,Updating,0,"Updating TF version might not be straightforward. We're planning to update to TF 2.11 in the next release, which should be out in the next few months.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/607
https://github.com/google/deepvariant/issues/607:68,security,updat,update,68,"Updating TF version might not be straightforward. We're planning to update to TF 2.11 in the next release, which should be out in the next few months.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/607
https://github.com/google/deepvariant/issues/607:56,testability,plan,planning,56,"Updating TF version might not be straightforward. We're planning to update to TF 2.11 in the next release, which should be out in the next few months.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/607
https://github.com/google/deepvariant/issues/607:67,availability,error,error-log,67,"Thanks @akolesnikov and @pichuan . Please find the details and the error-log below. Please note that I have upgraded the following dependencies according to Tensorflow - 2.11. . DV_BAZEL_VERSION=""5.3.0"". DV_GCP_OPTIMIZED_TF_WHL_VERSION=""2.11.0"". ABSL_VERSION=20210324.2. PROTOBUF_VERSION=3.19.6. **Error log:** . (03:11:57) ERROR: /opt/deepvariant/third_party/nucleus/io/BUILD:1051:11: Compiling third_party/nucleus/io/gfile.cc failed: (Exit 1): gcc failed: error executing command. (cd /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/execroot/com_google_deepvariant && \. exec env - \. PATH=/root/bin:/root/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \. PWD=/proc/self/cwd \. PYTHON_BIN_PATH=/usr/local/bin/python3 \. PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages \. TF2_BEHAVIOR=1 \. /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-. Also, at an earlier point in the build, there is one more issue - . Download from https://mirror.bazel.build/github.com/bazelbuild/rules_cc/archive/081771d4a0e9d7d3aa0eed2ef389fa4700dfb23e.tar.gz failed: class java.io.FileNotFoundException GET returned **404 Not Found**.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/607
https://github.com/google/deepvariant/issues/607:298,availability,Error,Error,298,"Thanks @akolesnikov and @pichuan . Please find the details and the error-log below. Please note that I have upgraded the following dependencies according to Tensorflow - 2.11. . DV_BAZEL_VERSION=""5.3.0"". DV_GCP_OPTIMIZED_TF_WHL_VERSION=""2.11.0"". ABSL_VERSION=20210324.2. PROTOBUF_VERSION=3.19.6. **Error log:** . (03:11:57) ERROR: /opt/deepvariant/third_party/nucleus/io/BUILD:1051:11: Compiling third_party/nucleus/io/gfile.cc failed: (Exit 1): gcc failed: error executing command. (cd /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/execroot/com_google_deepvariant && \. exec env - \. PATH=/root/bin:/root/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \. PWD=/proc/self/cwd \. PYTHON_BIN_PATH=/usr/local/bin/python3 \. PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages \. TF2_BEHAVIOR=1 \. /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-. Also, at an earlier point in the build, there is one more issue - . Download from https://mirror.bazel.build/github.com/bazelbuild/rules_cc/archive/081771d4a0e9d7d3aa0eed2ef389fa4700dfb23e.tar.gz failed: class java.io.FileNotFoundException GET returned **404 Not Found**.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/607
https://github.com/google/deepvariant/issues/607:324,availability,ERROR,ERROR,324,"Thanks @akolesnikov and @pichuan . Please find the details and the error-log below. Please note that I have upgraded the following dependencies according to Tensorflow - 2.11. . DV_BAZEL_VERSION=""5.3.0"". DV_GCP_OPTIMIZED_TF_WHL_VERSION=""2.11.0"". ABSL_VERSION=20210324.2. PROTOBUF_VERSION=3.19.6. **Error log:** . (03:11:57) ERROR: /opt/deepvariant/third_party/nucleus/io/BUILD:1051:11: Compiling third_party/nucleus/io/gfile.cc failed: (Exit 1): gcc failed: error executing command. (cd /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/execroot/com_google_deepvariant && \. exec env - \. PATH=/root/bin:/root/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \. PWD=/proc/self/cwd \. PYTHON_BIN_PATH=/usr/local/bin/python3 \. PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages \. TF2_BEHAVIOR=1 \. /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-. Also, at an earlier point in the build, there is one more issue - . Download from https://mirror.bazel.build/github.com/bazelbuild/rules_cc/archive/081771d4a0e9d7d3aa0eed2ef389fa4700dfb23e.tar.gz failed: class java.io.FileNotFoundException GET returned **404 Not Found**.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/607
https://github.com/google/deepvariant/issues/607:458,availability,error,error,458,"Thanks @akolesnikov and @pichuan . Please find the details and the error-log below. Please note that I have upgraded the following dependencies according to Tensorflow - 2.11. . DV_BAZEL_VERSION=""5.3.0"". DV_GCP_OPTIMIZED_TF_WHL_VERSION=""2.11.0"". ABSL_VERSION=20210324.2. PROTOBUF_VERSION=3.19.6. **Error log:** . (03:11:57) ERROR: /opt/deepvariant/third_party/nucleus/io/BUILD:1051:11: Compiling third_party/nucleus/io/gfile.cc failed: (Exit 1): gcc failed: error executing command. (cd /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/execroot/com_google_deepvariant && \. exec env - \. PATH=/root/bin:/root/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \. PWD=/proc/self/cwd \. PYTHON_BIN_PATH=/usr/local/bin/python3 \. PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages \. TF2_BEHAVIOR=1 \. /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-. Also, at an earlier point in the build, there is one more issue - . Download from https://mirror.bazel.build/github.com/bazelbuild/rules_cc/archive/081771d4a0e9d7d3aa0eed2ef389fa4700dfb23e.tar.gz failed: class java.io.FileNotFoundException GET returned **404 Not Found**.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/607
https://github.com/google/deepvariant/issues/607:989,availability,Down,Download,989,"Thanks @akolesnikov and @pichuan . Please find the details and the error-log below. Please note that I have upgraded the following dependencies according to Tensorflow - 2.11. . DV_BAZEL_VERSION=""5.3.0"". DV_GCP_OPTIMIZED_TF_WHL_VERSION=""2.11.0"". ABSL_VERSION=20210324.2. PROTOBUF_VERSION=3.19.6. **Error log:** . (03:11:57) ERROR: /opt/deepvariant/third_party/nucleus/io/BUILD:1051:11: Compiling third_party/nucleus/io/gfile.cc failed: (Exit 1): gcc failed: error executing command. (cd /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/execroot/com_google_deepvariant && \. exec env - \. PATH=/root/bin:/root/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \. PWD=/proc/self/cwd \. PYTHON_BIN_PATH=/usr/local/bin/python3 \. PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages \. TF2_BEHAVIOR=1 \. /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-. Also, at an earlier point in the build, there is one more issue - . Download from https://mirror.bazel.build/github.com/bazelbuild/rules_cc/archive/081771d4a0e9d7d3aa0eed2ef389fa4700dfb23e.tar.gz failed: class java.io.FileNotFoundException GET returned **404 Not Found**.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/607
https://github.com/google/deepvariant/issues/607:73,deployability,log,log,73,"Thanks @akolesnikov and @pichuan . Please find the details and the error-log below. Please note that I have upgraded the following dependencies according to Tensorflow - 2.11. . DV_BAZEL_VERSION=""5.3.0"". DV_GCP_OPTIMIZED_TF_WHL_VERSION=""2.11.0"". ABSL_VERSION=20210324.2. PROTOBUF_VERSION=3.19.6. **Error log:** . (03:11:57) ERROR: /opt/deepvariant/third_party/nucleus/io/BUILD:1051:11: Compiling third_party/nucleus/io/gfile.cc failed: (Exit 1): gcc failed: error executing command. (cd /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/execroot/com_google_deepvariant && \. exec env - \. PATH=/root/bin:/root/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \. PWD=/proc/self/cwd \. PYTHON_BIN_PATH=/usr/local/bin/python3 \. PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages \. TF2_BEHAVIOR=1 \. /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-. Also, at an earlier point in the build, there is one more issue - . Download from https://mirror.bazel.build/github.com/bazelbuild/rules_cc/archive/081771d4a0e9d7d3aa0eed2ef389fa4700dfb23e.tar.gz failed: class java.io.FileNotFoundException GET returned **404 Not Found**.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/607
https://github.com/google/deepvariant/issues/607:108,deployability,upgrad,upgraded,108,"Thanks @akolesnikov and @pichuan . Please find the details and the error-log below. Please note that I have upgraded the following dependencies according to Tensorflow - 2.11. . DV_BAZEL_VERSION=""5.3.0"". DV_GCP_OPTIMIZED_TF_WHL_VERSION=""2.11.0"". ABSL_VERSION=20210324.2. PROTOBUF_VERSION=3.19.6. **Error log:** . (03:11:57) ERROR: /opt/deepvariant/third_party/nucleus/io/BUILD:1051:11: Compiling third_party/nucleus/io/gfile.cc failed: (Exit 1): gcc failed: error executing command. (cd /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/execroot/com_google_deepvariant && \. exec env - \. PATH=/root/bin:/root/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \. PWD=/proc/self/cwd \. PYTHON_BIN_PATH=/usr/local/bin/python3 \. PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages \. TF2_BEHAVIOR=1 \. /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-. Also, at an earlier point in the build, there is one more issue - . Download from https://mirror.bazel.build/github.com/bazelbuild/rules_cc/archive/081771d4a0e9d7d3aa0eed2ef389fa4700dfb23e.tar.gz failed: class java.io.FileNotFoundException GET returned **404 Not Found**.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/607
https://github.com/google/deepvariant/issues/607:131,deployability,depend,dependencies,131,"Thanks @akolesnikov and @pichuan . Please find the details and the error-log below. Please note that I have upgraded the following dependencies according to Tensorflow - 2.11. . DV_BAZEL_VERSION=""5.3.0"". DV_GCP_OPTIMIZED_TF_WHL_VERSION=""2.11.0"". ABSL_VERSION=20210324.2. PROTOBUF_VERSION=3.19.6. **Error log:** . (03:11:57) ERROR: /opt/deepvariant/third_party/nucleus/io/BUILD:1051:11: Compiling third_party/nucleus/io/gfile.cc failed: (Exit 1): gcc failed: error executing command. (cd /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/execroot/com_google_deepvariant && \. exec env - \. PATH=/root/bin:/root/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \. PWD=/proc/self/cwd \. PYTHON_BIN_PATH=/usr/local/bin/python3 \. PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages \. TF2_BEHAVIOR=1 \. /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-. Also, at an earlier point in the build, there is one more issue - . Download from https://mirror.bazel.build/github.com/bazelbuild/rules_cc/archive/081771d4a0e9d7d3aa0eed2ef389fa4700dfb23e.tar.gz failed: class java.io.FileNotFoundException GET returned **404 Not Found**.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/607
https://github.com/google/deepvariant/issues/607:304,deployability,log,log,304,"Thanks @akolesnikov and @pichuan . Please find the details and the error-log below. Please note that I have upgraded the following dependencies according to Tensorflow - 2.11. . DV_BAZEL_VERSION=""5.3.0"". DV_GCP_OPTIMIZED_TF_WHL_VERSION=""2.11.0"". ABSL_VERSION=20210324.2. PROTOBUF_VERSION=3.19.6. **Error log:** . (03:11:57) ERROR: /opt/deepvariant/third_party/nucleus/io/BUILD:1051:11: Compiling third_party/nucleus/io/gfile.cc failed: (Exit 1): gcc failed: error executing command. (cd /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/execroot/com_google_deepvariant && \. exec env - \. PATH=/root/bin:/root/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \. PWD=/proc/self/cwd \. PYTHON_BIN_PATH=/usr/local/bin/python3 \. PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages \. TF2_BEHAVIOR=1 \. /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-. Also, at an earlier point in the build, there is one more issue - . Download from https://mirror.bazel.build/github.com/bazelbuild/rules_cc/archive/081771d4a0e9d7d3aa0eed2ef389fa4700dfb23e.tar.gz failed: class java.io.FileNotFoundException GET returned **404 Not Found**.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/607
https://github.com/google/deepvariant/issues/607:371,deployability,BUILD,BUILD,371,"Thanks @akolesnikov and @pichuan . Please find the details and the error-log below. Please note that I have upgraded the following dependencies according to Tensorflow - 2.11. . DV_BAZEL_VERSION=""5.3.0"". DV_GCP_OPTIMIZED_TF_WHL_VERSION=""2.11.0"". ABSL_VERSION=20210324.2. PROTOBUF_VERSION=3.19.6. **Error log:** . (03:11:57) ERROR: /opt/deepvariant/third_party/nucleus/io/BUILD:1051:11: Compiling third_party/nucleus/io/gfile.cc failed: (Exit 1): gcc failed: error executing command. (cd /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/execroot/com_google_deepvariant && \. exec env - \. PATH=/root/bin:/root/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \. PWD=/proc/self/cwd \. PYTHON_BIN_PATH=/usr/local/bin/python3 \. PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages \. TF2_BEHAVIOR=1 \. /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-. Also, at an earlier point in the build, there is one more issue - . Download from https://mirror.bazel.build/github.com/bazelbuild/rules_cc/archive/081771d4a0e9d7d3aa0eed2ef389fa4700dfb23e.tar.gz failed: class java.io.FileNotFoundException GET returned **404 Not Found**.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/607
https://github.com/google/deepvariant/issues/607:428,deployability,fail,failed,428,"Thanks @akolesnikov and @pichuan . Please find the details and the error-log below. Please note that I have upgraded the following dependencies according to Tensorflow - 2.11. . DV_BAZEL_VERSION=""5.3.0"". DV_GCP_OPTIMIZED_TF_WHL_VERSION=""2.11.0"". ABSL_VERSION=20210324.2. PROTOBUF_VERSION=3.19.6. **Error log:** . (03:11:57) ERROR: /opt/deepvariant/third_party/nucleus/io/BUILD:1051:11: Compiling third_party/nucleus/io/gfile.cc failed: (Exit 1): gcc failed: error executing command. (cd /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/execroot/com_google_deepvariant && \. exec env - \. PATH=/root/bin:/root/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \. PWD=/proc/self/cwd \. PYTHON_BIN_PATH=/usr/local/bin/python3 \. PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages \. TF2_BEHAVIOR=1 \. /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-. Also, at an earlier point in the build, there is one more issue - . Download from https://mirror.bazel.build/github.com/bazelbuild/rules_cc/archive/081771d4a0e9d7d3aa0eed2ef389fa4700dfb23e.tar.gz failed: class java.io.FileNotFoundException GET returned **404 Not Found**.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/607
https://github.com/google/deepvariant/issues/607:450,deployability,fail,failed,450,"Thanks @akolesnikov and @pichuan . Please find the details and the error-log below. Please note that I have upgraded the following dependencies according to Tensorflow - 2.11. . DV_BAZEL_VERSION=""5.3.0"". DV_GCP_OPTIMIZED_TF_WHL_VERSION=""2.11.0"". ABSL_VERSION=20210324.2. PROTOBUF_VERSION=3.19.6. **Error log:** . (03:11:57) ERROR: /opt/deepvariant/third_party/nucleus/io/BUILD:1051:11: Compiling third_party/nucleus/io/gfile.cc failed: (Exit 1): gcc failed: error executing command. (cd /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/execroot/com_google_deepvariant && \. exec env - \. PATH=/root/bin:/root/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \. PWD=/proc/self/cwd \. PYTHON_BIN_PATH=/usr/local/bin/python3 \. PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages \. TF2_BEHAVIOR=1 \. /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-. Also, at an earlier point in the build, there is one more issue - . Download from https://mirror.bazel.build/github.com/bazelbuild/rules_cc/archive/081771d4a0e9d7d3aa0eed2ef389fa4700dfb23e.tar.gz failed: class java.io.FileNotFoundException GET returned **404 Not Found**.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/607
https://github.com/google/deepvariant/issues/607:954,deployability,build,build,954,"Thanks @akolesnikov and @pichuan . Please find the details and the error-log below. Please note that I have upgraded the following dependencies according to Tensorflow - 2.11. . DV_BAZEL_VERSION=""5.3.0"". DV_GCP_OPTIMIZED_TF_WHL_VERSION=""2.11.0"". ABSL_VERSION=20210324.2. PROTOBUF_VERSION=3.19.6. **Error log:** . (03:11:57) ERROR: /opt/deepvariant/third_party/nucleus/io/BUILD:1051:11: Compiling third_party/nucleus/io/gfile.cc failed: (Exit 1): gcc failed: error executing command. (cd /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/execroot/com_google_deepvariant && \. exec env - \. PATH=/root/bin:/root/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \. PWD=/proc/self/cwd \. PYTHON_BIN_PATH=/usr/local/bin/python3 \. PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages \. TF2_BEHAVIOR=1 \. /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-. Also, at an earlier point in the build, there is one more issue - . Download from https://mirror.bazel.build/github.com/bazelbuild/rules_cc/archive/081771d4a0e9d7d3aa0eed2ef389fa4700dfb23e.tar.gz failed: class java.io.FileNotFoundException GET returned **404 Not Found**.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/607
https://github.com/google/deepvariant/issues/607:1024,deployability,build,build,1024,"Thanks @akolesnikov and @pichuan . Please find the details and the error-log below. Please note that I have upgraded the following dependencies according to Tensorflow - 2.11. . DV_BAZEL_VERSION=""5.3.0"". DV_GCP_OPTIMIZED_TF_WHL_VERSION=""2.11.0"". ABSL_VERSION=20210324.2. PROTOBUF_VERSION=3.19.6. **Error log:** . (03:11:57) ERROR: /opt/deepvariant/third_party/nucleus/io/BUILD:1051:11: Compiling third_party/nucleus/io/gfile.cc failed: (Exit 1): gcc failed: error executing command. (cd /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/execroot/com_google_deepvariant && \. exec env - \. PATH=/root/bin:/root/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \. PWD=/proc/self/cwd \. PYTHON_BIN_PATH=/usr/local/bin/python3 \. PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages \. TF2_BEHAVIOR=1 \. /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-. Also, at an earlier point in the build, there is one more issue - . Download from https://mirror.bazel.build/github.com/bazelbuild/rules_cc/archive/081771d4a0e9d7d3aa0eed2ef389fa4700dfb23e.tar.gz failed: class java.io.FileNotFoundException GET returned **404 Not Found**.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/607
https://github.com/google/deepvariant/issues/607:1117,deployability,fail,failed,1117,"Thanks @akolesnikov and @pichuan . Please find the details and the error-log below. Please note that I have upgraded the following dependencies according to Tensorflow - 2.11. . DV_BAZEL_VERSION=""5.3.0"". DV_GCP_OPTIMIZED_TF_WHL_VERSION=""2.11.0"". ABSL_VERSION=20210324.2. PROTOBUF_VERSION=3.19.6. **Error log:** . (03:11:57) ERROR: /opt/deepvariant/third_party/nucleus/io/BUILD:1051:11: Compiling third_party/nucleus/io/gfile.cc failed: (Exit 1): gcc failed: error executing command. (cd /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/execroot/com_google_deepvariant && \. exec env - \. PATH=/root/bin:/root/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \. PWD=/proc/self/cwd \. PYTHON_BIN_PATH=/usr/local/bin/python3 \. PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages \. TF2_BEHAVIOR=1 \. /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-. Also, at an earlier point in the build, there is one more issue - . Download from https://mirror.bazel.build/github.com/bazelbuild/rules_cc/archive/081771d4a0e9d7d3aa0eed2ef389fa4700dfb23e.tar.gz failed: class java.io.FileNotFoundException GET returned **404 Not Found**.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/607
https://github.com/google/deepvariant/issues/607:131,integrability,depend,dependencies,131,"Thanks @akolesnikov and @pichuan . Please find the details and the error-log below. Please note that I have upgraded the following dependencies according to Tensorflow - 2.11. . DV_BAZEL_VERSION=""5.3.0"". DV_GCP_OPTIMIZED_TF_WHL_VERSION=""2.11.0"". ABSL_VERSION=20210324.2. PROTOBUF_VERSION=3.19.6. **Error log:** . (03:11:57) ERROR: /opt/deepvariant/third_party/nucleus/io/BUILD:1051:11: Compiling third_party/nucleus/io/gfile.cc failed: (Exit 1): gcc failed: error executing command. (cd /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/execroot/com_google_deepvariant && \. exec env - \. PATH=/root/bin:/root/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \. PWD=/proc/self/cwd \. PYTHON_BIN_PATH=/usr/local/bin/python3 \. PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages \. TF2_BEHAVIOR=1 \. /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-. Also, at an earlier point in the build, there is one more issue - . Download from https://mirror.bazel.build/github.com/bazelbuild/rules_cc/archive/081771d4a0e9d7d3aa0eed2ef389fa4700dfb23e.tar.gz failed: class java.io.FileNotFoundException GET returned **404 Not Found**.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/607
https://github.com/google/deepvariant/issues/607:108,modifiability,upgrad,upgraded,108,"Thanks @akolesnikov and @pichuan . Please find the details and the error-log below. Please note that I have upgraded the following dependencies according to Tensorflow - 2.11. . DV_BAZEL_VERSION=""5.3.0"". DV_GCP_OPTIMIZED_TF_WHL_VERSION=""2.11.0"". ABSL_VERSION=20210324.2. PROTOBUF_VERSION=3.19.6. **Error log:** . (03:11:57) ERROR: /opt/deepvariant/third_party/nucleus/io/BUILD:1051:11: Compiling third_party/nucleus/io/gfile.cc failed: (Exit 1): gcc failed: error executing command. (cd /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/execroot/com_google_deepvariant && \. exec env - \. PATH=/root/bin:/root/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \. PWD=/proc/self/cwd \. PYTHON_BIN_PATH=/usr/local/bin/python3 \. PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages \. TF2_BEHAVIOR=1 \. /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-. Also, at an earlier point in the build, there is one more issue - . Download from https://mirror.bazel.build/github.com/bazelbuild/rules_cc/archive/081771d4a0e9d7d3aa0eed2ef389fa4700dfb23e.tar.gz failed: class java.io.FileNotFoundException GET returned **404 Not Found**.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/607
https://github.com/google/deepvariant/issues/607:131,modifiability,depend,dependencies,131,"Thanks @akolesnikov and @pichuan . Please find the details and the error-log below. Please note that I have upgraded the following dependencies according to Tensorflow - 2.11. . DV_BAZEL_VERSION=""5.3.0"". DV_GCP_OPTIMIZED_TF_WHL_VERSION=""2.11.0"". ABSL_VERSION=20210324.2. PROTOBUF_VERSION=3.19.6. **Error log:** . (03:11:57) ERROR: /opt/deepvariant/third_party/nucleus/io/BUILD:1051:11: Compiling third_party/nucleus/io/gfile.cc failed: (Exit 1): gcc failed: error executing command. (cd /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/execroot/com_google_deepvariant && \. exec env - \. PATH=/root/bin:/root/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \. PWD=/proc/self/cwd \. PYTHON_BIN_PATH=/usr/local/bin/python3 \. PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages \. TF2_BEHAVIOR=1 \. /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-. Also, at an earlier point in the build, there is one more issue - . Download from https://mirror.bazel.build/github.com/bazelbuild/rules_cc/archive/081771d4a0e9d7d3aa0eed2ef389fa4700dfb23e.tar.gz failed: class java.io.FileNotFoundException GET returned **404 Not Found**.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/607
https://github.com/google/deepvariant/issues/607:802,modifiability,pac,packages,802,"Thanks @akolesnikov and @pichuan . Please find the details and the error-log below. Please note that I have upgraded the following dependencies according to Tensorflow - 2.11. . DV_BAZEL_VERSION=""5.3.0"". DV_GCP_OPTIMIZED_TF_WHL_VERSION=""2.11.0"". ABSL_VERSION=20210324.2. PROTOBUF_VERSION=3.19.6. **Error log:** . (03:11:57) ERROR: /opt/deepvariant/third_party/nucleus/io/BUILD:1051:11: Compiling third_party/nucleus/io/gfile.cc failed: (Exit 1): gcc failed: error executing command. (cd /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/execroot/com_google_deepvariant && \. exec env - \. PATH=/root/bin:/root/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \. PWD=/proc/self/cwd \. PYTHON_BIN_PATH=/usr/local/bin/python3 \. PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages \. TF2_BEHAVIOR=1 \. /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-. Also, at an earlier point in the build, there is one more issue - . Download from https://mirror.bazel.build/github.com/bazelbuild/rules_cc/archive/081771d4a0e9d7d3aa0eed2ef389fa4700dfb23e.tar.gz failed: class java.io.FileNotFoundException GET returned **404 Not Found**.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/607
https://github.com/google/deepvariant/issues/607:904,modifiability,paramet,parameter,904,"Thanks @akolesnikov and @pichuan . Please find the details and the error-log below. Please note that I have upgraded the following dependencies according to Tensorflow - 2.11. . DV_BAZEL_VERSION=""5.3.0"". DV_GCP_OPTIMIZED_TF_WHL_VERSION=""2.11.0"". ABSL_VERSION=20210324.2. PROTOBUF_VERSION=3.19.6. **Error log:** . (03:11:57) ERROR: /opt/deepvariant/third_party/nucleus/io/BUILD:1051:11: Compiling third_party/nucleus/io/gfile.cc failed: (Exit 1): gcc failed: error executing command. (cd /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/execroot/com_google_deepvariant && \. exec env - \. PATH=/root/bin:/root/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \. PWD=/proc/self/cwd \. PYTHON_BIN_PATH=/usr/local/bin/python3 \. PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages \. TF2_BEHAVIOR=1 \. /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-. Also, at an earlier point in the build, there is one more issue - . Download from https://mirror.bazel.build/github.com/bazelbuild/rules_cc/archive/081771d4a0e9d7d3aa0eed2ef389fa4700dfb23e.tar.gz failed: class java.io.FileNotFoundException GET returned **404 Not Found**.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/607
https://github.com/google/deepvariant/issues/607:67,performance,error,error-log,67,"Thanks @akolesnikov and @pichuan . Please find the details and the error-log below. Please note that I have upgraded the following dependencies according to Tensorflow - 2.11. . DV_BAZEL_VERSION=""5.3.0"". DV_GCP_OPTIMIZED_TF_WHL_VERSION=""2.11.0"". ABSL_VERSION=20210324.2. PROTOBUF_VERSION=3.19.6. **Error log:** . (03:11:57) ERROR: /opt/deepvariant/third_party/nucleus/io/BUILD:1051:11: Compiling third_party/nucleus/io/gfile.cc failed: (Exit 1): gcc failed: error executing command. (cd /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/execroot/com_google_deepvariant && \. exec env - \. PATH=/root/bin:/root/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \. PWD=/proc/self/cwd \. PYTHON_BIN_PATH=/usr/local/bin/python3 \. PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages \. TF2_BEHAVIOR=1 \. /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-. Also, at an earlier point in the build, there is one more issue - . Download from https://mirror.bazel.build/github.com/bazelbuild/rules_cc/archive/081771d4a0e9d7d3aa0eed2ef389fa4700dfb23e.tar.gz failed: class java.io.FileNotFoundException GET returned **404 Not Found**.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/607
https://github.com/google/deepvariant/issues/607:298,performance,Error,Error,298,"Thanks @akolesnikov and @pichuan . Please find the details and the error-log below. Please note that I have upgraded the following dependencies according to Tensorflow - 2.11. . DV_BAZEL_VERSION=""5.3.0"". DV_GCP_OPTIMIZED_TF_WHL_VERSION=""2.11.0"". ABSL_VERSION=20210324.2. PROTOBUF_VERSION=3.19.6. **Error log:** . (03:11:57) ERROR: /opt/deepvariant/third_party/nucleus/io/BUILD:1051:11: Compiling third_party/nucleus/io/gfile.cc failed: (Exit 1): gcc failed: error executing command. (cd /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/execroot/com_google_deepvariant && \. exec env - \. PATH=/root/bin:/root/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \. PWD=/proc/self/cwd \. PYTHON_BIN_PATH=/usr/local/bin/python3 \. PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages \. TF2_BEHAVIOR=1 \. /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-. Also, at an earlier point in the build, there is one more issue - . Download from https://mirror.bazel.build/github.com/bazelbuild/rules_cc/archive/081771d4a0e9d7d3aa0eed2ef389fa4700dfb23e.tar.gz failed: class java.io.FileNotFoundException GET returned **404 Not Found**.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/607
https://github.com/google/deepvariant/issues/607:324,performance,ERROR,ERROR,324,"Thanks @akolesnikov and @pichuan . Please find the details and the error-log below. Please note that I have upgraded the following dependencies according to Tensorflow - 2.11. . DV_BAZEL_VERSION=""5.3.0"". DV_GCP_OPTIMIZED_TF_WHL_VERSION=""2.11.0"". ABSL_VERSION=20210324.2. PROTOBUF_VERSION=3.19.6. **Error log:** . (03:11:57) ERROR: /opt/deepvariant/third_party/nucleus/io/BUILD:1051:11: Compiling third_party/nucleus/io/gfile.cc failed: (Exit 1): gcc failed: error executing command. (cd /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/execroot/com_google_deepvariant && \. exec env - \. PATH=/root/bin:/root/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \. PWD=/proc/self/cwd \. PYTHON_BIN_PATH=/usr/local/bin/python3 \. PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages \. TF2_BEHAVIOR=1 \. /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-. Also, at an earlier point in the build, there is one more issue - . Download from https://mirror.bazel.build/github.com/bazelbuild/rules_cc/archive/081771d4a0e9d7d3aa0eed2ef389fa4700dfb23e.tar.gz failed: class java.io.FileNotFoundException GET returned **404 Not Found**.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/607
https://github.com/google/deepvariant/issues/607:458,performance,error,error,458,"Thanks @akolesnikov and @pichuan . Please find the details and the error-log below. Please note that I have upgraded the following dependencies according to Tensorflow - 2.11. . DV_BAZEL_VERSION=""5.3.0"". DV_GCP_OPTIMIZED_TF_WHL_VERSION=""2.11.0"". ABSL_VERSION=20210324.2. PROTOBUF_VERSION=3.19.6. **Error log:** . (03:11:57) ERROR: /opt/deepvariant/third_party/nucleus/io/BUILD:1051:11: Compiling third_party/nucleus/io/gfile.cc failed: (Exit 1): gcc failed: error executing command. (cd /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/execroot/com_google_deepvariant && \. exec env - \. PATH=/root/bin:/root/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \. PWD=/proc/self/cwd \. PYTHON_BIN_PATH=/usr/local/bin/python3 \. PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages \. TF2_BEHAVIOR=1 \. /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-. Also, at an earlier point in the build, there is one more issue - . Download from https://mirror.bazel.build/github.com/bazelbuild/rules_cc/archive/081771d4a0e9d7d3aa0eed2ef389fa4700dfb23e.tar.gz failed: class java.io.FileNotFoundException GET returned **404 Not Found**.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/607
https://github.com/google/deepvariant/issues/607:494,performance,cach,cache,494,"Thanks @akolesnikov and @pichuan . Please find the details and the error-log below. Please note that I have upgraded the following dependencies according to Tensorflow - 2.11. . DV_BAZEL_VERSION=""5.3.0"". DV_GCP_OPTIMIZED_TF_WHL_VERSION=""2.11.0"". ABSL_VERSION=20210324.2. PROTOBUF_VERSION=3.19.6. **Error log:** . (03:11:57) ERROR: /opt/deepvariant/third_party/nucleus/io/BUILD:1051:11: Compiling third_party/nucleus/io/gfile.cc failed: (Exit 1): gcc failed: error executing command. (cd /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/execroot/com_google_deepvariant && \. exec env - \. PATH=/root/bin:/root/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \. PWD=/proc/self/cwd \. PYTHON_BIN_PATH=/usr/local/bin/python3 \. PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages \. TF2_BEHAVIOR=1 \. /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-. Also, at an earlier point in the build, there is one more issue - . Download from https://mirror.bazel.build/github.com/bazelbuild/rules_cc/archive/081771d4a0e9d7d3aa0eed2ef389fa4700dfb23e.tar.gz failed: class java.io.FileNotFoundException GET returned **404 Not Found**.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/607
https://github.com/google/deepvariant/issues/607:428,reliability,fail,failed,428,"Thanks @akolesnikov and @pichuan . Please find the details and the error-log below. Please note that I have upgraded the following dependencies according to Tensorflow - 2.11. . DV_BAZEL_VERSION=""5.3.0"". DV_GCP_OPTIMIZED_TF_WHL_VERSION=""2.11.0"". ABSL_VERSION=20210324.2. PROTOBUF_VERSION=3.19.6. **Error log:** . (03:11:57) ERROR: /opt/deepvariant/third_party/nucleus/io/BUILD:1051:11: Compiling third_party/nucleus/io/gfile.cc failed: (Exit 1): gcc failed: error executing command. (cd /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/execroot/com_google_deepvariant && \. exec env - \. PATH=/root/bin:/root/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \. PWD=/proc/self/cwd \. PYTHON_BIN_PATH=/usr/local/bin/python3 \. PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages \. TF2_BEHAVIOR=1 \. /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-. Also, at an earlier point in the build, there is one more issue - . Download from https://mirror.bazel.build/github.com/bazelbuild/rules_cc/archive/081771d4a0e9d7d3aa0eed2ef389fa4700dfb23e.tar.gz failed: class java.io.FileNotFoundException GET returned **404 Not Found**.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/607
https://github.com/google/deepvariant/issues/607:450,reliability,fail,failed,450,"Thanks @akolesnikov and @pichuan . Please find the details and the error-log below. Please note that I have upgraded the following dependencies according to Tensorflow - 2.11. . DV_BAZEL_VERSION=""5.3.0"". DV_GCP_OPTIMIZED_TF_WHL_VERSION=""2.11.0"". ABSL_VERSION=20210324.2. PROTOBUF_VERSION=3.19.6. **Error log:** . (03:11:57) ERROR: /opt/deepvariant/third_party/nucleus/io/BUILD:1051:11: Compiling third_party/nucleus/io/gfile.cc failed: (Exit 1): gcc failed: error executing command. (cd /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/execroot/com_google_deepvariant && \. exec env - \. PATH=/root/bin:/root/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \. PWD=/proc/self/cwd \. PYTHON_BIN_PATH=/usr/local/bin/python3 \. PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages \. TF2_BEHAVIOR=1 \. /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-. Also, at an earlier point in the build, there is one more issue - . Download from https://mirror.bazel.build/github.com/bazelbuild/rules_cc/archive/081771d4a0e9d7d3aa0eed2ef389fa4700dfb23e.tar.gz failed: class java.io.FileNotFoundException GET returned **404 Not Found**.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/607
https://github.com/google/deepvariant/issues/607:1117,reliability,fail,failed,1117,"Thanks @akolesnikov and @pichuan . Please find the details and the error-log below. Please note that I have upgraded the following dependencies according to Tensorflow - 2.11. . DV_BAZEL_VERSION=""5.3.0"". DV_GCP_OPTIMIZED_TF_WHL_VERSION=""2.11.0"". ABSL_VERSION=20210324.2. PROTOBUF_VERSION=3.19.6. **Error log:** . (03:11:57) ERROR: /opt/deepvariant/third_party/nucleus/io/BUILD:1051:11: Compiling third_party/nucleus/io/gfile.cc failed: (Exit 1): gcc failed: error executing command. (cd /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/execroot/com_google_deepvariant && \. exec env - \. PATH=/root/bin:/root/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \. PWD=/proc/self/cwd \. PYTHON_BIN_PATH=/usr/local/bin/python3 \. PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages \. TF2_BEHAVIOR=1 \. /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-. Also, at an earlier point in the build, there is one more issue - . Download from https://mirror.bazel.build/github.com/bazelbuild/rules_cc/archive/081771d4a0e9d7d3aa0eed2ef389fa4700dfb23e.tar.gz failed: class java.io.FileNotFoundException GET returned **404 Not Found**.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/607
https://github.com/google/deepvariant/issues/607:67,safety,error,error-log,67,"Thanks @akolesnikov and @pichuan . Please find the details and the error-log below. Please note that I have upgraded the following dependencies according to Tensorflow - 2.11. . DV_BAZEL_VERSION=""5.3.0"". DV_GCP_OPTIMIZED_TF_WHL_VERSION=""2.11.0"". ABSL_VERSION=20210324.2. PROTOBUF_VERSION=3.19.6. **Error log:** . (03:11:57) ERROR: /opt/deepvariant/third_party/nucleus/io/BUILD:1051:11: Compiling third_party/nucleus/io/gfile.cc failed: (Exit 1): gcc failed: error executing command. (cd /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/execroot/com_google_deepvariant && \. exec env - \. PATH=/root/bin:/root/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \. PWD=/proc/self/cwd \. PYTHON_BIN_PATH=/usr/local/bin/python3 \. PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages \. TF2_BEHAVIOR=1 \. /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-. Also, at an earlier point in the build, there is one more issue - . Download from https://mirror.bazel.build/github.com/bazelbuild/rules_cc/archive/081771d4a0e9d7d3aa0eed2ef389fa4700dfb23e.tar.gz failed: class java.io.FileNotFoundException GET returned **404 Not Found**.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/607
https://github.com/google/deepvariant/issues/607:131,safety,depend,dependencies,131,"Thanks @akolesnikov and @pichuan . Please find the details and the error-log below. Please note that I have upgraded the following dependencies according to Tensorflow - 2.11. . DV_BAZEL_VERSION=""5.3.0"". DV_GCP_OPTIMIZED_TF_WHL_VERSION=""2.11.0"". ABSL_VERSION=20210324.2. PROTOBUF_VERSION=3.19.6. **Error log:** . (03:11:57) ERROR: /opt/deepvariant/third_party/nucleus/io/BUILD:1051:11: Compiling third_party/nucleus/io/gfile.cc failed: (Exit 1): gcc failed: error executing command. (cd /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/execroot/com_google_deepvariant && \. exec env - \. PATH=/root/bin:/root/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \. PWD=/proc/self/cwd \. PYTHON_BIN_PATH=/usr/local/bin/python3 \. PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages \. TF2_BEHAVIOR=1 \. /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-. Also, at an earlier point in the build, there is one more issue - . Download from https://mirror.bazel.build/github.com/bazelbuild/rules_cc/archive/081771d4a0e9d7d3aa0eed2ef389fa4700dfb23e.tar.gz failed: class java.io.FileNotFoundException GET returned **404 Not Found**.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/607
https://github.com/google/deepvariant/issues/607:298,safety,Error,Error,298,"Thanks @akolesnikov and @pichuan . Please find the details and the error-log below. Please note that I have upgraded the following dependencies according to Tensorflow - 2.11. . DV_BAZEL_VERSION=""5.3.0"". DV_GCP_OPTIMIZED_TF_WHL_VERSION=""2.11.0"". ABSL_VERSION=20210324.2. PROTOBUF_VERSION=3.19.6. **Error log:** . (03:11:57) ERROR: /opt/deepvariant/third_party/nucleus/io/BUILD:1051:11: Compiling third_party/nucleus/io/gfile.cc failed: (Exit 1): gcc failed: error executing command. (cd /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/execroot/com_google_deepvariant && \. exec env - \. PATH=/root/bin:/root/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \. PWD=/proc/self/cwd \. PYTHON_BIN_PATH=/usr/local/bin/python3 \. PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages \. TF2_BEHAVIOR=1 \. /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-. Also, at an earlier point in the build, there is one more issue - . Download from https://mirror.bazel.build/github.com/bazelbuild/rules_cc/archive/081771d4a0e9d7d3aa0eed2ef389fa4700dfb23e.tar.gz failed: class java.io.FileNotFoundException GET returned **404 Not Found**.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/607
https://github.com/google/deepvariant/issues/607:304,safety,log,log,304,"Thanks @akolesnikov and @pichuan . Please find the details and the error-log below. Please note that I have upgraded the following dependencies according to Tensorflow - 2.11. . DV_BAZEL_VERSION=""5.3.0"". DV_GCP_OPTIMIZED_TF_WHL_VERSION=""2.11.0"". ABSL_VERSION=20210324.2. PROTOBUF_VERSION=3.19.6. **Error log:** . (03:11:57) ERROR: /opt/deepvariant/third_party/nucleus/io/BUILD:1051:11: Compiling third_party/nucleus/io/gfile.cc failed: (Exit 1): gcc failed: error executing command. (cd /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/execroot/com_google_deepvariant && \. exec env - \. PATH=/root/bin:/root/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \. PWD=/proc/self/cwd \. PYTHON_BIN_PATH=/usr/local/bin/python3 \. PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages \. TF2_BEHAVIOR=1 \. /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-. Also, at an earlier point in the build, there is one more issue - . Download from https://mirror.bazel.build/github.com/bazelbuild/rules_cc/archive/081771d4a0e9d7d3aa0eed2ef389fa4700dfb23e.tar.gz failed: class java.io.FileNotFoundException GET returned **404 Not Found**.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/607
https://github.com/google/deepvariant/issues/607:324,safety,ERROR,ERROR,324,"Thanks @akolesnikov and @pichuan . Please find the details and the error-log below. Please note that I have upgraded the following dependencies according to Tensorflow - 2.11. . DV_BAZEL_VERSION=""5.3.0"". DV_GCP_OPTIMIZED_TF_WHL_VERSION=""2.11.0"". ABSL_VERSION=20210324.2. PROTOBUF_VERSION=3.19.6. **Error log:** . (03:11:57) ERROR: /opt/deepvariant/third_party/nucleus/io/BUILD:1051:11: Compiling third_party/nucleus/io/gfile.cc failed: (Exit 1): gcc failed: error executing command. (cd /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/execroot/com_google_deepvariant && \. exec env - \. PATH=/root/bin:/root/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \. PWD=/proc/self/cwd \. PYTHON_BIN_PATH=/usr/local/bin/python3 \. PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages \. TF2_BEHAVIOR=1 \. /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-. Also, at an earlier point in the build, there is one more issue - . Download from https://mirror.bazel.build/github.com/bazelbuild/rules_cc/archive/081771d4a0e9d7d3aa0eed2ef389fa4700dfb23e.tar.gz failed: class java.io.FileNotFoundException GET returned **404 Not Found**.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/607
https://github.com/google/deepvariant/issues/607:458,safety,error,error,458,"Thanks @akolesnikov and @pichuan . Please find the details and the error-log below. Please note that I have upgraded the following dependencies according to Tensorflow - 2.11. . DV_BAZEL_VERSION=""5.3.0"". DV_GCP_OPTIMIZED_TF_WHL_VERSION=""2.11.0"". ABSL_VERSION=20210324.2. PROTOBUF_VERSION=3.19.6. **Error log:** . (03:11:57) ERROR: /opt/deepvariant/third_party/nucleus/io/BUILD:1051:11: Compiling third_party/nucleus/io/gfile.cc failed: (Exit 1): gcc failed: error executing command. (cd /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/execroot/com_google_deepvariant && \. exec env - \. PATH=/root/bin:/root/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \. PWD=/proc/self/cwd \. PYTHON_BIN_PATH=/usr/local/bin/python3 \. PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages \. TF2_BEHAVIOR=1 \. /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-. Also, at an earlier point in the build, there is one more issue - . Download from https://mirror.bazel.build/github.com/bazelbuild/rules_cc/archive/081771d4a0e9d7d3aa0eed2ef389fa4700dfb23e.tar.gz failed: class java.io.FileNotFoundException GET returned **404 Not Found**.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/607
https://github.com/google/deepvariant/issues/607:73,security,log,log,73,"Thanks @akolesnikov and @pichuan . Please find the details and the error-log below. Please note that I have upgraded the following dependencies according to Tensorflow - 2.11. . DV_BAZEL_VERSION=""5.3.0"". DV_GCP_OPTIMIZED_TF_WHL_VERSION=""2.11.0"". ABSL_VERSION=20210324.2. PROTOBUF_VERSION=3.19.6. **Error log:** . (03:11:57) ERROR: /opt/deepvariant/third_party/nucleus/io/BUILD:1051:11: Compiling third_party/nucleus/io/gfile.cc failed: (Exit 1): gcc failed: error executing command. (cd /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/execroot/com_google_deepvariant && \. exec env - \. PATH=/root/bin:/root/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \. PWD=/proc/self/cwd \. PYTHON_BIN_PATH=/usr/local/bin/python3 \. PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages \. TF2_BEHAVIOR=1 \. /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-. Also, at an earlier point in the build, there is one more issue - . Download from https://mirror.bazel.build/github.com/bazelbuild/rules_cc/archive/081771d4a0e9d7d3aa0eed2ef389fa4700dfb23e.tar.gz failed: class java.io.FileNotFoundException GET returned **404 Not Found**.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/607
https://github.com/google/deepvariant/issues/607:304,security,log,log,304,"Thanks @akolesnikov and @pichuan . Please find the details and the error-log below. Please note that I have upgraded the following dependencies according to Tensorflow - 2.11. . DV_BAZEL_VERSION=""5.3.0"". DV_GCP_OPTIMIZED_TF_WHL_VERSION=""2.11.0"". ABSL_VERSION=20210324.2. PROTOBUF_VERSION=3.19.6. **Error log:** . (03:11:57) ERROR: /opt/deepvariant/third_party/nucleus/io/BUILD:1051:11: Compiling third_party/nucleus/io/gfile.cc failed: (Exit 1): gcc failed: error executing command. (cd /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/execroot/com_google_deepvariant && \. exec env - \. PATH=/root/bin:/root/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \. PWD=/proc/self/cwd \. PYTHON_BIN_PATH=/usr/local/bin/python3 \. PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages \. TF2_BEHAVIOR=1 \. /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-. Also, at an earlier point in the build, there is one more issue - . Download from https://mirror.bazel.build/github.com/bazelbuild/rules_cc/archive/081771d4a0e9d7d3aa0eed2ef389fa4700dfb23e.tar.gz failed: class java.io.FileNotFoundException GET returned **404 Not Found**.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/607
https://github.com/google/deepvariant/issues/607:73,testability,log,log,73,"Thanks @akolesnikov and @pichuan . Please find the details and the error-log below. Please note that I have upgraded the following dependencies according to Tensorflow - 2.11. . DV_BAZEL_VERSION=""5.3.0"". DV_GCP_OPTIMIZED_TF_WHL_VERSION=""2.11.0"". ABSL_VERSION=20210324.2. PROTOBUF_VERSION=3.19.6. **Error log:** . (03:11:57) ERROR: /opt/deepvariant/third_party/nucleus/io/BUILD:1051:11: Compiling third_party/nucleus/io/gfile.cc failed: (Exit 1): gcc failed: error executing command. (cd /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/execroot/com_google_deepvariant && \. exec env - \. PATH=/root/bin:/root/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \. PWD=/proc/self/cwd \. PYTHON_BIN_PATH=/usr/local/bin/python3 \. PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages \. TF2_BEHAVIOR=1 \. /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-. Also, at an earlier point in the build, there is one more issue - . Download from https://mirror.bazel.build/github.com/bazelbuild/rules_cc/archive/081771d4a0e9d7d3aa0eed2ef389fa4700dfb23e.tar.gz failed: class java.io.FileNotFoundException GET returned **404 Not Found**.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/607
https://github.com/google/deepvariant/issues/607:131,testability,depend,dependencies,131,"Thanks @akolesnikov and @pichuan . Please find the details and the error-log below. Please note that I have upgraded the following dependencies according to Tensorflow - 2.11. . DV_BAZEL_VERSION=""5.3.0"". DV_GCP_OPTIMIZED_TF_WHL_VERSION=""2.11.0"". ABSL_VERSION=20210324.2. PROTOBUF_VERSION=3.19.6. **Error log:** . (03:11:57) ERROR: /opt/deepvariant/third_party/nucleus/io/BUILD:1051:11: Compiling third_party/nucleus/io/gfile.cc failed: (Exit 1): gcc failed: error executing command. (cd /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/execroot/com_google_deepvariant && \. exec env - \. PATH=/root/bin:/root/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \. PWD=/proc/self/cwd \. PYTHON_BIN_PATH=/usr/local/bin/python3 \. PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages \. TF2_BEHAVIOR=1 \. /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-. Also, at an earlier point in the build, there is one more issue - . Download from https://mirror.bazel.build/github.com/bazelbuild/rules_cc/archive/081771d4a0e9d7d3aa0eed2ef389fa4700dfb23e.tar.gz failed: class java.io.FileNotFoundException GET returned **404 Not Found**.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/607
https://github.com/google/deepvariant/issues/607:304,testability,log,log,304,"Thanks @akolesnikov and @pichuan . Please find the details and the error-log below. Please note that I have upgraded the following dependencies according to Tensorflow - 2.11. . DV_BAZEL_VERSION=""5.3.0"". DV_GCP_OPTIMIZED_TF_WHL_VERSION=""2.11.0"". ABSL_VERSION=20210324.2. PROTOBUF_VERSION=3.19.6. **Error log:** . (03:11:57) ERROR: /opt/deepvariant/third_party/nucleus/io/BUILD:1051:11: Compiling third_party/nucleus/io/gfile.cc failed: (Exit 1): gcc failed: error executing command. (cd /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/execroot/com_google_deepvariant && \. exec env - \. PATH=/root/bin:/root/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \. PWD=/proc/self/cwd \. PYTHON_BIN_PATH=/usr/local/bin/python3 \. PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages \. TF2_BEHAVIOR=1 \. /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-. Also, at an earlier point in the build, there is one more issue - . Download from https://mirror.bazel.build/github.com/bazelbuild/rules_cc/archive/081771d4a0e9d7d3aa0eed2ef389fa4700dfb23e.tar.gz failed: class java.io.FileNotFoundException GET returned **404 Not Found**.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/607
https://github.com/google/deepvariant/issues/607:67,usability,error,error-log,67,"Thanks @akolesnikov and @pichuan . Please find the details and the error-log below. Please note that I have upgraded the following dependencies according to Tensorflow - 2.11. . DV_BAZEL_VERSION=""5.3.0"". DV_GCP_OPTIMIZED_TF_WHL_VERSION=""2.11.0"". ABSL_VERSION=20210324.2. PROTOBUF_VERSION=3.19.6. **Error log:** . (03:11:57) ERROR: /opt/deepvariant/third_party/nucleus/io/BUILD:1051:11: Compiling third_party/nucleus/io/gfile.cc failed: (Exit 1): gcc failed: error executing command. (cd /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/execroot/com_google_deepvariant && \. exec env - \. PATH=/root/bin:/root/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \. PWD=/proc/self/cwd \. PYTHON_BIN_PATH=/usr/local/bin/python3 \. PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages \. TF2_BEHAVIOR=1 \. /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-. Also, at an earlier point in the build, there is one more issue - . Download from https://mirror.bazel.build/github.com/bazelbuild/rules_cc/archive/081771d4a0e9d7d3aa0eed2ef389fa4700dfb23e.tar.gz failed: class java.io.FileNotFoundException GET returned **404 Not Found**.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/607
https://github.com/google/deepvariant/issues/607:298,usability,Error,Error,298,"Thanks @akolesnikov and @pichuan . Please find the details and the error-log below. Please note that I have upgraded the following dependencies according to Tensorflow - 2.11. . DV_BAZEL_VERSION=""5.3.0"". DV_GCP_OPTIMIZED_TF_WHL_VERSION=""2.11.0"". ABSL_VERSION=20210324.2. PROTOBUF_VERSION=3.19.6. **Error log:** . (03:11:57) ERROR: /opt/deepvariant/third_party/nucleus/io/BUILD:1051:11: Compiling third_party/nucleus/io/gfile.cc failed: (Exit 1): gcc failed: error executing command. (cd /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/execroot/com_google_deepvariant && \. exec env - \. PATH=/root/bin:/root/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \. PWD=/proc/self/cwd \. PYTHON_BIN_PATH=/usr/local/bin/python3 \. PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages \. TF2_BEHAVIOR=1 \. /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-. Also, at an earlier point in the build, there is one more issue - . Download from https://mirror.bazel.build/github.com/bazelbuild/rules_cc/archive/081771d4a0e9d7d3aa0eed2ef389fa4700dfb23e.tar.gz failed: class java.io.FileNotFoundException GET returned **404 Not Found**.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/607
https://github.com/google/deepvariant/issues/607:324,usability,ERROR,ERROR,324,"Thanks @akolesnikov and @pichuan . Please find the details and the error-log below. Please note that I have upgraded the following dependencies according to Tensorflow - 2.11. . DV_BAZEL_VERSION=""5.3.0"". DV_GCP_OPTIMIZED_TF_WHL_VERSION=""2.11.0"". ABSL_VERSION=20210324.2. PROTOBUF_VERSION=3.19.6. **Error log:** . (03:11:57) ERROR: /opt/deepvariant/third_party/nucleus/io/BUILD:1051:11: Compiling third_party/nucleus/io/gfile.cc failed: (Exit 1): gcc failed: error executing command. (cd /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/execroot/com_google_deepvariant && \. exec env - \. PATH=/root/bin:/root/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \. PWD=/proc/self/cwd \. PYTHON_BIN_PATH=/usr/local/bin/python3 \. PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages \. TF2_BEHAVIOR=1 \. /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-. Also, at an earlier point in the build, there is one more issue - . Download from https://mirror.bazel.build/github.com/bazelbuild/rules_cc/archive/081771d4a0e9d7d3aa0eed2ef389fa4700dfb23e.tar.gz failed: class java.io.FileNotFoundException GET returned **404 Not Found**.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/607
https://github.com/google/deepvariant/issues/607:458,usability,error,error,458,"Thanks @akolesnikov and @pichuan . Please find the details and the error-log below. Please note that I have upgraded the following dependencies according to Tensorflow - 2.11. . DV_BAZEL_VERSION=""5.3.0"". DV_GCP_OPTIMIZED_TF_WHL_VERSION=""2.11.0"". ABSL_VERSION=20210324.2. PROTOBUF_VERSION=3.19.6. **Error log:** . (03:11:57) ERROR: /opt/deepvariant/third_party/nucleus/io/BUILD:1051:11: Compiling third_party/nucleus/io/gfile.cc failed: (Exit 1): gcc failed: error executing command. (cd /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/execroot/com_google_deepvariant && \. exec env - \. PATH=/root/bin:/root/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \. PWD=/proc/self/cwd \. PYTHON_BIN_PATH=/usr/local/bin/python3 \. PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages \. TF2_BEHAVIOR=1 \. /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-. Also, at an earlier point in the build, there is one more issue - . Download from https://mirror.bazel.build/github.com/bazelbuild/rules_cc/archive/081771d4a0e9d7d3aa0eed2ef389fa4700dfb23e.tar.gz failed: class java.io.FileNotFoundException GET returned **404 Not Found**.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/607
https://github.com/google/deepvariant/issues/607:474,usability,command,command,474,"Thanks @akolesnikov and @pichuan . Please find the details and the error-log below. Please note that I have upgraded the following dependencies according to Tensorflow - 2.11. . DV_BAZEL_VERSION=""5.3.0"". DV_GCP_OPTIMIZED_TF_WHL_VERSION=""2.11.0"". ABSL_VERSION=20210324.2. PROTOBUF_VERSION=3.19.6. **Error log:** . (03:11:57) ERROR: /opt/deepvariant/third_party/nucleus/io/BUILD:1051:11: Compiling third_party/nucleus/io/gfile.cc failed: (Exit 1): gcc failed: error executing command. (cd /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/execroot/com_google_deepvariant && \. exec env - \. PATH=/root/bin:/root/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \. PWD=/proc/self/cwd \. PYTHON_BIN_PATH=/usr/local/bin/python3 \. PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages \. TF2_BEHAVIOR=1 \. /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-. Also, at an earlier point in the build, there is one more issue - . Download from https://mirror.bazel.build/github.com/bazelbuild/rules_cc/archive/081771d4a0e9d7d3aa0eed2ef389fa4700dfb23e.tar.gz failed: class java.io.FileNotFoundException GET returned **404 Not Found**.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/607
https://github.com/google/deepvariant/issues/607:33,deployability,releas,release,33,"Hi @SaurabhKalikar ,. The latest release (v1.5.0) is now using TensorFlow 2.11. We'll also update Nucleus in a few weeks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/607
https://github.com/google/deepvariant/issues/607:91,deployability,updat,update,91,"Hi @SaurabhKalikar ,. The latest release (v1.5.0) is now using TensorFlow 2.11. We'll also update Nucleus in a few weeks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/607
https://github.com/google/deepvariant/issues/607:91,safety,updat,update,91,"Hi @SaurabhKalikar ,. The latest release (v1.5.0) is now using TensorFlow 2.11. We'll also update Nucleus in a few weeks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/607
https://github.com/google/deepvariant/issues/607:91,security,updat,update,91,"Hi @SaurabhKalikar ,. The latest release (v1.5.0) is now using TensorFlow 2.11. We'll also update Nucleus in a few weeks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/607
https://github.com/google/deepvariant/issues/609:610,deployability,releas,released,610,"Hi @karoliinas ,. The DeepTrio model we trained and provided wasn't trained in the condition you described. So it won't work if you try to apply our model that way. . DeepVariant is a general framework that could be extended to multiple samples. (DeepTrio is basically an extension as a 3-sample model, where we trained two models - one to predict child, one to predict parents). . In order to create your own model with your customized semantics, you'll need to carefully create the examples and labels correctly, and train a model your own. We don't currently plan to extend the use cases for our officially released models. If you're interested in the advanced usage (creating your own images+labels and train a model), you can look at a few pointers: https://github.com/google/deepvariant/blob/r1.4/deepvariant/multisample_make_examples.py , https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-training-case-study.md . But we won't be able to provide step-by-step instructions for each use cases. In terms of de novo - I will ask @AndrewCarroll to give you a better answer.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/609
https://github.com/google/deepvariant/issues/609:31,energy efficiency,model,model,31,"Hi @karoliinas ,. The DeepTrio model we trained and provided wasn't trained in the condition you described. So it won't work if you try to apply our model that way. . DeepVariant is a general framework that could be extended to multiple samples. (DeepTrio is basically an extension as a 3-sample model, where we trained two models - one to predict child, one to predict parents). . In order to create your own model with your customized semantics, you'll need to carefully create the examples and labels correctly, and train a model your own. We don't currently plan to extend the use cases for our officially released models. If you're interested in the advanced usage (creating your own images+labels and train a model), you can look at a few pointers: https://github.com/google/deepvariant/blob/r1.4/deepvariant/multisample_make_examples.py , https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-training-case-study.md . But we won't be able to provide step-by-step instructions for each use cases. In terms of de novo - I will ask @AndrewCarroll to give you a better answer.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/609
https://github.com/google/deepvariant/issues/609:149,energy efficiency,model,model,149,"Hi @karoliinas ,. The DeepTrio model we trained and provided wasn't trained in the condition you described. So it won't work if you try to apply our model that way. . DeepVariant is a general framework that could be extended to multiple samples. (DeepTrio is basically an extension as a 3-sample model, where we trained two models - one to predict child, one to predict parents). . In order to create your own model with your customized semantics, you'll need to carefully create the examples and labels correctly, and train a model your own. We don't currently plan to extend the use cases for our officially released models. If you're interested in the advanced usage (creating your own images+labels and train a model), you can look at a few pointers: https://github.com/google/deepvariant/blob/r1.4/deepvariant/multisample_make_examples.py , https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-training-case-study.md . But we won't be able to provide step-by-step instructions for each use cases. In terms of de novo - I will ask @AndrewCarroll to give you a better answer.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/609
https://github.com/google/deepvariant/issues/609:296,energy efficiency,model,model,296,"Hi @karoliinas ,. The DeepTrio model we trained and provided wasn't trained in the condition you described. So it won't work if you try to apply our model that way. . DeepVariant is a general framework that could be extended to multiple samples. (DeepTrio is basically an extension as a 3-sample model, where we trained two models - one to predict child, one to predict parents). . In order to create your own model with your customized semantics, you'll need to carefully create the examples and labels correctly, and train a model your own. We don't currently plan to extend the use cases for our officially released models. If you're interested in the advanced usage (creating your own images+labels and train a model), you can look at a few pointers: https://github.com/google/deepvariant/blob/r1.4/deepvariant/multisample_make_examples.py , https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-training-case-study.md . But we won't be able to provide step-by-step instructions for each use cases. In terms of de novo - I will ask @AndrewCarroll to give you a better answer.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/609
https://github.com/google/deepvariant/issues/609:324,energy efficiency,model,models,324,"Hi @karoliinas ,. The DeepTrio model we trained and provided wasn't trained in the condition you described. So it won't work if you try to apply our model that way. . DeepVariant is a general framework that could be extended to multiple samples. (DeepTrio is basically an extension as a 3-sample model, where we trained two models - one to predict child, one to predict parents). . In order to create your own model with your customized semantics, you'll need to carefully create the examples and labels correctly, and train a model your own. We don't currently plan to extend the use cases for our officially released models. If you're interested in the advanced usage (creating your own images+labels and train a model), you can look at a few pointers: https://github.com/google/deepvariant/blob/r1.4/deepvariant/multisample_make_examples.py , https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-training-case-study.md . But we won't be able to provide step-by-step instructions for each use cases. In terms of de novo - I will ask @AndrewCarroll to give you a better answer.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/609
https://github.com/google/deepvariant/issues/609:340,energy efficiency,predict,predict,340,"Hi @karoliinas ,. The DeepTrio model we trained and provided wasn't trained in the condition you described. So it won't work if you try to apply our model that way. . DeepVariant is a general framework that could be extended to multiple samples. (DeepTrio is basically an extension as a 3-sample model, where we trained two models - one to predict child, one to predict parents). . In order to create your own model with your customized semantics, you'll need to carefully create the examples and labels correctly, and train a model your own. We don't currently plan to extend the use cases for our officially released models. If you're interested in the advanced usage (creating your own images+labels and train a model), you can look at a few pointers: https://github.com/google/deepvariant/blob/r1.4/deepvariant/multisample_make_examples.py , https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-training-case-study.md . But we won't be able to provide step-by-step instructions for each use cases. In terms of de novo - I will ask @AndrewCarroll to give you a better answer.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/609
https://github.com/google/deepvariant/issues/609:362,energy efficiency,predict,predict,362,"Hi @karoliinas ,. The DeepTrio model we trained and provided wasn't trained in the condition you described. So it won't work if you try to apply our model that way. . DeepVariant is a general framework that could be extended to multiple samples. (DeepTrio is basically an extension as a 3-sample model, where we trained two models - one to predict child, one to predict parents). . In order to create your own model with your customized semantics, you'll need to carefully create the examples and labels correctly, and train a model your own. We don't currently plan to extend the use cases for our officially released models. If you're interested in the advanced usage (creating your own images+labels and train a model), you can look at a few pointers: https://github.com/google/deepvariant/blob/r1.4/deepvariant/multisample_make_examples.py , https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-training-case-study.md . But we won't be able to provide step-by-step instructions for each use cases. In terms of de novo - I will ask @AndrewCarroll to give you a better answer.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/609
https://github.com/google/deepvariant/issues/609:410,energy efficiency,model,model,410,"Hi @karoliinas ,. The DeepTrio model we trained and provided wasn't trained in the condition you described. So it won't work if you try to apply our model that way. . DeepVariant is a general framework that could be extended to multiple samples. (DeepTrio is basically an extension as a 3-sample model, where we trained two models - one to predict child, one to predict parents). . In order to create your own model with your customized semantics, you'll need to carefully create the examples and labels correctly, and train a model your own. We don't currently plan to extend the use cases for our officially released models. If you're interested in the advanced usage (creating your own images+labels and train a model), you can look at a few pointers: https://github.com/google/deepvariant/blob/r1.4/deepvariant/multisample_make_examples.py , https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-training-case-study.md . But we won't be able to provide step-by-step instructions for each use cases. In terms of de novo - I will ask @AndrewCarroll to give you a better answer.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/609
https://github.com/google/deepvariant/issues/609:527,energy efficiency,model,model,527,"Hi @karoliinas ,. The DeepTrio model we trained and provided wasn't trained in the condition you described. So it won't work if you try to apply our model that way. . DeepVariant is a general framework that could be extended to multiple samples. (DeepTrio is basically an extension as a 3-sample model, where we trained two models - one to predict child, one to predict parents). . In order to create your own model with your customized semantics, you'll need to carefully create the examples and labels correctly, and train a model your own. We don't currently plan to extend the use cases for our officially released models. If you're interested in the advanced usage (creating your own images+labels and train a model), you can look at a few pointers: https://github.com/google/deepvariant/blob/r1.4/deepvariant/multisample_make_examples.py , https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-training-case-study.md . But we won't be able to provide step-by-step instructions for each use cases. In terms of de novo - I will ask @AndrewCarroll to give you a better answer.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/609
https://github.com/google/deepvariant/issues/609:552,energy efficiency,current,currently,552,"Hi @karoliinas ,. The DeepTrio model we trained and provided wasn't trained in the condition you described. So it won't work if you try to apply our model that way. . DeepVariant is a general framework that could be extended to multiple samples. (DeepTrio is basically an extension as a 3-sample model, where we trained two models - one to predict child, one to predict parents). . In order to create your own model with your customized semantics, you'll need to carefully create the examples and labels correctly, and train a model your own. We don't currently plan to extend the use cases for our officially released models. If you're interested in the advanced usage (creating your own images+labels and train a model), you can look at a few pointers: https://github.com/google/deepvariant/blob/r1.4/deepvariant/multisample_make_examples.py , https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-training-case-study.md . But we won't be able to provide step-by-step instructions for each use cases. In terms of de novo - I will ask @AndrewCarroll to give you a better answer.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/609
https://github.com/google/deepvariant/issues/609:619,energy efficiency,model,models,619,"Hi @karoliinas ,. The DeepTrio model we trained and provided wasn't trained in the condition you described. So it won't work if you try to apply our model that way. . DeepVariant is a general framework that could be extended to multiple samples. (DeepTrio is basically an extension as a 3-sample model, where we trained two models - one to predict child, one to predict parents). . In order to create your own model with your customized semantics, you'll need to carefully create the examples and labels correctly, and train a model your own. We don't currently plan to extend the use cases for our officially released models. If you're interested in the advanced usage (creating your own images+labels and train a model), you can look at a few pointers: https://github.com/google/deepvariant/blob/r1.4/deepvariant/multisample_make_examples.py , https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-training-case-study.md . But we won't be able to provide step-by-step instructions for each use cases. In terms of de novo - I will ask @AndrewCarroll to give you a better answer.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/609
https://github.com/google/deepvariant/issues/609:715,energy efficiency,model,model,715,"Hi @karoliinas ,. The DeepTrio model we trained and provided wasn't trained in the condition you described. So it won't work if you try to apply our model that way. . DeepVariant is a general framework that could be extended to multiple samples. (DeepTrio is basically an extension as a 3-sample model, where we trained two models - one to predict child, one to predict parents). . In order to create your own model with your customized semantics, you'll need to carefully create the examples and labels correctly, and train a model your own. We don't currently plan to extend the use cases for our officially released models. If you're interested in the advanced usage (creating your own images+labels and train a model), you can look at a few pointers: https://github.com/google/deepvariant/blob/r1.4/deepvariant/multisample_make_examples.py , https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-training-case-study.md . But we won't be able to provide step-by-step instructions for each use cases. In terms of de novo - I will ask @AndrewCarroll to give you a better answer.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/609
https://github.com/google/deepvariant/issues/609:437,interoperability,semant,semantics,437,"Hi @karoliinas ,. The DeepTrio model we trained and provided wasn't trained in the condition you described. So it won't work if you try to apply our model that way. . DeepVariant is a general framework that could be extended to multiple samples. (DeepTrio is basically an extension as a 3-sample model, where we trained two models - one to predict child, one to predict parents). . In order to create your own model with your customized semantics, you'll need to carefully create the examples and labels correctly, and train a model your own. We don't currently plan to extend the use cases for our officially released models. If you're interested in the advanced usage (creating your own images+labels and train a model), you can look at a few pointers: https://github.com/google/deepvariant/blob/r1.4/deepvariant/multisample_make_examples.py , https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-training-case-study.md . But we won't be able to provide step-by-step instructions for each use cases. In terms of de novo - I will ask @AndrewCarroll to give you a better answer.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/609
https://github.com/google/deepvariant/issues/609:216,modifiability,exten,extended,216,"Hi @karoliinas ,. The DeepTrio model we trained and provided wasn't trained in the condition you described. So it won't work if you try to apply our model that way. . DeepVariant is a general framework that could be extended to multiple samples. (DeepTrio is basically an extension as a 3-sample model, where we trained two models - one to predict child, one to predict parents). . In order to create your own model with your customized semantics, you'll need to carefully create the examples and labels correctly, and train a model your own. We don't currently plan to extend the use cases for our officially released models. If you're interested in the advanced usage (creating your own images+labels and train a model), you can look at a few pointers: https://github.com/google/deepvariant/blob/r1.4/deepvariant/multisample_make_examples.py , https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-training-case-study.md . But we won't be able to provide step-by-step instructions for each use cases. In terms of de novo - I will ask @AndrewCarroll to give you a better answer.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/609
https://github.com/google/deepvariant/issues/609:272,modifiability,extens,extension,272,"Hi @karoliinas ,. The DeepTrio model we trained and provided wasn't trained in the condition you described. So it won't work if you try to apply our model that way. . DeepVariant is a general framework that could be extended to multiple samples. (DeepTrio is basically an extension as a 3-sample model, where we trained two models - one to predict child, one to predict parents). . In order to create your own model with your customized semantics, you'll need to carefully create the examples and labels correctly, and train a model your own. We don't currently plan to extend the use cases for our officially released models. If you're interested in the advanced usage (creating your own images+labels and train a model), you can look at a few pointers: https://github.com/google/deepvariant/blob/r1.4/deepvariant/multisample_make_examples.py , https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-training-case-study.md . But we won't be able to provide step-by-step instructions for each use cases. In terms of de novo - I will ask @AndrewCarroll to give you a better answer.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/609
https://github.com/google/deepvariant/issues/609:570,modifiability,exten,extend,570,"Hi @karoliinas ,. The DeepTrio model we trained and provided wasn't trained in the condition you described. So it won't work if you try to apply our model that way. . DeepVariant is a general framework that could be extended to multiple samples. (DeepTrio is basically an extension as a 3-sample model, where we trained two models - one to predict child, one to predict parents). . In order to create your own model with your customized semantics, you'll need to carefully create the examples and labels correctly, and train a model your own. We don't currently plan to extend the use cases for our officially released models. If you're interested in the advanced usage (creating your own images+labels and train a model), you can look at a few pointers: https://github.com/google/deepvariant/blob/r1.4/deepvariant/multisample_make_examples.py , https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-training-case-study.md . But we won't be able to provide step-by-step instructions for each use cases. In terms of de novo - I will ask @AndrewCarroll to give you a better answer.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/609
https://github.com/google/deepvariant/issues/609:340,safety,predict,predict,340,"Hi @karoliinas ,. The DeepTrio model we trained and provided wasn't trained in the condition you described. So it won't work if you try to apply our model that way. . DeepVariant is a general framework that could be extended to multiple samples. (DeepTrio is basically an extension as a 3-sample model, where we trained two models - one to predict child, one to predict parents). . In order to create your own model with your customized semantics, you'll need to carefully create the examples and labels correctly, and train a model your own. We don't currently plan to extend the use cases for our officially released models. If you're interested in the advanced usage (creating your own images+labels and train a model), you can look at a few pointers: https://github.com/google/deepvariant/blob/r1.4/deepvariant/multisample_make_examples.py , https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-training-case-study.md . But we won't be able to provide step-by-step instructions for each use cases. In terms of de novo - I will ask @AndrewCarroll to give you a better answer.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/609
https://github.com/google/deepvariant/issues/609:362,safety,predict,predict,362,"Hi @karoliinas ,. The DeepTrio model we trained and provided wasn't trained in the condition you described. So it won't work if you try to apply our model that way. . DeepVariant is a general framework that could be extended to multiple samples. (DeepTrio is basically an extension as a 3-sample model, where we trained two models - one to predict child, one to predict parents). . In order to create your own model with your customized semantics, you'll need to carefully create the examples and labels correctly, and train a model your own. We don't currently plan to extend the use cases for our officially released models. If you're interested in the advanced usage (creating your own images+labels and train a model), you can look at a few pointers: https://github.com/google/deepvariant/blob/r1.4/deepvariant/multisample_make_examples.py , https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-training-case-study.md . But we won't be able to provide step-by-step instructions for each use cases. In terms of de novo - I will ask @AndrewCarroll to give you a better answer.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/609
https://github.com/google/deepvariant/issues/609:31,security,model,model,31,"Hi @karoliinas ,. The DeepTrio model we trained and provided wasn't trained in the condition you described. So it won't work if you try to apply our model that way. . DeepVariant is a general framework that could be extended to multiple samples. (DeepTrio is basically an extension as a 3-sample model, where we trained two models - one to predict child, one to predict parents). . In order to create your own model with your customized semantics, you'll need to carefully create the examples and labels correctly, and train a model your own. We don't currently plan to extend the use cases for our officially released models. If you're interested in the advanced usage (creating your own images+labels and train a model), you can look at a few pointers: https://github.com/google/deepvariant/blob/r1.4/deepvariant/multisample_make_examples.py , https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-training-case-study.md . But we won't be able to provide step-by-step instructions for each use cases. In terms of de novo - I will ask @AndrewCarroll to give you a better answer.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/609
https://github.com/google/deepvariant/issues/609:149,security,model,model,149,"Hi @karoliinas ,. The DeepTrio model we trained and provided wasn't trained in the condition you described. So it won't work if you try to apply our model that way. . DeepVariant is a general framework that could be extended to multiple samples. (DeepTrio is basically an extension as a 3-sample model, where we trained two models - one to predict child, one to predict parents). . In order to create your own model with your customized semantics, you'll need to carefully create the examples and labels correctly, and train a model your own. We don't currently plan to extend the use cases for our officially released models. If you're interested in the advanced usage (creating your own images+labels and train a model), you can look at a few pointers: https://github.com/google/deepvariant/blob/r1.4/deepvariant/multisample_make_examples.py , https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-training-case-study.md . But we won't be able to provide step-by-step instructions for each use cases. In terms of de novo - I will ask @AndrewCarroll to give you a better answer.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/609
https://github.com/google/deepvariant/issues/609:296,security,model,model,296,"Hi @karoliinas ,. The DeepTrio model we trained and provided wasn't trained in the condition you described. So it won't work if you try to apply our model that way. . DeepVariant is a general framework that could be extended to multiple samples. (DeepTrio is basically an extension as a 3-sample model, where we trained two models - one to predict child, one to predict parents). . In order to create your own model with your customized semantics, you'll need to carefully create the examples and labels correctly, and train a model your own. We don't currently plan to extend the use cases for our officially released models. If you're interested in the advanced usage (creating your own images+labels and train a model), you can look at a few pointers: https://github.com/google/deepvariant/blob/r1.4/deepvariant/multisample_make_examples.py , https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-training-case-study.md . But we won't be able to provide step-by-step instructions for each use cases. In terms of de novo - I will ask @AndrewCarroll to give you a better answer.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/609
https://github.com/google/deepvariant/issues/609:324,security,model,models,324,"Hi @karoliinas ,. The DeepTrio model we trained and provided wasn't trained in the condition you described. So it won't work if you try to apply our model that way. . DeepVariant is a general framework that could be extended to multiple samples. (DeepTrio is basically an extension as a 3-sample model, where we trained two models - one to predict child, one to predict parents). . In order to create your own model with your customized semantics, you'll need to carefully create the examples and labels correctly, and train a model your own. We don't currently plan to extend the use cases for our officially released models. If you're interested in the advanced usage (creating your own images+labels and train a model), you can look at a few pointers: https://github.com/google/deepvariant/blob/r1.4/deepvariant/multisample_make_examples.py , https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-training-case-study.md . But we won't be able to provide step-by-step instructions for each use cases. In terms of de novo - I will ask @AndrewCarroll to give you a better answer.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/609
https://github.com/google/deepvariant/issues/609:410,security,model,model,410,"Hi @karoliinas ,. The DeepTrio model we trained and provided wasn't trained in the condition you described. So it won't work if you try to apply our model that way. . DeepVariant is a general framework that could be extended to multiple samples. (DeepTrio is basically an extension as a 3-sample model, where we trained two models - one to predict child, one to predict parents). . In order to create your own model with your customized semantics, you'll need to carefully create the examples and labels correctly, and train a model your own. We don't currently plan to extend the use cases for our officially released models. If you're interested in the advanced usage (creating your own images+labels and train a model), you can look at a few pointers: https://github.com/google/deepvariant/blob/r1.4/deepvariant/multisample_make_examples.py , https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-training-case-study.md . But we won't be able to provide step-by-step instructions for each use cases. In terms of de novo - I will ask @AndrewCarroll to give you a better answer.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/609
https://github.com/google/deepvariant/issues/609:527,security,model,model,527,"Hi @karoliinas ,. The DeepTrio model we trained and provided wasn't trained in the condition you described. So it won't work if you try to apply our model that way. . DeepVariant is a general framework that could be extended to multiple samples. (DeepTrio is basically an extension as a 3-sample model, where we trained two models - one to predict child, one to predict parents). . In order to create your own model with your customized semantics, you'll need to carefully create the examples and labels correctly, and train a model your own. We don't currently plan to extend the use cases for our officially released models. If you're interested in the advanced usage (creating your own images+labels and train a model), you can look at a few pointers: https://github.com/google/deepvariant/blob/r1.4/deepvariant/multisample_make_examples.py , https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-training-case-study.md . But we won't be able to provide step-by-step instructions for each use cases. In terms of de novo - I will ask @AndrewCarroll to give you a better answer.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/609
https://github.com/google/deepvariant/issues/609:619,security,model,models,619,"Hi @karoliinas ,. The DeepTrio model we trained and provided wasn't trained in the condition you described. So it won't work if you try to apply our model that way. . DeepVariant is a general framework that could be extended to multiple samples. (DeepTrio is basically an extension as a 3-sample model, where we trained two models - one to predict child, one to predict parents). . In order to create your own model with your customized semantics, you'll need to carefully create the examples and labels correctly, and train a model your own. We don't currently plan to extend the use cases for our officially released models. If you're interested in the advanced usage (creating your own images+labels and train a model), you can look at a few pointers: https://github.com/google/deepvariant/blob/r1.4/deepvariant/multisample_make_examples.py , https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-training-case-study.md . But we won't be able to provide step-by-step instructions for each use cases. In terms of de novo - I will ask @AndrewCarroll to give you a better answer.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/609
https://github.com/google/deepvariant/issues/609:715,security,model,model,715,"Hi @karoliinas ,. The DeepTrio model we trained and provided wasn't trained in the condition you described. So it won't work if you try to apply our model that way. . DeepVariant is a general framework that could be extended to multiple samples. (DeepTrio is basically an extension as a 3-sample model, where we trained two models - one to predict child, one to predict parents). . In order to create your own model with your customized semantics, you'll need to carefully create the examples and labels correctly, and train a model your own. We don't currently plan to extend the use cases for our officially released models. If you're interested in the advanced usage (creating your own images+labels and train a model), you can look at a few pointers: https://github.com/google/deepvariant/blob/r1.4/deepvariant/multisample_make_examples.py , https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-training-case-study.md . But we won't be able to provide step-by-step instructions for each use cases. In terms of de novo - I will ask @AndrewCarroll to give you a better answer.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/609
https://github.com/google/deepvariant/issues/609:562,testability,plan,plan,562,"Hi @karoliinas ,. The DeepTrio model we trained and provided wasn't trained in the condition you described. So it won't work if you try to apply our model that way. . DeepVariant is a general framework that could be extended to multiple samples. (DeepTrio is basically an extension as a 3-sample model, where we trained two models - one to predict child, one to predict parents). . In order to create your own model with your customized semantics, you'll need to carefully create the examples and labels correctly, and train a model your own. We don't currently plan to extend the use cases for our officially released models. If you're interested in the advanced usage (creating your own images+labels and train a model), you can look at a few pointers: https://github.com/google/deepvariant/blob/r1.4/deepvariant/multisample_make_examples.py , https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-training-case-study.md . But we won't be able to provide step-by-step instructions for each use cases. In terms of de novo - I will ask @AndrewCarroll to give you a better answer.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/609
https://github.com/google/deepvariant/issues/609:426,usability,custom,customized,426,"Hi @karoliinas ,. The DeepTrio model we trained and provided wasn't trained in the condition you described. So it won't work if you try to apply our model that way. . DeepVariant is a general framework that could be extended to multiple samples. (DeepTrio is basically an extension as a 3-sample model, where we trained two models - one to predict child, one to predict parents). . In order to create your own model with your customized semantics, you'll need to carefully create the examples and labels correctly, and train a model your own. We don't currently plan to extend the use cases for our officially released models. If you're interested in the advanced usage (creating your own images+labels and train a model), you can look at a few pointers: https://github.com/google/deepvariant/blob/r1.4/deepvariant/multisample_make_examples.py , https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-training-case-study.md . But we won't be able to provide step-by-step instructions for each use cases. In terms of de novo - I will ask @AndrewCarroll to give you a better answer.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/609
https://github.com/google/deepvariant/issues/609:77,energy efficiency,model,model,77,"Hi @pichuan,. thank you for the quick reply! I wasn't planning on training a model, but good to know it's an option. I will consider running DeepVariant in multisample mode, as calling the variants for each parent twice is taking long. Might be better for those de novos too. . And thanks for the great tool! Previously I was using GATK haplotypecaller, but the workflow was quite cumbersome and the reference samples sequenced together with the families (hg002&hg005) have a much better concordance in DeepVariant, particularly with indels. Plus it's just one command :) . -Karoliina.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/609
https://github.com/google/deepvariant/issues/609:77,security,model,model,77,"Hi @pichuan,. thank you for the quick reply! I wasn't planning on training a model, but good to know it's an option. I will consider running DeepVariant in multisample mode, as calling the variants for each parent twice is taking long. Might be better for those de novos too. . And thanks for the great tool! Previously I was using GATK haplotypecaller, but the workflow was quite cumbersome and the reference samples sequenced together with the families (hg002&hg005) have a much better concordance in DeepVariant, particularly with indels. Plus it's just one command :) . -Karoliina.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/609
https://github.com/google/deepvariant/issues/609:54,testability,plan,planning,54,"Hi @pichuan,. thank you for the quick reply! I wasn't planning on training a model, but good to know it's an option. I will consider running DeepVariant in multisample mode, as calling the variants for each parent twice is taking long. Might be better for those de novos too. . And thanks for the great tool! Previously I was using GATK haplotypecaller, but the workflow was quite cumbersome and the reference samples sequenced together with the families (hg002&hg005) have a much better concordance in DeepVariant, particularly with indels. Plus it's just one command :) . -Karoliina.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/609
https://github.com/google/deepvariant/issues/609:303,usability,tool,tool,303,"Hi @pichuan,. thank you for the quick reply! I wasn't planning on training a model, but good to know it's an option. I will consider running DeepVariant in multisample mode, as calling the variants for each parent twice is taking long. Might be better for those de novos too. . And thanks for the great tool! Previously I was using GATK haplotypecaller, but the workflow was quite cumbersome and the reference samples sequenced together with the families (hg002&hg005) have a much better concordance in DeepVariant, particularly with indels. Plus it's just one command :) . -Karoliina.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/609
https://github.com/google/deepvariant/issues/609:362,usability,workflow,workflow,362,"Hi @pichuan,. thank you for the quick reply! I wasn't planning on training a model, but good to know it's an option. I will consider running DeepVariant in multisample mode, as calling the variants for each parent twice is taking long. Might be better for those de novos too. . And thanks for the great tool! Previously I was using GATK haplotypecaller, but the workflow was quite cumbersome and the reference samples sequenced together with the families (hg002&hg005) have a much better concordance in DeepVariant, particularly with indels. Plus it's just one command :) . -Karoliina.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/609
https://github.com/google/deepvariant/issues/609:561,usability,command,command,561,"Hi @pichuan,. thank you for the quick reply! I wasn't planning on training a model, but good to know it's an option. I will consider running DeepVariant in multisample mode, as calling the variants for each parent twice is taking long. Might be better for those de novos too. . And thanks for the great tool! Previously I was using GATK haplotypecaller, but the workflow was quite cumbersome and the reference samples sequenced together with the families (hg002&hg005) have a much better concordance in DeepVariant, particularly with indels. Plus it's just one command :) . -Karoliina.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/609
https://github.com/google/deepvariant/issues/609:144,availability,consist,consistent,144,"Hi @karoliinas . As Pi-Chuan mentioned, it is (in theory) possible to train a quartet model, but it would take work. If you want to always have consistent calls between parents and child, the best thing is probably to either use parent calling from only 1 DeepTrio run, or to run DeepVariant instead on the full quartet. In the event that you do want to use DeepTrio on both trios as opposed to DeepVariant on the four, there are some things you can look at to increase sensitivity for de novos. From our benchmarks, DeepTrio has a higher overall accuracy for de novos, but is more conservative (lower sensitivity, higher specificity). You could potentially use the GQ/PL values for no-call sites that are potential de novos sites and rank these by confidence, setting your own threshold. I mention the above possibility as an option. It sounds like for your use case of having a quartet and wanting higher sensitivity, the most straightfoward approach is to just use DeepVariant for joint genotyping. Thanks,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/609
https://github.com/google/deepvariant/issues/609:86,energy efficiency,model,model,86,"Hi @karoliinas . As Pi-Chuan mentioned, it is (in theory) possible to train a quartet model, but it would take work. If you want to always have consistent calls between parents and child, the best thing is probably to either use parent calling from only 1 DeepTrio run, or to run DeepVariant instead on the full quartet. In the event that you do want to use DeepTrio on both trios as opposed to DeepVariant on the four, there are some things you can look at to increase sensitivity for de novos. From our benchmarks, DeepTrio has a higher overall accuracy for de novos, but is more conservative (lower sensitivity, higher specificity). You could potentially use the GQ/PL values for no-call sites that are potential de novos sites and rank these by confidence, setting your own threshold. I mention the above possibility as an option. It sounds like for your use case of having a quartet and wanting higher sensitivity, the most straightfoward approach is to just use DeepVariant for joint genotyping. Thanks,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/609
https://github.com/google/deepvariant/issues/609:328,integrability,event,event,328,"Hi @karoliinas . As Pi-Chuan mentioned, it is (in theory) possible to train a quartet model, but it would take work. If you want to always have consistent calls between parents and child, the best thing is probably to either use parent calling from only 1 DeepTrio run, or to run DeepVariant instead on the full quartet. In the event that you do want to use DeepTrio on both trios as opposed to DeepVariant on the four, there are some things you can look at to increase sensitivity for de novos. From our benchmarks, DeepTrio has a higher overall accuracy for de novos, but is more conservative (lower sensitivity, higher specificity). You could potentially use the GQ/PL values for no-call sites that are potential de novos sites and rank these by confidence, setting your own threshold. I mention the above possibility as an option. It sounds like for your use case of having a quartet and wanting higher sensitivity, the most straightfoward approach is to just use DeepVariant for joint genotyping. Thanks,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/609
https://github.com/google/deepvariant/issues/609:622,interoperability,specif,specificity,622,"Hi @karoliinas . As Pi-Chuan mentioned, it is (in theory) possible to train a quartet model, but it would take work. If you want to always have consistent calls between parents and child, the best thing is probably to either use parent calling from only 1 DeepTrio run, or to run DeepVariant instead on the full quartet. In the event that you do want to use DeepTrio on both trios as opposed to DeepVariant on the four, there are some things you can look at to increase sensitivity for de novos. From our benchmarks, DeepTrio has a higher overall accuracy for de novos, but is more conservative (lower sensitivity, higher specificity). You could potentially use the GQ/PL values for no-call sites that are potential de novos sites and rank these by confidence, setting your own threshold. I mention the above possibility as an option. It sounds like for your use case of having a quartet and wanting higher sensitivity, the most straightfoward approach is to just use DeepVariant for joint genotyping. Thanks,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/609
https://github.com/google/deepvariant/issues/609:86,security,model,model,86,"Hi @karoliinas . As Pi-Chuan mentioned, it is (in theory) possible to train a quartet model, but it would take work. If you want to always have consistent calls between parents and child, the best thing is probably to either use parent calling from only 1 DeepTrio run, or to run DeepVariant instead on the full quartet. In the event that you do want to use DeepTrio on both trios as opposed to DeepVariant on the four, there are some things you can look at to increase sensitivity for de novos. From our benchmarks, DeepTrio has a higher overall accuracy for de novos, but is more conservative (lower sensitivity, higher specificity). You could potentially use the GQ/PL values for no-call sites that are potential de novos sites and rank these by confidence, setting your own threshold. I mention the above possibility as an option. It sounds like for your use case of having a quartet and wanting higher sensitivity, the most straightfoward approach is to just use DeepVariant for joint genotyping. Thanks,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/609
https://github.com/google/deepvariant/issues/609:144,usability,consist,consistent,144,"Hi @karoliinas . As Pi-Chuan mentioned, it is (in theory) possible to train a quartet model, but it would take work. If you want to always have consistent calls between parents and child, the best thing is probably to either use parent calling from only 1 DeepTrio run, or to run DeepVariant instead on the full quartet. In the event that you do want to use DeepTrio on both trios as opposed to DeepVariant on the four, there are some things you can look at to increase sensitivity for de novos. From our benchmarks, DeepTrio has a higher overall accuracy for de novos, but is more conservative (lower sensitivity, higher specificity). You could potentially use the GQ/PL values for no-call sites that are potential de novos sites and rank these by confidence, setting your own threshold. I mention the above possibility as an option. It sounds like for your use case of having a quartet and wanting higher sensitivity, the most straightfoward approach is to just use DeepVariant for joint genotyping. Thanks,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/609
https://github.com/google/deepvariant/issues/609:116,deployability,continu,continue,116,"Hi @AndrewCarroll and many thanks for these recommendations. I have carefully considered the options and decided to continue with DeepTrio. I'll look into ranking the no call sites based on the GQ/PL values, as you suggest. It will also be interesting to compare these with the previous results I have from GATK. Best,. Karoliina",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/609
https://github.com/google/deepvariant/issues/610:88,availability,down,down,88,"@asherrar Unfortunately, I'm unable to reproduce what you're seeing. I'm going to write down what I did. Maybe you can spot what differences we have:. Create a CentOS7 machine to test:. ```. gcloud compute instances create ""${USER}-centos7"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-7"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --zone ""us-west1-b"". gcloud compute ssh pichuan-centos7 --zone ""us-west1-b"". ```. By default the machine has Python2. ```. [pichuan@pichuan-centos7 ~]$ python --version . Python 2.7.5. ```. ## Install Python 3.8.10. ```. sudo yum update. sudo yum install gcc openssl-devel bzip2-devel libffi-devel -y. sudo yum install -y wget. sudo yum install -y python3. ```. ```. wget https://www.python.org/ftp/python/3.8.10/Python-3.8.10.tgz. tar xvfz Python-3.8.10.tgz. ```. ```. cd Python-3.8.10. ./configure --enable-optimizations. ```. ```. sudo yum install -y make. sudo make altinstall. ```. ```. [pichuan@pichuan-centos7 Python-3.8.10]$ python3.8 --version. Python 3.8.10. ```. ## Install Singularity. ```. sudo yum update -y && \. sudo yum groupinstall -y 'Development Tools' && \. sudo yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools \. cryptsetup. ```. ```. export VERSION=1.14.12 OS=linux ARCH=amd64. # Downloads the required Go package. wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz. # Extracts the archive. sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz. # Deletes the ``tar`` file. rm go$VERSION.$OS-$ARCH.tar.gz. export VERSION=3.8.4. wget https://github.com/hpcng/singularity/releases/download/v${VERSION}/singularity-${VERSION}.tar.gz. tar -xzf singularity-${VERSION}.tar.gz. pushd singularity-3.8.4. export PATH=/usr/local/go/bin:$PATH. ./mconfig. make -C builddir. sudo make -C builddir install. ```. Check version:. ```. [pichuan@pichuan-centos7 ~]$ singularity --version. singularity version 3.8.4. ```. ## Try running DeepVar",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:1345,availability,Down,Downloads,1345,"cloud"" \. --machine-type ""e2-standard-16"" \. --zone ""us-west1-b"". gcloud compute ssh pichuan-centos7 --zone ""us-west1-b"". ```. By default the machine has Python2. ```. [pichuan@pichuan-centos7 ~]$ python --version . Python 2.7.5. ```. ## Install Python 3.8.10. ```. sudo yum update. sudo yum install gcc openssl-devel bzip2-devel libffi-devel -y. sudo yum install -y wget. sudo yum install -y python3. ```. ```. wget https://www.python.org/ftp/python/3.8.10/Python-3.8.10.tgz. tar xvfz Python-3.8.10.tgz. ```. ```. cd Python-3.8.10. ./configure --enable-optimizations. ```. ```. sudo yum install -y make. sudo make altinstall. ```. ```. [pichuan@pichuan-centos7 Python-3.8.10]$ python3.8 --version. Python 3.8.10. ```. ## Install Singularity. ```. sudo yum update -y && \. sudo yum groupinstall -y 'Development Tools' && \. sudo yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools \. cryptsetup. ```. ```. export VERSION=1.14.12 OS=linux ARCH=amd64. # Downloads the required Go package. wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz. # Extracts the archive. sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz. # Deletes the ``tar`` file. rm go$VERSION.$OS-$ARCH.tar.gz. export VERSION=3.8.4. wget https://github.com/hpcng/singularity/releases/download/v${VERSION}/singularity-${VERSION}.tar.gz. tar -xzf singularity-${VERSION}.tar.gz. pushd singularity-3.8.4. export PATH=/usr/local/go/bin:$PATH. ./mconfig. make -C builddir. sudo make -C builddir install. ```. Check version:. ```. [pichuan@pichuan-centos7 ~]$ singularity --version. singularity version 3.8.4. ```. ## Try running DeepVariant with Singularity. I followed Quick Start to get data. ```. singularity pull docker://google/deepvariant:1.4.0. # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:1654,availability,down,download,1654,"ssl-devel bzip2-devel libffi-devel -y. sudo yum install -y wget. sudo yum install -y python3. ```. ```. wget https://www.python.org/ftp/python/3.8.10/Python-3.8.10.tgz. tar xvfz Python-3.8.10.tgz. ```. ```. cd Python-3.8.10. ./configure --enable-optimizations. ```. ```. sudo yum install -y make. sudo make altinstall. ```. ```. [pichuan@pichuan-centos7 Python-3.8.10]$ python3.8 --version. Python 3.8.10. ```. ## Install Singularity. ```. sudo yum update -y && \. sudo yum groupinstall -y 'Development Tools' && \. sudo yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools \. cryptsetup. ```. ```. export VERSION=1.14.12 OS=linux ARCH=amd64. # Downloads the required Go package. wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz. # Extracts the archive. sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz. # Deletes the ``tar`` file. rm go$VERSION.$OS-$ARCH.tar.gz. export VERSION=3.8.4. wget https://github.com/hpcng/singularity/releases/download/v${VERSION}/singularity-${VERSION}.tar.gz. tar -xzf singularity-${VERSION}.tar.gz. pushd singularity-3.8.4. export PATH=/usr/local/go/bin:$PATH. ./mconfig. make -C builddir. sudo make -C builddir install. ```. Check version:. ```. [pichuan@pichuan-centos7 ~]$ singularity --version. singularity version 3.8.4. ```. ## Try running DeepVariant with Singularity. I followed Quick Start to get data. ```. singularity pull docker://google/deepvariant:1.4.0. # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=1. ```. This worked for me. Check nu",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:3184,availability,cluster,cluster,3184,"\. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=1. ```. This worked for me. Check numpy version:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. python -c 'import numpy as np; print(np.version.version)'. ```. Which shows:. ```. 1.19.2. ```. It seems like my machine doesn't already have numpy, though:. ```. [pichuan@pichuan-centos7 Python-3.8.10]$ pip3.8 show numpy. WARNING: Package(s) not found: numpy. ```. doesn't show anything. ## Install numpy 1.23.0 to see it breaks things. I ran:. ```. pip3.8 install numpy==1.23.0. ```. (Because you mentioned your cluster has 1.23.0). Now this shows:. ```. [pichuan@pichuan-centos7 ~]$ pip3.8 show numpy. Name: numpy. Version: 1.23.0. Summary: NumPy is the fundamental package for array computing with Python. Home-page: https://www.numpy.org. Author: Travis E. Oliphant et al. Author-email: None. License: BSD. Location: /home/pichuan/.local/lib/python3.8/site-packages. Requires: . Required-by: . ```. Then I re-ran:. ```. # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=1. ```. Which still seems to work fine for me. And, this actually shows 1.23.0 as well:",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:4679,availability,error,error,4679,"lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. python -c 'import numpy as np; print(np.version.version)'. ```. Which shows:. ```. 1.19.2. ```. It seems like my machine doesn't already have numpy, though:. ```. [pichuan@pichuan-centos7 Python-3.8.10]$ pip3.8 show numpy. WARNING: Package(s) not found: numpy. ```. doesn't show anything. ## Install numpy 1.23.0 to see it breaks things. I ran:. ```. pip3.8 install numpy==1.23.0. ```. (Because you mentioned your cluster has 1.23.0). Now this shows:. ```. [pichuan@pichuan-centos7 ~]$ pip3.8 show numpy. Name: numpy. Version: 1.23.0. Summary: NumPy is the fundamental package for array computing with Python. Home-page: https://www.numpy.org. Author: Travis E. Oliphant et al. Author-email: None. License: BSD. Location: /home/pichuan/.local/lib/python3.8/site-packages. Requires: . Required-by: . ```. Then I re-ran:. ```. # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=1. ```. Which still seems to work fine for me. And, this actually shows 1.23.0 as well:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. python -c 'import numpy as np; print(np.version.version)'. ```. ```. 1.23.0. ```. I also checked TensorFlow version:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. python -c 'import tensorflow; print(tensorflow.__version__)'. ```. This shows:. ```. 2.7.0. ```. @asherrar Let me know what you think I need to change to try to reproduce the error you saw. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:556,deployability,version,version,556,"@asherrar Unfortunately, I'm unable to reproduce what you're seeing. I'm going to write down what I did. Maybe you can spot what differences we have:. Create a CentOS7 machine to test:. ```. gcloud compute instances create ""${USER}-centos7"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-7"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --zone ""us-west1-b"". gcloud compute ssh pichuan-centos7 --zone ""us-west1-b"". ```. By default the machine has Python2. ```. [pichuan@pichuan-centos7 ~]$ python --version . Python 2.7.5. ```. ## Install Python 3.8.10. ```. sudo yum update. sudo yum install gcc openssl-devel bzip2-devel libffi-devel -y. sudo yum install -y wget. sudo yum install -y python3. ```. ```. wget https://www.python.org/ftp/python/3.8.10/Python-3.8.10.tgz. tar xvfz Python-3.8.10.tgz. ```. ```. cd Python-3.8.10. ./configure --enable-optimizations. ```. ```. sudo yum install -y make. sudo make altinstall. ```. ```. [pichuan@pichuan-centos7 Python-3.8.10]$ python3.8 --version. Python 3.8.10. ```. ## Install Singularity. ```. sudo yum update -y && \. sudo yum groupinstall -y 'Development Tools' && \. sudo yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools \. cryptsetup. ```. ```. export VERSION=1.14.12 OS=linux ARCH=amd64. # Downloads the required Go package. wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz. # Extracts the archive. sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz. # Deletes the ``tar`` file. rm go$VERSION.$OS-$ARCH.tar.gz. export VERSION=3.8.4. wget https://github.com/hpcng/singularity/releases/download/v${VERSION}/singularity-${VERSION}.tar.gz. tar -xzf singularity-${VERSION}.tar.gz. pushd singularity-3.8.4. export PATH=/usr/local/go/bin:$PATH. ./mconfig. make -C builddir. sudo make -C builddir install. ```. Check version:. ```. [pichuan@pichuan-centos7 ~]$ singularity --version. singularity version 3.8.4. ```. ## Try running DeepVar",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:588,deployability,Instal,Install,588,"@asherrar Unfortunately, I'm unable to reproduce what you're seeing. I'm going to write down what I did. Maybe you can spot what differences we have:. Create a CentOS7 machine to test:. ```. gcloud compute instances create ""${USER}-centos7"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-7"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --zone ""us-west1-b"". gcloud compute ssh pichuan-centos7 --zone ""us-west1-b"". ```. By default the machine has Python2. ```. [pichuan@pichuan-centos7 ~]$ python --version . Python 2.7.5. ```. ## Install Python 3.8.10. ```. sudo yum update. sudo yum install gcc openssl-devel bzip2-devel libffi-devel -y. sudo yum install -y wget. sudo yum install -y python3. ```. ```. wget https://www.python.org/ftp/python/3.8.10/Python-3.8.10.tgz. tar xvfz Python-3.8.10.tgz. ```. ```. cd Python-3.8.10. ./configure --enable-optimizations. ```. ```. sudo yum install -y make. sudo make altinstall. ```. ```. [pichuan@pichuan-centos7 Python-3.8.10]$ python3.8 --version. Python 3.8.10. ```. ## Install Singularity. ```. sudo yum update -y && \. sudo yum groupinstall -y 'Development Tools' && \. sudo yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools \. cryptsetup. ```. ```. export VERSION=1.14.12 OS=linux ARCH=amd64. # Downloads the required Go package. wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz. # Extracts the archive. sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz. # Deletes the ``tar`` file. rm go$VERSION.$OS-$ARCH.tar.gz. export VERSION=3.8.4. wget https://github.com/hpcng/singularity/releases/download/v${VERSION}/singularity-${VERSION}.tar.gz. tar -xzf singularity-${VERSION}.tar.gz. pushd singularity-3.8.4. export PATH=/usr/local/go/bin:$PATH. ./mconfig. make -C builddir. sudo make -C builddir install. ```. Check version:. ```. [pichuan@pichuan-centos7 ~]$ singularity --version. singularity version 3.8.4. ```. ## Try running DeepVar",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:625,deployability,updat,update,625,"@asherrar Unfortunately, I'm unable to reproduce what you're seeing. I'm going to write down what I did. Maybe you can spot what differences we have:. Create a CentOS7 machine to test:. ```. gcloud compute instances create ""${USER}-centos7"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-7"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --zone ""us-west1-b"". gcloud compute ssh pichuan-centos7 --zone ""us-west1-b"". ```. By default the machine has Python2. ```. [pichuan@pichuan-centos7 ~]$ python --version . Python 2.7.5. ```. ## Install Python 3.8.10. ```. sudo yum update. sudo yum install gcc openssl-devel bzip2-devel libffi-devel -y. sudo yum install -y wget. sudo yum install -y python3. ```. ```. wget https://www.python.org/ftp/python/3.8.10/Python-3.8.10.tgz. tar xvfz Python-3.8.10.tgz. ```. ```. cd Python-3.8.10. ./configure --enable-optimizations. ```. ```. sudo yum install -y make. sudo make altinstall. ```. ```. [pichuan@pichuan-centos7 Python-3.8.10]$ python3.8 --version. Python 3.8.10. ```. ## Install Singularity. ```. sudo yum update -y && \. sudo yum groupinstall -y 'Development Tools' && \. sudo yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools \. cryptsetup. ```. ```. export VERSION=1.14.12 OS=linux ARCH=amd64. # Downloads the required Go package. wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz. # Extracts the archive. sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz. # Deletes the ``tar`` file. rm go$VERSION.$OS-$ARCH.tar.gz. export VERSION=3.8.4. wget https://github.com/hpcng/singularity/releases/download/v${VERSION}/singularity-${VERSION}.tar.gz. tar -xzf singularity-${VERSION}.tar.gz. pushd singularity-3.8.4. export PATH=/usr/local/go/bin:$PATH. ./mconfig. make -C builddir. sudo make -C builddir install. ```. Check version:. ```. [pichuan@pichuan-centos7 ~]$ singularity --version. singularity version 3.8.4. ```. ## Try running DeepVar",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:642,deployability,instal,install,642,"@asherrar Unfortunately, I'm unable to reproduce what you're seeing. I'm going to write down what I did. Maybe you can spot what differences we have:. Create a CentOS7 machine to test:. ```. gcloud compute instances create ""${USER}-centos7"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-7"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --zone ""us-west1-b"". gcloud compute ssh pichuan-centos7 --zone ""us-west1-b"". ```. By default the machine has Python2. ```. [pichuan@pichuan-centos7 ~]$ python --version . Python 2.7.5. ```. ## Install Python 3.8.10. ```. sudo yum update. sudo yum install gcc openssl-devel bzip2-devel libffi-devel -y. sudo yum install -y wget. sudo yum install -y python3. ```. ```. wget https://www.python.org/ftp/python/3.8.10/Python-3.8.10.tgz. tar xvfz Python-3.8.10.tgz. ```. ```. cd Python-3.8.10. ./configure --enable-optimizations. ```. ```. sudo yum install -y make. sudo make altinstall. ```. ```. [pichuan@pichuan-centos7 Python-3.8.10]$ python3.8 --version. Python 3.8.10. ```. ## Install Singularity. ```. sudo yum update -y && \. sudo yum groupinstall -y 'Development Tools' && \. sudo yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools \. cryptsetup. ```. ```. export VERSION=1.14.12 OS=linux ARCH=amd64. # Downloads the required Go package. wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz. # Extracts the archive. sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz. # Deletes the ``tar`` file. rm go$VERSION.$OS-$ARCH.tar.gz. export VERSION=3.8.4. wget https://github.com/hpcng/singularity/releases/download/v${VERSION}/singularity-${VERSION}.tar.gz. tar -xzf singularity-${VERSION}.tar.gz. pushd singularity-3.8.4. export PATH=/usr/local/go/bin:$PATH. ./mconfig. make -C builddir. sudo make -C builddir install. ```. Check version:. ```. [pichuan@pichuan-centos7 ~]$ singularity --version. singularity version 3.8.4. ```. ## Try running DeepVar",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:706,deployability,instal,install,706,"@asherrar Unfortunately, I'm unable to reproduce what you're seeing. I'm going to write down what I did. Maybe you can spot what differences we have:. Create a CentOS7 machine to test:. ```. gcloud compute instances create ""${USER}-centos7"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-7"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --zone ""us-west1-b"". gcloud compute ssh pichuan-centos7 --zone ""us-west1-b"". ```. By default the machine has Python2. ```. [pichuan@pichuan-centos7 ~]$ python --version . Python 2.7.5. ```. ## Install Python 3.8.10. ```. sudo yum update. sudo yum install gcc openssl-devel bzip2-devel libffi-devel -y. sudo yum install -y wget. sudo yum install -y python3. ```. ```. wget https://www.python.org/ftp/python/3.8.10/Python-3.8.10.tgz. tar xvfz Python-3.8.10.tgz. ```. ```. cd Python-3.8.10. ./configure --enable-optimizations. ```. ```. sudo yum install -y make. sudo make altinstall. ```. ```. [pichuan@pichuan-centos7 Python-3.8.10]$ python3.8 --version. Python 3.8.10. ```. ## Install Singularity. ```. sudo yum update -y && \. sudo yum groupinstall -y 'Development Tools' && \. sudo yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools \. cryptsetup. ```. ```. export VERSION=1.14.12 OS=linux ARCH=amd64. # Downloads the required Go package. wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz. # Extracts the archive. sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz. # Deletes the ``tar`` file. rm go$VERSION.$OS-$ARCH.tar.gz. export VERSION=3.8.4. wget https://github.com/hpcng/singularity/releases/download/v${VERSION}/singularity-${VERSION}.tar.gz. tar -xzf singularity-${VERSION}.tar.gz. pushd singularity-3.8.4. export PATH=/usr/local/go/bin:$PATH. ./mconfig. make -C builddir. sudo make -C builddir install. ```. Check version:. ```. [pichuan@pichuan-centos7 ~]$ singularity --version. singularity version 3.8.4. ```. ## Try running DeepVar",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:732,deployability,instal,install,732,"@asherrar Unfortunately, I'm unable to reproduce what you're seeing. I'm going to write down what I did. Maybe you can spot what differences we have:. Create a CentOS7 machine to test:. ```. gcloud compute instances create ""${USER}-centos7"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-7"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --zone ""us-west1-b"". gcloud compute ssh pichuan-centos7 --zone ""us-west1-b"". ```. By default the machine has Python2. ```. [pichuan@pichuan-centos7 ~]$ python --version . Python 2.7.5. ```. ## Install Python 3.8.10. ```. sudo yum update. sudo yum install gcc openssl-devel bzip2-devel libffi-devel -y. sudo yum install -y wget. sudo yum install -y python3. ```. ```. wget https://www.python.org/ftp/python/3.8.10/Python-3.8.10.tgz. tar xvfz Python-3.8.10.tgz. ```. ```. cd Python-3.8.10. ./configure --enable-optimizations. ```. ```. sudo yum install -y make. sudo make altinstall. ```. ```. [pichuan@pichuan-centos7 Python-3.8.10]$ python3.8 --version. Python 3.8.10. ```. ## Install Singularity. ```. sudo yum update -y && \. sudo yum groupinstall -y 'Development Tools' && \. sudo yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools \. cryptsetup. ```. ```. export VERSION=1.14.12 OS=linux ARCH=amd64. # Downloads the required Go package. wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz. # Extracts the archive. sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz. # Deletes the ``tar`` file. rm go$VERSION.$OS-$ARCH.tar.gz. export VERSION=3.8.4. wget https://github.com/hpcng/singularity/releases/download/v${VERSION}/singularity-${VERSION}.tar.gz. tar -xzf singularity-${VERSION}.tar.gz. pushd singularity-3.8.4. export PATH=/usr/local/go/bin:$PATH. ./mconfig. make -C builddir. sudo make -C builddir install. ```. Check version:. ```. [pichuan@pichuan-centos7 ~]$ singularity --version. singularity version 3.8.4. ```. ## Try running DeepVar",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:938,deployability,instal,install,938,"@asherrar Unfortunately, I'm unable to reproduce what you're seeing. I'm going to write down what I did. Maybe you can spot what differences we have:. Create a CentOS7 machine to test:. ```. gcloud compute instances create ""${USER}-centos7"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-7"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --zone ""us-west1-b"". gcloud compute ssh pichuan-centos7 --zone ""us-west1-b"". ```. By default the machine has Python2. ```. [pichuan@pichuan-centos7 ~]$ python --version . Python 2.7.5. ```. ## Install Python 3.8.10. ```. sudo yum update. sudo yum install gcc openssl-devel bzip2-devel libffi-devel -y. sudo yum install -y wget. sudo yum install -y python3. ```. ```. wget https://www.python.org/ftp/python/3.8.10/Python-3.8.10.tgz. tar xvfz Python-3.8.10.tgz. ```. ```. cd Python-3.8.10. ./configure --enable-optimizations. ```. ```. sudo yum install -y make. sudo make altinstall. ```. ```. [pichuan@pichuan-centos7 Python-3.8.10]$ python3.8 --version. Python 3.8.10. ```. ## Install Singularity. ```. sudo yum update -y && \. sudo yum groupinstall -y 'Development Tools' && \. sudo yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools \. cryptsetup. ```. ```. export VERSION=1.14.12 OS=linux ARCH=amd64. # Downloads the required Go package. wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz. # Extracts the archive. sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz. # Deletes the ``tar`` file. rm go$VERSION.$OS-$ARCH.tar.gz. export VERSION=3.8.4. wget https://github.com/hpcng/singularity/releases/download/v${VERSION}/singularity-${VERSION}.tar.gz. tar -xzf singularity-${VERSION}.tar.gz. pushd singularity-3.8.4. export PATH=/usr/local/go/bin:$PATH. ./mconfig. make -C builddir. sudo make -C builddir install. ```. Check version:. ```. [pichuan@pichuan-centos7 ~]$ singularity --version. singularity version 3.8.4. ```. ## Try running DeepVar",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:1040,deployability,version,version,1040,"duce what you're seeing. I'm going to write down what I did. Maybe you can spot what differences we have:. Create a CentOS7 machine to test:. ```. gcloud compute instances create ""${USER}-centos7"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-7"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --zone ""us-west1-b"". gcloud compute ssh pichuan-centos7 --zone ""us-west1-b"". ```. By default the machine has Python2. ```. [pichuan@pichuan-centos7 ~]$ python --version . Python 2.7.5. ```. ## Install Python 3.8.10. ```. sudo yum update. sudo yum install gcc openssl-devel bzip2-devel libffi-devel -y. sudo yum install -y wget. sudo yum install -y python3. ```. ```. wget https://www.python.org/ftp/python/3.8.10/Python-3.8.10.tgz. tar xvfz Python-3.8.10.tgz. ```. ```. cd Python-3.8.10. ./configure --enable-optimizations. ```. ```. sudo yum install -y make. sudo make altinstall. ```. ```. [pichuan@pichuan-centos7 Python-3.8.10]$ python3.8 --version. Python 3.8.10. ```. ## Install Singularity. ```. sudo yum update -y && \. sudo yum groupinstall -y 'Development Tools' && \. sudo yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools \. cryptsetup. ```. ```. export VERSION=1.14.12 OS=linux ARCH=amd64. # Downloads the required Go package. wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz. # Extracts the archive. sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz. # Deletes the ``tar`` file. rm go$VERSION.$OS-$ARCH.tar.gz. export VERSION=3.8.4. wget https://github.com/hpcng/singularity/releases/download/v${VERSION}/singularity-${VERSION}.tar.gz. tar -xzf singularity-${VERSION}.tar.gz. pushd singularity-3.8.4. export PATH=/usr/local/go/bin:$PATH. ./mconfig. make -C builddir. sudo make -C builddir install. ```. Check version:. ```. [pichuan@pichuan-centos7 ~]$ singularity --version. singularity version 3.8.4. ```. ## Try running DeepVariant with Singularity. I followed Quick Star",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:1072,deployability,Instal,Install,1072,"ng to write down what I did. Maybe you can spot what differences we have:. Create a CentOS7 machine to test:. ```. gcloud compute instances create ""${USER}-centos7"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-7"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --zone ""us-west1-b"". gcloud compute ssh pichuan-centos7 --zone ""us-west1-b"". ```. By default the machine has Python2. ```. [pichuan@pichuan-centos7 ~]$ python --version . Python 2.7.5. ```. ## Install Python 3.8.10. ```. sudo yum update. sudo yum install gcc openssl-devel bzip2-devel libffi-devel -y. sudo yum install -y wget. sudo yum install -y python3. ```. ```. wget https://www.python.org/ftp/python/3.8.10/Python-3.8.10.tgz. tar xvfz Python-3.8.10.tgz. ```. ```. cd Python-3.8.10. ./configure --enable-optimizations. ```. ```. sudo yum install -y make. sudo make altinstall. ```. ```. [pichuan@pichuan-centos7 Python-3.8.10]$ python3.8 --version. Python 3.8.10. ```. ## Install Singularity. ```. sudo yum update -y && \. sudo yum groupinstall -y 'Development Tools' && \. sudo yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools \. cryptsetup. ```. ```. export VERSION=1.14.12 OS=linux ARCH=amd64. # Downloads the required Go package. wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz. # Extracts the archive. sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz. # Deletes the ``tar`` file. rm go$VERSION.$OS-$ARCH.tar.gz. export VERSION=3.8.4. wget https://github.com/hpcng/singularity/releases/download/v${VERSION}/singularity-${VERSION}.tar.gz. tar -xzf singularity-${VERSION}.tar.gz. pushd singularity-3.8.4. export PATH=/usr/local/go/bin:$PATH. ./mconfig. make -C builddir. sudo make -C builddir install. ```. Check version:. ```. [pichuan@pichuan-centos7 ~]$ singularity --version. singularity version 3.8.4. ```. ## Try running DeepVariant with Singularity. I followed Quick Start to get data. ```. singularity ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:1107,deployability,updat,update,1107," you can spot what differences we have:. Create a CentOS7 machine to test:. ```. gcloud compute instances create ""${USER}-centos7"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-7"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --zone ""us-west1-b"". gcloud compute ssh pichuan-centos7 --zone ""us-west1-b"". ```. By default the machine has Python2. ```. [pichuan@pichuan-centos7 ~]$ python --version . Python 2.7.5. ```. ## Install Python 3.8.10. ```. sudo yum update. sudo yum install gcc openssl-devel bzip2-devel libffi-devel -y. sudo yum install -y wget. sudo yum install -y python3. ```. ```. wget https://www.python.org/ftp/python/3.8.10/Python-3.8.10.tgz. tar xvfz Python-3.8.10.tgz. ```. ```. cd Python-3.8.10. ./configure --enable-optimizations. ```. ```. sudo yum install -y make. sudo make altinstall. ```. ```. [pichuan@pichuan-centos7 Python-3.8.10]$ python3.8 --version. Python 3.8.10. ```. ## Install Singularity. ```. sudo yum update -y && \. sudo yum groupinstall -y 'Development Tools' && \. sudo yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools \. cryptsetup. ```. ```. export VERSION=1.14.12 OS=linux ARCH=amd64. # Downloads the required Go package. wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz. # Extracts the archive. sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz. # Deletes the ``tar`` file. rm go$VERSION.$OS-$ARCH.tar.gz. export VERSION=3.8.4. wget https://github.com/hpcng/singularity/releases/download/v${VERSION}/singularity-${VERSION}.tar.gz. tar -xzf singularity-${VERSION}.tar.gz. pushd singularity-3.8.4. export PATH=/usr/local/go/bin:$PATH. ./mconfig. make -C builddir. sudo make -C builddir install. ```. Check version:. ```. [pichuan@pichuan-centos7 ~]$ singularity --version. singularity version 3.8.4. ```. ## Try running DeepVariant with Singularity. I followed Quick Start to get data. ```. singularity pull docker://google/deepvariant:1",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:1183,deployability,instal,install,1183,"``. gcloud compute instances create ""${USER}-centos7"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-7"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --zone ""us-west1-b"". gcloud compute ssh pichuan-centos7 --zone ""us-west1-b"". ```. By default the machine has Python2. ```. [pichuan@pichuan-centos7 ~]$ python --version . Python 2.7.5. ```. ## Install Python 3.8.10. ```. sudo yum update. sudo yum install gcc openssl-devel bzip2-devel libffi-devel -y. sudo yum install -y wget. sudo yum install -y python3. ```. ```. wget https://www.python.org/ftp/python/3.8.10/Python-3.8.10.tgz. tar xvfz Python-3.8.10.tgz. ```. ```. cd Python-3.8.10. ./configure --enable-optimizations. ```. ```. sudo yum install -y make. sudo make altinstall. ```. ```. [pichuan@pichuan-centos7 Python-3.8.10]$ python3.8 --version. Python 3.8.10. ```. ## Install Singularity. ```. sudo yum update -y && \. sudo yum groupinstall -y 'Development Tools' && \. sudo yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools \. cryptsetup. ```. ```. export VERSION=1.14.12 OS=linux ARCH=amd64. # Downloads the required Go package. wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz. # Extracts the archive. sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz. # Deletes the ``tar`` file. rm go$VERSION.$OS-$ARCH.tar.gz. export VERSION=3.8.4. wget https://github.com/hpcng/singularity/releases/download/v${VERSION}/singularity-${VERSION}.tar.gz. tar -xzf singularity-${VERSION}.tar.gz. pushd singularity-3.8.4. export PATH=/usr/local/go/bin:$PATH. ./mconfig. make -C builddir. sudo make -C builddir install. ```. Check version:. ```. [pichuan@pichuan-centos7 ~]$ singularity --version. singularity version 3.8.4. ```. ## Try running DeepVariant with Singularity. I followed Quick Start to get data. ```. singularity pull docker://google/deepvariant:1.4.0. # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:1306,deployability,VERSION,VERSION,1306,"y ""centos-7"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --zone ""us-west1-b"". gcloud compute ssh pichuan-centos7 --zone ""us-west1-b"". ```. By default the machine has Python2. ```. [pichuan@pichuan-centos7 ~]$ python --version . Python 2.7.5. ```. ## Install Python 3.8.10. ```. sudo yum update. sudo yum install gcc openssl-devel bzip2-devel libffi-devel -y. sudo yum install -y wget. sudo yum install -y python3. ```. ```. wget https://www.python.org/ftp/python/3.8.10/Python-3.8.10.tgz. tar xvfz Python-3.8.10.tgz. ```. ```. cd Python-3.8.10. ./configure --enable-optimizations. ```. ```. sudo yum install -y make. sudo make altinstall. ```. ```. [pichuan@pichuan-centos7 Python-3.8.10]$ python3.8 --version. Python 3.8.10. ```. ## Install Singularity. ```. sudo yum update -y && \. sudo yum groupinstall -y 'Development Tools' && \. sudo yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools \. cryptsetup. ```. ```. export VERSION=1.14.12 OS=linux ARCH=amd64. # Downloads the required Go package. wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz. # Extracts the archive. sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz. # Deletes the ``tar`` file. rm go$VERSION.$OS-$ARCH.tar.gz. export VERSION=3.8.4. wget https://github.com/hpcng/singularity/releases/download/v${VERSION}/singularity-${VERSION}.tar.gz. tar -xzf singularity-${VERSION}.tar.gz. pushd singularity-3.8.4. export PATH=/usr/local/go/bin:$PATH. ./mconfig. make -C builddir. sudo make -C builddir install. ```. Check version:. ```. [pichuan@pichuan-centos7 ~]$ singularity --version. singularity version 3.8.4. ```. ## Try running DeepVariant with Singularity. I followed Quick Start to get data. ```. singularity pull docker://google/deepvariant:1.4.0. # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/u",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:1413,deployability,VERSION,VERSION,1413,"cloud compute ssh pichuan-centos7 --zone ""us-west1-b"". ```. By default the machine has Python2. ```. [pichuan@pichuan-centos7 ~]$ python --version . Python 2.7.5. ```. ## Install Python 3.8.10. ```. sudo yum update. sudo yum install gcc openssl-devel bzip2-devel libffi-devel -y. sudo yum install -y wget. sudo yum install -y python3. ```. ```. wget https://www.python.org/ftp/python/3.8.10/Python-3.8.10.tgz. tar xvfz Python-3.8.10.tgz. ```. ```. cd Python-3.8.10. ./configure --enable-optimizations. ```. ```. sudo yum install -y make. sudo make altinstall. ```. ```. [pichuan@pichuan-centos7 Python-3.8.10]$ python3.8 --version. Python 3.8.10. ```. ## Install Singularity. ```. sudo yum update -y && \. sudo yum groupinstall -y 'Development Tools' && \. sudo yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools \. cryptsetup. ```. ```. export VERSION=1.14.12 OS=linux ARCH=amd64. # Downloads the required Go package. wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz. # Extracts the archive. sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz. # Deletes the ``tar`` file. rm go$VERSION.$OS-$ARCH.tar.gz. export VERSION=3.8.4. wget https://github.com/hpcng/singularity/releases/download/v${VERSION}/singularity-${VERSION}.tar.gz. tar -xzf singularity-${VERSION}.tar.gz. pushd singularity-3.8.4. export PATH=/usr/local/go/bin:$PATH. ./mconfig. make -C builddir. sudo make -C builddir install. ```. Check version:. ```. [pichuan@pichuan-centos7 ~]$ singularity --version. singularity version 3.8.4. ```. ## Try running DeepVariant with Singularity. I followed Quick Start to get data. ```. singularity pull docker://google/deepvariant:1.4.0. # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:1495,deployability,VERSION,VERSION,1495," has Python2. ```. [pichuan@pichuan-centos7 ~]$ python --version . Python 2.7.5. ```. ## Install Python 3.8.10. ```. sudo yum update. sudo yum install gcc openssl-devel bzip2-devel libffi-devel -y. sudo yum install -y wget. sudo yum install -y python3. ```. ```. wget https://www.python.org/ftp/python/3.8.10/Python-3.8.10.tgz. tar xvfz Python-3.8.10.tgz. ```. ```. cd Python-3.8.10. ./configure --enable-optimizations. ```. ```. sudo yum install -y make. sudo make altinstall. ```. ```. [pichuan@pichuan-centos7 Python-3.8.10]$ python3.8 --version. Python 3.8.10. ```. ## Install Singularity. ```. sudo yum update -y && \. sudo yum groupinstall -y 'Development Tools' && \. sudo yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools \. cryptsetup. ```. ```. export VERSION=1.14.12 OS=linux ARCH=amd64. # Downloads the required Go package. wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz. # Extracts the archive. sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz. # Deletes the ``tar`` file. rm go$VERSION.$OS-$ARCH.tar.gz. export VERSION=3.8.4. wget https://github.com/hpcng/singularity/releases/download/v${VERSION}/singularity-${VERSION}.tar.gz. tar -xzf singularity-${VERSION}.tar.gz. pushd singularity-3.8.4. export PATH=/usr/local/go/bin:$PATH. ./mconfig. make -C builddir. sudo make -C builddir install. ```. Check version:. ```. [pichuan@pichuan-centos7 ~]$ singularity --version. singularity version 3.8.4. ```. ## Try running DeepVariant with Singularity. I followed Quick Start to get data. ```. singularity pull docker://google/deepvariant:1.4.0. # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_g",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:1555,deployability,VERSION,VERSION,1555,"sion . Python 2.7.5. ```. ## Install Python 3.8.10. ```. sudo yum update. sudo yum install gcc openssl-devel bzip2-devel libffi-devel -y. sudo yum install -y wget. sudo yum install -y python3. ```. ```. wget https://www.python.org/ftp/python/3.8.10/Python-3.8.10.tgz. tar xvfz Python-3.8.10.tgz. ```. ```. cd Python-3.8.10. ./configure --enable-optimizations. ```. ```. sudo yum install -y make. sudo make altinstall. ```. ```. [pichuan@pichuan-centos7 Python-3.8.10]$ python3.8 --version. Python 3.8.10. ```. ## Install Singularity. ```. sudo yum update -y && \. sudo yum groupinstall -y 'Development Tools' && \. sudo yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools \. cryptsetup. ```. ```. export VERSION=1.14.12 OS=linux ARCH=amd64. # Downloads the required Go package. wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz. # Extracts the archive. sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz. # Deletes the ``tar`` file. rm go$VERSION.$OS-$ARCH.tar.gz. export VERSION=3.8.4. wget https://github.com/hpcng/singularity/releases/download/v${VERSION}/singularity-${VERSION}.tar.gz. tar -xzf singularity-${VERSION}.tar.gz. pushd singularity-3.8.4. export PATH=/usr/local/go/bin:$PATH. ./mconfig. make -C builddir. sudo make -C builddir install. ```. Check version:. ```. [pichuan@pichuan-centos7 ~]$ singularity --version. singularity version 3.8.4. ```. ## Try running DeepVariant with Singularity. I followed Quick Start to get data. ```. singularity pull docker://google/deepvariant:1.4.0. # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_result",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:1588,deployability,VERSION,VERSION,1588,"all Python 3.8.10. ```. sudo yum update. sudo yum install gcc openssl-devel bzip2-devel libffi-devel -y. sudo yum install -y wget. sudo yum install -y python3. ```. ```. wget https://www.python.org/ftp/python/3.8.10/Python-3.8.10.tgz. tar xvfz Python-3.8.10.tgz. ```. ```. cd Python-3.8.10. ./configure --enable-optimizations. ```. ```. sudo yum install -y make. sudo make altinstall. ```. ```. [pichuan@pichuan-centos7 Python-3.8.10]$ python3.8 --version. Python 3.8.10. ```. ## Install Singularity. ```. sudo yum update -y && \. sudo yum groupinstall -y 'Development Tools' && \. sudo yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools \. cryptsetup. ```. ```. export VERSION=1.14.12 OS=linux ARCH=amd64. # Downloads the required Go package. wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz. # Extracts the archive. sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz. # Deletes the ``tar`` file. rm go$VERSION.$OS-$ARCH.tar.gz. export VERSION=3.8.4. wget https://github.com/hpcng/singularity/releases/download/v${VERSION}/singularity-${VERSION}.tar.gz. tar -xzf singularity-${VERSION}.tar.gz. pushd singularity-3.8.4. export PATH=/usr/local/go/bin:$PATH. ./mconfig. make -C builddir. sudo make -C builddir install. ```. Check version:. ```. [pichuan@pichuan-centos7 ~]$ singularity --version. singularity version 3.8.4. ```. ## Try running DeepVariant with Singularity. I followed Quick Start to get data. ```. singularity pull docker://google/deepvariant:1.4.0. # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:1645,deployability,releas,releases,1645," gcc openssl-devel bzip2-devel libffi-devel -y. sudo yum install -y wget. sudo yum install -y python3. ```. ```. wget https://www.python.org/ftp/python/3.8.10/Python-3.8.10.tgz. tar xvfz Python-3.8.10.tgz. ```. ```. cd Python-3.8.10. ./configure --enable-optimizations. ```. ```. sudo yum install -y make. sudo make altinstall. ```. ```. [pichuan@pichuan-centos7 Python-3.8.10]$ python3.8 --version. Python 3.8.10. ```. ## Install Singularity. ```. sudo yum update -y && \. sudo yum groupinstall -y 'Development Tools' && \. sudo yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools \. cryptsetup. ```. ```. export VERSION=1.14.12 OS=linux ARCH=amd64. # Downloads the required Go package. wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz. # Extracts the archive. sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz. # Deletes the ``tar`` file. rm go$VERSION.$OS-$ARCH.tar.gz. export VERSION=3.8.4. wget https://github.com/hpcng/singularity/releases/download/v${VERSION}/singularity-${VERSION}.tar.gz. tar -xzf singularity-${VERSION}.tar.gz. pushd singularity-3.8.4. export PATH=/usr/local/go/bin:$PATH. ./mconfig. make -C builddir. sudo make -C builddir install. ```. Check version:. ```. [pichuan@pichuan-centos7 ~]$ singularity --version. singularity version 3.8.4. ```. ## Try running DeepVariant with Singularity. I followed Quick Start to get data. ```. singularity pull docker://google/deepvariant:1.4.0. # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=1. ```. This worked for me.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:1666,deployability,VERSION,VERSION,1666,"ip2-devel libffi-devel -y. sudo yum install -y wget. sudo yum install -y python3. ```. ```. wget https://www.python.org/ftp/python/3.8.10/Python-3.8.10.tgz. tar xvfz Python-3.8.10.tgz. ```. ```. cd Python-3.8.10. ./configure --enable-optimizations. ```. ```. sudo yum install -y make. sudo make altinstall. ```. ```. [pichuan@pichuan-centos7 Python-3.8.10]$ python3.8 --version. Python 3.8.10. ```. ## Install Singularity. ```. sudo yum update -y && \. sudo yum groupinstall -y 'Development Tools' && \. sudo yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools \. cryptsetup. ```. ```. export VERSION=1.14.12 OS=linux ARCH=amd64. # Downloads the required Go package. wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz. # Extracts the archive. sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz. # Deletes the ``tar`` file. rm go$VERSION.$OS-$ARCH.tar.gz. export VERSION=3.8.4. wget https://github.com/hpcng/singularity/releases/download/v${VERSION}/singularity-${VERSION}.tar.gz. tar -xzf singularity-${VERSION}.tar.gz. pushd singularity-3.8.4. export PATH=/usr/local/go/bin:$PATH. ./mconfig. make -C builddir. sudo make -C builddir install. ```. Check version:. ```. [pichuan@pichuan-centos7 ~]$ singularity --version. singularity version 3.8.4. ```. ## Try running DeepVariant with Singularity. I followed Quick Start to get data. ```. singularity pull docker://google/deepvariant:1.4.0. # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=1. ```. This worked for me. Check numpy version:",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:1689,deployability,VERSION,VERSION,1689,"-y. sudo yum install -y wget. sudo yum install -y python3. ```. ```. wget https://www.python.org/ftp/python/3.8.10/Python-3.8.10.tgz. tar xvfz Python-3.8.10.tgz. ```. ```. cd Python-3.8.10. ./configure --enable-optimizations. ```. ```. sudo yum install -y make. sudo make altinstall. ```. ```. [pichuan@pichuan-centos7 Python-3.8.10]$ python3.8 --version. Python 3.8.10. ```. ## Install Singularity. ```. sudo yum update -y && \. sudo yum groupinstall -y 'Development Tools' && \. sudo yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools \. cryptsetup. ```. ```. export VERSION=1.14.12 OS=linux ARCH=amd64. # Downloads the required Go package. wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz. # Extracts the archive. sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz. # Deletes the ``tar`` file. rm go$VERSION.$OS-$ARCH.tar.gz. export VERSION=3.8.4. wget https://github.com/hpcng/singularity/releases/download/v${VERSION}/singularity-${VERSION}.tar.gz. tar -xzf singularity-${VERSION}.tar.gz. pushd singularity-3.8.4. export PATH=/usr/local/go/bin:$PATH. ./mconfig. make -C builddir. sudo make -C builddir install. ```. Check version:. ```. [pichuan@pichuan-centos7 ~]$ singularity --version. singularity version 3.8.4. ```. ## Try running DeepVariant with Singularity. I followed Quick Start to get data. ```. singularity pull docker://google/deepvariant:1.4.0. # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=1. ```. This worked for me. Check numpy version:. ```. singularity run ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:1729,deployability,VERSION,VERSION,1729,"nstall -y python3. ```. ```. wget https://www.python.org/ftp/python/3.8.10/Python-3.8.10.tgz. tar xvfz Python-3.8.10.tgz. ```. ```. cd Python-3.8.10. ./configure --enable-optimizations. ```. ```. sudo yum install -y make. sudo make altinstall. ```. ```. [pichuan@pichuan-centos7 Python-3.8.10]$ python3.8 --version. Python 3.8.10. ```. ## Install Singularity. ```. sudo yum update -y && \. sudo yum groupinstall -y 'Development Tools' && \. sudo yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools \. cryptsetup. ```. ```. export VERSION=1.14.12 OS=linux ARCH=amd64. # Downloads the required Go package. wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz. # Extracts the archive. sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz. # Deletes the ``tar`` file. rm go$VERSION.$OS-$ARCH.tar.gz. export VERSION=3.8.4. wget https://github.com/hpcng/singularity/releases/download/v${VERSION}/singularity-${VERSION}.tar.gz. tar -xzf singularity-${VERSION}.tar.gz. pushd singularity-3.8.4. export PATH=/usr/local/go/bin:$PATH. ./mconfig. make -C builddir. sudo make -C builddir install. ```. Check version:. ```. [pichuan@pichuan-centos7 ~]$ singularity --version. singularity version 3.8.4. ```. ## Try running DeepVariant with Singularity. I followed Quick Start to get data. ```. singularity pull docker://google/deepvariant:1.4.0. # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=1. ```. This worked for me. Check numpy version:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:1827,deployability,build,builddir,1827,"xvfz Python-3.8.10.tgz. ```. ```. cd Python-3.8.10. ./configure --enable-optimizations. ```. ```. sudo yum install -y make. sudo make altinstall. ```. ```. [pichuan@pichuan-centos7 Python-3.8.10]$ python3.8 --version. Python 3.8.10. ```. ## Install Singularity. ```. sudo yum update -y && \. sudo yum groupinstall -y 'Development Tools' && \. sudo yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools \. cryptsetup. ```. ```. export VERSION=1.14.12 OS=linux ARCH=amd64. # Downloads the required Go package. wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz. # Extracts the archive. sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz. # Deletes the ``tar`` file. rm go$VERSION.$OS-$ARCH.tar.gz. export VERSION=3.8.4. wget https://github.com/hpcng/singularity/releases/download/v${VERSION}/singularity-${VERSION}.tar.gz. tar -xzf singularity-${VERSION}.tar.gz. pushd singularity-3.8.4. export PATH=/usr/local/go/bin:$PATH. ./mconfig. make -C builddir. sudo make -C builddir install. ```. Check version:. ```. [pichuan@pichuan-centos7 ~]$ singularity --version. singularity version 3.8.4. ```. ## Try running DeepVariant with Singularity. I followed Quick Start to get data. ```. singularity pull docker://google/deepvariant:1.4.0. # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=1. ```. This worked for me. Check numpy version:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. python -c 'import numpy as np; print(np.version.version)'. ``",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:1850,deployability,build,builddir,1850," ```. ```. cd Python-3.8.10. ./configure --enable-optimizations. ```. ```. sudo yum install -y make. sudo make altinstall. ```. ```. [pichuan@pichuan-centos7 Python-3.8.10]$ python3.8 --version. Python 3.8.10. ```. ## Install Singularity. ```. sudo yum update -y && \. sudo yum groupinstall -y 'Development Tools' && \. sudo yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools \. cryptsetup. ```. ```. export VERSION=1.14.12 OS=linux ARCH=amd64. # Downloads the required Go package. wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz. # Extracts the archive. sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz. # Deletes the ``tar`` file. rm go$VERSION.$OS-$ARCH.tar.gz. export VERSION=3.8.4. wget https://github.com/hpcng/singularity/releases/download/v${VERSION}/singularity-${VERSION}.tar.gz. tar -xzf singularity-${VERSION}.tar.gz. pushd singularity-3.8.4. export PATH=/usr/local/go/bin:$PATH. ./mconfig. make -C builddir. sudo make -C builddir install. ```. Check version:. ```. [pichuan@pichuan-centos7 ~]$ singularity --version. singularity version 3.8.4. ```. ## Try running DeepVariant with Singularity. I followed Quick Start to get data. ```. singularity pull docker://google/deepvariant:1.4.0. # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=1. ```. This worked for me. Check numpy version:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. python -c 'import numpy as np; print(np.version.version)'. ```. Which shows:. ```. 1",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:1859,deployability,instal,install,1859,". cd Python-3.8.10. ./configure --enable-optimizations. ```. ```. sudo yum install -y make. sudo make altinstall. ```. ```. [pichuan@pichuan-centos7 Python-3.8.10]$ python3.8 --version. Python 3.8.10. ```. ## Install Singularity. ```. sudo yum update -y && \. sudo yum groupinstall -y 'Development Tools' && \. sudo yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools \. cryptsetup. ```. ```. export VERSION=1.14.12 OS=linux ARCH=amd64. # Downloads the required Go package. wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz. # Extracts the archive. sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz. # Deletes the ``tar`` file. rm go$VERSION.$OS-$ARCH.tar.gz. export VERSION=3.8.4. wget https://github.com/hpcng/singularity/releases/download/v${VERSION}/singularity-${VERSION}.tar.gz. tar -xzf singularity-${VERSION}.tar.gz. pushd singularity-3.8.4. export PATH=/usr/local/go/bin:$PATH. ./mconfig. make -C builddir. sudo make -C builddir install. ```. Check version:. ```. [pichuan@pichuan-centos7 ~]$ singularity --version. singularity version 3.8.4. ```. ## Try running DeepVariant with Singularity. I followed Quick Start to get data. ```. singularity pull docker://google/deepvariant:1.4.0. # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=1. ```. This worked for me. Check numpy version:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. python -c 'import numpy as np; print(np.version.version)'. ```. Which shows:. ```. 1.19.2. ``",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:1879,deployability,version,version,1879,"./configure --enable-optimizations. ```. ```. sudo yum install -y make. sudo make altinstall. ```. ```. [pichuan@pichuan-centos7 Python-3.8.10]$ python3.8 --version. Python 3.8.10. ```. ## Install Singularity. ```. sudo yum update -y && \. sudo yum groupinstall -y 'Development Tools' && \. sudo yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools \. cryptsetup. ```. ```. export VERSION=1.14.12 OS=linux ARCH=amd64. # Downloads the required Go package. wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz. # Extracts the archive. sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz. # Deletes the ``tar`` file. rm go$VERSION.$OS-$ARCH.tar.gz. export VERSION=3.8.4. wget https://github.com/hpcng/singularity/releases/download/v${VERSION}/singularity-${VERSION}.tar.gz. tar -xzf singularity-${VERSION}.tar.gz. pushd singularity-3.8.4. export PATH=/usr/local/go/bin:$PATH. ./mconfig. make -C builddir. sudo make -C builddir install. ```. Check version:. ```. [pichuan@pichuan-centos7 ~]$ singularity --version. singularity version 3.8.4. ```. ## Try running DeepVariant with Singularity. I followed Quick Start to get data. ```. singularity pull docker://google/deepvariant:1.4.0. # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=1. ```. This worked for me. Check numpy version:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. python -c 'import numpy as np; print(np.version.version)'. ```. Which shows:. ```. 1.19.2. ```. It seems like my ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:1937,deployability,version,version,1937,"tall -y make. sudo make altinstall. ```. ```. [pichuan@pichuan-centos7 Python-3.8.10]$ python3.8 --version. Python 3.8.10. ```. ## Install Singularity. ```. sudo yum update -y && \. sudo yum groupinstall -y 'Development Tools' && \. sudo yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools \. cryptsetup. ```. ```. export VERSION=1.14.12 OS=linux ARCH=amd64. # Downloads the required Go package. wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz. # Extracts the archive. sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz. # Deletes the ``tar`` file. rm go$VERSION.$OS-$ARCH.tar.gz. export VERSION=3.8.4. wget https://github.com/hpcng/singularity/releases/download/v${VERSION}/singularity-${VERSION}.tar.gz. tar -xzf singularity-${VERSION}.tar.gz. pushd singularity-3.8.4. export PATH=/usr/local/go/bin:$PATH. ./mconfig. make -C builddir. sudo make -C builddir install. ```. Check version:. ```. [pichuan@pichuan-centos7 ~]$ singularity --version. singularity version 3.8.4. ```. ## Try running DeepVariant with Singularity. I followed Quick Start to get data. ```. singularity pull docker://google/deepvariant:1.4.0. # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=1. ```. This worked for me. Check numpy version:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. python -c 'import numpy as np; print(np.version.version)'. ```. Which shows:. ```. 1.19.2. ```. It seems like my machine doesn't already have numpy, though:. ```. [pichuan",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:1958,deployability,version,version,1958,"ke altinstall. ```. ```. [pichuan@pichuan-centos7 Python-3.8.10]$ python3.8 --version. Python 3.8.10. ```. ## Install Singularity. ```. sudo yum update -y && \. sudo yum groupinstall -y 'Development Tools' && \. sudo yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools \. cryptsetup. ```. ```. export VERSION=1.14.12 OS=linux ARCH=amd64. # Downloads the required Go package. wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz. # Extracts the archive. sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz. # Deletes the ``tar`` file. rm go$VERSION.$OS-$ARCH.tar.gz. export VERSION=3.8.4. wget https://github.com/hpcng/singularity/releases/download/v${VERSION}/singularity-${VERSION}.tar.gz. tar -xzf singularity-${VERSION}.tar.gz. pushd singularity-3.8.4. export PATH=/usr/local/go/bin:$PATH. ./mconfig. make -C builddir. sudo make -C builddir install. ```. Check version:. ```. [pichuan@pichuan-centos7 ~]$ singularity --version. singularity version 3.8.4. ```. ## Try running DeepVariant with Singularity. I followed Quick Start to get data. ```. singularity pull docker://google/deepvariant:1.4.0. # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=1. ```. This worked for me. Check numpy version:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. python -c 'import numpy as np; print(np.version.version)'. ```. Which shows:. ```. 1.19.2. ```. It seems like my machine doesn't already have numpy, though:. ```. [pichuan@pichuan-centos7 Pyth",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:2662,deployability,version,version,2662,"VERSION}/singularity-${VERSION}.tar.gz. tar -xzf singularity-${VERSION}.tar.gz. pushd singularity-3.8.4. export PATH=/usr/local/go/bin:$PATH. ./mconfig. make -C builddir. sudo make -C builddir install. ```. Check version:. ```. [pichuan@pichuan-centos7 ~]$ singularity --version. singularity version 3.8.4. ```. ## Try running DeepVariant with Singularity. I followed Quick Start to get data. ```. singularity pull docker://google/deepvariant:1.4.0. # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=1. ```. This worked for me. Check numpy version:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. python -c 'import numpy as np; print(np.version.version)'. ```. Which shows:. ```. 1.19.2. ```. It seems like my machine doesn't already have numpy, though:. ```. [pichuan@pichuan-centos7 Python-3.8.10]$ pip3.8 show numpy. WARNING: Package(s) not found: numpy. ```. doesn't show anything. ## Install numpy 1.23.0 to see it breaks things. I ran:. ```. pip3.8 install numpy==1.23.0. ```. (Because you mentioned your cluster has 1.23.0). Now this shows:. ```. [pichuan@pichuan-centos7 ~]$ pip3.8 show numpy. Name: numpy. Version: 1.23.0. Summary: NumPy is the fundamental package for array computing with Python. Home-page: https://www.numpy.org. Author: Travis E. Oliphant et al. Author-email: None. License: BSD. Location: /home/pichuan/.local/lib/python3.8/site-packages. Requires: . Required-by: . ```. Then I re-ran:. ```. # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:2810,deployability,version,version,2810,"fig. make -C builddir. sudo make -C builddir install. ```. Check version:. ```. [pichuan@pichuan-centos7 ~]$ singularity --version. singularity version 3.8.4. ```. ## Try running DeepVariant with Singularity. I followed Quick Start to get data. ```. singularity pull docker://google/deepvariant:1.4.0. # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=1. ```. This worked for me. Check numpy version:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. python -c 'import numpy as np; print(np.version.version)'. ```. Which shows:. ```. 1.19.2. ```. It seems like my machine doesn't already have numpy, though:. ```. [pichuan@pichuan-centos7 Python-3.8.10]$ pip3.8 show numpy. WARNING: Package(s) not found: numpy. ```. doesn't show anything. ## Install numpy 1.23.0 to see it breaks things. I ran:. ```. pip3.8 install numpy==1.23.0. ```. (Because you mentioned your cluster has 1.23.0). Now this shows:. ```. [pichuan@pichuan-centos7 ~]$ pip3.8 show numpy. Name: numpy. Version: 1.23.0. Summary: NumPy is the fundamental package for array computing with Python. Home-page: https://www.numpy.org. Author: Travis E. Oliphant et al. Author-email: None. License: BSD. Location: /home/pichuan/.local/lib/python3.8/site-packages. Requires: . Required-by: . ```. Then I re-ran:. ```. # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.f",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:2818,deployability,version,version,2818,"e -C builddir. sudo make -C builddir install. ```. Check version:. ```. [pichuan@pichuan-centos7 ~]$ singularity --version. singularity version 3.8.4. ```. ## Try running DeepVariant with Singularity. I followed Quick Start to get data. ```. singularity pull docker://google/deepvariant:1.4.0. # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=1. ```. This worked for me. Check numpy version:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. python -c 'import numpy as np; print(np.version.version)'. ```. Which shows:. ```. 1.19.2. ```. It seems like my machine doesn't already have numpy, though:. ```. [pichuan@pichuan-centos7 Python-3.8.10]$ pip3.8 show numpy. WARNING: Package(s) not found: numpy. ```. doesn't show anything. ## Install numpy 1.23.0 to see it breaks things. I ran:. ```. pip3.8 install numpy==1.23.0. ```. (Because you mentioned your cluster has 1.23.0). Now this shows:. ```. [pichuan@pichuan-centos7 ~]$ pip3.8 show numpy. Name: numpy. Version: 1.23.0. Summary: NumPy is the fundamental package for array computing with Python. Home-page: https://www.numpy.org. Author: Travis E. Oliphant et al. Author-email: None. License: BSD. Location: /home/pichuan/.local/lib/python3.8/site-packages. Requires: . Required-by: . ```. Then I re-ran:. ```. # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:3062,deployability,Instal,Install,3062,"ngularity pull docker://google/deepvariant:1.4.0. # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=1. ```. This worked for me. Check numpy version:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. python -c 'import numpy as np; print(np.version.version)'. ```. Which shows:. ```. 1.19.2. ```. It seems like my machine doesn't already have numpy, though:. ```. [pichuan@pichuan-centos7 Python-3.8.10]$ pip3.8 show numpy. WARNING: Package(s) not found: numpy. ```. doesn't show anything. ## Install numpy 1.23.0 to see it breaks things. I ran:. ```. pip3.8 install numpy==1.23.0. ```. (Because you mentioned your cluster has 1.23.0). Now this shows:. ```. [pichuan@pichuan-centos7 ~]$ pip3.8 show numpy. Name: numpy. Version: 1.23.0. Summary: NumPy is the fundamental package for array computing with Python. Home-page: https://www.numpy.org. Author: Travis E. Oliphant et al. Author-email: None. License: BSD. Location: /home/pichuan/.local/lib/python3.8/site-packages. Requires: . Required-by: . ```. Then I re-ran:. ```. # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/interme",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:3128,deployability,instal,install,3128,"t. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=1. ```. This worked for me. Check numpy version:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. python -c 'import numpy as np; print(np.version.version)'. ```. Which shows:. ```. 1.19.2. ```. It seems like my machine doesn't already have numpy, though:. ```. [pichuan@pichuan-centos7 Python-3.8.10]$ pip3.8 show numpy. WARNING: Package(s) not found: numpy. ```. doesn't show anything. ## Install numpy 1.23.0 to see it breaks things. I ran:. ```. pip3.8 install numpy==1.23.0. ```. (Because you mentioned your cluster has 1.23.0). Now this shows:. ```. [pichuan@pichuan-centos7 ~]$ pip3.8 show numpy. Name: numpy. Version: 1.23.0. Summary: NumPy is the fundamental package for array computing with Python. Home-page: https://www.numpy.org. Author: Travis E. Oliphant et al. Author-email: None. License: BSD. Location: /home/pichuan/.local/lib/python3.8/site-packages. Requires: . Required-by: . ```. Then I re-ran:. ```. # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=1. ```. Which still seems to wo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:3184,deployability,cluster,cluster,3184,"\. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=1. ```. This worked for me. Check numpy version:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. python -c 'import numpy as np; print(np.version.version)'. ```. Which shows:. ```. 1.19.2. ```. It seems like my machine doesn't already have numpy, though:. ```. [pichuan@pichuan-centos7 Python-3.8.10]$ pip3.8 show numpy. WARNING: Package(s) not found: numpy. ```. doesn't show anything. ## Install numpy 1.23.0 to see it breaks things. I ran:. ```. pip3.8 install numpy==1.23.0. ```. (Because you mentioned your cluster has 1.23.0). Now this shows:. ```. [pichuan@pichuan-centos7 ~]$ pip3.8 show numpy. Name: numpy. Version: 1.23.0. Summary: NumPy is the fundamental package for array computing with Python. Home-page: https://www.numpy.org. Author: Travis E. Oliphant et al. Author-email: None. License: BSD. Location: /home/pichuan/.local/lib/python3.8/site-packages. Requires: . Required-by: . ```. Then I re-ran:. ```. # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=1. ```. Which still seems to work fine for me. And, this actually shows 1.23.0 as well:",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:3288,deployability,Version,Version,3288,"f=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=1. ```. This worked for me. Check numpy version:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. python -c 'import numpy as np; print(np.version.version)'. ```. Which shows:. ```. 1.19.2. ```. It seems like my machine doesn't already have numpy, though:. ```. [pichuan@pichuan-centos7 Python-3.8.10]$ pip3.8 show numpy. WARNING: Package(s) not found: numpy. ```. doesn't show anything. ## Install numpy 1.23.0 to see it breaks things. I ran:. ```. pip3.8 install numpy==1.23.0. ```. (Because you mentioned your cluster has 1.23.0). Now this shows:. ```. [pichuan@pichuan-centos7 ~]$ pip3.8 show numpy. Name: numpy. Version: 1.23.0. Summary: NumPy is the fundamental package for array computing with Python. Home-page: https://www.numpy.org. Author: Travis E. Oliphant et al. Author-email: None. License: BSD. Location: /home/pichuan/.local/lib/python3.8/site-packages. Requires: . Required-by: . ```. Then I re-ran:. ```. # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=1. ```. Which still seems to work fine for me. And, this actually shows 1.23.0 as well:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. pyth",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:4328,deployability,version,version,4328,"lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. python -c 'import numpy as np; print(np.version.version)'. ```. Which shows:. ```. 1.19.2. ```. It seems like my machine doesn't already have numpy, though:. ```. [pichuan@pichuan-centos7 Python-3.8.10]$ pip3.8 show numpy. WARNING: Package(s) not found: numpy. ```. doesn't show anything. ## Install numpy 1.23.0 to see it breaks things. I ran:. ```. pip3.8 install numpy==1.23.0. ```. (Because you mentioned your cluster has 1.23.0). Now this shows:. ```. [pichuan@pichuan-centos7 ~]$ pip3.8 show numpy. Name: numpy. Version: 1.23.0. Summary: NumPy is the fundamental package for array computing with Python. Home-page: https://www.numpy.org. Author: Travis E. Oliphant et al. Author-email: None. License: BSD. Location: /home/pichuan/.local/lib/python3.8/site-packages. Requires: . Required-by: . ```. Then I re-ran:. ```. # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=1. ```. Which still seems to work fine for me. And, this actually shows 1.23.0 as well:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. python -c 'import numpy as np; print(np.version.version)'. ```. ```. 1.23.0. ```. I also checked TensorFlow version:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. python -c 'import tensorflow; print(tensorflow.__version__)'. ```. This shows:. ```. 2.7.0. ```. @asherrar Let me know what you think I need to change to try to reproduce the error you saw. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:4336,deployability,version,version,4336,"lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. python -c 'import numpy as np; print(np.version.version)'. ```. Which shows:. ```. 1.19.2. ```. It seems like my machine doesn't already have numpy, though:. ```. [pichuan@pichuan-centos7 Python-3.8.10]$ pip3.8 show numpy. WARNING: Package(s) not found: numpy. ```. doesn't show anything. ## Install numpy 1.23.0 to see it breaks things. I ran:. ```. pip3.8 install numpy==1.23.0. ```. (Because you mentioned your cluster has 1.23.0). Now this shows:. ```. [pichuan@pichuan-centos7 ~]$ pip3.8 show numpy. Name: numpy. Version: 1.23.0. Summary: NumPy is the fundamental package for array computing with Python. Home-page: https://www.numpy.org. Author: Travis E. Oliphant et al. Author-email: None. License: BSD. Location: /home/pichuan/.local/lib/python3.8/site-packages. Requires: . Required-by: . ```. Then I re-ran:. ```. # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=1. ```. Which still seems to work fine for me. And, this actually shows 1.23.0 as well:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. python -c 'import numpy as np; print(np.version.version)'. ```. ```. 1.23.0. ```. I also checked TensorFlow version:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. python -c 'import tensorflow; print(tensorflow.__version__)'. ```. This shows:. ```. 2.7.0. ```. @asherrar Let me know what you think I need to change to try to reproduce the error you saw. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:4396,deployability,version,version,4396,"lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. python -c 'import numpy as np; print(np.version.version)'. ```. Which shows:. ```. 1.19.2. ```. It seems like my machine doesn't already have numpy, though:. ```. [pichuan@pichuan-centos7 Python-3.8.10]$ pip3.8 show numpy. WARNING: Package(s) not found: numpy. ```. doesn't show anything. ## Install numpy 1.23.0 to see it breaks things. I ran:. ```. pip3.8 install numpy==1.23.0. ```. (Because you mentioned your cluster has 1.23.0). Now this shows:. ```. [pichuan@pichuan-centos7 ~]$ pip3.8 show numpy. Name: numpy. Version: 1.23.0. Summary: NumPy is the fundamental package for array computing with Python. Home-page: https://www.numpy.org. Author: Travis E. Oliphant et al. Author-email: None. License: BSD. Location: /home/pichuan/.local/lib/python3.8/site-packages. Requires: . Required-by: . ```. Then I re-ran:. ```. # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=1. ```. Which still seems to work fine for me. And, this actually shows 1.23.0 as well:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. python -c 'import numpy as np; print(np.version.version)'. ```. ```. 1.23.0. ```. I also checked TensorFlow version:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. python -c 'import tensorflow; print(tensorflow.__version__)'. ```. This shows:. ```. 2.7.0. ```. @asherrar Let me know what you think I need to change to try to reproduce the error you saw. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:278,energy efficiency,cloud,cloud-platform,278,"@asherrar Unfortunately, I'm unable to reproduce what you're seeing. I'm going to write down what I did. Maybe you can spot what differences we have:. Create a CentOS7 machine to test:. ```. gcloud compute instances create ""${USER}-centos7"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-7"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --zone ""us-west1-b"". gcloud compute ssh pichuan-centos7 --zone ""us-west1-b"". ```. By default the machine has Python2. ```. [pichuan@pichuan-centos7 ~]$ python --version . Python 2.7.5. ```. ## Install Python 3.8.10. ```. sudo yum update. sudo yum install gcc openssl-devel bzip2-devel libffi-devel -y. sudo yum install -y wget. sudo yum install -y python3. ```. ```. wget https://www.python.org/ftp/python/3.8.10/Python-3.8.10.tgz. tar xvfz Python-3.8.10.tgz. ```. ```. cd Python-3.8.10. ./configure --enable-optimizations. ```. ```. sudo yum install -y make. sudo make altinstall. ```. ```. [pichuan@pichuan-centos7 Python-3.8.10]$ python3.8 --version. Python 3.8.10. ```. ## Install Singularity. ```. sudo yum update -y && \. sudo yum groupinstall -y 'Development Tools' && \. sudo yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools \. cryptsetup. ```. ```. export VERSION=1.14.12 OS=linux ARCH=amd64. # Downloads the required Go package. wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz. # Extracts the archive. sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz. # Deletes the ``tar`` file. rm go$VERSION.$OS-$ARCH.tar.gz. export VERSION=3.8.4. wget https://github.com/hpcng/singularity/releases/download/v${VERSION}/singularity-${VERSION}.tar.gz. tar -xzf singularity-${VERSION}.tar.gz. pushd singularity-3.8.4. export PATH=/usr/local/go/bin:$PATH. ./mconfig. make -C builddir. sudo make -C builddir install. ```. Check version:. ```. [pichuan@pichuan-centos7 ~]$ singularity --version. singularity version 3.8.4. ```. ## Try running DeepVar",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:350,energy efficiency,cloud,cloud,350,"@asherrar Unfortunately, I'm unable to reproduce what you're seeing. I'm going to write down what I did. Maybe you can spot what differences we have:. Create a CentOS7 machine to test:. ```. gcloud compute instances create ""${USER}-centos7"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-7"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --zone ""us-west1-b"". gcloud compute ssh pichuan-centos7 --zone ""us-west1-b"". ```. By default the machine has Python2. ```. [pichuan@pichuan-centos7 ~]$ python --version . Python 2.7.5. ```. ## Install Python 3.8.10. ```. sudo yum update. sudo yum install gcc openssl-devel bzip2-devel libffi-devel -y. sudo yum install -y wget. sudo yum install -y python3. ```. ```. wget https://www.python.org/ftp/python/3.8.10/Python-3.8.10.tgz. tar xvfz Python-3.8.10.tgz. ```. ```. cd Python-3.8.10. ./configure --enable-optimizations. ```. ```. sudo yum install -y make. sudo make altinstall. ```. ```. [pichuan@pichuan-centos7 Python-3.8.10]$ python3.8 --version. Python 3.8.10. ```. ## Install Singularity. ```. sudo yum update -y && \. sudo yum groupinstall -y 'Development Tools' && \. sudo yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools \. cryptsetup. ```. ```. export VERSION=1.14.12 OS=linux ARCH=amd64. # Downloads the required Go package. wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz. # Extracts the archive. sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz. # Deletes the ``tar`` file. rm go$VERSION.$OS-$ARCH.tar.gz. export VERSION=3.8.4. wget https://github.com/hpcng/singularity/releases/download/v${VERSION}/singularity-${VERSION}.tar.gz. tar -xzf singularity-${VERSION}.tar.gz. pushd singularity-3.8.4. export PATH=/usr/local/go/bin:$PATH. ./mconfig. make -C builddir. sudo make -C builddir install. ```. Check version:. ```. [pichuan@pichuan-centos7 ~]$ singularity --version. singularity version 3.8.4. ```. ## Try running DeepVar",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:904,energy efficiency,optim,optimizations,904,"@asherrar Unfortunately, I'm unable to reproduce what you're seeing. I'm going to write down what I did. Maybe you can spot what differences we have:. Create a CentOS7 machine to test:. ```. gcloud compute instances create ""${USER}-centos7"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-7"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --zone ""us-west1-b"". gcloud compute ssh pichuan-centos7 --zone ""us-west1-b"". ```. By default the machine has Python2. ```. [pichuan@pichuan-centos7 ~]$ python --version . Python 2.7.5. ```. ## Install Python 3.8.10. ```. sudo yum update. sudo yum install gcc openssl-devel bzip2-devel libffi-devel -y. sudo yum install -y wget. sudo yum install -y python3. ```. ```. wget https://www.python.org/ftp/python/3.8.10/Python-3.8.10.tgz. tar xvfz Python-3.8.10.tgz. ```. ```. cd Python-3.8.10. ./configure --enable-optimizations. ```. ```. sudo yum install -y make. sudo make altinstall. ```. ```. [pichuan@pichuan-centos7 Python-3.8.10]$ python3.8 --version. Python 3.8.10. ```. ## Install Singularity. ```. sudo yum update -y && \. sudo yum groupinstall -y 'Development Tools' && \. sudo yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools \. cryptsetup. ```. ```. export VERSION=1.14.12 OS=linux ARCH=amd64. # Downloads the required Go package. wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz. # Extracts the archive. sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz. # Deletes the ``tar`` file. rm go$VERSION.$OS-$ARCH.tar.gz. export VERSION=3.8.4. wget https://github.com/hpcng/singularity/releases/download/v${VERSION}/singularity-${VERSION}.tar.gz. tar -xzf singularity-${VERSION}.tar.gz. pushd singularity-3.8.4. export PATH=/usr/local/go/bin:$PATH. ./mconfig. make -C builddir. sudo make -C builddir install. ```. Check version:. ```. [pichuan@pichuan-centos7 ~]$ singularity --version. singularity version 3.8.4. ```. ## Try running DeepVar",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:556,integrability,version,version,556,"@asherrar Unfortunately, I'm unable to reproduce what you're seeing. I'm going to write down what I did. Maybe you can spot what differences we have:. Create a CentOS7 machine to test:. ```. gcloud compute instances create ""${USER}-centos7"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-7"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --zone ""us-west1-b"". gcloud compute ssh pichuan-centos7 --zone ""us-west1-b"". ```. By default the machine has Python2. ```. [pichuan@pichuan-centos7 ~]$ python --version . Python 2.7.5. ```. ## Install Python 3.8.10. ```. sudo yum update. sudo yum install gcc openssl-devel bzip2-devel libffi-devel -y. sudo yum install -y wget. sudo yum install -y python3. ```. ```. wget https://www.python.org/ftp/python/3.8.10/Python-3.8.10.tgz. tar xvfz Python-3.8.10.tgz. ```. ```. cd Python-3.8.10. ./configure --enable-optimizations. ```. ```. sudo yum install -y make. sudo make altinstall. ```. ```. [pichuan@pichuan-centos7 Python-3.8.10]$ python3.8 --version. Python 3.8.10. ```. ## Install Singularity. ```. sudo yum update -y && \. sudo yum groupinstall -y 'Development Tools' && \. sudo yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools \. cryptsetup. ```. ```. export VERSION=1.14.12 OS=linux ARCH=amd64. # Downloads the required Go package. wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz. # Extracts the archive. sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz. # Deletes the ``tar`` file. rm go$VERSION.$OS-$ARCH.tar.gz. export VERSION=3.8.4. wget https://github.com/hpcng/singularity/releases/download/v${VERSION}/singularity-${VERSION}.tar.gz. tar -xzf singularity-${VERSION}.tar.gz. pushd singularity-3.8.4. export PATH=/usr/local/go/bin:$PATH. ./mconfig. make -C builddir. sudo make -C builddir install. ```. Check version:. ```. [pichuan@pichuan-centos7 ~]$ singularity --version. singularity version 3.8.4. ```. ## Try running DeepVar",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:885,integrability,configur,configure,885,"@asherrar Unfortunately, I'm unable to reproduce what you're seeing. I'm going to write down what I did. Maybe you can spot what differences we have:. Create a CentOS7 machine to test:. ```. gcloud compute instances create ""${USER}-centos7"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-7"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --zone ""us-west1-b"". gcloud compute ssh pichuan-centos7 --zone ""us-west1-b"". ```. By default the machine has Python2. ```. [pichuan@pichuan-centos7 ~]$ python --version . Python 2.7.5. ```. ## Install Python 3.8.10. ```. sudo yum update. sudo yum install gcc openssl-devel bzip2-devel libffi-devel -y. sudo yum install -y wget. sudo yum install -y python3. ```. ```. wget https://www.python.org/ftp/python/3.8.10/Python-3.8.10.tgz. tar xvfz Python-3.8.10.tgz. ```. ```. cd Python-3.8.10. ./configure --enable-optimizations. ```. ```. sudo yum install -y make. sudo make altinstall. ```. ```. [pichuan@pichuan-centos7 Python-3.8.10]$ python3.8 --version. Python 3.8.10. ```. ## Install Singularity. ```. sudo yum update -y && \. sudo yum groupinstall -y 'Development Tools' && \. sudo yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools \. cryptsetup. ```. ```. export VERSION=1.14.12 OS=linux ARCH=amd64. # Downloads the required Go package. wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz. # Extracts the archive. sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz. # Deletes the ``tar`` file. rm go$VERSION.$OS-$ARCH.tar.gz. export VERSION=3.8.4. wget https://github.com/hpcng/singularity/releases/download/v${VERSION}/singularity-${VERSION}.tar.gz. tar -xzf singularity-${VERSION}.tar.gz. pushd singularity-3.8.4. export PATH=/usr/local/go/bin:$PATH. ./mconfig. make -C builddir. sudo make -C builddir install. ```. Check version:. ```. [pichuan@pichuan-centos7 ~]$ singularity --version. singularity version 3.8.4. ```. ## Try running DeepVar",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:1040,integrability,version,version,1040,"duce what you're seeing. I'm going to write down what I did. Maybe you can spot what differences we have:. Create a CentOS7 machine to test:. ```. gcloud compute instances create ""${USER}-centos7"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-7"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --zone ""us-west1-b"". gcloud compute ssh pichuan-centos7 --zone ""us-west1-b"". ```. By default the machine has Python2. ```. [pichuan@pichuan-centos7 ~]$ python --version . Python 2.7.5. ```. ## Install Python 3.8.10. ```. sudo yum update. sudo yum install gcc openssl-devel bzip2-devel libffi-devel -y. sudo yum install -y wget. sudo yum install -y python3. ```. ```. wget https://www.python.org/ftp/python/3.8.10/Python-3.8.10.tgz. tar xvfz Python-3.8.10.tgz. ```. ```. cd Python-3.8.10. ./configure --enable-optimizations. ```. ```. sudo yum install -y make. sudo make altinstall. ```. ```. [pichuan@pichuan-centos7 Python-3.8.10]$ python3.8 --version. Python 3.8.10. ```. ## Install Singularity. ```. sudo yum update -y && \. sudo yum groupinstall -y 'Development Tools' && \. sudo yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools \. cryptsetup. ```. ```. export VERSION=1.14.12 OS=linux ARCH=amd64. # Downloads the required Go package. wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz. # Extracts the archive. sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz. # Deletes the ``tar`` file. rm go$VERSION.$OS-$ARCH.tar.gz. export VERSION=3.8.4. wget https://github.com/hpcng/singularity/releases/download/v${VERSION}/singularity-${VERSION}.tar.gz. tar -xzf singularity-${VERSION}.tar.gz. pushd singularity-3.8.4. export PATH=/usr/local/go/bin:$PATH. ./mconfig. make -C builddir. sudo make -C builddir install. ```. Check version:. ```. [pichuan@pichuan-centos7 ~]$ singularity --version. singularity version 3.8.4. ```. ## Try running DeepVariant with Singularity. I followed Quick Star",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:1306,integrability,VERSION,VERSION,1306,"y ""centos-7"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --zone ""us-west1-b"". gcloud compute ssh pichuan-centos7 --zone ""us-west1-b"". ```. By default the machine has Python2. ```. [pichuan@pichuan-centos7 ~]$ python --version . Python 2.7.5. ```. ## Install Python 3.8.10. ```. sudo yum update. sudo yum install gcc openssl-devel bzip2-devel libffi-devel -y. sudo yum install -y wget. sudo yum install -y python3. ```. ```. wget https://www.python.org/ftp/python/3.8.10/Python-3.8.10.tgz. tar xvfz Python-3.8.10.tgz. ```. ```. cd Python-3.8.10. ./configure --enable-optimizations. ```. ```. sudo yum install -y make. sudo make altinstall. ```. ```. [pichuan@pichuan-centos7 Python-3.8.10]$ python3.8 --version. Python 3.8.10. ```. ## Install Singularity. ```. sudo yum update -y && \. sudo yum groupinstall -y 'Development Tools' && \. sudo yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools \. cryptsetup. ```. ```. export VERSION=1.14.12 OS=linux ARCH=amd64. # Downloads the required Go package. wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz. # Extracts the archive. sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz. # Deletes the ``tar`` file. rm go$VERSION.$OS-$ARCH.tar.gz. export VERSION=3.8.4. wget https://github.com/hpcng/singularity/releases/download/v${VERSION}/singularity-${VERSION}.tar.gz. tar -xzf singularity-${VERSION}.tar.gz. pushd singularity-3.8.4. export PATH=/usr/local/go/bin:$PATH. ./mconfig. make -C builddir. sudo make -C builddir install. ```. Check version:. ```. [pichuan@pichuan-centos7 ~]$ singularity --version. singularity version 3.8.4. ```. ## Try running DeepVariant with Singularity. I followed Quick Start to get data. ```. singularity pull docker://google/deepvariant:1.4.0. # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/u",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:1413,integrability,VERSION,VERSION,1413,"cloud compute ssh pichuan-centos7 --zone ""us-west1-b"". ```. By default the machine has Python2. ```. [pichuan@pichuan-centos7 ~]$ python --version . Python 2.7.5. ```. ## Install Python 3.8.10. ```. sudo yum update. sudo yum install gcc openssl-devel bzip2-devel libffi-devel -y. sudo yum install -y wget. sudo yum install -y python3. ```. ```. wget https://www.python.org/ftp/python/3.8.10/Python-3.8.10.tgz. tar xvfz Python-3.8.10.tgz. ```. ```. cd Python-3.8.10. ./configure --enable-optimizations. ```. ```. sudo yum install -y make. sudo make altinstall. ```. ```. [pichuan@pichuan-centos7 Python-3.8.10]$ python3.8 --version. Python 3.8.10. ```. ## Install Singularity. ```. sudo yum update -y && \. sudo yum groupinstall -y 'Development Tools' && \. sudo yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools \. cryptsetup. ```. ```. export VERSION=1.14.12 OS=linux ARCH=amd64. # Downloads the required Go package. wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz. # Extracts the archive. sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz. # Deletes the ``tar`` file. rm go$VERSION.$OS-$ARCH.tar.gz. export VERSION=3.8.4. wget https://github.com/hpcng/singularity/releases/download/v${VERSION}/singularity-${VERSION}.tar.gz. tar -xzf singularity-${VERSION}.tar.gz. pushd singularity-3.8.4. export PATH=/usr/local/go/bin:$PATH. ./mconfig. make -C builddir. sudo make -C builddir install. ```. Check version:. ```. [pichuan@pichuan-centos7 ~]$ singularity --version. singularity version 3.8.4. ```. ## Try running DeepVariant with Singularity. I followed Quick Start to get data. ```. singularity pull docker://google/deepvariant:1.4.0. # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:1495,integrability,VERSION,VERSION,1495," has Python2. ```. [pichuan@pichuan-centos7 ~]$ python --version . Python 2.7.5. ```. ## Install Python 3.8.10. ```. sudo yum update. sudo yum install gcc openssl-devel bzip2-devel libffi-devel -y. sudo yum install -y wget. sudo yum install -y python3. ```. ```. wget https://www.python.org/ftp/python/3.8.10/Python-3.8.10.tgz. tar xvfz Python-3.8.10.tgz. ```. ```. cd Python-3.8.10. ./configure --enable-optimizations. ```. ```. sudo yum install -y make. sudo make altinstall. ```. ```. [pichuan@pichuan-centos7 Python-3.8.10]$ python3.8 --version. Python 3.8.10. ```. ## Install Singularity. ```. sudo yum update -y && \. sudo yum groupinstall -y 'Development Tools' && \. sudo yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools \. cryptsetup. ```. ```. export VERSION=1.14.12 OS=linux ARCH=amd64. # Downloads the required Go package. wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz. # Extracts the archive. sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz. # Deletes the ``tar`` file. rm go$VERSION.$OS-$ARCH.tar.gz. export VERSION=3.8.4. wget https://github.com/hpcng/singularity/releases/download/v${VERSION}/singularity-${VERSION}.tar.gz. tar -xzf singularity-${VERSION}.tar.gz. pushd singularity-3.8.4. export PATH=/usr/local/go/bin:$PATH. ./mconfig. make -C builddir. sudo make -C builddir install. ```. Check version:. ```. [pichuan@pichuan-centos7 ~]$ singularity --version. singularity version 3.8.4. ```. ## Try running DeepVariant with Singularity. I followed Quick Start to get data. ```. singularity pull docker://google/deepvariant:1.4.0. # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_g",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:1555,integrability,VERSION,VERSION,1555,"sion . Python 2.7.5. ```. ## Install Python 3.8.10. ```. sudo yum update. sudo yum install gcc openssl-devel bzip2-devel libffi-devel -y. sudo yum install -y wget. sudo yum install -y python3. ```. ```. wget https://www.python.org/ftp/python/3.8.10/Python-3.8.10.tgz. tar xvfz Python-3.8.10.tgz. ```. ```. cd Python-3.8.10. ./configure --enable-optimizations. ```. ```. sudo yum install -y make. sudo make altinstall. ```. ```. [pichuan@pichuan-centos7 Python-3.8.10]$ python3.8 --version. Python 3.8.10. ```. ## Install Singularity. ```. sudo yum update -y && \. sudo yum groupinstall -y 'Development Tools' && \. sudo yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools \. cryptsetup. ```. ```. export VERSION=1.14.12 OS=linux ARCH=amd64. # Downloads the required Go package. wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz. # Extracts the archive. sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz. # Deletes the ``tar`` file. rm go$VERSION.$OS-$ARCH.tar.gz. export VERSION=3.8.4. wget https://github.com/hpcng/singularity/releases/download/v${VERSION}/singularity-${VERSION}.tar.gz. tar -xzf singularity-${VERSION}.tar.gz. pushd singularity-3.8.4. export PATH=/usr/local/go/bin:$PATH. ./mconfig. make -C builddir. sudo make -C builddir install. ```. Check version:. ```. [pichuan@pichuan-centos7 ~]$ singularity --version. singularity version 3.8.4. ```. ## Try running DeepVariant with Singularity. I followed Quick Start to get data. ```. singularity pull docker://google/deepvariant:1.4.0. # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_result",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:1588,integrability,VERSION,VERSION,1588,"all Python 3.8.10. ```. sudo yum update. sudo yum install gcc openssl-devel bzip2-devel libffi-devel -y. sudo yum install -y wget. sudo yum install -y python3. ```. ```. wget https://www.python.org/ftp/python/3.8.10/Python-3.8.10.tgz. tar xvfz Python-3.8.10.tgz. ```. ```. cd Python-3.8.10. ./configure --enable-optimizations. ```. ```. sudo yum install -y make. sudo make altinstall. ```. ```. [pichuan@pichuan-centos7 Python-3.8.10]$ python3.8 --version. Python 3.8.10. ```. ## Install Singularity. ```. sudo yum update -y && \. sudo yum groupinstall -y 'Development Tools' && \. sudo yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools \. cryptsetup. ```. ```. export VERSION=1.14.12 OS=linux ARCH=amd64. # Downloads the required Go package. wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz. # Extracts the archive. sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz. # Deletes the ``tar`` file. rm go$VERSION.$OS-$ARCH.tar.gz. export VERSION=3.8.4. wget https://github.com/hpcng/singularity/releases/download/v${VERSION}/singularity-${VERSION}.tar.gz. tar -xzf singularity-${VERSION}.tar.gz. pushd singularity-3.8.4. export PATH=/usr/local/go/bin:$PATH. ./mconfig. make -C builddir. sudo make -C builddir install. ```. Check version:. ```. [pichuan@pichuan-centos7 ~]$ singularity --version. singularity version 3.8.4. ```. ## Try running DeepVariant with Singularity. I followed Quick Start to get data. ```. singularity pull docker://google/deepvariant:1.4.0. # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:1666,integrability,VERSION,VERSION,1666,"ip2-devel libffi-devel -y. sudo yum install -y wget. sudo yum install -y python3. ```. ```. wget https://www.python.org/ftp/python/3.8.10/Python-3.8.10.tgz. tar xvfz Python-3.8.10.tgz. ```. ```. cd Python-3.8.10. ./configure --enable-optimizations. ```. ```. sudo yum install -y make. sudo make altinstall. ```. ```. [pichuan@pichuan-centos7 Python-3.8.10]$ python3.8 --version. Python 3.8.10. ```. ## Install Singularity. ```. sudo yum update -y && \. sudo yum groupinstall -y 'Development Tools' && \. sudo yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools \. cryptsetup. ```. ```. export VERSION=1.14.12 OS=linux ARCH=amd64. # Downloads the required Go package. wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz. # Extracts the archive. sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz. # Deletes the ``tar`` file. rm go$VERSION.$OS-$ARCH.tar.gz. export VERSION=3.8.4. wget https://github.com/hpcng/singularity/releases/download/v${VERSION}/singularity-${VERSION}.tar.gz. tar -xzf singularity-${VERSION}.tar.gz. pushd singularity-3.8.4. export PATH=/usr/local/go/bin:$PATH. ./mconfig. make -C builddir. sudo make -C builddir install. ```. Check version:. ```. [pichuan@pichuan-centos7 ~]$ singularity --version. singularity version 3.8.4. ```. ## Try running DeepVariant with Singularity. I followed Quick Start to get data. ```. singularity pull docker://google/deepvariant:1.4.0. # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=1. ```. This worked for me. Check numpy version:",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:1689,integrability,VERSION,VERSION,1689,"-y. sudo yum install -y wget. sudo yum install -y python3. ```. ```. wget https://www.python.org/ftp/python/3.8.10/Python-3.8.10.tgz. tar xvfz Python-3.8.10.tgz. ```. ```. cd Python-3.8.10. ./configure --enable-optimizations. ```. ```. sudo yum install -y make. sudo make altinstall. ```. ```. [pichuan@pichuan-centos7 Python-3.8.10]$ python3.8 --version. Python 3.8.10. ```. ## Install Singularity. ```. sudo yum update -y && \. sudo yum groupinstall -y 'Development Tools' && \. sudo yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools \. cryptsetup. ```. ```. export VERSION=1.14.12 OS=linux ARCH=amd64. # Downloads the required Go package. wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz. # Extracts the archive. sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz. # Deletes the ``tar`` file. rm go$VERSION.$OS-$ARCH.tar.gz. export VERSION=3.8.4. wget https://github.com/hpcng/singularity/releases/download/v${VERSION}/singularity-${VERSION}.tar.gz. tar -xzf singularity-${VERSION}.tar.gz. pushd singularity-3.8.4. export PATH=/usr/local/go/bin:$PATH. ./mconfig. make -C builddir. sudo make -C builddir install. ```. Check version:. ```. [pichuan@pichuan-centos7 ~]$ singularity --version. singularity version 3.8.4. ```. ## Try running DeepVariant with Singularity. I followed Quick Start to get data. ```. singularity pull docker://google/deepvariant:1.4.0. # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=1. ```. This worked for me. Check numpy version:. ```. singularity run ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:1729,integrability,VERSION,VERSION,1729,"nstall -y python3. ```. ```. wget https://www.python.org/ftp/python/3.8.10/Python-3.8.10.tgz. tar xvfz Python-3.8.10.tgz. ```. ```. cd Python-3.8.10. ./configure --enable-optimizations. ```. ```. sudo yum install -y make. sudo make altinstall. ```. ```. [pichuan@pichuan-centos7 Python-3.8.10]$ python3.8 --version. Python 3.8.10. ```. ## Install Singularity. ```. sudo yum update -y && \. sudo yum groupinstall -y 'Development Tools' && \. sudo yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools \. cryptsetup. ```. ```. export VERSION=1.14.12 OS=linux ARCH=amd64. # Downloads the required Go package. wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz. # Extracts the archive. sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz. # Deletes the ``tar`` file. rm go$VERSION.$OS-$ARCH.tar.gz. export VERSION=3.8.4. wget https://github.com/hpcng/singularity/releases/download/v${VERSION}/singularity-${VERSION}.tar.gz. tar -xzf singularity-${VERSION}.tar.gz. pushd singularity-3.8.4. export PATH=/usr/local/go/bin:$PATH. ./mconfig. make -C builddir. sudo make -C builddir install. ```. Check version:. ```. [pichuan@pichuan-centos7 ~]$ singularity --version. singularity version 3.8.4. ```. ## Try running DeepVariant with Singularity. I followed Quick Start to get data. ```. singularity pull docker://google/deepvariant:1.4.0. # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=1. ```. This worked for me. Check numpy version:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:1879,integrability,version,version,1879,"./configure --enable-optimizations. ```. ```. sudo yum install -y make. sudo make altinstall. ```. ```. [pichuan@pichuan-centos7 Python-3.8.10]$ python3.8 --version. Python 3.8.10. ```. ## Install Singularity. ```. sudo yum update -y && \. sudo yum groupinstall -y 'Development Tools' && \. sudo yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools \. cryptsetup. ```. ```. export VERSION=1.14.12 OS=linux ARCH=amd64. # Downloads the required Go package. wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz. # Extracts the archive. sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz. # Deletes the ``tar`` file. rm go$VERSION.$OS-$ARCH.tar.gz. export VERSION=3.8.4. wget https://github.com/hpcng/singularity/releases/download/v${VERSION}/singularity-${VERSION}.tar.gz. tar -xzf singularity-${VERSION}.tar.gz. pushd singularity-3.8.4. export PATH=/usr/local/go/bin:$PATH. ./mconfig. make -C builddir. sudo make -C builddir install. ```. Check version:. ```. [pichuan@pichuan-centos7 ~]$ singularity --version. singularity version 3.8.4. ```. ## Try running DeepVariant with Singularity. I followed Quick Start to get data. ```. singularity pull docker://google/deepvariant:1.4.0. # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=1. ```. This worked for me. Check numpy version:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. python -c 'import numpy as np; print(np.version.version)'. ```. Which shows:. ```. 1.19.2. ```. It seems like my ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:1937,integrability,version,version,1937,"tall -y make. sudo make altinstall. ```. ```. [pichuan@pichuan-centos7 Python-3.8.10]$ python3.8 --version. Python 3.8.10. ```. ## Install Singularity. ```. sudo yum update -y && \. sudo yum groupinstall -y 'Development Tools' && \. sudo yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools \. cryptsetup. ```. ```. export VERSION=1.14.12 OS=linux ARCH=amd64. # Downloads the required Go package. wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz. # Extracts the archive. sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz. # Deletes the ``tar`` file. rm go$VERSION.$OS-$ARCH.tar.gz. export VERSION=3.8.4. wget https://github.com/hpcng/singularity/releases/download/v${VERSION}/singularity-${VERSION}.tar.gz. tar -xzf singularity-${VERSION}.tar.gz. pushd singularity-3.8.4. export PATH=/usr/local/go/bin:$PATH. ./mconfig. make -C builddir. sudo make -C builddir install. ```. Check version:. ```. [pichuan@pichuan-centos7 ~]$ singularity --version. singularity version 3.8.4. ```. ## Try running DeepVariant with Singularity. I followed Quick Start to get data. ```. singularity pull docker://google/deepvariant:1.4.0. # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=1. ```. This worked for me. Check numpy version:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. python -c 'import numpy as np; print(np.version.version)'. ```. Which shows:. ```. 1.19.2. ```. It seems like my machine doesn't already have numpy, though:. ```. [pichuan",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:1958,integrability,version,version,1958,"ke altinstall. ```. ```. [pichuan@pichuan-centos7 Python-3.8.10]$ python3.8 --version. Python 3.8.10. ```. ## Install Singularity. ```. sudo yum update -y && \. sudo yum groupinstall -y 'Development Tools' && \. sudo yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools \. cryptsetup. ```. ```. export VERSION=1.14.12 OS=linux ARCH=amd64. # Downloads the required Go package. wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz. # Extracts the archive. sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz. # Deletes the ``tar`` file. rm go$VERSION.$OS-$ARCH.tar.gz. export VERSION=3.8.4. wget https://github.com/hpcng/singularity/releases/download/v${VERSION}/singularity-${VERSION}.tar.gz. tar -xzf singularity-${VERSION}.tar.gz. pushd singularity-3.8.4. export PATH=/usr/local/go/bin:$PATH. ./mconfig. make -C builddir. sudo make -C builddir install. ```. Check version:. ```. [pichuan@pichuan-centos7 ~]$ singularity --version. singularity version 3.8.4. ```. ## Try running DeepVariant with Singularity. I followed Quick Start to get data. ```. singularity pull docker://google/deepvariant:1.4.0. # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=1. ```. This worked for me. Check numpy version:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. python -c 'import numpy as np; print(np.version.version)'. ```. Which shows:. ```. 1.19.2. ```. It seems like my machine doesn't already have numpy, though:. ```. [pichuan@pichuan-centos7 Pyth",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:2662,integrability,version,version,2662,"VERSION}/singularity-${VERSION}.tar.gz. tar -xzf singularity-${VERSION}.tar.gz. pushd singularity-3.8.4. export PATH=/usr/local/go/bin:$PATH. ./mconfig. make -C builddir. sudo make -C builddir install. ```. Check version:. ```. [pichuan@pichuan-centos7 ~]$ singularity --version. singularity version 3.8.4. ```. ## Try running DeepVariant with Singularity. I followed Quick Start to get data. ```. singularity pull docker://google/deepvariant:1.4.0. # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=1. ```. This worked for me. Check numpy version:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. python -c 'import numpy as np; print(np.version.version)'. ```. Which shows:. ```. 1.19.2. ```. It seems like my machine doesn't already have numpy, though:. ```. [pichuan@pichuan-centos7 Python-3.8.10]$ pip3.8 show numpy. WARNING: Package(s) not found: numpy. ```. doesn't show anything. ## Install numpy 1.23.0 to see it breaks things. I ran:. ```. pip3.8 install numpy==1.23.0. ```. (Because you mentioned your cluster has 1.23.0). Now this shows:. ```. [pichuan@pichuan-centos7 ~]$ pip3.8 show numpy. Name: numpy. Version: 1.23.0. Summary: NumPy is the fundamental package for array computing with Python. Home-page: https://www.numpy.org. Author: Travis E. Oliphant et al. Author-email: None. License: BSD. Location: /home/pichuan/.local/lib/python3.8/site-packages. Requires: . Required-by: . ```. Then I re-ran:. ```. # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:2810,integrability,version,version,2810,"fig. make -C builddir. sudo make -C builddir install. ```. Check version:. ```. [pichuan@pichuan-centos7 ~]$ singularity --version. singularity version 3.8.4. ```. ## Try running DeepVariant with Singularity. I followed Quick Start to get data. ```. singularity pull docker://google/deepvariant:1.4.0. # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=1. ```. This worked for me. Check numpy version:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. python -c 'import numpy as np; print(np.version.version)'. ```. Which shows:. ```. 1.19.2. ```. It seems like my machine doesn't already have numpy, though:. ```. [pichuan@pichuan-centos7 Python-3.8.10]$ pip3.8 show numpy. WARNING: Package(s) not found: numpy. ```. doesn't show anything. ## Install numpy 1.23.0 to see it breaks things. I ran:. ```. pip3.8 install numpy==1.23.0. ```. (Because you mentioned your cluster has 1.23.0). Now this shows:. ```. [pichuan@pichuan-centos7 ~]$ pip3.8 show numpy. Name: numpy. Version: 1.23.0. Summary: NumPy is the fundamental package for array computing with Python. Home-page: https://www.numpy.org. Author: Travis E. Oliphant et al. Author-email: None. License: BSD. Location: /home/pichuan/.local/lib/python3.8/site-packages. Requires: . Required-by: . ```. Then I re-ran:. ```. # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.f",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:2818,integrability,version,version,2818,"e -C builddir. sudo make -C builddir install. ```. Check version:. ```. [pichuan@pichuan-centos7 ~]$ singularity --version. singularity version 3.8.4. ```. ## Try running DeepVariant with Singularity. I followed Quick Start to get data. ```. singularity pull docker://google/deepvariant:1.4.0. # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=1. ```. This worked for me. Check numpy version:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. python -c 'import numpy as np; print(np.version.version)'. ```. Which shows:. ```. 1.19.2. ```. It seems like my machine doesn't already have numpy, though:. ```. [pichuan@pichuan-centos7 Python-3.8.10]$ pip3.8 show numpy. WARNING: Package(s) not found: numpy. ```. doesn't show anything. ## Install numpy 1.23.0 to see it breaks things. I ran:. ```. pip3.8 install numpy==1.23.0. ```. (Because you mentioned your cluster has 1.23.0). Now this shows:. ```. [pichuan@pichuan-centos7 ~]$ pip3.8 show numpy. Name: numpy. Version: 1.23.0. Summary: NumPy is the fundamental package for array computing with Python. Home-page: https://www.numpy.org. Author: Travis E. Oliphant et al. Author-email: None. License: BSD. Location: /home/pichuan/.local/lib/python3.8/site-packages. Requires: . Required-by: . ```. Then I re-ran:. ```. # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:3288,integrability,Version,Version,3288,"f=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=1. ```. This worked for me. Check numpy version:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. python -c 'import numpy as np; print(np.version.version)'. ```. Which shows:. ```. 1.19.2. ```. It seems like my machine doesn't already have numpy, though:. ```. [pichuan@pichuan-centos7 Python-3.8.10]$ pip3.8 show numpy. WARNING: Package(s) not found: numpy. ```. doesn't show anything. ## Install numpy 1.23.0 to see it breaks things. I ran:. ```. pip3.8 install numpy==1.23.0. ```. (Because you mentioned your cluster has 1.23.0). Now this shows:. ```. [pichuan@pichuan-centos7 ~]$ pip3.8 show numpy. Name: numpy. Version: 1.23.0. Summary: NumPy is the fundamental package for array computing with Python. Home-page: https://www.numpy.org. Author: Travis E. Oliphant et al. Author-email: None. License: BSD. Location: /home/pichuan/.local/lib/python3.8/site-packages. Requires: . Required-by: . ```. Then I re-ran:. ```. # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=1. ```. Which still seems to work fine for me. And, this actually shows 1.23.0 as well:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. pyth",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:4328,integrability,version,version,4328,"lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. python -c 'import numpy as np; print(np.version.version)'. ```. Which shows:. ```. 1.19.2. ```. It seems like my machine doesn't already have numpy, though:. ```. [pichuan@pichuan-centos7 Python-3.8.10]$ pip3.8 show numpy. WARNING: Package(s) not found: numpy. ```. doesn't show anything. ## Install numpy 1.23.0 to see it breaks things. I ran:. ```. pip3.8 install numpy==1.23.0. ```. (Because you mentioned your cluster has 1.23.0). Now this shows:. ```. [pichuan@pichuan-centos7 ~]$ pip3.8 show numpy. Name: numpy. Version: 1.23.0. Summary: NumPy is the fundamental package for array computing with Python. Home-page: https://www.numpy.org. Author: Travis E. Oliphant et al. Author-email: None. License: BSD. Location: /home/pichuan/.local/lib/python3.8/site-packages. Requires: . Required-by: . ```. Then I re-ran:. ```. # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=1. ```. Which still seems to work fine for me. And, this actually shows 1.23.0 as well:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. python -c 'import numpy as np; print(np.version.version)'. ```. ```. 1.23.0. ```. I also checked TensorFlow version:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. python -c 'import tensorflow; print(tensorflow.__version__)'. ```. This shows:. ```. 2.7.0. ```. @asherrar Let me know what you think I need to change to try to reproduce the error you saw. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:4336,integrability,version,version,4336,"lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. python -c 'import numpy as np; print(np.version.version)'. ```. Which shows:. ```. 1.19.2. ```. It seems like my machine doesn't already have numpy, though:. ```. [pichuan@pichuan-centos7 Python-3.8.10]$ pip3.8 show numpy. WARNING: Package(s) not found: numpy. ```. doesn't show anything. ## Install numpy 1.23.0 to see it breaks things. I ran:. ```. pip3.8 install numpy==1.23.0. ```. (Because you mentioned your cluster has 1.23.0). Now this shows:. ```. [pichuan@pichuan-centos7 ~]$ pip3.8 show numpy. Name: numpy. Version: 1.23.0. Summary: NumPy is the fundamental package for array computing with Python. Home-page: https://www.numpy.org. Author: Travis E. Oliphant et al. Author-email: None. License: BSD. Location: /home/pichuan/.local/lib/python3.8/site-packages. Requires: . Required-by: . ```. Then I re-ran:. ```. # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=1. ```. Which still seems to work fine for me. And, this actually shows 1.23.0 as well:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. python -c 'import numpy as np; print(np.version.version)'. ```. ```. 1.23.0. ```. I also checked TensorFlow version:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. python -c 'import tensorflow; print(tensorflow.__version__)'. ```. This shows:. ```. 2.7.0. ```. @asherrar Let me know what you think I need to change to try to reproduce the error you saw. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:4396,integrability,version,version,4396,"lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. python -c 'import numpy as np; print(np.version.version)'. ```. Which shows:. ```. 1.19.2. ```. It seems like my machine doesn't already have numpy, though:. ```. [pichuan@pichuan-centos7 Python-3.8.10]$ pip3.8 show numpy. WARNING: Package(s) not found: numpy. ```. doesn't show anything. ## Install numpy 1.23.0 to see it breaks things. I ran:. ```. pip3.8 install numpy==1.23.0. ```. (Because you mentioned your cluster has 1.23.0). Now this shows:. ```. [pichuan@pichuan-centos7 ~]$ pip3.8 show numpy. Name: numpy. Version: 1.23.0. Summary: NumPy is the fundamental package for array computing with Python. Home-page: https://www.numpy.org. Author: Travis E. Oliphant et al. Author-email: None. License: BSD. Location: /home/pichuan/.local/lib/python3.8/site-packages. Requires: . Required-by: . ```. Then I re-ran:. ```. # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=1. ```. Which still seems to work fine for me. And, this actually shows 1.23.0 as well:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. python -c 'import numpy as np; print(np.version.version)'. ```. ```. 1.23.0. ```. I also checked TensorFlow version:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. python -c 'import tensorflow; print(tensorflow.__version__)'. ```. This shows:. ```. 2.7.0. ```. @asherrar Let me know what you think I need to change to try to reproduce the error you saw. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:284,interoperability,platform,platform,284,"@asherrar Unfortunately, I'm unable to reproduce what you're seeing. I'm going to write down what I did. Maybe you can spot what differences we have:. Create a CentOS7 machine to test:. ```. gcloud compute instances create ""${USER}-centos7"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-7"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --zone ""us-west1-b"". gcloud compute ssh pichuan-centos7 --zone ""us-west1-b"". ```. By default the machine has Python2. ```. [pichuan@pichuan-centos7 ~]$ python --version . Python 2.7.5. ```. ## Install Python 3.8.10. ```. sudo yum update. sudo yum install gcc openssl-devel bzip2-devel libffi-devel -y. sudo yum install -y wget. sudo yum install -y python3. ```. ```. wget https://www.python.org/ftp/python/3.8.10/Python-3.8.10.tgz. tar xvfz Python-3.8.10.tgz. ```. ```. cd Python-3.8.10. ./configure --enable-optimizations. ```. ```. sudo yum install -y make. sudo make altinstall. ```. ```. [pichuan@pichuan-centos7 Python-3.8.10]$ python3.8 --version. Python 3.8.10. ```. ## Install Singularity. ```. sudo yum update -y && \. sudo yum groupinstall -y 'Development Tools' && \. sudo yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools \. cryptsetup. ```. ```. export VERSION=1.14.12 OS=linux ARCH=amd64. # Downloads the required Go package. wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz. # Extracts the archive. sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz. # Deletes the ``tar`` file. rm go$VERSION.$OS-$ARCH.tar.gz. export VERSION=3.8.4. wget https://github.com/hpcng/singularity/releases/download/v${VERSION}/singularity-${VERSION}.tar.gz. tar -xzf singularity-${VERSION}.tar.gz. pushd singularity-3.8.4. export PATH=/usr/local/go/bin:$PATH. ./mconfig. make -C builddir. sudo make -C builddir install. ```. Check version:. ```. [pichuan@pichuan-centos7 ~]$ singularity --version. singularity version 3.8.4. ```. ## Try running DeepVar",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:379,interoperability,standard,standard-,379,"@asherrar Unfortunately, I'm unable to reproduce what you're seeing. I'm going to write down what I did. Maybe you can spot what differences we have:. Create a CentOS7 machine to test:. ```. gcloud compute instances create ""${USER}-centos7"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-7"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --zone ""us-west1-b"". gcloud compute ssh pichuan-centos7 --zone ""us-west1-b"". ```. By default the machine has Python2. ```. [pichuan@pichuan-centos7 ~]$ python --version . Python 2.7.5. ```. ## Install Python 3.8.10. ```. sudo yum update. sudo yum install gcc openssl-devel bzip2-devel libffi-devel -y. sudo yum install -y wget. sudo yum install -y python3. ```. ```. wget https://www.python.org/ftp/python/3.8.10/Python-3.8.10.tgz. tar xvfz Python-3.8.10.tgz. ```. ```. cd Python-3.8.10. ./configure --enable-optimizations. ```. ```. sudo yum install -y make. sudo make altinstall. ```. ```. [pichuan@pichuan-centos7 Python-3.8.10]$ python3.8 --version. Python 3.8.10. ```. ## Install Singularity. ```. sudo yum update -y && \. sudo yum groupinstall -y 'Development Tools' && \. sudo yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools \. cryptsetup. ```. ```. export VERSION=1.14.12 OS=linux ARCH=amd64. # Downloads the required Go package. wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz. # Extracts the archive. sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz. # Deletes the ``tar`` file. rm go$VERSION.$OS-$ARCH.tar.gz. export VERSION=3.8.4. wget https://github.com/hpcng/singularity/releases/download/v${VERSION}/singularity-${VERSION}.tar.gz. tar -xzf singularity-${VERSION}.tar.gz. pushd singularity-3.8.4. export PATH=/usr/local/go/bin:$PATH. ./mconfig. make -C builddir. sudo make -C builddir install. ```. Check version:. ```. [pichuan@pichuan-centos7 ~]$ singularity --version. singularity version 3.8.4. ```. ## Try running DeepVar",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:556,modifiability,version,version,556,"@asherrar Unfortunately, I'm unable to reproduce what you're seeing. I'm going to write down what I did. Maybe you can spot what differences we have:. Create a CentOS7 machine to test:. ```. gcloud compute instances create ""${USER}-centos7"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-7"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --zone ""us-west1-b"". gcloud compute ssh pichuan-centos7 --zone ""us-west1-b"". ```. By default the machine has Python2. ```. [pichuan@pichuan-centos7 ~]$ python --version . Python 2.7.5. ```. ## Install Python 3.8.10. ```. sudo yum update. sudo yum install gcc openssl-devel bzip2-devel libffi-devel -y. sudo yum install -y wget. sudo yum install -y python3. ```. ```. wget https://www.python.org/ftp/python/3.8.10/Python-3.8.10.tgz. tar xvfz Python-3.8.10.tgz. ```. ```. cd Python-3.8.10. ./configure --enable-optimizations. ```. ```. sudo yum install -y make. sudo make altinstall. ```. ```. [pichuan@pichuan-centos7 Python-3.8.10]$ python3.8 --version. Python 3.8.10. ```. ## Install Singularity. ```. sudo yum update -y && \. sudo yum groupinstall -y 'Development Tools' && \. sudo yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools \. cryptsetup. ```. ```. export VERSION=1.14.12 OS=linux ARCH=amd64. # Downloads the required Go package. wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz. # Extracts the archive. sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz. # Deletes the ``tar`` file. rm go$VERSION.$OS-$ARCH.tar.gz. export VERSION=3.8.4. wget https://github.com/hpcng/singularity/releases/download/v${VERSION}/singularity-${VERSION}.tar.gz. tar -xzf singularity-${VERSION}.tar.gz. pushd singularity-3.8.4. export PATH=/usr/local/go/bin:$PATH. ./mconfig. make -C builddir. sudo make -C builddir install. ```. Check version:. ```. [pichuan@pichuan-centos7 ~]$ singularity --version. singularity version 3.8.4. ```. ## Try running DeepVar",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:885,modifiability,configur,configure,885,"@asherrar Unfortunately, I'm unable to reproduce what you're seeing. I'm going to write down what I did. Maybe you can spot what differences we have:. Create a CentOS7 machine to test:. ```. gcloud compute instances create ""${USER}-centos7"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-7"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --zone ""us-west1-b"". gcloud compute ssh pichuan-centos7 --zone ""us-west1-b"". ```. By default the machine has Python2. ```. [pichuan@pichuan-centos7 ~]$ python --version . Python 2.7.5. ```. ## Install Python 3.8.10. ```. sudo yum update. sudo yum install gcc openssl-devel bzip2-devel libffi-devel -y. sudo yum install -y wget. sudo yum install -y python3. ```. ```. wget https://www.python.org/ftp/python/3.8.10/Python-3.8.10.tgz. tar xvfz Python-3.8.10.tgz. ```. ```. cd Python-3.8.10. ./configure --enable-optimizations. ```. ```. sudo yum install -y make. sudo make altinstall. ```. ```. [pichuan@pichuan-centos7 Python-3.8.10]$ python3.8 --version. Python 3.8.10. ```. ## Install Singularity. ```. sudo yum update -y && \. sudo yum groupinstall -y 'Development Tools' && \. sudo yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools \. cryptsetup. ```. ```. export VERSION=1.14.12 OS=linux ARCH=amd64. # Downloads the required Go package. wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz. # Extracts the archive. sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz. # Deletes the ``tar`` file. rm go$VERSION.$OS-$ARCH.tar.gz. export VERSION=3.8.4. wget https://github.com/hpcng/singularity/releases/download/v${VERSION}/singularity-${VERSION}.tar.gz. tar -xzf singularity-${VERSION}.tar.gz. pushd singularity-3.8.4. export PATH=/usr/local/go/bin:$PATH. ./mconfig. make -C builddir. sudo make -C builddir install. ```. Check version:. ```. [pichuan@pichuan-centos7 ~]$ singularity --version. singularity version 3.8.4. ```. ## Try running DeepVar",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:1040,modifiability,version,version,1040,"duce what you're seeing. I'm going to write down what I did. Maybe you can spot what differences we have:. Create a CentOS7 machine to test:. ```. gcloud compute instances create ""${USER}-centos7"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-7"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --zone ""us-west1-b"". gcloud compute ssh pichuan-centos7 --zone ""us-west1-b"". ```. By default the machine has Python2. ```. [pichuan@pichuan-centos7 ~]$ python --version . Python 2.7.5. ```. ## Install Python 3.8.10. ```. sudo yum update. sudo yum install gcc openssl-devel bzip2-devel libffi-devel -y. sudo yum install -y wget. sudo yum install -y python3. ```. ```. wget https://www.python.org/ftp/python/3.8.10/Python-3.8.10.tgz. tar xvfz Python-3.8.10.tgz. ```. ```. cd Python-3.8.10. ./configure --enable-optimizations. ```. ```. sudo yum install -y make. sudo make altinstall. ```. ```. [pichuan@pichuan-centos7 Python-3.8.10]$ python3.8 --version. Python 3.8.10. ```. ## Install Singularity. ```. sudo yum update -y && \. sudo yum groupinstall -y 'Development Tools' && \. sudo yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools \. cryptsetup. ```. ```. export VERSION=1.14.12 OS=linux ARCH=amd64. # Downloads the required Go package. wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz. # Extracts the archive. sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz. # Deletes the ``tar`` file. rm go$VERSION.$OS-$ARCH.tar.gz. export VERSION=3.8.4. wget https://github.com/hpcng/singularity/releases/download/v${VERSION}/singularity-${VERSION}.tar.gz. tar -xzf singularity-${VERSION}.tar.gz. pushd singularity-3.8.4. export PATH=/usr/local/go/bin:$PATH. ./mconfig. make -C builddir. sudo make -C builddir install. ```. Check version:. ```. [pichuan@pichuan-centos7 ~]$ singularity --version. singularity version 3.8.4. ```. ## Try running DeepVariant with Singularity. I followed Quick Star",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:1306,modifiability,VERSION,VERSION,1306,"y ""centos-7"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --zone ""us-west1-b"". gcloud compute ssh pichuan-centos7 --zone ""us-west1-b"". ```. By default the machine has Python2. ```. [pichuan@pichuan-centos7 ~]$ python --version . Python 2.7.5. ```. ## Install Python 3.8.10. ```. sudo yum update. sudo yum install gcc openssl-devel bzip2-devel libffi-devel -y. sudo yum install -y wget. sudo yum install -y python3. ```. ```. wget https://www.python.org/ftp/python/3.8.10/Python-3.8.10.tgz. tar xvfz Python-3.8.10.tgz. ```. ```. cd Python-3.8.10. ./configure --enable-optimizations. ```. ```. sudo yum install -y make. sudo make altinstall. ```. ```. [pichuan@pichuan-centos7 Python-3.8.10]$ python3.8 --version. Python 3.8.10. ```. ## Install Singularity. ```. sudo yum update -y && \. sudo yum groupinstall -y 'Development Tools' && \. sudo yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools \. cryptsetup. ```. ```. export VERSION=1.14.12 OS=linux ARCH=amd64. # Downloads the required Go package. wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz. # Extracts the archive. sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz. # Deletes the ``tar`` file. rm go$VERSION.$OS-$ARCH.tar.gz. export VERSION=3.8.4. wget https://github.com/hpcng/singularity/releases/download/v${VERSION}/singularity-${VERSION}.tar.gz. tar -xzf singularity-${VERSION}.tar.gz. pushd singularity-3.8.4. export PATH=/usr/local/go/bin:$PATH. ./mconfig. make -C builddir. sudo make -C builddir install. ```. Check version:. ```. [pichuan@pichuan-centos7 ~]$ singularity --version. singularity version 3.8.4. ```. ## Try running DeepVariant with Singularity. I followed Quick Start to get data. ```. singularity pull docker://google/deepvariant:1.4.0. # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/u",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:1371,modifiability,pac,package,1371,"""e2-standard-16"" \. --zone ""us-west1-b"". gcloud compute ssh pichuan-centos7 --zone ""us-west1-b"". ```. By default the machine has Python2. ```. [pichuan@pichuan-centos7 ~]$ python --version . Python 2.7.5. ```. ## Install Python 3.8.10. ```. sudo yum update. sudo yum install gcc openssl-devel bzip2-devel libffi-devel -y. sudo yum install -y wget. sudo yum install -y python3. ```. ```. wget https://www.python.org/ftp/python/3.8.10/Python-3.8.10.tgz. tar xvfz Python-3.8.10.tgz. ```. ```. cd Python-3.8.10. ./configure --enable-optimizations. ```. ```. sudo yum install -y make. sudo make altinstall. ```. ```. [pichuan@pichuan-centos7 Python-3.8.10]$ python3.8 --version. Python 3.8.10. ```. ## Install Singularity. ```. sudo yum update -y && \. sudo yum groupinstall -y 'Development Tools' && \. sudo yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools \. cryptsetup. ```. ```. export VERSION=1.14.12 OS=linux ARCH=amd64. # Downloads the required Go package. wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz. # Extracts the archive. sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz. # Deletes the ``tar`` file. rm go$VERSION.$OS-$ARCH.tar.gz. export VERSION=3.8.4. wget https://github.com/hpcng/singularity/releases/download/v${VERSION}/singularity-${VERSION}.tar.gz. tar -xzf singularity-${VERSION}.tar.gz. pushd singularity-3.8.4. export PATH=/usr/local/go/bin:$PATH. ./mconfig. make -C builddir. sudo make -C builddir install. ```. Check version:. ```. [pichuan@pichuan-centos7 ~]$ singularity --version. singularity version 3.8.4. ```. ## Try running DeepVariant with Singularity. I followed Quick Start to get data. ```. singularity pull docker://google/deepvariant:1.4.0. # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:1413,modifiability,VERSION,VERSION,1413,"cloud compute ssh pichuan-centos7 --zone ""us-west1-b"". ```. By default the machine has Python2. ```. [pichuan@pichuan-centos7 ~]$ python --version . Python 2.7.5. ```. ## Install Python 3.8.10. ```. sudo yum update. sudo yum install gcc openssl-devel bzip2-devel libffi-devel -y. sudo yum install -y wget. sudo yum install -y python3. ```. ```. wget https://www.python.org/ftp/python/3.8.10/Python-3.8.10.tgz. tar xvfz Python-3.8.10.tgz. ```. ```. cd Python-3.8.10. ./configure --enable-optimizations. ```. ```. sudo yum install -y make. sudo make altinstall. ```. ```. [pichuan@pichuan-centos7 Python-3.8.10]$ python3.8 --version. Python 3.8.10. ```. ## Install Singularity. ```. sudo yum update -y && \. sudo yum groupinstall -y 'Development Tools' && \. sudo yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools \. cryptsetup. ```. ```. export VERSION=1.14.12 OS=linux ARCH=amd64. # Downloads the required Go package. wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz. # Extracts the archive. sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz. # Deletes the ``tar`` file. rm go$VERSION.$OS-$ARCH.tar.gz. export VERSION=3.8.4. wget https://github.com/hpcng/singularity/releases/download/v${VERSION}/singularity-${VERSION}.tar.gz. tar -xzf singularity-${VERSION}.tar.gz. pushd singularity-3.8.4. export PATH=/usr/local/go/bin:$PATH. ./mconfig. make -C builddir. sudo make -C builddir install. ```. Check version:. ```. [pichuan@pichuan-centos7 ~]$ singularity --version. singularity version 3.8.4. ```. ## Try running DeepVariant with Singularity. I followed Quick Start to get data. ```. singularity pull docker://google/deepvariant:1.4.0. # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:1495,modifiability,VERSION,VERSION,1495," has Python2. ```. [pichuan@pichuan-centos7 ~]$ python --version . Python 2.7.5. ```. ## Install Python 3.8.10. ```. sudo yum update. sudo yum install gcc openssl-devel bzip2-devel libffi-devel -y. sudo yum install -y wget. sudo yum install -y python3. ```. ```. wget https://www.python.org/ftp/python/3.8.10/Python-3.8.10.tgz. tar xvfz Python-3.8.10.tgz. ```. ```. cd Python-3.8.10. ./configure --enable-optimizations. ```. ```. sudo yum install -y make. sudo make altinstall. ```. ```. [pichuan@pichuan-centos7 Python-3.8.10]$ python3.8 --version. Python 3.8.10. ```. ## Install Singularity. ```. sudo yum update -y && \. sudo yum groupinstall -y 'Development Tools' && \. sudo yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools \. cryptsetup. ```. ```. export VERSION=1.14.12 OS=linux ARCH=amd64. # Downloads the required Go package. wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz. # Extracts the archive. sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz. # Deletes the ``tar`` file. rm go$VERSION.$OS-$ARCH.tar.gz. export VERSION=3.8.4. wget https://github.com/hpcng/singularity/releases/download/v${VERSION}/singularity-${VERSION}.tar.gz. tar -xzf singularity-${VERSION}.tar.gz. pushd singularity-3.8.4. export PATH=/usr/local/go/bin:$PATH. ./mconfig. make -C builddir. sudo make -C builddir install. ```. Check version:. ```. [pichuan@pichuan-centos7 ~]$ singularity --version. singularity version 3.8.4. ```. ## Try running DeepVariant with Singularity. I followed Quick Start to get data. ```. singularity pull docker://google/deepvariant:1.4.0. # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_g",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:1555,modifiability,VERSION,VERSION,1555,"sion . Python 2.7.5. ```. ## Install Python 3.8.10. ```. sudo yum update. sudo yum install gcc openssl-devel bzip2-devel libffi-devel -y. sudo yum install -y wget. sudo yum install -y python3. ```. ```. wget https://www.python.org/ftp/python/3.8.10/Python-3.8.10.tgz. tar xvfz Python-3.8.10.tgz. ```. ```. cd Python-3.8.10. ./configure --enable-optimizations. ```. ```. sudo yum install -y make. sudo make altinstall. ```. ```. [pichuan@pichuan-centos7 Python-3.8.10]$ python3.8 --version. Python 3.8.10. ```. ## Install Singularity. ```. sudo yum update -y && \. sudo yum groupinstall -y 'Development Tools' && \. sudo yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools \. cryptsetup. ```. ```. export VERSION=1.14.12 OS=linux ARCH=amd64. # Downloads the required Go package. wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz. # Extracts the archive. sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz. # Deletes the ``tar`` file. rm go$VERSION.$OS-$ARCH.tar.gz. export VERSION=3.8.4. wget https://github.com/hpcng/singularity/releases/download/v${VERSION}/singularity-${VERSION}.tar.gz. tar -xzf singularity-${VERSION}.tar.gz. pushd singularity-3.8.4. export PATH=/usr/local/go/bin:$PATH. ./mconfig. make -C builddir. sudo make -C builddir install. ```. Check version:. ```. [pichuan@pichuan-centos7 ~]$ singularity --version. singularity version 3.8.4. ```. ## Try running DeepVariant with Singularity. I followed Quick Start to get data. ```. singularity pull docker://google/deepvariant:1.4.0. # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_result",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:1588,modifiability,VERSION,VERSION,1588,"all Python 3.8.10. ```. sudo yum update. sudo yum install gcc openssl-devel bzip2-devel libffi-devel -y. sudo yum install -y wget. sudo yum install -y python3. ```. ```. wget https://www.python.org/ftp/python/3.8.10/Python-3.8.10.tgz. tar xvfz Python-3.8.10.tgz. ```. ```. cd Python-3.8.10. ./configure --enable-optimizations. ```. ```. sudo yum install -y make. sudo make altinstall. ```. ```. [pichuan@pichuan-centos7 Python-3.8.10]$ python3.8 --version. Python 3.8.10. ```. ## Install Singularity. ```. sudo yum update -y && \. sudo yum groupinstall -y 'Development Tools' && \. sudo yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools \. cryptsetup. ```. ```. export VERSION=1.14.12 OS=linux ARCH=amd64. # Downloads the required Go package. wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz. # Extracts the archive. sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz. # Deletes the ``tar`` file. rm go$VERSION.$OS-$ARCH.tar.gz. export VERSION=3.8.4. wget https://github.com/hpcng/singularity/releases/download/v${VERSION}/singularity-${VERSION}.tar.gz. tar -xzf singularity-${VERSION}.tar.gz. pushd singularity-3.8.4. export PATH=/usr/local/go/bin:$PATH. ./mconfig. make -C builddir. sudo make -C builddir install. ```. Check version:. ```. [pichuan@pichuan-centos7 ~]$ singularity --version. singularity version 3.8.4. ```. ## Try running DeepVariant with Singularity. I followed Quick Start to get data. ```. singularity pull docker://google/deepvariant:1.4.0. # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:1666,modifiability,VERSION,VERSION,1666,"ip2-devel libffi-devel -y. sudo yum install -y wget. sudo yum install -y python3. ```. ```. wget https://www.python.org/ftp/python/3.8.10/Python-3.8.10.tgz. tar xvfz Python-3.8.10.tgz. ```. ```. cd Python-3.8.10. ./configure --enable-optimizations. ```. ```. sudo yum install -y make. sudo make altinstall. ```. ```. [pichuan@pichuan-centos7 Python-3.8.10]$ python3.8 --version. Python 3.8.10. ```. ## Install Singularity. ```. sudo yum update -y && \. sudo yum groupinstall -y 'Development Tools' && \. sudo yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools \. cryptsetup. ```. ```. export VERSION=1.14.12 OS=linux ARCH=amd64. # Downloads the required Go package. wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz. # Extracts the archive. sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz. # Deletes the ``tar`` file. rm go$VERSION.$OS-$ARCH.tar.gz. export VERSION=3.8.4. wget https://github.com/hpcng/singularity/releases/download/v${VERSION}/singularity-${VERSION}.tar.gz. tar -xzf singularity-${VERSION}.tar.gz. pushd singularity-3.8.4. export PATH=/usr/local/go/bin:$PATH. ./mconfig. make -C builddir. sudo make -C builddir install. ```. Check version:. ```. [pichuan@pichuan-centos7 ~]$ singularity --version. singularity version 3.8.4. ```. ## Try running DeepVariant with Singularity. I followed Quick Start to get data. ```. singularity pull docker://google/deepvariant:1.4.0. # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=1. ```. This worked for me. Check numpy version:",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:1689,modifiability,VERSION,VERSION,1689,"-y. sudo yum install -y wget. sudo yum install -y python3. ```. ```. wget https://www.python.org/ftp/python/3.8.10/Python-3.8.10.tgz. tar xvfz Python-3.8.10.tgz. ```. ```. cd Python-3.8.10. ./configure --enable-optimizations. ```. ```. sudo yum install -y make. sudo make altinstall. ```. ```. [pichuan@pichuan-centos7 Python-3.8.10]$ python3.8 --version. Python 3.8.10. ```. ## Install Singularity. ```. sudo yum update -y && \. sudo yum groupinstall -y 'Development Tools' && \. sudo yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools \. cryptsetup. ```. ```. export VERSION=1.14.12 OS=linux ARCH=amd64. # Downloads the required Go package. wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz. # Extracts the archive. sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz. # Deletes the ``tar`` file. rm go$VERSION.$OS-$ARCH.tar.gz. export VERSION=3.8.4. wget https://github.com/hpcng/singularity/releases/download/v${VERSION}/singularity-${VERSION}.tar.gz. tar -xzf singularity-${VERSION}.tar.gz. pushd singularity-3.8.4. export PATH=/usr/local/go/bin:$PATH. ./mconfig. make -C builddir. sudo make -C builddir install. ```. Check version:. ```. [pichuan@pichuan-centos7 ~]$ singularity --version. singularity version 3.8.4. ```. ## Try running DeepVariant with Singularity. I followed Quick Start to get data. ```. singularity pull docker://google/deepvariant:1.4.0. # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=1. ```. This worked for me. Check numpy version:. ```. singularity run ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:1729,modifiability,VERSION,VERSION,1729,"nstall -y python3. ```. ```. wget https://www.python.org/ftp/python/3.8.10/Python-3.8.10.tgz. tar xvfz Python-3.8.10.tgz. ```. ```. cd Python-3.8.10. ./configure --enable-optimizations. ```. ```. sudo yum install -y make. sudo make altinstall. ```. ```. [pichuan@pichuan-centos7 Python-3.8.10]$ python3.8 --version. Python 3.8.10. ```. ## Install Singularity. ```. sudo yum update -y && \. sudo yum groupinstall -y 'Development Tools' && \. sudo yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools \. cryptsetup. ```. ```. export VERSION=1.14.12 OS=linux ARCH=amd64. # Downloads the required Go package. wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz. # Extracts the archive. sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz. # Deletes the ``tar`` file. rm go$VERSION.$OS-$ARCH.tar.gz. export VERSION=3.8.4. wget https://github.com/hpcng/singularity/releases/download/v${VERSION}/singularity-${VERSION}.tar.gz. tar -xzf singularity-${VERSION}.tar.gz. pushd singularity-3.8.4. export PATH=/usr/local/go/bin:$PATH. ./mconfig. make -C builddir. sudo make -C builddir install. ```. Check version:. ```. [pichuan@pichuan-centos7 ~]$ singularity --version. singularity version 3.8.4. ```. ## Try running DeepVariant with Singularity. I followed Quick Start to get data. ```. singularity pull docker://google/deepvariant:1.4.0. # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=1. ```. This worked for me. Check numpy version:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:1879,modifiability,version,version,1879,"./configure --enable-optimizations. ```. ```. sudo yum install -y make. sudo make altinstall. ```. ```. [pichuan@pichuan-centos7 Python-3.8.10]$ python3.8 --version. Python 3.8.10. ```. ## Install Singularity. ```. sudo yum update -y && \. sudo yum groupinstall -y 'Development Tools' && \. sudo yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools \. cryptsetup. ```. ```. export VERSION=1.14.12 OS=linux ARCH=amd64. # Downloads the required Go package. wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz. # Extracts the archive. sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz. # Deletes the ``tar`` file. rm go$VERSION.$OS-$ARCH.tar.gz. export VERSION=3.8.4. wget https://github.com/hpcng/singularity/releases/download/v${VERSION}/singularity-${VERSION}.tar.gz. tar -xzf singularity-${VERSION}.tar.gz. pushd singularity-3.8.4. export PATH=/usr/local/go/bin:$PATH. ./mconfig. make -C builddir. sudo make -C builddir install. ```. Check version:. ```. [pichuan@pichuan-centos7 ~]$ singularity --version. singularity version 3.8.4. ```. ## Try running DeepVariant with Singularity. I followed Quick Start to get data. ```. singularity pull docker://google/deepvariant:1.4.0. # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=1. ```. This worked for me. Check numpy version:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. python -c 'import numpy as np; print(np.version.version)'. ```. Which shows:. ```. 1.19.2. ```. It seems like my ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:1937,modifiability,version,version,1937,"tall -y make. sudo make altinstall. ```. ```. [pichuan@pichuan-centos7 Python-3.8.10]$ python3.8 --version. Python 3.8.10. ```. ## Install Singularity. ```. sudo yum update -y && \. sudo yum groupinstall -y 'Development Tools' && \. sudo yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools \. cryptsetup. ```. ```. export VERSION=1.14.12 OS=linux ARCH=amd64. # Downloads the required Go package. wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz. # Extracts the archive. sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz. # Deletes the ``tar`` file. rm go$VERSION.$OS-$ARCH.tar.gz. export VERSION=3.8.4. wget https://github.com/hpcng/singularity/releases/download/v${VERSION}/singularity-${VERSION}.tar.gz. tar -xzf singularity-${VERSION}.tar.gz. pushd singularity-3.8.4. export PATH=/usr/local/go/bin:$PATH. ./mconfig. make -C builddir. sudo make -C builddir install. ```. Check version:. ```. [pichuan@pichuan-centos7 ~]$ singularity --version. singularity version 3.8.4. ```. ## Try running DeepVariant with Singularity. I followed Quick Start to get data. ```. singularity pull docker://google/deepvariant:1.4.0. # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=1. ```. This worked for me. Check numpy version:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. python -c 'import numpy as np; print(np.version.version)'. ```. Which shows:. ```. 1.19.2. ```. It seems like my machine doesn't already have numpy, though:. ```. [pichuan",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:1958,modifiability,version,version,1958,"ke altinstall. ```. ```. [pichuan@pichuan-centos7 Python-3.8.10]$ python3.8 --version. Python 3.8.10. ```. ## Install Singularity. ```. sudo yum update -y && \. sudo yum groupinstall -y 'Development Tools' && \. sudo yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools \. cryptsetup. ```. ```. export VERSION=1.14.12 OS=linux ARCH=amd64. # Downloads the required Go package. wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz. # Extracts the archive. sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz. # Deletes the ``tar`` file. rm go$VERSION.$OS-$ARCH.tar.gz. export VERSION=3.8.4. wget https://github.com/hpcng/singularity/releases/download/v${VERSION}/singularity-${VERSION}.tar.gz. tar -xzf singularity-${VERSION}.tar.gz. pushd singularity-3.8.4. export PATH=/usr/local/go/bin:$PATH. ./mconfig. make -C builddir. sudo make -C builddir install. ```. Check version:. ```. [pichuan@pichuan-centos7 ~]$ singularity --version. singularity version 3.8.4. ```. ## Try running DeepVariant with Singularity. I followed Quick Start to get data. ```. singularity pull docker://google/deepvariant:1.4.0. # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=1. ```. This worked for me. Check numpy version:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. python -c 'import numpy as np; print(np.version.version)'. ```. Which shows:. ```. 1.19.2. ```. It seems like my machine doesn't already have numpy, though:. ```. [pichuan@pichuan-centos7 Pyth",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:2662,modifiability,version,version,2662,"VERSION}/singularity-${VERSION}.tar.gz. tar -xzf singularity-${VERSION}.tar.gz. pushd singularity-3.8.4. export PATH=/usr/local/go/bin:$PATH. ./mconfig. make -C builddir. sudo make -C builddir install. ```. Check version:. ```. [pichuan@pichuan-centos7 ~]$ singularity --version. singularity version 3.8.4. ```. ## Try running DeepVariant with Singularity. I followed Quick Start to get data. ```. singularity pull docker://google/deepvariant:1.4.0. # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=1. ```. This worked for me. Check numpy version:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. python -c 'import numpy as np; print(np.version.version)'. ```. Which shows:. ```. 1.19.2. ```. It seems like my machine doesn't already have numpy, though:. ```. [pichuan@pichuan-centos7 Python-3.8.10]$ pip3.8 show numpy. WARNING: Package(s) not found: numpy. ```. doesn't show anything. ## Install numpy 1.23.0 to see it breaks things. I ran:. ```. pip3.8 install numpy==1.23.0. ```. (Because you mentioned your cluster has 1.23.0). Now this shows:. ```. [pichuan@pichuan-centos7 ~]$ pip3.8 show numpy. Name: numpy. Version: 1.23.0. Summary: NumPy is the fundamental package for array computing with Python. Home-page: https://www.numpy.org. Author: Travis E. Oliphant et al. Author-email: None. License: BSD. Location: /home/pichuan/.local/lib/python3.8/site-packages. Requires: . Required-by: . ```. Then I re-ran:. ```. # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:2810,modifiability,version,version,2810,"fig. make -C builddir. sudo make -C builddir install. ```. Check version:. ```. [pichuan@pichuan-centos7 ~]$ singularity --version. singularity version 3.8.4. ```. ## Try running DeepVariant with Singularity. I followed Quick Start to get data. ```. singularity pull docker://google/deepvariant:1.4.0. # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=1. ```. This worked for me. Check numpy version:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. python -c 'import numpy as np; print(np.version.version)'. ```. Which shows:. ```. 1.19.2. ```. It seems like my machine doesn't already have numpy, though:. ```. [pichuan@pichuan-centos7 Python-3.8.10]$ pip3.8 show numpy. WARNING: Package(s) not found: numpy. ```. doesn't show anything. ## Install numpy 1.23.0 to see it breaks things. I ran:. ```. pip3.8 install numpy==1.23.0. ```. (Because you mentioned your cluster has 1.23.0). Now this shows:. ```. [pichuan@pichuan-centos7 ~]$ pip3.8 show numpy. Name: numpy. Version: 1.23.0. Summary: NumPy is the fundamental package for array computing with Python. Home-page: https://www.numpy.org. Author: Travis E. Oliphant et al. Author-email: None. License: BSD. Location: /home/pichuan/.local/lib/python3.8/site-packages. Requires: . Required-by: . ```. Then I re-ran:. ```. # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.f",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:2818,modifiability,version,version,2818,"e -C builddir. sudo make -C builddir install. ```. Check version:. ```. [pichuan@pichuan-centos7 ~]$ singularity --version. singularity version 3.8.4. ```. ## Try running DeepVariant with Singularity. I followed Quick Start to get data. ```. singularity pull docker://google/deepvariant:1.4.0. # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=1. ```. This worked for me. Check numpy version:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. python -c 'import numpy as np; print(np.version.version)'. ```. Which shows:. ```. 1.19.2. ```. It seems like my machine doesn't already have numpy, though:. ```. [pichuan@pichuan-centos7 Python-3.8.10]$ pip3.8 show numpy. WARNING: Package(s) not found: numpy. ```. doesn't show anything. ## Install numpy 1.23.0 to see it breaks things. I ran:. ```. pip3.8 install numpy==1.23.0. ```. (Because you mentioned your cluster has 1.23.0). Now this shows:. ```. [pichuan@pichuan-centos7 ~]$ pip3.8 show numpy. Name: numpy. Version: 1.23.0. Summary: NumPy is the fundamental package for array computing with Python. Home-page: https://www.numpy.org. Author: Travis E. Oliphant et al. Author-email: None. License: BSD. Location: /home/pichuan/.local/lib/python3.8/site-packages. Requires: . Required-by: . ```. Then I re-ran:. ```. # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:3002,modifiability,Pac,Package,3002,"ith Singularity. I followed Quick Start to get data. ```. singularity pull docker://google/deepvariant:1.4.0. # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=1. ```. This worked for me. Check numpy version:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. python -c 'import numpy as np; print(np.version.version)'. ```. Which shows:. ```. 1.19.2. ```. It seems like my machine doesn't already have numpy, though:. ```. [pichuan@pichuan-centos7 Python-3.8.10]$ pip3.8 show numpy. WARNING: Package(s) not found: numpy. ```. doesn't show anything. ## Install numpy 1.23.0 to see it breaks things. I ran:. ```. pip3.8 install numpy==1.23.0. ```. (Because you mentioned your cluster has 1.23.0). Now this shows:. ```. [pichuan@pichuan-centos7 ~]$ pip3.8 show numpy. Name: numpy. Version: 1.23.0. Summary: NumPy is the fundamental package for array computing with Python. Home-page: https://www.numpy.org. Author: Travis E. Oliphant et al. Author-email: None. License: BSD. Location: /home/pichuan/.local/lib/python3.8/site-packages. Requires: . Required-by: . ```. Then I re-ran:. ```. # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:3288,modifiability,Version,Version,3288,"f=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=1. ```. This worked for me. Check numpy version:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. python -c 'import numpy as np; print(np.version.version)'. ```. Which shows:. ```. 1.19.2. ```. It seems like my machine doesn't already have numpy, though:. ```. [pichuan@pichuan-centos7 Python-3.8.10]$ pip3.8 show numpy. WARNING: Package(s) not found: numpy. ```. doesn't show anything. ## Install numpy 1.23.0 to see it breaks things. I ran:. ```. pip3.8 install numpy==1.23.0. ```. (Because you mentioned your cluster has 1.23.0). Now this shows:. ```. [pichuan@pichuan-centos7 ~]$ pip3.8 show numpy. Name: numpy. Version: 1.23.0. Summary: NumPy is the fundamental package for array computing with Python. Home-page: https://www.numpy.org. Author: Travis E. Oliphant et al. Author-email: None. License: BSD. Location: /home/pichuan/.local/lib/python3.8/site-packages. Requires: . Required-by: . ```. Then I re-ran:. ```. # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=1. ```. Which still seems to work fine for me. And, this actually shows 1.23.0 as well:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. pyth",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:3339,modifiability,pac,package,3339,"--reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=1. ```. This worked for me. Check numpy version:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. python -c 'import numpy as np; print(np.version.version)'. ```. Which shows:. ```. 1.19.2. ```. It seems like my machine doesn't already have numpy, though:. ```. [pichuan@pichuan-centos7 Python-3.8.10]$ pip3.8 show numpy. WARNING: Package(s) not found: numpy. ```. doesn't show anything. ## Install numpy 1.23.0 to see it breaks things. I ran:. ```. pip3.8 install numpy==1.23.0. ```. (Because you mentioned your cluster has 1.23.0). Now this shows:. ```. [pichuan@pichuan-centos7 ~]$ pip3.8 show numpy. Name: numpy. Version: 1.23.0. Summary: NumPy is the fundamental package for array computing with Python. Home-page: https://www.numpy.org. Author: Travis E. Oliphant et al. Author-email: None. License: BSD. Location: /home/pichuan/.local/lib/python3.8/site-packages. Requires: . Required-by: . ```. Then I re-ran:. ```. # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=1. ```. Which still seems to work fine for me. And, this actually shows 1.23.0 as well:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. python -c 'import numpy as np; print(np.version.version",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:3532,modifiability,pac,packages,3532,". --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=1. ```. This worked for me. Check numpy version:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. python -c 'import numpy as np; print(np.version.version)'. ```. Which shows:. ```. 1.19.2. ```. It seems like my machine doesn't already have numpy, though:. ```. [pichuan@pichuan-centos7 Python-3.8.10]$ pip3.8 show numpy. WARNING: Package(s) not found: numpy. ```. doesn't show anything. ## Install numpy 1.23.0 to see it breaks things. I ran:. ```. pip3.8 install numpy==1.23.0. ```. (Because you mentioned your cluster has 1.23.0). Now this shows:. ```. [pichuan@pichuan-centos7 ~]$ pip3.8 show numpy. Name: numpy. Version: 1.23.0. Summary: NumPy is the fundamental package for array computing with Python. Home-page: https://www.numpy.org. Author: Travis E. Oliphant et al. Author-email: None. License: BSD. Location: /home/pichuan/.local/lib/python3.8/site-packages. Requires: . Required-by: . ```. Then I re-ran:. ```. # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=1. ```. Which still seems to work fine for me. And, this actually shows 1.23.0 as well:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. python -c 'import numpy as np; print(np.version.version)'. ```. ```. 1.23.0. ```. I also checked TensorFlow version:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. python -c 'import tensorflow; pr",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:4328,modifiability,version,version,4328,"lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. python -c 'import numpy as np; print(np.version.version)'. ```. Which shows:. ```. 1.19.2. ```. It seems like my machine doesn't already have numpy, though:. ```. [pichuan@pichuan-centos7 Python-3.8.10]$ pip3.8 show numpy. WARNING: Package(s) not found: numpy. ```. doesn't show anything. ## Install numpy 1.23.0 to see it breaks things. I ran:. ```. pip3.8 install numpy==1.23.0. ```. (Because you mentioned your cluster has 1.23.0). Now this shows:. ```. [pichuan@pichuan-centos7 ~]$ pip3.8 show numpy. Name: numpy. Version: 1.23.0. Summary: NumPy is the fundamental package for array computing with Python. Home-page: https://www.numpy.org. Author: Travis E. Oliphant et al. Author-email: None. License: BSD. Location: /home/pichuan/.local/lib/python3.8/site-packages. Requires: . Required-by: . ```. Then I re-ran:. ```. # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=1. ```. Which still seems to work fine for me. And, this actually shows 1.23.0 as well:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. python -c 'import numpy as np; print(np.version.version)'. ```. ```. 1.23.0. ```. I also checked TensorFlow version:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. python -c 'import tensorflow; print(tensorflow.__version__)'. ```. This shows:. ```. 2.7.0. ```. @asherrar Let me know what you think I need to change to try to reproduce the error you saw. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:4336,modifiability,version,version,4336,"lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. python -c 'import numpy as np; print(np.version.version)'. ```. Which shows:. ```. 1.19.2. ```. It seems like my machine doesn't already have numpy, though:. ```. [pichuan@pichuan-centos7 Python-3.8.10]$ pip3.8 show numpy. WARNING: Package(s) not found: numpy. ```. doesn't show anything. ## Install numpy 1.23.0 to see it breaks things. I ran:. ```. pip3.8 install numpy==1.23.0. ```. (Because you mentioned your cluster has 1.23.0). Now this shows:. ```. [pichuan@pichuan-centos7 ~]$ pip3.8 show numpy. Name: numpy. Version: 1.23.0. Summary: NumPy is the fundamental package for array computing with Python. Home-page: https://www.numpy.org. Author: Travis E. Oliphant et al. Author-email: None. License: BSD. Location: /home/pichuan/.local/lib/python3.8/site-packages. Requires: . Required-by: . ```. Then I re-ran:. ```. # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=1. ```. Which still seems to work fine for me. And, this actually shows 1.23.0 as well:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. python -c 'import numpy as np; print(np.version.version)'. ```. ```. 1.23.0. ```. I also checked TensorFlow version:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. python -c 'import tensorflow; print(tensorflow.__version__)'. ```. This shows:. ```. 2.7.0. ```. @asherrar Let me know what you think I need to change to try to reproduce the error you saw. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:4396,modifiability,version,version,4396,"lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. python -c 'import numpy as np; print(np.version.version)'. ```. Which shows:. ```. 1.19.2. ```. It seems like my machine doesn't already have numpy, though:. ```. [pichuan@pichuan-centos7 Python-3.8.10]$ pip3.8 show numpy. WARNING: Package(s) not found: numpy. ```. doesn't show anything. ## Install numpy 1.23.0 to see it breaks things. I ran:. ```. pip3.8 install numpy==1.23.0. ```. (Because you mentioned your cluster has 1.23.0). Now this shows:. ```. [pichuan@pichuan-centos7 ~]$ pip3.8 show numpy. Name: numpy. Version: 1.23.0. Summary: NumPy is the fundamental package for array computing with Python. Home-page: https://www.numpy.org. Author: Travis E. Oliphant et al. Author-email: None. License: BSD. Location: /home/pichuan/.local/lib/python3.8/site-packages. Requires: . Required-by: . ```. Then I re-ran:. ```. # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=1. ```. Which still seems to work fine for me. And, this actually shows 1.23.0 as well:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. python -c 'import numpy as np; print(np.version.version)'. ```. ```. 1.23.0. ```. I also checked TensorFlow version:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. python -c 'import tensorflow; print(tensorflow.__version__)'. ```. This shows:. ```. 2.7.0. ```. @asherrar Let me know what you think I need to change to try to reproduce the error you saw. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:904,performance,optimiz,optimizations,904,"@asherrar Unfortunately, I'm unable to reproduce what you're seeing. I'm going to write down what I did. Maybe you can spot what differences we have:. Create a CentOS7 machine to test:. ```. gcloud compute instances create ""${USER}-centos7"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-7"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --zone ""us-west1-b"". gcloud compute ssh pichuan-centos7 --zone ""us-west1-b"". ```. By default the machine has Python2. ```. [pichuan@pichuan-centos7 ~]$ python --version . Python 2.7.5. ```. ## Install Python 3.8.10. ```. sudo yum update. sudo yum install gcc openssl-devel bzip2-devel libffi-devel -y. sudo yum install -y wget. sudo yum install -y python3. ```. ```. wget https://www.python.org/ftp/python/3.8.10/Python-3.8.10.tgz. tar xvfz Python-3.8.10.tgz. ```. ```. cd Python-3.8.10. ./configure --enable-optimizations. ```. ```. sudo yum install -y make. sudo make altinstall. ```. ```. [pichuan@pichuan-centos7 Python-3.8.10]$ python3.8 --version. Python 3.8.10. ```. ## Install Singularity. ```. sudo yum update -y && \. sudo yum groupinstall -y 'Development Tools' && \. sudo yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools \. cryptsetup. ```. ```. export VERSION=1.14.12 OS=linux ARCH=amd64. # Downloads the required Go package. wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz. # Extracts the archive. sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz. # Deletes the ``tar`` file. rm go$VERSION.$OS-$ARCH.tar.gz. export VERSION=3.8.4. wget https://github.com/hpcng/singularity/releases/download/v${VERSION}/singularity-${VERSION}.tar.gz. tar -xzf singularity-${VERSION}.tar.gz. pushd singularity-3.8.4. export PATH=/usr/local/go/bin:$PATH. ./mconfig. make -C builddir. sudo make -C builddir install. ```. Check version:. ```. [pichuan@pichuan-centos7 ~]$ singularity --version. singularity version 3.8.4. ```. ## Try running DeepVar",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:4679,performance,error,error,4679,"lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. python -c 'import numpy as np; print(np.version.version)'. ```. Which shows:. ```. 1.19.2. ```. It seems like my machine doesn't already have numpy, though:. ```. [pichuan@pichuan-centos7 Python-3.8.10]$ pip3.8 show numpy. WARNING: Package(s) not found: numpy. ```. doesn't show anything. ## Install numpy 1.23.0 to see it breaks things. I ran:. ```. pip3.8 install numpy==1.23.0. ```. (Because you mentioned your cluster has 1.23.0). Now this shows:. ```. [pichuan@pichuan-centos7 ~]$ pip3.8 show numpy. Name: numpy. Version: 1.23.0. Summary: NumPy is the fundamental package for array computing with Python. Home-page: https://www.numpy.org. Author: Travis E. Oliphant et al. Author-email: None. License: BSD. Location: /home/pichuan/.local/lib/python3.8/site-packages. Requires: . Required-by: . ```. Then I re-ran:. ```. # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=1. ```. Which still seems to work fine for me. And, this actually shows 1.23.0 as well:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. python -c 'import numpy as np; print(np.version.version)'. ```. ```. 1.23.0. ```. I also checked TensorFlow version:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. python -c 'import tensorflow; print(tensorflow.__version__)'. ```. This shows:. ```. 2.7.0. ```. @asherrar Let me know what you think I need to change to try to reproduce the error you saw. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:2891,reliability,doe,doesn,2891,"[pichuan@pichuan-centos7 ~]$ singularity --version. singularity version 3.8.4. ```. ## Try running DeepVariant with Singularity. I followed Quick Start to get data. ```. singularity pull docker://google/deepvariant:1.4.0. # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=1. ```. This worked for me. Check numpy version:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. python -c 'import numpy as np; print(np.version.version)'. ```. Which shows:. ```. 1.19.2. ```. It seems like my machine doesn't already have numpy, though:. ```. [pichuan@pichuan-centos7 Python-3.8.10]$ pip3.8 show numpy. WARNING: Package(s) not found: numpy. ```. doesn't show anything. ## Install numpy 1.23.0 to see it breaks things. I ran:. ```. pip3.8 install numpy==1.23.0. ```. (Because you mentioned your cluster has 1.23.0). Now this shows:. ```. [pichuan@pichuan-centos7 ~]$ pip3.8 show numpy. Name: numpy. Version: 1.23.0. Summary: NumPy is the fundamental package for array computing with Python. Home-page: https://www.numpy.org. Author: Travis E. Oliphant et al. Author-email: None. License: BSD. Location: /home/pichuan/.local/lib/python3.8/site-packages. Requires: . Required-by: . ```. Then I re-ran:. ```. # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr2",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:3036,reliability,doe,doesn,3036," Start to get data. ```. singularity pull docker://google/deepvariant:1.4.0. # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=1. ```. This worked for me. Check numpy version:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. python -c 'import numpy as np; print(np.version.version)'. ```. Which shows:. ```. 1.19.2. ```. It seems like my machine doesn't already have numpy, though:. ```. [pichuan@pichuan-centos7 Python-3.8.10]$ pip3.8 show numpy. WARNING: Package(s) not found: numpy. ```. doesn't show anything. ## Install numpy 1.23.0 to see it breaks things. I ran:. ```. pip3.8 install numpy==1.23.0. ```. (Because you mentioned your cluster has 1.23.0). Now this shows:. ```. [pichuan@pichuan-centos7 ~]$ pip3.8 show numpy. Name: numpy. Version: 1.23.0. Summary: NumPy is the fundamental package for array computing with Python. Home-page: https://www.numpy.org. Author: Travis E. Oliphant et al. Author-email: None. License: BSD. Location: /home/pichuan/.local/lib/python3.8/site-packages. Requires: . Required-by: . ```. Then I re-ran:. ```. # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:179,safety,test,test,179,"@asherrar Unfortunately, I'm unable to reproduce what you're seeing. I'm going to write down what I did. Maybe you can spot what differences we have:. Create a CentOS7 machine to test:. ```. gcloud compute instances create ""${USER}-centos7"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-7"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --zone ""us-west1-b"". gcloud compute ssh pichuan-centos7 --zone ""us-west1-b"". ```. By default the machine has Python2. ```. [pichuan@pichuan-centos7 ~]$ python --version . Python 2.7.5. ```. ## Install Python 3.8.10. ```. sudo yum update. sudo yum install gcc openssl-devel bzip2-devel libffi-devel -y. sudo yum install -y wget. sudo yum install -y python3. ```. ```. wget https://www.python.org/ftp/python/3.8.10/Python-3.8.10.tgz. tar xvfz Python-3.8.10.tgz. ```. ```. cd Python-3.8.10. ./configure --enable-optimizations. ```. ```. sudo yum install -y make. sudo make altinstall. ```. ```. [pichuan@pichuan-centos7 Python-3.8.10]$ python3.8 --version. Python 3.8.10. ```. ## Install Singularity. ```. sudo yum update -y && \. sudo yum groupinstall -y 'Development Tools' && \. sudo yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools \. cryptsetup. ```. ```. export VERSION=1.14.12 OS=linux ARCH=amd64. # Downloads the required Go package. wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz. # Extracts the archive. sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz. # Deletes the ``tar`` file. rm go$VERSION.$OS-$ARCH.tar.gz. export VERSION=3.8.4. wget https://github.com/hpcng/singularity/releases/download/v${VERSION}/singularity-${VERSION}.tar.gz. tar -xzf singularity-${VERSION}.tar.gz. pushd singularity-3.8.4. export PATH=/usr/local/go/bin:$PATH. ./mconfig. make -C builddir. sudo make -C builddir install. ```. Check version:. ```. [pichuan@pichuan-centos7 ~]$ singularity --version. singularity version 3.8.4. ```. ## Try running DeepVar",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:625,safety,updat,update,625,"@asherrar Unfortunately, I'm unable to reproduce what you're seeing. I'm going to write down what I did. Maybe you can spot what differences we have:. Create a CentOS7 machine to test:. ```. gcloud compute instances create ""${USER}-centos7"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-7"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --zone ""us-west1-b"". gcloud compute ssh pichuan-centos7 --zone ""us-west1-b"". ```. By default the machine has Python2. ```. [pichuan@pichuan-centos7 ~]$ python --version . Python 2.7.5. ```. ## Install Python 3.8.10. ```. sudo yum update. sudo yum install gcc openssl-devel bzip2-devel libffi-devel -y. sudo yum install -y wget. sudo yum install -y python3. ```. ```. wget https://www.python.org/ftp/python/3.8.10/Python-3.8.10.tgz. tar xvfz Python-3.8.10.tgz. ```. ```. cd Python-3.8.10. ./configure --enable-optimizations. ```. ```. sudo yum install -y make. sudo make altinstall. ```. ```. [pichuan@pichuan-centos7 Python-3.8.10]$ python3.8 --version. Python 3.8.10. ```. ## Install Singularity. ```. sudo yum update -y && \. sudo yum groupinstall -y 'Development Tools' && \. sudo yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools \. cryptsetup. ```. ```. export VERSION=1.14.12 OS=linux ARCH=amd64. # Downloads the required Go package. wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz. # Extracts the archive. sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz. # Deletes the ``tar`` file. rm go$VERSION.$OS-$ARCH.tar.gz. export VERSION=3.8.4. wget https://github.com/hpcng/singularity/releases/download/v${VERSION}/singularity-${VERSION}.tar.gz. tar -xzf singularity-${VERSION}.tar.gz. pushd singularity-3.8.4. export PATH=/usr/local/go/bin:$PATH. ./mconfig. make -C builddir. sudo make -C builddir install. ```. Check version:. ```. [pichuan@pichuan-centos7 ~]$ singularity --version. singularity version 3.8.4. ```. ## Try running DeepVar",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:1107,safety,updat,update,1107," you can spot what differences we have:. Create a CentOS7 machine to test:. ```. gcloud compute instances create ""${USER}-centos7"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-7"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --zone ""us-west1-b"". gcloud compute ssh pichuan-centos7 --zone ""us-west1-b"". ```. By default the machine has Python2. ```. [pichuan@pichuan-centos7 ~]$ python --version . Python 2.7.5. ```. ## Install Python 3.8.10. ```. sudo yum update. sudo yum install gcc openssl-devel bzip2-devel libffi-devel -y. sudo yum install -y wget. sudo yum install -y python3. ```. ```. wget https://www.python.org/ftp/python/3.8.10/Python-3.8.10.tgz. tar xvfz Python-3.8.10.tgz. ```. ```. cd Python-3.8.10. ./configure --enable-optimizations. ```. ```. sudo yum install -y make. sudo make altinstall. ```. ```. [pichuan@pichuan-centos7 Python-3.8.10]$ python3.8 --version. Python 3.8.10. ```. ## Install Singularity. ```. sudo yum update -y && \. sudo yum groupinstall -y 'Development Tools' && \. sudo yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools \. cryptsetup. ```. ```. export VERSION=1.14.12 OS=linux ARCH=amd64. # Downloads the required Go package. wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz. # Extracts the archive. sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz. # Deletes the ``tar`` file. rm go$VERSION.$OS-$ARCH.tar.gz. export VERSION=3.8.4. wget https://github.com/hpcng/singularity/releases/download/v${VERSION}/singularity-${VERSION}.tar.gz. tar -xzf singularity-${VERSION}.tar.gz. pushd singularity-3.8.4. export PATH=/usr/local/go/bin:$PATH. ./mconfig. make -C builddir. sudo make -C builddir install. ```. Check version:. ```. [pichuan@pichuan-centos7 ~]$ singularity --version. singularity version 3.8.4. ```. ## Try running DeepVariant with Singularity. I followed Quick Start to get data. ```. singularity pull docker://google/deepvariant:1",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:4679,safety,error,error,4679,"lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. python -c 'import numpy as np; print(np.version.version)'. ```. Which shows:. ```. 1.19.2. ```. It seems like my machine doesn't already have numpy, though:. ```. [pichuan@pichuan-centos7 Python-3.8.10]$ pip3.8 show numpy. WARNING: Package(s) not found: numpy. ```. doesn't show anything. ## Install numpy 1.23.0 to see it breaks things. I ran:. ```. pip3.8 install numpy==1.23.0. ```. (Because you mentioned your cluster has 1.23.0). Now this shows:. ```. [pichuan@pichuan-centos7 ~]$ pip3.8 show numpy. Name: numpy. Version: 1.23.0. Summary: NumPy is the fundamental package for array computing with Python. Home-page: https://www.numpy.org. Author: Travis E. Oliphant et al. Author-email: None. License: BSD. Location: /home/pichuan/.local/lib/python3.8/site-packages. Requires: . Required-by: . ```. Then I re-ran:. ```. # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=1. ```. Which still seems to work fine for me. And, this actually shows 1.23.0 as well:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. python -c 'import numpy as np; print(np.version.version)'. ```. ```. 1.23.0. ```. I also checked TensorFlow version:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. python -c 'import tensorflow; print(tensorflow.__version__)'. ```. This shows:. ```. 2.7.0. ```. @asherrar Let me know what you think I need to change to try to reproduce the error you saw. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:431,security,ssh,ssh,431,"@asherrar Unfortunately, I'm unable to reproduce what you're seeing. I'm going to write down what I did. Maybe you can spot what differences we have:. Create a CentOS7 machine to test:. ```. gcloud compute instances create ""${USER}-centos7"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-7"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --zone ""us-west1-b"". gcloud compute ssh pichuan-centos7 --zone ""us-west1-b"". ```. By default the machine has Python2. ```. [pichuan@pichuan-centos7 ~]$ python --version . Python 2.7.5. ```. ## Install Python 3.8.10. ```. sudo yum update. sudo yum install gcc openssl-devel bzip2-devel libffi-devel -y. sudo yum install -y wget. sudo yum install -y python3. ```. ```. wget https://www.python.org/ftp/python/3.8.10/Python-3.8.10.tgz. tar xvfz Python-3.8.10.tgz. ```. ```. cd Python-3.8.10. ./configure --enable-optimizations. ```. ```. sudo yum install -y make. sudo make altinstall. ```. ```. [pichuan@pichuan-centos7 Python-3.8.10]$ python3.8 --version. Python 3.8.10. ```. ## Install Singularity. ```. sudo yum update -y && \. sudo yum groupinstall -y 'Development Tools' && \. sudo yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools \. cryptsetup. ```. ```. export VERSION=1.14.12 OS=linux ARCH=amd64. # Downloads the required Go package. wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz. # Extracts the archive. sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz. # Deletes the ``tar`` file. rm go$VERSION.$OS-$ARCH.tar.gz. export VERSION=3.8.4. wget https://github.com/hpcng/singularity/releases/download/v${VERSION}/singularity-${VERSION}.tar.gz. tar -xzf singularity-${VERSION}.tar.gz. pushd singularity-3.8.4. export PATH=/usr/local/go/bin:$PATH. ./mconfig. make -C builddir. sudo make -C builddir install. ```. Check version:. ```. [pichuan@pichuan-centos7 ~]$ singularity --version. singularity version 3.8.4. ```. ## Try running DeepVar",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:625,security,updat,update,625,"@asherrar Unfortunately, I'm unable to reproduce what you're seeing. I'm going to write down what I did. Maybe you can spot what differences we have:. Create a CentOS7 machine to test:. ```. gcloud compute instances create ""${USER}-centos7"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-7"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --zone ""us-west1-b"". gcloud compute ssh pichuan-centos7 --zone ""us-west1-b"". ```. By default the machine has Python2. ```. [pichuan@pichuan-centos7 ~]$ python --version . Python 2.7.5. ```. ## Install Python 3.8.10. ```. sudo yum update. sudo yum install gcc openssl-devel bzip2-devel libffi-devel -y. sudo yum install -y wget. sudo yum install -y python3. ```. ```. wget https://www.python.org/ftp/python/3.8.10/Python-3.8.10.tgz. tar xvfz Python-3.8.10.tgz. ```. ```. cd Python-3.8.10. ./configure --enable-optimizations. ```. ```. sudo yum install -y make. sudo make altinstall. ```. ```. [pichuan@pichuan-centos7 Python-3.8.10]$ python3.8 --version. Python 3.8.10. ```. ## Install Singularity. ```. sudo yum update -y && \. sudo yum groupinstall -y 'Development Tools' && \. sudo yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools \. cryptsetup. ```. ```. export VERSION=1.14.12 OS=linux ARCH=amd64. # Downloads the required Go package. wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz. # Extracts the archive. sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz. # Deletes the ``tar`` file. rm go$VERSION.$OS-$ARCH.tar.gz. export VERSION=3.8.4. wget https://github.com/hpcng/singularity/releases/download/v${VERSION}/singularity-${VERSION}.tar.gz. tar -xzf singularity-${VERSION}.tar.gz. pushd singularity-3.8.4. export PATH=/usr/local/go/bin:$PATH. ./mconfig. make -C builddir. sudo make -C builddir install. ```. Check version:. ```. [pichuan@pichuan-centos7 ~]$ singularity --version. singularity version 3.8.4. ```. ## Try running DeepVar",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:885,security,configur,configure,885,"@asherrar Unfortunately, I'm unable to reproduce what you're seeing. I'm going to write down what I did. Maybe you can spot what differences we have:. Create a CentOS7 machine to test:. ```. gcloud compute instances create ""${USER}-centos7"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-7"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --zone ""us-west1-b"". gcloud compute ssh pichuan-centos7 --zone ""us-west1-b"". ```. By default the machine has Python2. ```. [pichuan@pichuan-centos7 ~]$ python --version . Python 2.7.5. ```. ## Install Python 3.8.10. ```. sudo yum update. sudo yum install gcc openssl-devel bzip2-devel libffi-devel -y. sudo yum install -y wget. sudo yum install -y python3. ```. ```. wget https://www.python.org/ftp/python/3.8.10/Python-3.8.10.tgz. tar xvfz Python-3.8.10.tgz. ```. ```. cd Python-3.8.10. ./configure --enable-optimizations. ```. ```. sudo yum install -y make. sudo make altinstall. ```. ```. [pichuan@pichuan-centos7 Python-3.8.10]$ python3.8 --version. Python 3.8.10. ```. ## Install Singularity. ```. sudo yum update -y && \. sudo yum groupinstall -y 'Development Tools' && \. sudo yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools \. cryptsetup. ```. ```. export VERSION=1.14.12 OS=linux ARCH=amd64. # Downloads the required Go package. wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz. # Extracts the archive. sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz. # Deletes the ``tar`` file. rm go$VERSION.$OS-$ARCH.tar.gz. export VERSION=3.8.4. wget https://github.com/hpcng/singularity/releases/download/v${VERSION}/singularity-${VERSION}.tar.gz. tar -xzf singularity-${VERSION}.tar.gz. pushd singularity-3.8.4. export PATH=/usr/local/go/bin:$PATH. ./mconfig. make -C builddir. sudo make -C builddir install. ```. Check version:. ```. [pichuan@pichuan-centos7 ~]$ singularity --version. singularity version 3.8.4. ```. ## Try running DeepVar",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:1107,security,updat,update,1107," you can spot what differences we have:. Create a CentOS7 machine to test:. ```. gcloud compute instances create ""${USER}-centos7"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-7"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --zone ""us-west1-b"". gcloud compute ssh pichuan-centos7 --zone ""us-west1-b"". ```. By default the machine has Python2. ```. [pichuan@pichuan-centos7 ~]$ python --version . Python 2.7.5. ```. ## Install Python 3.8.10. ```. sudo yum update. sudo yum install gcc openssl-devel bzip2-devel libffi-devel -y. sudo yum install -y wget. sudo yum install -y python3. ```. ```. wget https://www.python.org/ftp/python/3.8.10/Python-3.8.10.tgz. tar xvfz Python-3.8.10.tgz. ```. ```. cd Python-3.8.10. ./configure --enable-optimizations. ```. ```. sudo yum install -y make. sudo make altinstall. ```. ```. [pichuan@pichuan-centos7 Python-3.8.10]$ python3.8 --version. Python 3.8.10. ```. ## Install Singularity. ```. sudo yum update -y && \. sudo yum groupinstall -y 'Development Tools' && \. sudo yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools \. cryptsetup. ```. ```. export VERSION=1.14.12 OS=linux ARCH=amd64. # Downloads the required Go package. wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz. # Extracts the archive. sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz. # Deletes the ``tar`` file. rm go$VERSION.$OS-$ARCH.tar.gz. export VERSION=3.8.4. wget https://github.com/hpcng/singularity/releases/download/v${VERSION}/singularity-${VERSION}.tar.gz. tar -xzf singularity-${VERSION}.tar.gz. pushd singularity-3.8.4. export PATH=/usr/local/go/bin:$PATH. ./mconfig. make -C builddir. sudo make -C builddir install. ```. Check version:. ```. [pichuan@pichuan-centos7 ~]$ singularity --version. singularity version 3.8.4. ```. ## Try running DeepVariant with Singularity. I followed Quick Start to get data. ```. singularity pull docker://google/deepvariant:1",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:3414,security,Auth,Author,3414,"10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=1. ```. This worked for me. Check numpy version:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. python -c 'import numpy as np; print(np.version.version)'. ```. Which shows:. ```. 1.19.2. ```. It seems like my machine doesn't already have numpy, though:. ```. [pichuan@pichuan-centos7 Python-3.8.10]$ pip3.8 show numpy. WARNING: Package(s) not found: numpy. ```. doesn't show anything. ## Install numpy 1.23.0 to see it breaks things. I ran:. ```. pip3.8 install numpy==1.23.0. ```. (Because you mentioned your cluster has 1.23.0). Now this shows:. ```. [pichuan@pichuan-centos7 ~]$ pip3.8 show numpy. Name: numpy. Version: 1.23.0. Summary: NumPy is the fundamental package for array computing with Python. Home-page: https://www.numpy.org. Author: Travis E. Oliphant et al. Author-email: None. License: BSD. Location: /home/pichuan/.local/lib/python3.8/site-packages. Requires: . Required-by: . ```. Then I re-ran:. ```. # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=1. ```. Which still seems to work fine for me. And, this actually shows 1.23.0 as well:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. python -c 'import numpy as np; print(np.version.version)'. ```. ```. 1.23.0. ```. I also checked TensorFlow version:. ```. singul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:3448,security,Auth,Author-email,3448,"f=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=1. ```. This worked for me. Check numpy version:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. python -c 'import numpy as np; print(np.version.version)'. ```. Which shows:. ```. 1.19.2. ```. It seems like my machine doesn't already have numpy, though:. ```. [pichuan@pichuan-centos7 Python-3.8.10]$ pip3.8 show numpy. WARNING: Package(s) not found: numpy. ```. doesn't show anything. ## Install numpy 1.23.0 to see it breaks things. I ran:. ```. pip3.8 install numpy==1.23.0. ```. (Because you mentioned your cluster has 1.23.0). Now this shows:. ```. [pichuan@pichuan-centos7 ~]$ pip3.8 show numpy. Name: numpy. Version: 1.23.0. Summary: NumPy is the fundamental package for array computing with Python. Home-page: https://www.numpy.org. Author: Travis E. Oliphant et al. Author-email: None. License: BSD. Location: /home/pichuan/.local/lib/python3.8/site-packages. Requires: . Required-by: . ```. Then I re-ran:. ```. # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=1. ```. Which still seems to work fine for me. And, this actually shows 1.23.0 as well:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. python -c 'import numpy as np; print(np.version.version)'. ```. ```. 1.23.0. ```. I also checked TensorFlow version:. ```. singularity run -B /usr/lib/locale/:/usr/li",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:179,testability,test,test,179,"@asherrar Unfortunately, I'm unable to reproduce what you're seeing. I'm going to write down what I did. Maybe you can spot what differences we have:. Create a CentOS7 machine to test:. ```. gcloud compute instances create ""${USER}-centos7"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-7"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --zone ""us-west1-b"". gcloud compute ssh pichuan-centos7 --zone ""us-west1-b"". ```. By default the machine has Python2. ```. [pichuan@pichuan-centos7 ~]$ python --version . Python 2.7.5. ```. ## Install Python 3.8.10. ```. sudo yum update. sudo yum install gcc openssl-devel bzip2-devel libffi-devel -y. sudo yum install -y wget. sudo yum install -y python3. ```. ```. wget https://www.python.org/ftp/python/3.8.10/Python-3.8.10.tgz. tar xvfz Python-3.8.10.tgz. ```. ```. cd Python-3.8.10. ./configure --enable-optimizations. ```. ```. sudo yum install -y make. sudo make altinstall. ```. ```. [pichuan@pichuan-centos7 Python-3.8.10]$ python3.8 --version. Python 3.8.10. ```. ## Install Singularity. ```. sudo yum update -y && \. sudo yum groupinstall -y 'Development Tools' && \. sudo yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools \. cryptsetup. ```. ```. export VERSION=1.14.12 OS=linux ARCH=amd64. # Downloads the required Go package. wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz. # Extracts the archive. sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz. # Deletes the ``tar`` file. rm go$VERSION.$OS-$ARCH.tar.gz. export VERSION=3.8.4. wget https://github.com/hpcng/singularity/releases/download/v${VERSION}/singularity-${VERSION}.tar.gz. tar -xzf singularity-${VERSION}.tar.gz. pushd singularity-3.8.4. export PATH=/usr/local/go/bin:$PATH. ./mconfig. make -C builddir. sudo make -C builddir install. ```. Check version:. ```. [pichuan@pichuan-centos7 ~]$ singularity --version. singularity version 3.8.4. ```. ## Try running DeepVar",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:2325,testability,unit,unittest,2325,"x ARCH=amd64. # Downloads the required Go package. wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz. # Extracts the archive. sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz. # Deletes the ``tar`` file. rm go$VERSION.$OS-$ARCH.tar.gz. export VERSION=3.8.4. wget https://github.com/hpcng/singularity/releases/download/v${VERSION}/singularity-${VERSION}.tar.gz. tar -xzf singularity-${VERSION}.tar.gz. pushd singularity-3.8.4. export PATH=/usr/local/go/bin:$PATH. ./mconfig. make -C builddir. sudo make -C builddir install. ```. Check version:. ```. [pichuan@pichuan-centos7 ~]$ singularity --version. singularity version 3.8.4. ```. ## Try running DeepVariant with Singularity. I followed Quick Start to get data. ```. singularity pull docker://google/deepvariant:1.4.0. # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=1. ```. This worked for me. Check numpy version:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. python -c 'import numpy as np; print(np.version.version)'. ```. Which shows:. ```. 1.19.2. ```. It seems like my machine doesn't already have numpy, though:. ```. [pichuan@pichuan-centos7 Python-3.8.10]$ pip3.8 show numpy. WARNING: Package(s) not found: numpy. ```. doesn't show anything. ## Install numpy 1.23.0 to see it breaks things. I ran:. ```. pip3.8 install numpy==1.23.0. ```. (Because you mentioned your cluster has 1.23.0). Now this shows:. ```. [pichuan@pichuan-centos7 ~]$ pip3.8 show numpy. Name: numpy. Version: 1.23.0. Summary: NumPy is the fu",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:3804,testability,unit,unittest,3804,"lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. python -c 'import numpy as np; print(np.version.version)'. ```. Which shows:. ```. 1.19.2. ```. It seems like my machine doesn't already have numpy, though:. ```. [pichuan@pichuan-centos7 Python-3.8.10]$ pip3.8 show numpy. WARNING: Package(s) not found: numpy. ```. doesn't show anything. ## Install numpy 1.23.0 to see it breaks things. I ran:. ```. pip3.8 install numpy==1.23.0. ```. (Because you mentioned your cluster has 1.23.0). Now this shows:. ```. [pichuan@pichuan-centos7 ~]$ pip3.8 show numpy. Name: numpy. Version: 1.23.0. Summary: NumPy is the fundamental package for array computing with Python. Home-page: https://www.numpy.org. Author: Travis E. Oliphant et al. Author-email: None. License: BSD. Location: /home/pichuan/.local/lib/python3.8/site-packages. Requires: . Required-by: . ```. Then I re-ran:. ```. # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=1. ```. Which still seems to work fine for me. And, this actually shows 1.23.0 as well:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. python -c 'import numpy as np; print(np.version.version)'. ```. ```. 1.23.0. ```. I also checked TensorFlow version:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. python -c 'import tensorflow; print(tensorflow.__version__)'. ```. This shows:. ```. 2.7.0. ```. @asherrar Let me know what you think I need to change to try to reproduce the error you saw. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:226,usability,USER,USER,226,"@asherrar Unfortunately, I'm unable to reproduce what you're seeing. I'm going to write down what I did. Maybe you can spot what differences we have:. Create a CentOS7 machine to test:. ```. gcloud compute instances create ""${USER}-centos7"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-7"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --zone ""us-west1-b"". gcloud compute ssh pichuan-centos7 --zone ""us-west1-b"". ```. By default the machine has Python2. ```. [pichuan@pichuan-centos7 ~]$ python --version . Python 2.7.5. ```. ## Install Python 3.8.10. ```. sudo yum update. sudo yum install gcc openssl-devel bzip2-devel libffi-devel -y. sudo yum install -y wget. sudo yum install -y python3. ```. ```. wget https://www.python.org/ftp/python/3.8.10/Python-3.8.10.tgz. tar xvfz Python-3.8.10.tgz. ```. ```. cd Python-3.8.10. ./configure --enable-optimizations. ```. ```. sudo yum install -y make. sudo make altinstall. ```. ```. [pichuan@pichuan-centos7 Python-3.8.10]$ python3.8 --version. Python 3.8.10. ```. ## Install Singularity. ```. sudo yum update -y && \. sudo yum groupinstall -y 'Development Tools' && \. sudo yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools \. cryptsetup. ```. ```. export VERSION=1.14.12 OS=linux ARCH=amd64. # Downloads the required Go package. wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz. # Extracts the archive. sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz. # Deletes the ``tar`` file. rm go$VERSION.$OS-$ARCH.tar.gz. export VERSION=3.8.4. wget https://github.com/hpcng/singularity/releases/download/v${VERSION}/singularity-${VERSION}.tar.gz. tar -xzf singularity-${VERSION}.tar.gz. pushd singularity-3.8.4. export PATH=/usr/local/go/bin:$PATH. ./mconfig. make -C builddir. sudo make -C builddir install. ```. Check version:. ```. [pichuan@pichuan-centos7 ~]$ singularity --version. singularity version 3.8.4. ```. ## Try running DeepVar",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:1161,usability,Tool,Tools,1161,"OS7 machine to test:. ```. gcloud compute instances create ""${USER}-centos7"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-7"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --zone ""us-west1-b"". gcloud compute ssh pichuan-centos7 --zone ""us-west1-b"". ```. By default the machine has Python2. ```. [pichuan@pichuan-centos7 ~]$ python --version . Python 2.7.5. ```. ## Install Python 3.8.10. ```. sudo yum update. sudo yum install gcc openssl-devel bzip2-devel libffi-devel -y. sudo yum install -y wget. sudo yum install -y python3. ```. ```. wget https://www.python.org/ftp/python/3.8.10/Python-3.8.10.tgz. tar xvfz Python-3.8.10.tgz. ```. ```. cd Python-3.8.10. ./configure --enable-optimizations. ```. ```. sudo yum install -y make. sudo make altinstall. ```. ```. [pichuan@pichuan-centos7 Python-3.8.10]$ python3.8 --version. Python 3.8.10. ```. ## Install Singularity. ```. sudo yum update -y && \. sudo yum groupinstall -y 'Development Tools' && \. sudo yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools \. cryptsetup. ```. ```. export VERSION=1.14.12 OS=linux ARCH=amd64. # Downloads the required Go package. wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz. # Extracts the archive. sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz. # Deletes the ``tar`` file. rm go$VERSION.$OS-$ARCH.tar.gz. export VERSION=3.8.4. wget https://github.com/hpcng/singularity/releases/download/v${VERSION}/singularity-${VERSION}.tar.gz. tar -xzf singularity-${VERSION}.tar.gz. pushd singularity-3.8.4. export PATH=/usr/local/go/bin:$PATH. ./mconfig. make -C builddir. sudo make -C builddir install. ```. Check version:. ```. [pichuan@pichuan-centos7 ~]$ singularity --version. singularity version 3.8.4. ```. ## Try running DeepVariant with Singularity. I followed Quick Start to get data. ```. singularity pull docker://google/deepvariant:1.4.0. # Run DeepVariant. singularity run -B /usr/lib/l",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:1268,usability,tool,tools,1268,"e-full,cloud-platform"" \. --image-family ""centos-7"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --zone ""us-west1-b"". gcloud compute ssh pichuan-centos7 --zone ""us-west1-b"". ```. By default the machine has Python2. ```. [pichuan@pichuan-centos7 ~]$ python --version . Python 2.7.5. ```. ## Install Python 3.8.10. ```. sudo yum update. sudo yum install gcc openssl-devel bzip2-devel libffi-devel -y. sudo yum install -y wget. sudo yum install -y python3. ```. ```. wget https://www.python.org/ftp/python/3.8.10/Python-3.8.10.tgz. tar xvfz Python-3.8.10.tgz. ```. ```. cd Python-3.8.10. ./configure --enable-optimizations. ```. ```. sudo yum install -y make. sudo make altinstall. ```. ```. [pichuan@pichuan-centos7 Python-3.8.10]$ python3.8 --version. Python 3.8.10. ```. ## Install Singularity. ```. sudo yum update -y && \. sudo yum groupinstall -y 'Development Tools' && \. sudo yum install -y \. openssl-devel \. libuuid-devel \. libseccomp-devel \. wget \. squashfs-tools \. cryptsetup. ```. ```. export VERSION=1.14.12 OS=linux ARCH=amd64. # Downloads the required Go package. wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz. # Extracts the archive. sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz. # Deletes the ``tar`` file. rm go$VERSION.$OS-$ARCH.tar.gz. export VERSION=3.8.4. wget https://github.com/hpcng/singularity/releases/download/v${VERSION}/singularity-${VERSION}.tar.gz. tar -xzf singularity-${VERSION}.tar.gz. pushd singularity-3.8.4. export PATH=/usr/local/go/bin:$PATH. ./mconfig. make -C builddir. sudo make -C builddir install. ```. Check version:. ```. [pichuan@pichuan-centos7 ~]$ singularity --version. singularity version 3.8.4. ```. ## Try running DeepVariant with Singularity. I followed Quick Start to get data. ```. singularity pull docker://google/deepvariant:1.4.0. # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --m",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:4679,usability,error,error,4679,"lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. python -c 'import numpy as np; print(np.version.version)'. ```. Which shows:. ```. 1.19.2. ```. It seems like my machine doesn't already have numpy, though:. ```. [pichuan@pichuan-centos7 Python-3.8.10]$ pip3.8 show numpy. WARNING: Package(s) not found: numpy. ```. doesn't show anything. ## Install numpy 1.23.0 to see it breaks things. I ran:. ```. pip3.8 install numpy==1.23.0. ```. (Because you mentioned your cluster has 1.23.0). Now this shows:. ```. [pichuan@pichuan-centos7 ~]$ pip3.8 show numpy. Name: numpy. Version: 1.23.0. Summary: NumPy is the fundamental package for array computing with Python. Home-page: https://www.numpy.org. Author: Travis E. Oliphant et al. Author-email: None. License: BSD. Location: /home/pichuan/.local/lib/python3.8/site-packages. Requires: . Required-by: . ```. Then I re-ran:. ```. # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=1. ```. Which still seems to work fine for me. And, this actually shows 1.23.0 as well:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. python -c 'import numpy as np; print(np.version.version)'. ```. ```. 1.23.0. ```. I also checked TensorFlow version:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. python -c 'import tensorflow; print(tensorflow.__version__)'. ```. This shows:. ```. 2.7.0. ```. @asherrar Let me know what you think I need to change to try to reproduce the error you saw. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:49,usability,close,close,49,"Hi @asherrar , I haven't heard from you, so I'll close this issue for now. Please feel free to reopen again.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/611:82,availability,checkpoint,checkpoints,82,"Hi @adamnovak , . I can confirm that our current code is designed to wait for new checkpoints, and only evaluate new ones. I understand that it'll be great retrospectively evaluate older checkpoints too. But we haven't experimented with that. I haven't got time to do so; but in the next few weeks (to a month) I'll likely need to re-run the training tutorial anyway. I'll try to find a time to see if it could be easy to tweak that behavior (but I don't know for sure). I'll keep this issue open and assigned to me for now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:187,availability,checkpoint,checkpoints,187,"Hi @adamnovak , . I can confirm that our current code is designed to wait for new checkpoints, and only evaluate new ones. I understand that it'll be great retrospectively evaluate older checkpoints too. But we haven't experimented with that. I haven't got time to do so; but in the next few weeks (to a month) I'll likely need to re-run the training tutorial anyway. I'll try to find a time to see if it could be easy to tweak that behavior (but I don't know for sure). I'll keep this issue open and assigned to me for now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:41,energy efficiency,current,current,41,"Hi @adamnovak , . I can confirm that our current code is designed to wait for new checkpoints, and only evaluate new ones. I understand that it'll be great retrospectively evaluate older checkpoints too. But we haven't experimented with that. I haven't got time to do so; but in the next few weeks (to a month) I'll likely need to re-run the training tutorial anyway. I'll try to find a time to see if it could be easy to tweak that behavior (but I don't know for sure). I'll keep this issue open and assigned to me for now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:257,performance,time,time,257,"Hi @adamnovak , . I can confirm that our current code is designed to wait for new checkpoints, and only evaluate new ones. I understand that it'll be great retrospectively evaluate older checkpoints too. But we haven't experimented with that. I haven't got time to do so; but in the next few weeks (to a month) I'll likely need to re-run the training tutorial anyway. I'll try to find a time to see if it could be easy to tweak that behavior (but I don't know for sure). I'll keep this issue open and assigned to me for now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:387,performance,time,time,387,"Hi @adamnovak , . I can confirm that our current code is designed to wait for new checkpoints, and only evaluate new ones. I understand that it'll be great retrospectively evaluate older checkpoints too. But we haven't experimented with that. I haven't got time to do so; but in the next few weeks (to a month) I'll likely need to re-run the training tutorial anyway. I'll try to find a time to see if it could be easy to tweak that behavior (but I don't know for sure). I'll keep this issue open and assigned to me for now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:82,reliability,checkpoint,checkpoints,82,"Hi @adamnovak , . I can confirm that our current code is designed to wait for new checkpoints, and only evaluate new ones. I understand that it'll be great retrospectively evaluate older checkpoints too. But we haven't experimented with that. I haven't got time to do so; but in the next few weeks (to a month) I'll likely need to re-run the training tutorial anyway. I'll try to find a time to see if it could be easy to tweak that behavior (but I don't know for sure). I'll keep this issue open and assigned to me for now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:187,reliability,checkpoint,checkpoints,187,"Hi @adamnovak , . I can confirm that our current code is designed to wait for new checkpoints, and only evaluate new ones. I understand that it'll be great retrospectively evaluate older checkpoints too. But we haven't experimented with that. I haven't got time to do so; but in the next few weeks (to a month) I'll likely need to re-run the training tutorial anyway. I'll try to find a time to see if it could be easy to tweak that behavior (but I don't know for sure). I'll keep this issue open and assigned to me for now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:125,testability,understand,understand,125,"Hi @adamnovak , . I can confirm that our current code is designed to wait for new checkpoints, and only evaluate new ones. I understand that it'll be great retrospectively evaluate older checkpoints too. But we haven't experimented with that. I haven't got time to do so; but in the next few weeks (to a month) I'll likely need to re-run the training tutorial anyway. I'll try to find a time to see if it could be easy to tweak that behavior (but I don't know for sure). I'll keep this issue open and assigned to me for now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:24,usability,confirm,confirm,24,"Hi @adamnovak , . I can confirm that our current code is designed to wait for new checkpoints, and only evaluate new ones. I understand that it'll be great retrospectively evaluate older checkpoints too. But we haven't experimented with that. I haven't got time to do so; but in the next few weeks (to a month) I'll likely need to re-run the training tutorial anyway. I'll try to find a time to see if it could be easy to tweak that behavior (but I don't know for sure). I'll keep this issue open and assigned to me for now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:433,usability,behavi,behavior,433,"Hi @adamnovak , . I can confirm that our current code is designed to wait for new checkpoints, and only evaluate new ones. I understand that it'll be great retrospectively evaluate older checkpoints too. But we haven't experimented with that. I haven't got time to do so; but in the next few weeks (to a month) I'll likely need to re-run the training tutorial anyway. I'll try to find a time to see if it could be easy to tweak that behavior (but I don't know for sure). I'll keep this issue open and assigned to me for now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:66,energy efficiency,model,modeling,66,"Hi @adamnovak ,. given that we're planning on a migration for the modeling framework, we won't spend too much time on this. I'll close the issue. Feel free to reach out if there's anything you want to discuss.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:110,performance,time,time,110,"Hi @adamnovak ,. given that we're planning on a migration for the modeling framework, we won't spend too much time on this. I'll close the issue. Feel free to reach out if there's anything you want to discuss.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:66,security,model,modeling,66,"Hi @adamnovak ,. given that we're planning on a migration for the modeling framework, we won't spend too much time on this. I'll close the issue. Feel free to reach out if there's anything you want to discuss.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:34,testability,plan,planning,34,"Hi @adamnovak ,. given that we're planning on a migration for the modeling framework, we won't spend too much time on this. I'll close the issue. Feel free to reach out if there's anything you want to discuss.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:129,usability,close,close,129,"Hi @adamnovak ,. given that we're planning on a migration for the modeling framework, we won't spend too much time on this. I'll close the issue. Feel free to reach out if there's anything you want to discuss.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:37,availability,checkpoint,checkpoint,37,"@adamnovak,. Just try to copy an old checkpoint file as a new file so it gets an updated timestamp, since just quickly looking at the tensorflow source code it seems to just look for the latest file:. https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/checkpoint_utils.py#L150-L178. https://github.com/tensorflow/tensorflow/blob/55d62330dd9197e69ff8f1f03981784184706b2a/tensorflow/python/checkpoint/checkpoint_management.py#L326-L363. It if complains then it would be easy to tweak the checkpoints for what tensorflow is looking for in that directory. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:417,availability,checkpoint,checkpoint,417,"@adamnovak,. Just try to copy an old checkpoint file as a new file so it gets an updated timestamp, since just quickly looking at the tensorflow source code it seems to just look for the latest file:. https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/checkpoint_utils.py#L150-L178. https://github.com/tensorflow/tensorflow/blob/55d62330dd9197e69ff8f1f03981784184706b2a/tensorflow/python/checkpoint/checkpoint_management.py#L326-L363. It if complains then it would be easy to tweak the checkpoints for what tensorflow is looking for in that directory. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:515,availability,checkpoint,checkpoints,515,"@adamnovak,. Just try to copy an old checkpoint file as a new file so it gets an updated timestamp, since just quickly looking at the tensorflow source code it seems to just look for the latest file:. https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/checkpoint_utils.py#L150-L178. https://github.com/tensorflow/tensorflow/blob/55d62330dd9197e69ff8f1f03981784184706b2a/tensorflow/python/checkpoint/checkpoint_management.py#L326-L363. It if complains then it would be easy to tweak the checkpoints for what tensorflow is looking for in that directory. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:81,deployability,updat,updated,81,"@adamnovak,. Just try to copy an old checkpoint file as a new file so it gets an updated timestamp, since just quickly looking at the tensorflow source code it seems to just look for the latest file:. https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/checkpoint_utils.py#L150-L178. https://github.com/tensorflow/tensorflow/blob/55d62330dd9197e69ff8f1f03981784184706b2a/tensorflow/python/checkpoint/checkpoint_management.py#L326-L363. It if complains then it would be easy to tweak the checkpoints for what tensorflow is looking for in that directory. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:89,performance,time,timestamp,89,"@adamnovak,. Just try to copy an old checkpoint file as a new file so it gets an updated timestamp, since just quickly looking at the tensorflow source code it seems to just look for the latest file:. https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/checkpoint_utils.py#L150-L178. https://github.com/tensorflow/tensorflow/blob/55d62330dd9197e69ff8f1f03981784184706b2a/tensorflow/python/checkpoint/checkpoint_management.py#L326-L363. It if complains then it would be easy to tweak the checkpoints for what tensorflow is looking for in that directory. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:37,reliability,checkpoint,checkpoint,37,"@adamnovak,. Just try to copy an old checkpoint file as a new file so it gets an updated timestamp, since just quickly looking at the tensorflow source code it seems to just look for the latest file:. https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/checkpoint_utils.py#L150-L178. https://github.com/tensorflow/tensorflow/blob/55d62330dd9197e69ff8f1f03981784184706b2a/tensorflow/python/checkpoint/checkpoint_management.py#L326-L363. It if complains then it would be easy to tweak the checkpoints for what tensorflow is looking for in that directory. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:417,reliability,checkpoint,checkpoint,417,"@adamnovak,. Just try to copy an old checkpoint file as a new file so it gets an updated timestamp, since just quickly looking at the tensorflow source code it seems to just look for the latest file:. https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/checkpoint_utils.py#L150-L178. https://github.com/tensorflow/tensorflow/blob/55d62330dd9197e69ff8f1f03981784184706b2a/tensorflow/python/checkpoint/checkpoint_management.py#L326-L363. It if complains then it would be easy to tweak the checkpoints for what tensorflow is looking for in that directory. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:515,reliability,checkpoint,checkpoints,515,"@adamnovak,. Just try to copy an old checkpoint file as a new file so it gets an updated timestamp, since just quickly looking at the tensorflow source code it seems to just look for the latest file:. https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/checkpoint_utils.py#L150-L178. https://github.com/tensorflow/tensorflow/blob/55d62330dd9197e69ff8f1f03981784184706b2a/tensorflow/python/checkpoint/checkpoint_management.py#L326-L363. It if complains then it would be easy to tweak the checkpoints for what tensorflow is looking for in that directory. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:81,safety,updat,updated,81,"@adamnovak,. Just try to copy an old checkpoint file as a new file so it gets an updated timestamp, since just quickly looking at the tensorflow source code it seems to just look for the latest file:. https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/checkpoint_utils.py#L150-L178. https://github.com/tensorflow/tensorflow/blob/55d62330dd9197e69ff8f1f03981784184706b2a/tensorflow/python/checkpoint/checkpoint_management.py#L326-L363. It if complains then it would be easy to tweak the checkpoints for what tensorflow is looking for in that directory. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:470,safety,compl,complains,470,"@adamnovak,. Just try to copy an old checkpoint file as a new file so it gets an updated timestamp, since just quickly looking at the tensorflow source code it seems to just look for the latest file:. https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/checkpoint_utils.py#L150-L178. https://github.com/tensorflow/tensorflow/blob/55d62330dd9197e69ff8f1f03981784184706b2a/tensorflow/python/checkpoint/checkpoint_management.py#L326-L363. It if complains then it would be easy to tweak the checkpoints for what tensorflow is looking for in that directory. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:81,security,updat,updated,81,"@adamnovak,. Just try to copy an old checkpoint file as a new file so it gets an updated timestamp, since just quickly looking at the tensorflow source code it seems to just look for the latest file:. https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/checkpoint_utils.py#L150-L178. https://github.com/tensorflow/tensorflow/blob/55d62330dd9197e69ff8f1f03981784184706b2a/tensorflow/python/checkpoint/checkpoint_management.py#L326-L363. It if complains then it would be easy to tweak the checkpoints for what tensorflow is looking for in that directory. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:470,security,compl,complains,470,"@adamnovak,. Just try to copy an old checkpoint file as a new file so it gets an updated timestamp, since just quickly looking at the tensorflow source code it seems to just look for the latest file:. https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/checkpoint_utils.py#L150-L178. https://github.com/tensorflow/tensorflow/blob/55d62330dd9197e69ff8f1f03981784184706b2a/tensorflow/python/checkpoint/checkpoint_management.py#L326-L363. It if complains then it would be easy to tweak the checkpoints for what tensorflow is looking for in that directory. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:589,usability,help,helps,589,"@adamnovak,. Just try to copy an old checkpoint file as a new file so it gets an updated timestamp, since just quickly looking at the tensorflow source code it seems to just look for the latest file:. https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/checkpoint_utils.py#L150-L178. https://github.com/tensorflow/tensorflow/blob/55d62330dd9197e69ff8f1f03981784184706b2a/tensorflow/python/checkpoint/checkpoint_management.py#L326-L363. It if complains then it would be easy to tweak the checkpoints for what tensorflow is looking for in that directory. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/612:54,usability,help,helpful,54,"@yangyxt ,. Thank you for opening this issue. It'd be helpful if we could get a little more information about this variant. There could be a lot of reasons for a variant to be excluded. To start, can you please look at the VCF near this region to see if the candidate was generated and it is a RefCall or the candidate was not being picked at all? Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/612
https://github.com/google/deepvariant/issues/612:207,interoperability,Specif,Specifically,207,"@yangyxt ,. As for the request to emit realigned BAM. Please see the details how the realigner works: https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md#what-is-the-realigner-and-how-does-it-work . Specifically this details maybe helpful for you:. There is also the option to output the realigned reads, e.g. to inspect the new alignments in IGV. This can be done by passing the following parameters: `--make_examples_extra_args=""emit_realigned_reads=true,realigner_diagnostics=/output/realigned_reads""`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/612
https://github.com/google/deepvariant/issues/612:398,modifiability,paramet,parameters,398,"@yangyxt ,. As for the request to emit realigned BAM. Please see the details how the realigner works: https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md#what-is-the-realigner-and-how-does-it-work . Specifically this details maybe helpful for you:. There is also the option to output the realigned reads, e.g. to inspect the new alignments in IGV. This can be done by passing the following parameters: `--make_examples_extra_args=""emit_realigned_reads=true,realigner_diagnostics=/output/realigned_reads""`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/612
https://github.com/google/deepvariant/issues/612:192,reliability,doe,does-it-work,192,"@yangyxt ,. As for the request to emit realigned BAM. Please see the details how the realigner works: https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md#what-is-the-realigner-and-how-does-it-work . Specifically this details maybe helpful for you:. There is also the option to output the realigned reads, e.g. to inspect the new alignments in IGV. This can be done by passing the following parameters: `--make_examples_extra_args=""emit_realigned_reads=true,realigner_diagnostics=/output/realigned_reads""`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/612
https://github.com/google/deepvariant/issues/612:239,usability,help,helpful,239,"@yangyxt ,. As for the request to emit realigned BAM. Please see the details how the realigner works: https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md#what-is-the-realigner-and-how-does-it-work . Specifically this details maybe helpful for you:. There is also the option to output the realigned reads, e.g. to inspect the new alignments in IGV. This can be done by passing the following parameters: `--make_examples_extra_args=""emit_realigned_reads=true,realigner_diagnostics=/output/realigned_reads""`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/612
https://github.com/google/deepvariant/issues/612:19,usability,close,close,19,"Hi @yangyxt , I'll close this issue now. Feel free to open new issues if you have more questions for us.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/612
https://github.com/google/deepvariant/issues/613:346,interoperability,bind,bind,346,"Hi, I'm not a DeepVariant developer but ran through the same problem. They talk about it at the end of the [FAQ](https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md), where they recommend to add `export TMPDIR=""$PWD/tmp_dir""`. Sadly it was not enough for me, I also had to mount the temporary dir as well as input and output using the `--bind` option, so if setting `TMPDIR=""$PWD/tmp_dir""` is not enough, you could maybe try something like:. ```INPUT_DIR=/path1/4_Test/qingjiang/dpv. OUTPUT_DIR=/path1/4_Test/qingjiang/dpv. export TMPDIR=""$PWD/tmp_dir"". singularity run /path/dpv/deepvariant_1.4.0.sif /opt/deepvariant/bin/run_deepvariant \. --bind $TMPDIR:$TMPDIR,""${INPUT_DIR}"":input,""${OUTPUT_DIR}"":output \. --num_shards=3 \. --model_type=PACBIO \. --ref=input/QJref.fa \. --reads=input/input.bam \. --output_vcf=output/output.vcf.gz \. --output_gvcf=output/output.g.vcf.gz \. --intermediate_results_dir output/intermediate_results_dir \.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/613
https://github.com/google/deepvariant/issues/613:652,interoperability,bind,bind,652,"Hi, I'm not a DeepVariant developer but ran through the same problem. They talk about it at the end of the [FAQ](https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md), where they recommend to add `export TMPDIR=""$PWD/tmp_dir""`. Sadly it was not enough for me, I also had to mount the temporary dir as well as input and output using the `--bind` option, so if setting `TMPDIR=""$PWD/tmp_dir""` is not enough, you could maybe try something like:. ```INPUT_DIR=/path1/4_Test/qingjiang/dpv. OUTPUT_DIR=/path1/4_Test/qingjiang/dpv. export TMPDIR=""$PWD/tmp_dir"". singularity run /path/dpv/deepvariant_1.4.0.sif /opt/deepvariant/bin/run_deepvariant \. --bind $TMPDIR:$TMPDIR,""${INPUT_DIR}"":input,""${OUTPUT_DIR}"":output \. --num_shards=3 \. --model_type=PACBIO \. --ref=input/QJref.fa \. --reads=input/input.bam \. --output_vcf=output/output.vcf.gz \. --output_gvcf=output/output.g.vcf.gz \. --intermediate_results_dir output/intermediate_results_dir \.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/613
https://github.com/google/deepvariant/issues/613:346,modifiability,bind,bind,346,"Hi, I'm not a DeepVariant developer but ran through the same problem. They talk about it at the end of the [FAQ](https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md), where they recommend to add `export TMPDIR=""$PWD/tmp_dir""`. Sadly it was not enough for me, I also had to mount the temporary dir as well as input and output using the `--bind` option, so if setting `TMPDIR=""$PWD/tmp_dir""` is not enough, you could maybe try something like:. ```INPUT_DIR=/path1/4_Test/qingjiang/dpv. OUTPUT_DIR=/path1/4_Test/qingjiang/dpv. export TMPDIR=""$PWD/tmp_dir"". singularity run /path/dpv/deepvariant_1.4.0.sif /opt/deepvariant/bin/run_deepvariant \. --bind $TMPDIR:$TMPDIR,""${INPUT_DIR}"":input,""${OUTPUT_DIR}"":output \. --num_shards=3 \. --model_type=PACBIO \. --ref=input/QJref.fa \. --reads=input/input.bam \. --output_vcf=output/output.vcf.gz \. --output_gvcf=output/output.g.vcf.gz \. --intermediate_results_dir output/intermediate_results_dir \.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/613
https://github.com/google/deepvariant/issues/613:652,modifiability,bind,bind,652,"Hi, I'm not a DeepVariant developer but ran through the same problem. They talk about it at the end of the [FAQ](https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md), where they recommend to add `export TMPDIR=""$PWD/tmp_dir""`. Sadly it was not enough for me, I also had to mount the temporary dir as well as input and output using the `--bind` option, so if setting `TMPDIR=""$PWD/tmp_dir""` is not enough, you could maybe try something like:. ```INPUT_DIR=/path1/4_Test/qingjiang/dpv. OUTPUT_DIR=/path1/4_Test/qingjiang/dpv. export TMPDIR=""$PWD/tmp_dir"". singularity run /path/dpv/deepvariant_1.4.0.sif /opt/deepvariant/bin/run_deepvariant \. --bind $TMPDIR:$TMPDIR,""${INPUT_DIR}"":input,""${OUTPUT_DIR}"":output \. --num_shards=3 \. --model_type=PACBIO \. --ref=input/QJref.fa \. --reads=input/input.bam \. --output_vcf=output/output.vcf.gz \. --output_gvcf=output/output.g.vcf.gz \. --intermediate_results_dir output/intermediate_results_dir \.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/613
https://github.com/google/deepvariant/issues/613:751,modifiability,PAC,PACBIO,751,"Hi, I'm not a DeepVariant developer but ran through the same problem. They talk about it at the end of the [FAQ](https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md), where they recommend to add `export TMPDIR=""$PWD/tmp_dir""`. Sadly it was not enough for me, I also had to mount the temporary dir as well as input and output using the `--bind` option, so if setting `TMPDIR=""$PWD/tmp_dir""` is not enough, you could maybe try something like:. ```INPUT_DIR=/path1/4_Test/qingjiang/dpv. OUTPUT_DIR=/path1/4_Test/qingjiang/dpv. export TMPDIR=""$PWD/tmp_dir"". singularity run /path/dpv/deepvariant_1.4.0.sif /opt/deepvariant/bin/run_deepvariant \. --bind $TMPDIR:$TMPDIR,""${INPUT_DIR}"":input,""${OUTPUT_DIR}"":output \. --num_shards=3 \. --model_type=PACBIO \. --ref=input/QJref.fa \. --reads=input/input.bam \. --output_vcf=output/output.vcf.gz \. --output_gvcf=output/output.g.vcf.gz \. --intermediate_results_dir output/intermediate_results_dir \.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/613
https://github.com/google/deepvariant/issues/613:316,safety,input,input,316,"Hi, I'm not a DeepVariant developer but ran through the same problem. They talk about it at the end of the [FAQ](https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md), where they recommend to add `export TMPDIR=""$PWD/tmp_dir""`. Sadly it was not enough for me, I also had to mount the temporary dir as well as input and output using the `--bind` option, so if setting `TMPDIR=""$PWD/tmp_dir""` is not enough, you could maybe try something like:. ```INPUT_DIR=/path1/4_Test/qingjiang/dpv. OUTPUT_DIR=/path1/4_Test/qingjiang/dpv. export TMPDIR=""$PWD/tmp_dir"". singularity run /path/dpv/deepvariant_1.4.0.sif /opt/deepvariant/bin/run_deepvariant \. --bind $TMPDIR:$TMPDIR,""${INPUT_DIR}"":input,""${OUTPUT_DIR}"":output \. --num_shards=3 \. --model_type=PACBIO \. --ref=input/QJref.fa \. --reads=input/input.bam \. --output_vcf=output/output.vcf.gz \. --output_gvcf=output/output.g.vcf.gz \. --intermediate_results_dir output/intermediate_results_dir \.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/613
https://github.com/google/deepvariant/issues/613:688,safety,input,input,688,"Hi, I'm not a DeepVariant developer but ran through the same problem. They talk about it at the end of the [FAQ](https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md), where they recommend to add `export TMPDIR=""$PWD/tmp_dir""`. Sadly it was not enough for me, I also had to mount the temporary dir as well as input and output using the `--bind` option, so if setting `TMPDIR=""$PWD/tmp_dir""` is not enough, you could maybe try something like:. ```INPUT_DIR=/path1/4_Test/qingjiang/dpv. OUTPUT_DIR=/path1/4_Test/qingjiang/dpv. export TMPDIR=""$PWD/tmp_dir"". singularity run /path/dpv/deepvariant_1.4.0.sif /opt/deepvariant/bin/run_deepvariant \. --bind $TMPDIR:$TMPDIR,""${INPUT_DIR}"":input,""${OUTPUT_DIR}"":output \. --num_shards=3 \. --model_type=PACBIO \. --ref=input/QJref.fa \. --reads=input/input.bam \. --output_vcf=output/output.vcf.gz \. --output_gvcf=output/output.g.vcf.gz \. --intermediate_results_dir output/intermediate_results_dir \.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/613
https://github.com/google/deepvariant/issues/613:767,safety,input,input,767,"Hi, I'm not a DeepVariant developer but ran through the same problem. They talk about it at the end of the [FAQ](https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md), where they recommend to add `export TMPDIR=""$PWD/tmp_dir""`. Sadly it was not enough for me, I also had to mount the temporary dir as well as input and output using the `--bind` option, so if setting `TMPDIR=""$PWD/tmp_dir""` is not enough, you could maybe try something like:. ```INPUT_DIR=/path1/4_Test/qingjiang/dpv. OUTPUT_DIR=/path1/4_Test/qingjiang/dpv. export TMPDIR=""$PWD/tmp_dir"". singularity run /path/dpv/deepvariant_1.4.0.sif /opt/deepvariant/bin/run_deepvariant \. --bind $TMPDIR:$TMPDIR,""${INPUT_DIR}"":input,""${OUTPUT_DIR}"":output \. --num_shards=3 \. --model_type=PACBIO \. --ref=input/QJref.fa \. --reads=input/input.bam \. --output_vcf=output/output.vcf.gz \. --output_gvcf=output/output.g.vcf.gz \. --intermediate_results_dir output/intermediate_results_dir \.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/613
https://github.com/google/deepvariant/issues/613:793,safety,input,input,793,"Hi, I'm not a DeepVariant developer but ran through the same problem. They talk about it at the end of the [FAQ](https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md), where they recommend to add `export TMPDIR=""$PWD/tmp_dir""`. Sadly it was not enough for me, I also had to mount the temporary dir as well as input and output using the `--bind` option, so if setting `TMPDIR=""$PWD/tmp_dir""` is not enough, you could maybe try something like:. ```INPUT_DIR=/path1/4_Test/qingjiang/dpv. OUTPUT_DIR=/path1/4_Test/qingjiang/dpv. export TMPDIR=""$PWD/tmp_dir"". singularity run /path/dpv/deepvariant_1.4.0.sif /opt/deepvariant/bin/run_deepvariant \. --bind $TMPDIR:$TMPDIR,""${INPUT_DIR}"":input,""${OUTPUT_DIR}"":output \. --num_shards=3 \. --model_type=PACBIO \. --ref=input/QJref.fa \. --reads=input/input.bam \. --output_vcf=output/output.vcf.gz \. --output_gvcf=output/output.g.vcf.gz \. --intermediate_results_dir output/intermediate_results_dir \.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/613
https://github.com/google/deepvariant/issues/613:799,safety,input,input,799,"Hi, I'm not a DeepVariant developer but ran through the same problem. They talk about it at the end of the [FAQ](https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md), where they recommend to add `export TMPDIR=""$PWD/tmp_dir""`. Sadly it was not enough for me, I also had to mount the temporary dir as well as input and output using the `--bind` option, so if setting `TMPDIR=""$PWD/tmp_dir""` is not enough, you could maybe try something like:. ```INPUT_DIR=/path1/4_Test/qingjiang/dpv. OUTPUT_DIR=/path1/4_Test/qingjiang/dpv. export TMPDIR=""$PWD/tmp_dir"". singularity run /path/dpv/deepvariant_1.4.0.sif /opt/deepvariant/bin/run_deepvariant \. --bind $TMPDIR:$TMPDIR,""${INPUT_DIR}"":input,""${OUTPUT_DIR}"":output \. --num_shards=3 \. --model_type=PACBIO \. --ref=input/QJref.fa \. --reads=input/input.bam \. --output_vcf=output/output.vcf.gz \. --output_gvcf=output/output.g.vcf.gz \. --intermediate_results_dir output/intermediate_results_dir \.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/613
https://github.com/google/deepvariant/issues/613:316,usability,input,input,316,"Hi, I'm not a DeepVariant developer but ran through the same problem. They talk about it at the end of the [FAQ](https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md), where they recommend to add `export TMPDIR=""$PWD/tmp_dir""`. Sadly it was not enough for me, I also had to mount the temporary dir as well as input and output using the `--bind` option, so if setting `TMPDIR=""$PWD/tmp_dir""` is not enough, you could maybe try something like:. ```INPUT_DIR=/path1/4_Test/qingjiang/dpv. OUTPUT_DIR=/path1/4_Test/qingjiang/dpv. export TMPDIR=""$PWD/tmp_dir"". singularity run /path/dpv/deepvariant_1.4.0.sif /opt/deepvariant/bin/run_deepvariant \. --bind $TMPDIR:$TMPDIR,""${INPUT_DIR}"":input,""${OUTPUT_DIR}"":output \. --num_shards=3 \. --model_type=PACBIO \. --ref=input/QJref.fa \. --reads=input/input.bam \. --output_vcf=output/output.vcf.gz \. --output_gvcf=output/output.g.vcf.gz \. --intermediate_results_dir output/intermediate_results_dir \.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/613
https://github.com/google/deepvariant/issues/613:688,usability,input,input,688,"Hi, I'm not a DeepVariant developer but ran through the same problem. They talk about it at the end of the [FAQ](https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md), where they recommend to add `export TMPDIR=""$PWD/tmp_dir""`. Sadly it was not enough for me, I also had to mount the temporary dir as well as input and output using the `--bind` option, so if setting `TMPDIR=""$PWD/tmp_dir""` is not enough, you could maybe try something like:. ```INPUT_DIR=/path1/4_Test/qingjiang/dpv. OUTPUT_DIR=/path1/4_Test/qingjiang/dpv. export TMPDIR=""$PWD/tmp_dir"". singularity run /path/dpv/deepvariant_1.4.0.sif /opt/deepvariant/bin/run_deepvariant \. --bind $TMPDIR:$TMPDIR,""${INPUT_DIR}"":input,""${OUTPUT_DIR}"":output \. --num_shards=3 \. --model_type=PACBIO \. --ref=input/QJref.fa \. --reads=input/input.bam \. --output_vcf=output/output.vcf.gz \. --output_gvcf=output/output.g.vcf.gz \. --intermediate_results_dir output/intermediate_results_dir \.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/613
https://github.com/google/deepvariant/issues/613:767,usability,input,input,767,"Hi, I'm not a DeepVariant developer but ran through the same problem. They talk about it at the end of the [FAQ](https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md), where they recommend to add `export TMPDIR=""$PWD/tmp_dir""`. Sadly it was not enough for me, I also had to mount the temporary dir as well as input and output using the `--bind` option, so if setting `TMPDIR=""$PWD/tmp_dir""` is not enough, you could maybe try something like:. ```INPUT_DIR=/path1/4_Test/qingjiang/dpv. OUTPUT_DIR=/path1/4_Test/qingjiang/dpv. export TMPDIR=""$PWD/tmp_dir"". singularity run /path/dpv/deepvariant_1.4.0.sif /opt/deepvariant/bin/run_deepvariant \. --bind $TMPDIR:$TMPDIR,""${INPUT_DIR}"":input,""${OUTPUT_DIR}"":output \. --num_shards=3 \. --model_type=PACBIO \. --ref=input/QJref.fa \. --reads=input/input.bam \. --output_vcf=output/output.vcf.gz \. --output_gvcf=output/output.g.vcf.gz \. --intermediate_results_dir output/intermediate_results_dir \.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/613
https://github.com/google/deepvariant/issues/613:793,usability,input,input,793,"Hi, I'm not a DeepVariant developer but ran through the same problem. They talk about it at the end of the [FAQ](https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md), where they recommend to add `export TMPDIR=""$PWD/tmp_dir""`. Sadly it was not enough for me, I also had to mount the temporary dir as well as input and output using the `--bind` option, so if setting `TMPDIR=""$PWD/tmp_dir""` is not enough, you could maybe try something like:. ```INPUT_DIR=/path1/4_Test/qingjiang/dpv. OUTPUT_DIR=/path1/4_Test/qingjiang/dpv. export TMPDIR=""$PWD/tmp_dir"". singularity run /path/dpv/deepvariant_1.4.0.sif /opt/deepvariant/bin/run_deepvariant \. --bind $TMPDIR:$TMPDIR,""${INPUT_DIR}"":input,""${OUTPUT_DIR}"":output \. --num_shards=3 \. --model_type=PACBIO \. --ref=input/QJref.fa \. --reads=input/input.bam \. --output_vcf=output/output.vcf.gz \. --output_gvcf=output/output.g.vcf.gz \. --intermediate_results_dir output/intermediate_results_dir \.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/613
https://github.com/google/deepvariant/issues/613:799,usability,input,input,799,"Hi, I'm not a DeepVariant developer but ran through the same problem. They talk about it at the end of the [FAQ](https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md), where they recommend to add `export TMPDIR=""$PWD/tmp_dir""`. Sadly it was not enough for me, I also had to mount the temporary dir as well as input and output using the `--bind` option, so if setting `TMPDIR=""$PWD/tmp_dir""` is not enough, you could maybe try something like:. ```INPUT_DIR=/path1/4_Test/qingjiang/dpv. OUTPUT_DIR=/path1/4_Test/qingjiang/dpv. export TMPDIR=""$PWD/tmp_dir"". singularity run /path/dpv/deepvariant_1.4.0.sif /opt/deepvariant/bin/run_deepvariant \. --bind $TMPDIR:$TMPDIR,""${INPUT_DIR}"":input,""${OUTPUT_DIR}"":output \. --num_shards=3 \. --model_type=PACBIO \. --ref=input/QJref.fa \. --reads=input/input.bam \. --output_vcf=output/output.vcf.gz \. --output_gvcf=output/output.g.vcf.gz \. --intermediate_results_dir output/intermediate_results_dir \.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/613
https://github.com/google/deepvariant/issues/613:1115,availability,error,error,1115,"consult you. First I verify that my host directory is successfully mounted to the container directory.`/mnt/QJref.fa /mnt/input.bam`. ```. singularity exec -B $TMPDIR:$TMPDIR,""${WORK_DIR}"":/mnt /dellfsqd2/ST_OCEAN/USER/sunzhilong/1_Software/dpv/deepvariant_1.4.0.sif bash. Singularity> cd / && ls. bin boot dellfsqd2 dev environment etc home lib lib32 lib64 libx32 media mnt opt proc root run sbin singularity srv sys tmp usr var. Singularity> cd mnt && ls. QJref.fa input.bam QJref.fa.fai input.bam.bai tmp_dir. ```. Then I ran the following script. ```. cat test0215.sh. WORK_DIR=/path1/4_Test/qingjiang/dpv. export TMPDIR=""$PWD/tmp_dir"". singularity run -B$TMPDIR:$TMPDIR,""${WORK_DIR}"":/mnt \. /path1/1_Software/dpv/deepvariant_1.4.0.sif /opt/deepvariant/bin/run_deepvariant \. --num_shards=3 \. --model_type=PACBIO \. --ref=/mnt/QJref.fa \. --reads=/mnt/input.bam \. --output_vcf=/mnt/output.vcf.gz \. --output_gvcf=/mnt/output.g.vcf.gz \. --intermediate_results_dir /mnt/dpv \. ```. The core error is `ValueError: NOT_FOUND: Could not open /mnt/input.bam. [E::hts_open_format] Failed to open file ""/mnt/input.bam"" : No such file or directory`However, I have verified the existence of `/mnt/input.bam`. The complete error information is as follows. Sincerely look forward to your help! thank you. ```. sh test0215.sh. I0216 00:56:07.446549 140582811191104 run_deepvariant.py:342] Re-using the directory for intermediate results in /mnt/dpv. ***** Intermediate results will be written to /mnt/dpv in docker. ****. ***** Running the command:*****. time seq 0 2 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/mnt/QJref.fa"" --reads ""/mnt/input.bam"" --examples ""/mnt/dpv/make_examples.tfrecord@3.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --gvcf ""/mnt/dpv/gvcf.tfrecord@3.gz"" --max_reads_per_partition ""600"" --min_mapping_quality ""1"" --parse_sam_aux_fields --partition_size ""25000"" --phase_reads --pileup_image_width ""199"" --norealign_rea",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/613
https://github.com/google/deepvariant/issues/613:1338,availability,error,error,1338,"hilong/1_Software/dpv/deepvariant_1.4.0.sif bash. Singularity> cd / && ls. bin boot dellfsqd2 dev environment etc home lib lib32 lib64 libx32 media mnt opt proc root run sbin singularity srv sys tmp usr var. Singularity> cd mnt && ls. QJref.fa input.bam QJref.fa.fai input.bam.bai tmp_dir. ```. Then I ran the following script. ```. cat test0215.sh. WORK_DIR=/path1/4_Test/qingjiang/dpv. export TMPDIR=""$PWD/tmp_dir"". singularity run -B$TMPDIR:$TMPDIR,""${WORK_DIR}"":/mnt \. /path1/1_Software/dpv/deepvariant_1.4.0.sif /opt/deepvariant/bin/run_deepvariant \. --num_shards=3 \. --model_type=PACBIO \. --ref=/mnt/QJref.fa \. --reads=/mnt/input.bam \. --output_vcf=/mnt/output.vcf.gz \. --output_gvcf=/mnt/output.g.vcf.gz \. --intermediate_results_dir /mnt/dpv \. ```. The core error is `ValueError: NOT_FOUND: Could not open /mnt/input.bam. [E::hts_open_format] Failed to open file ""/mnt/input.bam"" : No such file or directory`However, I have verified the existence of `/mnt/input.bam`. The complete error information is as follows. Sincerely look forward to your help! thank you. ```. sh test0215.sh. I0216 00:56:07.446549 140582811191104 run_deepvariant.py:342] Re-using the directory for intermediate results in /mnt/dpv. ***** Intermediate results will be written to /mnt/dpv in docker. ****. ***** Running the command:*****. time seq 0 2 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/mnt/QJref.fa"" --reads ""/mnt/input.bam"" --examples ""/mnt/dpv/make_examples.tfrecord@3.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --gvcf ""/mnt/dpv/gvcf.tfrecord@3.gz"" --max_reads_per_partition ""600"" --min_mapping_quality ""1"" --parse_sam_aux_fields --partition_size ""25000"" --phase_reads --pileup_image_width ""199"" --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels ""0.12"" --task {}. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LC_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/613
https://github.com/google/deepvariant/issues/613:106,deployability,continu,continue,106,"Thanks for your help, I successfully solved the problem of TMPDIR. Then there is a new problem, I hope to continue to consult you. First I verify that my host directory is successfully mounted to the container directory.`/mnt/QJref.fa /mnt/input.bam`. ```. singularity exec -B $TMPDIR:$TMPDIR,""${WORK_DIR}"":/mnt /dellfsqd2/ST_OCEAN/USER/sunzhilong/1_Software/dpv/deepvariant_1.4.0.sif bash. Singularity> cd / && ls. bin boot dellfsqd2 dev environment etc home lib lib32 lib64 libx32 media mnt opt proc root run sbin singularity srv sys tmp usr var. Singularity> cd mnt && ls. QJref.fa input.bam QJref.fa.fai input.bam.bai tmp_dir. ```. Then I ran the following script. ```. cat test0215.sh. WORK_DIR=/path1/4_Test/qingjiang/dpv. export TMPDIR=""$PWD/tmp_dir"". singularity run -B$TMPDIR:$TMPDIR,""${WORK_DIR}"":/mnt \. /path1/1_Software/dpv/deepvariant_1.4.0.sif /opt/deepvariant/bin/run_deepvariant \. --num_shards=3 \. --model_type=PACBIO \. --ref=/mnt/QJref.fa \. --reads=/mnt/input.bam \. --output_vcf=/mnt/output.vcf.gz \. --output_gvcf=/mnt/output.g.vcf.gz \. --intermediate_results_dir /mnt/dpv \. ```. The core error is `ValueError: NOT_FOUND: Could not open /mnt/input.bam. [E::hts_open_format] Failed to open file ""/mnt/input.bam"" : No such file or directory`However, I have verified the existence of `/mnt/input.bam`. The complete error information is as follows. Sincerely look forward to your help! thank you. ```. sh test0215.sh. I0216 00:56:07.446549 140582811191104 run_deepvariant.py:342] Re-using the directory for intermediate results in /mnt/dpv. ***** Intermediate results will be written to /mnt/dpv in docker. ****. ***** Running the command:*****. time seq 0 2 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/mnt/QJref.fa"" --reads ""/mnt/input.bam"" --examples ""/mnt/dpv/make_examples.tfrecord@3.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --gvcf ""/mnt/dpv/gvcf.tfrecord@3.gz"" --max_reads_per_partition ""600"" --min_mappin",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/613
https://github.com/google/deepvariant/issues/613:200,deployability,contain,container,200,"Thanks for your help, I successfully solved the problem of TMPDIR. Then there is a new problem, I hope to continue to consult you. First I verify that my host directory is successfully mounted to the container directory.`/mnt/QJref.fa /mnt/input.bam`. ```. singularity exec -B $TMPDIR:$TMPDIR,""${WORK_DIR}"":/mnt /dellfsqd2/ST_OCEAN/USER/sunzhilong/1_Software/dpv/deepvariant_1.4.0.sif bash. Singularity> cd / && ls. bin boot dellfsqd2 dev environment etc home lib lib32 lib64 libx32 media mnt opt proc root run sbin singularity srv sys tmp usr var. Singularity> cd mnt && ls. QJref.fa input.bam QJref.fa.fai input.bam.bai tmp_dir. ```. Then I ran the following script. ```. cat test0215.sh. WORK_DIR=/path1/4_Test/qingjiang/dpv. export TMPDIR=""$PWD/tmp_dir"". singularity run -B$TMPDIR:$TMPDIR,""${WORK_DIR}"":/mnt \. /path1/1_Software/dpv/deepvariant_1.4.0.sif /opt/deepvariant/bin/run_deepvariant \. --num_shards=3 \. --model_type=PACBIO \. --ref=/mnt/QJref.fa \. --reads=/mnt/input.bam \. --output_vcf=/mnt/output.vcf.gz \. --output_gvcf=/mnt/output.g.vcf.gz \. --intermediate_results_dir /mnt/dpv \. ```. The core error is `ValueError: NOT_FOUND: Could not open /mnt/input.bam. [E::hts_open_format] Failed to open file ""/mnt/input.bam"" : No such file or directory`However, I have verified the existence of `/mnt/input.bam`. The complete error information is as follows. Sincerely look forward to your help! thank you. ```. sh test0215.sh. I0216 00:56:07.446549 140582811191104 run_deepvariant.py:342] Re-using the directory for intermediate results in /mnt/dpv. ***** Intermediate results will be written to /mnt/dpv in docker. ****. ***** Running the command:*****. time seq 0 2 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/mnt/QJref.fa"" --reads ""/mnt/input.bam"" --examples ""/mnt/dpv/make_examples.tfrecord@3.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --gvcf ""/mnt/dpv/gvcf.tfrecord@3.gz"" --max_reads_per_partition ""600"" --min_mappin",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/613
https://github.com/google/deepvariant/issues/613:1200,deployability,Fail,Failed,1200,"tainer directory.`/mnt/QJref.fa /mnt/input.bam`. ```. singularity exec -B $TMPDIR:$TMPDIR,""${WORK_DIR}"":/mnt /dellfsqd2/ST_OCEAN/USER/sunzhilong/1_Software/dpv/deepvariant_1.4.0.sif bash. Singularity> cd / && ls. bin boot dellfsqd2 dev environment etc home lib lib32 lib64 libx32 media mnt opt proc root run sbin singularity srv sys tmp usr var. Singularity> cd mnt && ls. QJref.fa input.bam QJref.fa.fai input.bam.bai tmp_dir. ```. Then I ran the following script. ```. cat test0215.sh. WORK_DIR=/path1/4_Test/qingjiang/dpv. export TMPDIR=""$PWD/tmp_dir"". singularity run -B$TMPDIR:$TMPDIR,""${WORK_DIR}"":/mnt \. /path1/1_Software/dpv/deepvariant_1.4.0.sif /opt/deepvariant/bin/run_deepvariant \. --num_shards=3 \. --model_type=PACBIO \. --ref=/mnt/QJref.fa \. --reads=/mnt/input.bam \. --output_vcf=/mnt/output.vcf.gz \. --output_gvcf=/mnt/output.g.vcf.gz \. --intermediate_results_dir /mnt/dpv \. ```. The core error is `ValueError: NOT_FOUND: Could not open /mnt/input.bam. [E::hts_open_format] Failed to open file ""/mnt/input.bam"" : No such file or directory`However, I have verified the existence of `/mnt/input.bam`. The complete error information is as follows. Sincerely look forward to your help! thank you. ```. sh test0215.sh. I0216 00:56:07.446549 140582811191104 run_deepvariant.py:342] Re-using the directory for intermediate results in /mnt/dpv. ***** Intermediate results will be written to /mnt/dpv in docker. ****. ***** Running the command:*****. time seq 0 2 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/mnt/QJref.fa"" --reads ""/mnt/input.bam"" --examples ""/mnt/dpv/make_examples.tfrecord@3.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --gvcf ""/mnt/dpv/gvcf.tfrecord@3.gz"" --max_reads_per_partition ""600"" --min_mapping_quality ""1"" --parse_sam_aux_fields --partition_size ""25000"" --phase_reads --pileup_image_width ""199"" --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels ""0.12"" --task {}.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/613
https://github.com/google/deepvariant/issues/613:2234,deployability,fail,failed,2234,": No such file or directory`However, I have verified the existence of `/mnt/input.bam`. The complete error information is as follows. Sincerely look forward to your help! thank you. ```. sh test0215.sh. I0216 00:56:07.446549 140582811191104 run_deepvariant.py:342] Re-using the directory for intermediate results in /mnt/dpv. ***** Intermediate results will be written to /mnt/dpv in docker. ****. ***** Running the command:*****. time seq 0 2 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/mnt/QJref.fa"" --reads ""/mnt/input.bam"" --examples ""/mnt/dpv/make_examples.tfrecord@3.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --gvcf ""/mnt/dpv/gvcf.tfrecord@3.gz"" --max_reads_per_partition ""600"" --min_mapping_quality ""1"" --parse_sam_aux_fields --partition_size ""25000"" --phase_reads --pileup_image_width ""199"" --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels ""0.12"" --task {}. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LC_CTYPE = ""C.UTF-8"",. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LC_CTYPE = ""C.UTF-8"",. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). [E::hts_open_format] Failed to open file ""/mnt/input.bam"" : No such file or directory. Traceback (most recent call last):. File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_p4oo9k4b/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>. app.run(main). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_p4oo9k4b/runfiles/absl_py/absl/app.py"", line 300, in run. _run_main(main, args). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/B",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/613
https://github.com/google/deepvariant/issues/613:2401,deployability,instal,installed,2401,"! thank you. ```. sh test0215.sh. I0216 00:56:07.446549 140582811191104 run_deepvariant.py:342] Re-using the directory for intermediate results in /mnt/dpv. ***** Intermediate results will be written to /mnt/dpv in docker. ****. ***** Running the command:*****. time seq 0 2 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/mnt/QJref.fa"" --reads ""/mnt/input.bam"" --examples ""/mnt/dpv/make_examples.tfrecord@3.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --gvcf ""/mnt/dpv/gvcf.tfrecord@3.gz"" --max_reads_per_partition ""600"" --min_mapping_quality ""1"" --parse_sam_aux_fields --partition_size ""25000"" --phase_reads --pileup_image_width ""199"" --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels ""0.12"" --task {}. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LC_CTYPE = ""C.UTF-8"",. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LC_CTYPE = ""C.UTF-8"",. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). [E::hts_open_format] Failed to open file ""/mnt/input.bam"" : No such file or directory. Traceback (most recent call last):. File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_p4oo9k4b/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>. app.run(main). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_p4oo9k4b/runfiles/absl_py/absl/app.py"", line 300, in run. _run_main(main, args). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_p4oo9k4b/runfiles/absl_py/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_p4oo9k4b/run",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/613
https://github.com/google/deepvariant/issues/613:2515,deployability,fail,failed,2515,"ectory for intermediate results in /mnt/dpv. ***** Intermediate results will be written to /mnt/dpv in docker. ****. ***** Running the command:*****. time seq 0 2 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/mnt/QJref.fa"" --reads ""/mnt/input.bam"" --examples ""/mnt/dpv/make_examples.tfrecord@3.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --gvcf ""/mnt/dpv/gvcf.tfrecord@3.gz"" --max_reads_per_partition ""600"" --min_mapping_quality ""1"" --parse_sam_aux_fields --partition_size ""25000"" --phase_reads --pileup_image_width ""199"" --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels ""0.12"" --task {}. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LC_CTYPE = ""C.UTF-8"",. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LC_CTYPE = ""C.UTF-8"",. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). [E::hts_open_format] Failed to open file ""/mnt/input.bam"" : No such file or directory. Traceback (most recent call last):. File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_p4oo9k4b/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>. app.run(main). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_p4oo9k4b/runfiles/absl_py/absl/app.py"", line 300, in run. _run_main(main, args). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_p4oo9k4b/runfiles/absl_py/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_p4oo9k4b/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 166, in main. options = default_options(add_fla",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/613
https://github.com/google/deepvariant/issues/613:2682,deployability,instal,installed,2682,"llel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/mnt/QJref.fa"" --reads ""/mnt/input.bam"" --examples ""/mnt/dpv/make_examples.tfrecord@3.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --gvcf ""/mnt/dpv/gvcf.tfrecord@3.gz"" --max_reads_per_partition ""600"" --min_mapping_quality ""1"" --parse_sam_aux_fields --partition_size ""25000"" --phase_reads --pileup_image_width ""199"" --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels ""0.12"" --task {}. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LC_CTYPE = ""C.UTF-8"",. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LC_CTYPE = ""C.UTF-8"",. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). [E::hts_open_format] Failed to open file ""/mnt/input.bam"" : No such file or directory. Traceback (most recent call last):. File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_p4oo9k4b/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>. app.run(main). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_p4oo9k4b/runfiles/absl_py/absl/app.py"", line 300, in run. _run_main(main, args). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_p4oo9k4b/runfiles/absl_py/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_p4oo9k4b/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 166, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_p4oo9k4b/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 128, in ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/613
https://github.com/google/deepvariant/issues/613:2787,deployability,Fail,Failed,2787,"--reads ""/mnt/input.bam"" --examples ""/mnt/dpv/make_examples.tfrecord@3.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --gvcf ""/mnt/dpv/gvcf.tfrecord@3.gz"" --max_reads_per_partition ""600"" --min_mapping_quality ""1"" --parse_sam_aux_fields --partition_size ""25000"" --phase_reads --pileup_image_width ""199"" --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels ""0.12"" --task {}. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LC_CTYPE = ""C.UTF-8"",. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LC_CTYPE = ""C.UTF-8"",. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). [E::hts_open_format] Failed to open file ""/mnt/input.bam"" : No such file or directory. Traceback (most recent call last):. File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_p4oo9k4b/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>. app.run(main). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_p4oo9k4b/runfiles/absl_py/absl/app.py"", line 300, in run. _run_main(main, args). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_p4oo9k4b/runfiles/absl_py/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_p4oo9k4b/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 166, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_p4oo9k4b/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 128, in default_options. samples_in_order, sample_role_to_train = one_sample_from_flags(. File ""/path1/4_Test/q",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/613
https://github.com/google/deepvariant/issues/613:3032,deployability,modul,module,3032,"s --partition_size ""25000"" --phase_reads --pileup_image_width ""199"" --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels ""0.12"" --task {}. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LC_CTYPE = ""C.UTF-8"",. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LC_CTYPE = ""C.UTF-8"",. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). [E::hts_open_format] Failed to open file ""/mnt/input.bam"" : No such file or directory. Traceback (most recent call last):. File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_p4oo9k4b/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>. app.run(main). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_p4oo9k4b/runfiles/absl_py/absl/app.py"", line 300, in run. _run_main(main, args). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_p4oo9k4b/runfiles/absl_py/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_p4oo9k4b/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 166, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_p4oo9k4b/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 128, in default_options. samples_in_order, sample_role_to_train = one_sample_from_flags(. File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_p4oo9k4b/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 85, in one_sample_from_flags. sample_name = make_examples_core.assign_sample_name(. File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/613
https://github.com/google/deepvariant/issues/613:4924,deployability,Fail,Failed,4924,"lags. sample_name = make_examples_core.assign_sample_name(. File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_p4oo9k4b/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 134, in assign_sample_name. with sam.SamReader(reads_filenames.split(',')[0]) as sam_reader:. File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_p4oo9k4b/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_p4oo9k4b/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 260, in _native_reader. return NativeSamReader(input_path, **kwargs). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_p4oo9k4b/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 227, in __init__. self._reader = sam_reader.SamReader.from_file(. ValueError: NOT_FOUND: Could not open /mnt/input.bam. [E::hts_open_format] Failed to open file ""/mnt/input.bam"" : No such file or directory. [E::hts_open_format] Failed to open file ""/mnt/input.bam"" : No such file or directory. Traceback (most recent call last):. File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_q7goget_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>. app.run(main). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_q7goget_/runfiles/absl_py/absl/app.py"", line 300, in run. _run_main(main, args). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_q7goget_/runfiles/absl_py/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_q7goget_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 166, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_q7goget_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 128, in default_options.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/613
https://github.com/google/deepvariant/issues/613:5011,deployability,Fail,Failed,5011,"ng/dpv/tmp_dir/Bazel.runfiles_p4oo9k4b/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 134, in assign_sample_name. with sam.SamReader(reads_filenames.split(',')[0]) as sam_reader:. File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_p4oo9k4b/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_p4oo9k4b/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 260, in _native_reader. return NativeSamReader(input_path, **kwargs). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_p4oo9k4b/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 227, in __init__. self._reader = sam_reader.SamReader.from_file(. ValueError: NOT_FOUND: Could not open /mnt/input.bam. [E::hts_open_format] Failed to open file ""/mnt/input.bam"" : No such file or directory. [E::hts_open_format] Failed to open file ""/mnt/input.bam"" : No such file or directory. Traceback (most recent call last):. File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_q7goget_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>. app.run(main). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_q7goget_/runfiles/absl_py/absl/app.py"", line 300, in run. _run_main(main, args). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_q7goget_/runfiles/absl_py/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_q7goget_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 166, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_q7goget_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 128, in default_options. samples_in_order, sample_role_to_train = one_sample_from_flags(. File ""/path1/4_Test/q",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/613
https://github.com/google/deepvariant/issues/613:5256,deployability,modul,module,5256,"ir/Bazel.runfiles_p4oo9k4b/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_p4oo9k4b/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 260, in _native_reader. return NativeSamReader(input_path, **kwargs). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_p4oo9k4b/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 227, in __init__. self._reader = sam_reader.SamReader.from_file(. ValueError: NOT_FOUND: Could not open /mnt/input.bam. [E::hts_open_format] Failed to open file ""/mnt/input.bam"" : No such file or directory. [E::hts_open_format] Failed to open file ""/mnt/input.bam"" : No such file or directory. Traceback (most recent call last):. File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_q7goget_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>. app.run(main). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_q7goget_/runfiles/absl_py/absl/app.py"", line 300, in run. _run_main(main, args). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_q7goget_/runfiles/absl_py/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_q7goget_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 166, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_q7goget_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 128, in default_options. samples_in_order, sample_role_to_train = one_sample_from_flags(. File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_q7goget_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 85, in one_sample_from_flags. sample_name = make_examples_core.assign_sample_name(. File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/613
https://github.com/google/deepvariant/issues/613:7306,deployability,modul,module,7306,"deepvariant/make_examples_core.py"", line 134, in assign_sample_name. with sam.SamReader(reads_filenames.split(',')[0]) as sam_reader:. File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_q7goget_/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_q7goget_/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 260, in _native_reader. return NativeSamReader(input_path, **kwargs). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_q7goget_/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 227, in __init__. self._reader = sam_reader.SamReader.from_file(. ValueError: NOT_FOUND: Could not open /mnt/input.bam. Traceback (most recent call last):. File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_gc9nyr7p/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>. app.run(main). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_gc9nyr7p/runfiles/absl_py/absl/app.py"", line 300, in run. _run_main(main, args). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_gc9nyr7p/runfiles/absl_py/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_gc9nyr7p/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 166, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_gc9nyr7p/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 128, in default_options. samples_in_order, sample_role_to_train = one_sample_from_flags(. File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_gc9nyr7p/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 85, in one_sample_from_flags. sample_name = make_examples_core.assign_sample_name(. File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/613
https://github.com/google/deepvariant/issues/613:9196,deployability,fail,failed,9196,"nt/make_examples.py"", line 166, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_gc9nyr7p/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 128, in default_options. samples_in_order, sample_role_to_train = one_sample_from_flags(. File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_gc9nyr7p/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 85, in one_sample_from_flags. sample_name = make_examples_core.assign_sample_name(. File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_gc9nyr7p/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 134, in assign_sample_name. with sam.SamReader(reads_filenames.split(',')[0]) as sam_reader:. File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_gc9nyr7p/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_gc9nyr7p/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 260, in _native_reader. return NativeSamReader(input_path, **kwargs). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_gc9nyr7p/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 227, in __init__. self._reader = sam_reader.SamReader.from_file(. ValueError: NOT_FOUND: Could not open /mnt/input.bam. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /mnt/QJref.fa --reads /mnt/input.bam --examples /mnt/dpv/make_examples.tfrecord@3.gz --add_hp_channel --alt_aligned_pileup diff_channels --gvcf /mnt/dpv/gvcf.tfrecord@3.gz --max_reads_per_partition 600 --min_mapping_quality 1 --parse_sam_aux_fields --partition_size 25000 --phase_reads --pileup_image_width 199 --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels 0.12 --task 1. real 0m32.021s. user 0m5.396s. sys 0m4.055s. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/613
https://github.com/google/deepvariant/issues/613:1110,energy efficiency,core,core,1110,"ue to consult you. First I verify that my host directory is successfully mounted to the container directory.`/mnt/QJref.fa /mnt/input.bam`. ```. singularity exec -B $TMPDIR:$TMPDIR,""${WORK_DIR}"":/mnt /dellfsqd2/ST_OCEAN/USER/sunzhilong/1_Software/dpv/deepvariant_1.4.0.sif bash. Singularity> cd / && ls. bin boot dellfsqd2 dev environment etc home lib lib32 lib64 libx32 media mnt opt proc root run sbin singularity srv sys tmp usr var. Singularity> cd mnt && ls. QJref.fa input.bam QJref.fa.fai input.bam.bai tmp_dir. ```. Then I ran the following script. ```. cat test0215.sh. WORK_DIR=/path1/4_Test/qingjiang/dpv. export TMPDIR=""$PWD/tmp_dir"". singularity run -B$TMPDIR:$TMPDIR,""${WORK_DIR}"":/mnt \. /path1/1_Software/dpv/deepvariant_1.4.0.sif /opt/deepvariant/bin/run_deepvariant \. --num_shards=3 \. --model_type=PACBIO \. --ref=/mnt/QJref.fa \. --reads=/mnt/input.bam \. --output_vcf=/mnt/output.vcf.gz \. --output_gvcf=/mnt/output.g.vcf.gz \. --intermediate_results_dir /mnt/dpv \. ```. The core error is `ValueError: NOT_FOUND: Could not open /mnt/input.bam. [E::hts_open_format] Failed to open file ""/mnt/input.bam"" : No such file or directory`However, I have verified the existence of `/mnt/input.bam`. The complete error information is as follows. Sincerely look forward to your help! thank you. ```. sh test0215.sh. I0216 00:56:07.446549 140582811191104 run_deepvariant.py:342] Re-using the directory for intermediate results in /mnt/dpv. ***** Intermediate results will be written to /mnt/dpv in docker. ****. ***** Running the command:*****. time seq 0 2 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/mnt/QJref.fa"" --reads ""/mnt/input.bam"" --examples ""/mnt/dpv/make_examples.tfrecord@3.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --gvcf ""/mnt/dpv/gvcf.tfrecord@3.gz"" --max_reads_per_partition ""600"" --min_mapping_quality ""1"" --parse_sam_aux_fields --partition_size ""25000"" --phase_reads --pileup_image_width ""199"" --noreali",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/613
https://github.com/google/deepvariant/issues/613:1711,integrability,buffer,buffer,1711,"qingjiang/dpv. export TMPDIR=""$PWD/tmp_dir"". singularity run -B$TMPDIR:$TMPDIR,""${WORK_DIR}"":/mnt \. /path1/1_Software/dpv/deepvariant_1.4.0.sif /opt/deepvariant/bin/run_deepvariant \. --num_shards=3 \. --model_type=PACBIO \. --ref=/mnt/QJref.fa \. --reads=/mnt/input.bam \. --output_vcf=/mnt/output.vcf.gz \. --output_gvcf=/mnt/output.g.vcf.gz \. --intermediate_results_dir /mnt/dpv \. ```. The core error is `ValueError: NOT_FOUND: Could not open /mnt/input.bam. [E::hts_open_format] Failed to open file ""/mnt/input.bam"" : No such file or directory`However, I have verified the existence of `/mnt/input.bam`. The complete error information is as follows. Sincerely look forward to your help! thank you. ```. sh test0215.sh. I0216 00:56:07.446549 140582811191104 run_deepvariant.py:342] Re-using the directory for intermediate results in /mnt/dpv. ***** Intermediate results will be written to /mnt/dpv in docker. ****. ***** Running the command:*****. time seq 0 2 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/mnt/QJref.fa"" --reads ""/mnt/input.bam"" --examples ""/mnt/dpv/make_examples.tfrecord@3.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --gvcf ""/mnt/dpv/gvcf.tfrecord@3.gz"" --max_reads_per_partition ""600"" --min_mapping_quality ""1"" --parse_sam_aux_fields --partition_size ""25000"" --phase_reads --pileup_image_width ""199"" --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels ""0.12"" --task {}. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LC_CTYPE = ""C.UTF-8"",. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LC_CTYPE = ""C.UTF-8"",. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/613
https://github.com/google/deepvariant/issues/613:2462,interoperability,standard,standard,2462,"82811191104 run_deepvariant.py:342] Re-using the directory for intermediate results in /mnt/dpv. ***** Intermediate results will be written to /mnt/dpv in docker. ****. ***** Running the command:*****. time seq 0 2 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/mnt/QJref.fa"" --reads ""/mnt/input.bam"" --examples ""/mnt/dpv/make_examples.tfrecord@3.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --gvcf ""/mnt/dpv/gvcf.tfrecord@3.gz"" --max_reads_per_partition ""600"" --min_mapping_quality ""1"" --parse_sam_aux_fields --partition_size ""25000"" --phase_reads --pileup_image_width ""199"" --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels ""0.12"" --task {}. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LC_CTYPE = ""C.UTF-8"",. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LC_CTYPE = ""C.UTF-8"",. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). [E::hts_open_format] Failed to open file ""/mnt/input.bam"" : No such file or directory. Traceback (most recent call last):. File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_p4oo9k4b/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>. app.run(main). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_p4oo9k4b/runfiles/absl_py/absl/app.py"", line 300, in run. _run_main(main, args). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_p4oo9k4b/runfiles/absl_py/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_p4oo9k4b/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/613
https://github.com/google/deepvariant/issues/613:2743,interoperability,standard,standard,2743,"mples --mode calling --ref ""/mnt/QJref.fa"" --reads ""/mnt/input.bam"" --examples ""/mnt/dpv/make_examples.tfrecord@3.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --gvcf ""/mnt/dpv/gvcf.tfrecord@3.gz"" --max_reads_per_partition ""600"" --min_mapping_quality ""1"" --parse_sam_aux_fields --partition_size ""25000"" --phase_reads --pileup_image_width ""199"" --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels ""0.12"" --task {}. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LC_CTYPE = ""C.UTF-8"",. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LC_CTYPE = ""C.UTF-8"",. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). [E::hts_open_format] Failed to open file ""/mnt/input.bam"" : No such file or directory. Traceback (most recent call last):. File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_p4oo9k4b/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>. app.run(main). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_p4oo9k4b/runfiles/absl_py/absl/app.py"", line 300, in run. _run_main(main, args). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_p4oo9k4b/runfiles/absl_py/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_p4oo9k4b/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 166, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_p4oo9k4b/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 128, in default_options. samples_in_order, sample_role_to_train = on",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/613
https://github.com/google/deepvariant/issues/613:930,modifiability,PAC,PACBIO,930,"Thanks for your help, I successfully solved the problem of TMPDIR. Then there is a new problem, I hope to continue to consult you. First I verify that my host directory is successfully mounted to the container directory.`/mnt/QJref.fa /mnt/input.bam`. ```. singularity exec -B $TMPDIR:$TMPDIR,""${WORK_DIR}"":/mnt /dellfsqd2/ST_OCEAN/USER/sunzhilong/1_Software/dpv/deepvariant_1.4.0.sif bash. Singularity> cd / && ls. bin boot dellfsqd2 dev environment etc home lib lib32 lib64 libx32 media mnt opt proc root run sbin singularity srv sys tmp usr var. Singularity> cd mnt && ls. QJref.fa input.bam QJref.fa.fai input.bam.bai tmp_dir. ```. Then I ran the following script. ```. cat test0215.sh. WORK_DIR=/path1/4_Test/qingjiang/dpv. export TMPDIR=""$PWD/tmp_dir"". singularity run -B$TMPDIR:$TMPDIR,""${WORK_DIR}"":/mnt \. /path1/1_Software/dpv/deepvariant_1.4.0.sif /opt/deepvariant/bin/run_deepvariant \. --num_shards=3 \. --model_type=PACBIO \. --ref=/mnt/QJref.fa \. --reads=/mnt/input.bam \. --output_vcf=/mnt/output.vcf.gz \. --output_gvcf=/mnt/output.g.vcf.gz \. --intermediate_results_dir /mnt/dpv \. ```. The core error is `ValueError: NOT_FOUND: Could not open /mnt/input.bam. [E::hts_open_format] Failed to open file ""/mnt/input.bam"" : No such file or directory`However, I have verified the existence of `/mnt/input.bam`. The complete error information is as follows. Sincerely look forward to your help! thank you. ```. sh test0215.sh. I0216 00:56:07.446549 140582811191104 run_deepvariant.py:342] Re-using the directory for intermediate results in /mnt/dpv. ***** Intermediate results will be written to /mnt/dpv in docker. ****. ***** Running the command:*****. time seq 0 2 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/mnt/QJref.fa"" --reads ""/mnt/input.bam"" --examples ""/mnt/dpv/make_examples.tfrecord@3.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --gvcf ""/mnt/dpv/gvcf.tfrecord@3.gz"" --max_reads_per_partition ""600"" --min_mappin",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/613
https://github.com/google/deepvariant/issues/613:1529,modifiability,interm,intermediate,1529," tmp usr var. Singularity> cd mnt && ls. QJref.fa input.bam QJref.fa.fai input.bam.bai tmp_dir. ```. Then I ran the following script. ```. cat test0215.sh. WORK_DIR=/path1/4_Test/qingjiang/dpv. export TMPDIR=""$PWD/tmp_dir"". singularity run -B$TMPDIR:$TMPDIR,""${WORK_DIR}"":/mnt \. /path1/1_Software/dpv/deepvariant_1.4.0.sif /opt/deepvariant/bin/run_deepvariant \. --num_shards=3 \. --model_type=PACBIO \. --ref=/mnt/QJref.fa \. --reads=/mnt/input.bam \. --output_vcf=/mnt/output.vcf.gz \. --output_gvcf=/mnt/output.g.vcf.gz \. --intermediate_results_dir /mnt/dpv \. ```. The core error is `ValueError: NOT_FOUND: Could not open /mnt/input.bam. [E::hts_open_format] Failed to open file ""/mnt/input.bam"" : No such file or directory`However, I have verified the existence of `/mnt/input.bam`. The complete error information is as follows. Sincerely look forward to your help! thank you. ```. sh test0215.sh. I0216 00:56:07.446549 140582811191104 run_deepvariant.py:342] Re-using the directory for intermediate results in /mnt/dpv. ***** Intermediate results will be written to /mnt/dpv in docker. ****. ***** Running the command:*****. time seq 0 2 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/mnt/QJref.fa"" --reads ""/mnt/input.bam"" --examples ""/mnt/dpv/make_examples.tfrecord@3.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --gvcf ""/mnt/dpv/gvcf.tfrecord@3.gz"" --max_reads_per_partition ""600"" --min_mapping_quality ""1"" --parse_sam_aux_fields --partition_size ""25000"" --phase_reads --pileup_image_width ""199"" --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels ""0.12"" --task {}. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LC_CTYPE = ""C.UTF-8"",. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warnin",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/613
https://github.com/google/deepvariant/issues/613:1569,modifiability,Interm,Intermediate,1569," QJref.fa input.bam QJref.fa.fai input.bam.bai tmp_dir. ```. Then I ran the following script. ```. cat test0215.sh. WORK_DIR=/path1/4_Test/qingjiang/dpv. export TMPDIR=""$PWD/tmp_dir"". singularity run -B$TMPDIR:$TMPDIR,""${WORK_DIR}"":/mnt \. /path1/1_Software/dpv/deepvariant_1.4.0.sif /opt/deepvariant/bin/run_deepvariant \. --num_shards=3 \. --model_type=PACBIO \. --ref=/mnt/QJref.fa \. --reads=/mnt/input.bam \. --output_vcf=/mnt/output.vcf.gz \. --output_gvcf=/mnt/output.g.vcf.gz \. --intermediate_results_dir /mnt/dpv \. ```. The core error is `ValueError: NOT_FOUND: Could not open /mnt/input.bam. [E::hts_open_format] Failed to open file ""/mnt/input.bam"" : No such file or directory`However, I have verified the existence of `/mnt/input.bam`. The complete error information is as follows. Sincerely look forward to your help! thank you. ```. sh test0215.sh. I0216 00:56:07.446549 140582811191104 run_deepvariant.py:342] Re-using the directory for intermediate results in /mnt/dpv. ***** Intermediate results will be written to /mnt/dpv in docker. ****. ***** Running the command:*****. time seq 0 2 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/mnt/QJref.fa"" --reads ""/mnt/input.bam"" --examples ""/mnt/dpv/make_examples.tfrecord@3.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --gvcf ""/mnt/dpv/gvcf.tfrecord@3.gz"" --max_reads_per_partition ""600"" --min_mapping_quality ""1"" --parse_sam_aux_fields --partition_size ""25000"" --phase_reads --pileup_image_width ""199"" --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels ""0.12"" --task {}. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LC_CTYPE = ""C.UTF-8"",. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale setting",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/613
https://github.com/google/deepvariant/issues/613:3032,modifiability,modul,module,3032,"s --partition_size ""25000"" --phase_reads --pileup_image_width ""199"" --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels ""0.12"" --task {}. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LC_CTYPE = ""C.UTF-8"",. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LC_CTYPE = ""C.UTF-8"",. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). [E::hts_open_format] Failed to open file ""/mnt/input.bam"" : No such file or directory. Traceback (most recent call last):. File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_p4oo9k4b/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>. app.run(main). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_p4oo9k4b/runfiles/absl_py/absl/app.py"", line 300, in run. _run_main(main, args). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_p4oo9k4b/runfiles/absl_py/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_p4oo9k4b/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 166, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_p4oo9k4b/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 128, in default_options. samples_in_order, sample_role_to_train = one_sample_from_flags(. File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_p4oo9k4b/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 85, in one_sample_from_flags. sample_name = make_examples_core.assign_sample_name(. File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/613
https://github.com/google/deepvariant/issues/613:5256,modifiability,modul,module,5256,"ir/Bazel.runfiles_p4oo9k4b/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_p4oo9k4b/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 260, in _native_reader. return NativeSamReader(input_path, **kwargs). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_p4oo9k4b/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 227, in __init__. self._reader = sam_reader.SamReader.from_file(. ValueError: NOT_FOUND: Could not open /mnt/input.bam. [E::hts_open_format] Failed to open file ""/mnt/input.bam"" : No such file or directory. [E::hts_open_format] Failed to open file ""/mnt/input.bam"" : No such file or directory. Traceback (most recent call last):. File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_q7goget_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>. app.run(main). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_q7goget_/runfiles/absl_py/absl/app.py"", line 300, in run. _run_main(main, args). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_q7goget_/runfiles/absl_py/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_q7goget_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 166, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_q7goget_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 128, in default_options. samples_in_order, sample_role_to_train = one_sample_from_flags(. File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_q7goget_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 85, in one_sample_from_flags. sample_name = make_examples_core.assign_sample_name(. File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/613
https://github.com/google/deepvariant/issues/613:7306,modifiability,modul,module,7306,"deepvariant/make_examples_core.py"", line 134, in assign_sample_name. with sam.SamReader(reads_filenames.split(',')[0]) as sam_reader:. File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_q7goget_/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_q7goget_/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 260, in _native_reader. return NativeSamReader(input_path, **kwargs). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_q7goget_/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 227, in __init__. self._reader = sam_reader.SamReader.from_file(. ValueError: NOT_FOUND: Could not open /mnt/input.bam. Traceback (most recent call last):. File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_gc9nyr7p/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>. app.run(main). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_gc9nyr7p/runfiles/absl_py/absl/app.py"", line 300, in run. _run_main(main, args). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_gc9nyr7p/runfiles/absl_py/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_gc9nyr7p/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 166, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_gc9nyr7p/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 128, in default_options. samples_in_order, sample_role_to_train = one_sample_from_flags(. File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_gc9nyr7p/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 85, in one_sample_from_flags. sample_name = make_examples_core.assign_sample_name(. File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/613
https://github.com/google/deepvariant/issues/613:1115,performance,error,error,1115,"consult you. First I verify that my host directory is successfully mounted to the container directory.`/mnt/QJref.fa /mnt/input.bam`. ```. singularity exec -B $TMPDIR:$TMPDIR,""${WORK_DIR}"":/mnt /dellfsqd2/ST_OCEAN/USER/sunzhilong/1_Software/dpv/deepvariant_1.4.0.sif bash. Singularity> cd / && ls. bin boot dellfsqd2 dev environment etc home lib lib32 lib64 libx32 media mnt opt proc root run sbin singularity srv sys tmp usr var. Singularity> cd mnt && ls. QJref.fa input.bam QJref.fa.fai input.bam.bai tmp_dir. ```. Then I ran the following script. ```. cat test0215.sh. WORK_DIR=/path1/4_Test/qingjiang/dpv. export TMPDIR=""$PWD/tmp_dir"". singularity run -B$TMPDIR:$TMPDIR,""${WORK_DIR}"":/mnt \. /path1/1_Software/dpv/deepvariant_1.4.0.sif /opt/deepvariant/bin/run_deepvariant \. --num_shards=3 \. --model_type=PACBIO \. --ref=/mnt/QJref.fa \. --reads=/mnt/input.bam \. --output_vcf=/mnt/output.vcf.gz \. --output_gvcf=/mnt/output.g.vcf.gz \. --intermediate_results_dir /mnt/dpv \. ```. The core error is `ValueError: NOT_FOUND: Could not open /mnt/input.bam. [E::hts_open_format] Failed to open file ""/mnt/input.bam"" : No such file or directory`However, I have verified the existence of `/mnt/input.bam`. The complete error information is as follows. Sincerely look forward to your help! thank you. ```. sh test0215.sh. I0216 00:56:07.446549 140582811191104 run_deepvariant.py:342] Re-using the directory for intermediate results in /mnt/dpv. ***** Intermediate results will be written to /mnt/dpv in docker. ****. ***** Running the command:*****. time seq 0 2 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/mnt/QJref.fa"" --reads ""/mnt/input.bam"" --examples ""/mnt/dpv/make_examples.tfrecord@3.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --gvcf ""/mnt/dpv/gvcf.tfrecord@3.gz"" --max_reads_per_partition ""600"" --min_mapping_quality ""1"" --parse_sam_aux_fields --partition_size ""25000"" --phase_reads --pileup_image_width ""199"" --norealign_rea",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/613
https://github.com/google/deepvariant/issues/613:1338,performance,error,error,1338,"hilong/1_Software/dpv/deepvariant_1.4.0.sif bash. Singularity> cd / && ls. bin boot dellfsqd2 dev environment etc home lib lib32 lib64 libx32 media mnt opt proc root run sbin singularity srv sys tmp usr var. Singularity> cd mnt && ls. QJref.fa input.bam QJref.fa.fai input.bam.bai tmp_dir. ```. Then I ran the following script. ```. cat test0215.sh. WORK_DIR=/path1/4_Test/qingjiang/dpv. export TMPDIR=""$PWD/tmp_dir"". singularity run -B$TMPDIR:$TMPDIR,""${WORK_DIR}"":/mnt \. /path1/1_Software/dpv/deepvariant_1.4.0.sif /opt/deepvariant/bin/run_deepvariant \. --num_shards=3 \. --model_type=PACBIO \. --ref=/mnt/QJref.fa \. --reads=/mnt/input.bam \. --output_vcf=/mnt/output.vcf.gz \. --output_gvcf=/mnt/output.g.vcf.gz \. --intermediate_results_dir /mnt/dpv \. ```. The core error is `ValueError: NOT_FOUND: Could not open /mnt/input.bam. [E::hts_open_format] Failed to open file ""/mnt/input.bam"" : No such file or directory`However, I have verified the existence of `/mnt/input.bam`. The complete error information is as follows. Sincerely look forward to your help! thank you. ```. sh test0215.sh. I0216 00:56:07.446549 140582811191104 run_deepvariant.py:342] Re-using the directory for intermediate results in /mnt/dpv. ***** Intermediate results will be written to /mnt/dpv in docker. ****. ***** Running the command:*****. time seq 0 2 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/mnt/QJref.fa"" --reads ""/mnt/input.bam"" --examples ""/mnt/dpv/make_examples.tfrecord@3.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --gvcf ""/mnt/dpv/gvcf.tfrecord@3.gz"" --max_reads_per_partition ""600"" --min_mapping_quality ""1"" --parse_sam_aux_fields --partition_size ""25000"" --phase_reads --pileup_image_width ""199"" --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels ""0.12"" --task {}. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LC_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/613
https://github.com/google/deepvariant/issues/613:1668,performance,time,time,1668,"``. cat test0215.sh. WORK_DIR=/path1/4_Test/qingjiang/dpv. export TMPDIR=""$PWD/tmp_dir"". singularity run -B$TMPDIR:$TMPDIR,""${WORK_DIR}"":/mnt \. /path1/1_Software/dpv/deepvariant_1.4.0.sif /opt/deepvariant/bin/run_deepvariant \. --num_shards=3 \. --model_type=PACBIO \. --ref=/mnt/QJref.fa \. --reads=/mnt/input.bam \. --output_vcf=/mnt/output.vcf.gz \. --output_gvcf=/mnt/output.g.vcf.gz \. --intermediate_results_dir /mnt/dpv \. ```. The core error is `ValueError: NOT_FOUND: Could not open /mnt/input.bam. [E::hts_open_format] Failed to open file ""/mnt/input.bam"" : No such file or directory`However, I have verified the existence of `/mnt/input.bam`. The complete error information is as follows. Sincerely look forward to your help! thank you. ```. sh test0215.sh. I0216 00:56:07.446549 140582811191104 run_deepvariant.py:342] Re-using the directory for intermediate results in /mnt/dpv. ***** Intermediate results will be written to /mnt/dpv in docker. ****. ***** Running the command:*****. time seq 0 2 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/mnt/QJref.fa"" --reads ""/mnt/input.bam"" --examples ""/mnt/dpv/make_examples.tfrecord@3.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --gvcf ""/mnt/dpv/gvcf.tfrecord@3.gz"" --max_reads_per_partition ""600"" --min_mapping_quality ""1"" --parse_sam_aux_fields --partition_size ""25000"" --phase_reads --pileup_image_width ""199"" --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels ""0.12"" --task {}. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LC_CTYPE = ""C.UTF-8"",. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LC_CTYPE = ""C.UTF-8"",. LANG = ""en_US.UTF-8"". are su",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/613
https://github.com/google/deepvariant/issues/613:1683,performance,parallel,parallel,1683,"sh. WORK_DIR=/path1/4_Test/qingjiang/dpv. export TMPDIR=""$PWD/tmp_dir"". singularity run -B$TMPDIR:$TMPDIR,""${WORK_DIR}"":/mnt \. /path1/1_Software/dpv/deepvariant_1.4.0.sif /opt/deepvariant/bin/run_deepvariant \. --num_shards=3 \. --model_type=PACBIO \. --ref=/mnt/QJref.fa \. --reads=/mnt/input.bam \. --output_vcf=/mnt/output.vcf.gz \. --output_gvcf=/mnt/output.g.vcf.gz \. --intermediate_results_dir /mnt/dpv \. ```. The core error is `ValueError: NOT_FOUND: Could not open /mnt/input.bam. [E::hts_open_format] Failed to open file ""/mnt/input.bam"" : No such file or directory`However, I have verified the existence of `/mnt/input.bam`. The complete error information is as follows. Sincerely look forward to your help! thank you. ```. sh test0215.sh. I0216 00:56:07.446549 140582811191104 run_deepvariant.py:342] Re-using the directory for intermediate results in /mnt/dpv. ***** Intermediate results will be written to /mnt/dpv in docker. ****. ***** Running the command:*****. time seq 0 2 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/mnt/QJref.fa"" --reads ""/mnt/input.bam"" --examples ""/mnt/dpv/make_examples.tfrecord@3.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --gvcf ""/mnt/dpv/gvcf.tfrecord@3.gz"" --max_reads_per_partition ""600"" --min_mapping_quality ""1"" --parse_sam_aux_fields --partition_size ""25000"" --phase_reads --pileup_image_width ""199"" --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels ""0.12"" --task {}. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LC_CTYPE = ""C.UTF-8"",. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LC_CTYPE = ""C.UTF-8"",. LANG = ""en_US.UTF-8"". are supported and insta",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/613
https://github.com/google/deepvariant/issues/613:9177,performance,parallel,parallel,9177,"nt/make_examples.py"", line 166, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_gc9nyr7p/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 128, in default_options. samples_in_order, sample_role_to_train = one_sample_from_flags(. File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_gc9nyr7p/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 85, in one_sample_from_flags. sample_name = make_examples_core.assign_sample_name(. File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_gc9nyr7p/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 134, in assign_sample_name. with sam.SamReader(reads_filenames.split(',')[0]) as sam_reader:. File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_gc9nyr7p/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_gc9nyr7p/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 260, in _native_reader. return NativeSamReader(input_path, **kwargs). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_gc9nyr7p/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 227, in __init__. self._reader = sam_reader.SamReader.from_file(. ValueError: NOT_FOUND: Could not open /mnt/input.bam. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /mnt/QJref.fa --reads /mnt/input.bam --examples /mnt/dpv/make_examples.tfrecord@3.gz --add_hp_channel --alt_aligned_pileup diff_channels --gvcf /mnt/dpv/gvcf.tfrecord@3.gz --max_reads_per_partition 600 --min_mapping_quality 1 --parse_sam_aux_fields --partition_size 25000 --phase_reads --pileup_image_width 199 --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels 0.12 --task 1. real 0m32.021s. user 0m5.396s. sys 0m4.055s. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/613
https://github.com/google/deepvariant/issues/613:1200,reliability,Fail,Failed,1200,"tainer directory.`/mnt/QJref.fa /mnt/input.bam`. ```. singularity exec -B $TMPDIR:$TMPDIR,""${WORK_DIR}"":/mnt /dellfsqd2/ST_OCEAN/USER/sunzhilong/1_Software/dpv/deepvariant_1.4.0.sif bash. Singularity> cd / && ls. bin boot dellfsqd2 dev environment etc home lib lib32 lib64 libx32 media mnt opt proc root run sbin singularity srv sys tmp usr var. Singularity> cd mnt && ls. QJref.fa input.bam QJref.fa.fai input.bam.bai tmp_dir. ```. Then I ran the following script. ```. cat test0215.sh. WORK_DIR=/path1/4_Test/qingjiang/dpv. export TMPDIR=""$PWD/tmp_dir"". singularity run -B$TMPDIR:$TMPDIR,""${WORK_DIR}"":/mnt \. /path1/1_Software/dpv/deepvariant_1.4.0.sif /opt/deepvariant/bin/run_deepvariant \. --num_shards=3 \. --model_type=PACBIO \. --ref=/mnt/QJref.fa \. --reads=/mnt/input.bam \. --output_vcf=/mnt/output.vcf.gz \. --output_gvcf=/mnt/output.g.vcf.gz \. --intermediate_results_dir /mnt/dpv \. ```. The core error is `ValueError: NOT_FOUND: Could not open /mnt/input.bam. [E::hts_open_format] Failed to open file ""/mnt/input.bam"" : No such file or directory`However, I have verified the existence of `/mnt/input.bam`. The complete error information is as follows. Sincerely look forward to your help! thank you. ```. sh test0215.sh. I0216 00:56:07.446549 140582811191104 run_deepvariant.py:342] Re-using the directory for intermediate results in /mnt/dpv. ***** Intermediate results will be written to /mnt/dpv in docker. ****. ***** Running the command:*****. time seq 0 2 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/mnt/QJref.fa"" --reads ""/mnt/input.bam"" --examples ""/mnt/dpv/make_examples.tfrecord@3.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --gvcf ""/mnt/dpv/gvcf.tfrecord@3.gz"" --max_reads_per_partition ""600"" --min_mapping_quality ""1"" --parse_sam_aux_fields --partition_size ""25000"" --phase_reads --pileup_image_width ""199"" --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels ""0.12"" --task {}.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/613
https://github.com/google/deepvariant/issues/613:2234,reliability,fail,failed,2234,": No such file or directory`However, I have verified the existence of `/mnt/input.bam`. The complete error information is as follows. Sincerely look forward to your help! thank you. ```. sh test0215.sh. I0216 00:56:07.446549 140582811191104 run_deepvariant.py:342] Re-using the directory for intermediate results in /mnt/dpv. ***** Intermediate results will be written to /mnt/dpv in docker. ****. ***** Running the command:*****. time seq 0 2 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/mnt/QJref.fa"" --reads ""/mnt/input.bam"" --examples ""/mnt/dpv/make_examples.tfrecord@3.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --gvcf ""/mnt/dpv/gvcf.tfrecord@3.gz"" --max_reads_per_partition ""600"" --min_mapping_quality ""1"" --parse_sam_aux_fields --partition_size ""25000"" --phase_reads --pileup_image_width ""199"" --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels ""0.12"" --task {}. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LC_CTYPE = ""C.UTF-8"",. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LC_CTYPE = ""C.UTF-8"",. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). [E::hts_open_format] Failed to open file ""/mnt/input.bam"" : No such file or directory. Traceback (most recent call last):. File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_p4oo9k4b/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>. app.run(main). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_p4oo9k4b/runfiles/absl_py/absl/app.py"", line 300, in run. _run_main(main, args). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/B",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/613
https://github.com/google/deepvariant/issues/613:2515,reliability,fail,failed,2515,"ectory for intermediate results in /mnt/dpv. ***** Intermediate results will be written to /mnt/dpv in docker. ****. ***** Running the command:*****. time seq 0 2 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/mnt/QJref.fa"" --reads ""/mnt/input.bam"" --examples ""/mnt/dpv/make_examples.tfrecord@3.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --gvcf ""/mnt/dpv/gvcf.tfrecord@3.gz"" --max_reads_per_partition ""600"" --min_mapping_quality ""1"" --parse_sam_aux_fields --partition_size ""25000"" --phase_reads --pileup_image_width ""199"" --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels ""0.12"" --task {}. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LC_CTYPE = ""C.UTF-8"",. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LC_CTYPE = ""C.UTF-8"",. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). [E::hts_open_format] Failed to open file ""/mnt/input.bam"" : No such file or directory. Traceback (most recent call last):. File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_p4oo9k4b/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>. app.run(main). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_p4oo9k4b/runfiles/absl_py/absl/app.py"", line 300, in run. _run_main(main, args). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_p4oo9k4b/runfiles/absl_py/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_p4oo9k4b/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 166, in main. options = default_options(add_fla",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/613
https://github.com/google/deepvariant/issues/613:2787,reliability,Fail,Failed,2787,"--reads ""/mnt/input.bam"" --examples ""/mnt/dpv/make_examples.tfrecord@3.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --gvcf ""/mnt/dpv/gvcf.tfrecord@3.gz"" --max_reads_per_partition ""600"" --min_mapping_quality ""1"" --parse_sam_aux_fields --partition_size ""25000"" --phase_reads --pileup_image_width ""199"" --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels ""0.12"" --task {}. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LC_CTYPE = ""C.UTF-8"",. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LC_CTYPE = ""C.UTF-8"",. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). [E::hts_open_format] Failed to open file ""/mnt/input.bam"" : No such file or directory. Traceback (most recent call last):. File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_p4oo9k4b/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>. app.run(main). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_p4oo9k4b/runfiles/absl_py/absl/app.py"", line 300, in run. _run_main(main, args). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_p4oo9k4b/runfiles/absl_py/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_p4oo9k4b/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 166, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_p4oo9k4b/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 128, in default_options. samples_in_order, sample_role_to_train = one_sample_from_flags(. File ""/path1/4_Test/q",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/613
https://github.com/google/deepvariant/issues/613:4924,reliability,Fail,Failed,4924,"lags. sample_name = make_examples_core.assign_sample_name(. File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_p4oo9k4b/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 134, in assign_sample_name. with sam.SamReader(reads_filenames.split(',')[0]) as sam_reader:. File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_p4oo9k4b/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_p4oo9k4b/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 260, in _native_reader. return NativeSamReader(input_path, **kwargs). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_p4oo9k4b/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 227, in __init__. self._reader = sam_reader.SamReader.from_file(. ValueError: NOT_FOUND: Could not open /mnt/input.bam. [E::hts_open_format] Failed to open file ""/mnt/input.bam"" : No such file or directory. [E::hts_open_format] Failed to open file ""/mnt/input.bam"" : No such file or directory. Traceback (most recent call last):. File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_q7goget_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>. app.run(main). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_q7goget_/runfiles/absl_py/absl/app.py"", line 300, in run. _run_main(main, args). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_q7goget_/runfiles/absl_py/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_q7goget_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 166, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_q7goget_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 128, in default_options.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/613
https://github.com/google/deepvariant/issues/613:5011,reliability,Fail,Failed,5011,"ng/dpv/tmp_dir/Bazel.runfiles_p4oo9k4b/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 134, in assign_sample_name. with sam.SamReader(reads_filenames.split(',')[0]) as sam_reader:. File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_p4oo9k4b/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_p4oo9k4b/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 260, in _native_reader. return NativeSamReader(input_path, **kwargs). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_p4oo9k4b/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 227, in __init__. self._reader = sam_reader.SamReader.from_file(. ValueError: NOT_FOUND: Could not open /mnt/input.bam. [E::hts_open_format] Failed to open file ""/mnt/input.bam"" : No such file or directory. [E::hts_open_format] Failed to open file ""/mnt/input.bam"" : No such file or directory. Traceback (most recent call last):. File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_q7goget_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>. app.run(main). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_q7goget_/runfiles/absl_py/absl/app.py"", line 300, in run. _run_main(main, args). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_q7goget_/runfiles/absl_py/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_q7goget_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 166, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_q7goget_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 128, in default_options. samples_in_order, sample_role_to_train = one_sample_from_flags(. File ""/path1/4_Test/q",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/613
https://github.com/google/deepvariant/issues/613:9196,reliability,fail,failed,9196,"nt/make_examples.py"", line 166, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_gc9nyr7p/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 128, in default_options. samples_in_order, sample_role_to_train = one_sample_from_flags(. File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_gc9nyr7p/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 85, in one_sample_from_flags. sample_name = make_examples_core.assign_sample_name(. File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_gc9nyr7p/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 134, in assign_sample_name. with sam.SamReader(reads_filenames.split(',')[0]) as sam_reader:. File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_gc9nyr7p/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_gc9nyr7p/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 260, in _native_reader. return NativeSamReader(input_path, **kwargs). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_gc9nyr7p/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 227, in __init__. self._reader = sam_reader.SamReader.from_file(. ValueError: NOT_FOUND: Could not open /mnt/input.bam. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /mnt/QJref.fa --reads /mnt/input.bam --examples /mnt/dpv/make_examples.tfrecord@3.gz --add_hp_channel --alt_aligned_pileup diff_channels --gvcf /mnt/dpv/gvcf.tfrecord@3.gz --max_reads_per_partition 600 --min_mapping_quality 1 --parse_sam_aux_fields --partition_size 25000 --phase_reads --pileup_image_width 199 --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels 0.12 --task 1. real 0m32.021s. user 0m5.396s. sys 0m4.055s. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/613
https://github.com/google/deepvariant/issues/613:240,safety,input,input,240,"Thanks for your help, I successfully solved the problem of TMPDIR. Then there is a new problem, I hope to continue to consult you. First I verify that my host directory is successfully mounted to the container directory.`/mnt/QJref.fa /mnt/input.bam`. ```. singularity exec -B $TMPDIR:$TMPDIR,""${WORK_DIR}"":/mnt /dellfsqd2/ST_OCEAN/USER/sunzhilong/1_Software/dpv/deepvariant_1.4.0.sif bash. Singularity> cd / && ls. bin boot dellfsqd2 dev environment etc home lib lib32 lib64 libx32 media mnt opt proc root run sbin singularity srv sys tmp usr var. Singularity> cd mnt && ls. QJref.fa input.bam QJref.fa.fai input.bam.bai tmp_dir. ```. Then I ran the following script. ```. cat test0215.sh. WORK_DIR=/path1/4_Test/qingjiang/dpv. export TMPDIR=""$PWD/tmp_dir"". singularity run -B$TMPDIR:$TMPDIR,""${WORK_DIR}"":/mnt \. /path1/1_Software/dpv/deepvariant_1.4.0.sif /opt/deepvariant/bin/run_deepvariant \. --num_shards=3 \. --model_type=PACBIO \. --ref=/mnt/QJref.fa \. --reads=/mnt/input.bam \. --output_vcf=/mnt/output.vcf.gz \. --output_gvcf=/mnt/output.g.vcf.gz \. --intermediate_results_dir /mnt/dpv \. ```. The core error is `ValueError: NOT_FOUND: Could not open /mnt/input.bam. [E::hts_open_format] Failed to open file ""/mnt/input.bam"" : No such file or directory`However, I have verified the existence of `/mnt/input.bam`. The complete error information is as follows. Sincerely look forward to your help! thank you. ```. sh test0215.sh. I0216 00:56:07.446549 140582811191104 run_deepvariant.py:342] Re-using the directory for intermediate results in /mnt/dpv. ***** Intermediate results will be written to /mnt/dpv in docker. ****. ***** Running the command:*****. time seq 0 2 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/mnt/QJref.fa"" --reads ""/mnt/input.bam"" --examples ""/mnt/dpv/make_examples.tfrecord@3.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --gvcf ""/mnt/dpv/gvcf.tfrecord@3.gz"" --max_reads_per_partition ""600"" --min_mappin",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/613
https://github.com/google/deepvariant/issues/613:585,safety,input,input,585,"Thanks for your help, I successfully solved the problem of TMPDIR. Then there is a new problem, I hope to continue to consult you. First I verify that my host directory is successfully mounted to the container directory.`/mnt/QJref.fa /mnt/input.bam`. ```. singularity exec -B $TMPDIR:$TMPDIR,""${WORK_DIR}"":/mnt /dellfsqd2/ST_OCEAN/USER/sunzhilong/1_Software/dpv/deepvariant_1.4.0.sif bash. Singularity> cd / && ls. bin boot dellfsqd2 dev environment etc home lib lib32 lib64 libx32 media mnt opt proc root run sbin singularity srv sys tmp usr var. Singularity> cd mnt && ls. QJref.fa input.bam QJref.fa.fai input.bam.bai tmp_dir. ```. Then I ran the following script. ```. cat test0215.sh. WORK_DIR=/path1/4_Test/qingjiang/dpv. export TMPDIR=""$PWD/tmp_dir"". singularity run -B$TMPDIR:$TMPDIR,""${WORK_DIR}"":/mnt \. /path1/1_Software/dpv/deepvariant_1.4.0.sif /opt/deepvariant/bin/run_deepvariant \. --num_shards=3 \. --model_type=PACBIO \. --ref=/mnt/QJref.fa \. --reads=/mnt/input.bam \. --output_vcf=/mnt/output.vcf.gz \. --output_gvcf=/mnt/output.g.vcf.gz \. --intermediate_results_dir /mnt/dpv \. ```. The core error is `ValueError: NOT_FOUND: Could not open /mnt/input.bam. [E::hts_open_format] Failed to open file ""/mnt/input.bam"" : No such file or directory`However, I have verified the existence of `/mnt/input.bam`. The complete error information is as follows. Sincerely look forward to your help! thank you. ```. sh test0215.sh. I0216 00:56:07.446549 140582811191104 run_deepvariant.py:342] Re-using the directory for intermediate results in /mnt/dpv. ***** Intermediate results will be written to /mnt/dpv in docker. ****. ***** Running the command:*****. time seq 0 2 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/mnt/QJref.fa"" --reads ""/mnt/input.bam"" --examples ""/mnt/dpv/make_examples.tfrecord@3.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --gvcf ""/mnt/dpv/gvcf.tfrecord@3.gz"" --max_reads_per_partition ""600"" --min_mappin",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/613
https://github.com/google/deepvariant/issues/613:608,safety,input,input,608,"Thanks for your help, I successfully solved the problem of TMPDIR. Then there is a new problem, I hope to continue to consult you. First I verify that my host directory is successfully mounted to the container directory.`/mnt/QJref.fa /mnt/input.bam`. ```. singularity exec -B $TMPDIR:$TMPDIR,""${WORK_DIR}"":/mnt /dellfsqd2/ST_OCEAN/USER/sunzhilong/1_Software/dpv/deepvariant_1.4.0.sif bash. Singularity> cd / && ls. bin boot dellfsqd2 dev environment etc home lib lib32 lib64 libx32 media mnt opt proc root run sbin singularity srv sys tmp usr var. Singularity> cd mnt && ls. QJref.fa input.bam QJref.fa.fai input.bam.bai tmp_dir. ```. Then I ran the following script. ```. cat test0215.sh. WORK_DIR=/path1/4_Test/qingjiang/dpv. export TMPDIR=""$PWD/tmp_dir"". singularity run -B$TMPDIR:$TMPDIR,""${WORK_DIR}"":/mnt \. /path1/1_Software/dpv/deepvariant_1.4.0.sif /opt/deepvariant/bin/run_deepvariant \. --num_shards=3 \. --model_type=PACBIO \. --ref=/mnt/QJref.fa \. --reads=/mnt/input.bam \. --output_vcf=/mnt/output.vcf.gz \. --output_gvcf=/mnt/output.g.vcf.gz \. --intermediate_results_dir /mnt/dpv \. ```. The core error is `ValueError: NOT_FOUND: Could not open /mnt/input.bam. [E::hts_open_format] Failed to open file ""/mnt/input.bam"" : No such file or directory`However, I have verified the existence of `/mnt/input.bam`. The complete error information is as follows. Sincerely look forward to your help! thank you. ```. sh test0215.sh. I0216 00:56:07.446549 140582811191104 run_deepvariant.py:342] Re-using the directory for intermediate results in /mnt/dpv. ***** Intermediate results will be written to /mnt/dpv in docker. ****. ***** Running the command:*****. time seq 0 2 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/mnt/QJref.fa"" --reads ""/mnt/input.bam"" --examples ""/mnt/dpv/make_examples.tfrecord@3.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --gvcf ""/mnt/dpv/gvcf.tfrecord@3.gz"" --max_reads_per_partition ""600"" --min_mappin",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/613
https://github.com/google/deepvariant/issues/613:976,safety,input,input,976,"Thanks for your help, I successfully solved the problem of TMPDIR. Then there is a new problem, I hope to continue to consult you. First I verify that my host directory is successfully mounted to the container directory.`/mnt/QJref.fa /mnt/input.bam`. ```. singularity exec -B $TMPDIR:$TMPDIR,""${WORK_DIR}"":/mnt /dellfsqd2/ST_OCEAN/USER/sunzhilong/1_Software/dpv/deepvariant_1.4.0.sif bash. Singularity> cd / && ls. bin boot dellfsqd2 dev environment etc home lib lib32 lib64 libx32 media mnt opt proc root run sbin singularity srv sys tmp usr var. Singularity> cd mnt && ls. QJref.fa input.bam QJref.fa.fai input.bam.bai tmp_dir. ```. Then I ran the following script. ```. cat test0215.sh. WORK_DIR=/path1/4_Test/qingjiang/dpv. export TMPDIR=""$PWD/tmp_dir"". singularity run -B$TMPDIR:$TMPDIR,""${WORK_DIR}"":/mnt \. /path1/1_Software/dpv/deepvariant_1.4.0.sif /opt/deepvariant/bin/run_deepvariant \. --num_shards=3 \. --model_type=PACBIO \. --ref=/mnt/QJref.fa \. --reads=/mnt/input.bam \. --output_vcf=/mnt/output.vcf.gz \. --output_gvcf=/mnt/output.g.vcf.gz \. --intermediate_results_dir /mnt/dpv \. ```. The core error is `ValueError: NOT_FOUND: Could not open /mnt/input.bam. [E::hts_open_format] Failed to open file ""/mnt/input.bam"" : No such file or directory`However, I have verified the existence of `/mnt/input.bam`. The complete error information is as follows. Sincerely look forward to your help! thank you. ```. sh test0215.sh. I0216 00:56:07.446549 140582811191104 run_deepvariant.py:342] Re-using the directory for intermediate results in /mnt/dpv. ***** Intermediate results will be written to /mnt/dpv in docker. ****. ***** Running the command:*****. time seq 0 2 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/mnt/QJref.fa"" --reads ""/mnt/input.bam"" --examples ""/mnt/dpv/make_examples.tfrecord@3.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --gvcf ""/mnt/dpv/gvcf.tfrecord@3.gz"" --max_reads_per_partition ""600"" --min_mappin",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/613
https://github.com/google/deepvariant/issues/613:1115,safety,error,error,1115,"consult you. First I verify that my host directory is successfully mounted to the container directory.`/mnt/QJref.fa /mnt/input.bam`. ```. singularity exec -B $TMPDIR:$TMPDIR,""${WORK_DIR}"":/mnt /dellfsqd2/ST_OCEAN/USER/sunzhilong/1_Software/dpv/deepvariant_1.4.0.sif bash. Singularity> cd / && ls. bin boot dellfsqd2 dev environment etc home lib lib32 lib64 libx32 media mnt opt proc root run sbin singularity srv sys tmp usr var. Singularity> cd mnt && ls. QJref.fa input.bam QJref.fa.fai input.bam.bai tmp_dir. ```. Then I ran the following script. ```. cat test0215.sh. WORK_DIR=/path1/4_Test/qingjiang/dpv. export TMPDIR=""$PWD/tmp_dir"". singularity run -B$TMPDIR:$TMPDIR,""${WORK_DIR}"":/mnt \. /path1/1_Software/dpv/deepvariant_1.4.0.sif /opt/deepvariant/bin/run_deepvariant \. --num_shards=3 \. --model_type=PACBIO \. --ref=/mnt/QJref.fa \. --reads=/mnt/input.bam \. --output_vcf=/mnt/output.vcf.gz \. --output_gvcf=/mnt/output.g.vcf.gz \. --intermediate_results_dir /mnt/dpv \. ```. The core error is `ValueError: NOT_FOUND: Could not open /mnt/input.bam. [E::hts_open_format] Failed to open file ""/mnt/input.bam"" : No such file or directory`However, I have verified the existence of `/mnt/input.bam`. The complete error information is as follows. Sincerely look forward to your help! thank you. ```. sh test0215.sh. I0216 00:56:07.446549 140582811191104 run_deepvariant.py:342] Re-using the directory for intermediate results in /mnt/dpv. ***** Intermediate results will be written to /mnt/dpv in docker. ****. ***** Running the command:*****. time seq 0 2 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/mnt/QJref.fa"" --reads ""/mnt/input.bam"" --examples ""/mnt/dpv/make_examples.tfrecord@3.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --gvcf ""/mnt/dpv/gvcf.tfrecord@3.gz"" --max_reads_per_partition ""600"" --min_mapping_quality ""1"" --parse_sam_aux_fields --partition_size ""25000"" --phase_reads --pileup_image_width ""199"" --norealign_rea",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/613
https://github.com/google/deepvariant/issues/613:1168,safety,input,input,1168," successfully mounted to the container directory.`/mnt/QJref.fa /mnt/input.bam`. ```. singularity exec -B $TMPDIR:$TMPDIR,""${WORK_DIR}"":/mnt /dellfsqd2/ST_OCEAN/USER/sunzhilong/1_Software/dpv/deepvariant_1.4.0.sif bash. Singularity> cd / && ls. bin boot dellfsqd2 dev environment etc home lib lib32 lib64 libx32 media mnt opt proc root run sbin singularity srv sys tmp usr var. Singularity> cd mnt && ls. QJref.fa input.bam QJref.fa.fai input.bam.bai tmp_dir. ```. Then I ran the following script. ```. cat test0215.sh. WORK_DIR=/path1/4_Test/qingjiang/dpv. export TMPDIR=""$PWD/tmp_dir"". singularity run -B$TMPDIR:$TMPDIR,""${WORK_DIR}"":/mnt \. /path1/1_Software/dpv/deepvariant_1.4.0.sif /opt/deepvariant/bin/run_deepvariant \. --num_shards=3 \. --model_type=PACBIO \. --ref=/mnt/QJref.fa \. --reads=/mnt/input.bam \. --output_vcf=/mnt/output.vcf.gz \. --output_gvcf=/mnt/output.g.vcf.gz \. --intermediate_results_dir /mnt/dpv \. ```. The core error is `ValueError: NOT_FOUND: Could not open /mnt/input.bam. [E::hts_open_format] Failed to open file ""/mnt/input.bam"" : No such file or directory`However, I have verified the existence of `/mnt/input.bam`. The complete error information is as follows. Sincerely look forward to your help! thank you. ```. sh test0215.sh. I0216 00:56:07.446549 140582811191104 run_deepvariant.py:342] Re-using the directory for intermediate results in /mnt/dpv. ***** Intermediate results will be written to /mnt/dpv in docker. ****. ***** Running the command:*****. time seq 0 2 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/mnt/QJref.fa"" --reads ""/mnt/input.bam"" --examples ""/mnt/dpv/make_examples.tfrecord@3.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --gvcf ""/mnt/dpv/gvcf.tfrecord@3.gz"" --max_reads_per_partition ""600"" --min_mapping_quality ""1"" --parse_sam_aux_fields --partition_size ""25000"" --phase_reads --pileup_image_width ""199"" --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_f",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/613
https://github.com/google/deepvariant/issues/613:1226,safety,input,input,1226,"ef.fa /mnt/input.bam`. ```. singularity exec -B $TMPDIR:$TMPDIR,""${WORK_DIR}"":/mnt /dellfsqd2/ST_OCEAN/USER/sunzhilong/1_Software/dpv/deepvariant_1.4.0.sif bash. Singularity> cd / && ls. bin boot dellfsqd2 dev environment etc home lib lib32 lib64 libx32 media mnt opt proc root run sbin singularity srv sys tmp usr var. Singularity> cd mnt && ls. QJref.fa input.bam QJref.fa.fai input.bam.bai tmp_dir. ```. Then I ran the following script. ```. cat test0215.sh. WORK_DIR=/path1/4_Test/qingjiang/dpv. export TMPDIR=""$PWD/tmp_dir"". singularity run -B$TMPDIR:$TMPDIR,""${WORK_DIR}"":/mnt \. /path1/1_Software/dpv/deepvariant_1.4.0.sif /opt/deepvariant/bin/run_deepvariant \. --num_shards=3 \. --model_type=PACBIO \. --ref=/mnt/QJref.fa \. --reads=/mnt/input.bam \. --output_vcf=/mnt/output.vcf.gz \. --output_gvcf=/mnt/output.g.vcf.gz \. --intermediate_results_dir /mnt/dpv \. ```. The core error is `ValueError: NOT_FOUND: Could not open /mnt/input.bam. [E::hts_open_format] Failed to open file ""/mnt/input.bam"" : No such file or directory`However, I have verified the existence of `/mnt/input.bam`. The complete error information is as follows. Sincerely look forward to your help! thank you. ```. sh test0215.sh. I0216 00:56:07.446549 140582811191104 run_deepvariant.py:342] Re-using the directory for intermediate results in /mnt/dpv. ***** Intermediate results will be written to /mnt/dpv in docker. ****. ***** Running the command:*****. time seq 0 2 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/mnt/QJref.fa"" --reads ""/mnt/input.bam"" --examples ""/mnt/dpv/make_examples.tfrecord@3.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --gvcf ""/mnt/dpv/gvcf.tfrecord@3.gz"" --max_reads_per_partition ""600"" --min_mapping_quality ""1"" --parse_sam_aux_fields --partition_size ""25000"" --phase_reads --pileup_image_width ""199"" --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels ""0.12"" --task {}. perl: warning: Setting lo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/613
https://github.com/google/deepvariant/issues/613:1313,safety,input,input,1313,"lfsqd2/ST_OCEAN/USER/sunzhilong/1_Software/dpv/deepvariant_1.4.0.sif bash. Singularity> cd / && ls. bin boot dellfsqd2 dev environment etc home lib lib32 lib64 libx32 media mnt opt proc root run sbin singularity srv sys tmp usr var. Singularity> cd mnt && ls. QJref.fa input.bam QJref.fa.fai input.bam.bai tmp_dir. ```. Then I ran the following script. ```. cat test0215.sh. WORK_DIR=/path1/4_Test/qingjiang/dpv. export TMPDIR=""$PWD/tmp_dir"". singularity run -B$TMPDIR:$TMPDIR,""${WORK_DIR}"":/mnt \. /path1/1_Software/dpv/deepvariant_1.4.0.sif /opt/deepvariant/bin/run_deepvariant \. --num_shards=3 \. --model_type=PACBIO \. --ref=/mnt/QJref.fa \. --reads=/mnt/input.bam \. --output_vcf=/mnt/output.vcf.gz \. --output_gvcf=/mnt/output.g.vcf.gz \. --intermediate_results_dir /mnt/dpv \. ```. The core error is `ValueError: NOT_FOUND: Could not open /mnt/input.bam. [E::hts_open_format] Failed to open file ""/mnt/input.bam"" : No such file or directory`However, I have verified the existence of `/mnt/input.bam`. The complete error information is as follows. Sincerely look forward to your help! thank you. ```. sh test0215.sh. I0216 00:56:07.446549 140582811191104 run_deepvariant.py:342] Re-using the directory for intermediate results in /mnt/dpv. ***** Intermediate results will be written to /mnt/dpv in docker. ****. ***** Running the command:*****. time seq 0 2 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/mnt/QJref.fa"" --reads ""/mnt/input.bam"" --examples ""/mnt/dpv/make_examples.tfrecord@3.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --gvcf ""/mnt/dpv/gvcf.tfrecord@3.gz"" --max_reads_per_partition ""600"" --min_mapping_quality ""1"" --parse_sam_aux_fields --partition_size ""25000"" --phase_reads --pileup_image_width ""199"" --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels ""0.12"" --task {}. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/613
https://github.com/google/deepvariant/issues/613:1329,safety,compl,complete,1329,"SER/sunzhilong/1_Software/dpv/deepvariant_1.4.0.sif bash. Singularity> cd / && ls. bin boot dellfsqd2 dev environment etc home lib lib32 lib64 libx32 media mnt opt proc root run sbin singularity srv sys tmp usr var. Singularity> cd mnt && ls. QJref.fa input.bam QJref.fa.fai input.bam.bai tmp_dir. ```. Then I ran the following script. ```. cat test0215.sh. WORK_DIR=/path1/4_Test/qingjiang/dpv. export TMPDIR=""$PWD/tmp_dir"". singularity run -B$TMPDIR:$TMPDIR,""${WORK_DIR}"":/mnt \. /path1/1_Software/dpv/deepvariant_1.4.0.sif /opt/deepvariant/bin/run_deepvariant \. --num_shards=3 \. --model_type=PACBIO \. --ref=/mnt/QJref.fa \. --reads=/mnt/input.bam \. --output_vcf=/mnt/output.vcf.gz \. --output_gvcf=/mnt/output.g.vcf.gz \. --intermediate_results_dir /mnt/dpv \. ```. The core error is `ValueError: NOT_FOUND: Could not open /mnt/input.bam. [E::hts_open_format] Failed to open file ""/mnt/input.bam"" : No such file or directory`However, I have verified the existence of `/mnt/input.bam`. The complete error information is as follows. Sincerely look forward to your help! thank you. ```. sh test0215.sh. I0216 00:56:07.446549 140582811191104 run_deepvariant.py:342] Re-using the directory for intermediate results in /mnt/dpv. ***** Intermediate results will be written to /mnt/dpv in docker. ****. ***** Running the command:*****. time seq 0 2 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/mnt/QJref.fa"" --reads ""/mnt/input.bam"" --examples ""/mnt/dpv/make_examples.tfrecord@3.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --gvcf ""/mnt/dpv/gvcf.tfrecord@3.gz"" --max_reads_per_partition ""600"" --min_mapping_quality ""1"" --parse_sam_aux_fields --partition_size ""25000"" --phase_reads --pileup_image_width ""199"" --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels ""0.12"" --task {}. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unse",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/613
https://github.com/google/deepvariant/issues/613:1338,safety,error,error,1338,"hilong/1_Software/dpv/deepvariant_1.4.0.sif bash. Singularity> cd / && ls. bin boot dellfsqd2 dev environment etc home lib lib32 lib64 libx32 media mnt opt proc root run sbin singularity srv sys tmp usr var. Singularity> cd mnt && ls. QJref.fa input.bam QJref.fa.fai input.bam.bai tmp_dir. ```. Then I ran the following script. ```. cat test0215.sh. WORK_DIR=/path1/4_Test/qingjiang/dpv. export TMPDIR=""$PWD/tmp_dir"". singularity run -B$TMPDIR:$TMPDIR,""${WORK_DIR}"":/mnt \. /path1/1_Software/dpv/deepvariant_1.4.0.sif /opt/deepvariant/bin/run_deepvariant \. --num_shards=3 \. --model_type=PACBIO \. --ref=/mnt/QJref.fa \. --reads=/mnt/input.bam \. --output_vcf=/mnt/output.vcf.gz \. --output_gvcf=/mnt/output.g.vcf.gz \. --intermediate_results_dir /mnt/dpv \. ```. The core error is `ValueError: NOT_FOUND: Could not open /mnt/input.bam. [E::hts_open_format] Failed to open file ""/mnt/input.bam"" : No such file or directory`However, I have verified the existence of `/mnt/input.bam`. The complete error information is as follows. Sincerely look forward to your help! thank you. ```. sh test0215.sh. I0216 00:56:07.446549 140582811191104 run_deepvariant.py:342] Re-using the directory for intermediate results in /mnt/dpv. ***** Intermediate results will be written to /mnt/dpv in docker. ****. ***** Running the command:*****. time seq 0 2 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/mnt/QJref.fa"" --reads ""/mnt/input.bam"" --examples ""/mnt/dpv/make_examples.tfrecord@3.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --gvcf ""/mnt/dpv/gvcf.tfrecord@3.gz"" --max_reads_per_partition ""600"" --min_mapping_quality ""1"" --parse_sam_aux_fields --partition_size ""25000"" --phase_reads --pileup_image_width ""199"" --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels ""0.12"" --task {}. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LC_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/613
https://github.com/google/deepvariant/issues/613:1804,safety,input,input,1804,"/mnt \. /path1/1_Software/dpv/deepvariant_1.4.0.sif /opt/deepvariant/bin/run_deepvariant \. --num_shards=3 \. --model_type=PACBIO \. --ref=/mnt/QJref.fa \. --reads=/mnt/input.bam \. --output_vcf=/mnt/output.vcf.gz \. --output_gvcf=/mnt/output.g.vcf.gz \. --intermediate_results_dir /mnt/dpv \. ```. The core error is `ValueError: NOT_FOUND: Could not open /mnt/input.bam. [E::hts_open_format] Failed to open file ""/mnt/input.bam"" : No such file or directory`However, I have verified the existence of `/mnt/input.bam`. The complete error information is as follows. Sincerely look forward to your help! thank you. ```. sh test0215.sh. I0216 00:56:07.446549 140582811191104 run_deepvariant.py:342] Re-using the directory for intermediate results in /mnt/dpv. ***** Intermediate results will be written to /mnt/dpv in docker. ****. ***** Running the command:*****. time seq 0 2 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/mnt/QJref.fa"" --reads ""/mnt/input.bam"" --examples ""/mnt/dpv/make_examples.tfrecord@3.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --gvcf ""/mnt/dpv/gvcf.tfrecord@3.gz"" --max_reads_per_partition ""600"" --min_mapping_quality ""1"" --parse_sam_aux_fields --partition_size ""25000"" --phase_reads --pileup_image_width ""199"" --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels ""0.12"" --task {}. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LC_CTYPE = ""C.UTF-8"",. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LC_CTYPE = ""C.UTF-8"",. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). [E::hts_open_format] Failed to open file ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/613
https://github.com/google/deepvariant/issues/613:2813,safety,input,input,2813,"-examples ""/mnt/dpv/make_examples.tfrecord@3.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --gvcf ""/mnt/dpv/gvcf.tfrecord@3.gz"" --max_reads_per_partition ""600"" --min_mapping_quality ""1"" --parse_sam_aux_fields --partition_size ""25000"" --phase_reads --pileup_image_width ""199"" --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels ""0.12"" --task {}. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LC_CTYPE = ""C.UTF-8"",. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LC_CTYPE = ""C.UTF-8"",. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). [E::hts_open_format] Failed to open file ""/mnt/input.bam"" : No such file or directory. Traceback (most recent call last):. File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_p4oo9k4b/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>. app.run(main). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_p4oo9k4b/runfiles/absl_py/absl/app.py"", line 300, in run. _run_main(main, args). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_p4oo9k4b/runfiles/absl_py/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_p4oo9k4b/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 166, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_p4oo9k4b/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 128, in default_options. samples_in_order, sample_role_to_train = one_sample_from_flags(. File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/613
https://github.com/google/deepvariant/issues/613:3032,safety,modul,module,3032,"s --partition_size ""25000"" --phase_reads --pileup_image_width ""199"" --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels ""0.12"" --task {}. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LC_CTYPE = ""C.UTF-8"",. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LC_CTYPE = ""C.UTF-8"",. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). [E::hts_open_format] Failed to open file ""/mnt/input.bam"" : No such file or directory. Traceback (most recent call last):. File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_p4oo9k4b/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>. app.run(main). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_p4oo9k4b/runfiles/absl_py/absl/app.py"", line 300, in run. _run_main(main, args). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_p4oo9k4b/runfiles/absl_py/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_p4oo9k4b/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 166, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_p4oo9k4b/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 128, in default_options. samples_in_order, sample_role_to_train = one_sample_from_flags(. File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_p4oo9k4b/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 85, in one_sample_from_flags. sample_name = make_examples_core.assign_sample_name(. File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/613
https://github.com/google/deepvariant/issues/613:4892,safety,input,input,4892,""", line 85, in one_sample_from_flags. sample_name = make_examples_core.assign_sample_name(. File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_p4oo9k4b/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 134, in assign_sample_name. with sam.SamReader(reads_filenames.split(',')[0]) as sam_reader:. File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_p4oo9k4b/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_p4oo9k4b/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 260, in _native_reader. return NativeSamReader(input_path, **kwargs). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_p4oo9k4b/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 227, in __init__. self._reader = sam_reader.SamReader.from_file(. ValueError: NOT_FOUND: Could not open /mnt/input.bam. [E::hts_open_format] Failed to open file ""/mnt/input.bam"" : No such file or directory. [E::hts_open_format] Failed to open file ""/mnt/input.bam"" : No such file or directory. Traceback (most recent call last):. File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_q7goget_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>. app.run(main). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_q7goget_/runfiles/absl_py/absl/app.py"", line 300, in run. _run_main(main, args). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_q7goget_/runfiles/absl_py/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_q7goget_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 166, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_q7goget_/runfiles/com_google_deepvariant/deepvariant/make_examples.py",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/613
https://github.com/google/deepvariant/issues/613:4950,safety,input,input,4950,"xamples_core.assign_sample_name(. File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_p4oo9k4b/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 134, in assign_sample_name. with sam.SamReader(reads_filenames.split(',')[0]) as sam_reader:. File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_p4oo9k4b/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_p4oo9k4b/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 260, in _native_reader. return NativeSamReader(input_path, **kwargs). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_p4oo9k4b/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 227, in __init__. self._reader = sam_reader.SamReader.from_file(. ValueError: NOT_FOUND: Could not open /mnt/input.bam. [E::hts_open_format] Failed to open file ""/mnt/input.bam"" : No such file or directory. [E::hts_open_format] Failed to open file ""/mnt/input.bam"" : No such file or directory. Traceback (most recent call last):. File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_q7goget_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>. app.run(main). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_q7goget_/runfiles/absl_py/absl/app.py"", line 300, in run. _run_main(main, args). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_q7goget_/runfiles/absl_py/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_q7goget_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 166, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_q7goget_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 128, in default_options. samples_in_order, sample_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/613
https://github.com/google/deepvariant/issues/613:5037,safety,input,input,5037,"les_p4oo9k4b/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 134, in assign_sample_name. with sam.SamReader(reads_filenames.split(',')[0]) as sam_reader:. File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_p4oo9k4b/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_p4oo9k4b/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 260, in _native_reader. return NativeSamReader(input_path, **kwargs). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_p4oo9k4b/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 227, in __init__. self._reader = sam_reader.SamReader.from_file(. ValueError: NOT_FOUND: Could not open /mnt/input.bam. [E::hts_open_format] Failed to open file ""/mnt/input.bam"" : No such file or directory. [E::hts_open_format] Failed to open file ""/mnt/input.bam"" : No such file or directory. Traceback (most recent call last):. File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_q7goget_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>. app.run(main). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_q7goget_/runfiles/absl_py/absl/app.py"", line 300, in run. _run_main(main, args). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_q7goget_/runfiles/absl_py/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_q7goget_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 166, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_q7goget_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 128, in default_options. samples_in_order, sample_role_to_train = one_sample_from_flags(. File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/613
https://github.com/google/deepvariant/issues/613:5256,safety,modul,module,5256,"ir/Bazel.runfiles_p4oo9k4b/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_p4oo9k4b/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 260, in _native_reader. return NativeSamReader(input_path, **kwargs). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_p4oo9k4b/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 227, in __init__. self._reader = sam_reader.SamReader.from_file(. ValueError: NOT_FOUND: Could not open /mnt/input.bam. [E::hts_open_format] Failed to open file ""/mnt/input.bam"" : No such file or directory. [E::hts_open_format] Failed to open file ""/mnt/input.bam"" : No such file or directory. Traceback (most recent call last):. File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_q7goget_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>. app.run(main). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_q7goget_/runfiles/absl_py/absl/app.py"", line 300, in run. _run_main(main, args). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_q7goget_/runfiles/absl_py/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_q7goget_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 166, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_q7goget_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 128, in default_options. samples_in_order, sample_role_to_train = one_sample_from_flags(. File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_q7goget_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 85, in one_sample_from_flags. sample_name = make_examples_core.assign_sample_name(. File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/613
https://github.com/google/deepvariant/issues/613:7116,safety,input,input,7116,""", line 85, in one_sample_from_flags. sample_name = make_examples_core.assign_sample_name(. File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_q7goget_/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 134, in assign_sample_name. with sam.SamReader(reads_filenames.split(',')[0]) as sam_reader:. File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_q7goget_/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_q7goget_/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 260, in _native_reader. return NativeSamReader(input_path, **kwargs). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_q7goget_/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 227, in __init__. self._reader = sam_reader.SamReader.from_file(. ValueError: NOT_FOUND: Could not open /mnt/input.bam. Traceback (most recent call last):. File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_gc9nyr7p/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>. app.run(main). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_gc9nyr7p/runfiles/absl_py/absl/app.py"", line 300, in run. _run_main(main, args). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_gc9nyr7p/runfiles/absl_py/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_gc9nyr7p/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 166, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_gc9nyr7p/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 128, in default_options. samples_in_order, sample_role_to_train = one_sample_from_flags(. File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_gc9nyr7p/runfiles/c",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/613
https://github.com/google/deepvariant/issues/613:7306,safety,modul,module,7306,"deepvariant/make_examples_core.py"", line 134, in assign_sample_name. with sam.SamReader(reads_filenames.split(',')[0]) as sam_reader:. File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_q7goget_/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_q7goget_/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 260, in _native_reader. return NativeSamReader(input_path, **kwargs). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_q7goget_/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 227, in __init__. self._reader = sam_reader.SamReader.from_file(. ValueError: NOT_FOUND: Could not open /mnt/input.bam. Traceback (most recent call last):. File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_gc9nyr7p/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>. app.run(main). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_gc9nyr7p/runfiles/absl_py/absl/app.py"", line 300, in run. _run_main(main, args). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_gc9nyr7p/runfiles/absl_py/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_gc9nyr7p/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 166, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_gc9nyr7p/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 128, in default_options. samples_in_order, sample_role_to_train = one_sample_from_flags(. File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_gc9nyr7p/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 85, in one_sample_from_flags. sample_name = make_examples_core.assign_sample_name(. File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/613
https://github.com/google/deepvariant/issues/613:9166,safety,input,input,9166,"nt/make_examples.py"", line 166, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_gc9nyr7p/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 128, in default_options. samples_in_order, sample_role_to_train = one_sample_from_flags(. File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_gc9nyr7p/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 85, in one_sample_from_flags. sample_name = make_examples_core.assign_sample_name(. File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_gc9nyr7p/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 134, in assign_sample_name. with sam.SamReader(reads_filenames.split(',')[0]) as sam_reader:. File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_gc9nyr7p/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_gc9nyr7p/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 260, in _native_reader. return NativeSamReader(input_path, **kwargs). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_gc9nyr7p/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 227, in __init__. self._reader = sam_reader.SamReader.from_file(. ValueError: NOT_FOUND: Could not open /mnt/input.bam. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /mnt/QJref.fa --reads /mnt/input.bam --examples /mnt/dpv/make_examples.tfrecord@3.gz --add_hp_channel --alt_aligned_pileup diff_channels --gvcf /mnt/dpv/gvcf.tfrecord@3.gz --max_reads_per_partition 600 --min_mapping_quality 1 --parse_sam_aux_fields --partition_size 25000 --phase_reads --pileup_image_width 199 --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels 0.12 --task 1. real 0m32.021s. user 0m5.396s. sys 0m4.055s. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/613
https://github.com/google/deepvariant/issues/613:9288,safety,input,input,9288,"nt/make_examples.py"", line 166, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_gc9nyr7p/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 128, in default_options. samples_in_order, sample_role_to_train = one_sample_from_flags(. File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_gc9nyr7p/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 85, in one_sample_from_flags. sample_name = make_examples_core.assign_sample_name(. File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_gc9nyr7p/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 134, in assign_sample_name. with sam.SamReader(reads_filenames.split(',')[0]) as sam_reader:. File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_gc9nyr7p/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_gc9nyr7p/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 260, in _native_reader. return NativeSamReader(input_path, **kwargs). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_gc9nyr7p/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 227, in __init__. self._reader = sam_reader.SamReader.from_file(. ValueError: NOT_FOUND: Could not open /mnt/input.bam. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /mnt/QJref.fa --reads /mnt/input.bam --examples /mnt/dpv/make_examples.tfrecord@3.gz --add_hp_channel --alt_aligned_pileup diff_channels --gvcf /mnt/dpv/gvcf.tfrecord@3.gz --max_reads_per_partition 600 --min_mapping_quality 1 --parse_sam_aux_fields --partition_size 25000 --phase_reads --pileup_image_width 199 --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels 0.12 --task 1. real 0m32.021s. user 0m5.396s. sys 0m4.055s. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/613
https://github.com/google/deepvariant/issues/613:1329,security,compl,complete,1329,"SER/sunzhilong/1_Software/dpv/deepvariant_1.4.0.sif bash. Singularity> cd / && ls. bin boot dellfsqd2 dev environment etc home lib lib32 lib64 libx32 media mnt opt proc root run sbin singularity srv sys tmp usr var. Singularity> cd mnt && ls. QJref.fa input.bam QJref.fa.fai input.bam.bai tmp_dir. ```. Then I ran the following script. ```. cat test0215.sh. WORK_DIR=/path1/4_Test/qingjiang/dpv. export TMPDIR=""$PWD/tmp_dir"". singularity run -B$TMPDIR:$TMPDIR,""${WORK_DIR}"":/mnt \. /path1/1_Software/dpv/deepvariant_1.4.0.sif /opt/deepvariant/bin/run_deepvariant \. --num_shards=3 \. --model_type=PACBIO \. --ref=/mnt/QJref.fa \. --reads=/mnt/input.bam \. --output_vcf=/mnt/output.vcf.gz \. --output_gvcf=/mnt/output.g.vcf.gz \. --intermediate_results_dir /mnt/dpv \. ```. The core error is `ValueError: NOT_FOUND: Could not open /mnt/input.bam. [E::hts_open_format] Failed to open file ""/mnt/input.bam"" : No such file or directory`However, I have verified the existence of `/mnt/input.bam`. The complete error information is as follows. Sincerely look forward to your help! thank you. ```. sh test0215.sh. I0216 00:56:07.446549 140582811191104 run_deepvariant.py:342] Re-using the directory for intermediate results in /mnt/dpv. ***** Intermediate results will be written to /mnt/dpv in docker. ****. ***** Running the command:*****. time seq 0 2 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/mnt/QJref.fa"" --reads ""/mnt/input.bam"" --examples ""/mnt/dpv/make_examples.tfrecord@3.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --gvcf ""/mnt/dpv/gvcf.tfrecord@3.gz"" --max_reads_per_partition ""600"" --min_mapping_quality ""1"" --parse_sam_aux_fields --partition_size ""25000"" --phase_reads --pileup_image_width ""199"" --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels ""0.12"" --task {}. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unse",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/613
https://github.com/google/deepvariant/issues/613:139,testability,verif,verify,139,"Thanks for your help, I successfully solved the problem of TMPDIR. Then there is a new problem, I hope to continue to consult you. First I verify that my host directory is successfully mounted to the container directory.`/mnt/QJref.fa /mnt/input.bam`. ```. singularity exec -B $TMPDIR:$TMPDIR,""${WORK_DIR}"":/mnt /dellfsqd2/ST_OCEAN/USER/sunzhilong/1_Software/dpv/deepvariant_1.4.0.sif bash. Singularity> cd / && ls. bin boot dellfsqd2 dev environment etc home lib lib32 lib64 libx32 media mnt opt proc root run sbin singularity srv sys tmp usr var. Singularity> cd mnt && ls. QJref.fa input.bam QJref.fa.fai input.bam.bai tmp_dir. ```. Then I ran the following script. ```. cat test0215.sh. WORK_DIR=/path1/4_Test/qingjiang/dpv. export TMPDIR=""$PWD/tmp_dir"". singularity run -B$TMPDIR:$TMPDIR,""${WORK_DIR}"":/mnt \. /path1/1_Software/dpv/deepvariant_1.4.0.sif /opt/deepvariant/bin/run_deepvariant \. --num_shards=3 \. --model_type=PACBIO \. --ref=/mnt/QJref.fa \. --reads=/mnt/input.bam \. --output_vcf=/mnt/output.vcf.gz \. --output_gvcf=/mnt/output.g.vcf.gz \. --intermediate_results_dir /mnt/dpv \. ```. The core error is `ValueError: NOT_FOUND: Could not open /mnt/input.bam. [E::hts_open_format] Failed to open file ""/mnt/input.bam"" : No such file or directory`However, I have verified the existence of `/mnt/input.bam`. The complete error information is as follows. Sincerely look forward to your help! thank you. ```. sh test0215.sh. I0216 00:56:07.446549 140582811191104 run_deepvariant.py:342] Re-using the directory for intermediate results in /mnt/dpv. ***** Intermediate results will be written to /mnt/dpv in docker. ****. ***** Running the command:*****. time seq 0 2 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/mnt/QJref.fa"" --reads ""/mnt/input.bam"" --examples ""/mnt/dpv/make_examples.tfrecord@3.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --gvcf ""/mnt/dpv/gvcf.tfrecord@3.gz"" --max_reads_per_partition ""600"" --min_mappin",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/613
https://github.com/google/deepvariant/issues/613:1281,testability,verif,verified,1281,"$TMPDIR,""${WORK_DIR}"":/mnt /dellfsqd2/ST_OCEAN/USER/sunzhilong/1_Software/dpv/deepvariant_1.4.0.sif bash. Singularity> cd / && ls. bin boot dellfsqd2 dev environment etc home lib lib32 lib64 libx32 media mnt opt proc root run sbin singularity srv sys tmp usr var. Singularity> cd mnt && ls. QJref.fa input.bam QJref.fa.fai input.bam.bai tmp_dir. ```. Then I ran the following script. ```. cat test0215.sh. WORK_DIR=/path1/4_Test/qingjiang/dpv. export TMPDIR=""$PWD/tmp_dir"". singularity run -B$TMPDIR:$TMPDIR,""${WORK_DIR}"":/mnt \. /path1/1_Software/dpv/deepvariant_1.4.0.sif /opt/deepvariant/bin/run_deepvariant \. --num_shards=3 \. --model_type=PACBIO \. --ref=/mnt/QJref.fa \. --reads=/mnt/input.bam \. --output_vcf=/mnt/output.vcf.gz \. --output_gvcf=/mnt/output.g.vcf.gz \. --intermediate_results_dir /mnt/dpv \. ```. The core error is `ValueError: NOT_FOUND: Could not open /mnt/input.bam. [E::hts_open_format] Failed to open file ""/mnt/input.bam"" : No such file or directory`However, I have verified the existence of `/mnt/input.bam`. The complete error information is as follows. Sincerely look forward to your help! thank you. ```. sh test0215.sh. I0216 00:56:07.446549 140582811191104 run_deepvariant.py:342] Re-using the directory for intermediate results in /mnt/dpv. ***** Intermediate results will be written to /mnt/dpv in docker. ****. ***** Running the command:*****. time seq 0 2 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/mnt/QJref.fa"" --reads ""/mnt/input.bam"" --examples ""/mnt/dpv/make_examples.tfrecord@3.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --gvcf ""/mnt/dpv/gvcf.tfrecord@3.gz"" --max_reads_per_partition ""600"" --min_mapping_quality ""1"" --parse_sam_aux_fields --partition_size ""25000"" --phase_reads --pileup_image_width ""199"" --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels ""0.12"" --task {}. perl: warning: Setting locale failed. perl: warning: Please check that your local",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/613
https://github.com/google/deepvariant/issues/613:2853,testability,Trace,Traceback,2853,"@3.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --gvcf ""/mnt/dpv/gvcf.tfrecord@3.gz"" --max_reads_per_partition ""600"" --min_mapping_quality ""1"" --parse_sam_aux_fields --partition_size ""25000"" --phase_reads --pileup_image_width ""199"" --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels ""0.12"" --task {}. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LC_CTYPE = ""C.UTF-8"",. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LC_CTYPE = ""C.UTF-8"",. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). [E::hts_open_format] Failed to open file ""/mnt/input.bam"" : No such file or directory. Traceback (most recent call last):. File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_p4oo9k4b/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>. app.run(main). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_p4oo9k4b/runfiles/absl_py/absl/app.py"", line 300, in run. _run_main(main, args). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_p4oo9k4b/runfiles/absl_py/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_p4oo9k4b/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 166, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_p4oo9k4b/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 128, in default_options. samples_in_order, sample_role_to_train = one_sample_from_flags(. File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_p4oo9k4b/runfiles/com_google_dee",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/613
https://github.com/google/deepvariant/issues/613:5077,testability,Trace,Traceback,5077,"nt/deepvariant/make_examples_core.py"", line 134, in assign_sample_name. with sam.SamReader(reads_filenames.split(',')[0]) as sam_reader:. File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_p4oo9k4b/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_p4oo9k4b/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 260, in _native_reader. return NativeSamReader(input_path, **kwargs). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_p4oo9k4b/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 227, in __init__. self._reader = sam_reader.SamReader.from_file(. ValueError: NOT_FOUND: Could not open /mnt/input.bam. [E::hts_open_format] Failed to open file ""/mnt/input.bam"" : No such file or directory. [E::hts_open_format] Failed to open file ""/mnt/input.bam"" : No such file or directory. Traceback (most recent call last):. File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_q7goget_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>. app.run(main). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_q7goget_/runfiles/absl_py/absl/app.py"", line 300, in run. _run_main(main, args). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_q7goget_/runfiles/absl_py/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_q7goget_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 166, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_q7goget_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 128, in default_options. samples_in_order, sample_role_to_train = one_sample_from_flags(. File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_q7goget_/runfiles/com_google_dee",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/613
https://github.com/google/deepvariant/issues/613:7127,testability,Trace,Traceback,7127,"n one_sample_from_flags. sample_name = make_examples_core.assign_sample_name(. File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_q7goget_/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 134, in assign_sample_name. with sam.SamReader(reads_filenames.split(',')[0]) as sam_reader:. File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_q7goget_/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_q7goget_/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 260, in _native_reader. return NativeSamReader(input_path, **kwargs). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_q7goget_/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 227, in __init__. self._reader = sam_reader.SamReader.from_file(. ValueError: NOT_FOUND: Could not open /mnt/input.bam. Traceback (most recent call last):. File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_gc9nyr7p/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>. app.run(main). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_gc9nyr7p/runfiles/absl_py/absl/app.py"", line 300, in run. _run_main(main, args). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_gc9nyr7p/runfiles/absl_py/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_gc9nyr7p/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 166, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_gc9nyr7p/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 128, in default_options. samples_in_order, sample_role_to_train = one_sample_from_flags(. File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_gc9nyr7p/runfiles/com_google_dee",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/613
https://github.com/google/deepvariant/issues/613:16,usability,help,help,16,"Thanks for your help, I successfully solved the problem of TMPDIR. Then there is a new problem, I hope to continue to consult you. First I verify that my host directory is successfully mounted to the container directory.`/mnt/QJref.fa /mnt/input.bam`. ```. singularity exec -B $TMPDIR:$TMPDIR,""${WORK_DIR}"":/mnt /dellfsqd2/ST_OCEAN/USER/sunzhilong/1_Software/dpv/deepvariant_1.4.0.sif bash. Singularity> cd / && ls. bin boot dellfsqd2 dev environment etc home lib lib32 lib64 libx32 media mnt opt proc root run sbin singularity srv sys tmp usr var. Singularity> cd mnt && ls. QJref.fa input.bam QJref.fa.fai input.bam.bai tmp_dir. ```. Then I ran the following script. ```. cat test0215.sh. WORK_DIR=/path1/4_Test/qingjiang/dpv. export TMPDIR=""$PWD/tmp_dir"". singularity run -B$TMPDIR:$TMPDIR,""${WORK_DIR}"":/mnt \. /path1/1_Software/dpv/deepvariant_1.4.0.sif /opt/deepvariant/bin/run_deepvariant \. --num_shards=3 \. --model_type=PACBIO \. --ref=/mnt/QJref.fa \. --reads=/mnt/input.bam \. --output_vcf=/mnt/output.vcf.gz \. --output_gvcf=/mnt/output.g.vcf.gz \. --intermediate_results_dir /mnt/dpv \. ```. The core error is `ValueError: NOT_FOUND: Could not open /mnt/input.bam. [E::hts_open_format] Failed to open file ""/mnt/input.bam"" : No such file or directory`However, I have verified the existence of `/mnt/input.bam`. The complete error information is as follows. Sincerely look forward to your help! thank you. ```. sh test0215.sh. I0216 00:56:07.446549 140582811191104 run_deepvariant.py:342] Re-using the directory for intermediate results in /mnt/dpv. ***** Intermediate results will be written to /mnt/dpv in docker. ****. ***** Running the command:*****. time seq 0 2 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/mnt/QJref.fa"" --reads ""/mnt/input.bam"" --examples ""/mnt/dpv/make_examples.tfrecord@3.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --gvcf ""/mnt/dpv/gvcf.tfrecord@3.gz"" --max_reads_per_partition ""600"" --min_mappin",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/613
https://github.com/google/deepvariant/issues/613:240,usability,input,input,240,"Thanks for your help, I successfully solved the problem of TMPDIR. Then there is a new problem, I hope to continue to consult you. First I verify that my host directory is successfully mounted to the container directory.`/mnt/QJref.fa /mnt/input.bam`. ```. singularity exec -B $TMPDIR:$TMPDIR,""${WORK_DIR}"":/mnt /dellfsqd2/ST_OCEAN/USER/sunzhilong/1_Software/dpv/deepvariant_1.4.0.sif bash. Singularity> cd / && ls. bin boot dellfsqd2 dev environment etc home lib lib32 lib64 libx32 media mnt opt proc root run sbin singularity srv sys tmp usr var. Singularity> cd mnt && ls. QJref.fa input.bam QJref.fa.fai input.bam.bai tmp_dir. ```. Then I ran the following script. ```. cat test0215.sh. WORK_DIR=/path1/4_Test/qingjiang/dpv. export TMPDIR=""$PWD/tmp_dir"". singularity run -B$TMPDIR:$TMPDIR,""${WORK_DIR}"":/mnt \. /path1/1_Software/dpv/deepvariant_1.4.0.sif /opt/deepvariant/bin/run_deepvariant \. --num_shards=3 \. --model_type=PACBIO \. --ref=/mnt/QJref.fa \. --reads=/mnt/input.bam \. --output_vcf=/mnt/output.vcf.gz \. --output_gvcf=/mnt/output.g.vcf.gz \. --intermediate_results_dir /mnt/dpv \. ```. The core error is `ValueError: NOT_FOUND: Could not open /mnt/input.bam. [E::hts_open_format] Failed to open file ""/mnt/input.bam"" : No such file or directory`However, I have verified the existence of `/mnt/input.bam`. The complete error information is as follows. Sincerely look forward to your help! thank you. ```. sh test0215.sh. I0216 00:56:07.446549 140582811191104 run_deepvariant.py:342] Re-using the directory for intermediate results in /mnt/dpv. ***** Intermediate results will be written to /mnt/dpv in docker. ****. ***** Running the command:*****. time seq 0 2 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/mnt/QJref.fa"" --reads ""/mnt/input.bam"" --examples ""/mnt/dpv/make_examples.tfrecord@3.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --gvcf ""/mnt/dpv/gvcf.tfrecord@3.gz"" --max_reads_per_partition ""600"" --min_mappin",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/613
https://github.com/google/deepvariant/issues/613:332,usability,USER,USER,332,"Thanks for your help, I successfully solved the problem of TMPDIR. Then there is a new problem, I hope to continue to consult you. First I verify that my host directory is successfully mounted to the container directory.`/mnt/QJref.fa /mnt/input.bam`. ```. singularity exec -B $TMPDIR:$TMPDIR,""${WORK_DIR}"":/mnt /dellfsqd2/ST_OCEAN/USER/sunzhilong/1_Software/dpv/deepvariant_1.4.0.sif bash. Singularity> cd / && ls. bin boot dellfsqd2 dev environment etc home lib lib32 lib64 libx32 media mnt opt proc root run sbin singularity srv sys tmp usr var. Singularity> cd mnt && ls. QJref.fa input.bam QJref.fa.fai input.bam.bai tmp_dir. ```. Then I ran the following script. ```. cat test0215.sh. WORK_DIR=/path1/4_Test/qingjiang/dpv. export TMPDIR=""$PWD/tmp_dir"". singularity run -B$TMPDIR:$TMPDIR,""${WORK_DIR}"":/mnt \. /path1/1_Software/dpv/deepvariant_1.4.0.sif /opt/deepvariant/bin/run_deepvariant \. --num_shards=3 \. --model_type=PACBIO \. --ref=/mnt/QJref.fa \. --reads=/mnt/input.bam \. --output_vcf=/mnt/output.vcf.gz \. --output_gvcf=/mnt/output.g.vcf.gz \. --intermediate_results_dir /mnt/dpv \. ```. The core error is `ValueError: NOT_FOUND: Could not open /mnt/input.bam. [E::hts_open_format] Failed to open file ""/mnt/input.bam"" : No such file or directory`However, I have verified the existence of `/mnt/input.bam`. The complete error information is as follows. Sincerely look forward to your help! thank you. ```. sh test0215.sh. I0216 00:56:07.446549 140582811191104 run_deepvariant.py:342] Re-using the directory for intermediate results in /mnt/dpv. ***** Intermediate results will be written to /mnt/dpv in docker. ****. ***** Running the command:*****. time seq 0 2 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/mnt/QJref.fa"" --reads ""/mnt/input.bam"" --examples ""/mnt/dpv/make_examples.tfrecord@3.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --gvcf ""/mnt/dpv/gvcf.tfrecord@3.gz"" --max_reads_per_partition ""600"" --min_mappin",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/613
https://github.com/google/deepvariant/issues/613:585,usability,input,input,585,"Thanks for your help, I successfully solved the problem of TMPDIR. Then there is a new problem, I hope to continue to consult you. First I verify that my host directory is successfully mounted to the container directory.`/mnt/QJref.fa /mnt/input.bam`. ```. singularity exec -B $TMPDIR:$TMPDIR,""${WORK_DIR}"":/mnt /dellfsqd2/ST_OCEAN/USER/sunzhilong/1_Software/dpv/deepvariant_1.4.0.sif bash. Singularity> cd / && ls. bin boot dellfsqd2 dev environment etc home lib lib32 lib64 libx32 media mnt opt proc root run sbin singularity srv sys tmp usr var. Singularity> cd mnt && ls. QJref.fa input.bam QJref.fa.fai input.bam.bai tmp_dir. ```. Then I ran the following script. ```. cat test0215.sh. WORK_DIR=/path1/4_Test/qingjiang/dpv. export TMPDIR=""$PWD/tmp_dir"". singularity run -B$TMPDIR:$TMPDIR,""${WORK_DIR}"":/mnt \. /path1/1_Software/dpv/deepvariant_1.4.0.sif /opt/deepvariant/bin/run_deepvariant \. --num_shards=3 \. --model_type=PACBIO \. --ref=/mnt/QJref.fa \. --reads=/mnt/input.bam \. --output_vcf=/mnt/output.vcf.gz \. --output_gvcf=/mnt/output.g.vcf.gz \. --intermediate_results_dir /mnt/dpv \. ```. The core error is `ValueError: NOT_FOUND: Could not open /mnt/input.bam. [E::hts_open_format] Failed to open file ""/mnt/input.bam"" : No such file or directory`However, I have verified the existence of `/mnt/input.bam`. The complete error information is as follows. Sincerely look forward to your help! thank you. ```. sh test0215.sh. I0216 00:56:07.446549 140582811191104 run_deepvariant.py:342] Re-using the directory for intermediate results in /mnt/dpv. ***** Intermediate results will be written to /mnt/dpv in docker. ****. ***** Running the command:*****. time seq 0 2 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/mnt/QJref.fa"" --reads ""/mnt/input.bam"" --examples ""/mnt/dpv/make_examples.tfrecord@3.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --gvcf ""/mnt/dpv/gvcf.tfrecord@3.gz"" --max_reads_per_partition ""600"" --min_mappin",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/613
https://github.com/google/deepvariant/issues/613:608,usability,input,input,608,"Thanks for your help, I successfully solved the problem of TMPDIR. Then there is a new problem, I hope to continue to consult you. First I verify that my host directory is successfully mounted to the container directory.`/mnt/QJref.fa /mnt/input.bam`. ```. singularity exec -B $TMPDIR:$TMPDIR,""${WORK_DIR}"":/mnt /dellfsqd2/ST_OCEAN/USER/sunzhilong/1_Software/dpv/deepvariant_1.4.0.sif bash. Singularity> cd / && ls. bin boot dellfsqd2 dev environment etc home lib lib32 lib64 libx32 media mnt opt proc root run sbin singularity srv sys tmp usr var. Singularity> cd mnt && ls. QJref.fa input.bam QJref.fa.fai input.bam.bai tmp_dir. ```. Then I ran the following script. ```. cat test0215.sh. WORK_DIR=/path1/4_Test/qingjiang/dpv. export TMPDIR=""$PWD/tmp_dir"". singularity run -B$TMPDIR:$TMPDIR,""${WORK_DIR}"":/mnt \. /path1/1_Software/dpv/deepvariant_1.4.0.sif /opt/deepvariant/bin/run_deepvariant \. --num_shards=3 \. --model_type=PACBIO \. --ref=/mnt/QJref.fa \. --reads=/mnt/input.bam \. --output_vcf=/mnt/output.vcf.gz \. --output_gvcf=/mnt/output.g.vcf.gz \. --intermediate_results_dir /mnt/dpv \. ```. The core error is `ValueError: NOT_FOUND: Could not open /mnt/input.bam. [E::hts_open_format] Failed to open file ""/mnt/input.bam"" : No such file or directory`However, I have verified the existence of `/mnt/input.bam`. The complete error information is as follows. Sincerely look forward to your help! thank you. ```. sh test0215.sh. I0216 00:56:07.446549 140582811191104 run_deepvariant.py:342] Re-using the directory for intermediate results in /mnt/dpv. ***** Intermediate results will be written to /mnt/dpv in docker. ****. ***** Running the command:*****. time seq 0 2 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/mnt/QJref.fa"" --reads ""/mnt/input.bam"" --examples ""/mnt/dpv/make_examples.tfrecord@3.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --gvcf ""/mnt/dpv/gvcf.tfrecord@3.gz"" --max_reads_per_partition ""600"" --min_mappin",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/613
https://github.com/google/deepvariant/issues/613:976,usability,input,input,976,"Thanks for your help, I successfully solved the problem of TMPDIR. Then there is a new problem, I hope to continue to consult you. First I verify that my host directory is successfully mounted to the container directory.`/mnt/QJref.fa /mnt/input.bam`. ```. singularity exec -B $TMPDIR:$TMPDIR,""${WORK_DIR}"":/mnt /dellfsqd2/ST_OCEAN/USER/sunzhilong/1_Software/dpv/deepvariant_1.4.0.sif bash. Singularity> cd / && ls. bin boot dellfsqd2 dev environment etc home lib lib32 lib64 libx32 media mnt opt proc root run sbin singularity srv sys tmp usr var. Singularity> cd mnt && ls. QJref.fa input.bam QJref.fa.fai input.bam.bai tmp_dir. ```. Then I ran the following script. ```. cat test0215.sh. WORK_DIR=/path1/4_Test/qingjiang/dpv. export TMPDIR=""$PWD/tmp_dir"". singularity run -B$TMPDIR:$TMPDIR,""${WORK_DIR}"":/mnt \. /path1/1_Software/dpv/deepvariant_1.4.0.sif /opt/deepvariant/bin/run_deepvariant \. --num_shards=3 \. --model_type=PACBIO \. --ref=/mnt/QJref.fa \. --reads=/mnt/input.bam \. --output_vcf=/mnt/output.vcf.gz \. --output_gvcf=/mnt/output.g.vcf.gz \. --intermediate_results_dir /mnt/dpv \. ```. The core error is `ValueError: NOT_FOUND: Could not open /mnt/input.bam. [E::hts_open_format] Failed to open file ""/mnt/input.bam"" : No such file or directory`However, I have verified the existence of `/mnt/input.bam`. The complete error information is as follows. Sincerely look forward to your help! thank you. ```. sh test0215.sh. I0216 00:56:07.446549 140582811191104 run_deepvariant.py:342] Re-using the directory for intermediate results in /mnt/dpv. ***** Intermediate results will be written to /mnt/dpv in docker. ****. ***** Running the command:*****. time seq 0 2 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/mnt/QJref.fa"" --reads ""/mnt/input.bam"" --examples ""/mnt/dpv/make_examples.tfrecord@3.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --gvcf ""/mnt/dpv/gvcf.tfrecord@3.gz"" --max_reads_per_partition ""600"" --min_mappin",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/613
https://github.com/google/deepvariant/issues/613:1115,usability,error,error,1115,"consult you. First I verify that my host directory is successfully mounted to the container directory.`/mnt/QJref.fa /mnt/input.bam`. ```. singularity exec -B $TMPDIR:$TMPDIR,""${WORK_DIR}"":/mnt /dellfsqd2/ST_OCEAN/USER/sunzhilong/1_Software/dpv/deepvariant_1.4.0.sif bash. Singularity> cd / && ls. bin boot dellfsqd2 dev environment etc home lib lib32 lib64 libx32 media mnt opt proc root run sbin singularity srv sys tmp usr var. Singularity> cd mnt && ls. QJref.fa input.bam QJref.fa.fai input.bam.bai tmp_dir. ```. Then I ran the following script. ```. cat test0215.sh. WORK_DIR=/path1/4_Test/qingjiang/dpv. export TMPDIR=""$PWD/tmp_dir"". singularity run -B$TMPDIR:$TMPDIR,""${WORK_DIR}"":/mnt \. /path1/1_Software/dpv/deepvariant_1.4.0.sif /opt/deepvariant/bin/run_deepvariant \. --num_shards=3 \. --model_type=PACBIO \. --ref=/mnt/QJref.fa \. --reads=/mnt/input.bam \. --output_vcf=/mnt/output.vcf.gz \. --output_gvcf=/mnt/output.g.vcf.gz \. --intermediate_results_dir /mnt/dpv \. ```. The core error is `ValueError: NOT_FOUND: Could not open /mnt/input.bam. [E::hts_open_format] Failed to open file ""/mnt/input.bam"" : No such file or directory`However, I have verified the existence of `/mnt/input.bam`. The complete error information is as follows. Sincerely look forward to your help! thank you. ```. sh test0215.sh. I0216 00:56:07.446549 140582811191104 run_deepvariant.py:342] Re-using the directory for intermediate results in /mnt/dpv. ***** Intermediate results will be written to /mnt/dpv in docker. ****. ***** Running the command:*****. time seq 0 2 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/mnt/QJref.fa"" --reads ""/mnt/input.bam"" --examples ""/mnt/dpv/make_examples.tfrecord@3.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --gvcf ""/mnt/dpv/gvcf.tfrecord@3.gz"" --max_reads_per_partition ""600"" --min_mapping_quality ""1"" --parse_sam_aux_fields --partition_size ""25000"" --phase_reads --pileup_image_width ""199"" --norealign_rea",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/613
https://github.com/google/deepvariant/issues/613:1168,usability,input,input,1168," successfully mounted to the container directory.`/mnt/QJref.fa /mnt/input.bam`. ```. singularity exec -B $TMPDIR:$TMPDIR,""${WORK_DIR}"":/mnt /dellfsqd2/ST_OCEAN/USER/sunzhilong/1_Software/dpv/deepvariant_1.4.0.sif bash. Singularity> cd / && ls. bin boot dellfsqd2 dev environment etc home lib lib32 lib64 libx32 media mnt opt proc root run sbin singularity srv sys tmp usr var. Singularity> cd mnt && ls. QJref.fa input.bam QJref.fa.fai input.bam.bai tmp_dir. ```. Then I ran the following script. ```. cat test0215.sh. WORK_DIR=/path1/4_Test/qingjiang/dpv. export TMPDIR=""$PWD/tmp_dir"". singularity run -B$TMPDIR:$TMPDIR,""${WORK_DIR}"":/mnt \. /path1/1_Software/dpv/deepvariant_1.4.0.sif /opt/deepvariant/bin/run_deepvariant \. --num_shards=3 \. --model_type=PACBIO \. --ref=/mnt/QJref.fa \. --reads=/mnt/input.bam \. --output_vcf=/mnt/output.vcf.gz \. --output_gvcf=/mnt/output.g.vcf.gz \. --intermediate_results_dir /mnt/dpv \. ```. The core error is `ValueError: NOT_FOUND: Could not open /mnt/input.bam. [E::hts_open_format] Failed to open file ""/mnt/input.bam"" : No such file or directory`However, I have verified the existence of `/mnt/input.bam`. The complete error information is as follows. Sincerely look forward to your help! thank you. ```. sh test0215.sh. I0216 00:56:07.446549 140582811191104 run_deepvariant.py:342] Re-using the directory for intermediate results in /mnt/dpv. ***** Intermediate results will be written to /mnt/dpv in docker. ****. ***** Running the command:*****. time seq 0 2 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/mnt/QJref.fa"" --reads ""/mnt/input.bam"" --examples ""/mnt/dpv/make_examples.tfrecord@3.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --gvcf ""/mnt/dpv/gvcf.tfrecord@3.gz"" --max_reads_per_partition ""600"" --min_mapping_quality ""1"" --parse_sam_aux_fields --partition_size ""25000"" --phase_reads --pileup_image_width ""199"" --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_f",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/613
https://github.com/google/deepvariant/issues/613:1226,usability,input,input,1226,"ef.fa /mnt/input.bam`. ```. singularity exec -B $TMPDIR:$TMPDIR,""${WORK_DIR}"":/mnt /dellfsqd2/ST_OCEAN/USER/sunzhilong/1_Software/dpv/deepvariant_1.4.0.sif bash. Singularity> cd / && ls. bin boot dellfsqd2 dev environment etc home lib lib32 lib64 libx32 media mnt opt proc root run sbin singularity srv sys tmp usr var. Singularity> cd mnt && ls. QJref.fa input.bam QJref.fa.fai input.bam.bai tmp_dir. ```. Then I ran the following script. ```. cat test0215.sh. WORK_DIR=/path1/4_Test/qingjiang/dpv. export TMPDIR=""$PWD/tmp_dir"". singularity run -B$TMPDIR:$TMPDIR,""${WORK_DIR}"":/mnt \. /path1/1_Software/dpv/deepvariant_1.4.0.sif /opt/deepvariant/bin/run_deepvariant \. --num_shards=3 \. --model_type=PACBIO \. --ref=/mnt/QJref.fa \. --reads=/mnt/input.bam \. --output_vcf=/mnt/output.vcf.gz \. --output_gvcf=/mnt/output.g.vcf.gz \. --intermediate_results_dir /mnt/dpv \. ```. The core error is `ValueError: NOT_FOUND: Could not open /mnt/input.bam. [E::hts_open_format] Failed to open file ""/mnt/input.bam"" : No such file or directory`However, I have verified the existence of `/mnt/input.bam`. The complete error information is as follows. Sincerely look forward to your help! thank you. ```. sh test0215.sh. I0216 00:56:07.446549 140582811191104 run_deepvariant.py:342] Re-using the directory for intermediate results in /mnt/dpv. ***** Intermediate results will be written to /mnt/dpv in docker. ****. ***** Running the command:*****. time seq 0 2 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/mnt/QJref.fa"" --reads ""/mnt/input.bam"" --examples ""/mnt/dpv/make_examples.tfrecord@3.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --gvcf ""/mnt/dpv/gvcf.tfrecord@3.gz"" --max_reads_per_partition ""600"" --min_mapping_quality ""1"" --parse_sam_aux_fields --partition_size ""25000"" --phase_reads --pileup_image_width ""199"" --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels ""0.12"" --task {}. perl: warning: Setting lo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/613
https://github.com/google/deepvariant/issues/613:1313,usability,input,input,1313,"lfsqd2/ST_OCEAN/USER/sunzhilong/1_Software/dpv/deepvariant_1.4.0.sif bash. Singularity> cd / && ls. bin boot dellfsqd2 dev environment etc home lib lib32 lib64 libx32 media mnt opt proc root run sbin singularity srv sys tmp usr var. Singularity> cd mnt && ls. QJref.fa input.bam QJref.fa.fai input.bam.bai tmp_dir. ```. Then I ran the following script. ```. cat test0215.sh. WORK_DIR=/path1/4_Test/qingjiang/dpv. export TMPDIR=""$PWD/tmp_dir"". singularity run -B$TMPDIR:$TMPDIR,""${WORK_DIR}"":/mnt \. /path1/1_Software/dpv/deepvariant_1.4.0.sif /opt/deepvariant/bin/run_deepvariant \. --num_shards=3 \. --model_type=PACBIO \. --ref=/mnt/QJref.fa \. --reads=/mnt/input.bam \. --output_vcf=/mnt/output.vcf.gz \. --output_gvcf=/mnt/output.g.vcf.gz \. --intermediate_results_dir /mnt/dpv \. ```. The core error is `ValueError: NOT_FOUND: Could not open /mnt/input.bam. [E::hts_open_format] Failed to open file ""/mnt/input.bam"" : No such file or directory`However, I have verified the existence of `/mnt/input.bam`. The complete error information is as follows. Sincerely look forward to your help! thank you. ```. sh test0215.sh. I0216 00:56:07.446549 140582811191104 run_deepvariant.py:342] Re-using the directory for intermediate results in /mnt/dpv. ***** Intermediate results will be written to /mnt/dpv in docker. ****. ***** Running the command:*****. time seq 0 2 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/mnt/QJref.fa"" --reads ""/mnt/input.bam"" --examples ""/mnt/dpv/make_examples.tfrecord@3.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --gvcf ""/mnt/dpv/gvcf.tfrecord@3.gz"" --max_reads_per_partition ""600"" --min_mapping_quality ""1"" --parse_sam_aux_fields --partition_size ""25000"" --phase_reads --pileup_image_width ""199"" --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels ""0.12"" --task {}. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/613
https://github.com/google/deepvariant/issues/613:1338,usability,error,error,1338,"hilong/1_Software/dpv/deepvariant_1.4.0.sif bash. Singularity> cd / && ls. bin boot dellfsqd2 dev environment etc home lib lib32 lib64 libx32 media mnt opt proc root run sbin singularity srv sys tmp usr var. Singularity> cd mnt && ls. QJref.fa input.bam QJref.fa.fai input.bam.bai tmp_dir. ```. Then I ran the following script. ```. cat test0215.sh. WORK_DIR=/path1/4_Test/qingjiang/dpv. export TMPDIR=""$PWD/tmp_dir"". singularity run -B$TMPDIR:$TMPDIR,""${WORK_DIR}"":/mnt \. /path1/1_Software/dpv/deepvariant_1.4.0.sif /opt/deepvariant/bin/run_deepvariant \. --num_shards=3 \. --model_type=PACBIO \. --ref=/mnt/QJref.fa \. --reads=/mnt/input.bam \. --output_vcf=/mnt/output.vcf.gz \. --output_gvcf=/mnt/output.g.vcf.gz \. --intermediate_results_dir /mnt/dpv \. ```. The core error is `ValueError: NOT_FOUND: Could not open /mnt/input.bam. [E::hts_open_format] Failed to open file ""/mnt/input.bam"" : No such file or directory`However, I have verified the existence of `/mnt/input.bam`. The complete error information is as follows. Sincerely look forward to your help! thank you. ```. sh test0215.sh. I0216 00:56:07.446549 140582811191104 run_deepvariant.py:342] Re-using the directory for intermediate results in /mnt/dpv. ***** Intermediate results will be written to /mnt/dpv in docker. ****. ***** Running the command:*****. time seq 0 2 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/mnt/QJref.fa"" --reads ""/mnt/input.bam"" --examples ""/mnt/dpv/make_examples.tfrecord@3.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --gvcf ""/mnt/dpv/gvcf.tfrecord@3.gz"" --max_reads_per_partition ""600"" --min_mapping_quality ""1"" --parse_sam_aux_fields --partition_size ""25000"" --phase_reads --pileup_image_width ""199"" --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels ""0.12"" --task {}. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LC_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/613
https://github.com/google/deepvariant/issues/613:1402,usability,help,help,1402,"cd / && ls. bin boot dellfsqd2 dev environment etc home lib lib32 lib64 libx32 media mnt opt proc root run sbin singularity srv sys tmp usr var. Singularity> cd mnt && ls. QJref.fa input.bam QJref.fa.fai input.bam.bai tmp_dir. ```. Then I ran the following script. ```. cat test0215.sh. WORK_DIR=/path1/4_Test/qingjiang/dpv. export TMPDIR=""$PWD/tmp_dir"". singularity run -B$TMPDIR:$TMPDIR,""${WORK_DIR}"":/mnt \. /path1/1_Software/dpv/deepvariant_1.4.0.sif /opt/deepvariant/bin/run_deepvariant \. --num_shards=3 \. --model_type=PACBIO \. --ref=/mnt/QJref.fa \. --reads=/mnt/input.bam \. --output_vcf=/mnt/output.vcf.gz \. --output_gvcf=/mnt/output.g.vcf.gz \. --intermediate_results_dir /mnt/dpv \. ```. The core error is `ValueError: NOT_FOUND: Could not open /mnt/input.bam. [E::hts_open_format] Failed to open file ""/mnt/input.bam"" : No such file or directory`However, I have verified the existence of `/mnt/input.bam`. The complete error information is as follows. Sincerely look forward to your help! thank you. ```. sh test0215.sh. I0216 00:56:07.446549 140582811191104 run_deepvariant.py:342] Re-using the directory for intermediate results in /mnt/dpv. ***** Intermediate results will be written to /mnt/dpv in docker. ****. ***** Running the command:*****. time seq 0 2 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/mnt/QJref.fa"" --reads ""/mnt/input.bam"" --examples ""/mnt/dpv/make_examples.tfrecord@3.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --gvcf ""/mnt/dpv/gvcf.tfrecord@3.gz"" --max_reads_per_partition ""600"" --min_mapping_quality ""1"" --parse_sam_aux_fields --partition_size ""25000"" --phase_reads --pileup_image_width ""199"" --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels ""0.12"" --task {}. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LC_CTYPE = ""C.UTF-8"",. LANG = ""en_US.UTF-8"". are supported and ins",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/613
https://github.com/google/deepvariant/issues/613:1653,usability,command,command,1653,"ing script. ```. cat test0215.sh. WORK_DIR=/path1/4_Test/qingjiang/dpv. export TMPDIR=""$PWD/tmp_dir"". singularity run -B$TMPDIR:$TMPDIR,""${WORK_DIR}"":/mnt \. /path1/1_Software/dpv/deepvariant_1.4.0.sif /opt/deepvariant/bin/run_deepvariant \. --num_shards=3 \. --model_type=PACBIO \. --ref=/mnt/QJref.fa \. --reads=/mnt/input.bam \. --output_vcf=/mnt/output.vcf.gz \. --output_gvcf=/mnt/output.g.vcf.gz \. --intermediate_results_dir /mnt/dpv \. ```. The core error is `ValueError: NOT_FOUND: Could not open /mnt/input.bam. [E::hts_open_format] Failed to open file ""/mnt/input.bam"" : No such file or directory`However, I have verified the existence of `/mnt/input.bam`. The complete error information is as follows. Sincerely look forward to your help! thank you. ```. sh test0215.sh. I0216 00:56:07.446549 140582811191104 run_deepvariant.py:342] Re-using the directory for intermediate results in /mnt/dpv. ***** Intermediate results will be written to /mnt/dpv in docker. ****. ***** Running the command:*****. time seq 0 2 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/mnt/QJref.fa"" --reads ""/mnt/input.bam"" --examples ""/mnt/dpv/make_examples.tfrecord@3.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --gvcf ""/mnt/dpv/gvcf.tfrecord@3.gz"" --max_reads_per_partition ""600"" --min_mapping_quality ""1"" --parse_sam_aux_fields --partition_size ""25000"" --phase_reads --pileup_image_width ""199"" --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels ""0.12"" --task {}. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LC_CTYPE = ""C.UTF-8"",. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LC_CTYPE = ""C.UTF-8"",. LANG = ""en_US.U",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/613
https://github.com/google/deepvariant/issues/613:1804,usability,input,input,1804,"/mnt \. /path1/1_Software/dpv/deepvariant_1.4.0.sif /opt/deepvariant/bin/run_deepvariant \. --num_shards=3 \. --model_type=PACBIO \. --ref=/mnt/QJref.fa \. --reads=/mnt/input.bam \. --output_vcf=/mnt/output.vcf.gz \. --output_gvcf=/mnt/output.g.vcf.gz \. --intermediate_results_dir /mnt/dpv \. ```. The core error is `ValueError: NOT_FOUND: Could not open /mnt/input.bam. [E::hts_open_format] Failed to open file ""/mnt/input.bam"" : No such file or directory`However, I have verified the existence of `/mnt/input.bam`. The complete error information is as follows. Sincerely look forward to your help! thank you. ```. sh test0215.sh. I0216 00:56:07.446549 140582811191104 run_deepvariant.py:342] Re-using the directory for intermediate results in /mnt/dpv. ***** Intermediate results will be written to /mnt/dpv in docker. ****. ***** Running the command:*****. time seq 0 2 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/mnt/QJref.fa"" --reads ""/mnt/input.bam"" --examples ""/mnt/dpv/make_examples.tfrecord@3.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --gvcf ""/mnt/dpv/gvcf.tfrecord@3.gz"" --max_reads_per_partition ""600"" --min_mapping_quality ""1"" --parse_sam_aux_fields --partition_size ""25000"" --phase_reads --pileup_image_width ""199"" --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels ""0.12"" --task {}. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LC_CTYPE = ""C.UTF-8"",. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LC_CTYPE = ""C.UTF-8"",. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). [E::hts_open_format] Failed to open file ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/613
https://github.com/google/deepvariant/issues/613:2387,usability,support,supported,2387,"d to your help! thank you. ```. sh test0215.sh. I0216 00:56:07.446549 140582811191104 run_deepvariant.py:342] Re-using the directory for intermediate results in /mnt/dpv. ***** Intermediate results will be written to /mnt/dpv in docker. ****. ***** Running the command:*****. time seq 0 2 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/mnt/QJref.fa"" --reads ""/mnt/input.bam"" --examples ""/mnt/dpv/make_examples.tfrecord@3.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --gvcf ""/mnt/dpv/gvcf.tfrecord@3.gz"" --max_reads_per_partition ""600"" --min_mapping_quality ""1"" --parse_sam_aux_fields --partition_size ""25000"" --phase_reads --pileup_image_width ""199"" --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels ""0.12"" --task {}. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LC_CTYPE = ""C.UTF-8"",. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LC_CTYPE = ""C.UTF-8"",. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). [E::hts_open_format] Failed to open file ""/mnt/input.bam"" : No such file or directory. Traceback (most recent call last):. File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_p4oo9k4b/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>. app.run(main). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_p4oo9k4b/runfiles/absl_py/absl/app.py"", line 300, in run. _run_main(main, args). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_p4oo9k4b/runfiles/absl_py/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfile",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/613
https://github.com/google/deepvariant/issues/613:2668,usability,support,supported,2668,"seq 0 2 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/mnt/QJref.fa"" --reads ""/mnt/input.bam"" --examples ""/mnt/dpv/make_examples.tfrecord@3.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --gvcf ""/mnt/dpv/gvcf.tfrecord@3.gz"" --max_reads_per_partition ""600"" --min_mapping_quality ""1"" --parse_sam_aux_fields --partition_size ""25000"" --phase_reads --pileup_image_width ""199"" --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels ""0.12"" --task {}. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LC_CTYPE = ""C.UTF-8"",. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LC_CTYPE = ""C.UTF-8"",. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). [E::hts_open_format] Failed to open file ""/mnt/input.bam"" : No such file or directory. Traceback (most recent call last):. File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_p4oo9k4b/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>. app.run(main). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_p4oo9k4b/runfiles/absl_py/absl/app.py"", line 300, in run. _run_main(main, args). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_p4oo9k4b/runfiles/absl_py/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_p4oo9k4b/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 166, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_p4oo9k4b/runfiles/com_google_deepvariant/deepvariant/make_examples.py"",",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/613
https://github.com/google/deepvariant/issues/613:2813,usability,input,input,2813,"-examples ""/mnt/dpv/make_examples.tfrecord@3.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --gvcf ""/mnt/dpv/gvcf.tfrecord@3.gz"" --max_reads_per_partition ""600"" --min_mapping_quality ""1"" --parse_sam_aux_fields --partition_size ""25000"" --phase_reads --pileup_image_width ""199"" --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels ""0.12"" --task {}. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LC_CTYPE = ""C.UTF-8"",. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LC_CTYPE = ""C.UTF-8"",. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). [E::hts_open_format] Failed to open file ""/mnt/input.bam"" : No such file or directory. Traceback (most recent call last):. File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_p4oo9k4b/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>. app.run(main). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_p4oo9k4b/runfiles/absl_py/absl/app.py"", line 300, in run. _run_main(main, args). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_p4oo9k4b/runfiles/absl_py/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_p4oo9k4b/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 166, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_p4oo9k4b/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 128, in default_options. samples_in_order, sample_role_to_train = one_sample_from_flags(. File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/613
https://github.com/google/deepvariant/issues/613:4892,usability,input,input,4892,""", line 85, in one_sample_from_flags. sample_name = make_examples_core.assign_sample_name(. File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_p4oo9k4b/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 134, in assign_sample_name. with sam.SamReader(reads_filenames.split(',')[0]) as sam_reader:. File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_p4oo9k4b/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_p4oo9k4b/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 260, in _native_reader. return NativeSamReader(input_path, **kwargs). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_p4oo9k4b/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 227, in __init__. self._reader = sam_reader.SamReader.from_file(. ValueError: NOT_FOUND: Could not open /mnt/input.bam. [E::hts_open_format] Failed to open file ""/mnt/input.bam"" : No such file or directory. [E::hts_open_format] Failed to open file ""/mnt/input.bam"" : No such file or directory. Traceback (most recent call last):. File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_q7goget_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>. app.run(main). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_q7goget_/runfiles/absl_py/absl/app.py"", line 300, in run. _run_main(main, args). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_q7goget_/runfiles/absl_py/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_q7goget_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 166, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_q7goget_/runfiles/com_google_deepvariant/deepvariant/make_examples.py",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/613
https://github.com/google/deepvariant/issues/613:4950,usability,input,input,4950,"xamples_core.assign_sample_name(. File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_p4oo9k4b/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 134, in assign_sample_name. with sam.SamReader(reads_filenames.split(',')[0]) as sam_reader:. File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_p4oo9k4b/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_p4oo9k4b/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 260, in _native_reader. return NativeSamReader(input_path, **kwargs). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_p4oo9k4b/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 227, in __init__. self._reader = sam_reader.SamReader.from_file(. ValueError: NOT_FOUND: Could not open /mnt/input.bam. [E::hts_open_format] Failed to open file ""/mnt/input.bam"" : No such file or directory. [E::hts_open_format] Failed to open file ""/mnt/input.bam"" : No such file or directory. Traceback (most recent call last):. File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_q7goget_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>. app.run(main). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_q7goget_/runfiles/absl_py/absl/app.py"", line 300, in run. _run_main(main, args). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_q7goget_/runfiles/absl_py/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_q7goget_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 166, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_q7goget_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 128, in default_options. samples_in_order, sample_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/613
https://github.com/google/deepvariant/issues/613:5037,usability,input,input,5037,"les_p4oo9k4b/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 134, in assign_sample_name. with sam.SamReader(reads_filenames.split(',')[0]) as sam_reader:. File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_p4oo9k4b/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_p4oo9k4b/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 260, in _native_reader. return NativeSamReader(input_path, **kwargs). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_p4oo9k4b/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 227, in __init__. self._reader = sam_reader.SamReader.from_file(. ValueError: NOT_FOUND: Could not open /mnt/input.bam. [E::hts_open_format] Failed to open file ""/mnt/input.bam"" : No such file or directory. [E::hts_open_format] Failed to open file ""/mnt/input.bam"" : No such file or directory. Traceback (most recent call last):. File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_q7goget_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>. app.run(main). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_q7goget_/runfiles/absl_py/absl/app.py"", line 300, in run. _run_main(main, args). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_q7goget_/runfiles/absl_py/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_q7goget_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 166, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_q7goget_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 128, in default_options. samples_in_order, sample_role_to_train = one_sample_from_flags(. File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/613
https://github.com/google/deepvariant/issues/613:7116,usability,input,input,7116,""", line 85, in one_sample_from_flags. sample_name = make_examples_core.assign_sample_name(. File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_q7goget_/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 134, in assign_sample_name. with sam.SamReader(reads_filenames.split(',')[0]) as sam_reader:. File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_q7goget_/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_q7goget_/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 260, in _native_reader. return NativeSamReader(input_path, **kwargs). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_q7goget_/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 227, in __init__. self._reader = sam_reader.SamReader.from_file(. ValueError: NOT_FOUND: Could not open /mnt/input.bam. Traceback (most recent call last):. File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_gc9nyr7p/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>. app.run(main). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_gc9nyr7p/runfiles/absl_py/absl/app.py"", line 300, in run. _run_main(main, args). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_gc9nyr7p/runfiles/absl_py/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_gc9nyr7p/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 166, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_gc9nyr7p/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 128, in default_options. samples_in_order, sample_role_to_train = one_sample_from_flags(. File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_gc9nyr7p/runfiles/c",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/613
https://github.com/google/deepvariant/issues/613:9166,usability,input,input,9166,"nt/make_examples.py"", line 166, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_gc9nyr7p/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 128, in default_options. samples_in_order, sample_role_to_train = one_sample_from_flags(. File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_gc9nyr7p/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 85, in one_sample_from_flags. sample_name = make_examples_core.assign_sample_name(. File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_gc9nyr7p/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 134, in assign_sample_name. with sam.SamReader(reads_filenames.split(',')[0]) as sam_reader:. File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_gc9nyr7p/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_gc9nyr7p/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 260, in _native_reader. return NativeSamReader(input_path, **kwargs). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_gc9nyr7p/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 227, in __init__. self._reader = sam_reader.SamReader.from_file(. ValueError: NOT_FOUND: Could not open /mnt/input.bam. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /mnt/QJref.fa --reads /mnt/input.bam --examples /mnt/dpv/make_examples.tfrecord@3.gz --add_hp_channel --alt_aligned_pileup diff_channels --gvcf /mnt/dpv/gvcf.tfrecord@3.gz --max_reads_per_partition 600 --min_mapping_quality 1 --parse_sam_aux_fields --partition_size 25000 --phase_reads --pileup_image_width 199 --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels 0.12 --task 1. real 0m32.021s. user 0m5.396s. sys 0m4.055s. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/613
https://github.com/google/deepvariant/issues/613:9288,usability,input,input,9288,"nt/make_examples.py"", line 166, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_gc9nyr7p/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 128, in default_options. samples_in_order, sample_role_to_train = one_sample_from_flags(. File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_gc9nyr7p/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 85, in one_sample_from_flags. sample_name = make_examples_core.assign_sample_name(. File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_gc9nyr7p/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 134, in assign_sample_name. with sam.SamReader(reads_filenames.split(',')[0]) as sam_reader:. File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_gc9nyr7p/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_gc9nyr7p/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 260, in _native_reader. return NativeSamReader(input_path, **kwargs). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_gc9nyr7p/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 227, in __init__. self._reader = sam_reader.SamReader.from_file(. ValueError: NOT_FOUND: Could not open /mnt/input.bam. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /mnt/QJref.fa --reads /mnt/input.bam --examples /mnt/dpv/make_examples.tfrecord@3.gz --add_hp_channel --alt_aligned_pileup diff_channels --gvcf /mnt/dpv/gvcf.tfrecord@3.gz --max_reads_per_partition 600 --min_mapping_quality 1 --parse_sam_aux_fields --partition_size 25000 --phase_reads --pileup_image_width 199 --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels 0.12 --task 1. real 0m32.021s. user 0m5.396s. sys 0m4.055s. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/613
https://github.com/google/deepvariant/issues/613:9686,usability,user,user,9686,"nt/make_examples.py"", line 166, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_gc9nyr7p/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 128, in default_options. samples_in_order, sample_role_to_train = one_sample_from_flags(. File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_gc9nyr7p/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 85, in one_sample_from_flags. sample_name = make_examples_core.assign_sample_name(. File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_gc9nyr7p/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 134, in assign_sample_name. with sam.SamReader(reads_filenames.split(',')[0]) as sam_reader:. File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_gc9nyr7p/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_gc9nyr7p/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 260, in _native_reader. return NativeSamReader(input_path, **kwargs). File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_gc9nyr7p/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 227, in __init__. self._reader = sam_reader.SamReader.from_file(. ValueError: NOT_FOUND: Could not open /mnt/input.bam. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /mnt/QJref.fa --reads /mnt/input.bam --examples /mnt/dpv/make_examples.tfrecord@3.gz --add_hp_channel --alt_aligned_pileup diff_channels --gvcf /mnt/dpv/gvcf.tfrecord@3.gz --max_reads_per_partition 600 --min_mapping_quality 1 --parse_sam_aux_fields --partition_size 25000 --phase_reads --pileup_image_width 199 --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels 0.12 --task 1. real 0m32.021s. user 0m5.396s. sys 0m4.055s. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/613
https://github.com/google/deepvariant/issues/613:304,safety,input,input,304,"Hi @Zero-Sun . It has been a while since our last answer. Not sure if you've resolved this yet. It seems like you did check the files with `ls` earlier. Is it possible to do something like:. ```. singularity run -B$TMPDIR:$TMPDIR,""${WORK_DIR}"":/mnt \. /path1/1_Software/dpv/deepvariant_1.4.0.sif ls /mnt/input.bam. ```. to make sure you can indeed see the file under the context?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/613
https://github.com/google/deepvariant/issues/613:371,testability,context,context,371,"Hi @Zero-Sun . It has been a while since our last answer. Not sure if you've resolved this yet. It seems like you did check the files with `ls` earlier. Is it possible to do something like:. ```. singularity run -B$TMPDIR:$TMPDIR,""${WORK_DIR}"":/mnt \. /path1/1_Software/dpv/deepvariant_1.4.0.sif ls /mnt/input.bam. ```. to make sure you can indeed see the file under the context?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/613
https://github.com/google/deepvariant/issues/613:304,usability,input,input,304,"Hi @Zero-Sun . It has been a while since our last answer. Not sure if you've resolved this yet. It seems like you did check the files with `ls` earlier. Is it possible to do something like:. ```. singularity run -B$TMPDIR:$TMPDIR,""${WORK_DIR}"":/mnt \. /path1/1_Software/dpv/deepvariant_1.4.0.sif ls /mnt/input.bam. ```. to make sure you can indeed see the file under the context?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/613
https://github.com/google/deepvariant/issues/613:20,usability,close,close,20,"Hi @Zero-Sun , I'll close this issue. Feel free to reopen if you still have questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/613
https://github.com/google/deepvariant/issues/614:190,availability,Down,Downsampling,190,"What is the coverage, approximately? DeepVariant's pileup images can only really fit ~100 reads at each locus, but if you have very high coverage that could cause it to use a lot of memory. Downsampling in that case to about 100X would help.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/614
https://github.com/google/deepvariant/issues/614:182,performance,memor,memory,182,"What is the coverage, approximately? DeepVariant's pileup images can only really fit ~100 reads at each locus, but if you have very high coverage that could cause it to use a lot of memory. Downsampling in that case to about 100X would help.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/614
https://github.com/google/deepvariant/issues/614:12,testability,coverag,coverage,12,"What is the coverage, approximately? DeepVariant's pileup images can only really fit ~100 reads at each locus, but if you have very high coverage that could cause it to use a lot of memory. Downsampling in that case to about 100X would help.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/614
https://github.com/google/deepvariant/issues/614:137,testability,coverag,coverage,137,"What is the coverage, approximately? DeepVariant's pileup images can only really fit ~100 reads at each locus, but if you have very high coverage that could cause it to use a lot of memory. Downsampling in that case to about 100X would help.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/614
https://github.com/google/deepvariant/issues/614:182,usability,memor,memory,182,"What is the coverage, approximately? DeepVariant's pileup images can only really fit ~100 reads at each locus, but if you have very high coverage that could cause it to use a lot of memory. Downsampling in that case to about 100X would help.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/614
https://github.com/google/deepvariant/issues/614:236,usability,help,help,236,"What is the coverage, approximately? DeepVariant's pileup images can only really fit ~100 reads at each locus, but if you have very high coverage that could cause it to use a lot of memory. Downsampling in that case to about 100X would help.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/614
https://github.com/google/deepvariant/issues/614:209,availability,down,downsampling,209,"Thank you for the response. Using samtools coverage, I see that mean coverage are in ~130X for some chromosomes and 224X with mitochondria with one of the files, and I suspect that other files are similar. By downsampling, do you mean something like splitting fastq files (say 3 files with 1/3 of original using something like fastqsplitter)? I have never done this before, so an explicit answer would be greatly appreciated. Also, do you have suggestions on what to use to merge gvcf or vcf output?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/614
https://github.com/google/deepvariant/issues/614:43,testability,coverag,coverage,43,"Thank you for the response. Using samtools coverage, I see that mean coverage are in ~130X for some chromosomes and 224X with mitochondria with one of the files, and I suspect that other files are similar. By downsampling, do you mean something like splitting fastq files (say 3 files with 1/3 of original using something like fastqsplitter)? I have never done this before, so an explicit answer would be greatly appreciated. Also, do you have suggestions on what to use to merge gvcf or vcf output?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/614
https://github.com/google/deepvariant/issues/614:69,testability,coverag,coverage,69,"Thank you for the response. Using samtools coverage, I see that mean coverage are in ~130X for some chromosomes and 224X with mitochondria with one of the files, and I suspect that other files are similar. By downsampling, do you mean something like splitting fastq files (say 3 files with 1/3 of original using something like fastqsplitter)? I have never done this before, so an explicit answer would be greatly appreciated. Also, do you have suggestions on what to use to merge gvcf or vcf output?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/614
https://github.com/google/deepvariant/issues/614:161,modifiability,Pac,PacBio,161,Closing this. DeepVariant ran 'successfully' after splitting fastq files - although the output was bizarre. It is pretty clear that using DeepVariant with older PacBio data is not worthwhile -- my impression is that older PacBio data is only useful for finding larger SVs and worth using only if there is nothing better (e.g. PacBio HiFi or ONT).,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/614
https://github.com/google/deepvariant/issues/614:222,modifiability,Pac,PacBio,222,Closing this. DeepVariant ran 'successfully' after splitting fastq files - although the output was bizarre. It is pretty clear that using DeepVariant with older PacBio data is not worthwhile -- my impression is that older PacBio data is only useful for finding larger SVs and worth using only if there is nothing better (e.g. PacBio HiFi or ONT).,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/614
https://github.com/google/deepvariant/issues/614:326,modifiability,Pac,PacBio,326,Closing this. DeepVariant ran 'successfully' after splitting fastq files - although the output was bizarre. It is pretty clear that using DeepVariant with older PacBio data is not worthwhile -- my impression is that older PacBio data is only useful for finding larger SVs and worth using only if there is nothing better (e.g. PacBio HiFi or ONT).,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/614
https://github.com/google/deepvariant/issues/614:121,usability,clear,clear,121,Closing this. DeepVariant ran 'successfully' after splitting fastq files - although the output was bizarre. It is pretty clear that using DeepVariant with older PacBio data is not worthwhile -- my impression is that older PacBio data is only useful for finding larger SVs and worth using only if there is nothing better (e.g. PacBio HiFi or ONT).,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/614
https://github.com/google/deepvariant/issues/615:161,interoperability,format,format,161,I covered this in a video on my YouTube channel actually :). It's right around this time point: https://youtu.be/MrVpn0vpIYU?t=885. You can also consult the VCF format specification for more details: https://samtools.github.io/hts-specs/VCFv4.2.pdf.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/615
https://github.com/google/deepvariant/issues/615:168,interoperability,specif,specification,168,I covered this in a video on my YouTube channel actually :). It's right around this time point: https://youtu.be/MrVpn0vpIYU?t=885. You can also consult the VCF format specification for more details: https://samtools.github.io/hts-specs/VCFv4.2.pdf.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/615
https://github.com/google/deepvariant/issues/615:84,performance,time,time,84,I covered this in a video on my YouTube channel actually :). It's right around this time point: https://youtu.be/MrVpn0vpIYU?t=885. You can also consult the VCF format specification for more details: https://samtools.github.io/hts-specs/VCFv4.2.pdf.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/615
https://github.com/google/deepvariant/issues/616:145,availability,error,errors,145,"You're very welcome :). We can speculate, but it might just be sort of random things happening on the edges of the regions due to low numbers of errors overall. If you look at the number of errors different between these runs, it's like 26 vs 24 false positives for indels. I wouldn't draw any conclusions of trends from such small numbers. If you're curious you could inspect them in IGV though. I don't think we have any more insight into this from the DeepVariant side than you do.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/616
https://github.com/google/deepvariant/issues/616:190,availability,error,errors,190,"You're very welcome :). We can speculate, but it might just be sort of random things happening on the edges of the regions due to low numbers of errors overall. If you look at the number of errors different between these runs, it's like 26 vs 24 false positives for indels. I wouldn't draw any conclusions of trends from such small numbers. If you're curious you could inspect them in IGV though. I don't think we have any more insight into this from the DeepVariant side than you do.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/616
https://github.com/google/deepvariant/issues/616:285,energy efficiency,draw,draw,285,"You're very welcome :). We can speculate, but it might just be sort of random things happening on the edges of the regions due to low numbers of errors overall. If you look at the number of errors different between these runs, it's like 26 vs 24 false positives for indels. I wouldn't draw any conclusions of trends from such small numbers. If you're curious you could inspect them in IGV though. I don't think we have any more insight into this from the DeepVariant side than you do.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/616
https://github.com/google/deepvariant/issues/616:145,performance,error,errors,145,"You're very welcome :). We can speculate, but it might just be sort of random things happening on the edges of the regions due to low numbers of errors overall. If you look at the number of errors different between these runs, it's like 26 vs 24 false positives for indels. I wouldn't draw any conclusions of trends from such small numbers. If you're curious you could inspect them in IGV though. I don't think we have any more insight into this from the DeepVariant side than you do.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/616
https://github.com/google/deepvariant/issues/616:190,performance,error,errors,190,"You're very welcome :). We can speculate, but it might just be sort of random things happening on the edges of the regions due to low numbers of errors overall. If you look at the number of errors different between these runs, it's like 26 vs 24 false positives for indels. I wouldn't draw any conclusions of trends from such small numbers. If you're curious you could inspect them in IGV though. I don't think we have any more insight into this from the DeepVariant side than you do.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/616
https://github.com/google/deepvariant/issues/616:145,safety,error,errors,145,"You're very welcome :). We can speculate, but it might just be sort of random things happening on the edges of the regions due to low numbers of errors overall. If you look at the number of errors different between these runs, it's like 26 vs 24 false positives for indels. I wouldn't draw any conclusions of trends from such small numbers. If you're curious you could inspect them in IGV though. I don't think we have any more insight into this from the DeepVariant side than you do.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/616
https://github.com/google/deepvariant/issues/616:190,safety,error,errors,190,"You're very welcome :). We can speculate, but it might just be sort of random things happening on the edges of the regions due to low numbers of errors overall. If you look at the number of errors different between these runs, it's like 26 vs 24 false positives for indels. I wouldn't draw any conclusions of trends from such small numbers. If you're curious you could inspect them in IGV though. I don't think we have any more insight into this from the DeepVariant side than you do.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/616
https://github.com/google/deepvariant/issues/616:145,usability,error,errors,145,"You're very welcome :). We can speculate, but it might just be sort of random things happening on the edges of the regions due to low numbers of errors overall. If you look at the number of errors different between these runs, it's like 26 vs 24 false positives for indels. I wouldn't draw any conclusions of trends from such small numbers. If you're curious you could inspect them in IGV though. I don't think we have any more insight into this from the DeepVariant side than you do.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/616
https://github.com/google/deepvariant/issues/616:190,usability,error,errors,190,"You're very welcome :). We can speculate, but it might just be sort of random things happening on the edges of the regions due to low numbers of errors overall. If you look at the number of errors different between these runs, it's like 26 vs 24 false positives for indels. I wouldn't draw any conclusions of trends from such small numbers. If you're curious you could inspect them in IGV though. I don't think we have any more insight into this from the DeepVariant side than you do.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/616
https://github.com/google/deepvariant/issues/616:43,integrability,event,event,43,"Thanks for your respond！. If it's a random event, I think I can accept it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/616
https://github.com/google/deepvariant/issues/616:82,availability,error,errors,82,"Hi @zxy1555847 . @MariaNattestad is correct that there are only a small number of errors, so it is hard to definitively tell you what is going on. . However, one thing I want to point out is that in general for exome sequencing, we expect for all analysis methods, accuracy will start dropping outside of the capture ranges with an increasing amount the farther we go from the capture. We also expect Indel to be affected more than SNP. . The reasons for this is that sequence coverage begins to drop toward the boundaries of the capture (the amount of this drop depends on the particular capture and the sequence context around it, but on average it will be the case). In general, lower coverage will mean lower accuracy, but we observe that coverage has a larger effect on Indels than SNPs (this is detailed in our [Extensive sequence dataset](https://www.biorxiv.org/content/10.1101/2020.12.11.422022v1) paper. The reasons that are complex (though if you want me to further elaborate, I can try). . In short, Indel accuracy dropping outside of capture regions is expected to some extent, and this is a function of the underlying sequencing method as opposed to the analysis method.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/616
https://github.com/google/deepvariant/issues/616:563,deployability,depend,depends,563,"Hi @zxy1555847 . @MariaNattestad is correct that there are only a small number of errors, so it is hard to definitively tell you what is going on. . However, one thing I want to point out is that in general for exome sequencing, we expect for all analysis methods, accuracy will start dropping outside of the capture ranges with an increasing amount the farther we go from the capture. We also expect Indel to be affected more than SNP. . The reasons for this is that sequence coverage begins to drop toward the boundaries of the capture (the amount of this drop depends on the particular capture and the sequence context around it, but on average it will be the case). In general, lower coverage will mean lower accuracy, but we observe that coverage has a larger effect on Indels than SNPs (this is detailed in our [Extensive sequence dataset](https://www.biorxiv.org/content/10.1101/2020.12.11.422022v1) paper. The reasons that are complex (though if you want me to further elaborate, I can try). . In short, Indel accuracy dropping outside of capture regions is expected to some extent, and this is a function of the underlying sequencing method as opposed to the analysis method.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/616
https://github.com/google/deepvariant/issues/616:730,deployability,observ,observe,730,"Hi @zxy1555847 . @MariaNattestad is correct that there are only a small number of errors, so it is hard to definitively tell you what is going on. . However, one thing I want to point out is that in general for exome sequencing, we expect for all analysis methods, accuracy will start dropping outside of the capture ranges with an increasing amount the farther we go from the capture. We also expect Indel to be affected more than SNP. . The reasons for this is that sequence coverage begins to drop toward the boundaries of the capture (the amount of this drop depends on the particular capture and the sequence context around it, but on average it will be the case). In general, lower coverage will mean lower accuracy, but we observe that coverage has a larger effect on Indels than SNPs (this is detailed in our [Extensive sequence dataset](https://www.biorxiv.org/content/10.1101/2020.12.11.422022v1) paper. The reasons that are complex (though if you want me to further elaborate, I can try). . In short, Indel accuracy dropping outside of capture regions is expected to some extent, and this is a function of the underlying sequencing method as opposed to the analysis method.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/616
https://github.com/google/deepvariant/issues/616:563,integrability,depend,depends,563,"Hi @zxy1555847 . @MariaNattestad is correct that there are only a small number of errors, so it is hard to definitively tell you what is going on. . However, one thing I want to point out is that in general for exome sequencing, we expect for all analysis methods, accuracy will start dropping outside of the capture ranges with an increasing amount the farther we go from the capture. We also expect Indel to be affected more than SNP. . The reasons for this is that sequence coverage begins to drop toward the boundaries of the capture (the amount of this drop depends on the particular capture and the sequence context around it, but on average it will be the case). In general, lower coverage will mean lower accuracy, but we observe that coverage has a larger effect on Indels than SNPs (this is detailed in our [Extensive sequence dataset](https://www.biorxiv.org/content/10.1101/2020.12.11.422022v1) paper. The reasons that are complex (though if you want me to further elaborate, I can try). . In short, Indel accuracy dropping outside of capture regions is expected to some extent, and this is a function of the underlying sequencing method as opposed to the analysis method.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/616
https://github.com/google/deepvariant/issues/616:563,modifiability,depend,depends,563,"Hi @zxy1555847 . @MariaNattestad is correct that there are only a small number of errors, so it is hard to definitively tell you what is going on. . However, one thing I want to point out is that in general for exome sequencing, we expect for all analysis methods, accuracy will start dropping outside of the capture ranges with an increasing amount the farther we go from the capture. We also expect Indel to be affected more than SNP. . The reasons for this is that sequence coverage begins to drop toward the boundaries of the capture (the amount of this drop depends on the particular capture and the sequence context around it, but on average it will be the case). In general, lower coverage will mean lower accuracy, but we observe that coverage has a larger effect on Indels than SNPs (this is detailed in our [Extensive sequence dataset](https://www.biorxiv.org/content/10.1101/2020.12.11.422022v1) paper. The reasons that are complex (though if you want me to further elaborate, I can try). . In short, Indel accuracy dropping outside of capture regions is expected to some extent, and this is a function of the underlying sequencing method as opposed to the analysis method.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/616
https://github.com/google/deepvariant/issues/616:818,modifiability,Extens,Extensive,818,"Hi @zxy1555847 . @MariaNattestad is correct that there are only a small number of errors, so it is hard to definitively tell you what is going on. . However, one thing I want to point out is that in general for exome sequencing, we expect for all analysis methods, accuracy will start dropping outside of the capture ranges with an increasing amount the farther we go from the capture. We also expect Indel to be affected more than SNP. . The reasons for this is that sequence coverage begins to drop toward the boundaries of the capture (the amount of this drop depends on the particular capture and the sequence context around it, but on average it will be the case). In general, lower coverage will mean lower accuracy, but we observe that coverage has a larger effect on Indels than SNPs (this is detailed in our [Extensive sequence dataset](https://www.biorxiv.org/content/10.1101/2020.12.11.422022v1) paper. The reasons that are complex (though if you want me to further elaborate, I can try). . In short, Indel accuracy dropping outside of capture regions is expected to some extent, and this is a function of the underlying sequencing method as opposed to the analysis method.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/616
https://github.com/google/deepvariant/issues/616:1083,modifiability,exten,extent,1083,"Hi @zxy1555847 . @MariaNattestad is correct that there are only a small number of errors, so it is hard to definitively tell you what is going on. . However, one thing I want to point out is that in general for exome sequencing, we expect for all analysis methods, accuracy will start dropping outside of the capture ranges with an increasing amount the farther we go from the capture. We also expect Indel to be affected more than SNP. . The reasons for this is that sequence coverage begins to drop toward the boundaries of the capture (the amount of this drop depends on the particular capture and the sequence context around it, but on average it will be the case). In general, lower coverage will mean lower accuracy, but we observe that coverage has a larger effect on Indels than SNPs (this is detailed in our [Extensive sequence dataset](https://www.biorxiv.org/content/10.1101/2020.12.11.422022v1) paper. The reasons that are complex (though if you want me to further elaborate, I can try). . In short, Indel accuracy dropping outside of capture regions is expected to some extent, and this is a function of the underlying sequencing method as opposed to the analysis method.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/616
https://github.com/google/deepvariant/issues/616:82,performance,error,errors,82,"Hi @zxy1555847 . @MariaNattestad is correct that there are only a small number of errors, so it is hard to definitively tell you what is going on. . However, one thing I want to point out is that in general for exome sequencing, we expect for all analysis methods, accuracy will start dropping outside of the capture ranges with an increasing amount the farther we go from the capture. We also expect Indel to be affected more than SNP. . The reasons for this is that sequence coverage begins to drop toward the boundaries of the capture (the amount of this drop depends on the particular capture and the sequence context around it, but on average it will be the case). In general, lower coverage will mean lower accuracy, but we observe that coverage has a larger effect on Indels than SNPs (this is detailed in our [Extensive sequence dataset](https://www.biorxiv.org/content/10.1101/2020.12.11.422022v1) paper. The reasons that are complex (though if you want me to further elaborate, I can try). . In short, Indel accuracy dropping outside of capture regions is expected to some extent, and this is a function of the underlying sequencing method as opposed to the analysis method.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/616
https://github.com/google/deepvariant/issues/616:870,performance,content,content,870,"Hi @zxy1555847 . @MariaNattestad is correct that there are only a small number of errors, so it is hard to definitively tell you what is going on. . However, one thing I want to point out is that in general for exome sequencing, we expect for all analysis methods, accuracy will start dropping outside of the capture ranges with an increasing amount the farther we go from the capture. We also expect Indel to be affected more than SNP. . The reasons for this is that sequence coverage begins to drop toward the boundaries of the capture (the amount of this drop depends on the particular capture and the sequence context around it, but on average it will be the case). In general, lower coverage will mean lower accuracy, but we observe that coverage has a larger effect on Indels than SNPs (this is detailed in our [Extensive sequence dataset](https://www.biorxiv.org/content/10.1101/2020.12.11.422022v1) paper. The reasons that are complex (though if you want me to further elaborate, I can try). . In short, Indel accuracy dropping outside of capture regions is expected to some extent, and this is a function of the underlying sequencing method as opposed to the analysis method.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/616
https://github.com/google/deepvariant/issues/616:82,safety,error,errors,82,"Hi @zxy1555847 . @MariaNattestad is correct that there are only a small number of errors, so it is hard to definitively tell you what is going on. . However, one thing I want to point out is that in general for exome sequencing, we expect for all analysis methods, accuracy will start dropping outside of the capture ranges with an increasing amount the farther we go from the capture. We also expect Indel to be affected more than SNP. . The reasons for this is that sequence coverage begins to drop toward the boundaries of the capture (the amount of this drop depends on the particular capture and the sequence context around it, but on average it will be the case). In general, lower coverage will mean lower accuracy, but we observe that coverage has a larger effect on Indels than SNPs (this is detailed in our [Extensive sequence dataset](https://www.biorxiv.org/content/10.1101/2020.12.11.422022v1) paper. The reasons that are complex (though if you want me to further elaborate, I can try). . In short, Indel accuracy dropping outside of capture regions is expected to some extent, and this is a function of the underlying sequencing method as opposed to the analysis method.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/616
https://github.com/google/deepvariant/issues/616:563,safety,depend,depends,563,"Hi @zxy1555847 . @MariaNattestad is correct that there are only a small number of errors, so it is hard to definitively tell you what is going on. . However, one thing I want to point out is that in general for exome sequencing, we expect for all analysis methods, accuracy will start dropping outside of the capture ranges with an increasing amount the farther we go from the capture. We also expect Indel to be affected more than SNP. . The reasons for this is that sequence coverage begins to drop toward the boundaries of the capture (the amount of this drop depends on the particular capture and the sequence context around it, but on average it will be the case). In general, lower coverage will mean lower accuracy, but we observe that coverage has a larger effect on Indels than SNPs (this is detailed in our [Extensive sequence dataset](https://www.biorxiv.org/content/10.1101/2020.12.11.422022v1) paper. The reasons that are complex (though if you want me to further elaborate, I can try). . In short, Indel accuracy dropping outside of capture regions is expected to some extent, and this is a function of the underlying sequencing method as opposed to the analysis method.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/616
https://github.com/google/deepvariant/issues/616:935,safety,compl,complex,935,"Hi @zxy1555847 . @MariaNattestad is correct that there are only a small number of errors, so it is hard to definitively tell you what is going on. . However, one thing I want to point out is that in general for exome sequencing, we expect for all analysis methods, accuracy will start dropping outside of the capture ranges with an increasing amount the farther we go from the capture. We also expect Indel to be affected more than SNP. . The reasons for this is that sequence coverage begins to drop toward the boundaries of the capture (the amount of this drop depends on the particular capture and the sequence context around it, but on average it will be the case). In general, lower coverage will mean lower accuracy, but we observe that coverage has a larger effect on Indels than SNPs (this is detailed in our [Extensive sequence dataset](https://www.biorxiv.org/content/10.1101/2020.12.11.422022v1) paper. The reasons that are complex (though if you want me to further elaborate, I can try). . In short, Indel accuracy dropping outside of capture regions is expected to some extent, and this is a function of the underlying sequencing method as opposed to the analysis method.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/616
https://github.com/google/deepvariant/issues/616:935,security,compl,complex,935,"Hi @zxy1555847 . @MariaNattestad is correct that there are only a small number of errors, so it is hard to definitively tell you what is going on. . However, one thing I want to point out is that in general for exome sequencing, we expect for all analysis methods, accuracy will start dropping outside of the capture ranges with an increasing amount the farther we go from the capture. We also expect Indel to be affected more than SNP. . The reasons for this is that sequence coverage begins to drop toward the boundaries of the capture (the amount of this drop depends on the particular capture and the sequence context around it, but on average it will be the case). In general, lower coverage will mean lower accuracy, but we observe that coverage has a larger effect on Indels than SNPs (this is detailed in our [Extensive sequence dataset](https://www.biorxiv.org/content/10.1101/2020.12.11.422022v1) paper. The reasons that are complex (though if you want me to further elaborate, I can try). . In short, Indel accuracy dropping outside of capture regions is expected to some extent, and this is a function of the underlying sequencing method as opposed to the analysis method.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/616
https://github.com/google/deepvariant/issues/616:477,testability,coverag,coverage,477,"Hi @zxy1555847 . @MariaNattestad is correct that there are only a small number of errors, so it is hard to definitively tell you what is going on. . However, one thing I want to point out is that in general for exome sequencing, we expect for all analysis methods, accuracy will start dropping outside of the capture ranges with an increasing amount the farther we go from the capture. We also expect Indel to be affected more than SNP. . The reasons for this is that sequence coverage begins to drop toward the boundaries of the capture (the amount of this drop depends on the particular capture and the sequence context around it, but on average it will be the case). In general, lower coverage will mean lower accuracy, but we observe that coverage has a larger effect on Indels than SNPs (this is detailed in our [Extensive sequence dataset](https://www.biorxiv.org/content/10.1101/2020.12.11.422022v1) paper. The reasons that are complex (though if you want me to further elaborate, I can try). . In short, Indel accuracy dropping outside of capture regions is expected to some extent, and this is a function of the underlying sequencing method as opposed to the analysis method.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/616
https://github.com/google/deepvariant/issues/616:563,testability,depend,depends,563,"Hi @zxy1555847 . @MariaNattestad is correct that there are only a small number of errors, so it is hard to definitively tell you what is going on. . However, one thing I want to point out is that in general for exome sequencing, we expect for all analysis methods, accuracy will start dropping outside of the capture ranges with an increasing amount the farther we go from the capture. We also expect Indel to be affected more than SNP. . The reasons for this is that sequence coverage begins to drop toward the boundaries of the capture (the amount of this drop depends on the particular capture and the sequence context around it, but on average it will be the case). In general, lower coverage will mean lower accuracy, but we observe that coverage has a larger effect on Indels than SNPs (this is detailed in our [Extensive sequence dataset](https://www.biorxiv.org/content/10.1101/2020.12.11.422022v1) paper. The reasons that are complex (though if you want me to further elaborate, I can try). . In short, Indel accuracy dropping outside of capture regions is expected to some extent, and this is a function of the underlying sequencing method as opposed to the analysis method.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/616
https://github.com/google/deepvariant/issues/616:614,testability,context,context,614,"Hi @zxy1555847 . @MariaNattestad is correct that there are only a small number of errors, so it is hard to definitively tell you what is going on. . However, one thing I want to point out is that in general for exome sequencing, we expect for all analysis methods, accuracy will start dropping outside of the capture ranges with an increasing amount the farther we go from the capture. We also expect Indel to be affected more than SNP. . The reasons for this is that sequence coverage begins to drop toward the boundaries of the capture (the amount of this drop depends on the particular capture and the sequence context around it, but on average it will be the case). In general, lower coverage will mean lower accuracy, but we observe that coverage has a larger effect on Indels than SNPs (this is detailed in our [Extensive sequence dataset](https://www.biorxiv.org/content/10.1101/2020.12.11.422022v1) paper. The reasons that are complex (though if you want me to further elaborate, I can try). . In short, Indel accuracy dropping outside of capture regions is expected to some extent, and this is a function of the underlying sequencing method as opposed to the analysis method.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/616
https://github.com/google/deepvariant/issues/616:688,testability,coverag,coverage,688,"Hi @zxy1555847 . @MariaNattestad is correct that there are only a small number of errors, so it is hard to definitively tell you what is going on. . However, one thing I want to point out is that in general for exome sequencing, we expect for all analysis methods, accuracy will start dropping outside of the capture ranges with an increasing amount the farther we go from the capture. We also expect Indel to be affected more than SNP. . The reasons for this is that sequence coverage begins to drop toward the boundaries of the capture (the amount of this drop depends on the particular capture and the sequence context around it, but on average it will be the case). In general, lower coverage will mean lower accuracy, but we observe that coverage has a larger effect on Indels than SNPs (this is detailed in our [Extensive sequence dataset](https://www.biorxiv.org/content/10.1101/2020.12.11.422022v1) paper. The reasons that are complex (though if you want me to further elaborate, I can try). . In short, Indel accuracy dropping outside of capture regions is expected to some extent, and this is a function of the underlying sequencing method as opposed to the analysis method.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/616
https://github.com/google/deepvariant/issues/616:730,testability,observ,observe,730,"Hi @zxy1555847 . @MariaNattestad is correct that there are only a small number of errors, so it is hard to definitively tell you what is going on. . However, one thing I want to point out is that in general for exome sequencing, we expect for all analysis methods, accuracy will start dropping outside of the capture ranges with an increasing amount the farther we go from the capture. We also expect Indel to be affected more than SNP. . The reasons for this is that sequence coverage begins to drop toward the boundaries of the capture (the amount of this drop depends on the particular capture and the sequence context around it, but on average it will be the case). In general, lower coverage will mean lower accuracy, but we observe that coverage has a larger effect on Indels than SNPs (this is detailed in our [Extensive sequence dataset](https://www.biorxiv.org/content/10.1101/2020.12.11.422022v1) paper. The reasons that are complex (though if you want me to further elaborate, I can try). . In short, Indel accuracy dropping outside of capture regions is expected to some extent, and this is a function of the underlying sequencing method as opposed to the analysis method.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/616
https://github.com/google/deepvariant/issues/616:743,testability,coverag,coverage,743,"Hi @zxy1555847 . @MariaNattestad is correct that there are only a small number of errors, so it is hard to definitively tell you what is going on. . However, one thing I want to point out is that in general for exome sequencing, we expect for all analysis methods, accuracy will start dropping outside of the capture ranges with an increasing amount the farther we go from the capture. We also expect Indel to be affected more than SNP. . The reasons for this is that sequence coverage begins to drop toward the boundaries of the capture (the amount of this drop depends on the particular capture and the sequence context around it, but on average it will be the case). In general, lower coverage will mean lower accuracy, but we observe that coverage has a larger effect on Indels than SNPs (this is detailed in our [Extensive sequence dataset](https://www.biorxiv.org/content/10.1101/2020.12.11.422022v1) paper. The reasons that are complex (though if you want me to further elaborate, I can try). . In short, Indel accuracy dropping outside of capture regions is expected to some extent, and this is a function of the underlying sequencing method as opposed to the analysis method.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/616
https://github.com/google/deepvariant/issues/616:82,usability,error,errors,82,"Hi @zxy1555847 . @MariaNattestad is correct that there are only a small number of errors, so it is hard to definitively tell you what is going on. . However, one thing I want to point out is that in general for exome sequencing, we expect for all analysis methods, accuracy will start dropping outside of the capture ranges with an increasing amount the farther we go from the capture. We also expect Indel to be affected more than SNP. . The reasons for this is that sequence coverage begins to drop toward the boundaries of the capture (the amount of this drop depends on the particular capture and the sequence context around it, but on average it will be the case). In general, lower coverage will mean lower accuracy, but we observe that coverage has a larger effect on Indels than SNPs (this is detailed in our [Extensive sequence dataset](https://www.biorxiv.org/content/10.1101/2020.12.11.422022v1) paper. The reasons that are complex (though if you want me to further elaborate, I can try). . In short, Indel accuracy dropping outside of capture regions is expected to some extent, and this is a function of the underlying sequencing method as opposed to the analysis method.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/616
https://github.com/google/deepvariant/issues/616:66,testability,understand,understand,66,Thank you very much for your kind and detailed reply. I now fully understand the nature of the problem.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/616
https://github.com/google/deepvariant/issues/617:131,energy efficiency,CPU,CPUS,131,"Hi @WenyuLiang ,. We have used the standard ONT mapping command to map Duplex reads to fastq:. ```. minimap2 -k 17 -ax map-ont -t <CPUS> <REF> <FASTQ>. ```. We have not tested other mapping modes for R10.4 to see if it improves overall mapping. If that is something you explore on your end and find improvements, please let us know by opening a github issue or reaching out over an email as we have not explored this extensively.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/617
https://github.com/google/deepvariant/issues/617:35,interoperability,standard,standard,35,"Hi @WenyuLiang ,. We have used the standard ONT mapping command to map Duplex reads to fastq:. ```. minimap2 -k 17 -ax map-ont -t <CPUS> <REF> <FASTQ>. ```. We have not tested other mapping modes for R10.4 to see if it improves overall mapping. If that is something you explore on your end and find improvements, please let us know by opening a github issue or reaching out over an email as we have not explored this extensively.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/617
https://github.com/google/deepvariant/issues/617:417,modifiability,extens,extensively,417,"Hi @WenyuLiang ,. We have used the standard ONT mapping command to map Duplex reads to fastq:. ```. minimap2 -k 17 -ax map-ont -t <CPUS> <REF> <FASTQ>. ```. We have not tested other mapping modes for R10.4 to see if it improves overall mapping. If that is something you explore on your end and find improvements, please let us know by opening a github issue or reaching out over an email as we have not explored this extensively.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/617
https://github.com/google/deepvariant/issues/617:131,performance,CPU,CPUS,131,"Hi @WenyuLiang ,. We have used the standard ONT mapping command to map Duplex reads to fastq:. ```. minimap2 -k 17 -ax map-ont -t <CPUS> <REF> <FASTQ>. ```. We have not tested other mapping modes for R10.4 to see if it improves overall mapping. If that is something you explore on your end and find improvements, please let us know by opening a github issue or reaching out over an email as we have not explored this extensively.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/617
https://github.com/google/deepvariant/issues/617:169,safety,test,tested,169,"Hi @WenyuLiang ,. We have used the standard ONT mapping command to map Duplex reads to fastq:. ```. minimap2 -k 17 -ax map-ont -t <CPUS> <REF> <FASTQ>. ```. We have not tested other mapping modes for R10.4 to see if it improves overall mapping. If that is something you explore on your end and find improvements, please let us know by opening a github issue or reaching out over an email as we have not explored this extensively.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/617
https://github.com/google/deepvariant/issues/617:169,testability,test,tested,169,"Hi @WenyuLiang ,. We have used the standard ONT mapping command to map Duplex reads to fastq:. ```. minimap2 -k 17 -ax map-ont -t <CPUS> <REF> <FASTQ>. ```. We have not tested other mapping modes for R10.4 to see if it improves overall mapping. If that is something you explore on your end and find improvements, please let us know by opening a github issue or reaching out over an email as we have not explored this extensively.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/617
https://github.com/google/deepvariant/issues/617:56,usability,command,command,56,"Hi @WenyuLiang ,. We have used the standard ONT mapping command to map Duplex reads to fastq:. ```. minimap2 -k 17 -ax map-ont -t <CPUS> <REF> <FASTQ>. ```. We have not tested other mapping modes for R10.4 to see if it improves overall mapping. If that is something you explore on your end and find improvements, please let us know by opening a github issue or reaching out over an email as we have not explored this extensively.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/617
https://github.com/google/deepvariant/issues/617:46,modifiability,paramet,parameters,46,Sure! I will post it here if there are better parameters to use. Thanks,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/617
https://github.com/google/deepvariant/issues/617:66,deployability,updat,update,66,"Hi @WenyuLiang ,. I'll close this issue now. Feel free to give us update here if you have found anything that might be useful to share with other users too. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/617
https://github.com/google/deepvariant/issues/617:129,interoperability,share,share,129,"Hi @WenyuLiang ,. I'll close this issue now. Feel free to give us update here if you have found anything that might be useful to share with other users too. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/617
https://github.com/google/deepvariant/issues/617:66,safety,updat,update,66,"Hi @WenyuLiang ,. I'll close this issue now. Feel free to give us update here if you have found anything that might be useful to share with other users too. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/617
https://github.com/google/deepvariant/issues/617:66,security,updat,update,66,"Hi @WenyuLiang ,. I'll close this issue now. Feel free to give us update here if you have found anything that might be useful to share with other users too. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/617
https://github.com/google/deepvariant/issues/617:23,usability,close,close,23,"Hi @WenyuLiang ,. I'll close this issue now. Feel free to give us update here if you have found anything that might be useful to share with other users too. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/617
https://github.com/google/deepvariant/issues/617:146,usability,user,users,146,"Hi @WenyuLiang ,. I'll close this issue now. Feel free to give us update here if you have found anything that might be useful to share with other users too. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/617
https://github.com/google/deepvariant/issues/618:41,deployability,stage,stages,41,"Hi @fardokhtsadat . DeepVariant works in stages - **make_examples** and **call_variants**. Roughly, **make_examples** uses a set of heuristics similar to those used in other non-ML methods to realign reads and generate a list of potential positions to evaluate by the neural network. In the *call_variants* stage, a neural network assesses the probability that a candidate is not a variant, heterozygous, or homozygous. In this variant case, after realignment occurred, two candidates were considered, a deletion CAGCAGCGCT -> C, a SNP C -> T at the same position, and reference. The reporting of allele information comes after realignment. Your realignment pileup shows that a T is present in reads in that position. To understand why in realignment these are chosen, we might have to get a snippet of your BAM file and look at the actual De Bruijn Graph to understand why the realignment scoring has this as preferred (and if any of those bases are from rescued soft-clip bases which can occur. The neural network decided that the T SNP was spurious and the deletion the more likely outcome. However, since all candidates are reported (even if a variant call is not made) and because this report in the VCF reports the supporting information observed after re-alignment, the VCF contains information for a rejected T call (as the genotype is 0/1 not 0/2) and the number of reads where a re-aligned T was observed. The reason all potentially seen sites are reported is to allow filtering for higher sensitivity by users, if desired, and for joint calling where rejected sites across several samples could be considered as evidence to reverse a no-call decision. It is also convenient for debugging, as it indicates whether a call was not made because the neural network decided a reference call or because a candidate was not generated at that position.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/618
https://github.com/google/deepvariant/issues/618:307,deployability,stage,stage,307,"Hi @fardokhtsadat . DeepVariant works in stages - **make_examples** and **call_variants**. Roughly, **make_examples** uses a set of heuristics similar to those used in other non-ML methods to realign reads and generate a list of potential positions to evaluate by the neural network. In the *call_variants* stage, a neural network assesses the probability that a candidate is not a variant, heterozygous, or homozygous. In this variant case, after realignment occurred, two candidates were considered, a deletion CAGCAGCGCT -> C, a SNP C -> T at the same position, and reference. The reporting of allele information comes after realignment. Your realignment pileup shows that a T is present in reads in that position. To understand why in realignment these are chosen, we might have to get a snippet of your BAM file and look at the actual De Bruijn Graph to understand why the realignment scoring has this as preferred (and if any of those bases are from rescued soft-clip bases which can occur. The neural network decided that the T SNP was spurious and the deletion the more likely outcome. However, since all candidates are reported (even if a variant call is not made) and because this report in the VCF reports the supporting information observed after re-alignment, the VCF contains information for a rejected T call (as the genotype is 0/1 not 0/2) and the number of reads where a re-aligned T was observed. The reason all potentially seen sites are reported is to allow filtering for higher sensitivity by users, if desired, and for joint calling where rejected sites across several samples could be considered as evidence to reverse a no-call decision. It is also convenient for debugging, as it indicates whether a call was not made because the neural network decided a reference call or because a candidate was not generated at that position.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/618
https://github.com/google/deepvariant/issues/618:1244,deployability,observ,observed,1244,"Hi @fardokhtsadat . DeepVariant works in stages - **make_examples** and **call_variants**. Roughly, **make_examples** uses a set of heuristics similar to those used in other non-ML methods to realign reads and generate a list of potential positions to evaluate by the neural network. In the *call_variants* stage, a neural network assesses the probability that a candidate is not a variant, heterozygous, or homozygous. In this variant case, after realignment occurred, two candidates were considered, a deletion CAGCAGCGCT -> C, a SNP C -> T at the same position, and reference. The reporting of allele information comes after realignment. Your realignment pileup shows that a T is present in reads in that position. To understand why in realignment these are chosen, we might have to get a snippet of your BAM file and look at the actual De Bruijn Graph to understand why the realignment scoring has this as preferred (and if any of those bases are from rescued soft-clip bases which can occur. The neural network decided that the T SNP was spurious and the deletion the more likely outcome. However, since all candidates are reported (even if a variant call is not made) and because this report in the VCF reports the supporting information observed after re-alignment, the VCF contains information for a rejected T call (as the genotype is 0/1 not 0/2) and the number of reads where a re-aligned T was observed. The reason all potentially seen sites are reported is to allow filtering for higher sensitivity by users, if desired, and for joint calling where rejected sites across several samples could be considered as evidence to reverse a no-call decision. It is also convenient for debugging, as it indicates whether a call was not made because the neural network decided a reference call or because a candidate was not generated at that position.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/618
https://github.com/google/deepvariant/issues/618:1281,deployability,contain,contains,1281,"Hi @fardokhtsadat . DeepVariant works in stages - **make_examples** and **call_variants**. Roughly, **make_examples** uses a set of heuristics similar to those used in other non-ML methods to realign reads and generate a list of potential positions to evaluate by the neural network. In the *call_variants* stage, a neural network assesses the probability that a candidate is not a variant, heterozygous, or homozygous. In this variant case, after realignment occurred, two candidates were considered, a deletion CAGCAGCGCT -> C, a SNP C -> T at the same position, and reference. The reporting of allele information comes after realignment. Your realignment pileup shows that a T is present in reads in that position. To understand why in realignment these are chosen, we might have to get a snippet of your BAM file and look at the actual De Bruijn Graph to understand why the realignment scoring has this as preferred (and if any of those bases are from rescued soft-clip bases which can occur. The neural network decided that the T SNP was spurious and the deletion the more likely outcome. However, since all candidates are reported (even if a variant call is not made) and because this report in the VCF reports the supporting information observed after re-alignment, the VCF contains information for a rejected T call (as the genotype is 0/1 not 0/2) and the number of reads where a re-aligned T was observed. The reason all potentially seen sites are reported is to allow filtering for higher sensitivity by users, if desired, and for joint calling where rejected sites across several samples could be considered as evidence to reverse a no-call decision. It is also convenient for debugging, as it indicates whether a call was not made because the neural network decided a reference call or because a candidate was not generated at that position.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/618
https://github.com/google/deepvariant/issues/618:1406,deployability,observ,observed,1406,"Hi @fardokhtsadat . DeepVariant works in stages - **make_examples** and **call_variants**. Roughly, **make_examples** uses a set of heuristics similar to those used in other non-ML methods to realign reads and generate a list of potential positions to evaluate by the neural network. In the *call_variants* stage, a neural network assesses the probability that a candidate is not a variant, heterozygous, or homozygous. In this variant case, after realignment occurred, two candidates were considered, a deletion CAGCAGCGCT -> C, a SNP C -> T at the same position, and reference. The reporting of allele information comes after realignment. Your realignment pileup shows that a T is present in reads in that position. To understand why in realignment these are chosen, we might have to get a snippet of your BAM file and look at the actual De Bruijn Graph to understand why the realignment scoring has this as preferred (and if any of those bases are from rescued soft-clip bases which can occur. The neural network decided that the T SNP was spurious and the deletion the more likely outcome. However, since all candidates are reported (even if a variant call is not made) and because this report in the VCF reports the supporting information observed after re-alignment, the VCF contains information for a rejected T call (as the genotype is 0/1 not 0/2) and the number of reads where a re-aligned T was observed. The reason all potentially seen sites are reported is to allow filtering for higher sensitivity by users, if desired, and for joint calling where rejected sites across several samples could be considered as evidence to reverse a no-call decision. It is also convenient for debugging, as it indicates whether a call was not made because the neural network decided a reference call or because a candidate was not generated at that position.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/618
https://github.com/google/deepvariant/issues/618:1479,integrability,filter,filtering,1479,"Hi @fardokhtsadat . DeepVariant works in stages - **make_examples** and **call_variants**. Roughly, **make_examples** uses a set of heuristics similar to those used in other non-ML methods to realign reads and generate a list of potential positions to evaluate by the neural network. In the *call_variants* stage, a neural network assesses the probability that a candidate is not a variant, heterozygous, or homozygous. In this variant case, after realignment occurred, two candidates were considered, a deletion CAGCAGCGCT -> C, a SNP C -> T at the same position, and reference. The reporting of allele information comes after realignment. Your realignment pileup shows that a T is present in reads in that position. To understand why in realignment these are chosen, we might have to get a snippet of your BAM file and look at the actual De Bruijn Graph to understand why the realignment scoring has this as preferred (and if any of those bases are from rescued soft-clip bases which can occur. The neural network decided that the T SNP was spurious and the deletion the more likely outcome. However, since all candidates are reported (even if a variant call is not made) and because this report in the VCF reports the supporting information observed after re-alignment, the VCF contains information for a rejected T call (as the genotype is 0/1 not 0/2) and the number of reads where a re-aligned T was observed. The reason all potentially seen sites are reported is to allow filtering for higher sensitivity by users, if desired, and for joint calling where rejected sites across several samples could be considered as evidence to reverse a no-call decision. It is also convenient for debugging, as it indicates whether a call was not made because the neural network decided a reference call or because a candidate was not generated at that position.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/618
https://github.com/google/deepvariant/issues/618:275,performance,network,network,275,"Hi @fardokhtsadat . DeepVariant works in stages - **make_examples** and **call_variants**. Roughly, **make_examples** uses a set of heuristics similar to those used in other non-ML methods to realign reads and generate a list of potential positions to evaluate by the neural network. In the *call_variants* stage, a neural network assesses the probability that a candidate is not a variant, heterozygous, or homozygous. In this variant case, after realignment occurred, two candidates were considered, a deletion CAGCAGCGCT -> C, a SNP C -> T at the same position, and reference. The reporting of allele information comes after realignment. Your realignment pileup shows that a T is present in reads in that position. To understand why in realignment these are chosen, we might have to get a snippet of your BAM file and look at the actual De Bruijn Graph to understand why the realignment scoring has this as preferred (and if any of those bases are from rescued soft-clip bases which can occur. The neural network decided that the T SNP was spurious and the deletion the more likely outcome. However, since all candidates are reported (even if a variant call is not made) and because this report in the VCF reports the supporting information observed after re-alignment, the VCF contains information for a rejected T call (as the genotype is 0/1 not 0/2) and the number of reads where a re-aligned T was observed. The reason all potentially seen sites are reported is to allow filtering for higher sensitivity by users, if desired, and for joint calling where rejected sites across several samples could be considered as evidence to reverse a no-call decision. It is also convenient for debugging, as it indicates whether a call was not made because the neural network decided a reference call or because a candidate was not generated at that position.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/618
https://github.com/google/deepvariant/issues/618:323,performance,network,network,323,"Hi @fardokhtsadat . DeepVariant works in stages - **make_examples** and **call_variants**. Roughly, **make_examples** uses a set of heuristics similar to those used in other non-ML methods to realign reads and generate a list of potential positions to evaluate by the neural network. In the *call_variants* stage, a neural network assesses the probability that a candidate is not a variant, heterozygous, or homozygous. In this variant case, after realignment occurred, two candidates were considered, a deletion CAGCAGCGCT -> C, a SNP C -> T at the same position, and reference. The reporting of allele information comes after realignment. Your realignment pileup shows that a T is present in reads in that position. To understand why in realignment these are chosen, we might have to get a snippet of your BAM file and look at the actual De Bruijn Graph to understand why the realignment scoring has this as preferred (and if any of those bases are from rescued soft-clip bases which can occur. The neural network decided that the T SNP was spurious and the deletion the more likely outcome. However, since all candidates are reported (even if a variant call is not made) and because this report in the VCF reports the supporting information observed after re-alignment, the VCF contains information for a rejected T call (as the genotype is 0/1 not 0/2) and the number of reads where a re-aligned T was observed. The reason all potentially seen sites are reported is to allow filtering for higher sensitivity by users, if desired, and for joint calling where rejected sites across several samples could be considered as evidence to reverse a no-call decision. It is also convenient for debugging, as it indicates whether a call was not made because the neural network decided a reference call or because a candidate was not generated at that position.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/618
https://github.com/google/deepvariant/issues/618:1008,performance,network,network,1008,"Hi @fardokhtsadat . DeepVariant works in stages - **make_examples** and **call_variants**. Roughly, **make_examples** uses a set of heuristics similar to those used in other non-ML methods to realign reads and generate a list of potential positions to evaluate by the neural network. In the *call_variants* stage, a neural network assesses the probability that a candidate is not a variant, heterozygous, or homozygous. In this variant case, after realignment occurred, two candidates were considered, a deletion CAGCAGCGCT -> C, a SNP C -> T at the same position, and reference. The reporting of allele information comes after realignment. Your realignment pileup shows that a T is present in reads in that position. To understand why in realignment these are chosen, we might have to get a snippet of your BAM file and look at the actual De Bruijn Graph to understand why the realignment scoring has this as preferred (and if any of those bases are from rescued soft-clip bases which can occur. The neural network decided that the T SNP was spurious and the deletion the more likely outcome. However, since all candidates are reported (even if a variant call is not made) and because this report in the VCF reports the supporting information observed after re-alignment, the VCF contains information for a rejected T call (as the genotype is 0/1 not 0/2) and the number of reads where a re-aligned T was observed. The reason all potentially seen sites are reported is to allow filtering for higher sensitivity by users, if desired, and for joint calling where rejected sites across several samples could be considered as evidence to reverse a no-call decision. It is also convenient for debugging, as it indicates whether a call was not made because the neural network decided a reference call or because a candidate was not generated at that position.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/618
https://github.com/google/deepvariant/issues/618:1763,performance,network,network,1763,"Hi @fardokhtsadat . DeepVariant works in stages - **make_examples** and **call_variants**. Roughly, **make_examples** uses a set of heuristics similar to those used in other non-ML methods to realign reads and generate a list of potential positions to evaluate by the neural network. In the *call_variants* stage, a neural network assesses the probability that a candidate is not a variant, heterozygous, or homozygous. In this variant case, after realignment occurred, two candidates were considered, a deletion CAGCAGCGCT -> C, a SNP C -> T at the same position, and reference. The reporting of allele information comes after realignment. Your realignment pileup shows that a T is present in reads in that position. To understand why in realignment these are chosen, we might have to get a snippet of your BAM file and look at the actual De Bruijn Graph to understand why the realignment scoring has this as preferred (and if any of those bases are from rescued soft-clip bases which can occur. The neural network decided that the T SNP was spurious and the deletion the more likely outcome. However, since all candidates are reported (even if a variant call is not made) and because this report in the VCF reports the supporting information observed after re-alignment, the VCF contains information for a rejected T call (as the genotype is 0/1 not 0/2) and the number of reads where a re-aligned T was observed. The reason all potentially seen sites are reported is to allow filtering for higher sensitivity by users, if desired, and for joint calling where rejected sites across several samples could be considered as evidence to reverse a no-call decision. It is also convenient for debugging, as it indicates whether a call was not made because the neural network decided a reference call or because a candidate was not generated at that position.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/618
https://github.com/google/deepvariant/issues/618:275,security,network,network,275,"Hi @fardokhtsadat . DeepVariant works in stages - **make_examples** and **call_variants**. Roughly, **make_examples** uses a set of heuristics similar to those used in other non-ML methods to realign reads and generate a list of potential positions to evaluate by the neural network. In the *call_variants* stage, a neural network assesses the probability that a candidate is not a variant, heterozygous, or homozygous. In this variant case, after realignment occurred, two candidates were considered, a deletion CAGCAGCGCT -> C, a SNP C -> T at the same position, and reference. The reporting of allele information comes after realignment. Your realignment pileup shows that a T is present in reads in that position. To understand why in realignment these are chosen, we might have to get a snippet of your BAM file and look at the actual De Bruijn Graph to understand why the realignment scoring has this as preferred (and if any of those bases are from rescued soft-clip bases which can occur. The neural network decided that the T SNP was spurious and the deletion the more likely outcome. However, since all candidates are reported (even if a variant call is not made) and because this report in the VCF reports the supporting information observed after re-alignment, the VCF contains information for a rejected T call (as the genotype is 0/1 not 0/2) and the number of reads where a re-aligned T was observed. The reason all potentially seen sites are reported is to allow filtering for higher sensitivity by users, if desired, and for joint calling where rejected sites across several samples could be considered as evidence to reverse a no-call decision. It is also convenient for debugging, as it indicates whether a call was not made because the neural network decided a reference call or because a candidate was not generated at that position.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/618
https://github.com/google/deepvariant/issues/618:323,security,network,network,323,"Hi @fardokhtsadat . DeepVariant works in stages - **make_examples** and **call_variants**. Roughly, **make_examples** uses a set of heuristics similar to those used in other non-ML methods to realign reads and generate a list of potential positions to evaluate by the neural network. In the *call_variants* stage, a neural network assesses the probability that a candidate is not a variant, heterozygous, or homozygous. In this variant case, after realignment occurred, two candidates were considered, a deletion CAGCAGCGCT -> C, a SNP C -> T at the same position, and reference. The reporting of allele information comes after realignment. Your realignment pileup shows that a T is present in reads in that position. To understand why in realignment these are chosen, we might have to get a snippet of your BAM file and look at the actual De Bruijn Graph to understand why the realignment scoring has this as preferred (and if any of those bases are from rescued soft-clip bases which can occur. The neural network decided that the T SNP was spurious and the deletion the more likely outcome. However, since all candidates are reported (even if a variant call is not made) and because this report in the VCF reports the supporting information observed after re-alignment, the VCF contains information for a rejected T call (as the genotype is 0/1 not 0/2) and the number of reads where a re-aligned T was observed. The reason all potentially seen sites are reported is to allow filtering for higher sensitivity by users, if desired, and for joint calling where rejected sites across several samples could be considered as evidence to reverse a no-call decision. It is also convenient for debugging, as it indicates whether a call was not made because the neural network decided a reference call or because a candidate was not generated at that position.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/618
https://github.com/google/deepvariant/issues/618:331,security,assess,assesses,331,"Hi @fardokhtsadat . DeepVariant works in stages - **make_examples** and **call_variants**. Roughly, **make_examples** uses a set of heuristics similar to those used in other non-ML methods to realign reads and generate a list of potential positions to evaluate by the neural network. In the *call_variants* stage, a neural network assesses the probability that a candidate is not a variant, heterozygous, or homozygous. In this variant case, after realignment occurred, two candidates were considered, a deletion CAGCAGCGCT -> C, a SNP C -> T at the same position, and reference. The reporting of allele information comes after realignment. Your realignment pileup shows that a T is present in reads in that position. To understand why in realignment these are chosen, we might have to get a snippet of your BAM file and look at the actual De Bruijn Graph to understand why the realignment scoring has this as preferred (and if any of those bases are from rescued soft-clip bases which can occur. The neural network decided that the T SNP was spurious and the deletion the more likely outcome. However, since all candidates are reported (even if a variant call is not made) and because this report in the VCF reports the supporting information observed after re-alignment, the VCF contains information for a rejected T call (as the genotype is 0/1 not 0/2) and the number of reads where a re-aligned T was observed. The reason all potentially seen sites are reported is to allow filtering for higher sensitivity by users, if desired, and for joint calling where rejected sites across several samples could be considered as evidence to reverse a no-call decision. It is also convenient for debugging, as it indicates whether a call was not made because the neural network decided a reference call or because a candidate was not generated at that position.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/618
https://github.com/google/deepvariant/issues/618:1008,security,network,network,1008,"Hi @fardokhtsadat . DeepVariant works in stages - **make_examples** and **call_variants**. Roughly, **make_examples** uses a set of heuristics similar to those used in other non-ML methods to realign reads and generate a list of potential positions to evaluate by the neural network. In the *call_variants* stage, a neural network assesses the probability that a candidate is not a variant, heterozygous, or homozygous. In this variant case, after realignment occurred, two candidates were considered, a deletion CAGCAGCGCT -> C, a SNP C -> T at the same position, and reference. The reporting of allele information comes after realignment. Your realignment pileup shows that a T is present in reads in that position. To understand why in realignment these are chosen, we might have to get a snippet of your BAM file and look at the actual De Bruijn Graph to understand why the realignment scoring has this as preferred (and if any of those bases are from rescued soft-clip bases which can occur. The neural network decided that the T SNP was spurious and the deletion the more likely outcome. However, since all candidates are reported (even if a variant call is not made) and because this report in the VCF reports the supporting information observed after re-alignment, the VCF contains information for a rejected T call (as the genotype is 0/1 not 0/2) and the number of reads where a re-aligned T was observed. The reason all potentially seen sites are reported is to allow filtering for higher sensitivity by users, if desired, and for joint calling where rejected sites across several samples could be considered as evidence to reverse a no-call decision. It is also convenient for debugging, as it indicates whether a call was not made because the neural network decided a reference call or because a candidate was not generated at that position.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/618
https://github.com/google/deepvariant/issues/618:1763,security,network,network,1763,"Hi @fardokhtsadat . DeepVariant works in stages - **make_examples** and **call_variants**. Roughly, **make_examples** uses a set of heuristics similar to those used in other non-ML methods to realign reads and generate a list of potential positions to evaluate by the neural network. In the *call_variants* stage, a neural network assesses the probability that a candidate is not a variant, heterozygous, or homozygous. In this variant case, after realignment occurred, two candidates were considered, a deletion CAGCAGCGCT -> C, a SNP C -> T at the same position, and reference. The reporting of allele information comes after realignment. Your realignment pileup shows that a T is present in reads in that position. To understand why in realignment these are chosen, we might have to get a snippet of your BAM file and look at the actual De Bruijn Graph to understand why the realignment scoring has this as preferred (and if any of those bases are from rescued soft-clip bases which can occur. The neural network decided that the T SNP was spurious and the deletion the more likely outcome. However, since all candidates are reported (even if a variant call is not made) and because this report in the VCF reports the supporting information observed after re-alignment, the VCF contains information for a rejected T call (as the genotype is 0/1 not 0/2) and the number of reads where a re-aligned T was observed. The reason all potentially seen sites are reported is to allow filtering for higher sensitivity by users, if desired, and for joint calling where rejected sites across several samples could be considered as evidence to reverse a no-call decision. It is also convenient for debugging, as it indicates whether a call was not made because the neural network decided a reference call or because a candidate was not generated at that position.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/618
https://github.com/google/deepvariant/issues/618:721,testability,understand,understand,721,"Hi @fardokhtsadat . DeepVariant works in stages - **make_examples** and **call_variants**. Roughly, **make_examples** uses a set of heuristics similar to those used in other non-ML methods to realign reads and generate a list of potential positions to evaluate by the neural network. In the *call_variants* stage, a neural network assesses the probability that a candidate is not a variant, heterozygous, or homozygous. In this variant case, after realignment occurred, two candidates were considered, a deletion CAGCAGCGCT -> C, a SNP C -> T at the same position, and reference. The reporting of allele information comes after realignment. Your realignment pileup shows that a T is present in reads in that position. To understand why in realignment these are chosen, we might have to get a snippet of your BAM file and look at the actual De Bruijn Graph to understand why the realignment scoring has this as preferred (and if any of those bases are from rescued soft-clip bases which can occur. The neural network decided that the T SNP was spurious and the deletion the more likely outcome. However, since all candidates are reported (even if a variant call is not made) and because this report in the VCF reports the supporting information observed after re-alignment, the VCF contains information for a rejected T call (as the genotype is 0/1 not 0/2) and the number of reads where a re-aligned T was observed. The reason all potentially seen sites are reported is to allow filtering for higher sensitivity by users, if desired, and for joint calling where rejected sites across several samples could be considered as evidence to reverse a no-call decision. It is also convenient for debugging, as it indicates whether a call was not made because the neural network decided a reference call or because a candidate was not generated at that position.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/618
https://github.com/google/deepvariant/issues/618:859,testability,understand,understand,859,"Hi @fardokhtsadat . DeepVariant works in stages - **make_examples** and **call_variants**. Roughly, **make_examples** uses a set of heuristics similar to those used in other non-ML methods to realign reads and generate a list of potential positions to evaluate by the neural network. In the *call_variants* stage, a neural network assesses the probability that a candidate is not a variant, heterozygous, or homozygous. In this variant case, after realignment occurred, two candidates were considered, a deletion CAGCAGCGCT -> C, a SNP C -> T at the same position, and reference. The reporting of allele information comes after realignment. Your realignment pileup shows that a T is present in reads in that position. To understand why in realignment these are chosen, we might have to get a snippet of your BAM file and look at the actual De Bruijn Graph to understand why the realignment scoring has this as preferred (and if any of those bases are from rescued soft-clip bases which can occur. The neural network decided that the T SNP was spurious and the deletion the more likely outcome. However, since all candidates are reported (even if a variant call is not made) and because this report in the VCF reports the supporting information observed after re-alignment, the VCF contains information for a rejected T call (as the genotype is 0/1 not 0/2) and the number of reads where a re-aligned T was observed. The reason all potentially seen sites are reported is to allow filtering for higher sensitivity by users, if desired, and for joint calling where rejected sites across several samples could be considered as evidence to reverse a no-call decision. It is also convenient for debugging, as it indicates whether a call was not made because the neural network decided a reference call or because a candidate was not generated at that position.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/618
https://github.com/google/deepvariant/issues/618:1244,testability,observ,observed,1244,"Hi @fardokhtsadat . DeepVariant works in stages - **make_examples** and **call_variants**. Roughly, **make_examples** uses a set of heuristics similar to those used in other non-ML methods to realign reads and generate a list of potential positions to evaluate by the neural network. In the *call_variants* stage, a neural network assesses the probability that a candidate is not a variant, heterozygous, or homozygous. In this variant case, after realignment occurred, two candidates were considered, a deletion CAGCAGCGCT -> C, a SNP C -> T at the same position, and reference. The reporting of allele information comes after realignment. Your realignment pileup shows that a T is present in reads in that position. To understand why in realignment these are chosen, we might have to get a snippet of your BAM file and look at the actual De Bruijn Graph to understand why the realignment scoring has this as preferred (and if any of those bases are from rescued soft-clip bases which can occur. The neural network decided that the T SNP was spurious and the deletion the more likely outcome. However, since all candidates are reported (even if a variant call is not made) and because this report in the VCF reports the supporting information observed after re-alignment, the VCF contains information for a rejected T call (as the genotype is 0/1 not 0/2) and the number of reads where a re-aligned T was observed. The reason all potentially seen sites are reported is to allow filtering for higher sensitivity by users, if desired, and for joint calling where rejected sites across several samples could be considered as evidence to reverse a no-call decision. It is also convenient for debugging, as it indicates whether a call was not made because the neural network decided a reference call or because a candidate was not generated at that position.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/618
https://github.com/google/deepvariant/issues/618:1406,testability,observ,observed,1406,"Hi @fardokhtsadat . DeepVariant works in stages - **make_examples** and **call_variants**. Roughly, **make_examples** uses a set of heuristics similar to those used in other non-ML methods to realign reads and generate a list of potential positions to evaluate by the neural network. In the *call_variants* stage, a neural network assesses the probability that a candidate is not a variant, heterozygous, or homozygous. In this variant case, after realignment occurred, two candidates were considered, a deletion CAGCAGCGCT -> C, a SNP C -> T at the same position, and reference. The reporting of allele information comes after realignment. Your realignment pileup shows that a T is present in reads in that position. To understand why in realignment these are chosen, we might have to get a snippet of your BAM file and look at the actual De Bruijn Graph to understand why the realignment scoring has this as preferred (and if any of those bases are from rescued soft-clip bases which can occur. The neural network decided that the T SNP was spurious and the deletion the more likely outcome. However, since all candidates are reported (even if a variant call is not made) and because this report in the VCF reports the supporting information observed after re-alignment, the VCF contains information for a rejected T call (as the genotype is 0/1 not 0/2) and the number of reads where a re-aligned T was observed. The reason all potentially seen sites are reported is to allow filtering for higher sensitivity by users, if desired, and for joint calling where rejected sites across several samples could be considered as evidence to reverse a no-call decision. It is also convenient for debugging, as it indicates whether a call was not made because the neural network decided a reference call or because a candidate was not generated at that position.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/618
https://github.com/google/deepvariant/issues/618:910,usability,prefer,preferred,910,"Hi @fardokhtsadat . DeepVariant works in stages - **make_examples** and **call_variants**. Roughly, **make_examples** uses a set of heuristics similar to those used in other non-ML methods to realign reads and generate a list of potential positions to evaluate by the neural network. In the *call_variants* stage, a neural network assesses the probability that a candidate is not a variant, heterozygous, or homozygous. In this variant case, after realignment occurred, two candidates were considered, a deletion CAGCAGCGCT -> C, a SNP C -> T at the same position, and reference. The reporting of allele information comes after realignment. Your realignment pileup shows that a T is present in reads in that position. To understand why in realignment these are chosen, we might have to get a snippet of your BAM file and look at the actual De Bruijn Graph to understand why the realignment scoring has this as preferred (and if any of those bases are from rescued soft-clip bases which can occur. The neural network decided that the T SNP was spurious and the deletion the more likely outcome. However, since all candidates are reported (even if a variant call is not made) and because this report in the VCF reports the supporting information observed after re-alignment, the VCF contains information for a rejected T call (as the genotype is 0/1 not 0/2) and the number of reads where a re-aligned T was observed. The reason all potentially seen sites are reported is to allow filtering for higher sensitivity by users, if desired, and for joint calling where rejected sites across several samples could be considered as evidence to reverse a no-call decision. It is also convenient for debugging, as it indicates whether a call was not made because the neural network decided a reference call or because a candidate was not generated at that position.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/618
https://github.com/google/deepvariant/issues/618:1221,usability,support,supporting,1221,"Hi @fardokhtsadat . DeepVariant works in stages - **make_examples** and **call_variants**. Roughly, **make_examples** uses a set of heuristics similar to those used in other non-ML methods to realign reads and generate a list of potential positions to evaluate by the neural network. In the *call_variants* stage, a neural network assesses the probability that a candidate is not a variant, heterozygous, or homozygous. In this variant case, after realignment occurred, two candidates were considered, a deletion CAGCAGCGCT -> C, a SNP C -> T at the same position, and reference. The reporting of allele information comes after realignment. Your realignment pileup shows that a T is present in reads in that position. To understand why in realignment these are chosen, we might have to get a snippet of your BAM file and look at the actual De Bruijn Graph to understand why the realignment scoring has this as preferred (and if any of those bases are from rescued soft-clip bases which can occur. The neural network decided that the T SNP was spurious and the deletion the more likely outcome. However, since all candidates are reported (even if a variant call is not made) and because this report in the VCF reports the supporting information observed after re-alignment, the VCF contains information for a rejected T call (as the genotype is 0/1 not 0/2) and the number of reads where a re-aligned T was observed. The reason all potentially seen sites are reported is to allow filtering for higher sensitivity by users, if desired, and for joint calling where rejected sites across several samples could be considered as evidence to reverse a no-call decision. It is also convenient for debugging, as it indicates whether a call was not made because the neural network decided a reference call or because a candidate was not generated at that position.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/618
https://github.com/google/deepvariant/issues/618:1515,usability,user,users,1515,"Hi @fardokhtsadat . DeepVariant works in stages - **make_examples** and **call_variants**. Roughly, **make_examples** uses a set of heuristics similar to those used in other non-ML methods to realign reads and generate a list of potential positions to evaluate by the neural network. In the *call_variants* stage, a neural network assesses the probability that a candidate is not a variant, heterozygous, or homozygous. In this variant case, after realignment occurred, two candidates were considered, a deletion CAGCAGCGCT -> C, a SNP C -> T at the same position, and reference. The reporting of allele information comes after realignment. Your realignment pileup shows that a T is present in reads in that position. To understand why in realignment these are chosen, we might have to get a snippet of your BAM file and look at the actual De Bruijn Graph to understand why the realignment scoring has this as preferred (and if any of those bases are from rescued soft-clip bases which can occur. The neural network decided that the T SNP was spurious and the deletion the more likely outcome. However, since all candidates are reported (even if a variant call is not made) and because this report in the VCF reports the supporting information observed after re-alignment, the VCF contains information for a rejected T call (as the genotype is 0/1 not 0/2) and the number of reads where a re-aligned T was observed. The reason all potentially seen sites are reported is to allow filtering for higher sensitivity by users, if desired, and for joint calling where rejected sites across several samples could be considered as evidence to reverse a no-call decision. It is also convenient for debugging, as it indicates whether a call was not made because the neural network decided a reference call or because a candidate was not generated at that position.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/618
https://github.com/google/deepvariant/issues/618:1706,usability,indicat,indicates,1706,"Hi @fardokhtsadat . DeepVariant works in stages - **make_examples** and **call_variants**. Roughly, **make_examples** uses a set of heuristics similar to those used in other non-ML methods to realign reads and generate a list of potential positions to evaluate by the neural network. In the *call_variants* stage, a neural network assesses the probability that a candidate is not a variant, heterozygous, or homozygous. In this variant case, after realignment occurred, two candidates were considered, a deletion CAGCAGCGCT -> C, a SNP C -> T at the same position, and reference. The reporting of allele information comes after realignment. Your realignment pileup shows that a T is present in reads in that position. To understand why in realignment these are chosen, we might have to get a snippet of your BAM file and look at the actual De Bruijn Graph to understand why the realignment scoring has this as preferred (and if any of those bases are from rescued soft-clip bases which can occur. The neural network decided that the T SNP was spurious and the deletion the more likely outcome. However, since all candidates are reported (even if a variant call is not made) and because this report in the VCF reports the supporting information observed after re-alignment, the VCF contains information for a rejected T call (as the genotype is 0/1 not 0/2) and the number of reads where a re-aligned T was observed. The reason all potentially seen sites are reported is to allow filtering for higher sensitivity by users, if desired, and for joint calling where rejected sites across several samples could be considered as evidence to reverse a no-call decision. It is also convenient for debugging, as it indicates whether a call was not made because the neural network decided a reference call or because a candidate was not generated at that position.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/618
https://github.com/google/deepvariant/issues/618:432,deployability,version,version,432,"@AndrewCarroll Thank you for the information. In the description above, you mentioned that deepvariant sees two candidates:. - CAGCAGCGCT -> C. - C -> T. However, I thought deepvariant would detect the following as the two candidates (Applying vt decompose produces the same as the following):. - CAGCAGCGCT -> C. - CAGCAGCGCT -> T. Am I misunderstanding this? Going back to the GT field, can it be that this behavior is changed in version 1.5.0? Looking results of deepvariant version 1.2.0, the GT field for all multi-allelic variants only supports the first alternate allele. I tried running deepvariant version 1.5.0 on the same alignment file, and from 47 multi-allelic variants, almost all have a GT field that supports both of the alternate alleles. Here is an example:. Here is an example:. Result of deepvariant version 1.2.0:. ```. NC_000001.11	6545786	.	C	A,T	.	.	.	GT:GQ:DP:AD:VAF:PL	1/0:40:91:0,54,37:0.593407,0.406593:50,44,53,44,0,61. ```. Result of deepvariant version 1.5.0:. ```. NC_000001.11	6545786	.	C	A,T	57	PASS	.	GT:GQ:DP:AD:VAF:PL	1/2:48:91:0,54,37:0.593407,0.406593:57,52,61,52,0,66. ````. However, from 47 multi-allelic variants, 4 have a GT field presented as 0/1. Is it safe to further process the VCF file to only keep the first allele for these 4 variants since deepvariant did not call them?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/618
https://github.com/google/deepvariant/issues/618:478,deployability,version,version,478,"@AndrewCarroll Thank you for the information. In the description above, you mentioned that deepvariant sees two candidates:. - CAGCAGCGCT -> C. - C -> T. However, I thought deepvariant would detect the following as the two candidates (Applying vt decompose produces the same as the following):. - CAGCAGCGCT -> C. - CAGCAGCGCT -> T. Am I misunderstanding this? Going back to the GT field, can it be that this behavior is changed in version 1.5.0? Looking results of deepvariant version 1.2.0, the GT field for all multi-allelic variants only supports the first alternate allele. I tried running deepvariant version 1.5.0 on the same alignment file, and from 47 multi-allelic variants, almost all have a GT field that supports both of the alternate alleles. Here is an example:. Here is an example:. Result of deepvariant version 1.2.0:. ```. NC_000001.11	6545786	.	C	A,T	.	.	.	GT:GQ:DP:AD:VAF:PL	1/0:40:91:0,54,37:0.593407,0.406593:50,44,53,44,0,61. ```. Result of deepvariant version 1.5.0:. ```. NC_000001.11	6545786	.	C	A,T	57	PASS	.	GT:GQ:DP:AD:VAF:PL	1/2:48:91:0,54,37:0.593407,0.406593:57,52,61,52,0,66. ````. However, from 47 multi-allelic variants, 4 have a GT field presented as 0/1. Is it safe to further process the VCF file to only keep the first allele for these 4 variants since deepvariant did not call them?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/618
https://github.com/google/deepvariant/issues/618:607,deployability,version,version,607,"@AndrewCarroll Thank you for the information. In the description above, you mentioned that deepvariant sees two candidates:. - CAGCAGCGCT -> C. - C -> T. However, I thought deepvariant would detect the following as the two candidates (Applying vt decompose produces the same as the following):. - CAGCAGCGCT -> C. - CAGCAGCGCT -> T. Am I misunderstanding this? Going back to the GT field, can it be that this behavior is changed in version 1.5.0? Looking results of deepvariant version 1.2.0, the GT field for all multi-allelic variants only supports the first alternate allele. I tried running deepvariant version 1.5.0 on the same alignment file, and from 47 multi-allelic variants, almost all have a GT field that supports both of the alternate alleles. Here is an example:. Here is an example:. Result of deepvariant version 1.2.0:. ```. NC_000001.11	6545786	.	C	A,T	.	.	.	GT:GQ:DP:AD:VAF:PL	1/0:40:91:0,54,37:0.593407,0.406593:50,44,53,44,0,61. ```. Result of deepvariant version 1.5.0:. ```. NC_000001.11	6545786	.	C	A,T	57	PASS	.	GT:GQ:DP:AD:VAF:PL	1/2:48:91:0,54,37:0.593407,0.406593:57,52,61,52,0,66. ````. However, from 47 multi-allelic variants, 4 have a GT field presented as 0/1. Is it safe to further process the VCF file to only keep the first allele for these 4 variants since deepvariant did not call them?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/618
https://github.com/google/deepvariant/issues/618:821,deployability,version,version,821,"@AndrewCarroll Thank you for the information. In the description above, you mentioned that deepvariant sees two candidates:. - CAGCAGCGCT -> C. - C -> T. However, I thought deepvariant would detect the following as the two candidates (Applying vt decompose produces the same as the following):. - CAGCAGCGCT -> C. - CAGCAGCGCT -> T. Am I misunderstanding this? Going back to the GT field, can it be that this behavior is changed in version 1.5.0? Looking results of deepvariant version 1.2.0, the GT field for all multi-allelic variants only supports the first alternate allele. I tried running deepvariant version 1.5.0 on the same alignment file, and from 47 multi-allelic variants, almost all have a GT field that supports both of the alternate alleles. Here is an example:. Here is an example:. Result of deepvariant version 1.2.0:. ```. NC_000001.11	6545786	.	C	A,T	.	.	.	GT:GQ:DP:AD:VAF:PL	1/0:40:91:0,54,37:0.593407,0.406593:50,44,53,44,0,61. ```. Result of deepvariant version 1.5.0:. ```. NC_000001.11	6545786	.	C	A,T	57	PASS	.	GT:GQ:DP:AD:VAF:PL	1/2:48:91:0,54,37:0.593407,0.406593:57,52,61,52,0,66. ````. However, from 47 multi-allelic variants, 4 have a GT field presented as 0/1. Is it safe to further process the VCF file to only keep the first allele for these 4 variants since deepvariant did not call them?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/618
https://github.com/google/deepvariant/issues/618:977,deployability,version,version,977,"@AndrewCarroll Thank you for the information. In the description above, you mentioned that deepvariant sees two candidates:. - CAGCAGCGCT -> C. - C -> T. However, I thought deepvariant would detect the following as the two candidates (Applying vt decompose produces the same as the following):. - CAGCAGCGCT -> C. - CAGCAGCGCT -> T. Am I misunderstanding this? Going back to the GT field, can it be that this behavior is changed in version 1.5.0? Looking results of deepvariant version 1.2.0, the GT field for all multi-allelic variants only supports the first alternate allele. I tried running deepvariant version 1.5.0 on the same alignment file, and from 47 multi-allelic variants, almost all have a GT field that supports both of the alternate alleles. Here is an example:. Here is an example:. Result of deepvariant version 1.2.0:. ```. NC_000001.11	6545786	.	C	A,T	.	.	.	GT:GQ:DP:AD:VAF:PL	1/0:40:91:0,54,37:0.593407,0.406593:50,44,53,44,0,61. ```. Result of deepvariant version 1.5.0:. ```. NC_000001.11	6545786	.	C	A,T	57	PASS	.	GT:GQ:DP:AD:VAF:PL	1/2:48:91:0,54,37:0.593407,0.406593:57,52,61,52,0,66. ````. However, from 47 multi-allelic variants, 4 have a GT field presented as 0/1. Is it safe to further process the VCF file to only keep the first allele for these 4 variants since deepvariant did not call them?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/618
https://github.com/google/deepvariant/issues/618:432,integrability,version,version,432,"@AndrewCarroll Thank you for the information. In the description above, you mentioned that deepvariant sees two candidates:. - CAGCAGCGCT -> C. - C -> T. However, I thought deepvariant would detect the following as the two candidates (Applying vt decompose produces the same as the following):. - CAGCAGCGCT -> C. - CAGCAGCGCT -> T. Am I misunderstanding this? Going back to the GT field, can it be that this behavior is changed in version 1.5.0? Looking results of deepvariant version 1.2.0, the GT field for all multi-allelic variants only supports the first alternate allele. I tried running deepvariant version 1.5.0 on the same alignment file, and from 47 multi-allelic variants, almost all have a GT field that supports both of the alternate alleles. Here is an example:. Here is an example:. Result of deepvariant version 1.2.0:. ```. NC_000001.11	6545786	.	C	A,T	.	.	.	GT:GQ:DP:AD:VAF:PL	1/0:40:91:0,54,37:0.593407,0.406593:50,44,53,44,0,61. ```. Result of deepvariant version 1.5.0:. ```. NC_000001.11	6545786	.	C	A,T	57	PASS	.	GT:GQ:DP:AD:VAF:PL	1/2:48:91:0,54,37:0.593407,0.406593:57,52,61,52,0,66. ````. However, from 47 multi-allelic variants, 4 have a GT field presented as 0/1. Is it safe to further process the VCF file to only keep the first allele for these 4 variants since deepvariant did not call them?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/618
https://github.com/google/deepvariant/issues/618:478,integrability,version,version,478,"@AndrewCarroll Thank you for the information. In the description above, you mentioned that deepvariant sees two candidates:. - CAGCAGCGCT -> C. - C -> T. However, I thought deepvariant would detect the following as the two candidates (Applying vt decompose produces the same as the following):. - CAGCAGCGCT -> C. - CAGCAGCGCT -> T. Am I misunderstanding this? Going back to the GT field, can it be that this behavior is changed in version 1.5.0? Looking results of deepvariant version 1.2.0, the GT field for all multi-allelic variants only supports the first alternate allele. I tried running deepvariant version 1.5.0 on the same alignment file, and from 47 multi-allelic variants, almost all have a GT field that supports both of the alternate alleles. Here is an example:. Here is an example:. Result of deepvariant version 1.2.0:. ```. NC_000001.11	6545786	.	C	A,T	.	.	.	GT:GQ:DP:AD:VAF:PL	1/0:40:91:0,54,37:0.593407,0.406593:50,44,53,44,0,61. ```. Result of deepvariant version 1.5.0:. ```. NC_000001.11	6545786	.	C	A,T	57	PASS	.	GT:GQ:DP:AD:VAF:PL	1/2:48:91:0,54,37:0.593407,0.406593:57,52,61,52,0,66. ````. However, from 47 multi-allelic variants, 4 have a GT field presented as 0/1. Is it safe to further process the VCF file to only keep the first allele for these 4 variants since deepvariant did not call them?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/618
https://github.com/google/deepvariant/issues/618:607,integrability,version,version,607,"@AndrewCarroll Thank you for the information. In the description above, you mentioned that deepvariant sees two candidates:. - CAGCAGCGCT -> C. - C -> T. However, I thought deepvariant would detect the following as the two candidates (Applying vt decompose produces the same as the following):. - CAGCAGCGCT -> C. - CAGCAGCGCT -> T. Am I misunderstanding this? Going back to the GT field, can it be that this behavior is changed in version 1.5.0? Looking results of deepvariant version 1.2.0, the GT field for all multi-allelic variants only supports the first alternate allele. I tried running deepvariant version 1.5.0 on the same alignment file, and from 47 multi-allelic variants, almost all have a GT field that supports both of the alternate alleles. Here is an example:. Here is an example:. Result of deepvariant version 1.2.0:. ```. NC_000001.11	6545786	.	C	A,T	.	.	.	GT:GQ:DP:AD:VAF:PL	1/0:40:91:0,54,37:0.593407,0.406593:50,44,53,44,0,61. ```. Result of deepvariant version 1.5.0:. ```. NC_000001.11	6545786	.	C	A,T	57	PASS	.	GT:GQ:DP:AD:VAF:PL	1/2:48:91:0,54,37:0.593407,0.406593:57,52,61,52,0,66. ````. However, from 47 multi-allelic variants, 4 have a GT field presented as 0/1. Is it safe to further process the VCF file to only keep the first allele for these 4 variants since deepvariant did not call them?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/618
https://github.com/google/deepvariant/issues/618:821,integrability,version,version,821,"@AndrewCarroll Thank you for the information. In the description above, you mentioned that deepvariant sees two candidates:. - CAGCAGCGCT -> C. - C -> T. However, I thought deepvariant would detect the following as the two candidates (Applying vt decompose produces the same as the following):. - CAGCAGCGCT -> C. - CAGCAGCGCT -> T. Am I misunderstanding this? Going back to the GT field, can it be that this behavior is changed in version 1.5.0? Looking results of deepvariant version 1.2.0, the GT field for all multi-allelic variants only supports the first alternate allele. I tried running deepvariant version 1.5.0 on the same alignment file, and from 47 multi-allelic variants, almost all have a GT field that supports both of the alternate alleles. Here is an example:. Here is an example:. Result of deepvariant version 1.2.0:. ```. NC_000001.11	6545786	.	C	A,T	.	.	.	GT:GQ:DP:AD:VAF:PL	1/0:40:91:0,54,37:0.593407,0.406593:50,44,53,44,0,61. ```. Result of deepvariant version 1.5.0:. ```. NC_000001.11	6545786	.	C	A,T	57	PASS	.	GT:GQ:DP:AD:VAF:PL	1/2:48:91:0,54,37:0.593407,0.406593:57,52,61,52,0,66. ````. However, from 47 multi-allelic variants, 4 have a GT field presented as 0/1. Is it safe to further process the VCF file to only keep the first allele for these 4 variants since deepvariant did not call them?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/618
https://github.com/google/deepvariant/issues/618:977,integrability,version,version,977,"@AndrewCarroll Thank you for the information. In the description above, you mentioned that deepvariant sees two candidates:. - CAGCAGCGCT -> C. - C -> T. However, I thought deepvariant would detect the following as the two candidates (Applying vt decompose produces the same as the following):. - CAGCAGCGCT -> C. - CAGCAGCGCT -> T. Am I misunderstanding this? Going back to the GT field, can it be that this behavior is changed in version 1.5.0? Looking results of deepvariant version 1.2.0, the GT field for all multi-allelic variants only supports the first alternate allele. I tried running deepvariant version 1.5.0 on the same alignment file, and from 47 multi-allelic variants, almost all have a GT field that supports both of the alternate alleles. Here is an example:. Here is an example:. Result of deepvariant version 1.2.0:. ```. NC_000001.11	6545786	.	C	A,T	.	.	.	GT:GQ:DP:AD:VAF:PL	1/0:40:91:0,54,37:0.593407,0.406593:50,44,53,44,0,61. ```. Result of deepvariant version 1.5.0:. ```. NC_000001.11	6545786	.	C	A,T	57	PASS	.	GT:GQ:DP:AD:VAF:PL	1/2:48:91:0,54,37:0.593407,0.406593:57,52,61,52,0,66. ````. However, from 47 multi-allelic variants, 4 have a GT field presented as 0/1. Is it safe to further process the VCF file to only keep the first allele for these 4 variants since deepvariant did not call them?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/618
https://github.com/google/deepvariant/issues/618:247,modifiability,deco,decompose,247,"@AndrewCarroll Thank you for the information. In the description above, you mentioned that deepvariant sees two candidates:. - CAGCAGCGCT -> C. - C -> T. However, I thought deepvariant would detect the following as the two candidates (Applying vt decompose produces the same as the following):. - CAGCAGCGCT -> C. - CAGCAGCGCT -> T. Am I misunderstanding this? Going back to the GT field, can it be that this behavior is changed in version 1.5.0? Looking results of deepvariant version 1.2.0, the GT field for all multi-allelic variants only supports the first alternate allele. I tried running deepvariant version 1.5.0 on the same alignment file, and from 47 multi-allelic variants, almost all have a GT field that supports both of the alternate alleles. Here is an example:. Here is an example:. Result of deepvariant version 1.2.0:. ```. NC_000001.11	6545786	.	C	A,T	.	.	.	GT:GQ:DP:AD:VAF:PL	1/0:40:91:0,54,37:0.593407,0.406593:50,44,53,44,0,61. ```. Result of deepvariant version 1.5.0:. ```. NC_000001.11	6545786	.	C	A,T	57	PASS	.	GT:GQ:DP:AD:VAF:PL	1/2:48:91:0,54,37:0.593407,0.406593:57,52,61,52,0,66. ````. However, from 47 multi-allelic variants, 4 have a GT field presented as 0/1. Is it safe to further process the VCF file to only keep the first allele for these 4 variants since deepvariant did not call them?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/618
https://github.com/google/deepvariant/issues/618:432,modifiability,version,version,432,"@AndrewCarroll Thank you for the information. In the description above, you mentioned that deepvariant sees two candidates:. - CAGCAGCGCT -> C. - C -> T. However, I thought deepvariant would detect the following as the two candidates (Applying vt decompose produces the same as the following):. - CAGCAGCGCT -> C. - CAGCAGCGCT -> T. Am I misunderstanding this? Going back to the GT field, can it be that this behavior is changed in version 1.5.0? Looking results of deepvariant version 1.2.0, the GT field for all multi-allelic variants only supports the first alternate allele. I tried running deepvariant version 1.5.0 on the same alignment file, and from 47 multi-allelic variants, almost all have a GT field that supports both of the alternate alleles. Here is an example:. Here is an example:. Result of deepvariant version 1.2.0:. ```. NC_000001.11	6545786	.	C	A,T	.	.	.	GT:GQ:DP:AD:VAF:PL	1/0:40:91:0,54,37:0.593407,0.406593:50,44,53,44,0,61. ```. Result of deepvariant version 1.5.0:. ```. NC_000001.11	6545786	.	C	A,T	57	PASS	.	GT:GQ:DP:AD:VAF:PL	1/2:48:91:0,54,37:0.593407,0.406593:57,52,61,52,0,66. ````. However, from 47 multi-allelic variants, 4 have a GT field presented as 0/1. Is it safe to further process the VCF file to only keep the first allele for these 4 variants since deepvariant did not call them?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/618
https://github.com/google/deepvariant/issues/618:478,modifiability,version,version,478,"@AndrewCarroll Thank you for the information. In the description above, you mentioned that deepvariant sees two candidates:. - CAGCAGCGCT -> C. - C -> T. However, I thought deepvariant would detect the following as the two candidates (Applying vt decompose produces the same as the following):. - CAGCAGCGCT -> C. - CAGCAGCGCT -> T. Am I misunderstanding this? Going back to the GT field, can it be that this behavior is changed in version 1.5.0? Looking results of deepvariant version 1.2.0, the GT field for all multi-allelic variants only supports the first alternate allele. I tried running deepvariant version 1.5.0 on the same alignment file, and from 47 multi-allelic variants, almost all have a GT field that supports both of the alternate alleles. Here is an example:. Here is an example:. Result of deepvariant version 1.2.0:. ```. NC_000001.11	6545786	.	C	A,T	.	.	.	GT:GQ:DP:AD:VAF:PL	1/0:40:91:0,54,37:0.593407,0.406593:50,44,53,44,0,61. ```. Result of deepvariant version 1.5.0:. ```. NC_000001.11	6545786	.	C	A,T	57	PASS	.	GT:GQ:DP:AD:VAF:PL	1/2:48:91:0,54,37:0.593407,0.406593:57,52,61,52,0,66. ````. However, from 47 multi-allelic variants, 4 have a GT field presented as 0/1. Is it safe to further process the VCF file to only keep the first allele for these 4 variants since deepvariant did not call them?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/618
https://github.com/google/deepvariant/issues/618:607,modifiability,version,version,607,"@AndrewCarroll Thank you for the information. In the description above, you mentioned that deepvariant sees two candidates:. - CAGCAGCGCT -> C. - C -> T. However, I thought deepvariant would detect the following as the two candidates (Applying vt decompose produces the same as the following):. - CAGCAGCGCT -> C. - CAGCAGCGCT -> T. Am I misunderstanding this? Going back to the GT field, can it be that this behavior is changed in version 1.5.0? Looking results of deepvariant version 1.2.0, the GT field for all multi-allelic variants only supports the first alternate allele. I tried running deepvariant version 1.5.0 on the same alignment file, and from 47 multi-allelic variants, almost all have a GT field that supports both of the alternate alleles. Here is an example:. Here is an example:. Result of deepvariant version 1.2.0:. ```. NC_000001.11	6545786	.	C	A,T	.	.	.	GT:GQ:DP:AD:VAF:PL	1/0:40:91:0,54,37:0.593407,0.406593:50,44,53,44,0,61. ```. Result of deepvariant version 1.5.0:. ```. NC_000001.11	6545786	.	C	A,T	57	PASS	.	GT:GQ:DP:AD:VAF:PL	1/2:48:91:0,54,37:0.593407,0.406593:57,52,61,52,0,66. ````. However, from 47 multi-allelic variants, 4 have a GT field presented as 0/1. Is it safe to further process the VCF file to only keep the first allele for these 4 variants since deepvariant did not call them?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/618
https://github.com/google/deepvariant/issues/618:821,modifiability,version,version,821,"@AndrewCarroll Thank you for the information. In the description above, you mentioned that deepvariant sees two candidates:. - CAGCAGCGCT -> C. - C -> T. However, I thought deepvariant would detect the following as the two candidates (Applying vt decompose produces the same as the following):. - CAGCAGCGCT -> C. - CAGCAGCGCT -> T. Am I misunderstanding this? Going back to the GT field, can it be that this behavior is changed in version 1.5.0? Looking results of deepvariant version 1.2.0, the GT field for all multi-allelic variants only supports the first alternate allele. I tried running deepvariant version 1.5.0 on the same alignment file, and from 47 multi-allelic variants, almost all have a GT field that supports both of the alternate alleles. Here is an example:. Here is an example:. Result of deepvariant version 1.2.0:. ```. NC_000001.11	6545786	.	C	A,T	.	.	.	GT:GQ:DP:AD:VAF:PL	1/0:40:91:0,54,37:0.593407,0.406593:50,44,53,44,0,61. ```. Result of deepvariant version 1.5.0:. ```. NC_000001.11	6545786	.	C	A,T	57	PASS	.	GT:GQ:DP:AD:VAF:PL	1/2:48:91:0,54,37:0.593407,0.406593:57,52,61,52,0,66. ````. However, from 47 multi-allelic variants, 4 have a GT field presented as 0/1. Is it safe to further process the VCF file to only keep the first allele for these 4 variants since deepvariant did not call them?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/618
https://github.com/google/deepvariant/issues/618:977,modifiability,version,version,977,"@AndrewCarroll Thank you for the information. In the description above, you mentioned that deepvariant sees two candidates:. - CAGCAGCGCT -> C. - C -> T. However, I thought deepvariant would detect the following as the two candidates (Applying vt decompose produces the same as the following):. - CAGCAGCGCT -> C. - CAGCAGCGCT -> T. Am I misunderstanding this? Going back to the GT field, can it be that this behavior is changed in version 1.5.0? Looking results of deepvariant version 1.2.0, the GT field for all multi-allelic variants only supports the first alternate allele. I tried running deepvariant version 1.5.0 on the same alignment file, and from 47 multi-allelic variants, almost all have a GT field that supports both of the alternate alleles. Here is an example:. Here is an example:. Result of deepvariant version 1.2.0:. ```. NC_000001.11	6545786	.	C	A,T	.	.	.	GT:GQ:DP:AD:VAF:PL	1/0:40:91:0,54,37:0.593407,0.406593:50,44,53,44,0,61. ```. Result of deepvariant version 1.5.0:. ```. NC_000001.11	6545786	.	C	A,T	57	PASS	.	GT:GQ:DP:AD:VAF:PL	1/2:48:91:0,54,37:0.593407,0.406593:57,52,61,52,0,66. ````. However, from 47 multi-allelic variants, 4 have a GT field presented as 0/1. Is it safe to further process the VCF file to only keep the first allele for these 4 variants since deepvariant did not call them?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/618
https://github.com/google/deepvariant/issues/618:191,safety,detect,detect,191,"@AndrewCarroll Thank you for the information. In the description above, you mentioned that deepvariant sees two candidates:. - CAGCAGCGCT -> C. - C -> T. However, I thought deepvariant would detect the following as the two candidates (Applying vt decompose produces the same as the following):. - CAGCAGCGCT -> C. - CAGCAGCGCT -> T. Am I misunderstanding this? Going back to the GT field, can it be that this behavior is changed in version 1.5.0? Looking results of deepvariant version 1.2.0, the GT field for all multi-allelic variants only supports the first alternate allele. I tried running deepvariant version 1.5.0 on the same alignment file, and from 47 multi-allelic variants, almost all have a GT field that supports both of the alternate alleles. Here is an example:. Here is an example:. Result of deepvariant version 1.2.0:. ```. NC_000001.11	6545786	.	C	A,T	.	.	.	GT:GQ:DP:AD:VAF:PL	1/0:40:91:0,54,37:0.593407,0.406593:50,44,53,44,0,61. ```. Result of deepvariant version 1.5.0:. ```. NC_000001.11	6545786	.	C	A,T	57	PASS	.	GT:GQ:DP:AD:VAF:PL	1/2:48:91:0,54,37:0.593407,0.406593:57,52,61,52,0,66. ````. However, from 47 multi-allelic variants, 4 have a GT field presented as 0/1. Is it safe to further process the VCF file to only keep the first allele for these 4 variants since deepvariant did not call them?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/618
https://github.com/google/deepvariant/issues/618:1199,safety,safe,safe,1199,"@AndrewCarroll Thank you for the information. In the description above, you mentioned that deepvariant sees two candidates:. - CAGCAGCGCT -> C. - C -> T. However, I thought deepvariant would detect the following as the two candidates (Applying vt decompose produces the same as the following):. - CAGCAGCGCT -> C. - CAGCAGCGCT -> T. Am I misunderstanding this? Going back to the GT field, can it be that this behavior is changed in version 1.5.0? Looking results of deepvariant version 1.2.0, the GT field for all multi-allelic variants only supports the first alternate allele. I tried running deepvariant version 1.5.0 on the same alignment file, and from 47 multi-allelic variants, almost all have a GT field that supports both of the alternate alleles. Here is an example:. Here is an example:. Result of deepvariant version 1.2.0:. ```. NC_000001.11	6545786	.	C	A,T	.	.	.	GT:GQ:DP:AD:VAF:PL	1/0:40:91:0,54,37:0.593407,0.406593:50,44,53,44,0,61. ```. Result of deepvariant version 1.5.0:. ```. NC_000001.11	6545786	.	C	A,T	57	PASS	.	GT:GQ:DP:AD:VAF:PL	1/2:48:91:0,54,37:0.593407,0.406593:57,52,61,52,0,66. ````. However, from 47 multi-allelic variants, 4 have a GT field presented as 0/1. Is it safe to further process the VCF file to only keep the first allele for these 4 variants since deepvariant did not call them?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/618
https://github.com/google/deepvariant/issues/618:191,security,detect,detect,191,"@AndrewCarroll Thank you for the information. In the description above, you mentioned that deepvariant sees two candidates:. - CAGCAGCGCT -> C. - C -> T. However, I thought deepvariant would detect the following as the two candidates (Applying vt decompose produces the same as the following):. - CAGCAGCGCT -> C. - CAGCAGCGCT -> T. Am I misunderstanding this? Going back to the GT field, can it be that this behavior is changed in version 1.5.0? Looking results of deepvariant version 1.2.0, the GT field for all multi-allelic variants only supports the first alternate allele. I tried running deepvariant version 1.5.0 on the same alignment file, and from 47 multi-allelic variants, almost all have a GT field that supports both of the alternate alleles. Here is an example:. Here is an example:. Result of deepvariant version 1.2.0:. ```. NC_000001.11	6545786	.	C	A,T	.	.	.	GT:GQ:DP:AD:VAF:PL	1/0:40:91:0,54,37:0.593407,0.406593:50,44,53,44,0,61. ```. Result of deepvariant version 1.5.0:. ```. NC_000001.11	6545786	.	C	A,T	57	PASS	.	GT:GQ:DP:AD:VAF:PL	1/2:48:91:0,54,37:0.593407,0.406593:57,52,61,52,0,66. ````. However, from 47 multi-allelic variants, 4 have a GT field presented as 0/1. Is it safe to further process the VCF file to only keep the first allele for these 4 variants since deepvariant did not call them?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/618
https://github.com/google/deepvariant/issues/618:409,usability,behavi,behavior,409,"@AndrewCarroll Thank you for the information. In the description above, you mentioned that deepvariant sees two candidates:. - CAGCAGCGCT -> C. - C -> T. However, I thought deepvariant would detect the following as the two candidates (Applying vt decompose produces the same as the following):. - CAGCAGCGCT -> C. - CAGCAGCGCT -> T. Am I misunderstanding this? Going back to the GT field, can it be that this behavior is changed in version 1.5.0? Looking results of deepvariant version 1.2.0, the GT field for all multi-allelic variants only supports the first alternate allele. I tried running deepvariant version 1.5.0 on the same alignment file, and from 47 multi-allelic variants, almost all have a GT field that supports both of the alternate alleles. Here is an example:. Here is an example:. Result of deepvariant version 1.2.0:. ```. NC_000001.11	6545786	.	C	A,T	.	.	.	GT:GQ:DP:AD:VAF:PL	1/0:40:91:0,54,37:0.593407,0.406593:50,44,53,44,0,61. ```. Result of deepvariant version 1.5.0:. ```. NC_000001.11	6545786	.	C	A,T	57	PASS	.	GT:GQ:DP:AD:VAF:PL	1/2:48:91:0,54,37:0.593407,0.406593:57,52,61,52,0,66. ````. However, from 47 multi-allelic variants, 4 have a GT field presented as 0/1. Is it safe to further process the VCF file to only keep the first allele for these 4 variants since deepvariant did not call them?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/618
https://github.com/google/deepvariant/issues/618:542,usability,support,supports,542,"@AndrewCarroll Thank you for the information. In the description above, you mentioned that deepvariant sees two candidates:. - CAGCAGCGCT -> C. - C -> T. However, I thought deepvariant would detect the following as the two candidates (Applying vt decompose produces the same as the following):. - CAGCAGCGCT -> C. - CAGCAGCGCT -> T. Am I misunderstanding this? Going back to the GT field, can it be that this behavior is changed in version 1.5.0? Looking results of deepvariant version 1.2.0, the GT field for all multi-allelic variants only supports the first alternate allele. I tried running deepvariant version 1.5.0 on the same alignment file, and from 47 multi-allelic variants, almost all have a GT field that supports both of the alternate alleles. Here is an example:. Here is an example:. Result of deepvariant version 1.2.0:. ```. NC_000001.11	6545786	.	C	A,T	.	.	.	GT:GQ:DP:AD:VAF:PL	1/0:40:91:0,54,37:0.593407,0.406593:50,44,53,44,0,61. ```. Result of deepvariant version 1.5.0:. ```. NC_000001.11	6545786	.	C	A,T	57	PASS	.	GT:GQ:DP:AD:VAF:PL	1/2:48:91:0,54,37:0.593407,0.406593:57,52,61,52,0,66. ````. However, from 47 multi-allelic variants, 4 have a GT field presented as 0/1. Is it safe to further process the VCF file to only keep the first allele for these 4 variants since deepvariant did not call them?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/618
https://github.com/google/deepvariant/issues/618:717,usability,support,supports,717,"@AndrewCarroll Thank you for the information. In the description above, you mentioned that deepvariant sees two candidates:. - CAGCAGCGCT -> C. - C -> T. However, I thought deepvariant would detect the following as the two candidates (Applying vt decompose produces the same as the following):. - CAGCAGCGCT -> C. - CAGCAGCGCT -> T. Am I misunderstanding this? Going back to the GT field, can it be that this behavior is changed in version 1.5.0? Looking results of deepvariant version 1.2.0, the GT field for all multi-allelic variants only supports the first alternate allele. I tried running deepvariant version 1.5.0 on the same alignment file, and from 47 multi-allelic variants, almost all have a GT field that supports both of the alternate alleles. Here is an example:. Here is an example:. Result of deepvariant version 1.2.0:. ```. NC_000001.11	6545786	.	C	A,T	.	.	.	GT:GQ:DP:AD:VAF:PL	1/0:40:91:0,54,37:0.593407,0.406593:50,44,53,44,0,61. ```. Result of deepvariant version 1.5.0:. ```. NC_000001.11	6545786	.	C	A,T	57	PASS	.	GT:GQ:DP:AD:VAF:PL	1/2:48:91:0,54,37:0.593407,0.406593:57,52,61,52,0,66. ````. However, from 47 multi-allelic variants, 4 have a GT field presented as 0/1. Is it safe to further process the VCF file to only keep the first allele for these 4 variants since deepvariant did not call them?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/618
https://github.com/google/deepvariant/issues/618:1322,availability,down,downstream,1322,"Hi @fardokhtsadat . I realize in looking at the VCF entry, I was mistaken in saying DeepVariant sees a SNP and an Indel. In fact, it sees the candidates as you wrote. CAGCAGCGCT -> C. CAGCAGCGCT -> T. That's entirely on me, it should be clear from the VCF, and I don't have an excuse to make such a basic mistake. The call looks correct, it does look like a HET deletion of AGCAGCGCT. It looks like the realigner has decided to place a gap at this position and left-align the trailing T at the edge of the deletion so that when it is present it looks like a SNP. It's hard to authoritatively say why the realigner is going something without getting the actual BAM snippet and looking at the realignment. That part is in the heuristics of DeepVariant not the neural net. As a result of the re-alignment, DeepVariant sees the T SNP as a candidate but seems to correctly reject it. I think that's what's going on. In any case, a candidate here for the SNP seems to be a function of the realigner? . The behavior should not have changed from DeepVariant v1.2 to v1.5. In general, uncalled multiallelic events should be a relatively rare phenomenon. . Generally, if you need to, you can prune the alleles that are not called in DeepVariant VCF as long as the result of pruning stays compliant with the VCF spec expected by any downstream tools.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/618
https://github.com/google/deepvariant/issues/618:1098,integrability,event,events,1098,"Hi @fardokhtsadat . I realize in looking at the VCF entry, I was mistaken in saying DeepVariant sees a SNP and an Indel. In fact, it sees the candidates as you wrote. CAGCAGCGCT -> C. CAGCAGCGCT -> T. That's entirely on me, it should be clear from the VCF, and I don't have an excuse to make such a basic mistake. The call looks correct, it does look like a HET deletion of AGCAGCGCT. It looks like the realigner has decided to place a gap at this position and left-align the trailing T at the edge of the deletion so that when it is present it looks like a SNP. It's hard to authoritatively say why the realigner is going something without getting the actual BAM snippet and looking at the realignment. That part is in the heuristics of DeepVariant not the neural net. As a result of the re-alignment, DeepVariant sees the T SNP as a candidate but seems to correctly reject it. I think that's what's going on. In any case, a candidate here for the SNP seems to be a function of the realigner? . The behavior should not have changed from DeepVariant v1.2 to v1.5. In general, uncalled multiallelic events should be a relatively rare phenomenon. . Generally, if you need to, you can prune the alleles that are not called in DeepVariant VCF as long as the result of pruning stays compliant with the VCF spec expected by any downstream tools.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/618
https://github.com/google/deepvariant/issues/618:1278,integrability,complian,compliant,1278,"Hi @fardokhtsadat . I realize in looking at the VCF entry, I was mistaken in saying DeepVariant sees a SNP and an Indel. In fact, it sees the candidates as you wrote. CAGCAGCGCT -> C. CAGCAGCGCT -> T. That's entirely on me, it should be clear from the VCF, and I don't have an excuse to make such a basic mistake. The call looks correct, it does look like a HET deletion of AGCAGCGCT. It looks like the realigner has decided to place a gap at this position and left-align the trailing T at the edge of the deletion so that when it is present it looks like a SNP. It's hard to authoritatively say why the realigner is going something without getting the actual BAM snippet and looking at the realignment. That part is in the heuristics of DeepVariant not the neural net. As a result of the re-alignment, DeepVariant sees the T SNP as a candidate but seems to correctly reject it. I think that's what's going on. In any case, a candidate here for the SNP seems to be a function of the realigner? . The behavior should not have changed from DeepVariant v1.2 to v1.5. In general, uncalled multiallelic events should be a relatively rare phenomenon. . Generally, if you need to, you can prune the alleles that are not called in DeepVariant VCF as long as the result of pruning stays compliant with the VCF spec expected by any downstream tools.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/618
https://github.com/google/deepvariant/issues/618:341,reliability,doe,does,341,"Hi @fardokhtsadat . I realize in looking at the VCF entry, I was mistaken in saying DeepVariant sees a SNP and an Indel. In fact, it sees the candidates as you wrote. CAGCAGCGCT -> C. CAGCAGCGCT -> T. That's entirely on me, it should be clear from the VCF, and I don't have an excuse to make such a basic mistake. The call looks correct, it does look like a HET deletion of AGCAGCGCT. It looks like the realigner has decided to place a gap at this position and left-align the trailing T at the edge of the deletion so that when it is present it looks like a SNP. It's hard to authoritatively say why the realigner is going something without getting the actual BAM snippet and looking at the realignment. That part is in the heuristics of DeepVariant not the neural net. As a result of the re-alignment, DeepVariant sees the T SNP as a candidate but seems to correctly reject it. I think that's what's going on. In any case, a candidate here for the SNP seems to be a function of the realigner? . The behavior should not have changed from DeepVariant v1.2 to v1.5. In general, uncalled multiallelic events should be a relatively rare phenomenon. . Generally, if you need to, you can prune the alleles that are not called in DeepVariant VCF as long as the result of pruning stays compliant with the VCF spec expected by any downstream tools.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/618
https://github.com/google/deepvariant/issues/618:1278,safety,compl,compliant,1278,"Hi @fardokhtsadat . I realize in looking at the VCF entry, I was mistaken in saying DeepVariant sees a SNP and an Indel. In fact, it sees the candidates as you wrote. CAGCAGCGCT -> C. CAGCAGCGCT -> T. That's entirely on me, it should be clear from the VCF, and I don't have an excuse to make such a basic mistake. The call looks correct, it does look like a HET deletion of AGCAGCGCT. It looks like the realigner has decided to place a gap at this position and left-align the trailing T at the edge of the deletion so that when it is present it looks like a SNP. It's hard to authoritatively say why the realigner is going something without getting the actual BAM snippet and looking at the realignment. That part is in the heuristics of DeepVariant not the neural net. As a result of the re-alignment, DeepVariant sees the T SNP as a candidate but seems to correctly reject it. I think that's what's going on. In any case, a candidate here for the SNP seems to be a function of the realigner? . The behavior should not have changed from DeepVariant v1.2 to v1.5. In general, uncalled multiallelic events should be a relatively rare phenomenon. . Generally, if you need to, you can prune the alleles that are not called in DeepVariant VCF as long as the result of pruning stays compliant with the VCF spec expected by any downstream tools.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/618
https://github.com/google/deepvariant/issues/618:576,security,auth,authoritatively,576,"Hi @fardokhtsadat . I realize in looking at the VCF entry, I was mistaken in saying DeepVariant sees a SNP and an Indel. In fact, it sees the candidates as you wrote. CAGCAGCGCT -> C. CAGCAGCGCT -> T. That's entirely on me, it should be clear from the VCF, and I don't have an excuse to make such a basic mistake. The call looks correct, it does look like a HET deletion of AGCAGCGCT. It looks like the realigner has decided to place a gap at this position and left-align the trailing T at the edge of the deletion so that when it is present it looks like a SNP. It's hard to authoritatively say why the realigner is going something without getting the actual BAM snippet and looking at the realignment. That part is in the heuristics of DeepVariant not the neural net. As a result of the re-alignment, DeepVariant sees the T SNP as a candidate but seems to correctly reject it. I think that's what's going on. In any case, a candidate here for the SNP seems to be a function of the realigner? . The behavior should not have changed from DeepVariant v1.2 to v1.5. In general, uncalled multiallelic events should be a relatively rare phenomenon. . Generally, if you need to, you can prune the alleles that are not called in DeepVariant VCF as long as the result of pruning stays compliant with the VCF spec expected by any downstream tools.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/618
https://github.com/google/deepvariant/issues/618:1278,security,compl,compliant,1278,"Hi @fardokhtsadat . I realize in looking at the VCF entry, I was mistaken in saying DeepVariant sees a SNP and an Indel. In fact, it sees the candidates as you wrote. CAGCAGCGCT -> C. CAGCAGCGCT -> T. That's entirely on me, it should be clear from the VCF, and I don't have an excuse to make such a basic mistake. The call looks correct, it does look like a HET deletion of AGCAGCGCT. It looks like the realigner has decided to place a gap at this position and left-align the trailing T at the edge of the deletion so that when it is present it looks like a SNP. It's hard to authoritatively say why the realigner is going something without getting the actual BAM snippet and looking at the realignment. That part is in the heuristics of DeepVariant not the neural net. As a result of the re-alignment, DeepVariant sees the T SNP as a candidate but seems to correctly reject it. I think that's what's going on. In any case, a candidate here for the SNP seems to be a function of the realigner? . The behavior should not have changed from DeepVariant v1.2 to v1.5. In general, uncalled multiallelic events should be a relatively rare phenomenon. . Generally, if you need to, you can prune the alleles that are not called in DeepVariant VCF as long as the result of pruning stays compliant with the VCF spec expected by any downstream tools.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/618
https://github.com/google/deepvariant/issues/618:237,usability,clear,clear,237,"Hi @fardokhtsadat . I realize in looking at the VCF entry, I was mistaken in saying DeepVariant sees a SNP and an Indel. In fact, it sees the candidates as you wrote. CAGCAGCGCT -> C. CAGCAGCGCT -> T. That's entirely on me, it should be clear from the VCF, and I don't have an excuse to make such a basic mistake. The call looks correct, it does look like a HET deletion of AGCAGCGCT. It looks like the realigner has decided to place a gap at this position and left-align the trailing T at the edge of the deletion so that when it is present it looks like a SNP. It's hard to authoritatively say why the realigner is going something without getting the actual BAM snippet and looking at the realignment. That part is in the heuristics of DeepVariant not the neural net. As a result of the re-alignment, DeepVariant sees the T SNP as a candidate but seems to correctly reject it. I think that's what's going on. In any case, a candidate here for the SNP seems to be a function of the realigner? . The behavior should not have changed from DeepVariant v1.2 to v1.5. In general, uncalled multiallelic events should be a relatively rare phenomenon. . Generally, if you need to, you can prune the alleles that are not called in DeepVariant VCF as long as the result of pruning stays compliant with the VCF spec expected by any downstream tools.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/618
https://github.com/google/deepvariant/issues/618:1000,usability,behavi,behavior,1000,"Hi @fardokhtsadat . I realize in looking at the VCF entry, I was mistaken in saying DeepVariant sees a SNP and an Indel. In fact, it sees the candidates as you wrote. CAGCAGCGCT -> C. CAGCAGCGCT -> T. That's entirely on me, it should be clear from the VCF, and I don't have an excuse to make such a basic mistake. The call looks correct, it does look like a HET deletion of AGCAGCGCT. It looks like the realigner has decided to place a gap at this position and left-align the trailing T at the edge of the deletion so that when it is present it looks like a SNP. It's hard to authoritatively say why the realigner is going something without getting the actual BAM snippet and looking at the realignment. That part is in the heuristics of DeepVariant not the neural net. As a result of the re-alignment, DeepVariant sees the T SNP as a candidate but seems to correctly reject it. I think that's what's going on. In any case, a candidate here for the SNP seems to be a function of the realigner? . The behavior should not have changed from DeepVariant v1.2 to v1.5. In general, uncalled multiallelic events should be a relatively rare phenomenon. . Generally, if you need to, you can prune the alleles that are not called in DeepVariant VCF as long as the result of pruning stays compliant with the VCF spec expected by any downstream tools.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/618
https://github.com/google/deepvariant/issues/618:1333,usability,tool,tools,1333,"Hi @fardokhtsadat . I realize in looking at the VCF entry, I was mistaken in saying DeepVariant sees a SNP and an Indel. In fact, it sees the candidates as you wrote. CAGCAGCGCT -> C. CAGCAGCGCT -> T. That's entirely on me, it should be clear from the VCF, and I don't have an excuse to make such a basic mistake. The call looks correct, it does look like a HET deletion of AGCAGCGCT. It looks like the realigner has decided to place a gap at this position and left-align the trailing T at the edge of the deletion so that when it is present it looks like a SNP. It's hard to authoritatively say why the realigner is going something without getting the actual BAM snippet and looking at the realignment. That part is in the heuristics of DeepVariant not the neural net. As a result of the re-alignment, DeepVariant sees the T SNP as a candidate but seems to correctly reject it. I think that's what's going on. In any case, a candidate here for the SNP seems to be a function of the realigner? . The behavior should not have changed from DeepVariant v1.2 to v1.5. In general, uncalled multiallelic events should be a relatively rare phenomenon. . Generally, if you need to, you can prune the alleles that are not called in DeepVariant VCF as long as the result of pruning stays compliant with the VCF spec expected by any downstream tools.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/618
https://github.com/google/deepvariant/issues/619:62,deployability,build,build,62,"Hi @sen1019san . You mentioned you're using singularity image build from dockerhub, and you're using the GPU version. 1. Just to make sure we can reproduce the steps, can you given an en example of the command you ran? 2. We have 1.5.0 as well. Does that work for you, or does that also crash?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:109,deployability,version,version,109,"Hi @sen1019san . You mentioned you're using singularity image build from dockerhub, and you're using the GPU version. 1. Just to make sure we can reproduce the steps, can you given an en example of the command you ran? 2. We have 1.5.0 as well. Does that work for you, or does that also crash?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:105,energy efficiency,GPU,GPU,105,"Hi @sen1019san . You mentioned you're using singularity image build from dockerhub, and you're using the GPU version. 1. Just to make sure we can reproduce the steps, can you given an en example of the command you ran? 2. We have 1.5.0 as well. Does that work for you, or does that also crash?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:109,integrability,version,version,109,"Hi @sen1019san . You mentioned you're using singularity image build from dockerhub, and you're using the GPU version. 1. Just to make sure we can reproduce the steps, can you given an en example of the command you ran? 2. We have 1.5.0 as well. Does that work for you, or does that also crash?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:109,modifiability,version,version,109,"Hi @sen1019san . You mentioned you're using singularity image build from dockerhub, and you're using the GPU version. 1. Just to make sure we can reproduce the steps, can you given an en example of the command you ran? 2. We have 1.5.0 as well. Does that work for you, or does that also crash?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:105,performance,GPU,GPU,105,"Hi @sen1019san . You mentioned you're using singularity image build from dockerhub, and you're using the GPU version. 1. Just to make sure we can reproduce the steps, can you given an en example of the command you ran? 2. We have 1.5.0 as well. Does that work for you, or does that also crash?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:245,reliability,Doe,Does,245,"Hi @sen1019san . You mentioned you're using singularity image build from dockerhub, and you're using the GPU version. 1. Just to make sure we can reproduce the steps, can you given an en example of the command you ran? 2. We have 1.5.0 as well. Does that work for you, or does that also crash?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:272,reliability,doe,does,272,"Hi @sen1019san . You mentioned you're using singularity image build from dockerhub, and you're using the GPU version. 1. Just to make sure we can reproduce the steps, can you given an en example of the command you ran? 2. We have 1.5.0 as well. Does that work for you, or does that also crash?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:202,usability,command,command,202,"Hi @sen1019san . You mentioned you're using singularity image build from dockerhub, and you're using the GPU version. 1. Just to make sure we can reproduce the steps, can you given an en example of the command you ran? 2. We have 1.5.0 as well. Does that work for you, or does that also crash?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:187,availability,error,error,187,"Hi, @pichuan, thanks for your quick replay. I ran the command `/opt/deepvariant/bin/run_deepvariant --version` using GPU version in singularity image . It return the `CUDA_ERROR_UNKNOWN` error as above I mentioned. Now I am trying to use the 1.5.0 version. I use the same test command and it return the same error as followings:. ```. Singularity> /opt/deepvariant/bin/run_deepvariant --version. 2023-03-16 09:39:40.831726: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-03-16 09:39:40.953050: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. 2023-03-16 09:39:42.599918: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. 2023-03-16 09:39:42.599962: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: yy01.local. 2023-03-16 09:39:42.599972: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: yy01.local. 2023-03-16 09:39:42.600020: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". 2023-03-16 09:39:42.600049: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 520.61.5. DeepVariant version 1.5.0. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:308,availability,error,error,308,"Hi, @pichuan, thanks for your quick replay. I ran the command `/opt/deepvariant/bin/run_deepvariant --version` using GPU version in singularity image . It return the `CUDA_ERROR_UNKNOWN` error as above I mentioned. Now I am trying to use the 1.5.0 version. I use the same test command and it return the same error as followings:. ```. Singularity> /opt/deepvariant/bin/run_deepvariant --version. 2023-03-16 09:39:40.831726: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-03-16 09:39:40.953050: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. 2023-03-16 09:39:42.599918: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. 2023-03-16 09:39:42.599962: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: yy01.local. 2023-03-16 09:39:42.599972: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: yy01.local. 2023-03-16 09:39:42.600020: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". 2023-03-16 09:39:42.600049: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 520.61.5. DeepVariant version 1.5.0. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:624,availability,operat,operations,624,"Hi, @pichuan, thanks for your quick replay. I ran the command `/opt/deepvariant/bin/run_deepvariant --version` using GPU version in singularity image . It return the `CUDA_ERROR_UNKNOWN` error as above I mentioned. Now I am trying to use the 1.5.0 version. I use the same test command and it return the same error as followings:. ```. Singularity> /opt/deepvariant/bin/run_deepvariant --version. 2023-03-16 09:39:40.831726: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-03-16 09:39:40.953050: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. 2023-03-16 09:39:42.599918: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. 2023-03-16 09:39:42.599962: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: yy01.local. 2023-03-16 09:39:42.599972: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: yy01.local. 2023-03-16 09:39:42.600020: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". 2023-03-16 09:39:42.600049: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 520.61.5. DeepVariant version 1.5.0. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:690,availability,operat,operations,690,"Hi, @pichuan, thanks for your quick replay. I ran the command `/opt/deepvariant/bin/run_deepvariant --version` using GPU version in singularity image . It return the `CUDA_ERROR_UNKNOWN` error as above I mentioned. Now I am trying to use the 1.5.0 version. I use the same test command and it return the same error as followings:. ```. Singularity> /opt/deepvariant/bin/run_deepvariant --version. 2023-03-16 09:39:40.831726: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-03-16 09:39:40.953050: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. 2023-03-16 09:39:42.599918: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. 2023-03-16 09:39:42.599962: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: yy01.local. 2023-03-16 09:39:42.599972: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: yy01.local. 2023-03-16 09:39:42.600020: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". 2023-03-16 09:39:42.600049: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 520.61.5. DeepVariant version 1.5.0. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:836,availability,operat,operations,836,"Hi, @pichuan, thanks for your quick replay. I ran the command `/opt/deepvariant/bin/run_deepvariant --version` using GPU version in singularity image . It return the `CUDA_ERROR_UNKNOWN` error as above I mentioned. Now I am trying to use the 1.5.0 version. I use the same test command and it return the same error as followings:. ```. Singularity> /opt/deepvariant/bin/run_deepvariant --version. 2023-03-16 09:39:40.831726: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-03-16 09:39:40.953050: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. 2023-03-16 09:39:42.599918: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. 2023-03-16 09:39:42.599962: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: yy01.local. 2023-03-16 09:39:42.599972: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: yy01.local. 2023-03-16 09:39:42.600020: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". 2023-03-16 09:39:42.600049: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 520.61.5. DeepVariant version 1.5.0. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:867,availability,sli,slightly,867,"Hi, @pichuan, thanks for your quick replay. I ran the command `/opt/deepvariant/bin/run_deepvariant --version` using GPU version in singularity image . It return the `CUDA_ERROR_UNKNOWN` error as above I mentioned. Now I am trying to use the 1.5.0 version. I use the same test command and it return the same error as followings:. ```. Singularity> /opt/deepvariant/bin/run_deepvariant --version. 2023-03-16 09:39:40.831726: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-03-16 09:39:40.953050: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. 2023-03-16 09:39:42.599918: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. 2023-03-16 09:39:42.599962: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: yy01.local. 2023-03-16 09:39:42.599972: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: yy01.local. 2023-03-16 09:39:42.600020: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". 2023-03-16 09:39:42.600049: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 520.61.5. DeepVariant version 1.5.0. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:936,availability,error,errors,936,"Hi, @pichuan, thanks for your quick replay. I ran the command `/opt/deepvariant/bin/run_deepvariant --version` using GPU version in singularity image . It return the `CUDA_ERROR_UNKNOWN` error as above I mentioned. Now I am trying to use the 1.5.0 version. I use the same test command and it return the same error as followings:. ```. Singularity> /opt/deepvariant/bin/run_deepvariant --version. 2023-03-16 09:39:40.831726: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-03-16 09:39:40.953050: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. 2023-03-16 09:39:42.599918: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. 2023-03-16 09:39:42.599962: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: yy01.local. 2023-03-16 09:39:42.599972: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: yy01.local. 2023-03-16 09:39:42.600020: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". 2023-03-16 09:39:42.600049: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 520.61.5. DeepVariant version 1.5.0. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:1198,availability,error,error,1198,"Hi, @pichuan, thanks for your quick replay. I ran the command `/opt/deepvariant/bin/run_deepvariant --version` using GPU version in singularity image . It return the `CUDA_ERROR_UNKNOWN` error as above I mentioned. Now I am trying to use the 1.5.0 version. I use the same test command and it return the same error as followings:. ```. Singularity> /opt/deepvariant/bin/run_deepvariant --version. 2023-03-16 09:39:40.831726: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-03-16 09:39:40.953050: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. 2023-03-16 09:39:42.599918: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. 2023-03-16 09:39:42.599962: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: yy01.local. 2023-03-16 09:39:42.599972: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: yy01.local. 2023-03-16 09:39:42.600020: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". 2023-03-16 09:39:42.600049: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 520.61.5. DeepVariant version 1.5.0. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:102,deployability,version,version,102,"Hi, @pichuan, thanks for your quick replay. I ran the command `/opt/deepvariant/bin/run_deepvariant --version` using GPU version in singularity image . It return the `CUDA_ERROR_UNKNOWN` error as above I mentioned. Now I am trying to use the 1.5.0 version. I use the same test command and it return the same error as followings:. ```. Singularity> /opt/deepvariant/bin/run_deepvariant --version. 2023-03-16 09:39:40.831726: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-03-16 09:39:40.953050: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. 2023-03-16 09:39:42.599918: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. 2023-03-16 09:39:42.599962: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: yy01.local. 2023-03-16 09:39:42.599972: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: yy01.local. 2023-03-16 09:39:42.600020: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". 2023-03-16 09:39:42.600049: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 520.61.5. DeepVariant version 1.5.0. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:121,deployability,version,version,121,"Hi, @pichuan, thanks for your quick replay. I ran the command `/opt/deepvariant/bin/run_deepvariant --version` using GPU version in singularity image . It return the `CUDA_ERROR_UNKNOWN` error as above I mentioned. Now I am trying to use the 1.5.0 version. I use the same test command and it return the same error as followings:. ```. Singularity> /opt/deepvariant/bin/run_deepvariant --version. 2023-03-16 09:39:40.831726: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-03-16 09:39:40.953050: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. 2023-03-16 09:39:42.599918: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. 2023-03-16 09:39:42.599962: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: yy01.local. 2023-03-16 09:39:42.599972: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: yy01.local. 2023-03-16 09:39:42.600020: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". 2023-03-16 09:39:42.600049: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 520.61.5. DeepVariant version 1.5.0. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:248,deployability,version,version,248,"Hi, @pichuan, thanks for your quick replay. I ran the command `/opt/deepvariant/bin/run_deepvariant --version` using GPU version in singularity image . It return the `CUDA_ERROR_UNKNOWN` error as above I mentioned. Now I am trying to use the 1.5.0 version. I use the same test command and it return the same error as followings:. ```. Singularity> /opt/deepvariant/bin/run_deepvariant --version. 2023-03-16 09:39:40.831726: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-03-16 09:39:40.953050: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. 2023-03-16 09:39:42.599918: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. 2023-03-16 09:39:42.599962: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: yy01.local. 2023-03-16 09:39:42.599972: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: yy01.local. 2023-03-16 09:39:42.600020: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". 2023-03-16 09:39:42.600049: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 520.61.5. DeepVariant version 1.5.0. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:387,deployability,version,version,387,"Hi, @pichuan, thanks for your quick replay. I ran the command `/opt/deepvariant/bin/run_deepvariant --version` using GPU version in singularity image . It return the `CUDA_ERROR_UNKNOWN` error as above I mentioned. Now I am trying to use the 1.5.0 version. I use the same test command and it return the same error as followings:. ```. Singularity> /opt/deepvariant/bin/run_deepvariant --version. 2023-03-16 09:39:40.831726: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-03-16 09:39:40.953050: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. 2023-03-16 09:39:42.599918: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. 2023-03-16 09:39:42.599962: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: yy01.local. 2023-03-16 09:39:42.599972: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: yy01.local. 2023-03-16 09:39:42.600020: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". 2023-03-16 09:39:42.600049: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 520.61.5. DeepVariant version 1.5.0. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:1147,deployability,fail,failed,1147,"Hi, @pichuan, thanks for your quick replay. I ran the command `/opt/deepvariant/bin/run_deepvariant --version` using GPU version in singularity image . It return the `CUDA_ERROR_UNKNOWN` error as above I mentioned. Now I am trying to use the 1.5.0 version. I use the same test command and it return the same error as followings:. ```. Singularity> /opt/deepvariant/bin/run_deepvariant --version. 2023-03-16 09:39:40.831726: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-03-16 09:39:40.953050: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. 2023-03-16 09:39:42.599918: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. 2023-03-16 09:39:42.599962: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: yy01.local. 2023-03-16 09:39:42.599972: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: yy01.local. 2023-03-16 09:39:42.600020: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". 2023-03-16 09:39:42.600049: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 520.61.5. DeepVariant version 1.5.0. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:1605,deployability,version,version,1605,"Hi, @pichuan, thanks for your quick replay. I ran the command `/opt/deepvariant/bin/run_deepvariant --version` using GPU version in singularity image . It return the `CUDA_ERROR_UNKNOWN` error as above I mentioned. Now I am trying to use the 1.5.0 version. I use the same test command and it return the same error as followings:. ```. Singularity> /opt/deepvariant/bin/run_deepvariant --version. 2023-03-16 09:39:40.831726: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-03-16 09:39:40.953050: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. 2023-03-16 09:39:42.599918: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. 2023-03-16 09:39:42.599962: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: yy01.local. 2023-03-16 09:39:42.599972: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: yy01.local. 2023-03-16 09:39:42.600020: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". 2023-03-16 09:39:42.600049: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 520.61.5. DeepVariant version 1.5.0. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:1692,deployability,version,version,1692,"Hi, @pichuan, thanks for your quick replay. I ran the command `/opt/deepvariant/bin/run_deepvariant --version` using GPU version in singularity image . It return the `CUDA_ERROR_UNKNOWN` error as above I mentioned. Now I am trying to use the 1.5.0 version. I use the same test command and it return the same error as followings:. ```. Singularity> /opt/deepvariant/bin/run_deepvariant --version. 2023-03-16 09:39:40.831726: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-03-16 09:39:40.953050: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. 2023-03-16 09:39:42.599918: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. 2023-03-16 09:39:42.599962: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: yy01.local. 2023-03-16 09:39:42.599972: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: yy01.local. 2023-03-16 09:39:42.600020: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". 2023-03-16 09:39:42.600049: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 520.61.5. DeepVariant version 1.5.0. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:1826,deployability,version,version,1826,"Hi, @pichuan, thanks for your quick replay. I ran the command `/opt/deepvariant/bin/run_deepvariant --version` using GPU version in singularity image . It return the `CUDA_ERROR_UNKNOWN` error as above I mentioned. Now I am trying to use the 1.5.0 version. I use the same test command and it return the same error as followings:. ```. Singularity> /opt/deepvariant/bin/run_deepvariant --version. 2023-03-16 09:39:40.831726: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-03-16 09:39:40.953050: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. 2023-03-16 09:39:42.599918: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. 2023-03-16 09:39:42.599962: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: yy01.local. 2023-03-16 09:39:42.599972: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: yy01.local. 2023-03-16 09:39:42.600020: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". 2023-03-16 09:39:42.600049: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 520.61.5. DeepVariant version 1.5.0. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:1860,deployability,version,version,1860,"Hi, @pichuan, thanks for your quick replay. I ran the command `/opt/deepvariant/bin/run_deepvariant --version` using GPU version in singularity image . It return the `CUDA_ERROR_UNKNOWN` error as above I mentioned. Now I am trying to use the 1.5.0 version. I use the same test command and it return the same error as followings:. ```. Singularity> /opt/deepvariant/bin/run_deepvariant --version. 2023-03-16 09:39:40.831726: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-03-16 09:39:40.953050: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. 2023-03-16 09:39:42.599918: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. 2023-03-16 09:39:42.599962: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: yy01.local. 2023-03-16 09:39:42.599972: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: yy01.local. 2023-03-16 09:39:42.600020: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". 2023-03-16 09:39:42.600049: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 520.61.5. DeepVariant version 1.5.0. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:117,energy efficiency,GPU,GPU,117,"Hi, @pichuan, thanks for your quick replay. I ran the command `/opt/deepvariant/bin/run_deepvariant --version` using GPU version in singularity image . It return the `CUDA_ERROR_UNKNOWN` error as above I mentioned. Now I am trying to use the 1.5.0 version. I use the same test command and it return the same error as followings:. ```. Singularity> /opt/deepvariant/bin/run_deepvariant --version. 2023-03-16 09:39:40.831726: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-03-16 09:39:40.953050: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. 2023-03-16 09:39:42.599918: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. 2023-03-16 09:39:42.599962: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: yy01.local. 2023-03-16 09:39:42.599972: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: yy01.local. 2023-03-16 09:39:42.600020: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". 2023-03-16 09:39:42.600049: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 520.61.5. DeepVariant version 1.5.0. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:437,energy efficiency,core,core,437,"Hi, @pichuan, thanks for your quick replay. I ran the command `/opt/deepvariant/bin/run_deepvariant --version` using GPU version in singularity image . It return the `CUDA_ERROR_UNKNOWN` error as above I mentioned. Now I am trying to use the 1.5.0 version. I use the same test command and it return the same error as followings:. ```. Singularity> /opt/deepvariant/bin/run_deepvariant --version. 2023-03-16 09:39:40.831726: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-03-16 09:39:40.953050: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. 2023-03-16 09:39:42.599918: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. 2023-03-16 09:39:42.599962: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: yy01.local. 2023-03-16 09:39:42.599972: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: yy01.local. 2023-03-16 09:39:42.600020: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". 2023-03-16 09:39:42.600049: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 520.61.5. DeepVariant version 1.5.0. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:503,energy efficiency,optim,optimized,503,"Hi, @pichuan, thanks for your quick replay. I ran the command `/opt/deepvariant/bin/run_deepvariant --version` using GPU version in singularity image . It return the `CUDA_ERROR_UNKNOWN` error as above I mentioned. Now I am trying to use the 1.5.0 version. I use the same test command and it return the same error as followings:. ```. Singularity> /opt/deepvariant/bin/run_deepvariant --version. 2023-03-16 09:39:40.831726: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-03-16 09:39:40.953050: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. 2023-03-16 09:39:42.599918: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. 2023-03-16 09:39:42.599962: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: yy01.local. 2023-03-16 09:39:42.599972: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: yy01.local. 2023-03-16 09:39:42.600020: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". 2023-03-16 09:39:42.600049: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 520.61.5. DeepVariant version 1.5.0. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:583,energy efficiency,CPU,CPU,583,"Hi, @pichuan, thanks for your quick replay. I ran the command `/opt/deepvariant/bin/run_deepvariant --version` using GPU version in singularity image . It return the `CUDA_ERROR_UNKNOWN` error as above I mentioned. Now I am trying to use the 1.5.0 version. I use the same test command and it return the same error as followings:. ```. Singularity> /opt/deepvariant/bin/run_deepvariant --version. 2023-03-16 09:39:40.831726: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-03-16 09:39:40.953050: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. 2023-03-16 09:39:42.599918: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. 2023-03-16 09:39:42.599962: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: yy01.local. 2023-03-16 09:39:42.599972: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: yy01.local. 2023-03-16 09:39:42.600020: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". 2023-03-16 09:39:42.600049: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 520.61.5. DeepVariant version 1.5.0. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:799,energy efficiency,core,core,799,"Hi, @pichuan, thanks for your quick replay. I ran the command `/opt/deepvariant/bin/run_deepvariant --version` using GPU version in singularity image . It return the `CUDA_ERROR_UNKNOWN` error as above I mentioned. Now I am trying to use the 1.5.0 version. I use the same test command and it return the same error as followings:. ```. Singularity> /opt/deepvariant/bin/run_deepvariant --version. 2023-03-16 09:39:40.831726: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-03-16 09:39:40.953050: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. 2023-03-16 09:39:42.599918: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. 2023-03-16 09:39:42.599962: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: yy01.local. 2023-03-16 09:39:42.599972: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: yy01.local. 2023-03-16 09:39:42.600020: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". 2023-03-16 09:39:42.600049: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 520.61.5. DeepVariant version 1.5.0. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:102,integrability,version,version,102,"Hi, @pichuan, thanks for your quick replay. I ran the command `/opt/deepvariant/bin/run_deepvariant --version` using GPU version in singularity image . It return the `CUDA_ERROR_UNKNOWN` error as above I mentioned. Now I am trying to use the 1.5.0 version. I use the same test command and it return the same error as followings:. ```. Singularity> /opt/deepvariant/bin/run_deepvariant --version. 2023-03-16 09:39:40.831726: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-03-16 09:39:40.953050: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. 2023-03-16 09:39:42.599918: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. 2023-03-16 09:39:42.599962: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: yy01.local. 2023-03-16 09:39:42.599972: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: yy01.local. 2023-03-16 09:39:42.600020: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". 2023-03-16 09:39:42.600049: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 520.61.5. DeepVariant version 1.5.0. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:121,integrability,version,version,121,"Hi, @pichuan, thanks for your quick replay. I ran the command `/opt/deepvariant/bin/run_deepvariant --version` using GPU version in singularity image . It return the `CUDA_ERROR_UNKNOWN` error as above I mentioned. Now I am trying to use the 1.5.0 version. I use the same test command and it return the same error as followings:. ```. Singularity> /opt/deepvariant/bin/run_deepvariant --version. 2023-03-16 09:39:40.831726: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-03-16 09:39:40.953050: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. 2023-03-16 09:39:42.599918: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. 2023-03-16 09:39:42.599962: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: yy01.local. 2023-03-16 09:39:42.599972: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: yy01.local. 2023-03-16 09:39:42.600020: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". 2023-03-16 09:39:42.600049: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 520.61.5. DeepVariant version 1.5.0. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:248,integrability,version,version,248,"Hi, @pichuan, thanks for your quick replay. I ran the command `/opt/deepvariant/bin/run_deepvariant --version` using GPU version in singularity image . It return the `CUDA_ERROR_UNKNOWN` error as above I mentioned. Now I am trying to use the 1.5.0 version. I use the same test command and it return the same error as followings:. ```. Singularity> /opt/deepvariant/bin/run_deepvariant --version. 2023-03-16 09:39:40.831726: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-03-16 09:39:40.953050: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. 2023-03-16 09:39:42.599918: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. 2023-03-16 09:39:42.599962: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: yy01.local. 2023-03-16 09:39:42.599972: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: yy01.local. 2023-03-16 09:39:42.600020: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". 2023-03-16 09:39:42.600049: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 520.61.5. DeepVariant version 1.5.0. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:387,integrability,version,version,387,"Hi, @pichuan, thanks for your quick replay. I ran the command `/opt/deepvariant/bin/run_deepvariant --version` using GPU version in singularity image . It return the `CUDA_ERROR_UNKNOWN` error as above I mentioned. Now I am trying to use the 1.5.0 version. I use the same test command and it return the same error as followings:. ```. Singularity> /opt/deepvariant/bin/run_deepvariant --version. 2023-03-16 09:39:40.831726: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-03-16 09:39:40.953050: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. 2023-03-16 09:39:42.599918: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. 2023-03-16 09:39:42.599962: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: yy01.local. 2023-03-16 09:39:42.599972: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: yy01.local. 2023-03-16 09:39:42.600020: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". 2023-03-16 09:39:42.600049: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 520.61.5. DeepVariant version 1.5.0. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:1605,integrability,version,version,1605,"Hi, @pichuan, thanks for your quick replay. I ran the command `/opt/deepvariant/bin/run_deepvariant --version` using GPU version in singularity image . It return the `CUDA_ERROR_UNKNOWN` error as above I mentioned. Now I am trying to use the 1.5.0 version. I use the same test command and it return the same error as followings:. ```. Singularity> /opt/deepvariant/bin/run_deepvariant --version. 2023-03-16 09:39:40.831726: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-03-16 09:39:40.953050: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. 2023-03-16 09:39:42.599918: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. 2023-03-16 09:39:42.599962: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: yy01.local. 2023-03-16 09:39:42.599972: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: yy01.local. 2023-03-16 09:39:42.600020: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". 2023-03-16 09:39:42.600049: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 520.61.5. DeepVariant version 1.5.0. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:1692,integrability,version,version,1692,"Hi, @pichuan, thanks for your quick replay. I ran the command `/opt/deepvariant/bin/run_deepvariant --version` using GPU version in singularity image . It return the `CUDA_ERROR_UNKNOWN` error as above I mentioned. Now I am trying to use the 1.5.0 version. I use the same test command and it return the same error as followings:. ```. Singularity> /opt/deepvariant/bin/run_deepvariant --version. 2023-03-16 09:39:40.831726: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-03-16 09:39:40.953050: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. 2023-03-16 09:39:42.599918: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. 2023-03-16 09:39:42.599962: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: yy01.local. 2023-03-16 09:39:42.599972: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: yy01.local. 2023-03-16 09:39:42.600020: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". 2023-03-16 09:39:42.600049: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 520.61.5. DeepVariant version 1.5.0. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:1826,integrability,version,version,1826,"Hi, @pichuan, thanks for your quick replay. I ran the command `/opt/deepvariant/bin/run_deepvariant --version` using GPU version in singularity image . It return the `CUDA_ERROR_UNKNOWN` error as above I mentioned. Now I am trying to use the 1.5.0 version. I use the same test command and it return the same error as followings:. ```. Singularity> /opt/deepvariant/bin/run_deepvariant --version. 2023-03-16 09:39:40.831726: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-03-16 09:39:40.953050: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. 2023-03-16 09:39:42.599918: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. 2023-03-16 09:39:42.599962: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: yy01.local. 2023-03-16 09:39:42.599972: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: yy01.local. 2023-03-16 09:39:42.600020: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". 2023-03-16 09:39:42.600049: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 520.61.5. DeepVariant version 1.5.0. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:1860,integrability,version,version,1860,"Hi, @pichuan, thanks for your quick replay. I ran the command `/opt/deepvariant/bin/run_deepvariant --version` using GPU version in singularity image . It return the `CUDA_ERROR_UNKNOWN` error as above I mentioned. Now I am trying to use the 1.5.0 version. I use the same test command and it return the same error as followings:. ```. Singularity> /opt/deepvariant/bin/run_deepvariant --version. 2023-03-16 09:39:40.831726: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-03-16 09:39:40.953050: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. 2023-03-16 09:39:42.599918: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. 2023-03-16 09:39:42.599962: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: yy01.local. 2023-03-16 09:39:42.599972: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: yy01.local. 2023-03-16 09:39:42.600020: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". 2023-03-16 09:39:42.600049: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 520.61.5. DeepVariant version 1.5.0. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:442,interoperability,platform,platform,442,"Hi, @pichuan, thanks for your quick replay. I ran the command `/opt/deepvariant/bin/run_deepvariant --version` using GPU version in singularity image . It return the `CUDA_ERROR_UNKNOWN` error as above I mentioned. Now I am trying to use the 1.5.0 version. I use the same test command and it return the same error as followings:. ```. Singularity> /opt/deepvariant/bin/run_deepvariant --version. 2023-03-16 09:39:40.831726: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-03-16 09:39:40.953050: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. 2023-03-16 09:39:42.599918: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. 2023-03-16 09:39:42.599962: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: yy01.local. 2023-03-16 09:39:42.599972: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: yy01.local. 2023-03-16 09:39:42.600020: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". 2023-03-16 09:39:42.600049: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 520.61.5. DeepVariant version 1.5.0. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:102,modifiability,version,version,102,"Hi, @pichuan, thanks for your quick replay. I ran the command `/opt/deepvariant/bin/run_deepvariant --version` using GPU version in singularity image . It return the `CUDA_ERROR_UNKNOWN` error as above I mentioned. Now I am trying to use the 1.5.0 version. I use the same test command and it return the same error as followings:. ```. Singularity> /opt/deepvariant/bin/run_deepvariant --version. 2023-03-16 09:39:40.831726: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-03-16 09:39:40.953050: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. 2023-03-16 09:39:42.599918: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. 2023-03-16 09:39:42.599962: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: yy01.local. 2023-03-16 09:39:42.599972: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: yy01.local. 2023-03-16 09:39:42.600020: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". 2023-03-16 09:39:42.600049: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 520.61.5. DeepVariant version 1.5.0. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:121,modifiability,version,version,121,"Hi, @pichuan, thanks for your quick replay. I ran the command `/opt/deepvariant/bin/run_deepvariant --version` using GPU version in singularity image . It return the `CUDA_ERROR_UNKNOWN` error as above I mentioned. Now I am trying to use the 1.5.0 version. I use the same test command and it return the same error as followings:. ```. Singularity> /opt/deepvariant/bin/run_deepvariant --version. 2023-03-16 09:39:40.831726: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-03-16 09:39:40.953050: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. 2023-03-16 09:39:42.599918: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. 2023-03-16 09:39:42.599962: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: yy01.local. 2023-03-16 09:39:42.599972: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: yy01.local. 2023-03-16 09:39:42.600020: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". 2023-03-16 09:39:42.600049: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 520.61.5. DeepVariant version 1.5.0. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:248,modifiability,version,version,248,"Hi, @pichuan, thanks for your quick replay. I ran the command `/opt/deepvariant/bin/run_deepvariant --version` using GPU version in singularity image . It return the `CUDA_ERROR_UNKNOWN` error as above I mentioned. Now I am trying to use the 1.5.0 version. I use the same test command and it return the same error as followings:. ```. Singularity> /opt/deepvariant/bin/run_deepvariant --version. 2023-03-16 09:39:40.831726: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-03-16 09:39:40.953050: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. 2023-03-16 09:39:42.599918: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. 2023-03-16 09:39:42.599962: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: yy01.local. 2023-03-16 09:39:42.599972: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: yy01.local. 2023-03-16 09:39:42.600020: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". 2023-03-16 09:39:42.600049: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 520.61.5. DeepVariant version 1.5.0. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:387,modifiability,version,version,387,"Hi, @pichuan, thanks for your quick replay. I ran the command `/opt/deepvariant/bin/run_deepvariant --version` using GPU version in singularity image . It return the `CUDA_ERROR_UNKNOWN` error as above I mentioned. Now I am trying to use the 1.5.0 version. I use the same test command and it return the same error as followings:. ```. Singularity> /opt/deepvariant/bin/run_deepvariant --version. 2023-03-16 09:39:40.831726: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-03-16 09:39:40.953050: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. 2023-03-16 09:39:42.599918: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. 2023-03-16 09:39:42.599962: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: yy01.local. 2023-03-16 09:39:42.599972: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: yy01.local. 2023-03-16 09:39:42.600020: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". 2023-03-16 09:39:42.600049: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 520.61.5. DeepVariant version 1.5.0. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:1016,modifiability,variab,variable,1016,"Hi, @pichuan, thanks for your quick replay. I ran the command `/opt/deepvariant/bin/run_deepvariant --version` using GPU version in singularity image . It return the `CUDA_ERROR_UNKNOWN` error as above I mentioned. Now I am trying to use the 1.5.0 version. I use the same test command and it return the same error as followings:. ```. Singularity> /opt/deepvariant/bin/run_deepvariant --version. 2023-03-16 09:39:40.831726: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-03-16 09:39:40.953050: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. 2023-03-16 09:39:42.599918: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. 2023-03-16 09:39:42.599962: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: yy01.local. 2023-03-16 09:39:42.599972: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: yy01.local. 2023-03-16 09:39:42.600020: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". 2023-03-16 09:39:42.600049: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 520.61.5. DeepVariant version 1.5.0. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:1605,modifiability,version,version,1605,"Hi, @pichuan, thanks for your quick replay. I ran the command `/opt/deepvariant/bin/run_deepvariant --version` using GPU version in singularity image . It return the `CUDA_ERROR_UNKNOWN` error as above I mentioned. Now I am trying to use the 1.5.0 version. I use the same test command and it return the same error as followings:. ```. Singularity> /opt/deepvariant/bin/run_deepvariant --version. 2023-03-16 09:39:40.831726: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-03-16 09:39:40.953050: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. 2023-03-16 09:39:42.599918: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. 2023-03-16 09:39:42.599962: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: yy01.local. 2023-03-16 09:39:42.599972: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: yy01.local. 2023-03-16 09:39:42.600020: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". 2023-03-16 09:39:42.600049: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 520.61.5. DeepVariant version 1.5.0. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:1692,modifiability,version,version,1692,"Hi, @pichuan, thanks for your quick replay. I ran the command `/opt/deepvariant/bin/run_deepvariant --version` using GPU version in singularity image . It return the `CUDA_ERROR_UNKNOWN` error as above I mentioned. Now I am trying to use the 1.5.0 version. I use the same test command and it return the same error as followings:. ```. Singularity> /opt/deepvariant/bin/run_deepvariant --version. 2023-03-16 09:39:40.831726: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-03-16 09:39:40.953050: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. 2023-03-16 09:39:42.599918: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. 2023-03-16 09:39:42.599962: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: yy01.local. 2023-03-16 09:39:42.599972: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: yy01.local. 2023-03-16 09:39:42.600020: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". 2023-03-16 09:39:42.600049: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 520.61.5. DeepVariant version 1.5.0. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:1826,modifiability,version,version,1826,"Hi, @pichuan, thanks for your quick replay. I ran the command `/opt/deepvariant/bin/run_deepvariant --version` using GPU version in singularity image . It return the `CUDA_ERROR_UNKNOWN` error as above I mentioned. Now I am trying to use the 1.5.0 version. I use the same test command and it return the same error as followings:. ```. Singularity> /opt/deepvariant/bin/run_deepvariant --version. 2023-03-16 09:39:40.831726: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-03-16 09:39:40.953050: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. 2023-03-16 09:39:42.599918: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. 2023-03-16 09:39:42.599962: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: yy01.local. 2023-03-16 09:39:42.599972: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: yy01.local. 2023-03-16 09:39:42.600020: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". 2023-03-16 09:39:42.600049: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 520.61.5. DeepVariant version 1.5.0. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:1860,modifiability,version,version,1860,"Hi, @pichuan, thanks for your quick replay. I ran the command `/opt/deepvariant/bin/run_deepvariant --version` using GPU version in singularity image . It return the `CUDA_ERROR_UNKNOWN` error as above I mentioned. Now I am trying to use the 1.5.0 version. I use the same test command and it return the same error as followings:. ```. Singularity> /opt/deepvariant/bin/run_deepvariant --version. 2023-03-16 09:39:40.831726: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-03-16 09:39:40.953050: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. 2023-03-16 09:39:42.599918: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. 2023-03-16 09:39:42.599962: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: yy01.local. 2023-03-16 09:39:42.599972: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: yy01.local. 2023-03-16 09:39:42.600020: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". 2023-03-16 09:39:42.600049: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 520.61.5. DeepVariant version 1.5.0. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:117,performance,GPU,GPU,117,"Hi, @pichuan, thanks for your quick replay. I ran the command `/opt/deepvariant/bin/run_deepvariant --version` using GPU version in singularity image . It return the `CUDA_ERROR_UNKNOWN` error as above I mentioned. Now I am trying to use the 1.5.0 version. I use the same test command and it return the same error as followings:. ```. Singularity> /opt/deepvariant/bin/run_deepvariant --version. 2023-03-16 09:39:40.831726: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-03-16 09:39:40.953050: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. 2023-03-16 09:39:42.599918: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. 2023-03-16 09:39:42.599962: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: yy01.local. 2023-03-16 09:39:42.599972: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: yy01.local. 2023-03-16 09:39:42.600020: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". 2023-03-16 09:39:42.600049: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 520.61.5. DeepVariant version 1.5.0. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:187,performance,error,error,187,"Hi, @pichuan, thanks for your quick replay. I ran the command `/opt/deepvariant/bin/run_deepvariant --version` using GPU version in singularity image . It return the `CUDA_ERROR_UNKNOWN` error as above I mentioned. Now I am trying to use the 1.5.0 version. I use the same test command and it return the same error as followings:. ```. Singularity> /opt/deepvariant/bin/run_deepvariant --version. 2023-03-16 09:39:40.831726: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-03-16 09:39:40.953050: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. 2023-03-16 09:39:42.599918: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. 2023-03-16 09:39:42.599962: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: yy01.local. 2023-03-16 09:39:42.599972: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: yy01.local. 2023-03-16 09:39:42.600020: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". 2023-03-16 09:39:42.600049: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 520.61.5. DeepVariant version 1.5.0. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:308,performance,error,error,308,"Hi, @pichuan, thanks for your quick replay. I ran the command `/opt/deepvariant/bin/run_deepvariant --version` using GPU version in singularity image . It return the `CUDA_ERROR_UNKNOWN` error as above I mentioned. Now I am trying to use the 1.5.0 version. I use the same test command and it return the same error as followings:. ```. Singularity> /opt/deepvariant/bin/run_deepvariant --version. 2023-03-16 09:39:40.831726: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-03-16 09:39:40.953050: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. 2023-03-16 09:39:42.599918: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. 2023-03-16 09:39:42.599962: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: yy01.local. 2023-03-16 09:39:42.599972: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: yy01.local. 2023-03-16 09:39:42.600020: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". 2023-03-16 09:39:42.600049: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 520.61.5. DeepVariant version 1.5.0. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:503,performance,optimiz,optimized,503,"Hi, @pichuan, thanks for your quick replay. I ran the command `/opt/deepvariant/bin/run_deepvariant --version` using GPU version in singularity image . It return the `CUDA_ERROR_UNKNOWN` error as above I mentioned. Now I am trying to use the 1.5.0 version. I use the same test command and it return the same error as followings:. ```. Singularity> /opt/deepvariant/bin/run_deepvariant --version. 2023-03-16 09:39:40.831726: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-03-16 09:39:40.953050: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. 2023-03-16 09:39:42.599918: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. 2023-03-16 09:39:42.599962: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: yy01.local. 2023-03-16 09:39:42.599972: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: yy01.local. 2023-03-16 09:39:42.600020: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". 2023-03-16 09:39:42.600049: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 520.61.5. DeepVariant version 1.5.0. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:537,performance,Network,Network,537,"Hi, @pichuan, thanks for your quick replay. I ran the command `/opt/deepvariant/bin/run_deepvariant --version` using GPU version in singularity image . It return the `CUDA_ERROR_UNKNOWN` error as above I mentioned. Now I am trying to use the 1.5.0 version. I use the same test command and it return the same error as followings:. ```. Singularity> /opt/deepvariant/bin/run_deepvariant --version. 2023-03-16 09:39:40.831726: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-03-16 09:39:40.953050: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. 2023-03-16 09:39:42.599918: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. 2023-03-16 09:39:42.599962: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: yy01.local. 2023-03-16 09:39:42.599972: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: yy01.local. 2023-03-16 09:39:42.600020: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". 2023-03-16 09:39:42.600049: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 520.61.5. DeepVariant version 1.5.0. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:583,performance,CPU,CPU,583,"Hi, @pichuan, thanks for your quick replay. I ran the command `/opt/deepvariant/bin/run_deepvariant --version` using GPU version in singularity image . It return the `CUDA_ERROR_UNKNOWN` error as above I mentioned. Now I am trying to use the 1.5.0 version. I use the same test command and it return the same error as followings:. ```. Singularity> /opt/deepvariant/bin/run_deepvariant --version. 2023-03-16 09:39:40.831726: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-03-16 09:39:40.953050: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. 2023-03-16 09:39:42.599918: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. 2023-03-16 09:39:42.599962: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: yy01.local. 2023-03-16 09:39:42.599972: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: yy01.local. 2023-03-16 09:39:42.600020: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". 2023-03-16 09:39:42.600049: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 520.61.5. DeepVariant version 1.5.0. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:603,performance,perform,performance-critical,603,"Hi, @pichuan, thanks for your quick replay. I ran the command `/opt/deepvariant/bin/run_deepvariant --version` using GPU version in singularity image . It return the `CUDA_ERROR_UNKNOWN` error as above I mentioned. Now I am trying to use the 1.5.0 version. I use the same test command and it return the same error as followings:. ```. Singularity> /opt/deepvariant/bin/run_deepvariant --version. 2023-03-16 09:39:40.831726: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-03-16 09:39:40.953050: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. 2023-03-16 09:39:42.599918: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. 2023-03-16 09:39:42.599962: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: yy01.local. 2023-03-16 09:39:42.599972: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: yy01.local. 2023-03-16 09:39:42.600020: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". 2023-03-16 09:39:42.600049: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 520.61.5. DeepVariant version 1.5.0. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:936,performance,error,errors,936,"Hi, @pichuan, thanks for your quick replay. I ran the command `/opt/deepvariant/bin/run_deepvariant --version` using GPU version in singularity image . It return the `CUDA_ERROR_UNKNOWN` error as above I mentioned. Now I am trying to use the 1.5.0 version. I use the same test command and it return the same error as followings:. ```. Singularity> /opt/deepvariant/bin/run_deepvariant --version. 2023-03-16 09:39:40.831726: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-03-16 09:39:40.953050: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. 2023-03-16 09:39:42.599918: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. 2023-03-16 09:39:42.599962: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: yy01.local. 2023-03-16 09:39:42.599972: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: yy01.local. 2023-03-16 09:39:42.600020: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". 2023-03-16 09:39:42.600049: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 520.61.5. DeepVariant version 1.5.0. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:1198,performance,error,error,1198,"Hi, @pichuan, thanks for your quick replay. I ran the command `/opt/deepvariant/bin/run_deepvariant --version` using GPU version in singularity image . It return the `CUDA_ERROR_UNKNOWN` error as above I mentioned. Now I am trying to use the 1.5.0 version. I use the same test command and it return the same error as followings:. ```. Singularity> /opt/deepvariant/bin/run_deepvariant --version. 2023-03-16 09:39:40.831726: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-03-16 09:39:40.953050: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. 2023-03-16 09:39:42.599918: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. 2023-03-16 09:39:42.599962: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: yy01.local. 2023-03-16 09:39:42.599972: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: yy01.local. 2023-03-16 09:39:42.600020: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". 2023-03-16 09:39:42.600049: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 520.61.5. DeepVariant version 1.5.0. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:867,reliability,sli,slightly,867,"Hi, @pichuan, thanks for your quick replay. I ran the command `/opt/deepvariant/bin/run_deepvariant --version` using GPU version in singularity image . It return the `CUDA_ERROR_UNKNOWN` error as above I mentioned. Now I am trying to use the 1.5.0 version. I use the same test command and it return the same error as followings:. ```. Singularity> /opt/deepvariant/bin/run_deepvariant --version. 2023-03-16 09:39:40.831726: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-03-16 09:39:40.953050: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. 2023-03-16 09:39:42.599918: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. 2023-03-16 09:39:42.599962: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: yy01.local. 2023-03-16 09:39:42.599972: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: yy01.local. 2023-03-16 09:39:42.600020: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". 2023-03-16 09:39:42.600049: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 520.61.5. DeepVariant version 1.5.0. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:1147,reliability,fail,failed,1147,"Hi, @pichuan, thanks for your quick replay. I ran the command `/opt/deepvariant/bin/run_deepvariant --version` using GPU version in singularity image . It return the `CUDA_ERROR_UNKNOWN` error as above I mentioned. Now I am trying to use the 1.5.0 version. I use the same test command and it return the same error as followings:. ```. Singularity> /opt/deepvariant/bin/run_deepvariant --version. 2023-03-16 09:39:40.831726: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-03-16 09:39:40.953050: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. 2023-03-16 09:39:42.599918: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. 2023-03-16 09:39:42.599962: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: yy01.local. 2023-03-16 09:39:42.599972: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: yy01.local. 2023-03-16 09:39:42.600020: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". 2023-03-16 09:39:42.600049: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 520.61.5. DeepVariant version 1.5.0. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:1321,reliability,diagno,diagnostic,1321,"Hi, @pichuan, thanks for your quick replay. I ran the command `/opt/deepvariant/bin/run_deepvariant --version` using GPU version in singularity image . It return the `CUDA_ERROR_UNKNOWN` error as above I mentioned. Now I am trying to use the 1.5.0 version. I use the same test command and it return the same error as followings:. ```. Singularity> /opt/deepvariant/bin/run_deepvariant --version. 2023-03-16 09:39:40.831726: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-03-16 09:39:40.953050: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. 2023-03-16 09:39:42.599918: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. 2023-03-16 09:39:42.599962: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: yy01.local. 2023-03-16 09:39:42.599972: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: yy01.local. 2023-03-16 09:39:42.600020: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". 2023-03-16 09:39:42.600049: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 520.61.5. DeepVariant version 1.5.0. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:187,safety,error,error,187,"Hi, @pichuan, thanks for your quick replay. I ran the command `/opt/deepvariant/bin/run_deepvariant --version` using GPU version in singularity image . It return the `CUDA_ERROR_UNKNOWN` error as above I mentioned. Now I am trying to use the 1.5.0 version. I use the same test command and it return the same error as followings:. ```. Singularity> /opt/deepvariant/bin/run_deepvariant --version. 2023-03-16 09:39:40.831726: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-03-16 09:39:40.953050: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. 2023-03-16 09:39:42.599918: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. 2023-03-16 09:39:42.599962: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: yy01.local. 2023-03-16 09:39:42.599972: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: yy01.local. 2023-03-16 09:39:42.600020: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". 2023-03-16 09:39:42.600049: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 520.61.5. DeepVariant version 1.5.0. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:272,safety,test,test,272,"Hi, @pichuan, thanks for your quick replay. I ran the command `/opt/deepvariant/bin/run_deepvariant --version` using GPU version in singularity image . It return the `CUDA_ERROR_UNKNOWN` error as above I mentioned. Now I am trying to use the 1.5.0 version. I use the same test command and it return the same error as followings:. ```. Singularity> /opt/deepvariant/bin/run_deepvariant --version. 2023-03-16 09:39:40.831726: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-03-16 09:39:40.953050: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. 2023-03-16 09:39:42.599918: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. 2023-03-16 09:39:42.599962: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: yy01.local. 2023-03-16 09:39:42.599972: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: yy01.local. 2023-03-16 09:39:42.600020: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". 2023-03-16 09:39:42.600049: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 520.61.5. DeepVariant version 1.5.0. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:308,safety,error,error,308,"Hi, @pichuan, thanks for your quick replay. I ran the command `/opt/deepvariant/bin/run_deepvariant --version` using GPU version in singularity image . It return the `CUDA_ERROR_UNKNOWN` error as above I mentioned. Now I am trying to use the 1.5.0 version. I use the same test command and it return the same error as followings:. ```. Singularity> /opt/deepvariant/bin/run_deepvariant --version. 2023-03-16 09:39:40.831726: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-03-16 09:39:40.953050: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. 2023-03-16 09:39:42.599918: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. 2023-03-16 09:39:42.599962: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: yy01.local. 2023-03-16 09:39:42.599972: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: yy01.local. 2023-03-16 09:39:42.600020: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". 2023-03-16 09:39:42.600049: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 520.61.5. DeepVariant version 1.5.0. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:936,safety,error,errors,936,"Hi, @pichuan, thanks for your quick replay. I ran the command `/opt/deepvariant/bin/run_deepvariant --version` using GPU version in singularity image . It return the `CUDA_ERROR_UNKNOWN` error as above I mentioned. Now I am trying to use the 1.5.0 version. I use the same test command and it return the same error as followings:. ```. Singularity> /opt/deepvariant/bin/run_deepvariant --version. 2023-03-16 09:39:40.831726: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-03-16 09:39:40.953050: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. 2023-03-16 09:39:42.599918: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. 2023-03-16 09:39:42.599962: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: yy01.local. 2023-03-16 09:39:42.599972: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: yy01.local. 2023-03-16 09:39:42.600020: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". 2023-03-16 09:39:42.600049: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 520.61.5. DeepVariant version 1.5.0. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:1198,safety,error,error,1198,"Hi, @pichuan, thanks for your quick replay. I ran the command `/opt/deepvariant/bin/run_deepvariant --version` using GPU version in singularity image . It return the `CUDA_ERROR_UNKNOWN` error as above I mentioned. Now I am trying to use the 1.5.0 version. I use the same test command and it return the same error as followings:. ```. Singularity> /opt/deepvariant/bin/run_deepvariant --version. 2023-03-16 09:39:40.831726: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-03-16 09:39:40.953050: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. 2023-03-16 09:39:42.599918: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. 2023-03-16 09:39:42.599962: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: yy01.local. 2023-03-16 09:39:42.599972: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: yy01.local. 2023-03-16 09:39:42.600020: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". 2023-03-16 09:39:42.600049: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 520.61.5. DeepVariant version 1.5.0. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:537,security,Network,Network,537,"Hi, @pichuan, thanks for your quick replay. I ran the command `/opt/deepvariant/bin/run_deepvariant --version` using GPU version in singularity image . It return the `CUDA_ERROR_UNKNOWN` error as above I mentioned. Now I am trying to use the 1.5.0 version. I use the same test command and it return the same error as followings:. ```. Singularity> /opt/deepvariant/bin/run_deepvariant --version. 2023-03-16 09:39:40.831726: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-03-16 09:39:40.953050: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. 2023-03-16 09:39:42.599918: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. 2023-03-16 09:39:42.599962: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: yy01.local. 2023-03-16 09:39:42.599972: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: yy01.local. 2023-03-16 09:39:42.600020: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". 2023-03-16 09:39:42.600049: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 520.61.5. DeepVariant version 1.5.0. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:272,testability,test,test,272,"Hi, @pichuan, thanks for your quick replay. I ran the command `/opt/deepvariant/bin/run_deepvariant --version` using GPU version in singularity image . It return the `CUDA_ERROR_UNKNOWN` error as above I mentioned. Now I am trying to use the 1.5.0 version. I use the same test command and it return the same error as followings:. ```. Singularity> /opt/deepvariant/bin/run_deepvariant --version. 2023-03-16 09:39:40.831726: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-03-16 09:39:40.953050: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. 2023-03-16 09:39:42.599918: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. 2023-03-16 09:39:42.599962: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: yy01.local. 2023-03-16 09:39:42.599972: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: yy01.local. 2023-03-16 09:39:42.600020: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". 2023-03-16 09:39:42.600049: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 520.61.5. DeepVariant version 1.5.0. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:1321,testability,diagno,diagnostic,1321,"Hi, @pichuan, thanks for your quick replay. I ran the command `/opt/deepvariant/bin/run_deepvariant --version` using GPU version in singularity image . It return the `CUDA_ERROR_UNKNOWN` error as above I mentioned. Now I am trying to use the 1.5.0 version. I use the same test command and it return the same error as followings:. ```. Singularity> /opt/deepvariant/bin/run_deepvariant --version. 2023-03-16 09:39:40.831726: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-03-16 09:39:40.953050: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. 2023-03-16 09:39:42.599918: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. 2023-03-16 09:39:42.599962: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: yy01.local. 2023-03-16 09:39:42.599972: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: yy01.local. 2023-03-16 09:39:42.600020: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". 2023-03-16 09:39:42.600049: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 520.61.5. DeepVariant version 1.5.0. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:54,usability,command,command,54,"Hi, @pichuan, thanks for your quick replay. I ran the command `/opt/deepvariant/bin/run_deepvariant --version` using GPU version in singularity image . It return the `CUDA_ERROR_UNKNOWN` error as above I mentioned. Now I am trying to use the 1.5.0 version. I use the same test command and it return the same error as followings:. ```. Singularity> /opt/deepvariant/bin/run_deepvariant --version. 2023-03-16 09:39:40.831726: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-03-16 09:39:40.953050: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. 2023-03-16 09:39:42.599918: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. 2023-03-16 09:39:42.599962: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: yy01.local. 2023-03-16 09:39:42.599972: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: yy01.local. 2023-03-16 09:39:42.600020: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". 2023-03-16 09:39:42.600049: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 520.61.5. DeepVariant version 1.5.0. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:187,usability,error,error,187,"Hi, @pichuan, thanks for your quick replay. I ran the command `/opt/deepvariant/bin/run_deepvariant --version` using GPU version in singularity image . It return the `CUDA_ERROR_UNKNOWN` error as above I mentioned. Now I am trying to use the 1.5.0 version. I use the same test command and it return the same error as followings:. ```. Singularity> /opt/deepvariant/bin/run_deepvariant --version. 2023-03-16 09:39:40.831726: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-03-16 09:39:40.953050: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. 2023-03-16 09:39:42.599918: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. 2023-03-16 09:39:42.599962: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: yy01.local. 2023-03-16 09:39:42.599972: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: yy01.local. 2023-03-16 09:39:42.600020: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". 2023-03-16 09:39:42.600049: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 520.61.5. DeepVariant version 1.5.0. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:277,usability,command,command,277,"Hi, @pichuan, thanks for your quick replay. I ran the command `/opt/deepvariant/bin/run_deepvariant --version` using GPU version in singularity image . It return the `CUDA_ERROR_UNKNOWN` error as above I mentioned. Now I am trying to use the 1.5.0 version. I use the same test command and it return the same error as followings:. ```. Singularity> /opt/deepvariant/bin/run_deepvariant --version. 2023-03-16 09:39:40.831726: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-03-16 09:39:40.953050: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. 2023-03-16 09:39:42.599918: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. 2023-03-16 09:39:42.599962: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: yy01.local. 2023-03-16 09:39:42.599972: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: yy01.local. 2023-03-16 09:39:42.600020: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". 2023-03-16 09:39:42.600049: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 520.61.5. DeepVariant version 1.5.0. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:308,usability,error,error,308,"Hi, @pichuan, thanks for your quick replay. I ran the command `/opt/deepvariant/bin/run_deepvariant --version` using GPU version in singularity image . It return the `CUDA_ERROR_UNKNOWN` error as above I mentioned. Now I am trying to use the 1.5.0 version. I use the same test command and it return the same error as followings:. ```. Singularity> /opt/deepvariant/bin/run_deepvariant --version. 2023-03-16 09:39:40.831726: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-03-16 09:39:40.953050: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. 2023-03-16 09:39:42.599918: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. 2023-03-16 09:39:42.599962: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: yy01.local. 2023-03-16 09:39:42.599972: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: yy01.local. 2023-03-16 09:39:42.600020: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". 2023-03-16 09:39:42.600049: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 520.61.5. DeepVariant version 1.5.0. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:603,usability,perform,performance-critical,603,"Hi, @pichuan, thanks for your quick replay. I ran the command `/opt/deepvariant/bin/run_deepvariant --version` using GPU version in singularity image . It return the `CUDA_ERROR_UNKNOWN` error as above I mentioned. Now I am trying to use the 1.5.0 version. I use the same test command and it return the same error as followings:. ```. Singularity> /opt/deepvariant/bin/run_deepvariant --version. 2023-03-16 09:39:40.831726: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-03-16 09:39:40.953050: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. 2023-03-16 09:39:42.599918: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. 2023-03-16 09:39:42.599962: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: yy01.local. 2023-03-16 09:39:42.599972: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: yy01.local. 2023-03-16 09:39:42.600020: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". 2023-03-16 09:39:42.600049: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 520.61.5. DeepVariant version 1.5.0. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:829,usability,custom,custom,829,"Hi, @pichuan, thanks for your quick replay. I ran the command `/opt/deepvariant/bin/run_deepvariant --version` using GPU version in singularity image . It return the `CUDA_ERROR_UNKNOWN` error as above I mentioned. Now I am trying to use the 1.5.0 version. I use the same test command and it return the same error as followings:. ```. Singularity> /opt/deepvariant/bin/run_deepvariant --version. 2023-03-16 09:39:40.831726: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-03-16 09:39:40.953050: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. 2023-03-16 09:39:42.599918: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. 2023-03-16 09:39:42.599962: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: yy01.local. 2023-03-16 09:39:42.599972: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: yy01.local. 2023-03-16 09:39:42.600020: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". 2023-03-16 09:39:42.600049: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 520.61.5. DeepVariant version 1.5.0. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:936,usability,error,errors,936,"Hi, @pichuan, thanks for your quick replay. I ran the command `/opt/deepvariant/bin/run_deepvariant --version` using GPU version in singularity image . It return the `CUDA_ERROR_UNKNOWN` error as above I mentioned. Now I am trying to use the 1.5.0 version. I use the same test command and it return the same error as followings:. ```. Singularity> /opt/deepvariant/bin/run_deepvariant --version. 2023-03-16 09:39:40.831726: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-03-16 09:39:40.953050: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. 2023-03-16 09:39:42.599918: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. 2023-03-16 09:39:42.599962: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: yy01.local. 2023-03-16 09:39:42.599972: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: yy01.local. 2023-03-16 09:39:42.600020: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". 2023-03-16 09:39:42.600049: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 520.61.5. DeepVariant version 1.5.0. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:1198,usability,error,error,1198,"Hi, @pichuan, thanks for your quick replay. I ran the command `/opt/deepvariant/bin/run_deepvariant --version` using GPU version in singularity image . It return the `CUDA_ERROR_UNKNOWN` error as above I mentioned. Now I am trying to use the 1.5.0 version. I use the same test command and it return the same error as followings:. ```. Singularity> /opt/deepvariant/bin/run_deepvariant --version. 2023-03-16 09:39:40.831726: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-03-16 09:39:40.953050: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. 2023-03-16 09:39:42.599918: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. 2023-03-16 09:39:42.599962: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: yy01.local. 2023-03-16 09:39:42.599972: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: yy01.local. 2023-03-16 09:39:42.600020: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". 2023-03-16 09:39:42.600049: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 520.61.5. DeepVariant version 1.5.0. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:8,interoperability,share,share,8,Can you share the command you ran right before you got into the place where you ran the command? Thanks.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:18,usability,command,command,18,Can you share the command you ran right before you got into the place where you ran the command? Thanks.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:88,usability,command,command,88,Can you share the command you ran right before you got into the place where you ran the command? Thanks.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:50,deployability,version,version,50,"Hi, @pichuan. I just run into the deepvariant GPU version using singularity as followings:. ```. singularity shell --nv ~/data/images/deepvariant_pgu_latest.simg. ```. The singularity image was built from dockerhub as followings:. ```. singularity pull deepvariant_pgu_latest.simg docker://google/deepvariant:latest-gpu. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:46,energy efficiency,GPU,GPU,46,"Hi, @pichuan. I just run into the deepvariant GPU version using singularity as followings:. ```. singularity shell --nv ~/data/images/deepvariant_pgu_latest.simg. ```. The singularity image was built from dockerhub as followings:. ```. singularity pull deepvariant_pgu_latest.simg docker://google/deepvariant:latest-gpu. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:316,energy efficiency,gpu,gpu,316,"Hi, @pichuan. I just run into the deepvariant GPU version using singularity as followings:. ```. singularity shell --nv ~/data/images/deepvariant_pgu_latest.simg. ```. The singularity image was built from dockerhub as followings:. ```. singularity pull deepvariant_pgu_latest.simg docker://google/deepvariant:latest-gpu. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:50,integrability,version,version,50,"Hi, @pichuan. I just run into the deepvariant GPU version using singularity as followings:. ```. singularity shell --nv ~/data/images/deepvariant_pgu_latest.simg. ```. The singularity image was built from dockerhub as followings:. ```. singularity pull deepvariant_pgu_latest.simg docker://google/deepvariant:latest-gpu. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:50,modifiability,version,version,50,"Hi, @pichuan. I just run into the deepvariant GPU version using singularity as followings:. ```. singularity shell --nv ~/data/images/deepvariant_pgu_latest.simg. ```. The singularity image was built from dockerhub as followings:. ```. singularity pull deepvariant_pgu_latest.simg docker://google/deepvariant:latest-gpu. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:46,performance,GPU,GPU,46,"Hi, @pichuan. I just run into the deepvariant GPU version using singularity as followings:. ```. singularity shell --nv ~/data/images/deepvariant_pgu_latest.simg. ```. The singularity image was built from dockerhub as followings:. ```. singularity pull deepvariant_pgu_latest.simg docker://google/deepvariant:latest-gpu. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:316,performance,gpu,gpu,316,"Hi, @pichuan. I just run into the deepvariant GPU version using singularity as followings:. ```. singularity shell --nv ~/data/images/deepvariant_pgu_latest.simg. ```. The singularity image was built from dockerhub as followings:. ```. singularity pull deepvariant_pgu_latest.simg docker://google/deepvariant:latest-gpu. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:17,deployability,instal,installed,17,"Do you have CUDA installed on your machine? Check whether CUDA is installed on your machine. For example, run:. ```. rpm -qa | grep cuda. ```. or. ```. nvcc --version. ```. If you don't have CUDA installed, please follow the instructions on https://docs.nvidia.com/cuda/cuda-installation-guide-linux/ to make sure you install it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:66,deployability,instal,installed,66,"Do you have CUDA installed on your machine? Check whether CUDA is installed on your machine. For example, run:. ```. rpm -qa | grep cuda. ```. or. ```. nvcc --version. ```. If you don't have CUDA installed, please follow the instructions on https://docs.nvidia.com/cuda/cuda-installation-guide-linux/ to make sure you install it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:159,deployability,version,version,159,"Do you have CUDA installed on your machine? Check whether CUDA is installed on your machine. For example, run:. ```. rpm -qa | grep cuda. ```. or. ```. nvcc --version. ```. If you don't have CUDA installed, please follow the instructions on https://docs.nvidia.com/cuda/cuda-installation-guide-linux/ to make sure you install it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:196,deployability,instal,installed,196,"Do you have CUDA installed on your machine? Check whether CUDA is installed on your machine. For example, run:. ```. rpm -qa | grep cuda. ```. or. ```. nvcc --version. ```. If you don't have CUDA installed, please follow the instructions on https://docs.nvidia.com/cuda/cuda-installation-guide-linux/ to make sure you install it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:275,deployability,instal,installation-guide-linux,275,"Do you have CUDA installed on your machine? Check whether CUDA is installed on your machine. For example, run:. ```. rpm -qa | grep cuda. ```. or. ```. nvcc --version. ```. If you don't have CUDA installed, please follow the instructions on https://docs.nvidia.com/cuda/cuda-installation-guide-linux/ to make sure you install it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:318,deployability,instal,install,318,"Do you have CUDA installed on your machine? Check whether CUDA is installed on your machine. For example, run:. ```. rpm -qa | grep cuda. ```. or. ```. nvcc --version. ```. If you don't have CUDA installed, please follow the instructions on https://docs.nvidia.com/cuda/cuda-installation-guide-linux/ to make sure you install it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:159,integrability,version,version,159,"Do you have CUDA installed on your machine? Check whether CUDA is installed on your machine. For example, run:. ```. rpm -qa | grep cuda. ```. or. ```. nvcc --version. ```. If you don't have CUDA installed, please follow the instructions on https://docs.nvidia.com/cuda/cuda-installation-guide-linux/ to make sure you install it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:159,modifiability,version,version,159,"Do you have CUDA installed on your machine? Check whether CUDA is installed on your machine. For example, run:. ```. rpm -qa | grep cuda. ```. or. ```. nvcc --version. ```. If you don't have CUDA installed, please follow the instructions on https://docs.nvidia.com/cuda/cuda-installation-guide-linux/ to make sure you install it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:288,usability,guid,guide-linux,288,"Do you have CUDA installed on your machine? Check whether CUDA is installed on your machine. For example, run:. ```. rpm -qa | grep cuda. ```. or. ```. nvcc --version. ```. If you don't have CUDA installed, please follow the instructions on https://docs.nvidia.com/cuda/cuda-installation-guide-linux/ to make sure you install it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:36,deployability,instal,installed,36,"Hi, @pichuan. I am sure the CUDA is installed on the host. The CUDA version is V11.8.89. ![image](https://user-images.githubusercontent.com/43125963/225511464-62c75283-b925-4b19-aea4-a75913ffd224.png).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:68,deployability,version,version,68,"Hi, @pichuan. I am sure the CUDA is installed on the host. The CUDA version is V11.8.89. ![image](https://user-images.githubusercontent.com/43125963/225511464-62c75283-b925-4b19-aea4-a75913ffd224.png).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:68,integrability,version,version,68,"Hi, @pichuan. I am sure the CUDA is installed on the host. The CUDA version is V11.8.89. ![image](https://user-images.githubusercontent.com/43125963/225511464-62c75283-b925-4b19-aea4-a75913ffd224.png).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:68,modifiability,version,version,68,"Hi, @pichuan. I am sure the CUDA is installed on the host. The CUDA version is V11.8.89. ![image](https://user-images.githubusercontent.com/43125963/225511464-62c75283-b925-4b19-aea4-a75913ffd224.png).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:106,usability,user,user-images,106,"Hi, @pichuan. I am sure the CUDA is installed on the host. The CUDA version is V11.8.89. ![image](https://user-images.githubusercontent.com/43125963/225511464-62c75283-b925-4b19-aea4-a75913ffd224.png).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:61,deployability,version,versions,61,I see. Right you mentioned that before. It could be that the versions are different. Let me see if it's possible for me to get a CentOS7 set up with CUDA version V11.8.89 and then test with that.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:154,deployability,version,version,154,I see. Right you mentioned that before. It could be that the versions are different. Let me see if it's possible for me to get a CentOS7 set up with CUDA version V11.8.89 and then test with that.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:61,integrability,version,versions,61,I see. Right you mentioned that before. It could be that the versions are different. Let me see if it's possible for me to get a CentOS7 set up with CUDA version V11.8.89 and then test with that.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:154,integrability,version,version,154,I see. Right you mentioned that before. It could be that the versions are different. Let me see if it's possible for me to get a CentOS7 set up with CUDA version V11.8.89 and then test with that.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:61,modifiability,version,versions,61,I see. Right you mentioned that before. It could be that the versions are different. Let me see if it's possible for me to get a CentOS7 set up with CUDA version V11.8.89 and then test with that.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:154,modifiability,version,version,154,I see. Right you mentioned that before. It could be that the versions are different. Let me see if it's possible for me to get a CentOS7 set up with CUDA version V11.8.89 and then test with that.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:180,safety,test,test,180,I see. Right you mentioned that before. It could be that the versions are different. Let me see if it's possible for me to get a CentOS7 set up with CUDA version V11.8.89 and then test with that.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:180,testability,test,test,180,I see. Right you mentioned that before. It could be that the versions are different. Let me see if it's possible for me to get a CentOS7 set up with CUDA version V11.8.89 and then test with that.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:135,availability,mainten,maintenance-policy,135,"I got a machine to test:. ```. gcloud compute instances create ""${USER}-gpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --maintenance-policy ""TERMINATE"" \. --accelerator=type=nvidia-tesla-p100,count=1 \. --image-family ""centos-7"" \. --image-project ""centos-cloud"" \. --machine-type ""n1-standard-16"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. On the machine, I install nvidia driver first:. ```. sudo yum update -y && sudo yum install -y python3. curl https://raw.githubusercontent.com/GoogleCloudPlatform/compute-gpu-installation/main/linux/install_gpu_driver.py --output install_gpu_driver.py. sudo python3 install_gpu_driver.py. ```. After that, I can confirm that nvidia-smi exists:. ```. [pichuan@pichuan-gpu2 ~]$ nvidia-smi. Thu Mar 16 04:47:54 2023 . +-----------------------------------------------------------------------------+. | NVIDIA-SMI 525.85.12 Driver Version: 525.85.12 CUDA Version: 12.0 |. |-------------------------------+----------------------+----------------------+. | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC |. | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. |. | | | MIG M. |. |===============================+======================+======================|. | 0 Tesla P100-PCIE... Off | 00000000:00:04.0 Off | 0 |. | N/A 34C P0 29W / 250W | 0MiB / 16384MiB | 1% Default |. | | | N/A |. +-------------------------------+----------------------+----------------------+. . +-----------------------------------------------------------------------------+. | Processes: |. | GPU GI CI PID Type Process name GPU Memory |. | ID ID Usage |. |=============================================================================|. | No running processes found |. +-----------------------------------------------------------------------------+. ```. Then I install cuda. This was from: https://developer.nvidia.com/cuda-downloads. ```. curl -O https://developer.download.nvidia.com/compute/cud",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:1927,availability,down,downloads,1927,"ersion: 525.85.12 CUDA Version: 12.0 |. |-------------------------------+----------------------+----------------------+. | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC |. | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. |. | | | MIG M. |. |===============================+======================+======================|. | 0 Tesla P100-PCIE... Off | 00000000:00:04.0 Off | 0 |. | N/A 34C P0 29W / 250W | 0MiB / 16384MiB | 1% Default |. | | | N/A |. +-------------------------------+----------------------+----------------------+. . +-----------------------------------------------------------------------------+. | Processes: |. | GPU GI CI PID Type Process name GPU Memory |. | ID ID Usage |. |=============================================================================|. | No running processes found |. +-----------------------------------------------------------------------------+. ```. Then I install cuda. This was from: https://developer.nvidia.com/cuda-downloads. ```. curl -O https://developer.download.nvidia.com/compute/cuda/12.1.0/local_installers/cuda_12.1.0_530.30.02_linux.run . export TERM=xterm. sudo sh cuda_12.1.0_530.30.02_linux.run. ```. ```. export PATH=/usr/local/cuda-12.1/bin:$PATH. export LD_LIBRARY_PATH=/usr/local/cuda-12.1/lib64:$LD_LIBRARY_PATH. sudo ldconfig. ```. ```. [pichuan@pichuan-gpu2 ~]$ nvcc --version. nvcc: NVIDIA (R) Cuda compiler driver. Copyright (c) 2005-2023 NVIDIA Corporation. Built on Tue_Feb__7_19:32:13_PST_2023. Cuda compilation tools, release 12.1, V12.1.66. Build cuda_12.1.r12.1/compiler.32415258_0. ```. This is not 11.8, but is a newer version. So let's test with it. Install Singularity:. ```. curl -O https://raw.githubusercontent.com/google/deepvariant/r1.5/scripts/install_singularity.sh. sed -i -e 's/apt-get/yum/g' install_singularity.sh. bash -x install_singularity.sh. ```. Check version:. ```. [pichuan@pichuan-gpu2 ~]$ singularity --version. singularity version 3.7.0. ```. The rest is similar to h",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:1969,availability,down,download,1969,"-------------------------------+----------------------+----------------------+. | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC |. | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. |. | | | MIG M. |. |===============================+======================+======================|. | 0 Tesla P100-PCIE... Off | 00000000:00:04.0 Off | 0 |. | N/A 34C P0 29W / 250W | 0MiB / 16384MiB | 1% Default |. | | | N/A |. +-------------------------------+----------------------+----------------------+. . +-----------------------------------------------------------------------------+. | Processes: |. | GPU GI CI PID Type Process name GPU Memory |. | ID ID Usage |. |=============================================================================|. | No running processes found |. +-----------------------------------------------------------------------------+. ```. Then I install cuda. This was from: https://developer.nvidia.com/cuda-downloads. ```. curl -O https://developer.download.nvidia.com/compute/cuda/12.1.0/local_installers/cuda_12.1.0_530.30.02_linux.run . export TERM=xterm. sudo sh cuda_12.1.0_530.30.02_linux.run. ```. ```. export PATH=/usr/local/cuda-12.1/bin:$PATH. export LD_LIBRARY_PATH=/usr/local/cuda-12.1/lib64:$LD_LIBRARY_PATH. sudo ldconfig. ```. ```. [pichuan@pichuan-gpu2 ~]$ nvcc --version. nvcc: NVIDIA (R) Cuda compiler driver. Copyright (c) 2005-2023 NVIDIA Corporation. Built on Tue_Feb__7_19:32:13_PST_2023. Cuda compilation tools, release 12.1, V12.1.66. Build cuda_12.1.r12.1/compiler.32415258_0. ```. This is not 11.8, but is a newer version. So let's test with it. Install Singularity:. ```. curl -O https://raw.githubusercontent.com/google/deepvariant/r1.5/scripts/install_singularity.sh. sed -i -e 's/apt-get/yum/g' install_singularity.sh. bash -x install_singularity.sh. ```. Check version:. ```. [pichuan@pichuan-gpu2 ~]$ singularity --version. singularity version 3.7.0. ```. The rest is similar to https://github.com/google/deepvariant/issu",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:423,deployability,instal,install,423,"I got a machine to test:. ```. gcloud compute instances create ""${USER}-gpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --maintenance-policy ""TERMINATE"" \. --accelerator=type=nvidia-tesla-p100,count=1 \. --image-family ""centos-7"" \. --image-project ""centos-cloud"" \. --machine-type ""n1-standard-16"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. On the machine, I install nvidia driver first:. ```. sudo yum update -y && sudo yum install -y python3. curl https://raw.githubusercontent.com/GoogleCloudPlatform/compute-gpu-installation/main/linux/install_gpu_driver.py --output install_gpu_driver.py. sudo python3 install_gpu_driver.py. ```. After that, I can confirm that nvidia-smi exists:. ```. [pichuan@pichuan-gpu2 ~]$ nvidia-smi. Thu Mar 16 04:47:54 2023 . +-----------------------------------------------------------------------------+. | NVIDIA-SMI 525.85.12 Driver Version: 525.85.12 CUDA Version: 12.0 |. |-------------------------------+----------------------+----------------------+. | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC |. | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. |. | | | MIG M. |. |===============================+======================+======================|. | 0 Tesla P100-PCIE... Off | 00000000:00:04.0 Off | 0 |. | N/A 34C P0 29W / 250W | 0MiB / 16384MiB | 1% Default |. | | | N/A |. +-------------------------------+----------------------+----------------------+. . +-----------------------------------------------------------------------------+. | Processes: |. | GPU GI CI PID Type Process name GPU Memory |. | ID ID Usage |. |=============================================================================|. | No running processes found |. +-----------------------------------------------------------------------------+. ```. Then I install cuda. This was from: https://developer.nvidia.com/cuda-downloads. ```. curl -O https://developer.download.nvidia.com/compute/cud",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:467,deployability,updat,update,467,"I got a machine to test:. ```. gcloud compute instances create ""${USER}-gpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --maintenance-policy ""TERMINATE"" \. --accelerator=type=nvidia-tesla-p100,count=1 \. --image-family ""centos-7"" \. --image-project ""centos-cloud"" \. --machine-type ""n1-standard-16"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. On the machine, I install nvidia driver first:. ```. sudo yum update -y && sudo yum install -y python3. curl https://raw.githubusercontent.com/GoogleCloudPlatform/compute-gpu-installation/main/linux/install_gpu_driver.py --output install_gpu_driver.py. sudo python3 install_gpu_driver.py. ```. After that, I can confirm that nvidia-smi exists:. ```. [pichuan@pichuan-gpu2 ~]$ nvidia-smi. Thu Mar 16 04:47:54 2023 . +-----------------------------------------------------------------------------+. | NVIDIA-SMI 525.85.12 Driver Version: 525.85.12 CUDA Version: 12.0 |. |-------------------------------+----------------------+----------------------+. | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC |. | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. |. | | | MIG M. |. |===============================+======================+======================|. | 0 Tesla P100-PCIE... Off | 00000000:00:04.0 Off | 0 |. | N/A 34C P0 29W / 250W | 0MiB / 16384MiB | 1% Default |. | | | N/A |. +-------------------------------+----------------------+----------------------+. . +-----------------------------------------------------------------------------+. | Processes: |. | GPU GI CI PID Type Process name GPU Memory |. | ID ID Usage |. |=============================================================================|. | No running processes found |. +-----------------------------------------------------------------------------+. ```. Then I install cuda. This was from: https://developer.nvidia.com/cuda-downloads. ```. curl -O https://developer.download.nvidia.com/compute/cud",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:489,deployability,instal,install,489,"I got a machine to test:. ```. gcloud compute instances create ""${USER}-gpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --maintenance-policy ""TERMINATE"" \. --accelerator=type=nvidia-tesla-p100,count=1 \. --image-family ""centos-7"" \. --image-project ""centos-cloud"" \. --machine-type ""n1-standard-16"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. On the machine, I install nvidia driver first:. ```. sudo yum update -y && sudo yum install -y python3. curl https://raw.githubusercontent.com/GoogleCloudPlatform/compute-gpu-installation/main/linux/install_gpu_driver.py --output install_gpu_driver.py. sudo python3 install_gpu_driver.py. ```. After that, I can confirm that nvidia-smi exists:. ```. [pichuan@pichuan-gpu2 ~]$ nvidia-smi. Thu Mar 16 04:47:54 2023 . +-----------------------------------------------------------------------------+. | NVIDIA-SMI 525.85.12 Driver Version: 525.85.12 CUDA Version: 12.0 |. |-------------------------------+----------------------+----------------------+. | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC |. | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. |. | | | MIG M. |. |===============================+======================+======================|. | 0 Tesla P100-PCIE... Off | 00000000:00:04.0 Off | 0 |. | N/A 34C P0 29W / 250W | 0MiB / 16384MiB | 1% Default |. | | | N/A |. +-------------------------------+----------------------+----------------------+. . +-----------------------------------------------------------------------------+. | Processes: |. | GPU GI CI PID Type Process name GPU Memory |. | ID ID Usage |. |=============================================================================|. | No running processes found |. +-----------------------------------------------------------------------------+. ```. Then I install cuda. This was from: https://developer.nvidia.com/cuda-downloads. ```. curl -O https://developer.download.nvidia.com/compute/cud",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:580,deployability,instal,installation,580,"I got a machine to test:. ```. gcloud compute instances create ""${USER}-gpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --maintenance-policy ""TERMINATE"" \. --accelerator=type=nvidia-tesla-p100,count=1 \. --image-family ""centos-7"" \. --image-project ""centos-cloud"" \. --machine-type ""n1-standard-16"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. On the machine, I install nvidia driver first:. ```. sudo yum update -y && sudo yum install -y python3. curl https://raw.githubusercontent.com/GoogleCloudPlatform/compute-gpu-installation/main/linux/install_gpu_driver.py --output install_gpu_driver.py. sudo python3 install_gpu_driver.py. ```. After that, I can confirm that nvidia-smi exists:. ```. [pichuan@pichuan-gpu2 ~]$ nvidia-smi. Thu Mar 16 04:47:54 2023 . +-----------------------------------------------------------------------------+. | NVIDIA-SMI 525.85.12 Driver Version: 525.85.12 CUDA Version: 12.0 |. |-------------------------------+----------------------+----------------------+. | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC |. | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. |. | | | MIG M. |. |===============================+======================+======================|. | 0 Tesla P100-PCIE... Off | 00000000:00:04.0 Off | 0 |. | N/A 34C P0 29W / 250W | 0MiB / 16384MiB | 1% Default |. | | | N/A |. +-------------------------------+----------------------+----------------------+. . +-----------------------------------------------------------------------------+. | Processes: |. | GPU GI CI PID Type Process name GPU Memory |. | ID ID Usage |. |=============================================================================|. | No running processes found |. +-----------------------------------------------------------------------------+. ```. Then I install cuda. This was from: https://developer.nvidia.com/cuda-downloads. ```. curl -O https://developer.download.nvidia.com/compute/cud",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:931,deployability,Version,Version,931,"I got a machine to test:. ```. gcloud compute instances create ""${USER}-gpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --maintenance-policy ""TERMINATE"" \. --accelerator=type=nvidia-tesla-p100,count=1 \. --image-family ""centos-7"" \. --image-project ""centos-cloud"" \. --machine-type ""n1-standard-16"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. On the machine, I install nvidia driver first:. ```. sudo yum update -y && sudo yum install -y python3. curl https://raw.githubusercontent.com/GoogleCloudPlatform/compute-gpu-installation/main/linux/install_gpu_driver.py --output install_gpu_driver.py. sudo python3 install_gpu_driver.py. ```. After that, I can confirm that nvidia-smi exists:. ```. [pichuan@pichuan-gpu2 ~]$ nvidia-smi. Thu Mar 16 04:47:54 2023 . +-----------------------------------------------------------------------------+. | NVIDIA-SMI 525.85.12 Driver Version: 525.85.12 CUDA Version: 12.0 |. |-------------------------------+----------------------+----------------------+. | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC |. | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. |. | | | MIG M. |. |===============================+======================+======================|. | 0 Tesla P100-PCIE... Off | 00000000:00:04.0 Off | 0 |. | N/A 34C P0 29W / 250W | 0MiB / 16384MiB | 1% Default |. | | | N/A |. +-------------------------------+----------------------+----------------------+. . +-----------------------------------------------------------------------------+. | Processes: |. | GPU GI CI PID Type Process name GPU Memory |. | ID ID Usage |. |=============================================================================|. | No running processes found |. +-----------------------------------------------------------------------------+. ```. Then I install cuda. This was from: https://developer.nvidia.com/cuda-downloads. ```. curl -O https://developer.download.nvidia.com/compute/cud",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:955,deployability,Version,Version,955,"I got a machine to test:. ```. gcloud compute instances create ""${USER}-gpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --maintenance-policy ""TERMINATE"" \. --accelerator=type=nvidia-tesla-p100,count=1 \. --image-family ""centos-7"" \. --image-project ""centos-cloud"" \. --machine-type ""n1-standard-16"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. On the machine, I install nvidia driver first:. ```. sudo yum update -y && sudo yum install -y python3. curl https://raw.githubusercontent.com/GoogleCloudPlatform/compute-gpu-installation/main/linux/install_gpu_driver.py --output install_gpu_driver.py. sudo python3 install_gpu_driver.py. ```. After that, I can confirm that nvidia-smi exists:. ```. [pichuan@pichuan-gpu2 ~]$ nvidia-smi. Thu Mar 16 04:47:54 2023 . +-----------------------------------------------------------------------------+. | NVIDIA-SMI 525.85.12 Driver Version: 525.85.12 CUDA Version: 12.0 |. |-------------------------------+----------------------+----------------------+. | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC |. | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. |. | | | MIG M. |. |===============================+======================+======================|. | 0 Tesla P100-PCIE... Off | 00000000:00:04.0 Off | 0 |. | N/A 34C P0 29W / 250W | 0MiB / 16384MiB | 1% Default |. | | | N/A |. +-------------------------------+----------------------+----------------------+. . +-----------------------------------------------------------------------------+. | Processes: |. | GPU GI CI PID Type Process name GPU Memory |. | ID ID Usage |. |=============================================================================|. | No running processes found |. +-----------------------------------------------------------------------------+. ```. Then I install cuda. This was from: https://developer.nvidia.com/cuda-downloads. ```. curl -O https://developer.download.nvidia.com/compute/cud",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:1864,deployability,instal,install,1864,"------------------------------+. | NVIDIA-SMI 525.85.12 Driver Version: 525.85.12 CUDA Version: 12.0 |. |-------------------------------+----------------------+----------------------+. | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC |. | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. |. | | | MIG M. |. |===============================+======================+======================|. | 0 Tesla P100-PCIE... Off | 00000000:00:04.0 Off | 0 |. | N/A 34C P0 29W / 250W | 0MiB / 16384MiB | 1% Default |. | | | N/A |. +-------------------------------+----------------------+----------------------+. . +-----------------------------------------------------------------------------+. | Processes: |. | GPU GI CI PID Type Process name GPU Memory |. | ID ID Usage |. |=============================================================================|. | No running processes found |. +-----------------------------------------------------------------------------+. ```. Then I install cuda. This was from: https://developer.nvidia.com/cuda-downloads. ```. curl -O https://developer.download.nvidia.com/compute/cuda/12.1.0/local_installers/cuda_12.1.0_530.30.02_linux.run . export TERM=xterm. sudo sh cuda_12.1.0_530.30.02_linux.run. ```. ```. export PATH=/usr/local/cuda-12.1/bin:$PATH. export LD_LIBRARY_PATH=/usr/local/cuda-12.1/lib64:$LD_LIBRARY_PATH. sudo ldconfig. ```. ```. [pichuan@pichuan-gpu2 ~]$ nvcc --version. nvcc: NVIDIA (R) Cuda compiler driver. Copyright (c) 2005-2023 NVIDIA Corporation. Built on Tue_Feb__7_19:32:13_PST_2023. Cuda compilation tools, release 12.1, V12.1.66. Build cuda_12.1.r12.1/compiler.32415258_0. ```. This is not 11.8, but is a newer version. So let's test with it. Install Singularity:. ```. curl -O https://raw.githubusercontent.com/google/deepvariant/r1.5/scripts/install_singularity.sh. sed -i -e 's/apt-get/yum/g' install_singularity.sh. bash -x install_singularity.sh. ```. Check version:. ```. [pichuan@pichuan-gpu2 ~]$ singularity --v",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:2300,deployability,version,version,2300,"... Off | 00000000:00:04.0 Off | 0 |. | N/A 34C P0 29W / 250W | 0MiB / 16384MiB | 1% Default |. | | | N/A |. +-------------------------------+----------------------+----------------------+. . +-----------------------------------------------------------------------------+. | Processes: |. | GPU GI CI PID Type Process name GPU Memory |. | ID ID Usage |. |=============================================================================|. | No running processes found |. +-----------------------------------------------------------------------------+. ```. Then I install cuda. This was from: https://developer.nvidia.com/cuda-downloads. ```. curl -O https://developer.download.nvidia.com/compute/cuda/12.1.0/local_installers/cuda_12.1.0_530.30.02_linux.run . export TERM=xterm. sudo sh cuda_12.1.0_530.30.02_linux.run. ```. ```. export PATH=/usr/local/cuda-12.1/bin:$PATH. export LD_LIBRARY_PATH=/usr/local/cuda-12.1/lib64:$LD_LIBRARY_PATH. sudo ldconfig. ```. ```. [pichuan@pichuan-gpu2 ~]$ nvcc --version. nvcc: NVIDIA (R) Cuda compiler driver. Copyright (c) 2005-2023 NVIDIA Corporation. Built on Tue_Feb__7_19:32:13_PST_2023. Cuda compilation tools, release 12.1, V12.1.66. Build cuda_12.1.r12.1/compiler.32415258_0. ```. This is not 11.8, but is a newer version. So let's test with it. Install Singularity:. ```. curl -O https://raw.githubusercontent.com/google/deepvariant/r1.5/scripts/install_singularity.sh. sed -i -e 's/apt-get/yum/g' install_singularity.sh. bash -x install_singularity.sh. ```. Check version:. ```. [pichuan@pichuan-gpu2 ~]$ singularity --version. singularity version 3.7.0. ```. The rest is similar to https://github.com/google/deepvariant/issues/514#issuecomment-1035630725 , but with v1.5.0. ```. # Pull the image. BIN_VERSION=1.5.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run DeepVariant. # Using ""--nv"" and ""${BIN_VERSION}-gpu"" is important. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VER",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:2455,deployability,releas,release,2455,"---------+----------------------+. . +-----------------------------------------------------------------------------+. | Processes: |. | GPU GI CI PID Type Process name GPU Memory |. | ID ID Usage |. |=============================================================================|. | No running processes found |. +-----------------------------------------------------------------------------+. ```. Then I install cuda. This was from: https://developer.nvidia.com/cuda-downloads. ```. curl -O https://developer.download.nvidia.com/compute/cuda/12.1.0/local_installers/cuda_12.1.0_530.30.02_linux.run . export TERM=xterm. sudo sh cuda_12.1.0_530.30.02_linux.run. ```. ```. export PATH=/usr/local/cuda-12.1/bin:$PATH. export LD_LIBRARY_PATH=/usr/local/cuda-12.1/lib64:$LD_LIBRARY_PATH. sudo ldconfig. ```. ```. [pichuan@pichuan-gpu2 ~]$ nvcc --version. nvcc: NVIDIA (R) Cuda compiler driver. Copyright (c) 2005-2023 NVIDIA Corporation. Built on Tue_Feb__7_19:32:13_PST_2023. Cuda compilation tools, release 12.1, V12.1.66. Build cuda_12.1.r12.1/compiler.32415258_0. ```. This is not 11.8, but is a newer version. So let's test with it. Install Singularity:. ```. curl -O https://raw.githubusercontent.com/google/deepvariant/r1.5/scripts/install_singularity.sh. sed -i -e 's/apt-get/yum/g' install_singularity.sh. bash -x install_singularity.sh. ```. Check version:. ```. [pichuan@pichuan-gpu2 ~]$ singularity --version. singularity version 3.7.0. ```. The rest is similar to https://github.com/google/deepvariant/issues/514#issuecomment-1035630725 , but with v1.5.0. ```. # Pull the image. BIN_VERSION=1.5.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run DeepVariant. # Using ""--nv"" and ""${BIN_VERSION}-gpu"" is important. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA1",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:2479,deployability,Build,Build,2479,"---------+. . +-----------------------------------------------------------------------------+. | Processes: |. | GPU GI CI PID Type Process name GPU Memory |. | ID ID Usage |. |=============================================================================|. | No running processes found |. +-----------------------------------------------------------------------------+. ```. Then I install cuda. This was from: https://developer.nvidia.com/cuda-downloads. ```. curl -O https://developer.download.nvidia.com/compute/cuda/12.1.0/local_installers/cuda_12.1.0_530.30.02_linux.run . export TERM=xterm. sudo sh cuda_12.1.0_530.30.02_linux.run. ```. ```. export PATH=/usr/local/cuda-12.1/bin:$PATH. export LD_LIBRARY_PATH=/usr/local/cuda-12.1/lib64:$LD_LIBRARY_PATH. sudo ldconfig. ```. ```. [pichuan@pichuan-gpu2 ~]$ nvcc --version. nvcc: NVIDIA (R) Cuda compiler driver. Copyright (c) 2005-2023 NVIDIA Corporation. Built on Tue_Feb__7_19:32:13_PST_2023. Cuda compilation tools, release 12.1, V12.1.66. Build cuda_12.1.r12.1/compiler.32415258_0. ```. This is not 11.8, but is a newer version. So let's test with it. Install Singularity:. ```. curl -O https://raw.githubusercontent.com/google/deepvariant/r1.5/scripts/install_singularity.sh. sed -i -e 's/apt-get/yum/g' install_singularity.sh. bash -x install_singularity.sh. ```. Check version:. ```. [pichuan@pichuan-gpu2 ~]$ singularity --version. singularity version 3.7.0. ```. The rest is similar to https://github.com/google/deepvariant/issues/514#issuecomment-1035630725 , but with v1.5.0. ```. # Pull the image. BIN_VERSION=1.5.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run DeepVariant. # Using ""--nv"" and ""${BIN_VERSION}-gpu"" is important. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:2560,deployability,version,version,2560,"----------+. | Processes: |. | GPU GI CI PID Type Process name GPU Memory |. | ID ID Usage |. |=============================================================================|. | No running processes found |. +-----------------------------------------------------------------------------+. ```. Then I install cuda. This was from: https://developer.nvidia.com/cuda-downloads. ```. curl -O https://developer.download.nvidia.com/compute/cuda/12.1.0/local_installers/cuda_12.1.0_530.30.02_linux.run . export TERM=xterm. sudo sh cuda_12.1.0_530.30.02_linux.run. ```. ```. export PATH=/usr/local/cuda-12.1/bin:$PATH. export LD_LIBRARY_PATH=/usr/local/cuda-12.1/lib64:$LD_LIBRARY_PATH. sudo ldconfig. ```. ```. [pichuan@pichuan-gpu2 ~]$ nvcc --version. nvcc: NVIDIA (R) Cuda compiler driver. Copyright (c) 2005-2023 NVIDIA Corporation. Built on Tue_Feb__7_19:32:13_PST_2023. Cuda compilation tools, release 12.1, V12.1.66. Build cuda_12.1.r12.1/compiler.32415258_0. ```. This is not 11.8, but is a newer version. So let's test with it. Install Singularity:. ```. curl -O https://raw.githubusercontent.com/google/deepvariant/r1.5/scripts/install_singularity.sh. sed -i -e 's/apt-get/yum/g' install_singularity.sh. bash -x install_singularity.sh. ```. Check version:. ```. [pichuan@pichuan-gpu2 ~]$ singularity --version. singularity version 3.7.0. ```. The rest is similar to https://github.com/google/deepvariant/issues/514#issuecomment-1035630725 , but with v1.5.0. ```. # Pull the image. BIN_VERSION=1.5.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run DeepVariant. # Using ""--nv"" and ""${BIN_VERSION}-gpu"" is important. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/ou",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:2592,deployability,Instal,Install,2592,"PU GI CI PID Type Process name GPU Memory |. | ID ID Usage |. |=============================================================================|. | No running processes found |. +-----------------------------------------------------------------------------+. ```. Then I install cuda. This was from: https://developer.nvidia.com/cuda-downloads. ```. curl -O https://developer.download.nvidia.com/compute/cuda/12.1.0/local_installers/cuda_12.1.0_530.30.02_linux.run . export TERM=xterm. sudo sh cuda_12.1.0_530.30.02_linux.run. ```. ```. export PATH=/usr/local/cuda-12.1/bin:$PATH. export LD_LIBRARY_PATH=/usr/local/cuda-12.1/lib64:$LD_LIBRARY_PATH. sudo ldconfig. ```. ```. [pichuan@pichuan-gpu2 ~]$ nvcc --version. nvcc: NVIDIA (R) Cuda compiler driver. Copyright (c) 2005-2023 NVIDIA Corporation. Built on Tue_Feb__7_19:32:13_PST_2023. Cuda compilation tools, release 12.1, V12.1.66. Build cuda_12.1.r12.1/compiler.32415258_0. ```. This is not 11.8, but is a newer version. So let's test with it. Install Singularity:. ```. curl -O https://raw.githubusercontent.com/google/deepvariant/r1.5/scripts/install_singularity.sh. sed -i -e 's/apt-get/yum/g' install_singularity.sh. bash -x install_singularity.sh. ```. Check version:. ```. [pichuan@pichuan-gpu2 ~]$ singularity --version. singularity version 3.7.0. ```. The rest is similar to https://github.com/google/deepvariant/issues/514#issuecomment-1035630725 , but with v1.5.0. ```. # Pull the image. BIN_VERSION=1.5.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run DeepVariant. # Using ""--nv"" and ""${BIN_VERSION}-gpu"" is important. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:2812,deployability,version,version,2812,"---------------------------------+. ```. Then I install cuda. This was from: https://developer.nvidia.com/cuda-downloads. ```. curl -O https://developer.download.nvidia.com/compute/cuda/12.1.0/local_installers/cuda_12.1.0_530.30.02_linux.run . export TERM=xterm. sudo sh cuda_12.1.0_530.30.02_linux.run. ```. ```. export PATH=/usr/local/cuda-12.1/bin:$PATH. export LD_LIBRARY_PATH=/usr/local/cuda-12.1/lib64:$LD_LIBRARY_PATH. sudo ldconfig. ```. ```. [pichuan@pichuan-gpu2 ~]$ nvcc --version. nvcc: NVIDIA (R) Cuda compiler driver. Copyright (c) 2005-2023 NVIDIA Corporation. Built on Tue_Feb__7_19:32:13_PST_2023. Cuda compilation tools, release 12.1, V12.1.66. Build cuda_12.1.r12.1/compiler.32415258_0. ```. This is not 11.8, but is a newer version. So let's test with it. Install Singularity:. ```. curl -O https://raw.githubusercontent.com/google/deepvariant/r1.5/scripts/install_singularity.sh. sed -i -e 's/apt-get/yum/g' install_singularity.sh. bash -x install_singularity.sh. ```. Check version:. ```. [pichuan@pichuan-gpu2 ~]$ singularity --version. singularity version 3.7.0. ```. The rest is similar to https://github.com/google/deepvariant/issues/514#issuecomment-1035630725 , but with v1.5.0. ```. # Pull the image. BIN_VERSION=1.5.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run DeepVariant. # Using ""--nv"" and ""${BIN_VERSION}-gpu"" is important. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=$(nproc). ```. That worked. I can use this setup to try the way you run it as well. I'll report back lat",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:2867,deployability,version,version,2867,"------------------------------+. ```. Then I install cuda. This was from: https://developer.nvidia.com/cuda-downloads. ```. curl -O https://developer.download.nvidia.com/compute/cuda/12.1.0/local_installers/cuda_12.1.0_530.30.02_linux.run . export TERM=xterm. sudo sh cuda_12.1.0_530.30.02_linux.run. ```. ```. export PATH=/usr/local/cuda-12.1/bin:$PATH. export LD_LIBRARY_PATH=/usr/local/cuda-12.1/lib64:$LD_LIBRARY_PATH. sudo ldconfig. ```. ```. [pichuan@pichuan-gpu2 ~]$ nvcc --version. nvcc: NVIDIA (R) Cuda compiler driver. Copyright (c) 2005-2023 NVIDIA Corporation. Built on Tue_Feb__7_19:32:13_PST_2023. Cuda compilation tools, release 12.1, V12.1.66. Build cuda_12.1.r12.1/compiler.32415258_0. ```. This is not 11.8, but is a newer version. So let's test with it. Install Singularity:. ```. curl -O https://raw.githubusercontent.com/google/deepvariant/r1.5/scripts/install_singularity.sh. sed -i -e 's/apt-get/yum/g' install_singularity.sh. bash -x install_singularity.sh. ```. Check version:. ```. [pichuan@pichuan-gpu2 ~]$ singularity --version. singularity version 3.7.0. ```. The rest is similar to https://github.com/google/deepvariant/issues/514#issuecomment-1035630725 , but with v1.5.0. ```. # Pull the image. BIN_VERSION=1.5.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run DeepVariant. # Using ""--nv"" and ""${BIN_VERSION}-gpu"" is important. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=$(nproc). ```. That worked. I can use this setup to try the way you run it as well. I'll report back later.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:2888,deployability,version,version,2888,"------------------------------+. ```. Then I install cuda. This was from: https://developer.nvidia.com/cuda-downloads. ```. curl -O https://developer.download.nvidia.com/compute/cuda/12.1.0/local_installers/cuda_12.1.0_530.30.02_linux.run . export TERM=xterm. sudo sh cuda_12.1.0_530.30.02_linux.run. ```. ```. export PATH=/usr/local/cuda-12.1/bin:$PATH. export LD_LIBRARY_PATH=/usr/local/cuda-12.1/lib64:$LD_LIBRARY_PATH. sudo ldconfig. ```. ```. [pichuan@pichuan-gpu2 ~]$ nvcc --version. nvcc: NVIDIA (R) Cuda compiler driver. Copyright (c) 2005-2023 NVIDIA Corporation. Built on Tue_Feb__7_19:32:13_PST_2023. Cuda compilation tools, release 12.1, V12.1.66. Build cuda_12.1.r12.1/compiler.32415258_0. ```. This is not 11.8, but is a newer version. So let's test with it. Install Singularity:. ```. curl -O https://raw.githubusercontent.com/google/deepvariant/r1.5/scripts/install_singularity.sh. sed -i -e 's/apt-get/yum/g' install_singularity.sh. bash -x install_singularity.sh. ```. Check version:. ```. [pichuan@pichuan-gpu2 ~]$ singularity --version. singularity version 3.7.0. ```. The rest is similar to https://github.com/google/deepvariant/issues/514#issuecomment-1035630725 , but with v1.5.0. ```. # Pull the image. BIN_VERSION=1.5.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run DeepVariant. # Using ""--nv"" and ""${BIN_VERSION}-gpu"" is important. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=$(nproc). ```. That worked. I can use this setup to try the way you run it as well. I'll report back later.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:72,energy efficiency,gpu,gpu,72,"I got a machine to test:. ```. gcloud compute instances create ""${USER}-gpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --maintenance-policy ""TERMINATE"" \. --accelerator=type=nvidia-tesla-p100,count=1 \. --image-family ""centos-7"" \. --image-project ""centos-cloud"" \. --machine-type ""n1-standard-16"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. On the machine, I install nvidia driver first:. ```. sudo yum update -y && sudo yum install -y python3. curl https://raw.githubusercontent.com/GoogleCloudPlatform/compute-gpu-installation/main/linux/install_gpu_driver.py --output install_gpu_driver.py. sudo python3 install_gpu_driver.py. ```. After that, I can confirm that nvidia-smi exists:. ```. [pichuan@pichuan-gpu2 ~]$ nvidia-smi. Thu Mar 16 04:47:54 2023 . +-----------------------------------------------------------------------------+. | NVIDIA-SMI 525.85.12 Driver Version: 525.85.12 CUDA Version: 12.0 |. |-------------------------------+----------------------+----------------------+. | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC |. | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. |. | | | MIG M. |. |===============================+======================+======================|. | 0 Tesla P100-PCIE... Off | 00000000:00:04.0 Off | 0 |. | N/A 34C P0 29W / 250W | 0MiB / 16384MiB | 1% Default |. | | | N/A |. +-------------------------------+----------------------+----------------------+. . +-----------------------------------------------------------------------------+. | Processes: |. | GPU GI CI PID Type Process name GPU Memory |. | ID ID Usage |. |=============================================================================|. | No running processes found |. +-----------------------------------------------------------------------------+. ```. Then I install cuda. This was from: https://developer.nvidia.com/cuda-downloads. ```. curl -O https://developer.download.nvidia.com/compute/cud",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:114,energy efficiency,cloud,cloud-platform,114,"I got a machine to test:. ```. gcloud compute instances create ""${USER}-gpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --maintenance-policy ""TERMINATE"" \. --accelerator=type=nvidia-tesla-p100,count=1 \. --image-family ""centos-7"" \. --image-project ""centos-cloud"" \. --machine-type ""n1-standard-16"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. On the machine, I install nvidia driver first:. ```. sudo yum update -y && sudo yum install -y python3. curl https://raw.githubusercontent.com/GoogleCloudPlatform/compute-gpu-installation/main/linux/install_gpu_driver.py --output install_gpu_driver.py. sudo python3 install_gpu_driver.py. ```. After that, I can confirm that nvidia-smi exists:. ```. [pichuan@pichuan-gpu2 ~]$ nvidia-smi. Thu Mar 16 04:47:54 2023 . +-----------------------------------------------------------------------------+. | NVIDIA-SMI 525.85.12 Driver Version: 525.85.12 CUDA Version: 12.0 |. |-------------------------------+----------------------+----------------------+. | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC |. | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. |. | | | MIG M. |. |===============================+======================+======================|. | 0 Tesla P100-PCIE... Off | 00000000:00:04.0 Off | 0 |. | N/A 34C P0 29W / 250W | 0MiB / 16384MiB | 1% Default |. | | | N/A |. +-------------------------------+----------------------+----------------------+. . +-----------------------------------------------------------------------------+. | Processes: |. | GPU GI CI PID Type Process name GPU Memory |. | ID ID Usage |. |=============================================================================|. | No running processes found |. +-----------------------------------------------------------------------------+. ```. Then I install cuda. This was from: https://developer.nvidia.com/cuda-downloads. ```. curl -O https://developer.download.nvidia.com/compute/cud",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:270,energy efficiency,cloud,cloud,270,"I got a machine to test:. ```. gcloud compute instances create ""${USER}-gpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --maintenance-policy ""TERMINATE"" \. --accelerator=type=nvidia-tesla-p100,count=1 \. --image-family ""centos-7"" \. --image-project ""centos-cloud"" \. --machine-type ""n1-standard-16"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. On the machine, I install nvidia driver first:. ```. sudo yum update -y && sudo yum install -y python3. curl https://raw.githubusercontent.com/GoogleCloudPlatform/compute-gpu-installation/main/linux/install_gpu_driver.py --output install_gpu_driver.py. sudo python3 install_gpu_driver.py. ```. After that, I can confirm that nvidia-smi exists:. ```. [pichuan@pichuan-gpu2 ~]$ nvidia-smi. Thu Mar 16 04:47:54 2023 . +-----------------------------------------------------------------------------+. | NVIDIA-SMI 525.85.12 Driver Version: 525.85.12 CUDA Version: 12.0 |. |-------------------------------+----------------------+----------------------+. | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC |. | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. |. | | | MIG M. |. |===============================+======================+======================|. | 0 Tesla P100-PCIE... Off | 00000000:00:04.0 Off | 0 |. | N/A 34C P0 29W / 250W | 0MiB / 16384MiB | 1% Default |. | | | N/A |. +-------------------------------+----------------------+----------------------+. . +-----------------------------------------------------------------------------+. | Processes: |. | GPU GI CI PID Type Process name GPU Memory |. | ID ID Usage |. |=============================================================================|. | No running processes found |. +-----------------------------------------------------------------------------+. ```. Then I install cuda. This was from: https://developer.nvidia.com/cuda-downloads. ```. curl -O https://developer.download.nvidia.com/compute/cud",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:370,energy efficiency,cpu,cpu-platform,370,"I got a machine to test:. ```. gcloud compute instances create ""${USER}-gpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --maintenance-policy ""TERMINATE"" \. --accelerator=type=nvidia-tesla-p100,count=1 \. --image-family ""centos-7"" \. --image-project ""centos-cloud"" \. --machine-type ""n1-standard-16"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. On the machine, I install nvidia driver first:. ```. sudo yum update -y && sudo yum install -y python3. curl https://raw.githubusercontent.com/GoogleCloudPlatform/compute-gpu-installation/main/linux/install_gpu_driver.py --output install_gpu_driver.py. sudo python3 install_gpu_driver.py. ```. After that, I can confirm that nvidia-smi exists:. ```. [pichuan@pichuan-gpu2 ~]$ nvidia-smi. Thu Mar 16 04:47:54 2023 . +-----------------------------------------------------------------------------+. | NVIDIA-SMI 525.85.12 Driver Version: 525.85.12 CUDA Version: 12.0 |. |-------------------------------+----------------------+----------------------+. | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC |. | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. |. | | | MIG M. |. |===============================+======================+======================|. | 0 Tesla P100-PCIE... Off | 00000000:00:04.0 Off | 0 |. | N/A 34C P0 29W / 250W | 0MiB / 16384MiB | 1% Default |. | | | N/A |. +-------------------------------+----------------------+----------------------+. . +-----------------------------------------------------------------------------+. | Processes: |. | GPU GI CI PID Type Process name GPU Memory |. | ID ID Usage |. |=============================================================================|. | No running processes found |. +-----------------------------------------------------------------------------+. ```. Then I install cuda. This was from: https://developer.nvidia.com/cuda-downloads. ```. curl -O https://developer.download.nvidia.com/compute/cud",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:576,energy efficiency,gpu,gpu-installation,576,"I got a machine to test:. ```. gcloud compute instances create ""${USER}-gpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --maintenance-policy ""TERMINATE"" \. --accelerator=type=nvidia-tesla-p100,count=1 \. --image-family ""centos-7"" \. --image-project ""centos-cloud"" \. --machine-type ""n1-standard-16"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. On the machine, I install nvidia driver first:. ```. sudo yum update -y && sudo yum install -y python3. curl https://raw.githubusercontent.com/GoogleCloudPlatform/compute-gpu-installation/main/linux/install_gpu_driver.py --output install_gpu_driver.py. sudo python3 install_gpu_driver.py. ```. After that, I can confirm that nvidia-smi exists:. ```. [pichuan@pichuan-gpu2 ~]$ nvidia-smi. Thu Mar 16 04:47:54 2023 . +-----------------------------------------------------------------------------+. | NVIDIA-SMI 525.85.12 Driver Version: 525.85.12 CUDA Version: 12.0 |. |-------------------------------+----------------------+----------------------+. | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC |. | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. |. | | | MIG M. |. |===============================+======================+======================|. | 0 Tesla P100-PCIE... Off | 00000000:00:04.0 Off | 0 |. | N/A 34C P0 29W / 250W | 0MiB / 16384MiB | 1% Default |. | | | N/A |. +-------------------------------+----------------------+----------------------+. . +-----------------------------------------------------------------------------+. | Processes: |. | GPU GI CI PID Type Process name GPU Memory |. | ID ID Usage |. |=============================================================================|. | No running processes found |. +-----------------------------------------------------------------------------+. ```. Then I install cuda. This was from: https://developer.nvidia.com/cuda-downloads. ```. curl -O https://developer.download.nvidia.com/compute/cud",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:1055,energy efficiency,GPU,GPU,1055,"reate ""${USER}-gpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --maintenance-policy ""TERMINATE"" \. --accelerator=type=nvidia-tesla-p100,count=1 \. --image-family ""centos-7"" \. --image-project ""centos-cloud"" \. --machine-type ""n1-standard-16"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. On the machine, I install nvidia driver first:. ```. sudo yum update -y && sudo yum install -y python3. curl https://raw.githubusercontent.com/GoogleCloudPlatform/compute-gpu-installation/main/linux/install_gpu_driver.py --output install_gpu_driver.py. sudo python3 install_gpu_driver.py. ```. After that, I can confirm that nvidia-smi exists:. ```. [pichuan@pichuan-gpu2 ~]$ nvidia-smi. Thu Mar 16 04:47:54 2023 . +-----------------------------------------------------------------------------+. | NVIDIA-SMI 525.85.12 Driver Version: 525.85.12 CUDA Version: 12.0 |. |-------------------------------+----------------------+----------------------+. | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC |. | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. |. | | | MIG M. |. |===============================+======================+======================|. | 0 Tesla P100-PCIE... Off | 00000000:00:04.0 Off | 0 |. | N/A 34C P0 29W / 250W | 0MiB / 16384MiB | 1% Default |. | | | N/A |. +-------------------------------+----------------------+----------------------+. . +-----------------------------------------------------------------------------+. | Processes: |. | GPU GI CI PID Type Process name GPU Memory |. | ID ID Usage |. |=============================================================================|. | No running processes found |. +-----------------------------------------------------------------------------+. ```. Then I install cuda. This was from: https://developer.nvidia.com/cuda-downloads. ```. curl -O https://developer.download.nvidia.com/compute/cuda/12.1.0/local_installers/cuda_12.1.0_530.30.02_linux.run",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:1165,energy efficiency,GPU,GPU-Util,1165,"--accelerator=type=nvidia-tesla-p100,count=1 \. --image-family ""centos-7"" \. --image-project ""centos-cloud"" \. --machine-type ""n1-standard-16"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. On the machine, I install nvidia driver first:. ```. sudo yum update -y && sudo yum install -y python3. curl https://raw.githubusercontent.com/GoogleCloudPlatform/compute-gpu-installation/main/linux/install_gpu_driver.py --output install_gpu_driver.py. sudo python3 install_gpu_driver.py. ```. After that, I can confirm that nvidia-smi exists:. ```. [pichuan@pichuan-gpu2 ~]$ nvidia-smi. Thu Mar 16 04:47:54 2023 . +-----------------------------------------------------------------------------+. | NVIDIA-SMI 525.85.12 Driver Version: 525.85.12 CUDA Version: 12.0 |. |-------------------------------+----------------------+----------------------+. | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC |. | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. |. | | | MIG M. |. |===============================+======================+======================|. | 0 Tesla P100-PCIE... Off | 00000000:00:04.0 Off | 0 |. | N/A 34C P0 29W / 250W | 0MiB / 16384MiB | 1% Default |. | | | N/A |. +-------------------------------+----------------------+----------------------+. . +-----------------------------------------------------------------------------+. | Processes: |. | GPU GI CI PID Type Process name GPU Memory |. | ID ID Usage |. |=============================================================================|. | No running processes found |. +-----------------------------------------------------------------------------+. ```. Then I install cuda. This was from: https://developer.nvidia.com/cuda-downloads. ```. curl -O https://developer.download.nvidia.com/compute/cuda/12.1.0/local_installers/cuda_12.1.0_530.30.02_linux.run . export TERM=xterm. sudo sh cuda_12.1.0_530.30.02_linux.run. ```. ```. export PATH=/usr/local/cuda-12.1/bin:$P",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:1595,energy efficiency,GPU,GPU,1595,"/linux/install_gpu_driver.py --output install_gpu_driver.py. sudo python3 install_gpu_driver.py. ```. After that, I can confirm that nvidia-smi exists:. ```. [pichuan@pichuan-gpu2 ~]$ nvidia-smi. Thu Mar 16 04:47:54 2023 . +-----------------------------------------------------------------------------+. | NVIDIA-SMI 525.85.12 Driver Version: 525.85.12 CUDA Version: 12.0 |. |-------------------------------+----------------------+----------------------+. | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC |. | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. |. | | | MIG M. |. |===============================+======================+======================|. | 0 Tesla P100-PCIE... Off | 00000000:00:04.0 Off | 0 |. | N/A 34C P0 29W / 250W | 0MiB / 16384MiB | 1% Default |. | | | N/A |. +-------------------------------+----------------------+----------------------+. . +-----------------------------------------------------------------------------+. | Processes: |. | GPU GI CI PID Type Process name GPU Memory |. | ID ID Usage |. |=============================================================================|. | No running processes found |. +-----------------------------------------------------------------------------+. ```. Then I install cuda. This was from: https://developer.nvidia.com/cuda-downloads. ```. curl -O https://developer.download.nvidia.com/compute/cuda/12.1.0/local_installers/cuda_12.1.0_530.30.02_linux.run . export TERM=xterm. sudo sh cuda_12.1.0_530.30.02_linux.run. ```. ```. export PATH=/usr/local/cuda-12.1/bin:$PATH. export LD_LIBRARY_PATH=/usr/local/cuda-12.1/lib64:$LD_LIBRARY_PATH. sudo ldconfig. ```. ```. [pichuan@pichuan-gpu2 ~]$ nvcc --version. nvcc: NVIDIA (R) Cuda compiler driver. Copyright (c) 2005-2023 NVIDIA Corporation. Built on Tue_Feb__7_19:32:13_PST_2023. Cuda compilation tools, release 12.1, V12.1.66. Build cuda_12.1.r12.1/compiler.32415258_0. ```. This is not 11.8, but is a newer version. So let's test with it. Insta",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:1627,energy efficiency,GPU,GPU,1627,"utput install_gpu_driver.py. sudo python3 install_gpu_driver.py. ```. After that, I can confirm that nvidia-smi exists:. ```. [pichuan@pichuan-gpu2 ~]$ nvidia-smi. Thu Mar 16 04:47:54 2023 . +-----------------------------------------------------------------------------+. | NVIDIA-SMI 525.85.12 Driver Version: 525.85.12 CUDA Version: 12.0 |. |-------------------------------+----------------------+----------------------+. | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC |. | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. |. | | | MIG M. |. |===============================+======================+======================|. | 0 Tesla P100-PCIE... Off | 00000000:00:04.0 Off | 0 |. | N/A 34C P0 29W / 250W | 0MiB / 16384MiB | 1% Default |. | | | N/A |. +-------------------------------+----------------------+----------------------+. . +-----------------------------------------------------------------------------+. | Processes: |. | GPU GI CI PID Type Process name GPU Memory |. | ID ID Usage |. |=============================================================================|. | No running processes found |. +-----------------------------------------------------------------------------+. ```. Then I install cuda. This was from: https://developer.nvidia.com/cuda-downloads. ```. curl -O https://developer.download.nvidia.com/compute/cuda/12.1.0/local_installers/cuda_12.1.0_530.30.02_linux.run . export TERM=xterm. sudo sh cuda_12.1.0_530.30.02_linux.run. ```. ```. export PATH=/usr/local/cuda-12.1/bin:$PATH. export LD_LIBRARY_PATH=/usr/local/cuda-12.1/lib64:$LD_LIBRARY_PATH. sudo ldconfig. ```. ```. [pichuan@pichuan-gpu2 ~]$ nvcc --version. nvcc: NVIDIA (R) Cuda compiler driver. Copyright (c) 2005-2023 NVIDIA Corporation. Built on Tue_Feb__7_19:32:13_PST_2023. Cuda compilation tools, release 12.1, V12.1.66. Build cuda_12.1.r12.1/compiler.32415258_0. ```. This is not 11.8, but is a newer version. So let's test with it. Install Singularity:. ```. curl -O ht",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:3126,energy efficiency,gpu,gpu,3126,"------------------------------+. ```. Then I install cuda. This was from: https://developer.nvidia.com/cuda-downloads. ```. curl -O https://developer.download.nvidia.com/compute/cuda/12.1.0/local_installers/cuda_12.1.0_530.30.02_linux.run . export TERM=xterm. sudo sh cuda_12.1.0_530.30.02_linux.run. ```. ```. export PATH=/usr/local/cuda-12.1/bin:$PATH. export LD_LIBRARY_PATH=/usr/local/cuda-12.1/lib64:$LD_LIBRARY_PATH. sudo ldconfig. ```. ```. [pichuan@pichuan-gpu2 ~]$ nvcc --version. nvcc: NVIDIA (R) Cuda compiler driver. Copyright (c) 2005-2023 NVIDIA Corporation. Built on Tue_Feb__7_19:32:13_PST_2023. Cuda compilation tools, release 12.1, V12.1.66. Build cuda_12.1.r12.1/compiler.32415258_0. ```. This is not 11.8, but is a newer version. So let's test with it. Install Singularity:. ```. curl -O https://raw.githubusercontent.com/google/deepvariant/r1.5/scripts/install_singularity.sh. sed -i -e 's/apt-get/yum/g' install_singularity.sh. bash -x install_singularity.sh. ```. Check version:. ```. [pichuan@pichuan-gpu2 ~]$ singularity --version. singularity version 3.7.0. ```. The rest is similar to https://github.com/google/deepvariant/issues/514#issuecomment-1035630725 , but with v1.5.0. ```. # Pull the image. BIN_VERSION=1.5.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run DeepVariant. # Using ""--nv"" and ""${BIN_VERSION}-gpu"" is important. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=$(nproc). ```. That worked. I can use this setup to try the way you run it as well. I'll report back later.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:3186,energy efficiency,gpu,gpu,3186,"------------------------------+. ```. Then I install cuda. This was from: https://developer.nvidia.com/cuda-downloads. ```. curl -O https://developer.download.nvidia.com/compute/cuda/12.1.0/local_installers/cuda_12.1.0_530.30.02_linux.run . export TERM=xterm. sudo sh cuda_12.1.0_530.30.02_linux.run. ```. ```. export PATH=/usr/local/cuda-12.1/bin:$PATH. export LD_LIBRARY_PATH=/usr/local/cuda-12.1/lib64:$LD_LIBRARY_PATH. sudo ldconfig. ```. ```. [pichuan@pichuan-gpu2 ~]$ nvcc --version. nvcc: NVIDIA (R) Cuda compiler driver. Copyright (c) 2005-2023 NVIDIA Corporation. Built on Tue_Feb__7_19:32:13_PST_2023. Cuda compilation tools, release 12.1, V12.1.66. Build cuda_12.1.r12.1/compiler.32415258_0. ```. This is not 11.8, but is a newer version. So let's test with it. Install Singularity:. ```. curl -O https://raw.githubusercontent.com/google/deepvariant/r1.5/scripts/install_singularity.sh. sed -i -e 's/apt-get/yum/g' install_singularity.sh. bash -x install_singularity.sh. ```. Check version:. ```. [pichuan@pichuan-gpu2 ~]$ singularity --version. singularity version 3.7.0. ```. The rest is similar to https://github.com/google/deepvariant/issues/514#issuecomment-1035630725 , but with v1.5.0. ```. # Pull the image. BIN_VERSION=1.5.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run DeepVariant. # Using ""--nv"" and ""${BIN_VERSION}-gpu"" is important. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=$(nproc). ```. That worked. I can use this setup to try the way you run it as well. I'll report back later.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:3310,energy efficiency,gpu,gpu,3310,"------------------------------+. ```. Then I install cuda. This was from: https://developer.nvidia.com/cuda-downloads. ```. curl -O https://developer.download.nvidia.com/compute/cuda/12.1.0/local_installers/cuda_12.1.0_530.30.02_linux.run . export TERM=xterm. sudo sh cuda_12.1.0_530.30.02_linux.run. ```. ```. export PATH=/usr/local/cuda-12.1/bin:$PATH. export LD_LIBRARY_PATH=/usr/local/cuda-12.1/lib64:$LD_LIBRARY_PATH. sudo ldconfig. ```. ```. [pichuan@pichuan-gpu2 ~]$ nvcc --version. nvcc: NVIDIA (R) Cuda compiler driver. Copyright (c) 2005-2023 NVIDIA Corporation. Built on Tue_Feb__7_19:32:13_PST_2023. Cuda compilation tools, release 12.1, V12.1.66. Build cuda_12.1.r12.1/compiler.32415258_0. ```. This is not 11.8, but is a newer version. So let's test with it. Install Singularity:. ```. curl -O https://raw.githubusercontent.com/google/deepvariant/r1.5/scripts/install_singularity.sh. sed -i -e 's/apt-get/yum/g' install_singularity.sh. bash -x install_singularity.sh. ```. Check version:. ```. [pichuan@pichuan-gpu2 ~]$ singularity --version. singularity version 3.7.0. ```. The rest is similar to https://github.com/google/deepvariant/issues/514#issuecomment-1035630725 , but with v1.5.0. ```. # Pull the image. BIN_VERSION=1.5.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run DeepVariant. # Using ""--nv"" and ""${BIN_VERSION}-gpu"" is important. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=$(nproc). ```. That worked. I can use this setup to try the way you run it as well. I'll report back later.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:931,integrability,Version,Version,931,"I got a machine to test:. ```. gcloud compute instances create ""${USER}-gpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --maintenance-policy ""TERMINATE"" \. --accelerator=type=nvidia-tesla-p100,count=1 \. --image-family ""centos-7"" \. --image-project ""centos-cloud"" \. --machine-type ""n1-standard-16"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. On the machine, I install nvidia driver first:. ```. sudo yum update -y && sudo yum install -y python3. curl https://raw.githubusercontent.com/GoogleCloudPlatform/compute-gpu-installation/main/linux/install_gpu_driver.py --output install_gpu_driver.py. sudo python3 install_gpu_driver.py. ```. After that, I can confirm that nvidia-smi exists:. ```. [pichuan@pichuan-gpu2 ~]$ nvidia-smi. Thu Mar 16 04:47:54 2023 . +-----------------------------------------------------------------------------+. | NVIDIA-SMI 525.85.12 Driver Version: 525.85.12 CUDA Version: 12.0 |. |-------------------------------+----------------------+----------------------+. | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC |. | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. |. | | | MIG M. |. |===============================+======================+======================|. | 0 Tesla P100-PCIE... Off | 00000000:00:04.0 Off | 0 |. | N/A 34C P0 29W / 250W | 0MiB / 16384MiB | 1% Default |. | | | N/A |. +-------------------------------+----------------------+----------------------+. . +-----------------------------------------------------------------------------+. | Processes: |. | GPU GI CI PID Type Process name GPU Memory |. | ID ID Usage |. |=============================================================================|. | No running processes found |. +-----------------------------------------------------------------------------+. ```. Then I install cuda. This was from: https://developer.nvidia.com/cuda-downloads. ```. curl -O https://developer.download.nvidia.com/compute/cud",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:955,integrability,Version,Version,955,"I got a machine to test:. ```. gcloud compute instances create ""${USER}-gpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --maintenance-policy ""TERMINATE"" \. --accelerator=type=nvidia-tesla-p100,count=1 \. --image-family ""centos-7"" \. --image-project ""centos-cloud"" \. --machine-type ""n1-standard-16"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. On the machine, I install nvidia driver first:. ```. sudo yum update -y && sudo yum install -y python3. curl https://raw.githubusercontent.com/GoogleCloudPlatform/compute-gpu-installation/main/linux/install_gpu_driver.py --output install_gpu_driver.py. sudo python3 install_gpu_driver.py. ```. After that, I can confirm that nvidia-smi exists:. ```. [pichuan@pichuan-gpu2 ~]$ nvidia-smi. Thu Mar 16 04:47:54 2023 . +-----------------------------------------------------------------------------+. | NVIDIA-SMI 525.85.12 Driver Version: 525.85.12 CUDA Version: 12.0 |. |-------------------------------+----------------------+----------------------+. | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC |. | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. |. | | | MIG M. |. |===============================+======================+======================|. | 0 Tesla P100-PCIE... Off | 00000000:00:04.0 Off | 0 |. | N/A 34C P0 29W / 250W | 0MiB / 16384MiB | 1% Default |. | | | N/A |. +-------------------------------+----------------------+----------------------+. . +-----------------------------------------------------------------------------+. | Processes: |. | GPU GI CI PID Type Process name GPU Memory |. | ID ID Usage |. |=============================================================================|. | No running processes found |. +-----------------------------------------------------------------------------+. ```. Then I install cuda. This was from: https://developer.nvidia.com/cuda-downloads. ```. curl -O https://developer.download.nvidia.com/compute/cud",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:2300,integrability,version,version,2300,"... Off | 00000000:00:04.0 Off | 0 |. | N/A 34C P0 29W / 250W | 0MiB / 16384MiB | 1% Default |. | | | N/A |. +-------------------------------+----------------------+----------------------+. . +-----------------------------------------------------------------------------+. | Processes: |. | GPU GI CI PID Type Process name GPU Memory |. | ID ID Usage |. |=============================================================================|. | No running processes found |. +-----------------------------------------------------------------------------+. ```. Then I install cuda. This was from: https://developer.nvidia.com/cuda-downloads. ```. curl -O https://developer.download.nvidia.com/compute/cuda/12.1.0/local_installers/cuda_12.1.0_530.30.02_linux.run . export TERM=xterm. sudo sh cuda_12.1.0_530.30.02_linux.run. ```. ```. export PATH=/usr/local/cuda-12.1/bin:$PATH. export LD_LIBRARY_PATH=/usr/local/cuda-12.1/lib64:$LD_LIBRARY_PATH. sudo ldconfig. ```. ```. [pichuan@pichuan-gpu2 ~]$ nvcc --version. nvcc: NVIDIA (R) Cuda compiler driver. Copyright (c) 2005-2023 NVIDIA Corporation. Built on Tue_Feb__7_19:32:13_PST_2023. Cuda compilation tools, release 12.1, V12.1.66. Build cuda_12.1.r12.1/compiler.32415258_0. ```. This is not 11.8, but is a newer version. So let's test with it. Install Singularity:. ```. curl -O https://raw.githubusercontent.com/google/deepvariant/r1.5/scripts/install_singularity.sh. sed -i -e 's/apt-get/yum/g' install_singularity.sh. bash -x install_singularity.sh. ```. Check version:. ```. [pichuan@pichuan-gpu2 ~]$ singularity --version. singularity version 3.7.0. ```. The rest is similar to https://github.com/google/deepvariant/issues/514#issuecomment-1035630725 , but with v1.5.0. ```. # Pull the image. BIN_VERSION=1.5.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run DeepVariant. # Using ""--nv"" and ""${BIN_VERSION}-gpu"" is important. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VER",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:2560,integrability,version,version,2560,"----------+. | Processes: |. | GPU GI CI PID Type Process name GPU Memory |. | ID ID Usage |. |=============================================================================|. | No running processes found |. +-----------------------------------------------------------------------------+. ```. Then I install cuda. This was from: https://developer.nvidia.com/cuda-downloads. ```. curl -O https://developer.download.nvidia.com/compute/cuda/12.1.0/local_installers/cuda_12.1.0_530.30.02_linux.run . export TERM=xterm. sudo sh cuda_12.1.0_530.30.02_linux.run. ```. ```. export PATH=/usr/local/cuda-12.1/bin:$PATH. export LD_LIBRARY_PATH=/usr/local/cuda-12.1/lib64:$LD_LIBRARY_PATH. sudo ldconfig. ```. ```. [pichuan@pichuan-gpu2 ~]$ nvcc --version. nvcc: NVIDIA (R) Cuda compiler driver. Copyright (c) 2005-2023 NVIDIA Corporation. Built on Tue_Feb__7_19:32:13_PST_2023. Cuda compilation tools, release 12.1, V12.1.66. Build cuda_12.1.r12.1/compiler.32415258_0. ```. This is not 11.8, but is a newer version. So let's test with it. Install Singularity:. ```. curl -O https://raw.githubusercontent.com/google/deepvariant/r1.5/scripts/install_singularity.sh. sed -i -e 's/apt-get/yum/g' install_singularity.sh. bash -x install_singularity.sh. ```. Check version:. ```. [pichuan@pichuan-gpu2 ~]$ singularity --version. singularity version 3.7.0. ```. The rest is similar to https://github.com/google/deepvariant/issues/514#issuecomment-1035630725 , but with v1.5.0. ```. # Pull the image. BIN_VERSION=1.5.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run DeepVariant. # Using ""--nv"" and ""${BIN_VERSION}-gpu"" is important. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/ou",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:2812,integrability,version,version,2812,"---------------------------------+. ```. Then I install cuda. This was from: https://developer.nvidia.com/cuda-downloads. ```. curl -O https://developer.download.nvidia.com/compute/cuda/12.1.0/local_installers/cuda_12.1.0_530.30.02_linux.run . export TERM=xterm. sudo sh cuda_12.1.0_530.30.02_linux.run. ```. ```. export PATH=/usr/local/cuda-12.1/bin:$PATH. export LD_LIBRARY_PATH=/usr/local/cuda-12.1/lib64:$LD_LIBRARY_PATH. sudo ldconfig. ```. ```. [pichuan@pichuan-gpu2 ~]$ nvcc --version. nvcc: NVIDIA (R) Cuda compiler driver. Copyright (c) 2005-2023 NVIDIA Corporation. Built on Tue_Feb__7_19:32:13_PST_2023. Cuda compilation tools, release 12.1, V12.1.66. Build cuda_12.1.r12.1/compiler.32415258_0. ```. This is not 11.8, but is a newer version. So let's test with it. Install Singularity:. ```. curl -O https://raw.githubusercontent.com/google/deepvariant/r1.5/scripts/install_singularity.sh. sed -i -e 's/apt-get/yum/g' install_singularity.sh. bash -x install_singularity.sh. ```. Check version:. ```. [pichuan@pichuan-gpu2 ~]$ singularity --version. singularity version 3.7.0. ```. The rest is similar to https://github.com/google/deepvariant/issues/514#issuecomment-1035630725 , but with v1.5.0. ```. # Pull the image. BIN_VERSION=1.5.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run DeepVariant. # Using ""--nv"" and ""${BIN_VERSION}-gpu"" is important. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=$(nproc). ```. That worked. I can use this setup to try the way you run it as well. I'll report back lat",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:2867,integrability,version,version,2867,"------------------------------+. ```. Then I install cuda. This was from: https://developer.nvidia.com/cuda-downloads. ```. curl -O https://developer.download.nvidia.com/compute/cuda/12.1.0/local_installers/cuda_12.1.0_530.30.02_linux.run . export TERM=xterm. sudo sh cuda_12.1.0_530.30.02_linux.run. ```. ```. export PATH=/usr/local/cuda-12.1/bin:$PATH. export LD_LIBRARY_PATH=/usr/local/cuda-12.1/lib64:$LD_LIBRARY_PATH. sudo ldconfig. ```. ```. [pichuan@pichuan-gpu2 ~]$ nvcc --version. nvcc: NVIDIA (R) Cuda compiler driver. Copyright (c) 2005-2023 NVIDIA Corporation. Built on Tue_Feb__7_19:32:13_PST_2023. Cuda compilation tools, release 12.1, V12.1.66. Build cuda_12.1.r12.1/compiler.32415258_0. ```. This is not 11.8, but is a newer version. So let's test with it. Install Singularity:. ```. curl -O https://raw.githubusercontent.com/google/deepvariant/r1.5/scripts/install_singularity.sh. sed -i -e 's/apt-get/yum/g' install_singularity.sh. bash -x install_singularity.sh. ```. Check version:. ```. [pichuan@pichuan-gpu2 ~]$ singularity --version. singularity version 3.7.0. ```. The rest is similar to https://github.com/google/deepvariant/issues/514#issuecomment-1035630725 , but with v1.5.0. ```. # Pull the image. BIN_VERSION=1.5.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run DeepVariant. # Using ""--nv"" and ""${BIN_VERSION}-gpu"" is important. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=$(nproc). ```. That worked. I can use this setup to try the way you run it as well. I'll report back later.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:2888,integrability,version,version,2888,"------------------------------+. ```. Then I install cuda. This was from: https://developer.nvidia.com/cuda-downloads. ```. curl -O https://developer.download.nvidia.com/compute/cuda/12.1.0/local_installers/cuda_12.1.0_530.30.02_linux.run . export TERM=xterm. sudo sh cuda_12.1.0_530.30.02_linux.run. ```. ```. export PATH=/usr/local/cuda-12.1/bin:$PATH. export LD_LIBRARY_PATH=/usr/local/cuda-12.1/lib64:$LD_LIBRARY_PATH. sudo ldconfig. ```. ```. [pichuan@pichuan-gpu2 ~]$ nvcc --version. nvcc: NVIDIA (R) Cuda compiler driver. Copyright (c) 2005-2023 NVIDIA Corporation. Built on Tue_Feb__7_19:32:13_PST_2023. Cuda compilation tools, release 12.1, V12.1.66. Build cuda_12.1.r12.1/compiler.32415258_0. ```. This is not 11.8, but is a newer version. So let's test with it. Install Singularity:. ```. curl -O https://raw.githubusercontent.com/google/deepvariant/r1.5/scripts/install_singularity.sh. sed -i -e 's/apt-get/yum/g' install_singularity.sh. bash -x install_singularity.sh. ```. Check version:. ```. [pichuan@pichuan-gpu2 ~]$ singularity --version. singularity version 3.7.0. ```. The rest is similar to https://github.com/google/deepvariant/issues/514#issuecomment-1035630725 , but with v1.5.0. ```. # Pull the image. BIN_VERSION=1.5.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run DeepVariant. # Using ""--nv"" and ""${BIN_VERSION}-gpu"" is important. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=$(nproc). ```. That worked. I can use this setup to try the way you run it as well. I'll report back later.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:120,interoperability,platform,platform,120,"I got a machine to test:. ```. gcloud compute instances create ""${USER}-gpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --maintenance-policy ""TERMINATE"" \. --accelerator=type=nvidia-tesla-p100,count=1 \. --image-family ""centos-7"" \. --image-project ""centos-cloud"" \. --machine-type ""n1-standard-16"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. On the machine, I install nvidia driver first:. ```. sudo yum update -y && sudo yum install -y python3. curl https://raw.githubusercontent.com/GoogleCloudPlatform/compute-gpu-installation/main/linux/install_gpu_driver.py --output install_gpu_driver.py. sudo python3 install_gpu_driver.py. ```. After that, I can confirm that nvidia-smi exists:. ```. [pichuan@pichuan-gpu2 ~]$ nvidia-smi. Thu Mar 16 04:47:54 2023 . +-----------------------------------------------------------------------------+. | NVIDIA-SMI 525.85.12 Driver Version: 525.85.12 CUDA Version: 12.0 |. |-------------------------------+----------------------+----------------------+. | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC |. | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. |. | | | MIG M. |. |===============================+======================+======================|. | 0 Tesla P100-PCIE... Off | 00000000:00:04.0 Off | 0 |. | N/A 34C P0 29W / 250W | 0MiB / 16384MiB | 1% Default |. | | | N/A |. +-------------------------------+----------------------+----------------------+. . +-----------------------------------------------------------------------------+. | Processes: |. | GPU GI CI PID Type Process name GPU Memory |. | ID ID Usage |. |=============================================================================|. | No running processes found |. +-----------------------------------------------------------------------------+. ```. Then I install cuda. This was from: https://developer.nvidia.com/cuda-downloads. ```. curl -O https://developer.download.nvidia.com/compute/cud",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:299,interoperability,standard,standard-,299,"I got a machine to test:. ```. gcloud compute instances create ""${USER}-gpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --maintenance-policy ""TERMINATE"" \. --accelerator=type=nvidia-tesla-p100,count=1 \. --image-family ""centos-7"" \. --image-project ""centos-cloud"" \. --machine-type ""n1-standard-16"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. On the machine, I install nvidia driver first:. ```. sudo yum update -y && sudo yum install -y python3. curl https://raw.githubusercontent.com/GoogleCloudPlatform/compute-gpu-installation/main/linux/install_gpu_driver.py --output install_gpu_driver.py. sudo python3 install_gpu_driver.py. ```. After that, I can confirm that nvidia-smi exists:. ```. [pichuan@pichuan-gpu2 ~]$ nvidia-smi. Thu Mar 16 04:47:54 2023 . +-----------------------------------------------------------------------------+. | NVIDIA-SMI 525.85.12 Driver Version: 525.85.12 CUDA Version: 12.0 |. |-------------------------------+----------------------+----------------------+. | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC |. | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. |. | | | MIG M. |. |===============================+======================+======================|. | 0 Tesla P100-PCIE... Off | 00000000:00:04.0 Off | 0 |. | N/A 34C P0 29W / 250W | 0MiB / 16384MiB | 1% Default |. | | | N/A |. +-------------------------------+----------------------+----------------------+. . +-----------------------------------------------------------------------------+. | Processes: |. | GPU GI CI PID Type Process name GPU Memory |. | ID ID Usage |. |=============================================================================|. | No running processes found |. +-----------------------------------------------------------------------------+. ```. Then I install cuda. This was from: https://developer.nvidia.com/cuda-downloads. ```. curl -O https://developer.download.nvidia.com/compute/cud",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:374,interoperability,platform,platform,374,"I got a machine to test:. ```. gcloud compute instances create ""${USER}-gpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --maintenance-policy ""TERMINATE"" \. --accelerator=type=nvidia-tesla-p100,count=1 \. --image-family ""centos-7"" \. --image-project ""centos-cloud"" \. --machine-type ""n1-standard-16"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. On the machine, I install nvidia driver first:. ```. sudo yum update -y && sudo yum install -y python3. curl https://raw.githubusercontent.com/GoogleCloudPlatform/compute-gpu-installation/main/linux/install_gpu_driver.py --output install_gpu_driver.py. sudo python3 install_gpu_driver.py. ```. After that, I can confirm that nvidia-smi exists:. ```. [pichuan@pichuan-gpu2 ~]$ nvidia-smi. Thu Mar 16 04:47:54 2023 . +-----------------------------------------------------------------------------+. | NVIDIA-SMI 525.85.12 Driver Version: 525.85.12 CUDA Version: 12.0 |. |-------------------------------+----------------------+----------------------+. | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC |. | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. |. | | | MIG M. |. |===============================+======================+======================|. | 0 Tesla P100-PCIE... Off | 00000000:00:04.0 Off | 0 |. | N/A 34C P0 29W / 250W | 0MiB / 16384MiB | 1% Default |. | | | N/A |. +-------------------------------+----------------------+----------------------+. . +-----------------------------------------------------------------------------+. | Processes: |. | GPU GI CI PID Type Process name GPU Memory |. | ID ID Usage |. |=============================================================================|. | No running processes found |. +-----------------------------------------------------------------------------+. ```. Then I install cuda. This was from: https://developer.nvidia.com/cuda-downloads. ```. curl -O https://developer.download.nvidia.com/compute/cud",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:931,modifiability,Version,Version,931,"I got a machine to test:. ```. gcloud compute instances create ""${USER}-gpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --maintenance-policy ""TERMINATE"" \. --accelerator=type=nvidia-tesla-p100,count=1 \. --image-family ""centos-7"" \. --image-project ""centos-cloud"" \. --machine-type ""n1-standard-16"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. On the machine, I install nvidia driver first:. ```. sudo yum update -y && sudo yum install -y python3. curl https://raw.githubusercontent.com/GoogleCloudPlatform/compute-gpu-installation/main/linux/install_gpu_driver.py --output install_gpu_driver.py. sudo python3 install_gpu_driver.py. ```. After that, I can confirm that nvidia-smi exists:. ```. [pichuan@pichuan-gpu2 ~]$ nvidia-smi. Thu Mar 16 04:47:54 2023 . +-----------------------------------------------------------------------------+. | NVIDIA-SMI 525.85.12 Driver Version: 525.85.12 CUDA Version: 12.0 |. |-------------------------------+----------------------+----------------------+. | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC |. | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. |. | | | MIG M. |. |===============================+======================+======================|. | 0 Tesla P100-PCIE... Off | 00000000:00:04.0 Off | 0 |. | N/A 34C P0 29W / 250W | 0MiB / 16384MiB | 1% Default |. | | | N/A |. +-------------------------------+----------------------+----------------------+. . +-----------------------------------------------------------------------------+. | Processes: |. | GPU GI CI PID Type Process name GPU Memory |. | ID ID Usage |. |=============================================================================|. | No running processes found |. +-----------------------------------------------------------------------------+. ```. Then I install cuda. This was from: https://developer.nvidia.com/cuda-downloads. ```. curl -O https://developer.download.nvidia.com/compute/cud",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:955,modifiability,Version,Version,955,"I got a machine to test:. ```. gcloud compute instances create ""${USER}-gpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --maintenance-policy ""TERMINATE"" \. --accelerator=type=nvidia-tesla-p100,count=1 \. --image-family ""centos-7"" \. --image-project ""centos-cloud"" \. --machine-type ""n1-standard-16"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. On the machine, I install nvidia driver first:. ```. sudo yum update -y && sudo yum install -y python3. curl https://raw.githubusercontent.com/GoogleCloudPlatform/compute-gpu-installation/main/linux/install_gpu_driver.py --output install_gpu_driver.py. sudo python3 install_gpu_driver.py. ```. After that, I can confirm that nvidia-smi exists:. ```. [pichuan@pichuan-gpu2 ~]$ nvidia-smi. Thu Mar 16 04:47:54 2023 . +-----------------------------------------------------------------------------+. | NVIDIA-SMI 525.85.12 Driver Version: 525.85.12 CUDA Version: 12.0 |. |-------------------------------+----------------------+----------------------+. | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC |. | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. |. | | | MIG M. |. |===============================+======================+======================|. | 0 Tesla P100-PCIE... Off | 00000000:00:04.0 Off | 0 |. | N/A 34C P0 29W / 250W | 0MiB / 16384MiB | 1% Default |. | | | N/A |. +-------------------------------+----------------------+----------------------+. . +-----------------------------------------------------------------------------+. | Processes: |. | GPU GI CI PID Type Process name GPU Memory |. | ID ID Usage |. |=============================================================================|. | No running processes found |. +-----------------------------------------------------------------------------+. ```. Then I install cuda. This was from: https://developer.nvidia.com/cuda-downloads. ```. curl -O https://developer.download.nvidia.com/compute/cud",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:2300,modifiability,version,version,2300,"... Off | 00000000:00:04.0 Off | 0 |. | N/A 34C P0 29W / 250W | 0MiB / 16384MiB | 1% Default |. | | | N/A |. +-------------------------------+----------------------+----------------------+. . +-----------------------------------------------------------------------------+. | Processes: |. | GPU GI CI PID Type Process name GPU Memory |. | ID ID Usage |. |=============================================================================|. | No running processes found |. +-----------------------------------------------------------------------------+. ```. Then I install cuda. This was from: https://developer.nvidia.com/cuda-downloads. ```. curl -O https://developer.download.nvidia.com/compute/cuda/12.1.0/local_installers/cuda_12.1.0_530.30.02_linux.run . export TERM=xterm. sudo sh cuda_12.1.0_530.30.02_linux.run. ```. ```. export PATH=/usr/local/cuda-12.1/bin:$PATH. export LD_LIBRARY_PATH=/usr/local/cuda-12.1/lib64:$LD_LIBRARY_PATH. sudo ldconfig. ```. ```. [pichuan@pichuan-gpu2 ~]$ nvcc --version. nvcc: NVIDIA (R) Cuda compiler driver. Copyright (c) 2005-2023 NVIDIA Corporation. Built on Tue_Feb__7_19:32:13_PST_2023. Cuda compilation tools, release 12.1, V12.1.66. Build cuda_12.1.r12.1/compiler.32415258_0. ```. This is not 11.8, but is a newer version. So let's test with it. Install Singularity:. ```. curl -O https://raw.githubusercontent.com/google/deepvariant/r1.5/scripts/install_singularity.sh. sed -i -e 's/apt-get/yum/g' install_singularity.sh. bash -x install_singularity.sh. ```. Check version:. ```. [pichuan@pichuan-gpu2 ~]$ singularity --version. singularity version 3.7.0. ```. The rest is similar to https://github.com/google/deepvariant/issues/514#issuecomment-1035630725 , but with v1.5.0. ```. # Pull the image. BIN_VERSION=1.5.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run DeepVariant. # Using ""--nv"" and ""${BIN_VERSION}-gpu"" is important. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VER",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:2560,modifiability,version,version,2560,"----------+. | Processes: |. | GPU GI CI PID Type Process name GPU Memory |. | ID ID Usage |. |=============================================================================|. | No running processes found |. +-----------------------------------------------------------------------------+. ```. Then I install cuda. This was from: https://developer.nvidia.com/cuda-downloads. ```. curl -O https://developer.download.nvidia.com/compute/cuda/12.1.0/local_installers/cuda_12.1.0_530.30.02_linux.run . export TERM=xterm. sudo sh cuda_12.1.0_530.30.02_linux.run. ```. ```. export PATH=/usr/local/cuda-12.1/bin:$PATH. export LD_LIBRARY_PATH=/usr/local/cuda-12.1/lib64:$LD_LIBRARY_PATH. sudo ldconfig. ```. ```. [pichuan@pichuan-gpu2 ~]$ nvcc --version. nvcc: NVIDIA (R) Cuda compiler driver. Copyright (c) 2005-2023 NVIDIA Corporation. Built on Tue_Feb__7_19:32:13_PST_2023. Cuda compilation tools, release 12.1, V12.1.66. Build cuda_12.1.r12.1/compiler.32415258_0. ```. This is not 11.8, but is a newer version. So let's test with it. Install Singularity:. ```. curl -O https://raw.githubusercontent.com/google/deepvariant/r1.5/scripts/install_singularity.sh. sed -i -e 's/apt-get/yum/g' install_singularity.sh. bash -x install_singularity.sh. ```. Check version:. ```. [pichuan@pichuan-gpu2 ~]$ singularity --version. singularity version 3.7.0. ```. The rest is similar to https://github.com/google/deepvariant/issues/514#issuecomment-1035630725 , but with v1.5.0. ```. # Pull the image. BIN_VERSION=1.5.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run DeepVariant. # Using ""--nv"" and ""${BIN_VERSION}-gpu"" is important. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/ou",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:2812,modifiability,version,version,2812,"---------------------------------+. ```. Then I install cuda. This was from: https://developer.nvidia.com/cuda-downloads. ```. curl -O https://developer.download.nvidia.com/compute/cuda/12.1.0/local_installers/cuda_12.1.0_530.30.02_linux.run . export TERM=xterm. sudo sh cuda_12.1.0_530.30.02_linux.run. ```. ```. export PATH=/usr/local/cuda-12.1/bin:$PATH. export LD_LIBRARY_PATH=/usr/local/cuda-12.1/lib64:$LD_LIBRARY_PATH. sudo ldconfig. ```. ```. [pichuan@pichuan-gpu2 ~]$ nvcc --version. nvcc: NVIDIA (R) Cuda compiler driver. Copyright (c) 2005-2023 NVIDIA Corporation. Built on Tue_Feb__7_19:32:13_PST_2023. Cuda compilation tools, release 12.1, V12.1.66. Build cuda_12.1.r12.1/compiler.32415258_0. ```. This is not 11.8, but is a newer version. So let's test with it. Install Singularity:. ```. curl -O https://raw.githubusercontent.com/google/deepvariant/r1.5/scripts/install_singularity.sh. sed -i -e 's/apt-get/yum/g' install_singularity.sh. bash -x install_singularity.sh. ```. Check version:. ```. [pichuan@pichuan-gpu2 ~]$ singularity --version. singularity version 3.7.0. ```. The rest is similar to https://github.com/google/deepvariant/issues/514#issuecomment-1035630725 , but with v1.5.0. ```. # Pull the image. BIN_VERSION=1.5.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run DeepVariant. # Using ""--nv"" and ""${BIN_VERSION}-gpu"" is important. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=$(nproc). ```. That worked. I can use this setup to try the way you run it as well. I'll report back lat",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:2867,modifiability,version,version,2867,"------------------------------+. ```. Then I install cuda. This was from: https://developer.nvidia.com/cuda-downloads. ```. curl -O https://developer.download.nvidia.com/compute/cuda/12.1.0/local_installers/cuda_12.1.0_530.30.02_linux.run . export TERM=xterm. sudo sh cuda_12.1.0_530.30.02_linux.run. ```. ```. export PATH=/usr/local/cuda-12.1/bin:$PATH. export LD_LIBRARY_PATH=/usr/local/cuda-12.1/lib64:$LD_LIBRARY_PATH. sudo ldconfig. ```. ```. [pichuan@pichuan-gpu2 ~]$ nvcc --version. nvcc: NVIDIA (R) Cuda compiler driver. Copyright (c) 2005-2023 NVIDIA Corporation. Built on Tue_Feb__7_19:32:13_PST_2023. Cuda compilation tools, release 12.1, V12.1.66. Build cuda_12.1.r12.1/compiler.32415258_0. ```. This is not 11.8, but is a newer version. So let's test with it. Install Singularity:. ```. curl -O https://raw.githubusercontent.com/google/deepvariant/r1.5/scripts/install_singularity.sh. sed -i -e 's/apt-get/yum/g' install_singularity.sh. bash -x install_singularity.sh. ```. Check version:. ```. [pichuan@pichuan-gpu2 ~]$ singularity --version. singularity version 3.7.0. ```. The rest is similar to https://github.com/google/deepvariant/issues/514#issuecomment-1035630725 , but with v1.5.0. ```. # Pull the image. BIN_VERSION=1.5.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run DeepVariant. # Using ""--nv"" and ""${BIN_VERSION}-gpu"" is important. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=$(nproc). ```. That worked. I can use this setup to try the way you run it as well. I'll report back later.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:2888,modifiability,version,version,2888,"------------------------------+. ```. Then I install cuda. This was from: https://developer.nvidia.com/cuda-downloads. ```. curl -O https://developer.download.nvidia.com/compute/cuda/12.1.0/local_installers/cuda_12.1.0_530.30.02_linux.run . export TERM=xterm. sudo sh cuda_12.1.0_530.30.02_linux.run. ```. ```. export PATH=/usr/local/cuda-12.1/bin:$PATH. export LD_LIBRARY_PATH=/usr/local/cuda-12.1/lib64:$LD_LIBRARY_PATH. sudo ldconfig. ```. ```. [pichuan@pichuan-gpu2 ~]$ nvcc --version. nvcc: NVIDIA (R) Cuda compiler driver. Copyright (c) 2005-2023 NVIDIA Corporation. Built on Tue_Feb__7_19:32:13_PST_2023. Cuda compilation tools, release 12.1, V12.1.66. Build cuda_12.1.r12.1/compiler.32415258_0. ```. This is not 11.8, but is a newer version. So let's test with it. Install Singularity:. ```. curl -O https://raw.githubusercontent.com/google/deepvariant/r1.5/scripts/install_singularity.sh. sed -i -e 's/apt-get/yum/g' install_singularity.sh. bash -x install_singularity.sh. ```. Check version:. ```. [pichuan@pichuan-gpu2 ~]$ singularity --version. singularity version 3.7.0. ```. The rest is similar to https://github.com/google/deepvariant/issues/514#issuecomment-1035630725 , but with v1.5.0. ```. # Pull the image. BIN_VERSION=1.5.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run DeepVariant. # Using ""--nv"" and ""${BIN_VERSION}-gpu"" is important. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=$(nproc). ```. That worked. I can use this setup to try the way you run it as well. I'll report back later.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:72,performance,gpu,gpu,72,"I got a machine to test:. ```. gcloud compute instances create ""${USER}-gpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --maintenance-policy ""TERMINATE"" \. --accelerator=type=nvidia-tesla-p100,count=1 \. --image-family ""centos-7"" \. --image-project ""centos-cloud"" \. --machine-type ""n1-standard-16"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. On the machine, I install nvidia driver first:. ```. sudo yum update -y && sudo yum install -y python3. curl https://raw.githubusercontent.com/GoogleCloudPlatform/compute-gpu-installation/main/linux/install_gpu_driver.py --output install_gpu_driver.py. sudo python3 install_gpu_driver.py. ```. After that, I can confirm that nvidia-smi exists:. ```. [pichuan@pichuan-gpu2 ~]$ nvidia-smi. Thu Mar 16 04:47:54 2023 . +-----------------------------------------------------------------------------+. | NVIDIA-SMI 525.85.12 Driver Version: 525.85.12 CUDA Version: 12.0 |. |-------------------------------+----------------------+----------------------+. | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC |. | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. |. | | | MIG M. |. |===============================+======================+======================|. | 0 Tesla P100-PCIE... Off | 00000000:00:04.0 Off | 0 |. | N/A 34C P0 29W / 250W | 0MiB / 16384MiB | 1% Default |. | | | N/A |. +-------------------------------+----------------------+----------------------+. . +-----------------------------------------------------------------------------+. | Processes: |. | GPU GI CI PID Type Process name GPU Memory |. | ID ID Usage |. |=============================================================================|. | No running processes found |. +-----------------------------------------------------------------------------+. ```. Then I install cuda. This was from: https://developer.nvidia.com/cuda-downloads. ```. curl -O https://developer.download.nvidia.com/compute/cud",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:322,performance,disk,disk-size,322,"I got a machine to test:. ```. gcloud compute instances create ""${USER}-gpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --maintenance-policy ""TERMINATE"" \. --accelerator=type=nvidia-tesla-p100,count=1 \. --image-family ""centos-7"" \. --image-project ""centos-cloud"" \. --machine-type ""n1-standard-16"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. On the machine, I install nvidia driver first:. ```. sudo yum update -y && sudo yum install -y python3. curl https://raw.githubusercontent.com/GoogleCloudPlatform/compute-gpu-installation/main/linux/install_gpu_driver.py --output install_gpu_driver.py. sudo python3 install_gpu_driver.py. ```. After that, I can confirm that nvidia-smi exists:. ```. [pichuan@pichuan-gpu2 ~]$ nvidia-smi. Thu Mar 16 04:47:54 2023 . +-----------------------------------------------------------------------------+. | NVIDIA-SMI 525.85.12 Driver Version: 525.85.12 CUDA Version: 12.0 |. |-------------------------------+----------------------+----------------------+. | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC |. | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. |. | | | MIG M. |. |===============================+======================+======================|. | 0 Tesla P100-PCIE... Off | 00000000:00:04.0 Off | 0 |. | N/A 34C P0 29W / 250W | 0MiB / 16384MiB | 1% Default |. | | | N/A |. +-------------------------------+----------------------+----------------------+. . +-----------------------------------------------------------------------------+. | Processes: |. | GPU GI CI PID Type Process name GPU Memory |. | ID ID Usage |. |=============================================================================|. | No running processes found |. +-----------------------------------------------------------------------------+. ```. Then I install cuda. This was from: https://developer.nvidia.com/cuda-downloads. ```. curl -O https://developer.download.nvidia.com/compute/cud",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:370,performance,cpu,cpu-platform,370,"I got a machine to test:. ```. gcloud compute instances create ""${USER}-gpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --maintenance-policy ""TERMINATE"" \. --accelerator=type=nvidia-tesla-p100,count=1 \. --image-family ""centos-7"" \. --image-project ""centos-cloud"" \. --machine-type ""n1-standard-16"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. On the machine, I install nvidia driver first:. ```. sudo yum update -y && sudo yum install -y python3. curl https://raw.githubusercontent.com/GoogleCloudPlatform/compute-gpu-installation/main/linux/install_gpu_driver.py --output install_gpu_driver.py. sudo python3 install_gpu_driver.py. ```. After that, I can confirm that nvidia-smi exists:. ```. [pichuan@pichuan-gpu2 ~]$ nvidia-smi. Thu Mar 16 04:47:54 2023 . +-----------------------------------------------------------------------------+. | NVIDIA-SMI 525.85.12 Driver Version: 525.85.12 CUDA Version: 12.0 |. |-------------------------------+----------------------+----------------------+. | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC |. | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. |. | | | MIG M. |. |===============================+======================+======================|. | 0 Tesla P100-PCIE... Off | 00000000:00:04.0 Off | 0 |. | N/A 34C P0 29W / 250W | 0MiB / 16384MiB | 1% Default |. | | | N/A |. +-------------------------------+----------------------+----------------------+. . +-----------------------------------------------------------------------------+. | Processes: |. | GPU GI CI PID Type Process name GPU Memory |. | ID ID Usage |. |=============================================================================|. | No running processes found |. +-----------------------------------------------------------------------------+. ```. Then I install cuda. This was from: https://developer.nvidia.com/cuda-downloads. ```. curl -O https://developer.download.nvidia.com/compute/cud",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:576,performance,gpu,gpu-installation,576,"I got a machine to test:. ```. gcloud compute instances create ""${USER}-gpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --maintenance-policy ""TERMINATE"" \. --accelerator=type=nvidia-tesla-p100,count=1 \. --image-family ""centos-7"" \. --image-project ""centos-cloud"" \. --machine-type ""n1-standard-16"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. On the machine, I install nvidia driver first:. ```. sudo yum update -y && sudo yum install -y python3. curl https://raw.githubusercontent.com/GoogleCloudPlatform/compute-gpu-installation/main/linux/install_gpu_driver.py --output install_gpu_driver.py. sudo python3 install_gpu_driver.py. ```. After that, I can confirm that nvidia-smi exists:. ```. [pichuan@pichuan-gpu2 ~]$ nvidia-smi. Thu Mar 16 04:47:54 2023 . +-----------------------------------------------------------------------------+. | NVIDIA-SMI 525.85.12 Driver Version: 525.85.12 CUDA Version: 12.0 |. |-------------------------------+----------------------+----------------------+. | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC |. | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. |. | | | MIG M. |. |===============================+======================+======================|. | 0 Tesla P100-PCIE... Off | 00000000:00:04.0 Off | 0 |. | N/A 34C P0 29W / 250W | 0MiB / 16384MiB | 1% Default |. | | | N/A |. +-------------------------------+----------------------+----------------------+. . +-----------------------------------------------------------------------------+. | Processes: |. | GPU GI CI PID Type Process name GPU Memory |. | ID ID Usage |. |=============================================================================|. | No running processes found |. +-----------------------------------------------------------------------------+. ```. Then I install cuda. This was from: https://developer.nvidia.com/cuda-downloads. ```. curl -O https://developer.download.nvidia.com/compute/cud",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:1055,performance,GPU,GPU,1055,"reate ""${USER}-gpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --maintenance-policy ""TERMINATE"" \. --accelerator=type=nvidia-tesla-p100,count=1 \. --image-family ""centos-7"" \. --image-project ""centos-cloud"" \. --machine-type ""n1-standard-16"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. On the machine, I install nvidia driver first:. ```. sudo yum update -y && sudo yum install -y python3. curl https://raw.githubusercontent.com/GoogleCloudPlatform/compute-gpu-installation/main/linux/install_gpu_driver.py --output install_gpu_driver.py. sudo python3 install_gpu_driver.py. ```. After that, I can confirm that nvidia-smi exists:. ```. [pichuan@pichuan-gpu2 ~]$ nvidia-smi. Thu Mar 16 04:47:54 2023 . +-----------------------------------------------------------------------------+. | NVIDIA-SMI 525.85.12 Driver Version: 525.85.12 CUDA Version: 12.0 |. |-------------------------------+----------------------+----------------------+. | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC |. | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. |. | | | MIG M. |. |===============================+======================+======================|. | 0 Tesla P100-PCIE... Off | 00000000:00:04.0 Off | 0 |. | N/A 34C P0 29W / 250W | 0MiB / 16384MiB | 1% Default |. | | | N/A |. +-------------------------------+----------------------+----------------------+. . +-----------------------------------------------------------------------------+. | Processes: |. | GPU GI CI PID Type Process name GPU Memory |. | ID ID Usage |. |=============================================================================|. | No running processes found |. +-----------------------------------------------------------------------------+. ```. Then I install cuda. This was from: https://developer.nvidia.com/cuda-downloads. ```. curl -O https://developer.download.nvidia.com/compute/cuda/12.1.0/local_installers/cuda_12.1.0_530.30.02_linux.run",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:1150,performance,Memor,Memory-Usage,1150,"ERMINATE"" \. --accelerator=type=nvidia-tesla-p100,count=1 \. --image-family ""centos-7"" \. --image-project ""centos-cloud"" \. --machine-type ""n1-standard-16"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. On the machine, I install nvidia driver first:. ```. sudo yum update -y && sudo yum install -y python3. curl https://raw.githubusercontent.com/GoogleCloudPlatform/compute-gpu-installation/main/linux/install_gpu_driver.py --output install_gpu_driver.py. sudo python3 install_gpu_driver.py. ```. After that, I can confirm that nvidia-smi exists:. ```. [pichuan@pichuan-gpu2 ~]$ nvidia-smi. Thu Mar 16 04:47:54 2023 . +-----------------------------------------------------------------------------+. | NVIDIA-SMI 525.85.12 Driver Version: 525.85.12 CUDA Version: 12.0 |. |-------------------------------+----------------------+----------------------+. | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC |. | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. |. | | | MIG M. |. |===============================+======================+======================|. | 0 Tesla P100-PCIE... Off | 00000000:00:04.0 Off | 0 |. | N/A 34C P0 29W / 250W | 0MiB / 16384MiB | 1% Default |. | | | N/A |. +-------------------------------+----------------------+----------------------+. . +-----------------------------------------------------------------------------+. | Processes: |. | GPU GI CI PID Type Process name GPU Memory |. | ID ID Usage |. |=============================================================================|. | No running processes found |. +-----------------------------------------------------------------------------+. ```. Then I install cuda. This was from: https://developer.nvidia.com/cuda-downloads. ```. curl -O https://developer.download.nvidia.com/compute/cuda/12.1.0/local_installers/cuda_12.1.0_530.30.02_linux.run . export TERM=xterm. sudo sh cuda_12.1.0_530.30.02_linux.run. ```. ```. export PATH=/usr/local/cud",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:1165,performance,GPU,GPU-Util,1165,"--accelerator=type=nvidia-tesla-p100,count=1 \. --image-family ""centos-7"" \. --image-project ""centos-cloud"" \. --machine-type ""n1-standard-16"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. On the machine, I install nvidia driver first:. ```. sudo yum update -y && sudo yum install -y python3. curl https://raw.githubusercontent.com/GoogleCloudPlatform/compute-gpu-installation/main/linux/install_gpu_driver.py --output install_gpu_driver.py. sudo python3 install_gpu_driver.py. ```. After that, I can confirm that nvidia-smi exists:. ```. [pichuan@pichuan-gpu2 ~]$ nvidia-smi. Thu Mar 16 04:47:54 2023 . +-----------------------------------------------------------------------------+. | NVIDIA-SMI 525.85.12 Driver Version: 525.85.12 CUDA Version: 12.0 |. |-------------------------------+----------------------+----------------------+. | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC |. | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. |. | | | MIG M. |. |===============================+======================+======================|. | 0 Tesla P100-PCIE... Off | 00000000:00:04.0 Off | 0 |. | N/A 34C P0 29W / 250W | 0MiB / 16384MiB | 1% Default |. | | | N/A |. +-------------------------------+----------------------+----------------------+. . +-----------------------------------------------------------------------------+. | Processes: |. | GPU GI CI PID Type Process name GPU Memory |. | ID ID Usage |. |=============================================================================|. | No running processes found |. +-----------------------------------------------------------------------------+. ```. Then I install cuda. This was from: https://developer.nvidia.com/cuda-downloads. ```. curl -O https://developer.download.nvidia.com/compute/cuda/12.1.0/local_installers/cuda_12.1.0_530.30.02_linux.run . export TERM=xterm. sudo sh cuda_12.1.0_530.30.02_linux.run. ```. ```. export PATH=/usr/local/cuda-12.1/bin:$P",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:1595,performance,GPU,GPU,1595,"/linux/install_gpu_driver.py --output install_gpu_driver.py. sudo python3 install_gpu_driver.py. ```. After that, I can confirm that nvidia-smi exists:. ```. [pichuan@pichuan-gpu2 ~]$ nvidia-smi. Thu Mar 16 04:47:54 2023 . +-----------------------------------------------------------------------------+. | NVIDIA-SMI 525.85.12 Driver Version: 525.85.12 CUDA Version: 12.0 |. |-------------------------------+----------------------+----------------------+. | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC |. | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. |. | | | MIG M. |. |===============================+======================+======================|. | 0 Tesla P100-PCIE... Off | 00000000:00:04.0 Off | 0 |. | N/A 34C P0 29W / 250W | 0MiB / 16384MiB | 1% Default |. | | | N/A |. +-------------------------------+----------------------+----------------------+. . +-----------------------------------------------------------------------------+. | Processes: |. | GPU GI CI PID Type Process name GPU Memory |. | ID ID Usage |. |=============================================================================|. | No running processes found |. +-----------------------------------------------------------------------------+. ```. Then I install cuda. This was from: https://developer.nvidia.com/cuda-downloads. ```. curl -O https://developer.download.nvidia.com/compute/cuda/12.1.0/local_installers/cuda_12.1.0_530.30.02_linux.run . export TERM=xterm. sudo sh cuda_12.1.0_530.30.02_linux.run. ```. ```. export PATH=/usr/local/cuda-12.1/bin:$PATH. export LD_LIBRARY_PATH=/usr/local/cuda-12.1/lib64:$LD_LIBRARY_PATH. sudo ldconfig. ```. ```. [pichuan@pichuan-gpu2 ~]$ nvcc --version. nvcc: NVIDIA (R) Cuda compiler driver. Copyright (c) 2005-2023 NVIDIA Corporation. Built on Tue_Feb__7_19:32:13_PST_2023. Cuda compilation tools, release 12.1, V12.1.66. Build cuda_12.1.r12.1/compiler.32415258_0. ```. This is not 11.8, but is a newer version. So let's test with it. Insta",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:1627,performance,GPU,GPU,1627,"utput install_gpu_driver.py. sudo python3 install_gpu_driver.py. ```. After that, I can confirm that nvidia-smi exists:. ```. [pichuan@pichuan-gpu2 ~]$ nvidia-smi. Thu Mar 16 04:47:54 2023 . +-----------------------------------------------------------------------------+. | NVIDIA-SMI 525.85.12 Driver Version: 525.85.12 CUDA Version: 12.0 |. |-------------------------------+----------------------+----------------------+. | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC |. | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. |. | | | MIG M. |. |===============================+======================+======================|. | 0 Tesla P100-PCIE... Off | 00000000:00:04.0 Off | 0 |. | N/A 34C P0 29W / 250W | 0MiB / 16384MiB | 1% Default |. | | | N/A |. +-------------------------------+----------------------+----------------------+. . +-----------------------------------------------------------------------------+. | Processes: |. | GPU GI CI PID Type Process name GPU Memory |. | ID ID Usage |. |=============================================================================|. | No running processes found |. +-----------------------------------------------------------------------------+. ```. Then I install cuda. This was from: https://developer.nvidia.com/cuda-downloads. ```. curl -O https://developer.download.nvidia.com/compute/cuda/12.1.0/local_installers/cuda_12.1.0_530.30.02_linux.run . export TERM=xterm. sudo sh cuda_12.1.0_530.30.02_linux.run. ```. ```. export PATH=/usr/local/cuda-12.1/bin:$PATH. export LD_LIBRARY_PATH=/usr/local/cuda-12.1/lib64:$LD_LIBRARY_PATH. sudo ldconfig. ```. ```. [pichuan@pichuan-gpu2 ~]$ nvcc --version. nvcc: NVIDIA (R) Cuda compiler driver. Copyright (c) 2005-2023 NVIDIA Corporation. Built on Tue_Feb__7_19:32:13_PST_2023. Cuda compilation tools, release 12.1, V12.1.66. Build cuda_12.1.r12.1/compiler.32415258_0. ```. This is not 11.8, but is a newer version. So let's test with it. Install Singularity:. ```. curl -O ht",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:1631,performance,Memor,Memory,1631," install_gpu_driver.py. sudo python3 install_gpu_driver.py. ```. After that, I can confirm that nvidia-smi exists:. ```. [pichuan@pichuan-gpu2 ~]$ nvidia-smi. Thu Mar 16 04:47:54 2023 . +-----------------------------------------------------------------------------+. | NVIDIA-SMI 525.85.12 Driver Version: 525.85.12 CUDA Version: 12.0 |. |-------------------------------+----------------------+----------------------+. | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC |. | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. |. | | | MIG M. |. |===============================+======================+======================|. | 0 Tesla P100-PCIE... Off | 00000000:00:04.0 Off | 0 |. | N/A 34C P0 29W / 250W | 0MiB / 16384MiB | 1% Default |. | | | N/A |. +-------------------------------+----------------------+----------------------+. . +-----------------------------------------------------------------------------+. | Processes: |. | GPU GI CI PID Type Process name GPU Memory |. | ID ID Usage |. |=============================================================================|. | No running processes found |. +-----------------------------------------------------------------------------+. ```. Then I install cuda. This was from: https://developer.nvidia.com/cuda-downloads. ```. curl -O https://developer.download.nvidia.com/compute/cuda/12.1.0/local_installers/cuda_12.1.0_530.30.02_linux.run . export TERM=xterm. sudo sh cuda_12.1.0_530.30.02_linux.run. ```. ```. export PATH=/usr/local/cuda-12.1/bin:$PATH. export LD_LIBRARY_PATH=/usr/local/cuda-12.1/lib64:$LD_LIBRARY_PATH. sudo ldconfig. ```. ```. [pichuan@pichuan-gpu2 ~]$ nvcc --version. nvcc: NVIDIA (R) Cuda compiler driver. Copyright (c) 2005-2023 NVIDIA Corporation. Built on Tue_Feb__7_19:32:13_PST_2023. Cuda compilation tools, release 12.1, V12.1.66. Build cuda_12.1.r12.1/compiler.32415258_0. ```. This is not 11.8, but is a newer version. So let's test with it. Install Singularity:. ```. curl -O https:/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:3126,performance,gpu,gpu,3126,"------------------------------+. ```. Then I install cuda. This was from: https://developer.nvidia.com/cuda-downloads. ```. curl -O https://developer.download.nvidia.com/compute/cuda/12.1.0/local_installers/cuda_12.1.0_530.30.02_linux.run . export TERM=xterm. sudo sh cuda_12.1.0_530.30.02_linux.run. ```. ```. export PATH=/usr/local/cuda-12.1/bin:$PATH. export LD_LIBRARY_PATH=/usr/local/cuda-12.1/lib64:$LD_LIBRARY_PATH. sudo ldconfig. ```. ```. [pichuan@pichuan-gpu2 ~]$ nvcc --version. nvcc: NVIDIA (R) Cuda compiler driver. Copyright (c) 2005-2023 NVIDIA Corporation. Built on Tue_Feb__7_19:32:13_PST_2023. Cuda compilation tools, release 12.1, V12.1.66. Build cuda_12.1.r12.1/compiler.32415258_0. ```. This is not 11.8, but is a newer version. So let's test with it. Install Singularity:. ```. curl -O https://raw.githubusercontent.com/google/deepvariant/r1.5/scripts/install_singularity.sh. sed -i -e 's/apt-get/yum/g' install_singularity.sh. bash -x install_singularity.sh. ```. Check version:. ```. [pichuan@pichuan-gpu2 ~]$ singularity --version. singularity version 3.7.0. ```. The rest is similar to https://github.com/google/deepvariant/issues/514#issuecomment-1035630725 , but with v1.5.0. ```. # Pull the image. BIN_VERSION=1.5.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run DeepVariant. # Using ""--nv"" and ""${BIN_VERSION}-gpu"" is important. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=$(nproc). ```. That worked. I can use this setup to try the way you run it as well. I'll report back later.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:3186,performance,gpu,gpu,3186,"------------------------------+. ```. Then I install cuda. This was from: https://developer.nvidia.com/cuda-downloads. ```. curl -O https://developer.download.nvidia.com/compute/cuda/12.1.0/local_installers/cuda_12.1.0_530.30.02_linux.run . export TERM=xterm. sudo sh cuda_12.1.0_530.30.02_linux.run. ```. ```. export PATH=/usr/local/cuda-12.1/bin:$PATH. export LD_LIBRARY_PATH=/usr/local/cuda-12.1/lib64:$LD_LIBRARY_PATH. sudo ldconfig. ```. ```. [pichuan@pichuan-gpu2 ~]$ nvcc --version. nvcc: NVIDIA (R) Cuda compiler driver. Copyright (c) 2005-2023 NVIDIA Corporation. Built on Tue_Feb__7_19:32:13_PST_2023. Cuda compilation tools, release 12.1, V12.1.66. Build cuda_12.1.r12.1/compiler.32415258_0. ```. This is not 11.8, but is a newer version. So let's test with it. Install Singularity:. ```. curl -O https://raw.githubusercontent.com/google/deepvariant/r1.5/scripts/install_singularity.sh. sed -i -e 's/apt-get/yum/g' install_singularity.sh. bash -x install_singularity.sh. ```. Check version:. ```. [pichuan@pichuan-gpu2 ~]$ singularity --version. singularity version 3.7.0. ```. The rest is similar to https://github.com/google/deepvariant/issues/514#issuecomment-1035630725 , but with v1.5.0. ```. # Pull the image. BIN_VERSION=1.5.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run DeepVariant. # Using ""--nv"" and ""${BIN_VERSION}-gpu"" is important. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=$(nproc). ```. That worked. I can use this setup to try the way you run it as well. I'll report back later.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:3310,performance,gpu,gpu,3310,"------------------------------+. ```. Then I install cuda. This was from: https://developer.nvidia.com/cuda-downloads. ```. curl -O https://developer.download.nvidia.com/compute/cuda/12.1.0/local_installers/cuda_12.1.0_530.30.02_linux.run . export TERM=xterm. sudo sh cuda_12.1.0_530.30.02_linux.run. ```. ```. export PATH=/usr/local/cuda-12.1/bin:$PATH. export LD_LIBRARY_PATH=/usr/local/cuda-12.1/lib64:$LD_LIBRARY_PATH. sudo ldconfig. ```. ```. [pichuan@pichuan-gpu2 ~]$ nvcc --version. nvcc: NVIDIA (R) Cuda compiler driver. Copyright (c) 2005-2023 NVIDIA Corporation. Built on Tue_Feb__7_19:32:13_PST_2023. Cuda compilation tools, release 12.1, V12.1.66. Build cuda_12.1.r12.1/compiler.32415258_0. ```. This is not 11.8, but is a newer version. So let's test with it. Install Singularity:. ```. curl -O https://raw.githubusercontent.com/google/deepvariant/r1.5/scripts/install_singularity.sh. sed -i -e 's/apt-get/yum/g' install_singularity.sh. bash -x install_singularity.sh. ```. Check version:. ```. [pichuan@pichuan-gpu2 ~]$ singularity --version. singularity version 3.7.0. ```. The rest is similar to https://github.com/google/deepvariant/issues/514#issuecomment-1035630725 , but with v1.5.0. ```. # Pull the image. BIN_VERSION=1.5.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run DeepVariant. # Using ""--nv"" and ""${BIN_VERSION}-gpu"" is important. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=$(nproc). ```. That worked. I can use this setup to try the way you run it as well. I'll report back later.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:135,reliability,mainten,maintenance-policy,135,"I got a machine to test:. ```. gcloud compute instances create ""${USER}-gpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --maintenance-policy ""TERMINATE"" \. --accelerator=type=nvidia-tesla-p100,count=1 \. --image-family ""centos-7"" \. --image-project ""centos-cloud"" \. --machine-type ""n1-standard-16"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. On the machine, I install nvidia driver first:. ```. sudo yum update -y && sudo yum install -y python3. curl https://raw.githubusercontent.com/GoogleCloudPlatform/compute-gpu-installation/main/linux/install_gpu_driver.py --output install_gpu_driver.py. sudo python3 install_gpu_driver.py. ```. After that, I can confirm that nvidia-smi exists:. ```. [pichuan@pichuan-gpu2 ~]$ nvidia-smi. Thu Mar 16 04:47:54 2023 . +-----------------------------------------------------------------------------+. | NVIDIA-SMI 525.85.12 Driver Version: 525.85.12 CUDA Version: 12.0 |. |-------------------------------+----------------------+----------------------+. | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC |. | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. |. | | | MIG M. |. |===============================+======================+======================|. | 0 Tesla P100-PCIE... Off | 00000000:00:04.0 Off | 0 |. | N/A 34C P0 29W / 250W | 0MiB / 16384MiB | 1% Default |. | | | N/A |. +-------------------------------+----------------------+----------------------+. . +-----------------------------------------------------------------------------+. | Processes: |. | GPU GI CI PID Type Process name GPU Memory |. | ID ID Usage |. |=============================================================================|. | No running processes found |. +-----------------------------------------------------------------------------+. ```. Then I install cuda. This was from: https://developer.nvidia.com/cuda-downloads. ```. curl -O https://developer.download.nvidia.com/compute/cud",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:19,safety,test,test,19,"I got a machine to test:. ```. gcloud compute instances create ""${USER}-gpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --maintenance-policy ""TERMINATE"" \. --accelerator=type=nvidia-tesla-p100,count=1 \. --image-family ""centos-7"" \. --image-project ""centos-cloud"" \. --machine-type ""n1-standard-16"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. On the machine, I install nvidia driver first:. ```. sudo yum update -y && sudo yum install -y python3. curl https://raw.githubusercontent.com/GoogleCloudPlatform/compute-gpu-installation/main/linux/install_gpu_driver.py --output install_gpu_driver.py. sudo python3 install_gpu_driver.py. ```. After that, I can confirm that nvidia-smi exists:. ```. [pichuan@pichuan-gpu2 ~]$ nvidia-smi. Thu Mar 16 04:47:54 2023 . +-----------------------------------------------------------------------------+. | NVIDIA-SMI 525.85.12 Driver Version: 525.85.12 CUDA Version: 12.0 |. |-------------------------------+----------------------+----------------------+. | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC |. | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. |. | | | MIG M. |. |===============================+======================+======================|. | 0 Tesla P100-PCIE... Off | 00000000:00:04.0 Off | 0 |. | N/A 34C P0 29W / 250W | 0MiB / 16384MiB | 1% Default |. | | | N/A |. +-------------------------------+----------------------+----------------------+. . +-----------------------------------------------------------------------------+. | Processes: |. | GPU GI CI PID Type Process name GPU Memory |. | ID ID Usage |. |=============================================================================|. | No running processes found |. +-----------------------------------------------------------------------------+. ```. Then I install cuda. This was from: https://developer.nvidia.com/cuda-downloads. ```. curl -O https://developer.download.nvidia.com/compute/cud",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:467,safety,updat,update,467,"I got a machine to test:. ```. gcloud compute instances create ""${USER}-gpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --maintenance-policy ""TERMINATE"" \. --accelerator=type=nvidia-tesla-p100,count=1 \. --image-family ""centos-7"" \. --image-project ""centos-cloud"" \. --machine-type ""n1-standard-16"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. On the machine, I install nvidia driver first:. ```. sudo yum update -y && sudo yum install -y python3. curl https://raw.githubusercontent.com/GoogleCloudPlatform/compute-gpu-installation/main/linux/install_gpu_driver.py --output install_gpu_driver.py. sudo python3 install_gpu_driver.py. ```. After that, I can confirm that nvidia-smi exists:. ```. [pichuan@pichuan-gpu2 ~]$ nvidia-smi. Thu Mar 16 04:47:54 2023 . +-----------------------------------------------------------------------------+. | NVIDIA-SMI 525.85.12 Driver Version: 525.85.12 CUDA Version: 12.0 |. |-------------------------------+----------------------+----------------------+. | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC |. | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. |. | | | MIG M. |. |===============================+======================+======================|. | 0 Tesla P100-PCIE... Off | 00000000:00:04.0 Off | 0 |. | N/A 34C P0 29W / 250W | 0MiB / 16384MiB | 1% Default |. | | | N/A |. +-------------------------------+----------------------+----------------------+. . +-----------------------------------------------------------------------------+. | Processes: |. | GPU GI CI PID Type Process name GPU Memory |. | ID ID Usage |. |=============================================================================|. | No running processes found |. +-----------------------------------------------------------------------------+. ```. Then I install cuda. This was from: https://developer.nvidia.com/cuda-downloads. ```. curl -O https://developer.download.nvidia.com/compute/cud",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:2578,safety,test,test,2578,"rocesses: |. | GPU GI CI PID Type Process name GPU Memory |. | ID ID Usage |. |=============================================================================|. | No running processes found |. +-----------------------------------------------------------------------------+. ```. Then I install cuda. This was from: https://developer.nvidia.com/cuda-downloads. ```. curl -O https://developer.download.nvidia.com/compute/cuda/12.1.0/local_installers/cuda_12.1.0_530.30.02_linux.run . export TERM=xterm. sudo sh cuda_12.1.0_530.30.02_linux.run. ```. ```. export PATH=/usr/local/cuda-12.1/bin:$PATH. export LD_LIBRARY_PATH=/usr/local/cuda-12.1/lib64:$LD_LIBRARY_PATH. sudo ldconfig. ```. ```. [pichuan@pichuan-gpu2 ~]$ nvcc --version. nvcc: NVIDIA (R) Cuda compiler driver. Copyright (c) 2005-2023 NVIDIA Corporation. Built on Tue_Feb__7_19:32:13_PST_2023. Cuda compilation tools, release 12.1, V12.1.66. Build cuda_12.1.r12.1/compiler.32415258_0. ```. This is not 11.8, but is a newer version. So let's test with it. Install Singularity:. ```. curl -O https://raw.githubusercontent.com/google/deepvariant/r1.5/scripts/install_singularity.sh. sed -i -e 's/apt-get/yum/g' install_singularity.sh. bash -x install_singularity.sh. ```. Check version:. ```. [pichuan@pichuan-gpu2 ~]$ singularity --version. singularity version 3.7.0. ```. The rest is similar to https://github.com/google/deepvariant/issues/514#issuecomment-1035630725 , but with v1.5.0. ```. # Pull the image. BIN_VERSION=1.5.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run DeepVariant. # Using ""--nv"" and ""${BIN_VERSION}-gpu"" is important. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. -",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:147,security,polic,policy,147,"I got a machine to test:. ```. gcloud compute instances create ""${USER}-gpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --maintenance-policy ""TERMINATE"" \. --accelerator=type=nvidia-tesla-p100,count=1 \. --image-family ""centos-7"" \. --image-project ""centos-cloud"" \. --machine-type ""n1-standard-16"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. On the machine, I install nvidia driver first:. ```. sudo yum update -y && sudo yum install -y python3. curl https://raw.githubusercontent.com/GoogleCloudPlatform/compute-gpu-installation/main/linux/install_gpu_driver.py --output install_gpu_driver.py. sudo python3 install_gpu_driver.py. ```. After that, I can confirm that nvidia-smi exists:. ```. [pichuan@pichuan-gpu2 ~]$ nvidia-smi. Thu Mar 16 04:47:54 2023 . +-----------------------------------------------------------------------------+. | NVIDIA-SMI 525.85.12 Driver Version: 525.85.12 CUDA Version: 12.0 |. |-------------------------------+----------------------+----------------------+. | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC |. | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. |. | | | MIG M. |. |===============================+======================+======================|. | 0 Tesla P100-PCIE... Off | 00000000:00:04.0 Off | 0 |. | N/A 34C P0 29W / 250W | 0MiB / 16384MiB | 1% Default |. | | | N/A |. +-------------------------------+----------------------+----------------------+. . +-----------------------------------------------------------------------------+. | Processes: |. | GPU GI CI PID Type Process name GPU Memory |. | ID ID Usage |. |=============================================================================|. | No running processes found |. +-----------------------------------------------------------------------------+. ```. Then I install cuda. This was from: https://developer.nvidia.com/cuda-downloads. ```. curl -O https://developer.download.nvidia.com/compute/cud",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:467,security,updat,update,467,"I got a machine to test:. ```. gcloud compute instances create ""${USER}-gpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --maintenance-policy ""TERMINATE"" \. --accelerator=type=nvidia-tesla-p100,count=1 \. --image-family ""centos-7"" \. --image-project ""centos-cloud"" \. --machine-type ""n1-standard-16"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. On the machine, I install nvidia driver first:. ```. sudo yum update -y && sudo yum install -y python3. curl https://raw.githubusercontent.com/GoogleCloudPlatform/compute-gpu-installation/main/linux/install_gpu_driver.py --output install_gpu_driver.py. sudo python3 install_gpu_driver.py. ```. After that, I can confirm that nvidia-smi exists:. ```. [pichuan@pichuan-gpu2 ~]$ nvidia-smi. Thu Mar 16 04:47:54 2023 . +-----------------------------------------------------------------------------+. | NVIDIA-SMI 525.85.12 Driver Version: 525.85.12 CUDA Version: 12.0 |. |-------------------------------+----------------------+----------------------+. | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC |. | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. |. | | | MIG M. |. |===============================+======================+======================|. | 0 Tesla P100-PCIE... Off | 00000000:00:04.0 Off | 0 |. | N/A 34C P0 29W / 250W | 0MiB / 16384MiB | 1% Default |. | | | N/A |. +-------------------------------+----------------------+----------------------+. . +-----------------------------------------------------------------------------+. | Processes: |. | GPU GI CI PID Type Process name GPU Memory |. | ID ID Usage |. |=============================================================================|. | No running processes found |. +-----------------------------------------------------------------------------+. ```. Then I install cuda. This was from: https://developer.nvidia.com/cuda-downloads. ```. curl -O https://developer.download.nvidia.com/compute/cud",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:2730,security,apt,apt-get,2730,"==|. | No running processes found |. +-----------------------------------------------------------------------------+. ```. Then I install cuda. This was from: https://developer.nvidia.com/cuda-downloads. ```. curl -O https://developer.download.nvidia.com/compute/cuda/12.1.0/local_installers/cuda_12.1.0_530.30.02_linux.run . export TERM=xterm. sudo sh cuda_12.1.0_530.30.02_linux.run. ```. ```. export PATH=/usr/local/cuda-12.1/bin:$PATH. export LD_LIBRARY_PATH=/usr/local/cuda-12.1/lib64:$LD_LIBRARY_PATH. sudo ldconfig. ```. ```. [pichuan@pichuan-gpu2 ~]$ nvcc --version. nvcc: NVIDIA (R) Cuda compiler driver. Copyright (c) 2005-2023 NVIDIA Corporation. Built on Tue_Feb__7_19:32:13_PST_2023. Cuda compilation tools, release 12.1, V12.1.66. Build cuda_12.1.r12.1/compiler.32415258_0. ```. This is not 11.8, but is a newer version. So let's test with it. Install Singularity:. ```. curl -O https://raw.githubusercontent.com/google/deepvariant/r1.5/scripts/install_singularity.sh. sed -i -e 's/apt-get/yum/g' install_singularity.sh. bash -x install_singularity.sh. ```. Check version:. ```. [pichuan@pichuan-gpu2 ~]$ singularity --version. singularity version 3.7.0. ```. The rest is similar to https://github.com/google/deepvariant/issues/514#issuecomment-1035630725 , but with v1.5.0. ```. # Pull the image. BIN_VERSION=1.5.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run DeepVariant. # Using ""--nv"" and ""${BIN_VERSION}-gpu"" is important. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=$(nproc). ```. That wo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:19,testability,test,test,19,"I got a machine to test:. ```. gcloud compute instances create ""${USER}-gpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --maintenance-policy ""TERMINATE"" \. --accelerator=type=nvidia-tesla-p100,count=1 \. --image-family ""centos-7"" \. --image-project ""centos-cloud"" \. --machine-type ""n1-standard-16"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. On the machine, I install nvidia driver first:. ```. sudo yum update -y && sudo yum install -y python3. curl https://raw.githubusercontent.com/GoogleCloudPlatform/compute-gpu-installation/main/linux/install_gpu_driver.py --output install_gpu_driver.py. sudo python3 install_gpu_driver.py. ```. After that, I can confirm that nvidia-smi exists:. ```. [pichuan@pichuan-gpu2 ~]$ nvidia-smi. Thu Mar 16 04:47:54 2023 . +-----------------------------------------------------------------------------+. | NVIDIA-SMI 525.85.12 Driver Version: 525.85.12 CUDA Version: 12.0 |. |-------------------------------+----------------------+----------------------+. | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC |. | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. |. | | | MIG M. |. |===============================+======================+======================|. | 0 Tesla P100-PCIE... Off | 00000000:00:04.0 Off | 0 |. | N/A 34C P0 29W / 250W | 0MiB / 16384MiB | 1% Default |. | | | N/A |. +-------------------------------+----------------------+----------------------+. . +-----------------------------------------------------------------------------+. | Processes: |. | GPU GI CI PID Type Process name GPU Memory |. | ID ID Usage |. |=============================================================================|. | No running processes found |. +-----------------------------------------------------------------------------+. ```. Then I install cuda. This was from: https://developer.nvidia.com/cuda-downloads. ```. curl -O https://developer.download.nvidia.com/compute/cud",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:2578,testability,test,test,2578,"rocesses: |. | GPU GI CI PID Type Process name GPU Memory |. | ID ID Usage |. |=============================================================================|. | No running processes found |. +-----------------------------------------------------------------------------+. ```. Then I install cuda. This was from: https://developer.nvidia.com/cuda-downloads. ```. curl -O https://developer.download.nvidia.com/compute/cuda/12.1.0/local_installers/cuda_12.1.0_530.30.02_linux.run . export TERM=xterm. sudo sh cuda_12.1.0_530.30.02_linux.run. ```. ```. export PATH=/usr/local/cuda-12.1/bin:$PATH. export LD_LIBRARY_PATH=/usr/local/cuda-12.1/lib64:$LD_LIBRARY_PATH. sudo ldconfig. ```. ```. [pichuan@pichuan-gpu2 ~]$ nvcc --version. nvcc: NVIDIA (R) Cuda compiler driver. Copyright (c) 2005-2023 NVIDIA Corporation. Built on Tue_Feb__7_19:32:13_PST_2023. Cuda compilation tools, release 12.1, V12.1.66. Build cuda_12.1.r12.1/compiler.32415258_0. ```. This is not 11.8, but is a newer version. So let's test with it. Install Singularity:. ```. curl -O https://raw.githubusercontent.com/google/deepvariant/r1.5/scripts/install_singularity.sh. sed -i -e 's/apt-get/yum/g' install_singularity.sh. bash -x install_singularity.sh. ```. Check version:. ```. [pichuan@pichuan-gpu2 ~]$ singularity --version. singularity version 3.7.0. ```. The rest is similar to https://github.com/google/deepvariant/issues/514#issuecomment-1035630725 , but with v1.5.0. ```. # Pull the image. BIN_VERSION=1.5.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run DeepVariant. # Using ""--nv"" and ""${BIN_VERSION}-gpu"" is important. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. -",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:3415,testability,unit,unittest,3415,"------------------------------+. ```. Then I install cuda. This was from: https://developer.nvidia.com/cuda-downloads. ```. curl -O https://developer.download.nvidia.com/compute/cuda/12.1.0/local_installers/cuda_12.1.0_530.30.02_linux.run . export TERM=xterm. sudo sh cuda_12.1.0_530.30.02_linux.run. ```. ```. export PATH=/usr/local/cuda-12.1/bin:$PATH. export LD_LIBRARY_PATH=/usr/local/cuda-12.1/lib64:$LD_LIBRARY_PATH. sudo ldconfig. ```. ```. [pichuan@pichuan-gpu2 ~]$ nvcc --version. nvcc: NVIDIA (R) Cuda compiler driver. Copyright (c) 2005-2023 NVIDIA Corporation. Built on Tue_Feb__7_19:32:13_PST_2023. Cuda compilation tools, release 12.1, V12.1.66. Build cuda_12.1.r12.1/compiler.32415258_0. ```. This is not 11.8, but is a newer version. So let's test with it. Install Singularity:. ```. curl -O https://raw.githubusercontent.com/google/deepvariant/r1.5/scripts/install_singularity.sh. sed -i -e 's/apt-get/yum/g' install_singularity.sh. bash -x install_singularity.sh. ```. Check version:. ```. [pichuan@pichuan-gpu2 ~]$ singularity --version. singularity version 3.7.0. ```. The rest is similar to https://github.com/google/deepvariant/issues/514#issuecomment-1035630725 , but with v1.5.0. ```. # Pull the image. BIN_VERSION=1.5.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run DeepVariant. # Using ""--nv"" and ""${BIN_VERSION}-gpu"" is important. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=$(nproc). ```. That worked. I can use this setup to try the way you run it as well. I'll report back later.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:66,usability,USER,USER,66,"I got a machine to test:. ```. gcloud compute instances create ""${USER}-gpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --maintenance-policy ""TERMINATE"" \. --accelerator=type=nvidia-tesla-p100,count=1 \. --image-family ""centos-7"" \. --image-project ""centos-cloud"" \. --machine-type ""n1-standard-16"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. On the machine, I install nvidia driver first:. ```. sudo yum update -y && sudo yum install -y python3. curl https://raw.githubusercontent.com/GoogleCloudPlatform/compute-gpu-installation/main/linux/install_gpu_driver.py --output install_gpu_driver.py. sudo python3 install_gpu_driver.py. ```. After that, I can confirm that nvidia-smi exists:. ```. [pichuan@pichuan-gpu2 ~]$ nvidia-smi. Thu Mar 16 04:47:54 2023 . +-----------------------------------------------------------------------------+. | NVIDIA-SMI 525.85.12 Driver Version: 525.85.12 CUDA Version: 12.0 |. |-------------------------------+----------------------+----------------------+. | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC |. | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. |. | | | MIG M. |. |===============================+======================+======================|. | 0 Tesla P100-PCIE... Off | 00000000:00:04.0 Off | 0 |. | N/A 34C P0 29W / 250W | 0MiB / 16384MiB | 1% Default |. | | | N/A |. +-------------------------------+----------------------+----------------------+. . +-----------------------------------------------------------------------------+. | Processes: |. | GPU GI CI PID Type Process name GPU Memory |. | ID ID Usage |. |=============================================================================|. | No running processes found |. +-----------------------------------------------------------------------------+. ```. Then I install cuda. This was from: https://developer.nvidia.com/cuda-downloads. ```. curl -O https://developer.download.nvidia.com/compute/cud",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:717,usability,confirm,confirm,717,"I got a machine to test:. ```. gcloud compute instances create ""${USER}-gpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --maintenance-policy ""TERMINATE"" \. --accelerator=type=nvidia-tesla-p100,count=1 \. --image-family ""centos-7"" \. --image-project ""centos-cloud"" \. --machine-type ""n1-standard-16"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. On the machine, I install nvidia driver first:. ```. sudo yum update -y && sudo yum install -y python3. curl https://raw.githubusercontent.com/GoogleCloudPlatform/compute-gpu-installation/main/linux/install_gpu_driver.py --output install_gpu_driver.py. sudo python3 install_gpu_driver.py. ```. After that, I can confirm that nvidia-smi exists:. ```. [pichuan@pichuan-gpu2 ~]$ nvidia-smi. Thu Mar 16 04:47:54 2023 . +-----------------------------------------------------------------------------+. | NVIDIA-SMI 525.85.12 Driver Version: 525.85.12 CUDA Version: 12.0 |. |-------------------------------+----------------------+----------------------+. | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC |. | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. |. | | | MIG M. |. |===============================+======================+======================|. | 0 Tesla P100-PCIE... Off | 00000000:00:04.0 Off | 0 |. | N/A 34C P0 29W / 250W | 0MiB / 16384MiB | 1% Default |. | | | N/A |. +-------------------------------+----------------------+----------------------+. . +-----------------------------------------------------------------------------+. | Processes: |. | GPU GI CI PID Type Process name GPU Memory |. | ID ID Usage |. |=============================================================================|. | No running processes found |. +-----------------------------------------------------------------------------+. ```. Then I install cuda. This was from: https://developer.nvidia.com/cuda-downloads. ```. curl -O https://developer.download.nvidia.com/compute/cud",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:1150,usability,Memor,Memory-Usage,1150,"ERMINATE"" \. --accelerator=type=nvidia-tesla-p100,count=1 \. --image-family ""centos-7"" \. --image-project ""centos-cloud"" \. --machine-type ""n1-standard-16"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. On the machine, I install nvidia driver first:. ```. sudo yum update -y && sudo yum install -y python3. curl https://raw.githubusercontent.com/GoogleCloudPlatform/compute-gpu-installation/main/linux/install_gpu_driver.py --output install_gpu_driver.py. sudo python3 install_gpu_driver.py. ```. After that, I can confirm that nvidia-smi exists:. ```. [pichuan@pichuan-gpu2 ~]$ nvidia-smi. Thu Mar 16 04:47:54 2023 . +-----------------------------------------------------------------------------+. | NVIDIA-SMI 525.85.12 Driver Version: 525.85.12 CUDA Version: 12.0 |. |-------------------------------+----------------------+----------------------+. | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC |. | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. |. | | | MIG M. |. |===============================+======================+======================|. | 0 Tesla P100-PCIE... Off | 00000000:00:04.0 Off | 0 |. | N/A 34C P0 29W / 250W | 0MiB / 16384MiB | 1% Default |. | | | N/A |. +-------------------------------+----------------------+----------------------+. . +-----------------------------------------------------------------------------+. | Processes: |. | GPU GI CI PID Type Process name GPU Memory |. | ID ID Usage |. |=============================================================================|. | No running processes found |. +-----------------------------------------------------------------------------+. ```. Then I install cuda. This was from: https://developer.nvidia.com/cuda-downloads. ```. curl -O https://developer.download.nvidia.com/compute/cuda/12.1.0/local_installers/cuda_12.1.0_530.30.02_linux.run . export TERM=xterm. sudo sh cuda_12.1.0_530.30.02_linux.run. ```. ```. export PATH=/usr/local/cud",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:1631,usability,Memor,Memory,1631," install_gpu_driver.py. sudo python3 install_gpu_driver.py. ```. After that, I can confirm that nvidia-smi exists:. ```. [pichuan@pichuan-gpu2 ~]$ nvidia-smi. Thu Mar 16 04:47:54 2023 . +-----------------------------------------------------------------------------+. | NVIDIA-SMI 525.85.12 Driver Version: 525.85.12 CUDA Version: 12.0 |. |-------------------------------+----------------------+----------------------+. | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC |. | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. |. | | | MIG M. |. |===============================+======================+======================|. | 0 Tesla P100-PCIE... Off | 00000000:00:04.0 Off | 0 |. | N/A 34C P0 29W / 250W | 0MiB / 16384MiB | 1% Default |. | | | N/A |. +-------------------------------+----------------------+----------------------+. . +-----------------------------------------------------------------------------+. | Processes: |. | GPU GI CI PID Type Process name GPU Memory |. | ID ID Usage |. |=============================================================================|. | No running processes found |. +-----------------------------------------------------------------------------+. ```. Then I install cuda. This was from: https://developer.nvidia.com/cuda-downloads. ```. curl -O https://developer.download.nvidia.com/compute/cuda/12.1.0/local_installers/cuda_12.1.0_530.30.02_linux.run . export TERM=xterm. sudo sh cuda_12.1.0_530.30.02_linux.run. ```. ```. export PATH=/usr/local/cuda-12.1/bin:$PATH. export LD_LIBRARY_PATH=/usr/local/cuda-12.1/lib64:$LD_LIBRARY_PATH. sudo ldconfig. ```. ```. [pichuan@pichuan-gpu2 ~]$ nvcc --version. nvcc: NVIDIA (R) Cuda compiler driver. Copyright (c) 2005-2023 NVIDIA Corporation. Built on Tue_Feb__7_19:32:13_PST_2023. Cuda compilation tools, release 12.1, V12.1.66. Build cuda_12.1.r12.1/compiler.32415258_0. ```. This is not 11.8, but is a newer version. So let's test with it. Install Singularity:. ```. curl -O https:/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:2448,usability,tool,tools,2448,"-----------------+----------------------+. . +-----------------------------------------------------------------------------+. | Processes: |. | GPU GI CI PID Type Process name GPU Memory |. | ID ID Usage |. |=============================================================================|. | No running processes found |. +-----------------------------------------------------------------------------+. ```. Then I install cuda. This was from: https://developer.nvidia.com/cuda-downloads. ```. curl -O https://developer.download.nvidia.com/compute/cuda/12.1.0/local_installers/cuda_12.1.0_530.30.02_linux.run . export TERM=xterm. sudo sh cuda_12.1.0_530.30.02_linux.run. ```. ```. export PATH=/usr/local/cuda-12.1/bin:$PATH. export LD_LIBRARY_PATH=/usr/local/cuda-12.1/lib64:$LD_LIBRARY_PATH. sudo ldconfig. ```. ```. [pichuan@pichuan-gpu2 ~]$ nvcc --version. nvcc: NVIDIA (R) Cuda compiler driver. Copyright (c) 2005-2023 NVIDIA Corporation. Built on Tue_Feb__7_19:32:13_PST_2023. Cuda compilation tools, release 12.1, V12.1.66. Build cuda_12.1.r12.1/compiler.32415258_0. ```. This is not 11.8, but is a newer version. So let's test with it. Install Singularity:. ```. curl -O https://raw.githubusercontent.com/google/deepvariant/r1.5/scripts/install_singularity.sh. sed -i -e 's/apt-get/yum/g' install_singularity.sh. bash -x install_singularity.sh. ```. Check version:. ```. [pichuan@pichuan-gpu2 ~]$ singularity --version. singularity version 3.7.0. ```. The rest is similar to https://github.com/google/deepvariant/issues/514#issuecomment-1035630725 , but with v1.5.0. ```. # Pull the image. BIN_VERSION=1.5.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run DeepVariant. # Using ""--nv"" and ""${BIN_VERSION}-gpu"" is important. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_D",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:210,availability,error,error,210,"Actually, when I took a closer look at the logs, it says:. ```. 2023-03-16 05:35:30.141288: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. 2023-03-16 05:35:30.141355: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: pichuan-gpu2. 2023-03-16 05:35:30.141366: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: pichuan-gpu2. 2023-03-16 05:35:30.141423: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". 2023-03-16 05:35:30.141471: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 530.30.2. ```. So it seems like I'm able to reproduce this issue. . Let me take a closer look. I'll also want to test this on Ubuntu. I tested before release, but I'll want to test it again.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:43,deployability,log,logs,43,"Actually, when I took a closer look at the logs, it says:. ```. 2023-03-16 05:35:30.141288: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. 2023-03-16 05:35:30.141355: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: pichuan-gpu2. 2023-03-16 05:35:30.141366: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: pichuan-gpu2. 2023-03-16 05:35:30.141423: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". 2023-03-16 05:35:30.141471: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 530.30.2. ```. So it seems like I'm able to reproduce this issue. . Let me take a closer look. I'll also want to test this on Ubuntu. I tested before release, but I'll want to test it again.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:159,deployability,fail,failed,159,"Actually, when I took a closer look at the logs, it says:. ```. 2023-03-16 05:35:30.141288: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. 2023-03-16 05:35:30.141355: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: pichuan-gpu2. 2023-03-16 05:35:30.141366: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: pichuan-gpu2. 2023-03-16 05:35:30.141423: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". 2023-03-16 05:35:30.141471: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 530.30.2. ```. So it seems like I'm able to reproduce this issue. . Let me take a closer look. I'll also want to test this on Ubuntu. I tested before release, but I'll want to test it again.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:621,deployability,version,version,621,"Actually, when I took a closer look at the logs, it says:. ```. 2023-03-16 05:35:30.141288: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. 2023-03-16 05:35:30.141355: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: pichuan-gpu2. 2023-03-16 05:35:30.141366: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: pichuan-gpu2. 2023-03-16 05:35:30.141423: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". 2023-03-16 05:35:30.141471: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 530.30.2. ```. So it seems like I'm able to reproduce this issue. . Let me take a closer look. I'll also want to test this on Ubuntu. I tested before release, but I'll want to test it again.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:708,deployability,version,version,708,"Actually, when I took a closer look at the logs, it says:. ```. 2023-03-16 05:35:30.141288: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. 2023-03-16 05:35:30.141355: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: pichuan-gpu2. 2023-03-16 05:35:30.141366: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: pichuan-gpu2. 2023-03-16 05:35:30.141423: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". 2023-03-16 05:35:30.141471: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 530.30.2. ```. So it seems like I'm able to reproduce this issue. . Let me take a closer look. I'll also want to test this on Ubuntu. I tested before release, but I'll want to test it again.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:842,deployability,version,version,842,"Actually, when I took a closer look at the logs, it says:. ```. 2023-03-16 05:35:30.141288: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. 2023-03-16 05:35:30.141355: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: pichuan-gpu2. 2023-03-16 05:35:30.141366: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: pichuan-gpu2. 2023-03-16 05:35:30.141423: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". 2023-03-16 05:35:30.141471: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 530.30.2. ```. So it seems like I'm able to reproduce this issue. . Let me take a closer look. I'll also want to test this on Ubuntu. I tested before release, but I'll want to test it again.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:1004,deployability,releas,release,1004,"Actually, when I took a closer look at the logs, it says:. ```. 2023-03-16 05:35:30.141288: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. 2023-03-16 05:35:30.141355: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: pichuan-gpu2. 2023-03-16 05:35:30.141366: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: pichuan-gpu2. 2023-03-16 05:35:30.141423: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". 2023-03-16 05:35:30.141471: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 530.30.2. ```. So it seems like I'm able to reproduce this issue. . Let me take a closer look. I'll also want to test this on Ubuntu. I tested before release, but I'll want to test it again.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:621,integrability,version,version,621,"Actually, when I took a closer look at the logs, it says:. ```. 2023-03-16 05:35:30.141288: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. 2023-03-16 05:35:30.141355: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: pichuan-gpu2. 2023-03-16 05:35:30.141366: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: pichuan-gpu2. 2023-03-16 05:35:30.141423: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". 2023-03-16 05:35:30.141471: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 530.30.2. ```. So it seems like I'm able to reproduce this issue. . Let me take a closer look. I'll also want to test this on Ubuntu. I tested before release, but I'll want to test it again.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:708,integrability,version,version,708,"Actually, when I took a closer look at the logs, it says:. ```. 2023-03-16 05:35:30.141288: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. 2023-03-16 05:35:30.141355: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: pichuan-gpu2. 2023-03-16 05:35:30.141366: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: pichuan-gpu2. 2023-03-16 05:35:30.141423: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". 2023-03-16 05:35:30.141471: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 530.30.2. ```. So it seems like I'm able to reproduce this issue. . Let me take a closer look. I'll also want to test this on Ubuntu. I tested before release, but I'll want to test it again.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:842,integrability,version,version,842,"Actually, when I took a closer look at the logs, it says:. ```. 2023-03-16 05:35:30.141288: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. 2023-03-16 05:35:30.141355: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: pichuan-gpu2. 2023-03-16 05:35:30.141366: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: pichuan-gpu2. 2023-03-16 05:35:30.141423: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". 2023-03-16 05:35:30.141471: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 530.30.2. ```. So it seems like I'm able to reproduce this issue. . Let me take a closer look. I'll also want to test this on Ubuntu. I tested before release, but I'll want to test it again.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:621,modifiability,version,version,621,"Actually, when I took a closer look at the logs, it says:. ```. 2023-03-16 05:35:30.141288: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. 2023-03-16 05:35:30.141355: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: pichuan-gpu2. 2023-03-16 05:35:30.141366: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: pichuan-gpu2. 2023-03-16 05:35:30.141423: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". 2023-03-16 05:35:30.141471: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 530.30.2. ```. So it seems like I'm able to reproduce this issue. . Let me take a closer look. I'll also want to test this on Ubuntu. I tested before release, but I'll want to test it again.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:708,modifiability,version,version,708,"Actually, when I took a closer look at the logs, it says:. ```. 2023-03-16 05:35:30.141288: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. 2023-03-16 05:35:30.141355: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: pichuan-gpu2. 2023-03-16 05:35:30.141366: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: pichuan-gpu2. 2023-03-16 05:35:30.141423: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". 2023-03-16 05:35:30.141471: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 530.30.2. ```. So it seems like I'm able to reproduce this issue. . Let me take a closer look. I'll also want to test this on Ubuntu. I tested before release, but I'll want to test it again.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:842,modifiability,version,version,842,"Actually, when I took a closer look at the logs, it says:. ```. 2023-03-16 05:35:30.141288: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. 2023-03-16 05:35:30.141355: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: pichuan-gpu2. 2023-03-16 05:35:30.141366: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: pichuan-gpu2. 2023-03-16 05:35:30.141423: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". 2023-03-16 05:35:30.141471: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 530.30.2. ```. So it seems like I'm able to reproduce this issue. . Let me take a closer look. I'll also want to test this on Ubuntu. I tested before release, but I'll want to test it again.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:210,performance,error,error,210,"Actually, when I took a closer look at the logs, it says:. ```. 2023-03-16 05:35:30.141288: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. 2023-03-16 05:35:30.141355: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: pichuan-gpu2. 2023-03-16 05:35:30.141366: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: pichuan-gpu2. 2023-03-16 05:35:30.141423: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". 2023-03-16 05:35:30.141471: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 530.30.2. ```. So it seems like I'm able to reproduce this issue. . Let me take a closer look. I'll also want to test this on Ubuntu. I tested before release, but I'll want to test it again.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:159,reliability,fail,failed,159,"Actually, when I took a closer look at the logs, it says:. ```. 2023-03-16 05:35:30.141288: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. 2023-03-16 05:35:30.141355: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: pichuan-gpu2. 2023-03-16 05:35:30.141366: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: pichuan-gpu2. 2023-03-16 05:35:30.141423: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". 2023-03-16 05:35:30.141471: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 530.30.2. ```. So it seems like I'm able to reproduce this issue. . Let me take a closer look. I'll also want to test this on Ubuntu. I tested before release, but I'll want to test it again.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:333,reliability,diagno,diagnostic,333,"Actually, when I took a closer look at the logs, it says:. ```. 2023-03-16 05:35:30.141288: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. 2023-03-16 05:35:30.141355: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: pichuan-gpu2. 2023-03-16 05:35:30.141366: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: pichuan-gpu2. 2023-03-16 05:35:30.141423: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". 2023-03-16 05:35:30.141471: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 530.30.2. ```. So it seems like I'm able to reproduce this issue. . Let me take a closer look. I'll also want to test this on Ubuntu. I tested before release, but I'll want to test it again.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:43,safety,log,logs,43,"Actually, when I took a closer look at the logs, it says:. ```. 2023-03-16 05:35:30.141288: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. 2023-03-16 05:35:30.141355: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: pichuan-gpu2. 2023-03-16 05:35:30.141366: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: pichuan-gpu2. 2023-03-16 05:35:30.141423: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". 2023-03-16 05:35:30.141471: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 530.30.2. ```. So it seems like I'm able to reproduce this issue. . Let me take a closer look. I'll also want to test this on Ubuntu. I tested before release, but I'll want to test it again.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:210,safety,error,error,210,"Actually, when I took a closer look at the logs, it says:. ```. 2023-03-16 05:35:30.141288: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. 2023-03-16 05:35:30.141355: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: pichuan-gpu2. 2023-03-16 05:35:30.141366: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: pichuan-gpu2. 2023-03-16 05:35:30.141423: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". 2023-03-16 05:35:30.141471: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 530.30.2. ```. So it seems like I'm able to reproduce this issue. . Let me take a closer look. I'll also want to test this on Ubuntu. I tested before release, but I'll want to test it again.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:967,safety,test,test,967,"Actually, when I took a closer look at the logs, it says:. ```. 2023-03-16 05:35:30.141288: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. 2023-03-16 05:35:30.141355: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: pichuan-gpu2. 2023-03-16 05:35:30.141366: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: pichuan-gpu2. 2023-03-16 05:35:30.141423: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". 2023-03-16 05:35:30.141471: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 530.30.2. ```. So it seems like I'm able to reproduce this issue. . Let me take a closer look. I'll also want to test this on Ubuntu. I tested before release, but I'll want to test it again.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:990,safety,test,tested,990,"Actually, when I took a closer look at the logs, it says:. ```. 2023-03-16 05:35:30.141288: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. 2023-03-16 05:35:30.141355: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: pichuan-gpu2. 2023-03-16 05:35:30.141366: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: pichuan-gpu2. 2023-03-16 05:35:30.141423: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". 2023-03-16 05:35:30.141471: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 530.30.2. ```. So it seems like I'm able to reproduce this issue. . Let me take a closer look. I'll also want to test this on Ubuntu. I tested before release, but I'll want to test it again.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:1030,safety,test,test,1030,"Actually, when I took a closer look at the logs, it says:. ```. 2023-03-16 05:35:30.141288: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. 2023-03-16 05:35:30.141355: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: pichuan-gpu2. 2023-03-16 05:35:30.141366: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: pichuan-gpu2. 2023-03-16 05:35:30.141423: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". 2023-03-16 05:35:30.141471: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 530.30.2. ```. So it seems like I'm able to reproduce this issue. . Let me take a closer look. I'll also want to test this on Ubuntu. I tested before release, but I'll want to test it again.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:43,security,log,logs,43,"Actually, when I took a closer look at the logs, it says:. ```. 2023-03-16 05:35:30.141288: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. 2023-03-16 05:35:30.141355: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: pichuan-gpu2. 2023-03-16 05:35:30.141366: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: pichuan-gpu2. 2023-03-16 05:35:30.141423: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". 2023-03-16 05:35:30.141471: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 530.30.2. ```. So it seems like I'm able to reproduce this issue. . Let me take a closer look. I'll also want to test this on Ubuntu. I tested before release, but I'll want to test it again.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:43,testability,log,logs,43,"Actually, when I took a closer look at the logs, it says:. ```. 2023-03-16 05:35:30.141288: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. 2023-03-16 05:35:30.141355: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: pichuan-gpu2. 2023-03-16 05:35:30.141366: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: pichuan-gpu2. 2023-03-16 05:35:30.141423: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". 2023-03-16 05:35:30.141471: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 530.30.2. ```. So it seems like I'm able to reproduce this issue. . Let me take a closer look. I'll also want to test this on Ubuntu. I tested before release, but I'll want to test it again.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:333,testability,diagno,diagnostic,333,"Actually, when I took a closer look at the logs, it says:. ```. 2023-03-16 05:35:30.141288: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. 2023-03-16 05:35:30.141355: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: pichuan-gpu2. 2023-03-16 05:35:30.141366: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: pichuan-gpu2. 2023-03-16 05:35:30.141423: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". 2023-03-16 05:35:30.141471: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 530.30.2. ```. So it seems like I'm able to reproduce this issue. . Let me take a closer look. I'll also want to test this on Ubuntu. I tested before release, but I'll want to test it again.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:967,testability,test,test,967,"Actually, when I took a closer look at the logs, it says:. ```. 2023-03-16 05:35:30.141288: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. 2023-03-16 05:35:30.141355: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: pichuan-gpu2. 2023-03-16 05:35:30.141366: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: pichuan-gpu2. 2023-03-16 05:35:30.141423: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". 2023-03-16 05:35:30.141471: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 530.30.2. ```. So it seems like I'm able to reproduce this issue. . Let me take a closer look. I'll also want to test this on Ubuntu. I tested before release, but I'll want to test it again.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:990,testability,test,tested,990,"Actually, when I took a closer look at the logs, it says:. ```. 2023-03-16 05:35:30.141288: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. 2023-03-16 05:35:30.141355: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: pichuan-gpu2. 2023-03-16 05:35:30.141366: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: pichuan-gpu2. 2023-03-16 05:35:30.141423: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". 2023-03-16 05:35:30.141471: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 530.30.2. ```. So it seems like I'm able to reproduce this issue. . Let me take a closer look. I'll also want to test this on Ubuntu. I tested before release, but I'll want to test it again.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:1030,testability,test,test,1030,"Actually, when I took a closer look at the logs, it says:. ```. 2023-03-16 05:35:30.141288: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. 2023-03-16 05:35:30.141355: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: pichuan-gpu2. 2023-03-16 05:35:30.141366: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: pichuan-gpu2. 2023-03-16 05:35:30.141423: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". 2023-03-16 05:35:30.141471: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 530.30.2. ```. So it seems like I'm able to reproduce this issue. . Let me take a closer look. I'll also want to test this on Ubuntu. I tested before release, but I'll want to test it again.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:24,usability,close,closer,24,"Actually, when I took a closer look at the logs, it says:. ```. 2023-03-16 05:35:30.141288: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. 2023-03-16 05:35:30.141355: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: pichuan-gpu2. 2023-03-16 05:35:30.141366: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: pichuan-gpu2. 2023-03-16 05:35:30.141423: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". 2023-03-16 05:35:30.141471: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 530.30.2. ```. So it seems like I'm able to reproduce this issue. . Let me take a closer look. I'll also want to test this on Ubuntu. I tested before release, but I'll want to test it again.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:210,usability,error,error,210,"Actually, when I took a closer look at the logs, it says:. ```. 2023-03-16 05:35:30.141288: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. 2023-03-16 05:35:30.141355: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: pichuan-gpu2. 2023-03-16 05:35:30.141366: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: pichuan-gpu2. 2023-03-16 05:35:30.141423: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". 2023-03-16 05:35:30.141471: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 530.30.2. ```. So it seems like I'm able to reproduce this issue. . Let me take a closer look. I'll also want to test this on Ubuntu. I tested before release, but I'll want to test it again.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:936,usability,close,closer,936,"Actually, when I took a closer look at the logs, it says:. ```. 2023-03-16 05:35:30.141288: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. 2023-03-16 05:35:30.141355: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: pichuan-gpu2. 2023-03-16 05:35:30.141366: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: pichuan-gpu2. 2023-03-16 05:35:30.141423: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". 2023-03-16 05:35:30.141471: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 530.30.2. ```. So it seems like I'm able to reproduce this issue. . Let me take a closer look. I'll also want to test this on Ubuntu. I tested before release, but I'll want to test it again.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:115,energy efficiency,gpu,gpu,115,"I ran:. ```. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf;print(tf.test.is_gpu_available())'. ```. This printed False, which confirmed that this setup was indeed not seeing GPU. :(. Let me test on Ubuntu to get another data point.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:270,energy efficiency,GPU,GPU,270,"I ran:. ```. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf;print(tf.test.is_gpu_available())'. ```. This printed False, which confirmed that this setup was indeed not seeing GPU. :(. Let me test on Ubuntu to get another data point.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:115,performance,gpu,gpu,115,"I ran:. ```. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf;print(tf.test.is_gpu_available())'. ```. This printed False, which confirmed that this setup was indeed not seeing GPU. :(. Let me test on Ubuntu to get another data point.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:270,performance,GPU,GPU,270,"I ran:. ```. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf;print(tf.test.is_gpu_available())'. ```. This printed False, which confirmed that this setup was indeed not seeing GPU. :(. Let me test on Ubuntu to get another data point.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:164,safety,test,test,164,"I ran:. ```. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf;print(tf.test.is_gpu_available())'. ```. This printed False, which confirmed that this setup was indeed not seeing GPU. :(. Let me test on Ubuntu to get another data point.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:286,safety,test,test,286,"I ran:. ```. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf;print(tf.test.is_gpu_available())'. ```. This printed False, which confirmed that this setup was indeed not seeing GPU. :(. Let me test on Ubuntu to get another data point.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:164,testability,test,test,164,"I ran:. ```. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf;print(tf.test.is_gpu_available())'. ```. This printed False, which confirmed that this setup was indeed not seeing GPU. :(. Let me test on Ubuntu to get another data point.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:286,testability,test,test,286,"I ran:. ```. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf;print(tf.test.is_gpu_available())'. ```. This printed False, which confirmed that this setup was indeed not seeing GPU. :(. Let me test on Ubuntu to get another data point.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:222,usability,confirm,confirmed,222,"I ran:. ```. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf;print(tf.test.is_gpu_available())'. ```. This printed False, which confirmed that this setup was indeed not seeing GPU. :(. Let me test on Ubuntu to get another data point.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:169,availability,down,download-archive,169,"I'm going to try installing CUDA 11.3 on the CentOS7 machine first, to confirm whether that will address the issue. I followed: https://developer.nvidia.com/cuda-11.3.0-download-archive?target_os=Linux. ```. wget https://developer.download.nvidia.com/compute/cuda/11.3.0/local_installers/cuda_11.3.0_465.19.01_linux.run. sudo sh cuda_11.3.0_465.19.01_linux.run. ```. ```. export PATH=/usr/local/cuda-11.3/bin:$PATH. export LD_LIBRARY_PATH=/usr/local/cuda-11.3/lib64:$LD_LIBRARY_PATH. sudo ldconfig. ```. ```. [pichuan@pichuan-gpu2 ~]$ nvcc --version. nvcc: NVIDIA (R) Cuda compiler driver. Copyright (c) 2005-2021 NVIDIA Corporation. Built on Sun_Mar_21_19:15:46_PDT_2021. Cuda compilation tools, release 11.3, V11.3.58. Build cuda_11.3.r11.3/compiler.29745058_0. ```. Now, with this, I tried:. ```. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf;print(tf.test.is_gpu_available())'. ```. which still gave me False! Hmm. I'll search the error on the internet to see if I can find something useful",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:231,availability,down,download,231,"I'm going to try installing CUDA 11.3 on the CentOS7 machine first, to confirm whether that will address the issue. I followed: https://developer.nvidia.com/cuda-11.3.0-download-archive?target_os=Linux. ```. wget https://developer.download.nvidia.com/compute/cuda/11.3.0/local_installers/cuda_11.3.0_465.19.01_linux.run. sudo sh cuda_11.3.0_465.19.01_linux.run. ```. ```. export PATH=/usr/local/cuda-11.3/bin:$PATH. export LD_LIBRARY_PATH=/usr/local/cuda-11.3/lib64:$LD_LIBRARY_PATH. sudo ldconfig. ```. ```. [pichuan@pichuan-gpu2 ~]$ nvcc --version. nvcc: NVIDIA (R) Cuda compiler driver. Copyright (c) 2005-2021 NVIDIA Corporation. Built on Sun_Mar_21_19:15:46_PDT_2021. Cuda compilation tools, release 11.3, V11.3.58. Build cuda_11.3.r11.3/compiler.29745058_0. ```. Now, with this, I tried:. ```. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf;print(tf.test.is_gpu_available())'. ```. which still gave me False! Hmm. I'll search the error on the internet to see if I can find something useful",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:1031,availability,error,error,1031,"I'm going to try installing CUDA 11.3 on the CentOS7 machine first, to confirm whether that will address the issue. I followed: https://developer.nvidia.com/cuda-11.3.0-download-archive?target_os=Linux. ```. wget https://developer.download.nvidia.com/compute/cuda/11.3.0/local_installers/cuda_11.3.0_465.19.01_linux.run. sudo sh cuda_11.3.0_465.19.01_linux.run. ```. ```. export PATH=/usr/local/cuda-11.3/bin:$PATH. export LD_LIBRARY_PATH=/usr/local/cuda-11.3/lib64:$LD_LIBRARY_PATH. sudo ldconfig. ```. ```. [pichuan@pichuan-gpu2 ~]$ nvcc --version. nvcc: NVIDIA (R) Cuda compiler driver. Copyright (c) 2005-2021 NVIDIA Corporation. Built on Sun_Mar_21_19:15:46_PDT_2021. Cuda compilation tools, release 11.3, V11.3.58. Build cuda_11.3.r11.3/compiler.29745058_0. ```. Now, with this, I tried:. ```. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf;print(tf.test.is_gpu_available())'. ```. which still gave me False! Hmm. I'll search the error on the internet to see if I can find something useful",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:17,deployability,instal,installing,17,"I'm going to try installing CUDA 11.3 on the CentOS7 machine first, to confirm whether that will address the issue. I followed: https://developer.nvidia.com/cuda-11.3.0-download-archive?target_os=Linux. ```. wget https://developer.download.nvidia.com/compute/cuda/11.3.0/local_installers/cuda_11.3.0_465.19.01_linux.run. sudo sh cuda_11.3.0_465.19.01_linux.run. ```. ```. export PATH=/usr/local/cuda-11.3/bin:$PATH. export LD_LIBRARY_PATH=/usr/local/cuda-11.3/lib64:$LD_LIBRARY_PATH. sudo ldconfig. ```. ```. [pichuan@pichuan-gpu2 ~]$ nvcc --version. nvcc: NVIDIA (R) Cuda compiler driver. Copyright (c) 2005-2021 NVIDIA Corporation. Built on Sun_Mar_21_19:15:46_PDT_2021. Cuda compilation tools, release 11.3, V11.3.58. Build cuda_11.3.r11.3/compiler.29745058_0. ```. Now, with this, I tried:. ```. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf;print(tf.test.is_gpu_available())'. ```. which still gave me False! Hmm. I'll search the error on the internet to see if I can find something useful",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:542,deployability,version,version,542,"I'm going to try installing CUDA 11.3 on the CentOS7 machine first, to confirm whether that will address the issue. I followed: https://developer.nvidia.com/cuda-11.3.0-download-archive?target_os=Linux. ```. wget https://developer.download.nvidia.com/compute/cuda/11.3.0/local_installers/cuda_11.3.0_465.19.01_linux.run. sudo sh cuda_11.3.0_465.19.01_linux.run. ```. ```. export PATH=/usr/local/cuda-11.3/bin:$PATH. export LD_LIBRARY_PATH=/usr/local/cuda-11.3/lib64:$LD_LIBRARY_PATH. sudo ldconfig. ```. ```. [pichuan@pichuan-gpu2 ~]$ nvcc --version. nvcc: NVIDIA (R) Cuda compiler driver. Copyright (c) 2005-2021 NVIDIA Corporation. Built on Sun_Mar_21_19:15:46_PDT_2021. Cuda compilation tools, release 11.3, V11.3.58. Build cuda_11.3.r11.3/compiler.29745058_0. ```. Now, with this, I tried:. ```. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf;print(tf.test.is_gpu_available())'. ```. which still gave me False! Hmm. I'll search the error on the internet to see if I can find something useful",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:697,deployability,releas,release,697,"I'm going to try installing CUDA 11.3 on the CentOS7 machine first, to confirm whether that will address the issue. I followed: https://developer.nvidia.com/cuda-11.3.0-download-archive?target_os=Linux. ```. wget https://developer.download.nvidia.com/compute/cuda/11.3.0/local_installers/cuda_11.3.0_465.19.01_linux.run. sudo sh cuda_11.3.0_465.19.01_linux.run. ```. ```. export PATH=/usr/local/cuda-11.3/bin:$PATH. export LD_LIBRARY_PATH=/usr/local/cuda-11.3/lib64:$LD_LIBRARY_PATH. sudo ldconfig. ```. ```. [pichuan@pichuan-gpu2 ~]$ nvcc --version. nvcc: NVIDIA (R) Cuda compiler driver. Copyright (c) 2005-2021 NVIDIA Corporation. Built on Sun_Mar_21_19:15:46_PDT_2021. Cuda compilation tools, release 11.3, V11.3.58. Build cuda_11.3.r11.3/compiler.29745058_0. ```. Now, with this, I tried:. ```. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf;print(tf.test.is_gpu_available())'. ```. which still gave me False! Hmm. I'll search the error on the internet to see if I can find something useful",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:721,deployability,Build,Build,721,"I'm going to try installing CUDA 11.3 on the CentOS7 machine first, to confirm whether that will address the issue. I followed: https://developer.nvidia.com/cuda-11.3.0-download-archive?target_os=Linux. ```. wget https://developer.download.nvidia.com/compute/cuda/11.3.0/local_installers/cuda_11.3.0_465.19.01_linux.run. sudo sh cuda_11.3.0_465.19.01_linux.run. ```. ```. export PATH=/usr/local/cuda-11.3/bin:$PATH. export LD_LIBRARY_PATH=/usr/local/cuda-11.3/lib64:$LD_LIBRARY_PATH. sudo ldconfig. ```. ```. [pichuan@pichuan-gpu2 ~]$ nvcc --version. nvcc: NVIDIA (R) Cuda compiler driver. Copyright (c) 2005-2021 NVIDIA Corporation. Built on Sun_Mar_21_19:15:46_PDT_2021. Cuda compilation tools, release 11.3, V11.3.58. Build cuda_11.3.r11.3/compiler.29745058_0. ```. Now, with this, I tried:. ```. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf;print(tf.test.is_gpu_available())'. ```. which still gave me False! Hmm. I'll search the error on the internet to see if I can find something useful",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:902,energy efficiency,gpu,gpu,902,"I'm going to try installing CUDA 11.3 on the CentOS7 machine first, to confirm whether that will address the issue. I followed: https://developer.nvidia.com/cuda-11.3.0-download-archive?target_os=Linux. ```. wget https://developer.download.nvidia.com/compute/cuda/11.3.0/local_installers/cuda_11.3.0_465.19.01_linux.run. sudo sh cuda_11.3.0_465.19.01_linux.run. ```. ```. export PATH=/usr/local/cuda-11.3/bin:$PATH. export LD_LIBRARY_PATH=/usr/local/cuda-11.3/lib64:$LD_LIBRARY_PATH. sudo ldconfig. ```. ```. [pichuan@pichuan-gpu2 ~]$ nvcc --version. nvcc: NVIDIA (R) Cuda compiler driver. Copyright (c) 2005-2021 NVIDIA Corporation. Built on Sun_Mar_21_19:15:46_PDT_2021. Cuda compilation tools, release 11.3, V11.3.58. Build cuda_11.3.r11.3/compiler.29745058_0. ```. Now, with this, I tried:. ```. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf;print(tf.test.is_gpu_available())'. ```. which still gave me False! Hmm. I'll search the error on the internet to see if I can find something useful",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:542,integrability,version,version,542,"I'm going to try installing CUDA 11.3 on the CentOS7 machine first, to confirm whether that will address the issue. I followed: https://developer.nvidia.com/cuda-11.3.0-download-archive?target_os=Linux. ```. wget https://developer.download.nvidia.com/compute/cuda/11.3.0/local_installers/cuda_11.3.0_465.19.01_linux.run. sudo sh cuda_11.3.0_465.19.01_linux.run. ```. ```. export PATH=/usr/local/cuda-11.3/bin:$PATH. export LD_LIBRARY_PATH=/usr/local/cuda-11.3/lib64:$LD_LIBRARY_PATH. sudo ldconfig. ```. ```. [pichuan@pichuan-gpu2 ~]$ nvcc --version. nvcc: NVIDIA (R) Cuda compiler driver. Copyright (c) 2005-2021 NVIDIA Corporation. Built on Sun_Mar_21_19:15:46_PDT_2021. Cuda compilation tools, release 11.3, V11.3.58. Build cuda_11.3.r11.3/compiler.29745058_0. ```. Now, with this, I tried:. ```. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf;print(tf.test.is_gpu_available())'. ```. which still gave me False! Hmm. I'll search the error on the internet to see if I can find something useful",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:542,modifiability,version,version,542,"I'm going to try installing CUDA 11.3 on the CentOS7 machine first, to confirm whether that will address the issue. I followed: https://developer.nvidia.com/cuda-11.3.0-download-archive?target_os=Linux. ```. wget https://developer.download.nvidia.com/compute/cuda/11.3.0/local_installers/cuda_11.3.0_465.19.01_linux.run. sudo sh cuda_11.3.0_465.19.01_linux.run. ```. ```. export PATH=/usr/local/cuda-11.3/bin:$PATH. export LD_LIBRARY_PATH=/usr/local/cuda-11.3/lib64:$LD_LIBRARY_PATH. sudo ldconfig. ```. ```. [pichuan@pichuan-gpu2 ~]$ nvcc --version. nvcc: NVIDIA (R) Cuda compiler driver. Copyright (c) 2005-2021 NVIDIA Corporation. Built on Sun_Mar_21_19:15:46_PDT_2021. Cuda compilation tools, release 11.3, V11.3.58. Build cuda_11.3.r11.3/compiler.29745058_0. ```. Now, with this, I tried:. ```. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf;print(tf.test.is_gpu_available())'. ```. which still gave me False! Hmm. I'll search the error on the internet to see if I can find something useful",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:902,performance,gpu,gpu,902,"I'm going to try installing CUDA 11.3 on the CentOS7 machine first, to confirm whether that will address the issue. I followed: https://developer.nvidia.com/cuda-11.3.0-download-archive?target_os=Linux. ```. wget https://developer.download.nvidia.com/compute/cuda/11.3.0/local_installers/cuda_11.3.0_465.19.01_linux.run. sudo sh cuda_11.3.0_465.19.01_linux.run. ```. ```. export PATH=/usr/local/cuda-11.3/bin:$PATH. export LD_LIBRARY_PATH=/usr/local/cuda-11.3/lib64:$LD_LIBRARY_PATH. sudo ldconfig. ```. ```. [pichuan@pichuan-gpu2 ~]$ nvcc --version. nvcc: NVIDIA (R) Cuda compiler driver. Copyright (c) 2005-2021 NVIDIA Corporation. Built on Sun_Mar_21_19:15:46_PDT_2021. Cuda compilation tools, release 11.3, V11.3.58. Build cuda_11.3.r11.3/compiler.29745058_0. ```. Now, with this, I tried:. ```. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf;print(tf.test.is_gpu_available())'. ```. which still gave me False! Hmm. I'll search the error on the internet to see if I can find something useful",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:1031,performance,error,error,1031,"I'm going to try installing CUDA 11.3 on the CentOS7 machine first, to confirm whether that will address the issue. I followed: https://developer.nvidia.com/cuda-11.3.0-download-archive?target_os=Linux. ```. wget https://developer.download.nvidia.com/compute/cuda/11.3.0/local_installers/cuda_11.3.0_465.19.01_linux.run. sudo sh cuda_11.3.0_465.19.01_linux.run. ```. ```. export PATH=/usr/local/cuda-11.3/bin:$PATH. export LD_LIBRARY_PATH=/usr/local/cuda-11.3/lib64:$LD_LIBRARY_PATH. sudo ldconfig. ```. ```. [pichuan@pichuan-gpu2 ~]$ nvcc --version. nvcc: NVIDIA (R) Cuda compiler driver. Copyright (c) 2005-2021 NVIDIA Corporation. Built on Sun_Mar_21_19:15:46_PDT_2021. Cuda compilation tools, release 11.3, V11.3.58. Build cuda_11.3.r11.3/compiler.29745058_0. ```. Now, with this, I tried:. ```. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf;print(tf.test.is_gpu_available())'. ```. which still gave me False! Hmm. I'll search the error on the internet to see if I can find something useful",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:951,safety,test,test,951,"I'm going to try installing CUDA 11.3 on the CentOS7 machine first, to confirm whether that will address the issue. I followed: https://developer.nvidia.com/cuda-11.3.0-download-archive?target_os=Linux. ```. wget https://developer.download.nvidia.com/compute/cuda/11.3.0/local_installers/cuda_11.3.0_465.19.01_linux.run. sudo sh cuda_11.3.0_465.19.01_linux.run. ```. ```. export PATH=/usr/local/cuda-11.3/bin:$PATH. export LD_LIBRARY_PATH=/usr/local/cuda-11.3/lib64:$LD_LIBRARY_PATH. sudo ldconfig. ```. ```. [pichuan@pichuan-gpu2 ~]$ nvcc --version. nvcc: NVIDIA (R) Cuda compiler driver. Copyright (c) 2005-2021 NVIDIA Corporation. Built on Sun_Mar_21_19:15:46_PDT_2021. Cuda compilation tools, release 11.3, V11.3.58. Build cuda_11.3.r11.3/compiler.29745058_0. ```. Now, with this, I tried:. ```. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf;print(tf.test.is_gpu_available())'. ```. which still gave me False! Hmm. I'll search the error on the internet to see if I can find something useful",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:1031,safety,error,error,1031,"I'm going to try installing CUDA 11.3 on the CentOS7 machine first, to confirm whether that will address the issue. I followed: https://developer.nvidia.com/cuda-11.3.0-download-archive?target_os=Linux. ```. wget https://developer.download.nvidia.com/compute/cuda/11.3.0/local_installers/cuda_11.3.0_465.19.01_linux.run. sudo sh cuda_11.3.0_465.19.01_linux.run. ```. ```. export PATH=/usr/local/cuda-11.3/bin:$PATH. export LD_LIBRARY_PATH=/usr/local/cuda-11.3/lib64:$LD_LIBRARY_PATH. sudo ldconfig. ```. ```. [pichuan@pichuan-gpu2 ~]$ nvcc --version. nvcc: NVIDIA (R) Cuda compiler driver. Copyright (c) 2005-2021 NVIDIA Corporation. Built on Sun_Mar_21_19:15:46_PDT_2021. Cuda compilation tools, release 11.3, V11.3.58. Build cuda_11.3.r11.3/compiler.29745058_0. ```. Now, with this, I tried:. ```. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf;print(tf.test.is_gpu_available())'. ```. which still gave me False! Hmm. I'll search the error on the internet to see if I can find something useful",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:951,testability,test,test,951,"I'm going to try installing CUDA 11.3 on the CentOS7 machine first, to confirm whether that will address the issue. I followed: https://developer.nvidia.com/cuda-11.3.0-download-archive?target_os=Linux. ```. wget https://developer.download.nvidia.com/compute/cuda/11.3.0/local_installers/cuda_11.3.0_465.19.01_linux.run. sudo sh cuda_11.3.0_465.19.01_linux.run. ```. ```. export PATH=/usr/local/cuda-11.3/bin:$PATH. export LD_LIBRARY_PATH=/usr/local/cuda-11.3/lib64:$LD_LIBRARY_PATH. sudo ldconfig. ```. ```. [pichuan@pichuan-gpu2 ~]$ nvcc --version. nvcc: NVIDIA (R) Cuda compiler driver. Copyright (c) 2005-2021 NVIDIA Corporation. Built on Sun_Mar_21_19:15:46_PDT_2021. Cuda compilation tools, release 11.3, V11.3.58. Build cuda_11.3.r11.3/compiler.29745058_0. ```. Now, with this, I tried:. ```. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf;print(tf.test.is_gpu_available())'. ```. which still gave me False! Hmm. I'll search the error on the internet to see if I can find something useful",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:71,usability,confirm,confirm,71,"I'm going to try installing CUDA 11.3 on the CentOS7 machine first, to confirm whether that will address the issue. I followed: https://developer.nvidia.com/cuda-11.3.0-download-archive?target_os=Linux. ```. wget https://developer.download.nvidia.com/compute/cuda/11.3.0/local_installers/cuda_11.3.0_465.19.01_linux.run. sudo sh cuda_11.3.0_465.19.01_linux.run. ```. ```. export PATH=/usr/local/cuda-11.3/bin:$PATH. export LD_LIBRARY_PATH=/usr/local/cuda-11.3/lib64:$LD_LIBRARY_PATH. sudo ldconfig. ```. ```. [pichuan@pichuan-gpu2 ~]$ nvcc --version. nvcc: NVIDIA (R) Cuda compiler driver. Copyright (c) 2005-2021 NVIDIA Corporation. Built on Sun_Mar_21_19:15:46_PDT_2021. Cuda compilation tools, release 11.3, V11.3.58. Build cuda_11.3.r11.3/compiler.29745058_0. ```. Now, with this, I tried:. ```. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf;print(tf.test.is_gpu_available())'. ```. which still gave me False! Hmm. I'll search the error on the internet to see if I can find something useful",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:690,usability,tool,tools,690,"I'm going to try installing CUDA 11.3 on the CentOS7 machine first, to confirm whether that will address the issue. I followed: https://developer.nvidia.com/cuda-11.3.0-download-archive?target_os=Linux. ```. wget https://developer.download.nvidia.com/compute/cuda/11.3.0/local_installers/cuda_11.3.0_465.19.01_linux.run. sudo sh cuda_11.3.0_465.19.01_linux.run. ```. ```. export PATH=/usr/local/cuda-11.3/bin:$PATH. export LD_LIBRARY_PATH=/usr/local/cuda-11.3/lib64:$LD_LIBRARY_PATH. sudo ldconfig. ```. ```. [pichuan@pichuan-gpu2 ~]$ nvcc --version. nvcc: NVIDIA (R) Cuda compiler driver. Copyright (c) 2005-2021 NVIDIA Corporation. Built on Sun_Mar_21_19:15:46_PDT_2021. Cuda compilation tools, release 11.3, V11.3.58. Build cuda_11.3.r11.3/compiler.29745058_0. ```. Now, with this, I tried:. ```. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf;print(tf.test.is_gpu_available())'. ```. which still gave me False! Hmm. I'll search the error on the internet to see if I can find something useful",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:1031,usability,error,error,1031,"I'm going to try installing CUDA 11.3 on the CentOS7 machine first, to confirm whether that will address the issue. I followed: https://developer.nvidia.com/cuda-11.3.0-download-archive?target_os=Linux. ```. wget https://developer.download.nvidia.com/compute/cuda/11.3.0/local_installers/cuda_11.3.0_465.19.01_linux.run. sudo sh cuda_11.3.0_465.19.01_linux.run. ```. ```. export PATH=/usr/local/cuda-11.3/bin:$PATH. export LD_LIBRARY_PATH=/usr/local/cuda-11.3/lib64:$LD_LIBRARY_PATH. sudo ldconfig. ```. ```. [pichuan@pichuan-gpu2 ~]$ nvcc --version. nvcc: NVIDIA (R) Cuda compiler driver. Copyright (c) 2005-2021 NVIDIA Corporation. Built on Sun_Mar_21_19:15:46_PDT_2021. Cuda compilation tools, release 11.3, V11.3.58. Build cuda_11.3.r11.3/compiler.29745058_0. ```. Now, with this, I tried:. ```. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf;print(tf.test.is_gpu_available())'. ```. which still gave me False! Hmm. I'll search the error on the internet to see if I can find something useful",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:97,deployability,contain,container,97,"Hi @pichuan,. Try dropping first via a shell like this, and check the LD_LIBRARY_PATH inside the container match with the CUDA libraries location there as would be seen by the applications:. singularity shell --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" . Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:295,energy efficiency,gpu,gpu,295,"Hi @pichuan,. Try dropping first via a shell like this, and check the LD_LIBRARY_PATH inside the container match with the CUDA libraries location there as would be seen by the applications:. singularity shell --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" . Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:295,performance,gpu,gpu,295,"Hi @pichuan,. Try dropping first via a shell like this, and check the LD_LIBRARY_PATH inside the container match with the CUDA libraries location there as would be seen by the applications:. singularity shell --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" . Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:400,availability,down,download,400,"I'm going to try out this:. https://stackoverflow.com/questions/67045622/tensorflow-stream-executor-cuda-cuda-driver-cc328-failed-call-to-cuinit-cuda. on the machine where I just installed CUDA 11.3. First, just `sudo yum install nvidia-modprobe` didn't seem to work for me. So I had this workaround first:. ```. sudo su. cat <<EOF > /etc/yum.repos.d/nvidia.repo. [nvidia]. baseurl=https://developer.download.nvidia.com/compute/cuda/repos/rhel7/x86_64/. enabled=1. gpgcheck=0. EOF. ```. And then. ```. sudo yum install nvidia-modprobe. ```. Check version:. ```. nvidia-modprobe --version. ```. shows:. ```. nvidia-modprobe: version 440.118.02. ```. check CUDA version again:. ```. [pichuan@pichuan-gpu2 ~]$ nvcc --version. nvcc: NVIDIA (R) Cuda compiler driver. Copyright (c) 2005-2021 NVIDIA Corporation. Built on Sun_Mar_21_19:15:46_PDT_2021. Cuda compilation tools, release 11.3, V11.3.58. Build cuda_11.3.r11.3/compiler.29745058_0. ```. Try this again:. ```. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf;print(tf.test.is_gpu_available())'. ```. Still false -- didn't seem to help:. ```. 2023-03-16 07:01:52.583106: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. 2023-03-16 07:01:52.583190: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: pichuan-gpu2. 2023-03-16 07:01:52.583209: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: pichuan-gpu2. 2023-03-16 07:01:52.583304: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". 2023-03-16 07:01:52.583352: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 465.19.1. False. ```. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:1334,availability,error,error,1334,"c/yum.repos.d/nvidia.repo. [nvidia]. baseurl=https://developer.download.nvidia.com/compute/cuda/repos/rhel7/x86_64/. enabled=1. gpgcheck=0. EOF. ```. And then. ```. sudo yum install nvidia-modprobe. ```. Check version:. ```. nvidia-modprobe --version. ```. shows:. ```. nvidia-modprobe: version 440.118.02. ```. check CUDA version again:. ```. [pichuan@pichuan-gpu2 ~]$ nvcc --version. nvcc: NVIDIA (R) Cuda compiler driver. Copyright (c) 2005-2021 NVIDIA Corporation. Built on Sun_Mar_21_19:15:46_PDT_2021. Cuda compilation tools, release 11.3, V11.3.58. Build cuda_11.3.r11.3/compiler.29745058_0. ```. Try this again:. ```. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf;print(tf.test.is_gpu_available())'. ```. Still false -- didn't seem to help:. ```. 2023-03-16 07:01:52.583106: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. 2023-03-16 07:01:52.583190: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: pichuan-gpu2. 2023-03-16 07:01:52.583209: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: pichuan-gpu2. 2023-03-16 07:01:52.583304: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". 2023-03-16 07:01:52.583352: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 465.19.1. False. ```. I tried one more thing, which is rebooting after installing nvidia-modprobe. I did:. ```. gcloud compute instances reset --zone us-west1-b pichuan-gpu2. ```. and then ssh back to the machine. ```. BIN_VERSION=1.5.0. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'imp",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:2033,availability,reboot,rebooting,2033,"T_2021. Cuda compilation tools, release 11.3, V11.3.58. Build cuda_11.3.r11.3/compiler.29745058_0. ```. Try this again:. ```. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf;print(tf.test.is_gpu_available())'. ```. Still false -- didn't seem to help:. ```. 2023-03-16 07:01:52.583106: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. 2023-03-16 07:01:52.583190: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: pichuan-gpu2. 2023-03-16 07:01:52.583209: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: pichuan-gpu2. 2023-03-16 07:01:52.583304: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". 2023-03-16 07:01:52.583352: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 465.19.1. False. ```. I tried one more thing, which is rebooting after installing nvidia-modprobe. I did:. ```. gcloud compute instances reset --zone us-west1-b pichuan-gpu2. ```. and then ssh back to the machine. ```. BIN_VERSION=1.5.0. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf;print(tf.test.is_gpu_available())'. ```. Oh wow, that actually worked. --> it returns True. Although, now I'm seeing @pgrosu 's comment above, and started wondering if it could be that restarting means I reset $LD_LIBRARY_PATH (because now it's empty , and the command above still worked). So, my new hypothesis that ""nvidia-modprobe helped"" might not be true. Need to test with clean setup again :). But at least something works now! I just don't exactly know what helped yet :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:36,deployability,stack,stackoverflow,36,"I'm going to try out this:. https://stackoverflow.com/questions/67045622/tensorflow-stream-executor-cuda-cuda-driver-cc328-failed-call-to-cuinit-cuda. on the machine where I just installed CUDA 11.3. First, just `sudo yum install nvidia-modprobe` didn't seem to work for me. So I had this workaround first:. ```. sudo su. cat <<EOF > /etc/yum.repos.d/nvidia.repo. [nvidia]. baseurl=https://developer.download.nvidia.com/compute/cuda/repos/rhel7/x86_64/. enabled=1. gpgcheck=0. EOF. ```. And then. ```. sudo yum install nvidia-modprobe. ```. Check version:. ```. nvidia-modprobe --version. ```. shows:. ```. nvidia-modprobe: version 440.118.02. ```. check CUDA version again:. ```. [pichuan@pichuan-gpu2 ~]$ nvcc --version. nvcc: NVIDIA (R) Cuda compiler driver. Copyright (c) 2005-2021 NVIDIA Corporation. Built on Sun_Mar_21_19:15:46_PDT_2021. Cuda compilation tools, release 11.3, V11.3.58. Build cuda_11.3.r11.3/compiler.29745058_0. ```. Try this again:. ```. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf;print(tf.test.is_gpu_available())'. ```. Still false -- didn't seem to help:. ```. 2023-03-16 07:01:52.583106: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. 2023-03-16 07:01:52.583190: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: pichuan-gpu2. 2023-03-16 07:01:52.583209: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: pichuan-gpu2. 2023-03-16 07:01:52.583304: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". 2023-03-16 07:01:52.583352: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 465.19.1. False. ```. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:123,deployability,fail,failed-call-to-cuinit-cuda,123,"I'm going to try out this:. https://stackoverflow.com/questions/67045622/tensorflow-stream-executor-cuda-cuda-driver-cc328-failed-call-to-cuinit-cuda. on the machine where I just installed CUDA 11.3. First, just `sudo yum install nvidia-modprobe` didn't seem to work for me. So I had this workaround first:. ```. sudo su. cat <<EOF > /etc/yum.repos.d/nvidia.repo. [nvidia]. baseurl=https://developer.download.nvidia.com/compute/cuda/repos/rhel7/x86_64/. enabled=1. gpgcheck=0. EOF. ```. And then. ```. sudo yum install nvidia-modprobe. ```. Check version:. ```. nvidia-modprobe --version. ```. shows:. ```. nvidia-modprobe: version 440.118.02. ```. check CUDA version again:. ```. [pichuan@pichuan-gpu2 ~]$ nvcc --version. nvcc: NVIDIA (R) Cuda compiler driver. Copyright (c) 2005-2021 NVIDIA Corporation. Built on Sun_Mar_21_19:15:46_PDT_2021. Cuda compilation tools, release 11.3, V11.3.58. Build cuda_11.3.r11.3/compiler.29745058_0. ```. Try this again:. ```. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf;print(tf.test.is_gpu_available())'. ```. Still false -- didn't seem to help:. ```. 2023-03-16 07:01:52.583106: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. 2023-03-16 07:01:52.583190: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: pichuan-gpu2. 2023-03-16 07:01:52.583209: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: pichuan-gpu2. 2023-03-16 07:01:52.583304: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". 2023-03-16 07:01:52.583352: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 465.19.1. False. ```. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:179,deployability,instal,installed,179,"I'm going to try out this:. https://stackoverflow.com/questions/67045622/tensorflow-stream-executor-cuda-cuda-driver-cc328-failed-call-to-cuinit-cuda. on the machine where I just installed CUDA 11.3. First, just `sudo yum install nvidia-modprobe` didn't seem to work for me. So I had this workaround first:. ```. sudo su. cat <<EOF > /etc/yum.repos.d/nvidia.repo. [nvidia]. baseurl=https://developer.download.nvidia.com/compute/cuda/repos/rhel7/x86_64/. enabled=1. gpgcheck=0. EOF. ```. And then. ```. sudo yum install nvidia-modprobe. ```. Check version:. ```. nvidia-modprobe --version. ```. shows:. ```. nvidia-modprobe: version 440.118.02. ```. check CUDA version again:. ```. [pichuan@pichuan-gpu2 ~]$ nvcc --version. nvcc: NVIDIA (R) Cuda compiler driver. Copyright (c) 2005-2021 NVIDIA Corporation. Built on Sun_Mar_21_19:15:46_PDT_2021. Cuda compilation tools, release 11.3, V11.3.58. Build cuda_11.3.r11.3/compiler.29745058_0. ```. Try this again:. ```. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf;print(tf.test.is_gpu_available())'. ```. Still false -- didn't seem to help:. ```. 2023-03-16 07:01:52.583106: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. 2023-03-16 07:01:52.583190: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: pichuan-gpu2. 2023-03-16 07:01:52.583209: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: pichuan-gpu2. 2023-03-16 07:01:52.583304: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". 2023-03-16 07:01:52.583352: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 465.19.1. False. ```. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:222,deployability,instal,install,222,"I'm going to try out this:. https://stackoverflow.com/questions/67045622/tensorflow-stream-executor-cuda-cuda-driver-cc328-failed-call-to-cuinit-cuda. on the machine where I just installed CUDA 11.3. First, just `sudo yum install nvidia-modprobe` didn't seem to work for me. So I had this workaround first:. ```. sudo su. cat <<EOF > /etc/yum.repos.d/nvidia.repo. [nvidia]. baseurl=https://developer.download.nvidia.com/compute/cuda/repos/rhel7/x86_64/. enabled=1. gpgcheck=0. EOF. ```. And then. ```. sudo yum install nvidia-modprobe. ```. Check version:. ```. nvidia-modprobe --version. ```. shows:. ```. nvidia-modprobe: version 440.118.02. ```. check CUDA version again:. ```. [pichuan@pichuan-gpu2 ~]$ nvcc --version. nvcc: NVIDIA (R) Cuda compiler driver. Copyright (c) 2005-2021 NVIDIA Corporation. Built on Sun_Mar_21_19:15:46_PDT_2021. Cuda compilation tools, release 11.3, V11.3.58. Build cuda_11.3.r11.3/compiler.29745058_0. ```. Try this again:. ```. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf;print(tf.test.is_gpu_available())'. ```. Still false -- didn't seem to help:. ```. 2023-03-16 07:01:52.583106: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. 2023-03-16 07:01:52.583190: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: pichuan-gpu2. 2023-03-16 07:01:52.583209: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: pichuan-gpu2. 2023-03-16 07:01:52.583304: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". 2023-03-16 07:01:52.583352: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 465.19.1. False. ```. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:511,deployability,instal,install,511,"I'm going to try out this:. https://stackoverflow.com/questions/67045622/tensorflow-stream-executor-cuda-cuda-driver-cc328-failed-call-to-cuinit-cuda. on the machine where I just installed CUDA 11.3. First, just `sudo yum install nvidia-modprobe` didn't seem to work for me. So I had this workaround first:. ```. sudo su. cat <<EOF > /etc/yum.repos.d/nvidia.repo. [nvidia]. baseurl=https://developer.download.nvidia.com/compute/cuda/repos/rhel7/x86_64/. enabled=1. gpgcheck=0. EOF. ```. And then. ```. sudo yum install nvidia-modprobe. ```. Check version:. ```. nvidia-modprobe --version. ```. shows:. ```. nvidia-modprobe: version 440.118.02. ```. check CUDA version again:. ```. [pichuan@pichuan-gpu2 ~]$ nvcc --version. nvcc: NVIDIA (R) Cuda compiler driver. Copyright (c) 2005-2021 NVIDIA Corporation. Built on Sun_Mar_21_19:15:46_PDT_2021. Cuda compilation tools, release 11.3, V11.3.58. Build cuda_11.3.r11.3/compiler.29745058_0. ```. Try this again:. ```. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf;print(tf.test.is_gpu_available())'. ```. Still false -- didn't seem to help:. ```. 2023-03-16 07:01:52.583106: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. 2023-03-16 07:01:52.583190: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: pichuan-gpu2. 2023-03-16 07:01:52.583209: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: pichuan-gpu2. 2023-03-16 07:01:52.583304: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". 2023-03-16 07:01:52.583352: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 465.19.1. False. ```. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:547,deployability,version,version,547,"I'm going to try out this:. https://stackoverflow.com/questions/67045622/tensorflow-stream-executor-cuda-cuda-driver-cc328-failed-call-to-cuinit-cuda. on the machine where I just installed CUDA 11.3. First, just `sudo yum install nvidia-modprobe` didn't seem to work for me. So I had this workaround first:. ```. sudo su. cat <<EOF > /etc/yum.repos.d/nvidia.repo. [nvidia]. baseurl=https://developer.download.nvidia.com/compute/cuda/repos/rhel7/x86_64/. enabled=1. gpgcheck=0. EOF. ```. And then. ```. sudo yum install nvidia-modprobe. ```. Check version:. ```. nvidia-modprobe --version. ```. shows:. ```. nvidia-modprobe: version 440.118.02. ```. check CUDA version again:. ```. [pichuan@pichuan-gpu2 ~]$ nvcc --version. nvcc: NVIDIA (R) Cuda compiler driver. Copyright (c) 2005-2021 NVIDIA Corporation. Built on Sun_Mar_21_19:15:46_PDT_2021. Cuda compilation tools, release 11.3, V11.3.58. Build cuda_11.3.r11.3/compiler.29745058_0. ```. Try this again:. ```. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf;print(tf.test.is_gpu_available())'. ```. Still false -- didn't seem to help:. ```. 2023-03-16 07:01:52.583106: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. 2023-03-16 07:01:52.583190: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: pichuan-gpu2. 2023-03-16 07:01:52.583209: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: pichuan-gpu2. 2023-03-16 07:01:52.583304: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". 2023-03-16 07:01:52.583352: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 465.19.1. False. ```. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:580,deployability,version,version,580,"I'm going to try out this:. https://stackoverflow.com/questions/67045622/tensorflow-stream-executor-cuda-cuda-driver-cc328-failed-call-to-cuinit-cuda. on the machine where I just installed CUDA 11.3. First, just `sudo yum install nvidia-modprobe` didn't seem to work for me. So I had this workaround first:. ```. sudo su. cat <<EOF > /etc/yum.repos.d/nvidia.repo. [nvidia]. baseurl=https://developer.download.nvidia.com/compute/cuda/repos/rhel7/x86_64/. enabled=1. gpgcheck=0. EOF. ```. And then. ```. sudo yum install nvidia-modprobe. ```. Check version:. ```. nvidia-modprobe --version. ```. shows:. ```. nvidia-modprobe: version 440.118.02. ```. check CUDA version again:. ```. [pichuan@pichuan-gpu2 ~]$ nvcc --version. nvcc: NVIDIA (R) Cuda compiler driver. Copyright (c) 2005-2021 NVIDIA Corporation. Built on Sun_Mar_21_19:15:46_PDT_2021. Cuda compilation tools, release 11.3, V11.3.58. Build cuda_11.3.r11.3/compiler.29745058_0. ```. Try this again:. ```. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf;print(tf.test.is_gpu_available())'. ```. Still false -- didn't seem to help:. ```. 2023-03-16 07:01:52.583106: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. 2023-03-16 07:01:52.583190: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: pichuan-gpu2. 2023-03-16 07:01:52.583209: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: pichuan-gpu2. 2023-03-16 07:01:52.583304: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". 2023-03-16 07:01:52.583352: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 465.19.1. False. ```. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:624,deployability,version,version,624,"I'm going to try out this:. https://stackoverflow.com/questions/67045622/tensorflow-stream-executor-cuda-cuda-driver-cc328-failed-call-to-cuinit-cuda. on the machine where I just installed CUDA 11.3. First, just `sudo yum install nvidia-modprobe` didn't seem to work for me. So I had this workaround first:. ```. sudo su. cat <<EOF > /etc/yum.repos.d/nvidia.repo. [nvidia]. baseurl=https://developer.download.nvidia.com/compute/cuda/repos/rhel7/x86_64/. enabled=1. gpgcheck=0. EOF. ```. And then. ```. sudo yum install nvidia-modprobe. ```. Check version:. ```. nvidia-modprobe --version. ```. shows:. ```. nvidia-modprobe: version 440.118.02. ```. check CUDA version again:. ```. [pichuan@pichuan-gpu2 ~]$ nvcc --version. nvcc: NVIDIA (R) Cuda compiler driver. Copyright (c) 2005-2021 NVIDIA Corporation. Built on Sun_Mar_21_19:15:46_PDT_2021. Cuda compilation tools, release 11.3, V11.3.58. Build cuda_11.3.r11.3/compiler.29745058_0. ```. Try this again:. ```. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf;print(tf.test.is_gpu_available())'. ```. Still false -- didn't seem to help:. ```. 2023-03-16 07:01:52.583106: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. 2023-03-16 07:01:52.583190: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: pichuan-gpu2. 2023-03-16 07:01:52.583209: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: pichuan-gpu2. 2023-03-16 07:01:52.583304: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". 2023-03-16 07:01:52.583352: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 465.19.1. False. ```. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:660,deployability,version,version,660,"I'm going to try out this:. https://stackoverflow.com/questions/67045622/tensorflow-stream-executor-cuda-cuda-driver-cc328-failed-call-to-cuinit-cuda. on the machine where I just installed CUDA 11.3. First, just `sudo yum install nvidia-modprobe` didn't seem to work for me. So I had this workaround first:. ```. sudo su. cat <<EOF > /etc/yum.repos.d/nvidia.repo. [nvidia]. baseurl=https://developer.download.nvidia.com/compute/cuda/repos/rhel7/x86_64/. enabled=1. gpgcheck=0. EOF. ```. And then. ```. sudo yum install nvidia-modprobe. ```. Check version:. ```. nvidia-modprobe --version. ```. shows:. ```. nvidia-modprobe: version 440.118.02. ```. check CUDA version again:. ```. [pichuan@pichuan-gpu2 ~]$ nvcc --version. nvcc: NVIDIA (R) Cuda compiler driver. Copyright (c) 2005-2021 NVIDIA Corporation. Built on Sun_Mar_21_19:15:46_PDT_2021. Cuda compilation tools, release 11.3, V11.3.58. Build cuda_11.3.r11.3/compiler.29745058_0. ```. Try this again:. ```. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf;print(tf.test.is_gpu_available())'. ```. Still false -- didn't seem to help:. ```. 2023-03-16 07:01:52.583106: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. 2023-03-16 07:01:52.583190: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: pichuan-gpu2. 2023-03-16 07:01:52.583209: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: pichuan-gpu2. 2023-03-16 07:01:52.583304: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". 2023-03-16 07:01:52.583352: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 465.19.1. False. ```. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:714,deployability,version,version,714,"I'm going to try out this:. https://stackoverflow.com/questions/67045622/tensorflow-stream-executor-cuda-cuda-driver-cc328-failed-call-to-cuinit-cuda. on the machine where I just installed CUDA 11.3. First, just `sudo yum install nvidia-modprobe` didn't seem to work for me. So I had this workaround first:. ```. sudo su. cat <<EOF > /etc/yum.repos.d/nvidia.repo. [nvidia]. baseurl=https://developer.download.nvidia.com/compute/cuda/repos/rhel7/x86_64/. enabled=1. gpgcheck=0. EOF. ```. And then. ```. sudo yum install nvidia-modprobe. ```. Check version:. ```. nvidia-modprobe --version. ```. shows:. ```. nvidia-modprobe: version 440.118.02. ```. check CUDA version again:. ```. [pichuan@pichuan-gpu2 ~]$ nvcc --version. nvcc: NVIDIA (R) Cuda compiler driver. Copyright (c) 2005-2021 NVIDIA Corporation. Built on Sun_Mar_21_19:15:46_PDT_2021. Cuda compilation tools, release 11.3, V11.3.58. Build cuda_11.3.r11.3/compiler.29745058_0. ```. Try this again:. ```. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf;print(tf.test.is_gpu_available())'. ```. Still false -- didn't seem to help:. ```. 2023-03-16 07:01:52.583106: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. 2023-03-16 07:01:52.583190: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: pichuan-gpu2. 2023-03-16 07:01:52.583209: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: pichuan-gpu2. 2023-03-16 07:01:52.583304: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". 2023-03-16 07:01:52.583352: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 465.19.1. False. ```. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:869,deployability,releas,release,869,"I'm going to try out this:. https://stackoverflow.com/questions/67045622/tensorflow-stream-executor-cuda-cuda-driver-cc328-failed-call-to-cuinit-cuda. on the machine where I just installed CUDA 11.3. First, just `sudo yum install nvidia-modprobe` didn't seem to work for me. So I had this workaround first:. ```. sudo su. cat <<EOF > /etc/yum.repos.d/nvidia.repo. [nvidia]. baseurl=https://developer.download.nvidia.com/compute/cuda/repos/rhel7/x86_64/. enabled=1. gpgcheck=0. EOF. ```. And then. ```. sudo yum install nvidia-modprobe. ```. Check version:. ```. nvidia-modprobe --version. ```. shows:. ```. nvidia-modprobe: version 440.118.02. ```. check CUDA version again:. ```. [pichuan@pichuan-gpu2 ~]$ nvcc --version. nvcc: NVIDIA (R) Cuda compiler driver. Copyright (c) 2005-2021 NVIDIA Corporation. Built on Sun_Mar_21_19:15:46_PDT_2021. Cuda compilation tools, release 11.3, V11.3.58. Build cuda_11.3.r11.3/compiler.29745058_0. ```. Try this again:. ```. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf;print(tf.test.is_gpu_available())'. ```. Still false -- didn't seem to help:. ```. 2023-03-16 07:01:52.583106: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. 2023-03-16 07:01:52.583190: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: pichuan-gpu2. 2023-03-16 07:01:52.583209: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: pichuan-gpu2. 2023-03-16 07:01:52.583304: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". 2023-03-16 07:01:52.583352: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 465.19.1. False. ```. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:893,deployability,Build,Build,893,"I'm going to try out this:. https://stackoverflow.com/questions/67045622/tensorflow-stream-executor-cuda-cuda-driver-cc328-failed-call-to-cuinit-cuda. on the machine where I just installed CUDA 11.3. First, just `sudo yum install nvidia-modprobe` didn't seem to work for me. So I had this workaround first:. ```. sudo su. cat <<EOF > /etc/yum.repos.d/nvidia.repo. [nvidia]. baseurl=https://developer.download.nvidia.com/compute/cuda/repos/rhel7/x86_64/. enabled=1. gpgcheck=0. EOF. ```. And then. ```. sudo yum install nvidia-modprobe. ```. Check version:. ```. nvidia-modprobe --version. ```. shows:. ```. nvidia-modprobe: version 440.118.02. ```. check CUDA version again:. ```. [pichuan@pichuan-gpu2 ~]$ nvcc --version. nvcc: NVIDIA (R) Cuda compiler driver. Copyright (c) 2005-2021 NVIDIA Corporation. Built on Sun_Mar_21_19:15:46_PDT_2021. Cuda compilation tools, release 11.3, V11.3.58. Build cuda_11.3.r11.3/compiler.29745058_0. ```. Try this again:. ```. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf;print(tf.test.is_gpu_available())'. ```. Still false -- didn't seem to help:. ```. 2023-03-16 07:01:52.583106: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. 2023-03-16 07:01:52.583190: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: pichuan-gpu2. 2023-03-16 07:01:52.583209: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: pichuan-gpu2. 2023-03-16 07:01:52.583304: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". 2023-03-16 07:01:52.583352: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 465.19.1. False. ```. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:1283,deployability,fail,failed,1283,"is workaround first:. ```. sudo su. cat <<EOF > /etc/yum.repos.d/nvidia.repo. [nvidia]. baseurl=https://developer.download.nvidia.com/compute/cuda/repos/rhel7/x86_64/. enabled=1. gpgcheck=0. EOF. ```. And then. ```. sudo yum install nvidia-modprobe. ```. Check version:. ```. nvidia-modprobe --version. ```. shows:. ```. nvidia-modprobe: version 440.118.02. ```. check CUDA version again:. ```. [pichuan@pichuan-gpu2 ~]$ nvcc --version. nvcc: NVIDIA (R) Cuda compiler driver. Copyright (c) 2005-2021 NVIDIA Corporation. Built on Sun_Mar_21_19:15:46_PDT_2021. Cuda compilation tools, release 11.3, V11.3.58. Build cuda_11.3.r11.3/compiler.29745058_0. ```. Try this again:. ```. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf;print(tf.test.is_gpu_available())'. ```. Still false -- didn't seem to help:. ```. 2023-03-16 07:01:52.583106: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. 2023-03-16 07:01:52.583190: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: pichuan-gpu2. 2023-03-16 07:01:52.583209: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: pichuan-gpu2. 2023-03-16 07:01:52.583304: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". 2023-03-16 07:01:52.583352: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 465.19.1. False. ```. I tried one more thing, which is rebooting after installing nvidia-modprobe. I did:. ```. gcloud compute instances reset --zone us-west1-b pichuan-gpu2. ```. and then ssh back to the machine. ```. BIN_VERSION=1.5.0. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://goo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:1745,deployability,version,version,1745,"iler driver. Copyright (c) 2005-2021 NVIDIA Corporation. Built on Sun_Mar_21_19:15:46_PDT_2021. Cuda compilation tools, release 11.3, V11.3.58. Build cuda_11.3.r11.3/compiler.29745058_0. ```. Try this again:. ```. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf;print(tf.test.is_gpu_available())'. ```. Still false -- didn't seem to help:. ```. 2023-03-16 07:01:52.583106: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. 2023-03-16 07:01:52.583190: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: pichuan-gpu2. 2023-03-16 07:01:52.583209: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: pichuan-gpu2. 2023-03-16 07:01:52.583304: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". 2023-03-16 07:01:52.583352: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 465.19.1. False. ```. I tried one more thing, which is rebooting after installing nvidia-modprobe. I did:. ```. gcloud compute instances reset --zone us-west1-b pichuan-gpu2. ```. and then ssh back to the machine. ```. BIN_VERSION=1.5.0. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf;print(tf.test.is_gpu_available())'. ```. Oh wow, that actually worked. --> it returns True. Although, now I'm seeing @pgrosu 's comment above, and started wondering if it could be that restarting means I reset $LD_LIBRARY_PATH (because now it's empty , and the command above still worked). So, my new hypothesis that ""nvidia-modprobe helped"" might not be true. Need to test with clean setup ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:1832,deployability,version,version,1832,"DT_2021. Cuda compilation tools, release 11.3, V11.3.58. Build cuda_11.3.r11.3/compiler.29745058_0. ```. Try this again:. ```. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf;print(tf.test.is_gpu_available())'. ```. Still false -- didn't seem to help:. ```. 2023-03-16 07:01:52.583106: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. 2023-03-16 07:01:52.583190: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: pichuan-gpu2. 2023-03-16 07:01:52.583209: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: pichuan-gpu2. 2023-03-16 07:01:52.583304: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". 2023-03-16 07:01:52.583352: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 465.19.1. False. ```. I tried one more thing, which is rebooting after installing nvidia-modprobe. I did:. ```. gcloud compute instances reset --zone us-west1-b pichuan-gpu2. ```. and then ssh back to the machine. ```. BIN_VERSION=1.5.0. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf;print(tf.test.is_gpu_available())'. ```. Oh wow, that actually worked. --> it returns True. Although, now I'm seeing @pgrosu 's comment above, and started wondering if it could be that restarting means I reset $LD_LIBRARY_PATH (because now it's empty , and the command above still worked). So, my new hypothesis that ""nvidia-modprobe helped"" might not be true. Need to test with clean setup again :). But at least something works now! I just don't exactly know what helped yet :",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:1966,deployability,version,version,1966,"T_2021. Cuda compilation tools, release 11.3, V11.3.58. Build cuda_11.3.r11.3/compiler.29745058_0. ```. Try this again:. ```. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf;print(tf.test.is_gpu_available())'. ```. Still false -- didn't seem to help:. ```. 2023-03-16 07:01:52.583106: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. 2023-03-16 07:01:52.583190: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: pichuan-gpu2. 2023-03-16 07:01:52.583209: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: pichuan-gpu2. 2023-03-16 07:01:52.583304: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". 2023-03-16 07:01:52.583352: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 465.19.1. False. ```. I tried one more thing, which is rebooting after installing nvidia-modprobe. I did:. ```. gcloud compute instances reset --zone us-west1-b pichuan-gpu2. ```. and then ssh back to the machine. ```. BIN_VERSION=1.5.0. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf;print(tf.test.is_gpu_available())'. ```. Oh wow, that actually worked. --> it returns True. Although, now I'm seeing @pgrosu 's comment above, and started wondering if it could be that restarting means I reset $LD_LIBRARY_PATH (because now it's empty , and the command above still worked). So, my new hypothesis that ""nvidia-modprobe helped"" might not be true. Need to test with clean setup again :). But at least something works now! I just don't exactly know what helped yet :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:2049,deployability,instal,installing,2049,"T_2021. Cuda compilation tools, release 11.3, V11.3.58. Build cuda_11.3.r11.3/compiler.29745058_0. ```. Try this again:. ```. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf;print(tf.test.is_gpu_available())'. ```. Still false -- didn't seem to help:. ```. 2023-03-16 07:01:52.583106: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. 2023-03-16 07:01:52.583190: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: pichuan-gpu2. 2023-03-16 07:01:52.583209: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: pichuan-gpu2. 2023-03-16 07:01:52.583304: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". 2023-03-16 07:01:52.583352: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 465.19.1. False. ```. I tried one more thing, which is rebooting after installing nvidia-modprobe. I did:. ```. gcloud compute instances reset --zone us-west1-b pichuan-gpu2. ```. and then ssh back to the machine. ```. BIN_VERSION=1.5.0. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf;print(tf.test.is_gpu_available())'. ```. Oh wow, that actually worked. --> it returns True. Although, now I'm seeing @pgrosu 's comment above, and started wondering if it could be that restarting means I reset $LD_LIBRARY_PATH (because now it's empty , and the command above still worked). So, my new hypothesis that ""nvidia-modprobe helped"" might not be true. Need to test with clean setup again :). But at least something works now! I just don't exactly know what helped yet :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:1065,energy efficiency,gpu,gpu,1065,"45622/tensorflow-stream-executor-cuda-cuda-driver-cc328-failed-call-to-cuinit-cuda. on the machine where I just installed CUDA 11.3. First, just `sudo yum install nvidia-modprobe` didn't seem to work for me. So I had this workaround first:. ```. sudo su. cat <<EOF > /etc/yum.repos.d/nvidia.repo. [nvidia]. baseurl=https://developer.download.nvidia.com/compute/cuda/repos/rhel7/x86_64/. enabled=1. gpgcheck=0. EOF. ```. And then. ```. sudo yum install nvidia-modprobe. ```. Check version:. ```. nvidia-modprobe --version. ```. shows:. ```. nvidia-modprobe: version 440.118.02. ```. check CUDA version again:. ```. [pichuan@pichuan-gpu2 ~]$ nvcc --version. nvcc: NVIDIA (R) Cuda compiler driver. Copyright (c) 2005-2021 NVIDIA Corporation. Built on Sun_Mar_21_19:15:46_PDT_2021. Cuda compilation tools, release 11.3, V11.3.58. Build cuda_11.3.r11.3/compiler.29745058_0. ```. Try this again:. ```. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf;print(tf.test.is_gpu_available())'. ```. Still false -- didn't seem to help:. ```. 2023-03-16 07:01:52.583106: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. 2023-03-16 07:01:52.583190: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: pichuan-gpu2. 2023-03-16 07:01:52.583209: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: pichuan-gpu2. 2023-03-16 07:01:52.583304: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". 2023-03-16 07:01:52.583352: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 465.19.1. False. ```. I tried one more thing, which is rebooting after installing nvidia-",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:2318,energy efficiency,gpu,gpu,2318,"T_2021. Cuda compilation tools, release 11.3, V11.3.58. Build cuda_11.3.r11.3/compiler.29745058_0. ```. Try this again:. ```. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf;print(tf.test.is_gpu_available())'. ```. Still false -- didn't seem to help:. ```. 2023-03-16 07:01:52.583106: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. 2023-03-16 07:01:52.583190: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: pichuan-gpu2. 2023-03-16 07:01:52.583209: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: pichuan-gpu2. 2023-03-16 07:01:52.583304: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". 2023-03-16 07:01:52.583352: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 465.19.1. False. ```. I tried one more thing, which is rebooting after installing nvidia-modprobe. I did:. ```. gcloud compute instances reset --zone us-west1-b pichuan-gpu2. ```. and then ssh back to the machine. ```. BIN_VERSION=1.5.0. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf;print(tf.test.is_gpu_available())'. ```. Oh wow, that actually worked. --> it returns True. Although, now I'm seeing @pgrosu 's comment above, and started wondering if it could be that restarting means I reset $LD_LIBRARY_PATH (because now it's empty , and the command above still worked). So, my new hypothesis that ""nvidia-modprobe helped"" might not be true. Need to test with clean setup again :). But at least something works now! I just don't exactly know what helped yet :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:547,integrability,version,version,547,"I'm going to try out this:. https://stackoverflow.com/questions/67045622/tensorflow-stream-executor-cuda-cuda-driver-cc328-failed-call-to-cuinit-cuda. on the machine where I just installed CUDA 11.3. First, just `sudo yum install nvidia-modprobe` didn't seem to work for me. So I had this workaround first:. ```. sudo su. cat <<EOF > /etc/yum.repos.d/nvidia.repo. [nvidia]. baseurl=https://developer.download.nvidia.com/compute/cuda/repos/rhel7/x86_64/. enabled=1. gpgcheck=0. EOF. ```. And then. ```. sudo yum install nvidia-modprobe. ```. Check version:. ```. nvidia-modprobe --version. ```. shows:. ```. nvidia-modprobe: version 440.118.02. ```. check CUDA version again:. ```. [pichuan@pichuan-gpu2 ~]$ nvcc --version. nvcc: NVIDIA (R) Cuda compiler driver. Copyright (c) 2005-2021 NVIDIA Corporation. Built on Sun_Mar_21_19:15:46_PDT_2021. Cuda compilation tools, release 11.3, V11.3.58. Build cuda_11.3.r11.3/compiler.29745058_0. ```. Try this again:. ```. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf;print(tf.test.is_gpu_available())'. ```. Still false -- didn't seem to help:. ```. 2023-03-16 07:01:52.583106: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. 2023-03-16 07:01:52.583190: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: pichuan-gpu2. 2023-03-16 07:01:52.583209: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: pichuan-gpu2. 2023-03-16 07:01:52.583304: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". 2023-03-16 07:01:52.583352: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 465.19.1. False. ```. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:580,integrability,version,version,580,"I'm going to try out this:. https://stackoverflow.com/questions/67045622/tensorflow-stream-executor-cuda-cuda-driver-cc328-failed-call-to-cuinit-cuda. on the machine where I just installed CUDA 11.3. First, just `sudo yum install nvidia-modprobe` didn't seem to work for me. So I had this workaround first:. ```. sudo su. cat <<EOF > /etc/yum.repos.d/nvidia.repo. [nvidia]. baseurl=https://developer.download.nvidia.com/compute/cuda/repos/rhel7/x86_64/. enabled=1. gpgcheck=0. EOF. ```. And then. ```. sudo yum install nvidia-modprobe. ```. Check version:. ```. nvidia-modprobe --version. ```. shows:. ```. nvidia-modprobe: version 440.118.02. ```. check CUDA version again:. ```. [pichuan@pichuan-gpu2 ~]$ nvcc --version. nvcc: NVIDIA (R) Cuda compiler driver. Copyright (c) 2005-2021 NVIDIA Corporation. Built on Sun_Mar_21_19:15:46_PDT_2021. Cuda compilation tools, release 11.3, V11.3.58. Build cuda_11.3.r11.3/compiler.29745058_0. ```. Try this again:. ```. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf;print(tf.test.is_gpu_available())'. ```. Still false -- didn't seem to help:. ```. 2023-03-16 07:01:52.583106: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. 2023-03-16 07:01:52.583190: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: pichuan-gpu2. 2023-03-16 07:01:52.583209: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: pichuan-gpu2. 2023-03-16 07:01:52.583304: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". 2023-03-16 07:01:52.583352: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 465.19.1. False. ```. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:624,integrability,version,version,624,"I'm going to try out this:. https://stackoverflow.com/questions/67045622/tensorflow-stream-executor-cuda-cuda-driver-cc328-failed-call-to-cuinit-cuda. on the machine where I just installed CUDA 11.3. First, just `sudo yum install nvidia-modprobe` didn't seem to work for me. So I had this workaround first:. ```. sudo su. cat <<EOF > /etc/yum.repos.d/nvidia.repo. [nvidia]. baseurl=https://developer.download.nvidia.com/compute/cuda/repos/rhel7/x86_64/. enabled=1. gpgcheck=0. EOF. ```. And then. ```. sudo yum install nvidia-modprobe. ```. Check version:. ```. nvidia-modprobe --version. ```. shows:. ```. nvidia-modprobe: version 440.118.02. ```. check CUDA version again:. ```. [pichuan@pichuan-gpu2 ~]$ nvcc --version. nvcc: NVIDIA (R) Cuda compiler driver. Copyright (c) 2005-2021 NVIDIA Corporation. Built on Sun_Mar_21_19:15:46_PDT_2021. Cuda compilation tools, release 11.3, V11.3.58. Build cuda_11.3.r11.3/compiler.29745058_0. ```. Try this again:. ```. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf;print(tf.test.is_gpu_available())'. ```. Still false -- didn't seem to help:. ```. 2023-03-16 07:01:52.583106: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. 2023-03-16 07:01:52.583190: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: pichuan-gpu2. 2023-03-16 07:01:52.583209: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: pichuan-gpu2. 2023-03-16 07:01:52.583304: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". 2023-03-16 07:01:52.583352: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 465.19.1. False. ```. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:660,integrability,version,version,660,"I'm going to try out this:. https://stackoverflow.com/questions/67045622/tensorflow-stream-executor-cuda-cuda-driver-cc328-failed-call-to-cuinit-cuda. on the machine where I just installed CUDA 11.3. First, just `sudo yum install nvidia-modprobe` didn't seem to work for me. So I had this workaround first:. ```. sudo su. cat <<EOF > /etc/yum.repos.d/nvidia.repo. [nvidia]. baseurl=https://developer.download.nvidia.com/compute/cuda/repos/rhel7/x86_64/. enabled=1. gpgcheck=0. EOF. ```. And then. ```. sudo yum install nvidia-modprobe. ```. Check version:. ```. nvidia-modprobe --version. ```. shows:. ```. nvidia-modprobe: version 440.118.02. ```. check CUDA version again:. ```. [pichuan@pichuan-gpu2 ~]$ nvcc --version. nvcc: NVIDIA (R) Cuda compiler driver. Copyright (c) 2005-2021 NVIDIA Corporation. Built on Sun_Mar_21_19:15:46_PDT_2021. Cuda compilation tools, release 11.3, V11.3.58. Build cuda_11.3.r11.3/compiler.29745058_0. ```. Try this again:. ```. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf;print(tf.test.is_gpu_available())'. ```. Still false -- didn't seem to help:. ```. 2023-03-16 07:01:52.583106: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. 2023-03-16 07:01:52.583190: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: pichuan-gpu2. 2023-03-16 07:01:52.583209: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: pichuan-gpu2. 2023-03-16 07:01:52.583304: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". 2023-03-16 07:01:52.583352: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 465.19.1. False. ```. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:714,integrability,version,version,714,"I'm going to try out this:. https://stackoverflow.com/questions/67045622/tensorflow-stream-executor-cuda-cuda-driver-cc328-failed-call-to-cuinit-cuda. on the machine where I just installed CUDA 11.3. First, just `sudo yum install nvidia-modprobe` didn't seem to work for me. So I had this workaround first:. ```. sudo su. cat <<EOF > /etc/yum.repos.d/nvidia.repo. [nvidia]. baseurl=https://developer.download.nvidia.com/compute/cuda/repos/rhel7/x86_64/. enabled=1. gpgcheck=0. EOF. ```. And then. ```. sudo yum install nvidia-modprobe. ```. Check version:. ```. nvidia-modprobe --version. ```. shows:. ```. nvidia-modprobe: version 440.118.02. ```. check CUDA version again:. ```. [pichuan@pichuan-gpu2 ~]$ nvcc --version. nvcc: NVIDIA (R) Cuda compiler driver. Copyright (c) 2005-2021 NVIDIA Corporation. Built on Sun_Mar_21_19:15:46_PDT_2021. Cuda compilation tools, release 11.3, V11.3.58. Build cuda_11.3.r11.3/compiler.29745058_0. ```. Try this again:. ```. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf;print(tf.test.is_gpu_available())'. ```. Still false -- didn't seem to help:. ```. 2023-03-16 07:01:52.583106: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. 2023-03-16 07:01:52.583190: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: pichuan-gpu2. 2023-03-16 07:01:52.583209: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: pichuan-gpu2. 2023-03-16 07:01:52.583304: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". 2023-03-16 07:01:52.583352: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 465.19.1. False. ```. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:1745,integrability,version,version,1745,"iler driver. Copyright (c) 2005-2021 NVIDIA Corporation. Built on Sun_Mar_21_19:15:46_PDT_2021. Cuda compilation tools, release 11.3, V11.3.58. Build cuda_11.3.r11.3/compiler.29745058_0. ```. Try this again:. ```. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf;print(tf.test.is_gpu_available())'. ```. Still false -- didn't seem to help:. ```. 2023-03-16 07:01:52.583106: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. 2023-03-16 07:01:52.583190: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: pichuan-gpu2. 2023-03-16 07:01:52.583209: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: pichuan-gpu2. 2023-03-16 07:01:52.583304: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". 2023-03-16 07:01:52.583352: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 465.19.1. False. ```. I tried one more thing, which is rebooting after installing nvidia-modprobe. I did:. ```. gcloud compute instances reset --zone us-west1-b pichuan-gpu2. ```. and then ssh back to the machine. ```. BIN_VERSION=1.5.0. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf;print(tf.test.is_gpu_available())'. ```. Oh wow, that actually worked. --> it returns True. Although, now I'm seeing @pgrosu 's comment above, and started wondering if it could be that restarting means I reset $LD_LIBRARY_PATH (because now it's empty , and the command above still worked). So, my new hypothesis that ""nvidia-modprobe helped"" might not be true. Need to test with clean setup ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:1832,integrability,version,version,1832,"DT_2021. Cuda compilation tools, release 11.3, V11.3.58. Build cuda_11.3.r11.3/compiler.29745058_0. ```. Try this again:. ```. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf;print(tf.test.is_gpu_available())'. ```. Still false -- didn't seem to help:. ```. 2023-03-16 07:01:52.583106: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. 2023-03-16 07:01:52.583190: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: pichuan-gpu2. 2023-03-16 07:01:52.583209: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: pichuan-gpu2. 2023-03-16 07:01:52.583304: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". 2023-03-16 07:01:52.583352: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 465.19.1. False. ```. I tried one more thing, which is rebooting after installing nvidia-modprobe. I did:. ```. gcloud compute instances reset --zone us-west1-b pichuan-gpu2. ```. and then ssh back to the machine. ```. BIN_VERSION=1.5.0. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf;print(tf.test.is_gpu_available())'. ```. Oh wow, that actually worked. --> it returns True. Although, now I'm seeing @pgrosu 's comment above, and started wondering if it could be that restarting means I reset $LD_LIBRARY_PATH (because now it's empty , and the command above still worked). So, my new hypothesis that ""nvidia-modprobe helped"" might not be true. Need to test with clean setup again :). But at least something works now! I just don't exactly know what helped yet :",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:1966,integrability,version,version,1966,"T_2021. Cuda compilation tools, release 11.3, V11.3.58. Build cuda_11.3.r11.3/compiler.29745058_0. ```. Try this again:. ```. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf;print(tf.test.is_gpu_available())'. ```. Still false -- didn't seem to help:. ```. 2023-03-16 07:01:52.583106: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. 2023-03-16 07:01:52.583190: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: pichuan-gpu2. 2023-03-16 07:01:52.583209: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: pichuan-gpu2. 2023-03-16 07:01:52.583304: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". 2023-03-16 07:01:52.583352: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 465.19.1. False. ```. I tried one more thing, which is rebooting after installing nvidia-modprobe. I did:. ```. gcloud compute instances reset --zone us-west1-b pichuan-gpu2. ```. and then ssh back to the machine. ```. BIN_VERSION=1.5.0. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf;print(tf.test.is_gpu_available())'. ```. Oh wow, that actually worked. --> it returns True. Although, now I'm seeing @pgrosu 's comment above, and started wondering if it could be that restarting means I reset $LD_LIBRARY_PATH (because now it's empty , and the command above still worked). So, my new hypothesis that ""nvidia-modprobe helped"" might not be true. Need to test with clean setup again :). But at least something works now! I just don't exactly know what helped yet :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:547,modifiability,version,version,547,"I'm going to try out this:. https://stackoverflow.com/questions/67045622/tensorflow-stream-executor-cuda-cuda-driver-cc328-failed-call-to-cuinit-cuda. on the machine where I just installed CUDA 11.3. First, just `sudo yum install nvidia-modprobe` didn't seem to work for me. So I had this workaround first:. ```. sudo su. cat <<EOF > /etc/yum.repos.d/nvidia.repo. [nvidia]. baseurl=https://developer.download.nvidia.com/compute/cuda/repos/rhel7/x86_64/. enabled=1. gpgcheck=0. EOF. ```. And then. ```. sudo yum install nvidia-modprobe. ```. Check version:. ```. nvidia-modprobe --version. ```. shows:. ```. nvidia-modprobe: version 440.118.02. ```. check CUDA version again:. ```. [pichuan@pichuan-gpu2 ~]$ nvcc --version. nvcc: NVIDIA (R) Cuda compiler driver. Copyright (c) 2005-2021 NVIDIA Corporation. Built on Sun_Mar_21_19:15:46_PDT_2021. Cuda compilation tools, release 11.3, V11.3.58. Build cuda_11.3.r11.3/compiler.29745058_0. ```. Try this again:. ```. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf;print(tf.test.is_gpu_available())'. ```. Still false -- didn't seem to help:. ```. 2023-03-16 07:01:52.583106: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. 2023-03-16 07:01:52.583190: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: pichuan-gpu2. 2023-03-16 07:01:52.583209: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: pichuan-gpu2. 2023-03-16 07:01:52.583304: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". 2023-03-16 07:01:52.583352: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 465.19.1. False. ```. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:580,modifiability,version,version,580,"I'm going to try out this:. https://stackoverflow.com/questions/67045622/tensorflow-stream-executor-cuda-cuda-driver-cc328-failed-call-to-cuinit-cuda. on the machine where I just installed CUDA 11.3. First, just `sudo yum install nvidia-modprobe` didn't seem to work for me. So I had this workaround first:. ```. sudo su. cat <<EOF > /etc/yum.repos.d/nvidia.repo. [nvidia]. baseurl=https://developer.download.nvidia.com/compute/cuda/repos/rhel7/x86_64/. enabled=1. gpgcheck=0. EOF. ```. And then. ```. sudo yum install nvidia-modprobe. ```. Check version:. ```. nvidia-modprobe --version. ```. shows:. ```. nvidia-modprobe: version 440.118.02. ```. check CUDA version again:. ```. [pichuan@pichuan-gpu2 ~]$ nvcc --version. nvcc: NVIDIA (R) Cuda compiler driver. Copyright (c) 2005-2021 NVIDIA Corporation. Built on Sun_Mar_21_19:15:46_PDT_2021. Cuda compilation tools, release 11.3, V11.3.58. Build cuda_11.3.r11.3/compiler.29745058_0. ```. Try this again:. ```. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf;print(tf.test.is_gpu_available())'. ```. Still false -- didn't seem to help:. ```. 2023-03-16 07:01:52.583106: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. 2023-03-16 07:01:52.583190: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: pichuan-gpu2. 2023-03-16 07:01:52.583209: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: pichuan-gpu2. 2023-03-16 07:01:52.583304: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". 2023-03-16 07:01:52.583352: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 465.19.1. False. ```. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:624,modifiability,version,version,624,"I'm going to try out this:. https://stackoverflow.com/questions/67045622/tensorflow-stream-executor-cuda-cuda-driver-cc328-failed-call-to-cuinit-cuda. on the machine where I just installed CUDA 11.3. First, just `sudo yum install nvidia-modprobe` didn't seem to work for me. So I had this workaround first:. ```. sudo su. cat <<EOF > /etc/yum.repos.d/nvidia.repo. [nvidia]. baseurl=https://developer.download.nvidia.com/compute/cuda/repos/rhel7/x86_64/. enabled=1. gpgcheck=0. EOF. ```. And then. ```. sudo yum install nvidia-modprobe. ```. Check version:. ```. nvidia-modprobe --version. ```. shows:. ```. nvidia-modprobe: version 440.118.02. ```. check CUDA version again:. ```. [pichuan@pichuan-gpu2 ~]$ nvcc --version. nvcc: NVIDIA (R) Cuda compiler driver. Copyright (c) 2005-2021 NVIDIA Corporation. Built on Sun_Mar_21_19:15:46_PDT_2021. Cuda compilation tools, release 11.3, V11.3.58. Build cuda_11.3.r11.3/compiler.29745058_0. ```. Try this again:. ```. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf;print(tf.test.is_gpu_available())'. ```. Still false -- didn't seem to help:. ```. 2023-03-16 07:01:52.583106: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. 2023-03-16 07:01:52.583190: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: pichuan-gpu2. 2023-03-16 07:01:52.583209: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: pichuan-gpu2. 2023-03-16 07:01:52.583304: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". 2023-03-16 07:01:52.583352: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 465.19.1. False. ```. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:660,modifiability,version,version,660,"I'm going to try out this:. https://stackoverflow.com/questions/67045622/tensorflow-stream-executor-cuda-cuda-driver-cc328-failed-call-to-cuinit-cuda. on the machine where I just installed CUDA 11.3. First, just `sudo yum install nvidia-modprobe` didn't seem to work for me. So I had this workaround first:. ```. sudo su. cat <<EOF > /etc/yum.repos.d/nvidia.repo. [nvidia]. baseurl=https://developer.download.nvidia.com/compute/cuda/repos/rhel7/x86_64/. enabled=1. gpgcheck=0. EOF. ```. And then. ```. sudo yum install nvidia-modprobe. ```. Check version:. ```. nvidia-modprobe --version. ```. shows:. ```. nvidia-modprobe: version 440.118.02. ```. check CUDA version again:. ```. [pichuan@pichuan-gpu2 ~]$ nvcc --version. nvcc: NVIDIA (R) Cuda compiler driver. Copyright (c) 2005-2021 NVIDIA Corporation. Built on Sun_Mar_21_19:15:46_PDT_2021. Cuda compilation tools, release 11.3, V11.3.58. Build cuda_11.3.r11.3/compiler.29745058_0. ```. Try this again:. ```. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf;print(tf.test.is_gpu_available())'. ```. Still false -- didn't seem to help:. ```. 2023-03-16 07:01:52.583106: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. 2023-03-16 07:01:52.583190: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: pichuan-gpu2. 2023-03-16 07:01:52.583209: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: pichuan-gpu2. 2023-03-16 07:01:52.583304: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". 2023-03-16 07:01:52.583352: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 465.19.1. False. ```. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:714,modifiability,version,version,714,"I'm going to try out this:. https://stackoverflow.com/questions/67045622/tensorflow-stream-executor-cuda-cuda-driver-cc328-failed-call-to-cuinit-cuda. on the machine where I just installed CUDA 11.3. First, just `sudo yum install nvidia-modprobe` didn't seem to work for me. So I had this workaround first:. ```. sudo su. cat <<EOF > /etc/yum.repos.d/nvidia.repo. [nvidia]. baseurl=https://developer.download.nvidia.com/compute/cuda/repos/rhel7/x86_64/. enabled=1. gpgcheck=0. EOF. ```. And then. ```. sudo yum install nvidia-modprobe. ```. Check version:. ```. nvidia-modprobe --version. ```. shows:. ```. nvidia-modprobe: version 440.118.02. ```. check CUDA version again:. ```. [pichuan@pichuan-gpu2 ~]$ nvcc --version. nvcc: NVIDIA (R) Cuda compiler driver. Copyright (c) 2005-2021 NVIDIA Corporation. Built on Sun_Mar_21_19:15:46_PDT_2021. Cuda compilation tools, release 11.3, V11.3.58. Build cuda_11.3.r11.3/compiler.29745058_0. ```. Try this again:. ```. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf;print(tf.test.is_gpu_available())'. ```. Still false -- didn't seem to help:. ```. 2023-03-16 07:01:52.583106: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. 2023-03-16 07:01:52.583190: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: pichuan-gpu2. 2023-03-16 07:01:52.583209: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: pichuan-gpu2. 2023-03-16 07:01:52.583304: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". 2023-03-16 07:01:52.583352: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 465.19.1. False. ```. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:1745,modifiability,version,version,1745,"iler driver. Copyright (c) 2005-2021 NVIDIA Corporation. Built on Sun_Mar_21_19:15:46_PDT_2021. Cuda compilation tools, release 11.3, V11.3.58. Build cuda_11.3.r11.3/compiler.29745058_0. ```. Try this again:. ```. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf;print(tf.test.is_gpu_available())'. ```. Still false -- didn't seem to help:. ```. 2023-03-16 07:01:52.583106: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. 2023-03-16 07:01:52.583190: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: pichuan-gpu2. 2023-03-16 07:01:52.583209: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: pichuan-gpu2. 2023-03-16 07:01:52.583304: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". 2023-03-16 07:01:52.583352: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 465.19.1. False. ```. I tried one more thing, which is rebooting after installing nvidia-modprobe. I did:. ```. gcloud compute instances reset --zone us-west1-b pichuan-gpu2. ```. and then ssh back to the machine. ```. BIN_VERSION=1.5.0. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf;print(tf.test.is_gpu_available())'. ```. Oh wow, that actually worked. --> it returns True. Although, now I'm seeing @pgrosu 's comment above, and started wondering if it could be that restarting means I reset $LD_LIBRARY_PATH (because now it's empty , and the command above still worked). So, my new hypothesis that ""nvidia-modprobe helped"" might not be true. Need to test with clean setup ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:1832,modifiability,version,version,1832,"DT_2021. Cuda compilation tools, release 11.3, V11.3.58. Build cuda_11.3.r11.3/compiler.29745058_0. ```. Try this again:. ```. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf;print(tf.test.is_gpu_available())'. ```. Still false -- didn't seem to help:. ```. 2023-03-16 07:01:52.583106: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. 2023-03-16 07:01:52.583190: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: pichuan-gpu2. 2023-03-16 07:01:52.583209: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: pichuan-gpu2. 2023-03-16 07:01:52.583304: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". 2023-03-16 07:01:52.583352: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 465.19.1. False. ```. I tried one more thing, which is rebooting after installing nvidia-modprobe. I did:. ```. gcloud compute instances reset --zone us-west1-b pichuan-gpu2. ```. and then ssh back to the machine. ```. BIN_VERSION=1.5.0. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf;print(tf.test.is_gpu_available())'. ```. Oh wow, that actually worked. --> it returns True. Although, now I'm seeing @pgrosu 's comment above, and started wondering if it could be that restarting means I reset $LD_LIBRARY_PATH (because now it's empty , and the command above still worked). So, my new hypothesis that ""nvidia-modprobe helped"" might not be true. Need to test with clean setup again :). But at least something works now! I just don't exactly know what helped yet :",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:1966,modifiability,version,version,1966,"T_2021. Cuda compilation tools, release 11.3, V11.3.58. Build cuda_11.3.r11.3/compiler.29745058_0. ```. Try this again:. ```. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf;print(tf.test.is_gpu_available())'. ```. Still false -- didn't seem to help:. ```. 2023-03-16 07:01:52.583106: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. 2023-03-16 07:01:52.583190: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: pichuan-gpu2. 2023-03-16 07:01:52.583209: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: pichuan-gpu2. 2023-03-16 07:01:52.583304: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". 2023-03-16 07:01:52.583352: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 465.19.1. False. ```. I tried one more thing, which is rebooting after installing nvidia-modprobe. I did:. ```. gcloud compute instances reset --zone us-west1-b pichuan-gpu2. ```. and then ssh back to the machine. ```. BIN_VERSION=1.5.0. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf;print(tf.test.is_gpu_available())'. ```. Oh wow, that actually worked. --> it returns True. Although, now I'm seeing @pgrosu 's comment above, and started wondering if it could be that restarting means I reset $LD_LIBRARY_PATH (because now it's empty , and the command above still worked). So, my new hypothesis that ""nvidia-modprobe helped"" might not be true. Need to test with clean setup again :). But at least something works now! I just don't exactly know what helped yet :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:1065,performance,gpu,gpu,1065,"45622/tensorflow-stream-executor-cuda-cuda-driver-cc328-failed-call-to-cuinit-cuda. on the machine where I just installed CUDA 11.3. First, just `sudo yum install nvidia-modprobe` didn't seem to work for me. So I had this workaround first:. ```. sudo su. cat <<EOF > /etc/yum.repos.d/nvidia.repo. [nvidia]. baseurl=https://developer.download.nvidia.com/compute/cuda/repos/rhel7/x86_64/. enabled=1. gpgcheck=0. EOF. ```. And then. ```. sudo yum install nvidia-modprobe. ```. Check version:. ```. nvidia-modprobe --version. ```. shows:. ```. nvidia-modprobe: version 440.118.02. ```. check CUDA version again:. ```. [pichuan@pichuan-gpu2 ~]$ nvcc --version. nvcc: NVIDIA (R) Cuda compiler driver. Copyright (c) 2005-2021 NVIDIA Corporation. Built on Sun_Mar_21_19:15:46_PDT_2021. Cuda compilation tools, release 11.3, V11.3.58. Build cuda_11.3.r11.3/compiler.29745058_0. ```. Try this again:. ```. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf;print(tf.test.is_gpu_available())'. ```. Still false -- didn't seem to help:. ```. 2023-03-16 07:01:52.583106: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. 2023-03-16 07:01:52.583190: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: pichuan-gpu2. 2023-03-16 07:01:52.583209: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: pichuan-gpu2. 2023-03-16 07:01:52.583304: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". 2023-03-16 07:01:52.583352: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 465.19.1. False. ```. I tried one more thing, which is rebooting after installing nvidia-",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:1334,performance,error,error,1334,"c/yum.repos.d/nvidia.repo. [nvidia]. baseurl=https://developer.download.nvidia.com/compute/cuda/repos/rhel7/x86_64/. enabled=1. gpgcheck=0. EOF. ```. And then. ```. sudo yum install nvidia-modprobe. ```. Check version:. ```. nvidia-modprobe --version. ```. shows:. ```. nvidia-modprobe: version 440.118.02. ```. check CUDA version again:. ```. [pichuan@pichuan-gpu2 ~]$ nvcc --version. nvcc: NVIDIA (R) Cuda compiler driver. Copyright (c) 2005-2021 NVIDIA Corporation. Built on Sun_Mar_21_19:15:46_PDT_2021. Cuda compilation tools, release 11.3, V11.3.58. Build cuda_11.3.r11.3/compiler.29745058_0. ```. Try this again:. ```. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf;print(tf.test.is_gpu_available())'. ```. Still false -- didn't seem to help:. ```. 2023-03-16 07:01:52.583106: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. 2023-03-16 07:01:52.583190: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: pichuan-gpu2. 2023-03-16 07:01:52.583209: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: pichuan-gpu2. 2023-03-16 07:01:52.583304: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". 2023-03-16 07:01:52.583352: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 465.19.1. False. ```. I tried one more thing, which is rebooting after installing nvidia-modprobe. I did:. ```. gcloud compute instances reset --zone us-west1-b pichuan-gpu2. ```. and then ssh back to the machine. ```. BIN_VERSION=1.5.0. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'imp",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:2318,performance,gpu,gpu,2318,"T_2021. Cuda compilation tools, release 11.3, V11.3.58. Build cuda_11.3.r11.3/compiler.29745058_0. ```. Try this again:. ```. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf;print(tf.test.is_gpu_available())'. ```. Still false -- didn't seem to help:. ```. 2023-03-16 07:01:52.583106: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. 2023-03-16 07:01:52.583190: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: pichuan-gpu2. 2023-03-16 07:01:52.583209: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: pichuan-gpu2. 2023-03-16 07:01:52.583304: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". 2023-03-16 07:01:52.583352: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 465.19.1. False. ```. I tried one more thing, which is rebooting after installing nvidia-modprobe. I did:. ```. gcloud compute instances reset --zone us-west1-b pichuan-gpu2. ```. and then ssh back to the machine. ```. BIN_VERSION=1.5.0. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf;print(tf.test.is_gpu_available())'. ```. Oh wow, that actually worked. --> it returns True. Although, now I'm seeing @pgrosu 's comment above, and started wondering if it could be that restarting means I reset $LD_LIBRARY_PATH (because now it's empty , and the command above still worked). So, my new hypothesis that ""nvidia-modprobe helped"" might not be true. Need to test with clean setup again :). But at least something works now! I just don't exactly know what helped yet :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:123,reliability,fail,failed-call-to-cuinit-cuda,123,"I'm going to try out this:. https://stackoverflow.com/questions/67045622/tensorflow-stream-executor-cuda-cuda-driver-cc328-failed-call-to-cuinit-cuda. on the machine where I just installed CUDA 11.3. First, just `sudo yum install nvidia-modprobe` didn't seem to work for me. So I had this workaround first:. ```. sudo su. cat <<EOF > /etc/yum.repos.d/nvidia.repo. [nvidia]. baseurl=https://developer.download.nvidia.com/compute/cuda/repos/rhel7/x86_64/. enabled=1. gpgcheck=0. EOF. ```. And then. ```. sudo yum install nvidia-modprobe. ```. Check version:. ```. nvidia-modprobe --version. ```. shows:. ```. nvidia-modprobe: version 440.118.02. ```. check CUDA version again:. ```. [pichuan@pichuan-gpu2 ~]$ nvcc --version. nvcc: NVIDIA (R) Cuda compiler driver. Copyright (c) 2005-2021 NVIDIA Corporation. Built on Sun_Mar_21_19:15:46_PDT_2021. Cuda compilation tools, release 11.3, V11.3.58. Build cuda_11.3.r11.3/compiler.29745058_0. ```. Try this again:. ```. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf;print(tf.test.is_gpu_available())'. ```. Still false -- didn't seem to help:. ```. 2023-03-16 07:01:52.583106: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. 2023-03-16 07:01:52.583190: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: pichuan-gpu2. 2023-03-16 07:01:52.583209: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: pichuan-gpu2. 2023-03-16 07:01:52.583304: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". 2023-03-16 07:01:52.583352: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 465.19.1. False. ```. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:1283,reliability,fail,failed,1283,"is workaround first:. ```. sudo su. cat <<EOF > /etc/yum.repos.d/nvidia.repo. [nvidia]. baseurl=https://developer.download.nvidia.com/compute/cuda/repos/rhel7/x86_64/. enabled=1. gpgcheck=0. EOF. ```. And then. ```. sudo yum install nvidia-modprobe. ```. Check version:. ```. nvidia-modprobe --version. ```. shows:. ```. nvidia-modprobe: version 440.118.02. ```. check CUDA version again:. ```. [pichuan@pichuan-gpu2 ~]$ nvcc --version. nvcc: NVIDIA (R) Cuda compiler driver. Copyright (c) 2005-2021 NVIDIA Corporation. Built on Sun_Mar_21_19:15:46_PDT_2021. Cuda compilation tools, release 11.3, V11.3.58. Build cuda_11.3.r11.3/compiler.29745058_0. ```. Try this again:. ```. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf;print(tf.test.is_gpu_available())'. ```. Still false -- didn't seem to help:. ```. 2023-03-16 07:01:52.583106: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. 2023-03-16 07:01:52.583190: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: pichuan-gpu2. 2023-03-16 07:01:52.583209: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: pichuan-gpu2. 2023-03-16 07:01:52.583304: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". 2023-03-16 07:01:52.583352: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 465.19.1. False. ```. I tried one more thing, which is rebooting after installing nvidia-modprobe. I did:. ```. gcloud compute instances reset --zone us-west1-b pichuan-gpu2. ```. and then ssh back to the machine. ```. BIN_VERSION=1.5.0. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://goo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:1457,reliability,diagno,diagnostic,1457,"1. gpgcheck=0. EOF. ```. And then. ```. sudo yum install nvidia-modprobe. ```. Check version:. ```. nvidia-modprobe --version. ```. shows:. ```. nvidia-modprobe: version 440.118.02. ```. check CUDA version again:. ```. [pichuan@pichuan-gpu2 ~]$ nvcc --version. nvcc: NVIDIA (R) Cuda compiler driver. Copyright (c) 2005-2021 NVIDIA Corporation. Built on Sun_Mar_21_19:15:46_PDT_2021. Cuda compilation tools, release 11.3, V11.3.58. Build cuda_11.3.r11.3/compiler.29745058_0. ```. Try this again:. ```. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf;print(tf.test.is_gpu_available())'. ```. Still false -- didn't seem to help:. ```. 2023-03-16 07:01:52.583106: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. 2023-03-16 07:01:52.583190: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: pichuan-gpu2. 2023-03-16 07:01:52.583209: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: pichuan-gpu2. 2023-03-16 07:01:52.583304: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". 2023-03-16 07:01:52.583352: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 465.19.1. False. ```. I tried one more thing, which is rebooting after installing nvidia-modprobe. I did:. ```. gcloud compute instances reset --zone us-west1-b pichuan-gpu2. ```. and then ssh back to the machine. ```. BIN_VERSION=1.5.0. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf;print(tf.test.is_gpu_available())'. ```. Oh wow, that actually worked. --> it returns True. Although, no",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:1114,safety,test,test,1114,"-cc328-failed-call-to-cuinit-cuda. on the machine where I just installed CUDA 11.3. First, just `sudo yum install nvidia-modprobe` didn't seem to work for me. So I had this workaround first:. ```. sudo su. cat <<EOF > /etc/yum.repos.d/nvidia.repo. [nvidia]. baseurl=https://developer.download.nvidia.com/compute/cuda/repos/rhel7/x86_64/. enabled=1. gpgcheck=0. EOF. ```. And then. ```. sudo yum install nvidia-modprobe. ```. Check version:. ```. nvidia-modprobe --version. ```. shows:. ```. nvidia-modprobe: version 440.118.02. ```. check CUDA version again:. ```. [pichuan@pichuan-gpu2 ~]$ nvcc --version. nvcc: NVIDIA (R) Cuda compiler driver. Copyright (c) 2005-2021 NVIDIA Corporation. Built on Sun_Mar_21_19:15:46_PDT_2021. Cuda compilation tools, release 11.3, V11.3.58. Build cuda_11.3.r11.3/compiler.29745058_0. ```. Try this again:. ```. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf;print(tf.test.is_gpu_available())'. ```. Still false -- didn't seem to help:. ```. 2023-03-16 07:01:52.583106: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. 2023-03-16 07:01:52.583190: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: pichuan-gpu2. 2023-03-16 07:01:52.583209: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: pichuan-gpu2. 2023-03-16 07:01:52.583304: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". 2023-03-16 07:01:52.583352: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 465.19.1. False. ```. I tried one more thing, which is rebooting after installing nvidia-modprobe. I did:. ```. gcloud compute instances r",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:1334,safety,error,error,1334,"c/yum.repos.d/nvidia.repo. [nvidia]. baseurl=https://developer.download.nvidia.com/compute/cuda/repos/rhel7/x86_64/. enabled=1. gpgcheck=0. EOF. ```. And then. ```. sudo yum install nvidia-modprobe. ```. Check version:. ```. nvidia-modprobe --version. ```. shows:. ```. nvidia-modprobe: version 440.118.02. ```. check CUDA version again:. ```. [pichuan@pichuan-gpu2 ~]$ nvcc --version. nvcc: NVIDIA (R) Cuda compiler driver. Copyright (c) 2005-2021 NVIDIA Corporation. Built on Sun_Mar_21_19:15:46_PDT_2021. Cuda compilation tools, release 11.3, V11.3.58. Build cuda_11.3.r11.3/compiler.29745058_0. ```. Try this again:. ```. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf;print(tf.test.is_gpu_available())'. ```. Still false -- didn't seem to help:. ```. 2023-03-16 07:01:52.583106: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. 2023-03-16 07:01:52.583190: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: pichuan-gpu2. 2023-03-16 07:01:52.583209: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: pichuan-gpu2. 2023-03-16 07:01:52.583304: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". 2023-03-16 07:01:52.583352: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 465.19.1. False. ```. I tried one more thing, which is rebooting after installing nvidia-modprobe. I did:. ```. gcloud compute instances reset --zone us-west1-b pichuan-gpu2. ```. and then ssh back to the machine. ```. BIN_VERSION=1.5.0. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'imp",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:2367,safety,test,test,2367,"T_2021. Cuda compilation tools, release 11.3, V11.3.58. Build cuda_11.3.r11.3/compiler.29745058_0. ```. Try this again:. ```. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf;print(tf.test.is_gpu_available())'. ```. Still false -- didn't seem to help:. ```. 2023-03-16 07:01:52.583106: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. 2023-03-16 07:01:52.583190: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: pichuan-gpu2. 2023-03-16 07:01:52.583209: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: pichuan-gpu2. 2023-03-16 07:01:52.583304: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". 2023-03-16 07:01:52.583352: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 465.19.1. False. ```. I tried one more thing, which is rebooting after installing nvidia-modprobe. I did:. ```. gcloud compute instances reset --zone us-west1-b pichuan-gpu2. ```. and then ssh back to the machine. ```. BIN_VERSION=1.5.0. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf;print(tf.test.is_gpu_available())'. ```. Oh wow, that actually worked. --> it returns True. Although, now I'm seeing @pgrosu 's comment above, and started wondering if it could be that restarting means I reset $LD_LIBRARY_PATH (because now it's empty , and the command above still worked). So, my new hypothesis that ""nvidia-modprobe helped"" might not be true. Need to test with clean setup again :). But at least something works now! I just don't exactly know what helped yet :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:2727,safety,test,test,2727,"T_2021. Cuda compilation tools, release 11.3, V11.3.58. Build cuda_11.3.r11.3/compiler.29745058_0. ```. Try this again:. ```. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf;print(tf.test.is_gpu_available())'. ```. Still false -- didn't seem to help:. ```. 2023-03-16 07:01:52.583106: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. 2023-03-16 07:01:52.583190: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: pichuan-gpu2. 2023-03-16 07:01:52.583209: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: pichuan-gpu2. 2023-03-16 07:01:52.583304: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". 2023-03-16 07:01:52.583352: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 465.19.1. False. ```. I tried one more thing, which is rebooting after installing nvidia-modprobe. I did:. ```. gcloud compute instances reset --zone us-west1-b pichuan-gpu2. ```. and then ssh back to the machine. ```. BIN_VERSION=1.5.0. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf;print(tf.test.is_gpu_available())'. ```. Oh wow, that actually worked. --> it returns True. Although, now I'm seeing @pgrosu 's comment above, and started wondering if it could be that restarting means I reset $LD_LIBRARY_PATH (because now it's empty , and the command above still worked). So, my new hypothesis that ""nvidia-modprobe helped"" might not be true. Need to test with clean setup again :). But at least something works now! I just don't exactly know what helped yet :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:2167,security,ssh,ssh,2167,"T_2021. Cuda compilation tools, release 11.3, V11.3.58. Build cuda_11.3.r11.3/compiler.29745058_0. ```. Try this again:. ```. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf;print(tf.test.is_gpu_available())'. ```. Still false -- didn't seem to help:. ```. 2023-03-16 07:01:52.583106: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. 2023-03-16 07:01:52.583190: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: pichuan-gpu2. 2023-03-16 07:01:52.583209: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: pichuan-gpu2. 2023-03-16 07:01:52.583304: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". 2023-03-16 07:01:52.583352: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 465.19.1. False. ```. I tried one more thing, which is rebooting after installing nvidia-modprobe. I did:. ```. gcloud compute instances reset --zone us-west1-b pichuan-gpu2. ```. and then ssh back to the machine. ```. BIN_VERSION=1.5.0. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf;print(tf.test.is_gpu_available())'. ```. Oh wow, that actually worked. --> it returns True. Although, now I'm seeing @pgrosu 's comment above, and started wondering if it could be that restarting means I reset $LD_LIBRARY_PATH (because now it's empty , and the command above still worked). So, my new hypothesis that ""nvidia-modprobe helped"" might not be true. Need to test with clean setup again :). But at least something works now! I just don't exactly know what helped yet :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:1114,testability,test,test,1114,"-cc328-failed-call-to-cuinit-cuda. on the machine where I just installed CUDA 11.3. First, just `sudo yum install nvidia-modprobe` didn't seem to work for me. So I had this workaround first:. ```. sudo su. cat <<EOF > /etc/yum.repos.d/nvidia.repo. [nvidia]. baseurl=https://developer.download.nvidia.com/compute/cuda/repos/rhel7/x86_64/. enabled=1. gpgcheck=0. EOF. ```. And then. ```. sudo yum install nvidia-modprobe. ```. Check version:. ```. nvidia-modprobe --version. ```. shows:. ```. nvidia-modprobe: version 440.118.02. ```. check CUDA version again:. ```. [pichuan@pichuan-gpu2 ~]$ nvcc --version. nvcc: NVIDIA (R) Cuda compiler driver. Copyright (c) 2005-2021 NVIDIA Corporation. Built on Sun_Mar_21_19:15:46_PDT_2021. Cuda compilation tools, release 11.3, V11.3.58. Build cuda_11.3.r11.3/compiler.29745058_0. ```. Try this again:. ```. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf;print(tf.test.is_gpu_available())'. ```. Still false -- didn't seem to help:. ```. 2023-03-16 07:01:52.583106: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. 2023-03-16 07:01:52.583190: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: pichuan-gpu2. 2023-03-16 07:01:52.583209: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: pichuan-gpu2. 2023-03-16 07:01:52.583304: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". 2023-03-16 07:01:52.583352: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 465.19.1. False. ```. I tried one more thing, which is rebooting after installing nvidia-modprobe. I did:. ```. gcloud compute instances r",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:1457,testability,diagno,diagnostic,1457,"1. gpgcheck=0. EOF. ```. And then. ```. sudo yum install nvidia-modprobe. ```. Check version:. ```. nvidia-modprobe --version. ```. shows:. ```. nvidia-modprobe: version 440.118.02. ```. check CUDA version again:. ```. [pichuan@pichuan-gpu2 ~]$ nvcc --version. nvcc: NVIDIA (R) Cuda compiler driver. Copyright (c) 2005-2021 NVIDIA Corporation. Built on Sun_Mar_21_19:15:46_PDT_2021. Cuda compilation tools, release 11.3, V11.3.58. Build cuda_11.3.r11.3/compiler.29745058_0. ```. Try this again:. ```. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf;print(tf.test.is_gpu_available())'. ```. Still false -- didn't seem to help:. ```. 2023-03-16 07:01:52.583106: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. 2023-03-16 07:01:52.583190: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: pichuan-gpu2. 2023-03-16 07:01:52.583209: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: pichuan-gpu2. 2023-03-16 07:01:52.583304: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". 2023-03-16 07:01:52.583352: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 465.19.1. False. ```. I tried one more thing, which is rebooting after installing nvidia-modprobe. I did:. ```. gcloud compute instances reset --zone us-west1-b pichuan-gpu2. ```. and then ssh back to the machine. ```. BIN_VERSION=1.5.0. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf;print(tf.test.is_gpu_available())'. ```. Oh wow, that actually worked. --> it returns True. Although, no",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:2367,testability,test,test,2367,"T_2021. Cuda compilation tools, release 11.3, V11.3.58. Build cuda_11.3.r11.3/compiler.29745058_0. ```. Try this again:. ```. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf;print(tf.test.is_gpu_available())'. ```. Still false -- didn't seem to help:. ```. 2023-03-16 07:01:52.583106: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. 2023-03-16 07:01:52.583190: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: pichuan-gpu2. 2023-03-16 07:01:52.583209: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: pichuan-gpu2. 2023-03-16 07:01:52.583304: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". 2023-03-16 07:01:52.583352: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 465.19.1. False. ```. I tried one more thing, which is rebooting after installing nvidia-modprobe. I did:. ```. gcloud compute instances reset --zone us-west1-b pichuan-gpu2. ```. and then ssh back to the machine. ```. BIN_VERSION=1.5.0. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf;print(tf.test.is_gpu_available())'. ```. Oh wow, that actually worked. --> it returns True. Although, now I'm seeing @pgrosu 's comment above, and started wondering if it could be that restarting means I reset $LD_LIBRARY_PATH (because now it's empty , and the command above still worked). So, my new hypothesis that ""nvidia-modprobe helped"" might not be true. Need to test with clean setup again :). But at least something works now! I just don't exactly know what helped yet :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:2727,testability,test,test,2727,"T_2021. Cuda compilation tools, release 11.3, V11.3.58. Build cuda_11.3.r11.3/compiler.29745058_0. ```. Try this again:. ```. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf;print(tf.test.is_gpu_available())'. ```. Still false -- didn't seem to help:. ```. 2023-03-16 07:01:52.583106: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. 2023-03-16 07:01:52.583190: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: pichuan-gpu2. 2023-03-16 07:01:52.583209: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: pichuan-gpu2. 2023-03-16 07:01:52.583304: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". 2023-03-16 07:01:52.583352: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 465.19.1. False. ```. I tried one more thing, which is rebooting after installing nvidia-modprobe. I did:. ```. gcloud compute instances reset --zone us-west1-b pichuan-gpu2. ```. and then ssh back to the machine. ```. BIN_VERSION=1.5.0. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf;print(tf.test.is_gpu_available())'. ```. Oh wow, that actually worked. --> it returns True. Although, now I'm seeing @pgrosu 's comment above, and started wondering if it could be that restarting means I reset $LD_LIBRARY_PATH (because now it's empty , and the command above still worked). So, my new hypothesis that ""nvidia-modprobe helped"" might not be true. Need to test with clean setup again :). But at least something works now! I just don't exactly know what helped yet :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:862,usability,tool,tools,862,"I'm going to try out this:. https://stackoverflow.com/questions/67045622/tensorflow-stream-executor-cuda-cuda-driver-cc328-failed-call-to-cuinit-cuda. on the machine where I just installed CUDA 11.3. First, just `sudo yum install nvidia-modprobe` didn't seem to work for me. So I had this workaround first:. ```. sudo su. cat <<EOF > /etc/yum.repos.d/nvidia.repo. [nvidia]. baseurl=https://developer.download.nvidia.com/compute/cuda/repos/rhel7/x86_64/. enabled=1. gpgcheck=0. EOF. ```. And then. ```. sudo yum install nvidia-modprobe. ```. Check version:. ```. nvidia-modprobe --version. ```. shows:. ```. nvidia-modprobe: version 440.118.02. ```. check CUDA version again:. ```. [pichuan@pichuan-gpu2 ~]$ nvcc --version. nvcc: NVIDIA (R) Cuda compiler driver. Copyright (c) 2005-2021 NVIDIA Corporation. Built on Sun_Mar_21_19:15:46_PDT_2021. Cuda compilation tools, release 11.3, V11.3.58. Build cuda_11.3.r11.3/compiler.29745058_0. ```. Try this again:. ```. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf;print(tf.test.is_gpu_available())'. ```. Still false -- didn't seem to help:. ```. 2023-03-16 07:01:52.583106: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. 2023-03-16 07:01:52.583190: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: pichuan-gpu2. 2023-03-16 07:01:52.583209: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: pichuan-gpu2. 2023-03-16 07:01:52.583304: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". 2023-03-16 07:01:52.583352: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 465.19.1. False. ```. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:1176,usability,help,help,1176," installed CUDA 11.3. First, just `sudo yum install nvidia-modprobe` didn't seem to work for me. So I had this workaround first:. ```. sudo su. cat <<EOF > /etc/yum.repos.d/nvidia.repo. [nvidia]. baseurl=https://developer.download.nvidia.com/compute/cuda/repos/rhel7/x86_64/. enabled=1. gpgcheck=0. EOF. ```. And then. ```. sudo yum install nvidia-modprobe. ```. Check version:. ```. nvidia-modprobe --version. ```. shows:. ```. nvidia-modprobe: version 440.118.02. ```. check CUDA version again:. ```. [pichuan@pichuan-gpu2 ~]$ nvcc --version. nvcc: NVIDIA (R) Cuda compiler driver. Copyright (c) 2005-2021 NVIDIA Corporation. Built on Sun_Mar_21_19:15:46_PDT_2021. Cuda compilation tools, release 11.3, V11.3.58. Build cuda_11.3.r11.3/compiler.29745058_0. ```. Try this again:. ```. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf;print(tf.test.is_gpu_available())'. ```. Still false -- didn't seem to help:. ```. 2023-03-16 07:01:52.583106: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. 2023-03-16 07:01:52.583190: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: pichuan-gpu2. 2023-03-16 07:01:52.583209: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: pichuan-gpu2. 2023-03-16 07:01:52.583304: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". 2023-03-16 07:01:52.583352: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 465.19.1. False. ```. I tried one more thing, which is rebooting after installing nvidia-modprobe. I did:. ```. gcloud compute instances reset --zone us-west1-b pichuan-gpu2. ```. and then ssh back to",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:1334,usability,error,error,1334,"c/yum.repos.d/nvidia.repo. [nvidia]. baseurl=https://developer.download.nvidia.com/compute/cuda/repos/rhel7/x86_64/. enabled=1. gpgcheck=0. EOF. ```. And then. ```. sudo yum install nvidia-modprobe. ```. Check version:. ```. nvidia-modprobe --version. ```. shows:. ```. nvidia-modprobe: version 440.118.02. ```. check CUDA version again:. ```. [pichuan@pichuan-gpu2 ~]$ nvcc --version. nvcc: NVIDIA (R) Cuda compiler driver. Copyright (c) 2005-2021 NVIDIA Corporation. Built on Sun_Mar_21_19:15:46_PDT_2021. Cuda compilation tools, release 11.3, V11.3.58. Build cuda_11.3.r11.3/compiler.29745058_0. ```. Try this again:. ```. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf;print(tf.test.is_gpu_available())'. ```. Still false -- didn't seem to help:. ```. 2023-03-16 07:01:52.583106: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. 2023-03-16 07:01:52.583190: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: pichuan-gpu2. 2023-03-16 07:01:52.583209: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: pichuan-gpu2. 2023-03-16 07:01:52.583304: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". 2023-03-16 07:01:52.583352: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 465.19.1. False. ```. I tried one more thing, which is rebooting after installing nvidia-modprobe. I did:. ```. gcloud compute instances reset --zone us-west1-b pichuan-gpu2. ```. and then ssh back to the machine. ```. BIN_VERSION=1.5.0. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'imp",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:2619,usability,command,command,2619,"T_2021. Cuda compilation tools, release 11.3, V11.3.58. Build cuda_11.3.r11.3/compiler.29745058_0. ```. Try this again:. ```. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf;print(tf.test.is_gpu_available())'. ```. Still false -- didn't seem to help:. ```. 2023-03-16 07:01:52.583106: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. 2023-03-16 07:01:52.583190: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: pichuan-gpu2. 2023-03-16 07:01:52.583209: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: pichuan-gpu2. 2023-03-16 07:01:52.583304: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". 2023-03-16 07:01:52.583352: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 465.19.1. False. ```. I tried one more thing, which is rebooting after installing nvidia-modprobe. I did:. ```. gcloud compute instances reset --zone us-west1-b pichuan-gpu2. ```. and then ssh back to the machine. ```. BIN_VERSION=1.5.0. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf;print(tf.test.is_gpu_available())'. ```. Oh wow, that actually worked. --> it returns True. Although, now I'm seeing @pgrosu 's comment above, and started wondering if it could be that restarting means I reset $LD_LIBRARY_PATH (because now it's empty , and the command above still worked). So, my new hypothesis that ""nvidia-modprobe helped"" might not be true. Need to test with clean setup again :). But at least something works now! I just don't exactly know what helped yet :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:2692,usability,help,helped,2692,"T_2021. Cuda compilation tools, release 11.3, V11.3.58. Build cuda_11.3.r11.3/compiler.29745058_0. ```. Try this again:. ```. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf;print(tf.test.is_gpu_available())'. ```. Still false -- didn't seem to help:. ```. 2023-03-16 07:01:52.583106: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. 2023-03-16 07:01:52.583190: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: pichuan-gpu2. 2023-03-16 07:01:52.583209: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: pichuan-gpu2. 2023-03-16 07:01:52.583304: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". 2023-03-16 07:01:52.583352: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 465.19.1. False. ```. I tried one more thing, which is rebooting after installing nvidia-modprobe. I did:. ```. gcloud compute instances reset --zone us-west1-b pichuan-gpu2. ```. and then ssh back to the machine. ```. BIN_VERSION=1.5.0. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf;print(tf.test.is_gpu_available())'. ```. Oh wow, that actually worked. --> it returns True. Although, now I'm seeing @pgrosu 's comment above, and started wondering if it could be that restarting means I reset $LD_LIBRARY_PATH (because now it's empty , and the command above still worked). So, my new hypothesis that ""nvidia-modprobe helped"" might not be true. Need to test with clean setup again :). But at least something works now! I just don't exactly know what helped yet :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:2824,usability,help,helped,2824,"T_2021. Cuda compilation tools, release 11.3, V11.3.58. Build cuda_11.3.r11.3/compiler.29745058_0. ```. Try this again:. ```. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf;print(tf.test.is_gpu_available())'. ```. Still false -- didn't seem to help:. ```. 2023-03-16 07:01:52.583106: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. 2023-03-16 07:01:52.583190: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: pichuan-gpu2. 2023-03-16 07:01:52.583209: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: pichuan-gpu2. 2023-03-16 07:01:52.583304: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". 2023-03-16 07:01:52.583352: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 465.19.1. False. ```. I tried one more thing, which is rebooting after installing nvidia-modprobe. I did:. ```. gcloud compute instances reset --zone us-west1-b pichuan-gpu2. ```. and then ssh back to the machine. ```. BIN_VERSION=1.5.0. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf;print(tf.test.is_gpu_available())'. ```. Oh wow, that actually worked. --> it returns True. Although, now I'm seeing @pgrosu 's comment above, and started wondering if it could be that restarting means I reset $LD_LIBRARY_PATH (because now it's empty , and the command above still worked). So, my new hypothesis that ""nvidia-modprobe helped"" might not be true. Need to test with clean setup again :). But at least something works now! I just don't exactly know what helped yet :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:285,availability,reboot,rebooting,285,"Hi, @pichuan . I am trying to run with the followings command:. ```. singularity shell -B /usr/lib/locale/:/usr/lib/locale/ --nv ~/data/images/deepvariant_gpu. ```. But it still doesn't work. There are two files in the `/usr/lib/locale/` folder, so I don't know whether it works after rebooting the machine. . ![image](https://user-images.githubusercontent.com/43125963/225547562-bf86b5df-60cd-45d4-8d7c-a6ff986090f4.png). I will ask the HPC manager to reboot the machine. . Thanks a lot!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:453,availability,reboot,reboot,453,"Hi, @pichuan . I am trying to run with the followings command:. ```. singularity shell -B /usr/lib/locale/:/usr/lib/locale/ --nv ~/data/images/deepvariant_gpu. ```. But it still doesn't work. There are two files in the `/usr/lib/locale/` folder, so I don't know whether it works after rebooting the machine. . ![image](https://user-images.githubusercontent.com/43125963/225547562-bf86b5df-60cd-45d4-8d7c-a6ff986090f4.png). I will ask the HPC manager to reboot the machine. . Thanks a lot!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:442,deployability,manag,manager,442,"Hi, @pichuan . I am trying to run with the followings command:. ```. singularity shell -B /usr/lib/locale/:/usr/lib/locale/ --nv ~/data/images/deepvariant_gpu. ```. But it still doesn't work. There are two files in the `/usr/lib/locale/` folder, so I don't know whether it works after rebooting the machine. . ![image](https://user-images.githubusercontent.com/43125963/225547562-bf86b5df-60cd-45d4-8d7c-a6ff986090f4.png). I will ask the HPC manager to reboot the machine. . Thanks a lot!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:442,energy efficiency,manag,manager,442,"Hi, @pichuan . I am trying to run with the followings command:. ```. singularity shell -B /usr/lib/locale/:/usr/lib/locale/ --nv ~/data/images/deepvariant_gpu. ```. But it still doesn't work. There are two files in the `/usr/lib/locale/` folder, so I don't know whether it works after rebooting the machine. . ![image](https://user-images.githubusercontent.com/43125963/225547562-bf86b5df-60cd-45d4-8d7c-a6ff986090f4.png). I will ask the HPC manager to reboot the machine. . Thanks a lot!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:178,reliability,doe,doesn,178,"Hi, @pichuan . I am trying to run with the followings command:. ```. singularity shell -B /usr/lib/locale/:/usr/lib/locale/ --nv ~/data/images/deepvariant_gpu. ```. But it still doesn't work. There are two files in the `/usr/lib/locale/` folder, so I don't know whether it works after rebooting the machine. . ![image](https://user-images.githubusercontent.com/43125963/225547562-bf86b5df-60cd-45d4-8d7c-a6ff986090f4.png). I will ask the HPC manager to reboot the machine. . Thanks a lot!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:442,safety,manag,manager,442,"Hi, @pichuan . I am trying to run with the followings command:. ```. singularity shell -B /usr/lib/locale/:/usr/lib/locale/ --nv ~/data/images/deepvariant_gpu. ```. But it still doesn't work. There are two files in the `/usr/lib/locale/` folder, so I don't know whether it works after rebooting the machine. . ![image](https://user-images.githubusercontent.com/43125963/225547562-bf86b5df-60cd-45d4-8d7c-a6ff986090f4.png). I will ask the HPC manager to reboot the machine. . Thanks a lot!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:54,usability,command,command,54,"Hi, @pichuan . I am trying to run with the followings command:. ```. singularity shell -B /usr/lib/locale/:/usr/lib/locale/ --nv ~/data/images/deepvariant_gpu. ```. But it still doesn't work. There are two files in the `/usr/lib/locale/` folder, so I don't know whether it works after rebooting the machine. . ![image](https://user-images.githubusercontent.com/43125963/225547562-bf86b5df-60cd-45d4-8d7c-a6ff986090f4.png). I will ask the HPC manager to reboot the machine. . Thanks a lot!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:327,usability,user,user-images,327,"Hi, @pichuan . I am trying to run with the followings command:. ```. singularity shell -B /usr/lib/locale/:/usr/lib/locale/ --nv ~/data/images/deepvariant_gpu. ```. But it still doesn't work. There are two files in the `/usr/lib/locale/` folder, so I don't know whether it works after rebooting the machine. . ![image](https://user-images.githubusercontent.com/43125963/225547562-bf86b5df-60cd-45d4-8d7c-a6ff986090f4.png). I will ask the HPC manager to reboot the machine. . Thanks a lot!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:230,reliability,doe,does,230,"Hi @sen1019san,. You shouldn't need to reset the machine. Try to find where the libraries are with the following commands:. ```cd /usr```. ```find | grep cuda | grep lib```. And this should find it if they are under /usr. If that does not find them you can try one higher under the root (/) directory or the /lib one. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:113,usability,command,commands,113,"Hi @sen1019san,. You shouldn't need to reset the machine. Try to find where the libraries are with the following commands:. ```cd /usr```. ```find | grep cuda | grep lib```. And this should find it if they are under /usr. If that does not find them you can try one higher under the root (/) directory or the /lib one. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:341,energy efficiency,gpu,gpu,341,"Hi, @pgrosu. Thanks for your reply. . I find a lot of lib files following your command:. ```. cd /usr. find | grep cuda | grep lib. ```. Here is the output file. [find_cude_lib.txt](https://github.com/google/deepvariant/files/10988201/find_cude_lib.txt). Do you have any suggestion for me to set the variable before starting the deepvariant-gpu singularity program?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:300,modifiability,variab,variable,300,"Hi, @pgrosu. Thanks for your reply. . I find a lot of lib files following your command:. ```. cd /usr. find | grep cuda | grep lib. ```. Here is the output file. [find_cude_lib.txt](https://github.com/google/deepvariant/files/10988201/find_cude_lib.txt). Do you have any suggestion for me to set the variable before starting the deepvariant-gpu singularity program?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:341,performance,gpu,gpu,341,"Hi, @pgrosu. Thanks for your reply. . I find a lot of lib files following your command:. ```. cd /usr. find | grep cuda | grep lib. ```. Here is the output file. [find_cude_lib.txt](https://github.com/google/deepvariant/files/10988201/find_cude_lib.txt). Do you have any suggestion for me to set the variable before starting the deepvariant-gpu singularity program?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:79,usability,command,command,79,"Hi, @pgrosu. Thanks for your reply. . I find a lot of lib files following your command:. ```. cd /usr. find | grep cuda | grep lib. ```. Here is the output file. [find_cude_lib.txt](https://github.com/google/deepvariant/files/10988201/find_cude_lib.txt). Do you have any suggestion for me to set the variable before starting the deepvariant-gpu singularity program?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:92,deployability,fail,fails,92,"Hi @sen1019san ,. Are you starting a fresh instance everytime? In my experience singularity fails to load all the modules for GPUs to be detected. So you can try this before your singularity command:. `nvidia-modprobe -u -c=0`. This will load all the required modules for singularity to see the GPUs. Otherwise, you can run one of the CUDA samples before running the singularity command. Let me know if this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:114,deployability,modul,modules,114,"Hi @sen1019san ,. Are you starting a fresh instance everytime? In my experience singularity fails to load all the modules for GPUs to be detected. So you can try this before your singularity command:. `nvidia-modprobe -u -c=0`. This will load all the required modules for singularity to see the GPUs. Otherwise, you can run one of the CUDA samples before running the singularity command. Let me know if this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:260,deployability,modul,modules,260,"Hi @sen1019san ,. Are you starting a fresh instance everytime? In my experience singularity fails to load all the modules for GPUs to be detected. So you can try this before your singularity command:. `nvidia-modprobe -u -c=0`. This will load all the required modules for singularity to see the GPUs. Otherwise, you can run one of the CUDA samples before running the singularity command. Let me know if this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:101,energy efficiency,load,load,101,"Hi @sen1019san ,. Are you starting a fresh instance everytime? In my experience singularity fails to load all the modules for GPUs to be detected. So you can try this before your singularity command:. `nvidia-modprobe -u -c=0`. This will load all the required modules for singularity to see the GPUs. Otherwise, you can run one of the CUDA samples before running the singularity command. Let me know if this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:126,energy efficiency,GPU,GPUs,126,"Hi @sen1019san ,. Are you starting a fresh instance everytime? In my experience singularity fails to load all the modules for GPUs to be detected. So you can try this before your singularity command:. `nvidia-modprobe -u -c=0`. This will load all the required modules for singularity to see the GPUs. Otherwise, you can run one of the CUDA samples before running the singularity command. Let me know if this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:238,energy efficiency,load,load,238,"Hi @sen1019san ,. Are you starting a fresh instance everytime? In my experience singularity fails to load all the modules for GPUs to be detected. So you can try this before your singularity command:. `nvidia-modprobe -u -c=0`. This will load all the required modules for singularity to see the GPUs. Otherwise, you can run one of the CUDA samples before running the singularity command. Let me know if this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:295,energy efficiency,GPU,GPUs,295,"Hi @sen1019san ,. Are you starting a fresh instance everytime? In my experience singularity fails to load all the modules for GPUs to be detected. So you can try this before your singularity command:. `nvidia-modprobe -u -c=0`. This will load all the required modules for singularity to see the GPUs. Otherwise, you can run one of the CUDA samples before running the singularity command. Let me know if this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:114,modifiability,modul,modules,114,"Hi @sen1019san ,. Are you starting a fresh instance everytime? In my experience singularity fails to load all the modules for GPUs to be detected. So you can try this before your singularity command:. `nvidia-modprobe -u -c=0`. This will load all the required modules for singularity to see the GPUs. Otherwise, you can run one of the CUDA samples before running the singularity command. Let me know if this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:260,modifiability,modul,modules,260,"Hi @sen1019san ,. Are you starting a fresh instance everytime? In my experience singularity fails to load all the modules for GPUs to be detected. So you can try this before your singularity command:. `nvidia-modprobe -u -c=0`. This will load all the required modules for singularity to see the GPUs. Otherwise, you can run one of the CUDA samples before running the singularity command. Let me know if this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:101,performance,load,load,101,"Hi @sen1019san ,. Are you starting a fresh instance everytime? In my experience singularity fails to load all the modules for GPUs to be detected. So you can try this before your singularity command:. `nvidia-modprobe -u -c=0`. This will load all the required modules for singularity to see the GPUs. Otherwise, you can run one of the CUDA samples before running the singularity command. Let me know if this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:126,performance,GPU,GPUs,126,"Hi @sen1019san ,. Are you starting a fresh instance everytime? In my experience singularity fails to load all the modules for GPUs to be detected. So you can try this before your singularity command:. `nvidia-modprobe -u -c=0`. This will load all the required modules for singularity to see the GPUs. Otherwise, you can run one of the CUDA samples before running the singularity command. Let me know if this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:238,performance,load,load,238,"Hi @sen1019san ,. Are you starting a fresh instance everytime? In my experience singularity fails to load all the modules for GPUs to be detected. So you can try this before your singularity command:. `nvidia-modprobe -u -c=0`. This will load all the required modules for singularity to see the GPUs. Otherwise, you can run one of the CUDA samples before running the singularity command. Let me know if this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:295,performance,GPU,GPUs,295,"Hi @sen1019san ,. Are you starting a fresh instance everytime? In my experience singularity fails to load all the modules for GPUs to be detected. So you can try this before your singularity command:. `nvidia-modprobe -u -c=0`. This will load all the required modules for singularity to see the GPUs. Otherwise, you can run one of the CUDA samples before running the singularity command. Let me know if this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:92,reliability,fail,fails,92,"Hi @sen1019san ,. Are you starting a fresh instance everytime? In my experience singularity fails to load all the modules for GPUs to be detected. So you can try this before your singularity command:. `nvidia-modprobe -u -c=0`. This will load all the required modules for singularity to see the GPUs. Otherwise, you can run one of the CUDA samples before running the singularity command. Let me know if this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:114,safety,modul,modules,114,"Hi @sen1019san ,. Are you starting a fresh instance everytime? In my experience singularity fails to load all the modules for GPUs to be detected. So you can try this before your singularity command:. `nvidia-modprobe -u -c=0`. This will load all the required modules for singularity to see the GPUs. Otherwise, you can run one of the CUDA samples before running the singularity command. Let me know if this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:137,safety,detect,detected,137,"Hi @sen1019san ,. Are you starting a fresh instance everytime? In my experience singularity fails to load all the modules for GPUs to be detected. So you can try this before your singularity command:. `nvidia-modprobe -u -c=0`. This will load all the required modules for singularity to see the GPUs. Otherwise, you can run one of the CUDA samples before running the singularity command. Let me know if this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:260,safety,modul,modules,260,"Hi @sen1019san ,. Are you starting a fresh instance everytime? In my experience singularity fails to load all the modules for GPUs to be detected. So you can try this before your singularity command:. `nvidia-modprobe -u -c=0`. This will load all the required modules for singularity to see the GPUs. Otherwise, you can run one of the CUDA samples before running the singularity command. Let me know if this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:137,security,detect,detected,137,"Hi @sen1019san ,. Are you starting a fresh instance everytime? In my experience singularity fails to load all the modules for GPUs to be detected. So you can try this before your singularity command:. `nvidia-modprobe -u -c=0`. This will load all the required modules for singularity to see the GPUs. Otherwise, you can run one of the CUDA samples before running the singularity command. Let me know if this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:69,usability,experien,experience,69,"Hi @sen1019san ,. Are you starting a fresh instance everytime? In my experience singularity fails to load all the modules for GPUs to be detected. So you can try this before your singularity command:. `nvidia-modprobe -u -c=0`. This will load all the required modules for singularity to see the GPUs. Otherwise, you can run one of the CUDA samples before running the singularity command. Let me know if this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:191,usability,command,command,191,"Hi @sen1019san ,. Are you starting a fresh instance everytime? In my experience singularity fails to load all the modules for GPUs to be detected. So you can try this before your singularity command:. `nvidia-modprobe -u -c=0`. This will load all the required modules for singularity to see the GPUs. Otherwise, you can run one of the CUDA samples before running the singularity command. Let me know if this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:379,usability,command,command,379,"Hi @sen1019san ,. Are you starting a fresh instance everytime? In my experience singularity fails to load all the modules for GPUs to be detected. So you can try this before your singularity command:. `nvidia-modprobe -u -c=0`. This will load all the required modules for singularity to see the GPUs. Otherwise, you can run one of the CUDA samples before running the singularity command. Let me know if this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:408,usability,help,helps,408,"Hi @sen1019san ,. Are you starting a fresh instance everytime? In my experience singularity fails to load all the modules for GPUs to be detected. So you can try this before your singularity command:. `nvidia-modprobe -u -c=0`. This will load all the required modules for singularity to see the GPUs. Otherwise, you can run one of the CUDA samples before running the singularity command. Let me know if this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:104,energy efficiency,GPU,GPU,104,"Hi, @pbtrial. Thanks for your reply. It works before I run the singularity. Now the deepvariant can use GPU in singularity.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:104,performance,GPU,GPU,104,"Hi, @pbtrial. Thanks for your reply. It works before I run the singularity. Now the deepvariant can use GPU in singularity.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:90,usability,help,help,90,Great. @sen1019san glad to hear that it works now! And thanks for @pgrosu and @pbtrial 's help!,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/620:889,availability,consist,consistently,889,"Hi @pamelameza . The contents of the INFO field are generally used by other calling methods to describe statistics about the variant position that are used in the process of variant calling (for example ""##INFO=<ID=ClippingRankSum,Number=1,Type=Float,Description=""Z-score From Wilcoxon rank sum test of Alt vs. Ref number of hard clipped bases"">""). . In DeepVariant, instead of directly calculating many of these values to use in a statistical model, the more raw read-level data is directly presented to a neural network and the neural network itself determines what properties in the raw data are useful for variant calling. So we don't populate the INFO field as we don't calculate many of those derived features. One thing that people often use the INFO field for is to filter variant calls by different properties. We've looked into the most effective ways to filter calls, and it is consistently the case that the sample level GQ field is well-calibrated to the probability of a call being correct. If you want to see some data for this, you can look at the calibration in Figure 2 of the DeepVariant paper (https://www.nature.com/articles/nbt.4235) or at the general work in our cohort calling paper (https://academic.oup.com/bioinformatics/article/36/24/5582/6064144). Are there fields in particular you generally use within INFO? It's unlikely we could easily add them, but it might be useful for us to know. Thank you,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/620
https://github.com/google/deepvariant/issues/620:444,energy efficiency,model,model,444,"Hi @pamelameza . The contents of the INFO field are generally used by other calling methods to describe statistics about the variant position that are used in the process of variant calling (for example ""##INFO=<ID=ClippingRankSum,Number=1,Type=Float,Description=""Z-score From Wilcoxon rank sum test of Alt vs. Ref number of hard clipped bases"">""). . In DeepVariant, instead of directly calculating many of these values to use in a statistical model, the more raw read-level data is directly presented to a neural network and the neural network itself determines what properties in the raw data are useful for variant calling. So we don't populate the INFO field as we don't calculate many of those derived features. One thing that people often use the INFO field for is to filter variant calls by different properties. We've looked into the most effective ways to filter calls, and it is consistently the case that the sample level GQ field is well-calibrated to the probability of a call being correct. If you want to see some data for this, you can look at the calibration in Figure 2 of the DeepVariant paper (https://www.nature.com/articles/nbt.4235) or at the general work in our cohort calling paper (https://academic.oup.com/bioinformatics/article/36/24/5582/6064144). Are there fields in particular you generally use within INFO? It's unlikely we could easily add them, but it might be useful for us to know. Thank you,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/620
https://github.com/google/deepvariant/issues/620:774,integrability,filter,filter,774,"Hi @pamelameza . The contents of the INFO field are generally used by other calling methods to describe statistics about the variant position that are used in the process of variant calling (for example ""##INFO=<ID=ClippingRankSum,Number=1,Type=Float,Description=""Z-score From Wilcoxon rank sum test of Alt vs. Ref number of hard clipped bases"">""). . In DeepVariant, instead of directly calculating many of these values to use in a statistical model, the more raw read-level data is directly presented to a neural network and the neural network itself determines what properties in the raw data are useful for variant calling. So we don't populate the INFO field as we don't calculate many of those derived features. One thing that people often use the INFO field for is to filter variant calls by different properties. We've looked into the most effective ways to filter calls, and it is consistently the case that the sample level GQ field is well-calibrated to the probability of a call being correct. If you want to see some data for this, you can look at the calibration in Figure 2 of the DeepVariant paper (https://www.nature.com/articles/nbt.4235) or at the general work in our cohort calling paper (https://academic.oup.com/bioinformatics/article/36/24/5582/6064144). Are there fields in particular you generally use within INFO? It's unlikely we could easily add them, but it might be useful for us to know. Thank you,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/620
https://github.com/google/deepvariant/issues/620:865,integrability,filter,filter,865,"Hi @pamelameza . The contents of the INFO field are generally used by other calling methods to describe statistics about the variant position that are used in the process of variant calling (for example ""##INFO=<ID=ClippingRankSum,Number=1,Type=Float,Description=""Z-score From Wilcoxon rank sum test of Alt vs. Ref number of hard clipped bases"">""). . In DeepVariant, instead of directly calculating many of these values to use in a statistical model, the more raw read-level data is directly presented to a neural network and the neural network itself determines what properties in the raw data are useful for variant calling. So we don't populate the INFO field as we don't calculate many of those derived features. One thing that people often use the INFO field for is to filter variant calls by different properties. We've looked into the most effective ways to filter calls, and it is consistently the case that the sample level GQ field is well-calibrated to the probability of a call being correct. If you want to see some data for this, you can look at the calibration in Figure 2 of the DeepVariant paper (https://www.nature.com/articles/nbt.4235) or at the general work in our cohort calling paper (https://academic.oup.com/bioinformatics/article/36/24/5582/6064144). Are there fields in particular you generally use within INFO? It's unlikely we could easily add them, but it might be useful for us to know. Thank you,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/620
https://github.com/google/deepvariant/issues/620:21,performance,content,contents,21,"Hi @pamelameza . The contents of the INFO field are generally used by other calling methods to describe statistics about the variant position that are used in the process of variant calling (for example ""##INFO=<ID=ClippingRankSum,Number=1,Type=Float,Description=""Z-score From Wilcoxon rank sum test of Alt vs. Ref number of hard clipped bases"">""). . In DeepVariant, instead of directly calculating many of these values to use in a statistical model, the more raw read-level data is directly presented to a neural network and the neural network itself determines what properties in the raw data are useful for variant calling. So we don't populate the INFO field as we don't calculate many of those derived features. One thing that people often use the INFO field for is to filter variant calls by different properties. We've looked into the most effective ways to filter calls, and it is consistently the case that the sample level GQ field is well-calibrated to the probability of a call being correct. If you want to see some data for this, you can look at the calibration in Figure 2 of the DeepVariant paper (https://www.nature.com/articles/nbt.4235) or at the general work in our cohort calling paper (https://academic.oup.com/bioinformatics/article/36/24/5582/6064144). Are there fields in particular you generally use within INFO? It's unlikely we could easily add them, but it might be useful for us to know. Thank you,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/620
https://github.com/google/deepvariant/issues/620:514,performance,network,network,514,"Hi @pamelameza . The contents of the INFO field are generally used by other calling methods to describe statistics about the variant position that are used in the process of variant calling (for example ""##INFO=<ID=ClippingRankSum,Number=1,Type=Float,Description=""Z-score From Wilcoxon rank sum test of Alt vs. Ref number of hard clipped bases"">""). . In DeepVariant, instead of directly calculating many of these values to use in a statistical model, the more raw read-level data is directly presented to a neural network and the neural network itself determines what properties in the raw data are useful for variant calling. So we don't populate the INFO field as we don't calculate many of those derived features. One thing that people often use the INFO field for is to filter variant calls by different properties. We've looked into the most effective ways to filter calls, and it is consistently the case that the sample level GQ field is well-calibrated to the probability of a call being correct. If you want to see some data for this, you can look at the calibration in Figure 2 of the DeepVariant paper (https://www.nature.com/articles/nbt.4235) or at the general work in our cohort calling paper (https://academic.oup.com/bioinformatics/article/36/24/5582/6064144). Are there fields in particular you generally use within INFO? It's unlikely we could easily add them, but it might be useful for us to know. Thank you,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/620
https://github.com/google/deepvariant/issues/620:537,performance,network,network,537,"Hi @pamelameza . The contents of the INFO field are generally used by other calling methods to describe statistics about the variant position that are used in the process of variant calling (for example ""##INFO=<ID=ClippingRankSum,Number=1,Type=Float,Description=""Z-score From Wilcoxon rank sum test of Alt vs. Ref number of hard clipped bases"">""). . In DeepVariant, instead of directly calculating many of these values to use in a statistical model, the more raw read-level data is directly presented to a neural network and the neural network itself determines what properties in the raw data are useful for variant calling. So we don't populate the INFO field as we don't calculate many of those derived features. One thing that people often use the INFO field for is to filter variant calls by different properties. We've looked into the most effective ways to filter calls, and it is consistently the case that the sample level GQ field is well-calibrated to the probability of a call being correct. If you want to see some data for this, you can look at the calibration in Figure 2 of the DeepVariant paper (https://www.nature.com/articles/nbt.4235) or at the general work in our cohort calling paper (https://academic.oup.com/bioinformatics/article/36/24/5582/6064144). Are there fields in particular you generally use within INFO? It's unlikely we could easily add them, but it might be useful for us to know. Thank you,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/620
https://github.com/google/deepvariant/issues/620:295,safety,test,test,295,"Hi @pamelameza . The contents of the INFO field are generally used by other calling methods to describe statistics about the variant position that are used in the process of variant calling (for example ""##INFO=<ID=ClippingRankSum,Number=1,Type=Float,Description=""Z-score From Wilcoxon rank sum test of Alt vs. Ref number of hard clipped bases"">""). . In DeepVariant, instead of directly calculating many of these values to use in a statistical model, the more raw read-level data is directly presented to a neural network and the neural network itself determines what properties in the raw data are useful for variant calling. So we don't populate the INFO field as we don't calculate many of those derived features. One thing that people often use the INFO field for is to filter variant calls by different properties. We've looked into the most effective ways to filter calls, and it is consistently the case that the sample level GQ field is well-calibrated to the probability of a call being correct. If you want to see some data for this, you can look at the calibration in Figure 2 of the DeepVariant paper (https://www.nature.com/articles/nbt.4235) or at the general work in our cohort calling paper (https://academic.oup.com/bioinformatics/article/36/24/5582/6064144). Are there fields in particular you generally use within INFO? It's unlikely we could easily add them, but it might be useful for us to know. Thank you,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/620
https://github.com/google/deepvariant/issues/620:444,security,model,model,444,"Hi @pamelameza . The contents of the INFO field are generally used by other calling methods to describe statistics about the variant position that are used in the process of variant calling (for example ""##INFO=<ID=ClippingRankSum,Number=1,Type=Float,Description=""Z-score From Wilcoxon rank sum test of Alt vs. Ref number of hard clipped bases"">""). . In DeepVariant, instead of directly calculating many of these values to use in a statistical model, the more raw read-level data is directly presented to a neural network and the neural network itself determines what properties in the raw data are useful for variant calling. So we don't populate the INFO field as we don't calculate many of those derived features. One thing that people often use the INFO field for is to filter variant calls by different properties. We've looked into the most effective ways to filter calls, and it is consistently the case that the sample level GQ field is well-calibrated to the probability of a call being correct. If you want to see some data for this, you can look at the calibration in Figure 2 of the DeepVariant paper (https://www.nature.com/articles/nbt.4235) or at the general work in our cohort calling paper (https://academic.oup.com/bioinformatics/article/36/24/5582/6064144). Are there fields in particular you generally use within INFO? It's unlikely we could easily add them, but it might be useful for us to know. Thank you,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/620
https://github.com/google/deepvariant/issues/620:514,security,network,network,514,"Hi @pamelameza . The contents of the INFO field are generally used by other calling methods to describe statistics about the variant position that are used in the process of variant calling (for example ""##INFO=<ID=ClippingRankSum,Number=1,Type=Float,Description=""Z-score From Wilcoxon rank sum test of Alt vs. Ref number of hard clipped bases"">""). . In DeepVariant, instead of directly calculating many of these values to use in a statistical model, the more raw read-level data is directly presented to a neural network and the neural network itself determines what properties in the raw data are useful for variant calling. So we don't populate the INFO field as we don't calculate many of those derived features. One thing that people often use the INFO field for is to filter variant calls by different properties. We've looked into the most effective ways to filter calls, and it is consistently the case that the sample level GQ field is well-calibrated to the probability of a call being correct. If you want to see some data for this, you can look at the calibration in Figure 2 of the DeepVariant paper (https://www.nature.com/articles/nbt.4235) or at the general work in our cohort calling paper (https://academic.oup.com/bioinformatics/article/36/24/5582/6064144). Are there fields in particular you generally use within INFO? It's unlikely we could easily add them, but it might be useful for us to know. Thank you,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/620
https://github.com/google/deepvariant/issues/620:537,security,network,network,537,"Hi @pamelameza . The contents of the INFO field are generally used by other calling methods to describe statistics about the variant position that are used in the process of variant calling (for example ""##INFO=<ID=ClippingRankSum,Number=1,Type=Float,Description=""Z-score From Wilcoxon rank sum test of Alt vs. Ref number of hard clipped bases"">""). . In DeepVariant, instead of directly calculating many of these values to use in a statistical model, the more raw read-level data is directly presented to a neural network and the neural network itself determines what properties in the raw data are useful for variant calling. So we don't populate the INFO field as we don't calculate many of those derived features. One thing that people often use the INFO field for is to filter variant calls by different properties. We've looked into the most effective ways to filter calls, and it is consistently the case that the sample level GQ field is well-calibrated to the probability of a call being correct. If you want to see some data for this, you can look at the calibration in Figure 2 of the DeepVariant paper (https://www.nature.com/articles/nbt.4235) or at the general work in our cohort calling paper (https://academic.oup.com/bioinformatics/article/36/24/5582/6064144). Are there fields in particular you generally use within INFO? It's unlikely we could easily add them, but it might be useful for us to know. Thank you,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/620
https://github.com/google/deepvariant/issues/620:295,testability,test,test,295,"Hi @pamelameza . The contents of the INFO field are generally used by other calling methods to describe statistics about the variant position that are used in the process of variant calling (for example ""##INFO=<ID=ClippingRankSum,Number=1,Type=Float,Description=""Z-score From Wilcoxon rank sum test of Alt vs. Ref number of hard clipped bases"">""). . In DeepVariant, instead of directly calculating many of these values to use in a statistical model, the more raw read-level data is directly presented to a neural network and the neural network itself determines what properties in the raw data are useful for variant calling. So we don't populate the INFO field as we don't calculate many of those derived features. One thing that people often use the INFO field for is to filter variant calls by different properties. We've looked into the most effective ways to filter calls, and it is consistently the case that the sample level GQ field is well-calibrated to the probability of a call being correct. If you want to see some data for this, you can look at the calibration in Figure 2 of the DeepVariant paper (https://www.nature.com/articles/nbt.4235) or at the general work in our cohort calling paper (https://academic.oup.com/bioinformatics/article/36/24/5582/6064144). Are there fields in particular you generally use within INFO? It's unlikely we could easily add them, but it might be useful for us to know. Thank you,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/620
https://github.com/google/deepvariant/issues/620:847,usability,effectiv,effective,847,"Hi @pamelameza . The contents of the INFO field are generally used by other calling methods to describe statistics about the variant position that are used in the process of variant calling (for example ""##INFO=<ID=ClippingRankSum,Number=1,Type=Float,Description=""Z-score From Wilcoxon rank sum test of Alt vs. Ref number of hard clipped bases"">""). . In DeepVariant, instead of directly calculating many of these values to use in a statistical model, the more raw read-level data is directly presented to a neural network and the neural network itself determines what properties in the raw data are useful for variant calling. So we don't populate the INFO field as we don't calculate many of those derived features. One thing that people often use the INFO field for is to filter variant calls by different properties. We've looked into the most effective ways to filter calls, and it is consistently the case that the sample level GQ field is well-calibrated to the probability of a call being correct. If you want to see some data for this, you can look at the calibration in Figure 2 of the DeepVariant paper (https://www.nature.com/articles/nbt.4235) or at the general work in our cohort calling paper (https://academic.oup.com/bioinformatics/article/36/24/5582/6064144). Are there fields in particular you generally use within INFO? It's unlikely we could easily add them, but it might be useful for us to know. Thank you,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/620
https://github.com/google/deepvariant/issues/620:889,usability,consist,consistently,889,"Hi @pamelameza . The contents of the INFO field are generally used by other calling methods to describe statistics about the variant position that are used in the process of variant calling (for example ""##INFO=<ID=ClippingRankSum,Number=1,Type=Float,Description=""Z-score From Wilcoxon rank sum test of Alt vs. Ref number of hard clipped bases"">""). . In DeepVariant, instead of directly calculating many of these values to use in a statistical model, the more raw read-level data is directly presented to a neural network and the neural network itself determines what properties in the raw data are useful for variant calling. So we don't populate the INFO field as we don't calculate many of those derived features. One thing that people often use the INFO field for is to filter variant calls by different properties. We've looked into the most effective ways to filter calls, and it is consistently the case that the sample level GQ field is well-calibrated to the probability of a call being correct. If you want to see some data for this, you can look at the calibration in Figure 2 of the DeepVariant paper (https://www.nature.com/articles/nbt.4235) or at the general work in our cohort calling paper (https://academic.oup.com/bioinformatics/article/36/24/5582/6064144). Are there fields in particular you generally use within INFO? It's unlikely we could easily add them, but it might be useful for us to know. Thank you,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/620
https://github.com/google/deepvariant/issues/622:51,energy efficiency,model,models,51,"If you question is if WGS and WES exactly the same models then the answer is no, they are not the same model, we train WGS model and WES models separately specific for each task.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/622
https://github.com/google/deepvariant/issues/622:103,energy efficiency,model,model,103,"If you question is if WGS and WES exactly the same models then the answer is no, they are not the same model, we train WGS model and WES models separately specific for each task.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/622
https://github.com/google/deepvariant/issues/622:123,energy efficiency,model,model,123,"If you question is if WGS and WES exactly the same models then the answer is no, they are not the same model, we train WGS model and WES models separately specific for each task.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/622
https://github.com/google/deepvariant/issues/622:137,energy efficiency,model,models,137,"If you question is if WGS and WES exactly the same models then the answer is no, they are not the same model, we train WGS model and WES models separately specific for each task.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/622
https://github.com/google/deepvariant/issues/622:155,interoperability,specif,specific,155,"If you question is if WGS and WES exactly the same models then the answer is no, they are not the same model, we train WGS model and WES models separately specific for each task.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/622
https://github.com/google/deepvariant/issues/622:51,security,model,models,51,"If you question is if WGS and WES exactly the same models then the answer is no, they are not the same model, we train WGS model and WES models separately specific for each task.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/622
https://github.com/google/deepvariant/issues/622:103,security,model,model,103,"If you question is if WGS and WES exactly the same models then the answer is no, they are not the same model, we train WGS model and WES models separately specific for each task.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/622
https://github.com/google/deepvariant/issues/622:123,security,model,model,123,"If you question is if WGS and WES exactly the same models then the answer is no, they are not the same model, we train WGS model and WES models separately specific for each task.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/622
https://github.com/google/deepvariant/issues/622:137,security,model,models,137,"If you question is if WGS and WES exactly the same models then the answer is no, they are not the same model, we train WGS model and WES models separately specific for each task.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/622
https://github.com/google/deepvariant/issues/623:350,availability,operat,operating,350,"Hi @crazysummerW . Thank you for your question. The most recent release of DeepVariant (v1.5) has been trained with both Illumina and Element data for the WGS model, and we found a single model performs well for both data. Even in earlier releases before training with Element data, we observe that the DeepVariant Illumina model doesn't have issues operating on Element data.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/623
https://github.com/google/deepvariant/issues/623:64,deployability,releas,release,64,"Hi @crazysummerW . Thank you for your question. The most recent release of DeepVariant (v1.5) has been trained with both Illumina and Element data for the WGS model, and we found a single model performs well for both data. Even in earlier releases before training with Element data, we observe that the DeepVariant Illumina model doesn't have issues operating on Element data.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/623
https://github.com/google/deepvariant/issues/623:239,deployability,releas,releases,239,"Hi @crazysummerW . Thank you for your question. The most recent release of DeepVariant (v1.5) has been trained with both Illumina and Element data for the WGS model, and we found a single model performs well for both data. Even in earlier releases before training with Element data, we observe that the DeepVariant Illumina model doesn't have issues operating on Element data.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/623
https://github.com/google/deepvariant/issues/623:286,deployability,observ,observe,286,"Hi @crazysummerW . Thank you for your question. The most recent release of DeepVariant (v1.5) has been trained with both Illumina and Element data for the WGS model, and we found a single model performs well for both data. Even in earlier releases before training with Element data, we observe that the DeepVariant Illumina model doesn't have issues operating on Element data.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/623
https://github.com/google/deepvariant/issues/623:159,energy efficiency,model,model,159,"Hi @crazysummerW . Thank you for your question. The most recent release of DeepVariant (v1.5) has been trained with both Illumina and Element data for the WGS model, and we found a single model performs well for both data. Even in earlier releases before training with Element data, we observe that the DeepVariant Illumina model doesn't have issues operating on Element data.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/623
https://github.com/google/deepvariant/issues/623:188,energy efficiency,model,model,188,"Hi @crazysummerW . Thank you for your question. The most recent release of DeepVariant (v1.5) has been trained with both Illumina and Element data for the WGS model, and we found a single model performs well for both data. Even in earlier releases before training with Element data, we observe that the DeepVariant Illumina model doesn't have issues operating on Element data.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/623
https://github.com/google/deepvariant/issues/623:324,energy efficiency,model,model,324,"Hi @crazysummerW . Thank you for your question. The most recent release of DeepVariant (v1.5) has been trained with both Illumina and Element data for the WGS model, and we found a single model performs well for both data. Even in earlier releases before training with Element data, we observe that the DeepVariant Illumina model doesn't have issues operating on Element data.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/623
https://github.com/google/deepvariant/issues/623:194,performance,perform,performs,194,"Hi @crazysummerW . Thank you for your question. The most recent release of DeepVariant (v1.5) has been trained with both Illumina and Element data for the WGS model, and we found a single model performs well for both data. Even in earlier releases before training with Element data, we observe that the DeepVariant Illumina model doesn't have issues operating on Element data.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/623
https://github.com/google/deepvariant/issues/623:330,reliability,doe,doesn,330,"Hi @crazysummerW . Thank you for your question. The most recent release of DeepVariant (v1.5) has been trained with both Illumina and Element data for the WGS model, and we found a single model performs well for both data. Even in earlier releases before training with Element data, we observe that the DeepVariant Illumina model doesn't have issues operating on Element data.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/623
https://github.com/google/deepvariant/issues/623:159,security,model,model,159,"Hi @crazysummerW . Thank you for your question. The most recent release of DeepVariant (v1.5) has been trained with both Illumina and Element data for the WGS model, and we found a single model performs well for both data. Even in earlier releases before training with Element data, we observe that the DeepVariant Illumina model doesn't have issues operating on Element data.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/623
https://github.com/google/deepvariant/issues/623:188,security,model,model,188,"Hi @crazysummerW . Thank you for your question. The most recent release of DeepVariant (v1.5) has been trained with both Illumina and Element data for the WGS model, and we found a single model performs well for both data. Even in earlier releases before training with Element data, we observe that the DeepVariant Illumina model doesn't have issues operating on Element data.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/623
https://github.com/google/deepvariant/issues/623:324,security,model,model,324,"Hi @crazysummerW . Thank you for your question. The most recent release of DeepVariant (v1.5) has been trained with both Illumina and Element data for the WGS model, and we found a single model performs well for both data. Even in earlier releases before training with Element data, we observe that the DeepVariant Illumina model doesn't have issues operating on Element data.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/623
https://github.com/google/deepvariant/issues/623:286,testability,observ,observe,286,"Hi @crazysummerW . Thank you for your question. The most recent release of DeepVariant (v1.5) has been trained with both Illumina and Element data for the WGS model, and we found a single model performs well for both data. Even in earlier releases before training with Element data, we observe that the DeepVariant Illumina model doesn't have issues operating on Element data.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/623
https://github.com/google/deepvariant/issues/623:194,usability,perform,performs,194,"Hi @crazysummerW . Thank you for your question. The most recent release of DeepVariant (v1.5) has been trained with both Illumina and Element data for the WGS model, and we found a single model performs well for both data. Even in earlier releases before training with Element data, we observe that the DeepVariant Illumina model doesn't have issues operating on Element data.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/623
https://github.com/google/deepvariant/issues/623:111,availability,down,downloaded,111,"@crazysummerW ,. Your truth file looks empty, see the TRUTH.TOTAL is 0. Can you please make sure that you have downloaded the right file and it contains records in the VCF?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/623
https://github.com/google/deepvariant/issues/623:144,deployability,contain,contains,144,"@crazysummerW ,. Your truth file looks empty, see the TRUTH.TOTAL is 0. Can you please make sure that you have downloaded the right file and it contains records in the VCF?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/623
https://github.com/google/deepvariant/issues/624:15,deployability,releas,release,15,"For the v1.5.0 release we did not train a new RNA-seq model and therefore did not release a new model. You can stick with v1.4.0 model+codebase to run the RNA-seq model or you can use this v1.4.0 model with the v1.5.0 codebase (we have not tested directly, but it should work). Please let me know if you run into any issues.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/624
https://github.com/google/deepvariant/issues/624:82,deployability,releas,release,82,"For the v1.5.0 release we did not train a new RNA-seq model and therefore did not release a new model. You can stick with v1.4.0 model+codebase to run the RNA-seq model or you can use this v1.4.0 model with the v1.5.0 codebase (we have not tested directly, but it should work). Please let me know if you run into any issues.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/624
https://github.com/google/deepvariant/issues/624:54,energy efficiency,model,model,54,"For the v1.5.0 release we did not train a new RNA-seq model and therefore did not release a new model. You can stick with v1.4.0 model+codebase to run the RNA-seq model or you can use this v1.4.0 model with the v1.5.0 codebase (we have not tested directly, but it should work). Please let me know if you run into any issues.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/624
https://github.com/google/deepvariant/issues/624:96,energy efficiency,model,model,96,"For the v1.5.0 release we did not train a new RNA-seq model and therefore did not release a new model. You can stick with v1.4.0 model+codebase to run the RNA-seq model or you can use this v1.4.0 model with the v1.5.0 codebase (we have not tested directly, but it should work). Please let me know if you run into any issues.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/624
https://github.com/google/deepvariant/issues/624:129,energy efficiency,model,model,129,"For the v1.5.0 release we did not train a new RNA-seq model and therefore did not release a new model. You can stick with v1.4.0 model+codebase to run the RNA-seq model or you can use this v1.4.0 model with the v1.5.0 codebase (we have not tested directly, but it should work). Please let me know if you run into any issues.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/624
https://github.com/google/deepvariant/issues/624:163,energy efficiency,model,model,163,"For the v1.5.0 release we did not train a new RNA-seq model and therefore did not release a new model. You can stick with v1.4.0 model+codebase to run the RNA-seq model or you can use this v1.4.0 model with the v1.5.0 codebase (we have not tested directly, but it should work). Please let me know if you run into any issues.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/624
https://github.com/google/deepvariant/issues/624:196,energy efficiency,model,model,196,"For the v1.5.0 release we did not train a new RNA-seq model and therefore did not release a new model. You can stick with v1.4.0 model+codebase to run the RNA-seq model or you can use this v1.4.0 model with the v1.5.0 codebase (we have not tested directly, but it should work). Please let me know if you run into any issues.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/624
https://github.com/google/deepvariant/issues/624:240,safety,test,tested,240,"For the v1.5.0 release we did not train a new RNA-seq model and therefore did not release a new model. You can stick with v1.4.0 model+codebase to run the RNA-seq model or you can use this v1.4.0 model with the v1.5.0 codebase (we have not tested directly, but it should work). Please let me know if you run into any issues.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/624
https://github.com/google/deepvariant/issues/624:54,security,model,model,54,"For the v1.5.0 release we did not train a new RNA-seq model and therefore did not release a new model. You can stick with v1.4.0 model+codebase to run the RNA-seq model or you can use this v1.4.0 model with the v1.5.0 codebase (we have not tested directly, but it should work). Please let me know if you run into any issues.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/624
https://github.com/google/deepvariant/issues/624:96,security,model,model,96,"For the v1.5.0 release we did not train a new RNA-seq model and therefore did not release a new model. You can stick with v1.4.0 model+codebase to run the RNA-seq model or you can use this v1.4.0 model with the v1.5.0 codebase (we have not tested directly, but it should work). Please let me know if you run into any issues.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/624
https://github.com/google/deepvariant/issues/624:129,security,model,model,129,"For the v1.5.0 release we did not train a new RNA-seq model and therefore did not release a new model. You can stick with v1.4.0 model+codebase to run the RNA-seq model or you can use this v1.4.0 model with the v1.5.0 codebase (we have not tested directly, but it should work). Please let me know if you run into any issues.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/624
https://github.com/google/deepvariant/issues/624:163,security,model,model,163,"For the v1.5.0 release we did not train a new RNA-seq model and therefore did not release a new model. You can stick with v1.4.0 model+codebase to run the RNA-seq model or you can use this v1.4.0 model with the v1.5.0 codebase (we have not tested directly, but it should work). Please let me know if you run into any issues.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/624
https://github.com/google/deepvariant/issues/624:196,security,model,model,196,"For the v1.5.0 release we did not train a new RNA-seq model and therefore did not release a new model. You can stick with v1.4.0 model+codebase to run the RNA-seq model or you can use this v1.4.0 model with the v1.5.0 codebase (we have not tested directly, but it should work). Please let me know if you run into any issues.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/624
https://github.com/google/deepvariant/issues/624:240,testability,test,tested,240,"For the v1.5.0 release we did not train a new RNA-seq model and therefore did not release a new model. You can stick with v1.4.0 model+codebase to run the RNA-seq model or you can use this v1.4.0 model with the v1.5.0 codebase (we have not tested directly, but it should work). Please let me know if you run into any issues.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/624
https://github.com/google/deepvariant/issues/624:55,energy efficiency,model,model,55,"Great thanks. v1.5.0 codebase does run with the v1.4.0 model, so will compare the results soon. Hopefully no major differences",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/624
https://github.com/google/deepvariant/issues/624:30,reliability,doe,does,30,"Great thanks. v1.5.0 codebase does run with the v1.4.0 model, so will compare the results soon. Hopefully no major differences",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/624
https://github.com/google/deepvariant/issues/624:55,security,model,model,55,"Great thanks. v1.5.0 codebase does run with the v1.4.0 model, so will compare the results soon. Hopefully no major differences",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/624
https://github.com/google/deepvariant/issues/624:52,usability,feedback,feedback,52,Excellent! Please let us know if you have any other feedback. I will close this issue now.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/624
https://github.com/google/deepvariant/issues/624:69,usability,close,close,69,Excellent! Please let us know if you have any other feedback. I will close this issue now.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/624
https://github.com/google/deepvariant/issues/624:11,availability,sli,slight,11,"There is a slight change in performance, but since v1.5.0 already has slightly different output/performance to v1.4.0, I don't think there is any negative consequence to using RNA model v1.4.0 on DV v1.5.0.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/624
https://github.com/google/deepvariant/issues/624:70,availability,sli,slightly,70,"There is a slight change in performance, but since v1.5.0 already has slightly different output/performance to v1.4.0, I don't think there is any negative consequence to using RNA model v1.4.0 on DV v1.5.0.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/624
https://github.com/google/deepvariant/issues/624:180,energy efficiency,model,model,180,"There is a slight change in performance, but since v1.5.0 already has slightly different output/performance to v1.4.0, I don't think there is any negative consequence to using RNA model v1.4.0 on DV v1.5.0.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/624
https://github.com/google/deepvariant/issues/624:28,performance,perform,performance,28,"There is a slight change in performance, but since v1.5.0 already has slightly different output/performance to v1.4.0, I don't think there is any negative consequence to using RNA model v1.4.0 on DV v1.5.0.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/624
https://github.com/google/deepvariant/issues/624:96,performance,perform,performance,96,"There is a slight change in performance, but since v1.5.0 already has slightly different output/performance to v1.4.0, I don't think there is any negative consequence to using RNA model v1.4.0 on DV v1.5.0.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/624
https://github.com/google/deepvariant/issues/624:11,reliability,sli,slight,11,"There is a slight change in performance, but since v1.5.0 already has slightly different output/performance to v1.4.0, I don't think there is any negative consequence to using RNA model v1.4.0 on DV v1.5.0.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/624
https://github.com/google/deepvariant/issues/624:70,reliability,sli,slightly,70,"There is a slight change in performance, but since v1.5.0 already has slightly different output/performance to v1.4.0, I don't think there is any negative consequence to using RNA model v1.4.0 on DV v1.5.0.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/624
https://github.com/google/deepvariant/issues/624:180,security,model,model,180,"There is a slight change in performance, but since v1.5.0 already has slightly different output/performance to v1.4.0, I don't think there is any negative consequence to using RNA model v1.4.0 on DV v1.5.0.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/624
https://github.com/google/deepvariant/issues/624:28,usability,perform,performance,28,"There is a slight change in performance, but since v1.5.0 already has slightly different output/performance to v1.4.0, I don't think there is any negative consequence to using RNA model v1.4.0 on DV v1.5.0.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/624
https://github.com/google/deepvariant/issues/624:96,usability,perform,performance,96,"There is a slight change in performance, but since v1.5.0 already has slightly different output/performance to v1.4.0, I don't think there is any negative consequence to using RNA model v1.4.0 on DV v1.5.0.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/624
https://github.com/google/deepvariant/issues/624:15,deployability,updat,update,15,Thanks for the update.The differences could be related to a change in the Tensorflow version. We welcome any additional feedback you have.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/624
https://github.com/google/deepvariant/issues/624:85,deployability,version,version,85,Thanks for the update.The differences could be related to a change in the Tensorflow version. We welcome any additional feedback you have.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/624
https://github.com/google/deepvariant/issues/624:85,integrability,version,version,85,Thanks for the update.The differences could be related to a change in the Tensorflow version. We welcome any additional feedback you have.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/624
https://github.com/google/deepvariant/issues/624:85,modifiability,version,version,85,Thanks for the update.The differences could be related to a change in the Tensorflow version. We welcome any additional feedback you have.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/624
https://github.com/google/deepvariant/issues/624:15,safety,updat,update,15,Thanks for the update.The differences could be related to a change in the Tensorflow version. We welcome any additional feedback you have.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/624
https://github.com/google/deepvariant/issues/624:15,security,updat,update,15,Thanks for the update.The differences could be related to a change in the Tensorflow version. We welcome any additional feedback you have.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/624
https://github.com/google/deepvariant/issues/624:120,usability,feedback,feedback,120,Thanks for the update.The differences could be related to a change in the Tensorflow version. We welcome any additional feedback you have.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/624
https://github.com/google/deepvariant/issues/625:732,energy efficiency,core,cores,732,"Hi @raisa-petrovic - you can run DeepVariant as a single command:. ```bash. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \ **Replace this string with exactly one of the following [WGS,WES,PACBIO,ONT_R104,HYBRID_PACBIO_ILLUMINA]**. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \ **This flag is optional. Set to keep the intermediate results. --num_shards=1 **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.**. ```. which should resolve this issue. Alternatively, you'll need to add the insert_size channel to your examples to work with the latest WES and WGS models. We added an insert_size channel in v1.4.0 as it increased the accuracy of those models. I believe adding `--channels=insert_size` after `make_examples.zip` should resolve the issue. Let me know if either of these approaches work for you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/625
https://github.com/google/deepvariant/issues/625:796,energy efficiency,CPU,CPU,796,"Hi @raisa-petrovic - you can run DeepVariant as a single command:. ```bash. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \ **Replace this string with exactly one of the following [WGS,WES,PACBIO,ONT_R104,HYBRID_PACBIO_ILLUMINA]**. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \ **This flag is optional. Set to keep the intermediate results. --num_shards=1 **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.**. ```. which should resolve this issue. Alternatively, you'll need to add the insert_size channel to your examples to work with the latest WES and WGS models. We added an insert_size channel in v1.4.0 as it increased the accuracy of those models. I believe adding `--channels=insert_size` after `make_examples.zip` should resolve the issue. Let me know if either of these approaches work for you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/625
https://github.com/google/deepvariant/issues/625:800,energy efficiency,core,cores,800,"Hi @raisa-petrovic - you can run DeepVariant as a single command:. ```bash. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \ **Replace this string with exactly one of the following [WGS,WES,PACBIO,ONT_R104,HYBRID_PACBIO_ILLUMINA]**. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \ **This flag is optional. Set to keep the intermediate results. --num_shards=1 **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.**. ```. which should resolve this issue. Alternatively, you'll need to add the insert_size channel to your examples to work with the latest WES and WGS models. We added an insert_size channel in v1.4.0 as it increased the accuracy of those models. I believe adding `--channels=insert_size` after `make_examples.zip` should resolve the issue. Let me know if either of these approaches work for you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/625
https://github.com/google/deepvariant/issues/625:968,energy efficiency,model,models,968,"Hi @raisa-petrovic - you can run DeepVariant as a single command:. ```bash. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \ **Replace this string with exactly one of the following [WGS,WES,PACBIO,ONT_R104,HYBRID_PACBIO_ILLUMINA]**. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \ **This flag is optional. Set to keep the intermediate results. --num_shards=1 **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.**. ```. which should resolve this issue. Alternatively, you'll need to add the insert_size channel to your examples to work with the latest WES and WGS models. We added an insert_size channel in v1.4.0 as it increased the accuracy of those models. I believe adding `--channels=insert_size` after `make_examples.zip` should resolve the issue. Let me know if either of these approaches work for you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/625
https://github.com/google/deepvariant/issues/625:1056,energy efficiency,model,models,1056,"Hi @raisa-petrovic - you can run DeepVariant as a single command:. ```bash. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \ **Replace this string with exactly one of the following [WGS,WES,PACBIO,ONT_R104,HYBRID_PACBIO_ILLUMINA]**. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \ **This flag is optional. Set to keep the intermediate results. --num_shards=1 **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.**. ```. which should resolve this issue. Alternatively, you'll need to add the insert_size channel to your examples to work with the latest WES and WGS models. We added an insert_size channel in v1.4.0 as it increased the accuracy of those models. I believe adding `--channels=insert_size` after `make_examples.zip` should resolve the issue. Let me know if either of these approaches work for you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/625
https://github.com/google/deepvariant/issues/625:320,modifiability,PAC,PACBIO,320,"Hi @raisa-petrovic - you can run DeepVariant as a single command:. ```bash. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \ **Replace this string with exactly one of the following [WGS,WES,PACBIO,ONT_R104,HYBRID_PACBIO_ILLUMINA]**. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \ **This flag is optional. Set to keep the intermediate results. --num_shards=1 **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.**. ```. which should resolve this issue. Alternatively, you'll need to add the insert_size channel to your examples to work with the latest WES and WGS models. We added an insert_size channel in v1.4.0 as it increased the accuracy of those models. I believe adding `--channels=insert_size` after `make_examples.zip` should resolve the issue. Let me know if either of these approaches work for you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/625
https://github.com/google/deepvariant/issues/625:684,modifiability,interm,intermediate,684,"Hi @raisa-petrovic - you can run DeepVariant as a single command:. ```bash. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \ **Replace this string with exactly one of the following [WGS,WES,PACBIO,ONT_R104,HYBRID_PACBIO_ILLUMINA]**. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \ **This flag is optional. Set to keep the intermediate results. --num_shards=1 **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.**. ```. which should resolve this issue. Alternatively, you'll need to add the insert_size channel to your examples to work with the latest WES and WGS models. We added an insert_size channel in v1.4.0 as it increased the accuracy of those models. I believe adding `--channels=insert_size` after `make_examples.zip` should resolve the issue. Let me know if either of these approaches work for you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/625
https://github.com/google/deepvariant/issues/625:796,performance,CPU,CPU,796,"Hi @raisa-petrovic - you can run DeepVariant as a single command:. ```bash. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \ **Replace this string with exactly one of the following [WGS,WES,PACBIO,ONT_R104,HYBRID_PACBIO_ILLUMINA]**. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \ **This flag is optional. Set to keep the intermediate results. --num_shards=1 **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.**. ```. which should resolve this issue. Alternatively, you'll need to add the insert_size channel to your examples to work with the latest WES and WGS models. We added an insert_size channel in v1.4.0 as it increased the accuracy of those models. I believe adding `--channels=insert_size` after `make_examples.zip` should resolve the issue. Let me know if either of these approaches work for you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/625
https://github.com/google/deepvariant/issues/625:115,safety,input,input,115,"Hi @raisa-petrovic - you can run DeepVariant as a single command:. ```bash. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \ **Replace this string with exactly one of the following [WGS,WES,PACBIO,ONT_R104,HYBRID_PACBIO_ILLUMINA]**. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \ **This flag is optional. Set to keep the intermediate results. --num_shards=1 **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.**. ```. which should resolve this issue. Alternatively, you'll need to add the insert_size channel to your examples to work with the latest WES and WGS models. We added an insert_size channel in v1.4.0 as it increased the accuracy of those models. I believe adding `--channels=insert_size` after `make_examples.zip` should resolve the issue. Let me know if either of these approaches work for you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/625
https://github.com/google/deepvariant/issues/625:370,safety,input,input,370,"Hi @raisa-petrovic - you can run DeepVariant as a single command:. ```bash. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \ **Replace this string with exactly one of the following [WGS,WES,PACBIO,ONT_R104,HYBRID_PACBIO_ILLUMINA]**. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \ **This flag is optional. Set to keep the intermediate results. --num_shards=1 **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.**. ```. which should resolve this issue. Alternatively, you'll need to add the insert_size channel to your examples to work with the latest WES and WGS models. We added an insert_size channel in v1.4.0 as it increased the accuracy of those models. I believe adding `--channels=insert_size` after `make_examples.zip` should resolve the issue. Let me know if either of these approaches work for you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/625
https://github.com/google/deepvariant/issues/625:419,safety,input,input,419,"Hi @raisa-petrovic - you can run DeepVariant as a single command:. ```bash. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \ **Replace this string with exactly one of the following [WGS,WES,PACBIO,ONT_R104,HYBRID_PACBIO_ILLUMINA]**. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \ **This flag is optional. Set to keep the intermediate results. --num_shards=1 **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.**. ```. which should resolve this issue. Alternatively, you'll need to add the insert_size channel to your examples to work with the latest WES and WGS models. We added an insert_size channel in v1.4.0 as it increased the accuracy of those models. I believe adding `--channels=insert_size` after `make_examples.zip` should resolve the issue. Let me know if either of these approaches work for you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/625
https://github.com/google/deepvariant/issues/625:968,security,model,models,968,"Hi @raisa-petrovic - you can run DeepVariant as a single command:. ```bash. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \ **Replace this string with exactly one of the following [WGS,WES,PACBIO,ONT_R104,HYBRID_PACBIO_ILLUMINA]**. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \ **This flag is optional. Set to keep the intermediate results. --num_shards=1 **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.**. ```. which should resolve this issue. Alternatively, you'll need to add the insert_size channel to your examples to work with the latest WES and WGS models. We added an insert_size channel in v1.4.0 as it increased the accuracy of those models. I believe adding `--channels=insert_size` after `make_examples.zip` should resolve the issue. Let me know if either of these approaches work for you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/625
https://github.com/google/deepvariant/issues/625:1056,security,model,models,1056,"Hi @raisa-petrovic - you can run DeepVariant as a single command:. ```bash. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \ **Replace this string with exactly one of the following [WGS,WES,PACBIO,ONT_R104,HYBRID_PACBIO_ILLUMINA]**. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \ **This flag is optional. Set to keep the intermediate results. --num_shards=1 **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.**. ```. which should resolve this issue. Alternatively, you'll need to add the insert_size channel to your examples to work with the latest WES and WGS models. We added an insert_size channel in v1.4.0 as it increased the accuracy of those models. I believe adding `--channels=insert_size` after `make_examples.zip` should resolve the issue. Let me know if either of these approaches work for you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/625
https://github.com/google/deepvariant/issues/625:392,testability,unit,unittest,392,"Hi @raisa-petrovic - you can run DeepVariant as a single command:. ```bash. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \ **Replace this string with exactly one of the following [WGS,WES,PACBIO,ONT_R104,HYBRID_PACBIO_ILLUMINA]**. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \ **This flag is optional. Set to keep the intermediate results. --num_shards=1 **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.**. ```. which should resolve this issue. Alternatively, you'll need to add the insert_size channel to your examples to work with the latest WES and WGS models. We added an insert_size channel in v1.4.0 as it increased the accuracy of those models. I believe adding `--channels=insert_size` after `make_examples.zip` should resolve the issue. Let me know if either of these approaches work for you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/625
https://github.com/google/deepvariant/issues/625:57,usability,command,command,57,"Hi @raisa-petrovic - you can run DeepVariant as a single command:. ```bash. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \ **Replace this string with exactly one of the following [WGS,WES,PACBIO,ONT_R104,HYBRID_PACBIO_ILLUMINA]**. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \ **This flag is optional. Set to keep the intermediate results. --num_shards=1 **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.**. ```. which should resolve this issue. Alternatively, you'll need to add the insert_size channel to your examples to work with the latest WES and WGS models. We added an insert_size channel in v1.4.0 as it increased the accuracy of those models. I believe adding `--channels=insert_size` after `make_examples.zip` should resolve the issue. Let me know if either of these approaches work for you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/625
https://github.com/google/deepvariant/issues/625:115,usability,input,input,115,"Hi @raisa-petrovic - you can run DeepVariant as a single command:. ```bash. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \ **Replace this string with exactly one of the following [WGS,WES,PACBIO,ONT_R104,HYBRID_PACBIO_ILLUMINA]**. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \ **This flag is optional. Set to keep the intermediate results. --num_shards=1 **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.**. ```. which should resolve this issue. Alternatively, you'll need to add the insert_size channel to your examples to work with the latest WES and WGS models. We added an insert_size channel in v1.4.0 as it increased the accuracy of those models. I believe adding `--channels=insert_size` after `make_examples.zip` should resolve the issue. Let me know if either of these approaches work for you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/625
https://github.com/google/deepvariant/issues/625:370,usability,input,input,370,"Hi @raisa-petrovic - you can run DeepVariant as a single command:. ```bash. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \ **Replace this string with exactly one of the following [WGS,WES,PACBIO,ONT_R104,HYBRID_PACBIO_ILLUMINA]**. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \ **This flag is optional. Set to keep the intermediate results. --num_shards=1 **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.**. ```. which should resolve this issue. Alternatively, you'll need to add the insert_size channel to your examples to work with the latest WES and WGS models. We added an insert_size channel in v1.4.0 as it increased the accuracy of those models. I believe adding `--channels=insert_size` after `make_examples.zip` should resolve the issue. Let me know if either of these approaches work for you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/625
https://github.com/google/deepvariant/issues/625:419,usability,input,input,419,"Hi @raisa-petrovic - you can run DeepVariant as a single command:. ```bash. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \ **Replace this string with exactly one of the following [WGS,WES,PACBIO,ONT_R104,HYBRID_PACBIO_ILLUMINA]**. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \ **This flag is optional. Set to keep the intermediate results. --num_shards=1 **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.**. ```. which should resolve this issue. Alternatively, you'll need to add the insert_size channel to your examples to work with the latest WES and WGS models. We added an insert_size channel in v1.4.0 as it increased the accuracy of those models. I believe adding `--channels=insert_size` after `make_examples.zip` should resolve the issue. Let me know if either of these approaches work for you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/625
https://github.com/google/deepvariant/issues/625:19,usability,close,close,19,Excellent - I will close this now.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/625
https://github.com/google/deepvariant/issues/626:95,deployability,version,version,95,"Hi @srbehera . The PrecisionFDA submission reflects a pre-DV1.0 code base. The most comparable version would be v1.0, but there are some code and data changes that occurred between the pFDA submission and the general release. There is one minor difference that could be partially relevant here, which is that I believe that the PrecisionFDA results were generated with --min_mapping_quality=1 where our default for DV1.0-1.5 is min_mapping_quality=5. We generally don't see a large accuracy improvement from that, though. The precisionFDA submission does not have additional filtering, and reflects the output of DeepVariant from the model as occurs with other versions. . One thing to note for fluctuations between precision and recall is that much of the difference can be exactly where on the ROC the model is chosen. If you would like higher recall, it may be possible to use the PL values to set a threshold below the no-call/RefCall threshold. Does this answer your question? Is there more information I can give which will be helpful?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/626
https://github.com/google/deepvariant/issues/626:217,deployability,releas,release,217,"Hi @srbehera . The PrecisionFDA submission reflects a pre-DV1.0 code base. The most comparable version would be v1.0, but there are some code and data changes that occurred between the pFDA submission and the general release. There is one minor difference that could be partially relevant here, which is that I believe that the PrecisionFDA results were generated with --min_mapping_quality=1 where our default for DV1.0-1.5 is min_mapping_quality=5. We generally don't see a large accuracy improvement from that, though. The precisionFDA submission does not have additional filtering, and reflects the output of DeepVariant from the model as occurs with other versions. . One thing to note for fluctuations between precision and recall is that much of the difference can be exactly where on the ROC the model is chosen. If you would like higher recall, it may be possible to use the PL values to set a threshold below the no-call/RefCall threshold. Does this answer your question? Is there more information I can give which will be helpful?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/626
https://github.com/google/deepvariant/issues/626:661,deployability,version,versions,661,"Hi @srbehera . The PrecisionFDA submission reflects a pre-DV1.0 code base. The most comparable version would be v1.0, but there are some code and data changes that occurred between the pFDA submission and the general release. There is one minor difference that could be partially relevant here, which is that I believe that the PrecisionFDA results were generated with --min_mapping_quality=1 where our default for DV1.0-1.5 is min_mapping_quality=5. We generally don't see a large accuracy improvement from that, though. The precisionFDA submission does not have additional filtering, and reflects the output of DeepVariant from the model as occurs with other versions. . One thing to note for fluctuations between precision and recall is that much of the difference can be exactly where on the ROC the model is chosen. If you would like higher recall, it may be possible to use the PL values to set a threshold below the no-call/RefCall threshold. Does this answer your question? Is there more information I can give which will be helpful?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/626
https://github.com/google/deepvariant/issues/626:634,energy efficiency,model,model,634,"Hi @srbehera . The PrecisionFDA submission reflects a pre-DV1.0 code base. The most comparable version would be v1.0, but there are some code and data changes that occurred between the pFDA submission and the general release. There is one minor difference that could be partially relevant here, which is that I believe that the PrecisionFDA results were generated with --min_mapping_quality=1 where our default for DV1.0-1.5 is min_mapping_quality=5. We generally don't see a large accuracy improvement from that, though. The precisionFDA submission does not have additional filtering, and reflects the output of DeepVariant from the model as occurs with other versions. . One thing to note for fluctuations between precision and recall is that much of the difference can be exactly where on the ROC the model is chosen. If you would like higher recall, it may be possible to use the PL values to set a threshold below the no-call/RefCall threshold. Does this answer your question? Is there more information I can give which will be helpful?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/626
https://github.com/google/deepvariant/issues/626:804,energy efficiency,model,model,804,"Hi @srbehera . The PrecisionFDA submission reflects a pre-DV1.0 code base. The most comparable version would be v1.0, but there are some code and data changes that occurred between the pFDA submission and the general release. There is one minor difference that could be partially relevant here, which is that I believe that the PrecisionFDA results were generated with --min_mapping_quality=1 where our default for DV1.0-1.5 is min_mapping_quality=5. We generally don't see a large accuracy improvement from that, though. The precisionFDA submission does not have additional filtering, and reflects the output of DeepVariant from the model as occurs with other versions. . One thing to note for fluctuations between precision and recall is that much of the difference can be exactly where on the ROC the model is chosen. If you would like higher recall, it may be possible to use the PL values to set a threshold below the no-call/RefCall threshold. Does this answer your question? Is there more information I can give which will be helpful?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/626
https://github.com/google/deepvariant/issues/626:32,integrability,sub,submission,32,"Hi @srbehera . The PrecisionFDA submission reflects a pre-DV1.0 code base. The most comparable version would be v1.0, but there are some code and data changes that occurred between the pFDA submission and the general release. There is one minor difference that could be partially relevant here, which is that I believe that the PrecisionFDA results were generated with --min_mapping_quality=1 where our default for DV1.0-1.5 is min_mapping_quality=5. We generally don't see a large accuracy improvement from that, though. The precisionFDA submission does not have additional filtering, and reflects the output of DeepVariant from the model as occurs with other versions. . One thing to note for fluctuations between precision and recall is that much of the difference can be exactly where on the ROC the model is chosen. If you would like higher recall, it may be possible to use the PL values to set a threshold below the no-call/RefCall threshold. Does this answer your question? Is there more information I can give which will be helpful?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/626
https://github.com/google/deepvariant/issues/626:95,integrability,version,version,95,"Hi @srbehera . The PrecisionFDA submission reflects a pre-DV1.0 code base. The most comparable version would be v1.0, but there are some code and data changes that occurred between the pFDA submission and the general release. There is one minor difference that could be partially relevant here, which is that I believe that the PrecisionFDA results were generated with --min_mapping_quality=1 where our default for DV1.0-1.5 is min_mapping_quality=5. We generally don't see a large accuracy improvement from that, though. The precisionFDA submission does not have additional filtering, and reflects the output of DeepVariant from the model as occurs with other versions. . One thing to note for fluctuations between precision and recall is that much of the difference can be exactly where on the ROC the model is chosen. If you would like higher recall, it may be possible to use the PL values to set a threshold below the no-call/RefCall threshold. Does this answer your question? Is there more information I can give which will be helpful?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/626
https://github.com/google/deepvariant/issues/626:190,integrability,sub,submission,190,"Hi @srbehera . The PrecisionFDA submission reflects a pre-DV1.0 code base. The most comparable version would be v1.0, but there are some code and data changes that occurred between the pFDA submission and the general release. There is one minor difference that could be partially relevant here, which is that I believe that the PrecisionFDA results were generated with --min_mapping_quality=1 where our default for DV1.0-1.5 is min_mapping_quality=5. We generally don't see a large accuracy improvement from that, though. The precisionFDA submission does not have additional filtering, and reflects the output of DeepVariant from the model as occurs with other versions. . One thing to note for fluctuations between precision and recall is that much of the difference can be exactly where on the ROC the model is chosen. If you would like higher recall, it may be possible to use the PL values to set a threshold below the no-call/RefCall threshold. Does this answer your question? Is there more information I can give which will be helpful?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/626
https://github.com/google/deepvariant/issues/626:539,integrability,sub,submission,539,"Hi @srbehera . The PrecisionFDA submission reflects a pre-DV1.0 code base. The most comparable version would be v1.0, but there are some code and data changes that occurred between the pFDA submission and the general release. There is one minor difference that could be partially relevant here, which is that I believe that the PrecisionFDA results were generated with --min_mapping_quality=1 where our default for DV1.0-1.5 is min_mapping_quality=5. We generally don't see a large accuracy improvement from that, though. The precisionFDA submission does not have additional filtering, and reflects the output of DeepVariant from the model as occurs with other versions. . One thing to note for fluctuations between precision and recall is that much of the difference can be exactly where on the ROC the model is chosen. If you would like higher recall, it may be possible to use the PL values to set a threshold below the no-call/RefCall threshold. Does this answer your question? Is there more information I can give which will be helpful?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/626
https://github.com/google/deepvariant/issues/626:575,integrability,filter,filtering,575,"Hi @srbehera . The PrecisionFDA submission reflects a pre-DV1.0 code base. The most comparable version would be v1.0, but there are some code and data changes that occurred between the pFDA submission and the general release. There is one minor difference that could be partially relevant here, which is that I believe that the PrecisionFDA results were generated with --min_mapping_quality=1 where our default for DV1.0-1.5 is min_mapping_quality=5. We generally don't see a large accuracy improvement from that, though. The precisionFDA submission does not have additional filtering, and reflects the output of DeepVariant from the model as occurs with other versions. . One thing to note for fluctuations between precision and recall is that much of the difference can be exactly where on the ROC the model is chosen. If you would like higher recall, it may be possible to use the PL values to set a threshold below the no-call/RefCall threshold. Does this answer your question? Is there more information I can give which will be helpful?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/626
https://github.com/google/deepvariant/issues/626:661,integrability,version,versions,661,"Hi @srbehera . The PrecisionFDA submission reflects a pre-DV1.0 code base. The most comparable version would be v1.0, but there are some code and data changes that occurred between the pFDA submission and the general release. There is one minor difference that could be partially relevant here, which is that I believe that the PrecisionFDA results were generated with --min_mapping_quality=1 where our default for DV1.0-1.5 is min_mapping_quality=5. We generally don't see a large accuracy improvement from that, though. The precisionFDA submission does not have additional filtering, and reflects the output of DeepVariant from the model as occurs with other versions. . One thing to note for fluctuations between precision and recall is that much of the difference can be exactly where on the ROC the model is chosen. If you would like higher recall, it may be possible to use the PL values to set a threshold below the no-call/RefCall threshold. Does this answer your question? Is there more information I can give which will be helpful?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/626
https://github.com/google/deepvariant/issues/626:95,modifiability,version,version,95,"Hi @srbehera . The PrecisionFDA submission reflects a pre-DV1.0 code base. The most comparable version would be v1.0, but there are some code and data changes that occurred between the pFDA submission and the general release. There is one minor difference that could be partially relevant here, which is that I believe that the PrecisionFDA results were generated with --min_mapping_quality=1 where our default for DV1.0-1.5 is min_mapping_quality=5. We generally don't see a large accuracy improvement from that, though. The precisionFDA submission does not have additional filtering, and reflects the output of DeepVariant from the model as occurs with other versions. . One thing to note for fluctuations between precision and recall is that much of the difference can be exactly where on the ROC the model is chosen. If you would like higher recall, it may be possible to use the PL values to set a threshold below the no-call/RefCall threshold. Does this answer your question? Is there more information I can give which will be helpful?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/626
https://github.com/google/deepvariant/issues/626:661,modifiability,version,versions,661,"Hi @srbehera . The PrecisionFDA submission reflects a pre-DV1.0 code base. The most comparable version would be v1.0, but there are some code and data changes that occurred between the pFDA submission and the general release. There is one minor difference that could be partially relevant here, which is that I believe that the PrecisionFDA results were generated with --min_mapping_quality=1 where our default for DV1.0-1.5 is min_mapping_quality=5. We generally don't see a large accuracy improvement from that, though. The precisionFDA submission does not have additional filtering, and reflects the output of DeepVariant from the model as occurs with other versions. . One thing to note for fluctuations between precision and recall is that much of the difference can be exactly where on the ROC the model is chosen. If you would like higher recall, it may be possible to use the PL values to set a threshold below the no-call/RefCall threshold. Does this answer your question? Is there more information I can give which will be helpful?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/626
https://github.com/google/deepvariant/issues/626:550,reliability,doe,does,550,"Hi @srbehera . The PrecisionFDA submission reflects a pre-DV1.0 code base. The most comparable version would be v1.0, but there are some code and data changes that occurred between the pFDA submission and the general release. There is one minor difference that could be partially relevant here, which is that I believe that the PrecisionFDA results were generated with --min_mapping_quality=1 where our default for DV1.0-1.5 is min_mapping_quality=5. We generally don't see a large accuracy improvement from that, though. The precisionFDA submission does not have additional filtering, and reflects the output of DeepVariant from the model as occurs with other versions. . One thing to note for fluctuations between precision and recall is that much of the difference can be exactly where on the ROC the model is chosen. If you would like higher recall, it may be possible to use the PL values to set a threshold below the no-call/RefCall threshold. Does this answer your question? Is there more information I can give which will be helpful?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/626
https://github.com/google/deepvariant/issues/626:950,reliability,Doe,Does,950,"Hi @srbehera . The PrecisionFDA submission reflects a pre-DV1.0 code base. The most comparable version would be v1.0, but there are some code and data changes that occurred between the pFDA submission and the general release. There is one minor difference that could be partially relevant here, which is that I believe that the PrecisionFDA results were generated with --min_mapping_quality=1 where our default for DV1.0-1.5 is min_mapping_quality=5. We generally don't see a large accuracy improvement from that, though. The precisionFDA submission does not have additional filtering, and reflects the output of DeepVariant from the model as occurs with other versions. . One thing to note for fluctuations between precision and recall is that much of the difference can be exactly where on the ROC the model is chosen. If you would like higher recall, it may be possible to use the PL values to set a threshold below the no-call/RefCall threshold. Does this answer your question? Is there more information I can give which will be helpful?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/626
https://github.com/google/deepvariant/issues/626:634,security,model,model,634,"Hi @srbehera . The PrecisionFDA submission reflects a pre-DV1.0 code base. The most comparable version would be v1.0, but there are some code and data changes that occurred between the pFDA submission and the general release. There is one minor difference that could be partially relevant here, which is that I believe that the PrecisionFDA results were generated with --min_mapping_quality=1 where our default for DV1.0-1.5 is min_mapping_quality=5. We generally don't see a large accuracy improvement from that, though. The precisionFDA submission does not have additional filtering, and reflects the output of DeepVariant from the model as occurs with other versions. . One thing to note for fluctuations between precision and recall is that much of the difference can be exactly where on the ROC the model is chosen. If you would like higher recall, it may be possible to use the PL values to set a threshold below the no-call/RefCall threshold. Does this answer your question? Is there more information I can give which will be helpful?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/626
https://github.com/google/deepvariant/issues/626:804,security,model,model,804,"Hi @srbehera . The PrecisionFDA submission reflects a pre-DV1.0 code base. The most comparable version would be v1.0, but there are some code and data changes that occurred between the pFDA submission and the general release. There is one minor difference that could be partially relevant here, which is that I believe that the PrecisionFDA results were generated with --min_mapping_quality=1 where our default for DV1.0-1.5 is min_mapping_quality=5. We generally don't see a large accuracy improvement from that, though. The precisionFDA submission does not have additional filtering, and reflects the output of DeepVariant from the model as occurs with other versions. . One thing to note for fluctuations between precision and recall is that much of the difference can be exactly where on the ROC the model is chosen. If you would like higher recall, it may be possible to use the PL values to set a threshold below the no-call/RefCall threshold. Does this answer your question? Is there more information I can give which will be helpful?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/626
https://github.com/google/deepvariant/issues/626:1033,usability,help,helpful,1033,"Hi @srbehera . The PrecisionFDA submission reflects a pre-DV1.0 code base. The most comparable version would be v1.0, but there are some code and data changes that occurred between the pFDA submission and the general release. There is one minor difference that could be partially relevant here, which is that I believe that the PrecisionFDA results were generated with --min_mapping_quality=1 where our default for DV1.0-1.5 is min_mapping_quality=5. We generally don't see a large accuracy improvement from that, though. The precisionFDA submission does not have additional filtering, and reflects the output of DeepVariant from the model as occurs with other versions. . One thing to note for fluctuations between precision and recall is that much of the difference can be exactly where on the ROC the model is chosen. If you would like higher recall, it may be possible to use the PL values to set a threshold below the no-call/RefCall threshold. Does this answer your question? Is there more information I can give which will be helpful?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/626
https://github.com/google/deepvariant/issues/627:470,availability,error,error,470,"Hi, here is the output:. `. dv_call_variants.py -h. Baseline DeepVariant arguments. File ""/tmp/Bazel.runfiles_dwq0j2la/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 356. f'The file {input_shape_file} should contain 3 integers'). ^. SyntaxError: invalid syntax. `. If you check dv_make_examples.py and dv_postprocess_variants.py, you can find similar output. I also tried to solve this syntax by replace f-strings to .format, and get another syntax error:. `Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_gxfcz7rb/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 38, in <module>. from deepvariant import make_examples_core. File ""/tmp/Bazel.runfiles_gxfcz7rb/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 645. options: deepvariant_pb2.SampleOptions. ^. SyntaxError: invalid syntax. `.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/627
https://github.com/google/deepvariant/issues/627:229,deployability,contain,contain,229,"Hi, here is the output:. `. dv_call_variants.py -h. Baseline DeepVariant arguments. File ""/tmp/Bazel.runfiles_dwq0j2la/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 356. f'The file {input_shape_file} should contain 3 integers'). ^. SyntaxError: invalid syntax. `. If you check dv_make_examples.py and dv_postprocess_variants.py, you can find similar output. I also tried to solve this syntax by replace f-strings to .format, and get another syntax error:. `Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_gxfcz7rb/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 38, in <module>. from deepvariant import make_examples_core. File ""/tmp/Bazel.runfiles_gxfcz7rb/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 645. options: deepvariant_pb2.SampleOptions. ^. SyntaxError: invalid syntax. `.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/627
https://github.com/google/deepvariant/issues/627:626,deployability,modul,module,626,"Hi, here is the output:. `. dv_call_variants.py -h. Baseline DeepVariant arguments. File ""/tmp/Bazel.runfiles_dwq0j2la/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 356. f'The file {input_shape_file} should contain 3 integers'). ^. SyntaxError: invalid syntax. `. If you check dv_make_examples.py and dv_postprocess_variants.py, you can find similar output. I also tried to solve this syntax by replace f-strings to .format, and get another syntax error:. `Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_gxfcz7rb/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 38, in <module>. from deepvariant import make_examples_core. File ""/tmp/Bazel.runfiles_gxfcz7rb/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 645. options: deepvariant_pb2.SampleOptions. ^. SyntaxError: invalid syntax. `.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/627
https://github.com/google/deepvariant/issues/627:439,interoperability,format,format,439,"Hi, here is the output:. `. dv_call_variants.py -h. Baseline DeepVariant arguments. File ""/tmp/Bazel.runfiles_dwq0j2la/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 356. f'The file {input_shape_file} should contain 3 integers'). ^. SyntaxError: invalid syntax. `. If you check dv_make_examples.py and dv_postprocess_variants.py, you can find similar output. I also tried to solve this syntax by replace f-strings to .format, and get another syntax error:. `Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_gxfcz7rb/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 38, in <module>. from deepvariant import make_examples_core. File ""/tmp/Bazel.runfiles_gxfcz7rb/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 645. options: deepvariant_pb2.SampleOptions. ^. SyntaxError: invalid syntax. `.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/627
https://github.com/google/deepvariant/issues/627:626,modifiability,modul,module,626,"Hi, here is the output:. `. dv_call_variants.py -h. Baseline DeepVariant arguments. File ""/tmp/Bazel.runfiles_dwq0j2la/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 356. f'The file {input_shape_file} should contain 3 integers'). ^. SyntaxError: invalid syntax. `. If you check dv_make_examples.py and dv_postprocess_variants.py, you can find similar output. I also tried to solve this syntax by replace f-strings to .format, and get another syntax error:. `Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_gxfcz7rb/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 38, in <module>. from deepvariant import make_examples_core. File ""/tmp/Bazel.runfiles_gxfcz7rb/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 645. options: deepvariant_pb2.SampleOptions. ^. SyntaxError: invalid syntax. `.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/627
https://github.com/google/deepvariant/issues/627:470,performance,error,error,470,"Hi, here is the output:. `. dv_call_variants.py -h. Baseline DeepVariant arguments. File ""/tmp/Bazel.runfiles_dwq0j2la/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 356. f'The file {input_shape_file} should contain 3 integers'). ^. SyntaxError: invalid syntax. `. If you check dv_make_examples.py and dv_postprocess_variants.py, you can find similar output. I also tried to solve this syntax by replace f-strings to .format, and get another syntax error:. `Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_gxfcz7rb/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 38, in <module>. from deepvariant import make_examples_core. File ""/tmp/Bazel.runfiles_gxfcz7rb/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 645. options: deepvariant_pb2.SampleOptions. ^. SyntaxError: invalid syntax. `.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/627
https://github.com/google/deepvariant/issues/627:470,safety,error,error,470,"Hi, here is the output:. `. dv_call_variants.py -h. Baseline DeepVariant arguments. File ""/tmp/Bazel.runfiles_dwq0j2la/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 356. f'The file {input_shape_file} should contain 3 integers'). ^. SyntaxError: invalid syntax. `. If you check dv_make_examples.py and dv_postprocess_variants.py, you can find similar output. I also tried to solve this syntax by replace f-strings to .format, and get another syntax error:. `Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_gxfcz7rb/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 38, in <module>. from deepvariant import make_examples_core. File ""/tmp/Bazel.runfiles_gxfcz7rb/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 645. options: deepvariant_pb2.SampleOptions. ^. SyntaxError: invalid syntax. `.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/627
https://github.com/google/deepvariant/issues/627:626,safety,modul,module,626,"Hi, here is the output:. `. dv_call_variants.py -h. Baseline DeepVariant arguments. File ""/tmp/Bazel.runfiles_dwq0j2la/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 356. f'The file {input_shape_file} should contain 3 integers'). ^. SyntaxError: invalid syntax. `. If you check dv_make_examples.py and dv_postprocess_variants.py, you can find similar output. I also tried to solve this syntax by replace f-strings to .format, and get another syntax error:. `Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_gxfcz7rb/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 38, in <module>. from deepvariant import make_examples_core. File ""/tmp/Bazel.runfiles_gxfcz7rb/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 645. options: deepvariant_pb2.SampleOptions. ^. SyntaxError: invalid syntax. `.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/627
https://github.com/google/deepvariant/issues/627:479,testability,Trace,Traceback,479,"Hi, here is the output:. `. dv_call_variants.py -h. Baseline DeepVariant arguments. File ""/tmp/Bazel.runfiles_dwq0j2la/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 356. f'The file {input_shape_file} should contain 3 integers'). ^. SyntaxError: invalid syntax. `. If you check dv_make_examples.py and dv_postprocess_variants.py, you can find similar output. I also tried to solve this syntax by replace f-strings to .format, and get another syntax error:. `Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_gxfcz7rb/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 38, in <module>. from deepvariant import make_examples_core. File ""/tmp/Bazel.runfiles_gxfcz7rb/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 645. options: deepvariant_pb2.SampleOptions. ^. SyntaxError: invalid syntax. `.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/627
https://github.com/google/deepvariant/issues/627:470,usability,error,error,470,"Hi, here is the output:. `. dv_call_variants.py -h. Baseline DeepVariant arguments. File ""/tmp/Bazel.runfiles_dwq0j2la/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 356. f'The file {input_shape_file} should contain 3 integers'). ^. SyntaxError: invalid syntax. `. If you check dv_make_examples.py and dv_postprocess_variants.py, you can find similar output. I also tried to solve this syntax by replace f-strings to .format, and get another syntax error:. `Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_gxfcz7rb/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 38, in <module>. from deepvariant import make_examples_core. File ""/tmp/Bazel.runfiles_gxfcz7rb/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 645. options: deepvariant_pb2.SampleOptions. ^. SyntaxError: invalid syntax. `.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/627
https://github.com/google/deepvariant/issues/627:701,availability,error,error,701,"@richard-nm Can you paste your command too? I wonder if you're using older code with newer models. In 1.3.0, we used to have this file that specifies the shape in 3 integers:. ```. $ gsutil cat gs://deepvariant/models/DeepVariant/1.3.0/DeepVariant-inception_v3-1.3.0+data-wgs_standard/model.ckpt.input_shape. 100 221 6. ```. In the later moment, we changed the format. For example, 1.4.0:. ```. $ gsutil cat gs://deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-wgs_standard/model.ckpt.example_info.json. {""version"": ""1.5.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. (We also added one more channel, which is why the shape is now 100 221 7.). From the error messages you're getting, it seems like you're using code version older than 1.4.0.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/627
https://github.com/google/deepvariant/issues/627:531,deployability,version,version,531,"@richard-nm Can you paste your command too? I wonder if you're using older code with newer models. In 1.3.0, we used to have this file that specifies the shape in 3 integers:. ```. $ gsutil cat gs://deepvariant/models/DeepVariant/1.3.0/DeepVariant-inception_v3-1.3.0+data-wgs_standard/model.ckpt.input_shape. 100 221 6. ```. In the later moment, we changed the format. For example, 1.4.0:. ```. $ gsutil cat gs://deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-wgs_standard/model.ckpt.example_info.json. {""version"": ""1.5.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. (We also added one more channel, which is why the shape is now 100 221 7.). From the error messages you're getting, it seems like you're using code version older than 1.4.0.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/627
https://github.com/google/deepvariant/issues/627:764,deployability,version,version,764,"@richard-nm Can you paste your command too? I wonder if you're using older code with newer models. In 1.3.0, we used to have this file that specifies the shape in 3 integers:. ```. $ gsutil cat gs://deepvariant/models/DeepVariant/1.3.0/DeepVariant-inception_v3-1.3.0+data-wgs_standard/model.ckpt.input_shape. 100 221 6. ```. In the later moment, we changed the format. For example, 1.4.0:. ```. $ gsutil cat gs://deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-wgs_standard/model.ckpt.example_info.json. {""version"": ""1.5.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. (We also added one more channel, which is why the shape is now 100 221 7.). From the error messages you're getting, it seems like you're using code version older than 1.4.0.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/627
https://github.com/google/deepvariant/issues/627:91,energy efficiency,model,models,91,"@richard-nm Can you paste your command too? I wonder if you're using older code with newer models. In 1.3.0, we used to have this file that specifies the shape in 3 integers:. ```. $ gsutil cat gs://deepvariant/models/DeepVariant/1.3.0/DeepVariant-inception_v3-1.3.0+data-wgs_standard/model.ckpt.input_shape. 100 221 6. ```. In the later moment, we changed the format. For example, 1.4.0:. ```. $ gsutil cat gs://deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-wgs_standard/model.ckpt.example_info.json. {""version"": ""1.5.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. (We also added one more channel, which is why the shape is now 100 221 7.). From the error messages you're getting, it seems like you're using code version older than 1.4.0.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/627
https://github.com/google/deepvariant/issues/627:211,energy efficiency,model,models,211,"@richard-nm Can you paste your command too? I wonder if you're using older code with newer models. In 1.3.0, we used to have this file that specifies the shape in 3 integers:. ```. $ gsutil cat gs://deepvariant/models/DeepVariant/1.3.0/DeepVariant-inception_v3-1.3.0+data-wgs_standard/model.ckpt.input_shape. 100 221 6. ```. In the later moment, we changed the format. For example, 1.4.0:. ```. $ gsutil cat gs://deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-wgs_standard/model.ckpt.example_info.json. {""version"": ""1.5.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. (We also added one more channel, which is why the shape is now 100 221 7.). From the error messages you're getting, it seems like you're using code version older than 1.4.0.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/627
https://github.com/google/deepvariant/issues/627:285,energy efficiency,model,model,285,"@richard-nm Can you paste your command too? I wonder if you're using older code with newer models. In 1.3.0, we used to have this file that specifies the shape in 3 integers:. ```. $ gsutil cat gs://deepvariant/models/DeepVariant/1.3.0/DeepVariant-inception_v3-1.3.0+data-wgs_standard/model.ckpt.input_shape. 100 221 6. ```. In the later moment, we changed the format. For example, 1.4.0:. ```. $ gsutil cat gs://deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-wgs_standard/model.ckpt.example_info.json. {""version"": ""1.5.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. (We also added one more channel, which is why the shape is now 100 221 7.). From the error messages you're getting, it seems like you're using code version older than 1.4.0.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/627
https://github.com/google/deepvariant/issues/627:425,energy efficiency,model,models,425,"@richard-nm Can you paste your command too? I wonder if you're using older code with newer models. In 1.3.0, we used to have this file that specifies the shape in 3 integers:. ```. $ gsutil cat gs://deepvariant/models/DeepVariant/1.3.0/DeepVariant-inception_v3-1.3.0+data-wgs_standard/model.ckpt.input_shape. 100 221 6. ```. In the later moment, we changed the format. For example, 1.4.0:. ```. $ gsutil cat gs://deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-wgs_standard/model.ckpt.example_info.json. {""version"": ""1.5.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. (We also added one more channel, which is why the shape is now 100 221 7.). From the error messages you're getting, it seems like you're using code version older than 1.4.0.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/627
https://github.com/google/deepvariant/issues/627:499,energy efficiency,model,model,499,"@richard-nm Can you paste your command too? I wonder if you're using older code with newer models. In 1.3.0, we used to have this file that specifies the shape in 3 integers:. ```. $ gsutil cat gs://deepvariant/models/DeepVariant/1.3.0/DeepVariant-inception_v3-1.3.0+data-wgs_standard/model.ckpt.input_shape. 100 221 6. ```. In the later moment, we changed the format. For example, 1.4.0:. ```. $ gsutil cat gs://deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-wgs_standard/model.ckpt.example_info.json. {""version"": ""1.5.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. (We also added one more channel, which is why the shape is now 100 221 7.). From the error messages you're getting, it seems like you're using code version older than 1.4.0.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/627
https://github.com/google/deepvariant/issues/627:531,integrability,version,version,531,"@richard-nm Can you paste your command too? I wonder if you're using older code with newer models. In 1.3.0, we used to have this file that specifies the shape in 3 integers:. ```. $ gsutil cat gs://deepvariant/models/DeepVariant/1.3.0/DeepVariant-inception_v3-1.3.0+data-wgs_standard/model.ckpt.input_shape. 100 221 6. ```. In the later moment, we changed the format. For example, 1.4.0:. ```. $ gsutil cat gs://deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-wgs_standard/model.ckpt.example_info.json. {""version"": ""1.5.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. (We also added one more channel, which is why the shape is now 100 221 7.). From the error messages you're getting, it seems like you're using code version older than 1.4.0.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/627
https://github.com/google/deepvariant/issues/627:707,integrability,messag,messages,707,"@richard-nm Can you paste your command too? I wonder if you're using older code with newer models. In 1.3.0, we used to have this file that specifies the shape in 3 integers:. ```. $ gsutil cat gs://deepvariant/models/DeepVariant/1.3.0/DeepVariant-inception_v3-1.3.0+data-wgs_standard/model.ckpt.input_shape. 100 221 6. ```. In the later moment, we changed the format. For example, 1.4.0:. ```. $ gsutil cat gs://deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-wgs_standard/model.ckpt.example_info.json. {""version"": ""1.5.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. (We also added one more channel, which is why the shape is now 100 221 7.). From the error messages you're getting, it seems like you're using code version older than 1.4.0.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/627
https://github.com/google/deepvariant/issues/627:764,integrability,version,version,764,"@richard-nm Can you paste your command too? I wonder if you're using older code with newer models. In 1.3.0, we used to have this file that specifies the shape in 3 integers:. ```. $ gsutil cat gs://deepvariant/models/DeepVariant/1.3.0/DeepVariant-inception_v3-1.3.0+data-wgs_standard/model.ckpt.input_shape. 100 221 6. ```. In the later moment, we changed the format. For example, 1.4.0:. ```. $ gsutil cat gs://deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-wgs_standard/model.ckpt.example_info.json. {""version"": ""1.5.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. (We also added one more channel, which is why the shape is now 100 221 7.). From the error messages you're getting, it seems like you're using code version older than 1.4.0.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/627
https://github.com/google/deepvariant/issues/627:140,interoperability,specif,specifies,140,"@richard-nm Can you paste your command too? I wonder if you're using older code with newer models. In 1.3.0, we used to have this file that specifies the shape in 3 integers:. ```. $ gsutil cat gs://deepvariant/models/DeepVariant/1.3.0/DeepVariant-inception_v3-1.3.0+data-wgs_standard/model.ckpt.input_shape. 100 221 6. ```. In the later moment, we changed the format. For example, 1.4.0:. ```. $ gsutil cat gs://deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-wgs_standard/model.ckpt.example_info.json. {""version"": ""1.5.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. (We also added one more channel, which is why the shape is now 100 221 7.). From the error messages you're getting, it seems like you're using code version older than 1.4.0.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/627
https://github.com/google/deepvariant/issues/627:361,interoperability,format,format,361,"@richard-nm Can you paste your command too? I wonder if you're using older code with newer models. In 1.3.0, we used to have this file that specifies the shape in 3 integers:. ```. $ gsutil cat gs://deepvariant/models/DeepVariant/1.3.0/DeepVariant-inception_v3-1.3.0+data-wgs_standard/model.ckpt.input_shape. 100 221 6. ```. In the later moment, we changed the format. For example, 1.4.0:. ```. $ gsutil cat gs://deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-wgs_standard/model.ckpt.example_info.json. {""version"": ""1.5.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. (We also added one more channel, which is why the shape is now 100 221 7.). From the error messages you're getting, it seems like you're using code version older than 1.4.0.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/627
https://github.com/google/deepvariant/issues/627:707,interoperability,messag,messages,707,"@richard-nm Can you paste your command too? I wonder if you're using older code with newer models. In 1.3.0, we used to have this file that specifies the shape in 3 integers:. ```. $ gsutil cat gs://deepvariant/models/DeepVariant/1.3.0/DeepVariant-inception_v3-1.3.0+data-wgs_standard/model.ckpt.input_shape. 100 221 6. ```. In the later moment, we changed the format. For example, 1.4.0:. ```. $ gsutil cat gs://deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-wgs_standard/model.ckpt.example_info.json. {""version"": ""1.5.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. (We also added one more channel, which is why the shape is now 100 221 7.). From the error messages you're getting, it seems like you're using code version older than 1.4.0.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/627
https://github.com/google/deepvariant/issues/627:531,modifiability,version,version,531,"@richard-nm Can you paste your command too? I wonder if you're using older code with newer models. In 1.3.0, we used to have this file that specifies the shape in 3 integers:. ```. $ gsutil cat gs://deepvariant/models/DeepVariant/1.3.0/DeepVariant-inception_v3-1.3.0+data-wgs_standard/model.ckpt.input_shape. 100 221 6. ```. In the later moment, we changed the format. For example, 1.4.0:. ```. $ gsutil cat gs://deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-wgs_standard/model.ckpt.example_info.json. {""version"": ""1.5.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. (We also added one more channel, which is why the shape is now 100 221 7.). From the error messages you're getting, it seems like you're using code version older than 1.4.0.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/627
https://github.com/google/deepvariant/issues/627:764,modifiability,version,version,764,"@richard-nm Can you paste your command too? I wonder if you're using older code with newer models. In 1.3.0, we used to have this file that specifies the shape in 3 integers:. ```. $ gsutil cat gs://deepvariant/models/DeepVariant/1.3.0/DeepVariant-inception_v3-1.3.0+data-wgs_standard/model.ckpt.input_shape. 100 221 6. ```. In the later moment, we changed the format. For example, 1.4.0:. ```. $ gsutil cat gs://deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-wgs_standard/model.ckpt.example_info.json. {""version"": ""1.5.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. (We also added one more channel, which is why the shape is now 100 221 7.). From the error messages you're getting, it seems like you're using code version older than 1.4.0.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/627
https://github.com/google/deepvariant/issues/627:701,performance,error,error,701,"@richard-nm Can you paste your command too? I wonder if you're using older code with newer models. In 1.3.0, we used to have this file that specifies the shape in 3 integers:. ```. $ gsutil cat gs://deepvariant/models/DeepVariant/1.3.0/DeepVariant-inception_v3-1.3.0+data-wgs_standard/model.ckpt.input_shape. 100 221 6. ```. In the later moment, we changed the format. For example, 1.4.0:. ```. $ gsutil cat gs://deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-wgs_standard/model.ckpt.example_info.json. {""version"": ""1.5.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. (We also added one more channel, which is why the shape is now 100 221 7.). From the error messages you're getting, it seems like you're using code version older than 1.4.0.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/627
https://github.com/google/deepvariant/issues/627:701,safety,error,error,701,"@richard-nm Can you paste your command too? I wonder if you're using older code with newer models. In 1.3.0, we used to have this file that specifies the shape in 3 integers:. ```. $ gsutil cat gs://deepvariant/models/DeepVariant/1.3.0/DeepVariant-inception_v3-1.3.0+data-wgs_standard/model.ckpt.input_shape. 100 221 6. ```. In the later moment, we changed the format. For example, 1.4.0:. ```. $ gsutil cat gs://deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-wgs_standard/model.ckpt.example_info.json. {""version"": ""1.5.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. (We also added one more channel, which is why the shape is now 100 221 7.). From the error messages you're getting, it seems like you're using code version older than 1.4.0.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/627
https://github.com/google/deepvariant/issues/627:91,security,model,models,91,"@richard-nm Can you paste your command too? I wonder if you're using older code with newer models. In 1.3.0, we used to have this file that specifies the shape in 3 integers:. ```. $ gsutil cat gs://deepvariant/models/DeepVariant/1.3.0/DeepVariant-inception_v3-1.3.0+data-wgs_standard/model.ckpt.input_shape. 100 221 6. ```. In the later moment, we changed the format. For example, 1.4.0:. ```. $ gsutil cat gs://deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-wgs_standard/model.ckpt.example_info.json. {""version"": ""1.5.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. (We also added one more channel, which is why the shape is now 100 221 7.). From the error messages you're getting, it seems like you're using code version older than 1.4.0.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/627
https://github.com/google/deepvariant/issues/627:211,security,model,models,211,"@richard-nm Can you paste your command too? I wonder if you're using older code with newer models. In 1.3.0, we used to have this file that specifies the shape in 3 integers:. ```. $ gsutil cat gs://deepvariant/models/DeepVariant/1.3.0/DeepVariant-inception_v3-1.3.0+data-wgs_standard/model.ckpt.input_shape. 100 221 6. ```. In the later moment, we changed the format. For example, 1.4.0:. ```. $ gsutil cat gs://deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-wgs_standard/model.ckpt.example_info.json. {""version"": ""1.5.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. (We also added one more channel, which is why the shape is now 100 221 7.). From the error messages you're getting, it seems like you're using code version older than 1.4.0.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/627
https://github.com/google/deepvariant/issues/627:285,security,model,model,285,"@richard-nm Can you paste your command too? I wonder if you're using older code with newer models. In 1.3.0, we used to have this file that specifies the shape in 3 integers:. ```. $ gsutil cat gs://deepvariant/models/DeepVariant/1.3.0/DeepVariant-inception_v3-1.3.0+data-wgs_standard/model.ckpt.input_shape. 100 221 6. ```. In the later moment, we changed the format. For example, 1.4.0:. ```. $ gsutil cat gs://deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-wgs_standard/model.ckpt.example_info.json. {""version"": ""1.5.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. (We also added one more channel, which is why the shape is now 100 221 7.). From the error messages you're getting, it seems like you're using code version older than 1.4.0.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/627
https://github.com/google/deepvariant/issues/627:425,security,model,models,425,"@richard-nm Can you paste your command too? I wonder if you're using older code with newer models. In 1.3.0, we used to have this file that specifies the shape in 3 integers:. ```. $ gsutil cat gs://deepvariant/models/DeepVariant/1.3.0/DeepVariant-inception_v3-1.3.0+data-wgs_standard/model.ckpt.input_shape. 100 221 6. ```. In the later moment, we changed the format. For example, 1.4.0:. ```. $ gsutil cat gs://deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-wgs_standard/model.ckpt.example_info.json. {""version"": ""1.5.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. (We also added one more channel, which is why the shape is now 100 221 7.). From the error messages you're getting, it seems like you're using code version older than 1.4.0.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/627
https://github.com/google/deepvariant/issues/627:499,security,model,model,499,"@richard-nm Can you paste your command too? I wonder if you're using older code with newer models. In 1.3.0, we used to have this file that specifies the shape in 3 integers:. ```. $ gsutil cat gs://deepvariant/models/DeepVariant/1.3.0/DeepVariant-inception_v3-1.3.0+data-wgs_standard/model.ckpt.input_shape. 100 221 6. ```. In the later moment, we changed the format. For example, 1.4.0:. ```. $ gsutil cat gs://deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-wgs_standard/model.ckpt.example_info.json. {""version"": ""1.5.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. (We also added one more channel, which is why the shape is now 100 221 7.). From the error messages you're getting, it seems like you're using code version older than 1.4.0.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/627
https://github.com/google/deepvariant/issues/627:31,usability,command,command,31,"@richard-nm Can you paste your command too? I wonder if you're using older code with newer models. In 1.3.0, we used to have this file that specifies the shape in 3 integers:. ```. $ gsutil cat gs://deepvariant/models/DeepVariant/1.3.0/DeepVariant-inception_v3-1.3.0+data-wgs_standard/model.ckpt.input_shape. 100 221 6. ```. In the later moment, we changed the format. For example, 1.4.0:. ```. $ gsutil cat gs://deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-wgs_standard/model.ckpt.example_info.json. {""version"": ""1.5.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. (We also added one more channel, which is why the shape is now 100 221 7.). From the error messages you're getting, it seems like you're using code version older than 1.4.0.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/627
https://github.com/google/deepvariant/issues/627:701,usability,error,error,701,"@richard-nm Can you paste your command too? I wonder if you're using older code with newer models. In 1.3.0, we used to have this file that specifies the shape in 3 integers:. ```. $ gsutil cat gs://deepvariant/models/DeepVariant/1.3.0/DeepVariant-inception_v3-1.3.0+data-wgs_standard/model.ckpt.input_shape. 100 221 6. ```. In the later moment, we changed the format. For example, 1.4.0:. ```. $ gsutil cat gs://deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-wgs_standard/model.ckpt.example_info.json. {""version"": ""1.5.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. (We also added one more channel, which is why the shape is now 100 221 7.). From the error messages you're getting, it seems like you're using code version older than 1.4.0.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/627
https://github.com/google/deepvariant/issues/627:30,availability,error,error,30,"Sorry for my wrong posting of error messages. I have tried different versions of deepvariant in conda, and messed up the conda environment. . Actually I posted the deepvarint (1.3.0) error messages. : <(. Here is my attempts on the deepvariant 1.4.0: . `. $ conda create -n deepvar python=3.7.5. $ conda install -c bioconda deepvariant=1.4.0. $ conda activate deepvar. $ dv_call_variants.py -h . `. And I got the similar error messages:. Baseline DeepVariant arguments. File ""/tmp/Bazel.runfiles_j_t9fnwc/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 374. raise ValueError(f'Shape mismatch in {example_info_json} and '. ^. SyntaxError: invalid syntax. Wrapper arguments. usage: dv_call_variants.py [--cores CORES] --outfile OUTFILE --examples. EXAMPLES --sample SAMPLE. [--model {hybrid,pacbio,wes,wgs}] [-h]. DeepVariant call_variants wrapper. optional arguments:. --cores CORES. --outfile OUTFILE. --examples EXAMPLES Example directory from make_examples. --sample SAMPLE Sample name. --model {hybrid,pacbio,wes,wgs}. DeepVariant trained model to use, defaults to wgs. -h, --help. usage: dv_call_variants.py [--cores CORES] --outfile OUTFILE --examples. EXAMPLES --sample SAMPLE. [--model {hybrid,pacbio,wes,wgs}] [-h]. dv_call_variants.py: error: the following arguments are required: --outfile, --examples, --sample.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/627
https://github.com/google/deepvariant/issues/627:183,availability,error,error,183,"Sorry for my wrong posting of error messages. I have tried different versions of deepvariant in conda, and messed up the conda environment. . Actually I posted the deepvarint (1.3.0) error messages. : <(. Here is my attempts on the deepvariant 1.4.0: . `. $ conda create -n deepvar python=3.7.5. $ conda install -c bioconda deepvariant=1.4.0. $ conda activate deepvar. $ dv_call_variants.py -h . `. And I got the similar error messages:. Baseline DeepVariant arguments. File ""/tmp/Bazel.runfiles_j_t9fnwc/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 374. raise ValueError(f'Shape mismatch in {example_info_json} and '. ^. SyntaxError: invalid syntax. Wrapper arguments. usage: dv_call_variants.py [--cores CORES] --outfile OUTFILE --examples. EXAMPLES --sample SAMPLE. [--model {hybrid,pacbio,wes,wgs}] [-h]. DeepVariant call_variants wrapper. optional arguments:. --cores CORES. --outfile OUTFILE. --examples EXAMPLES Example directory from make_examples. --sample SAMPLE Sample name. --model {hybrid,pacbio,wes,wgs}. DeepVariant trained model to use, defaults to wgs. -h, --help. usage: dv_call_variants.py [--cores CORES] --outfile OUTFILE --examples. EXAMPLES --sample SAMPLE. [--model {hybrid,pacbio,wes,wgs}] [-h]. dv_call_variants.py: error: the following arguments are required: --outfile, --examples, --sample.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/627
https://github.com/google/deepvariant/issues/627:421,availability,error,error,421,"Sorry for my wrong posting of error messages. I have tried different versions of deepvariant in conda, and messed up the conda environment. . Actually I posted the deepvarint (1.3.0) error messages. : <(. Here is my attempts on the deepvariant 1.4.0: . `. $ conda create -n deepvar python=3.7.5. $ conda install -c bioconda deepvariant=1.4.0. $ conda activate deepvar. $ dv_call_variants.py -h . `. And I got the similar error messages:. Baseline DeepVariant arguments. File ""/tmp/Bazel.runfiles_j_t9fnwc/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 374. raise ValueError(f'Shape mismatch in {example_info_json} and '. ^. SyntaxError: invalid syntax. Wrapper arguments. usage: dv_call_variants.py [--cores CORES] --outfile OUTFILE --examples. EXAMPLES --sample SAMPLE. [--model {hybrid,pacbio,wes,wgs}] [-h]. DeepVariant call_variants wrapper. optional arguments:. --cores CORES. --outfile OUTFILE. --examples EXAMPLES Example directory from make_examples. --sample SAMPLE Sample name. --model {hybrid,pacbio,wes,wgs}. DeepVariant trained model to use, defaults to wgs. -h, --help. usage: dv_call_variants.py [--cores CORES] --outfile OUTFILE --examples. EXAMPLES --sample SAMPLE. [--model {hybrid,pacbio,wes,wgs}] [-h]. dv_call_variants.py: error: the following arguments are required: --outfile, --examples, --sample.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/627
https://github.com/google/deepvariant/issues/627:1265,availability,error,error,1265,"Sorry for my wrong posting of error messages. I have tried different versions of deepvariant in conda, and messed up the conda environment. . Actually I posted the deepvarint (1.3.0) error messages. : <(. Here is my attempts on the deepvariant 1.4.0: . `. $ conda create -n deepvar python=3.7.5. $ conda install -c bioconda deepvariant=1.4.0. $ conda activate deepvar. $ dv_call_variants.py -h . `. And I got the similar error messages:. Baseline DeepVariant arguments. File ""/tmp/Bazel.runfiles_j_t9fnwc/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 374. raise ValueError(f'Shape mismatch in {example_info_json} and '. ^. SyntaxError: invalid syntax. Wrapper arguments. usage: dv_call_variants.py [--cores CORES] --outfile OUTFILE --examples. EXAMPLES --sample SAMPLE. [--model {hybrid,pacbio,wes,wgs}] [-h]. DeepVariant call_variants wrapper. optional arguments:. --cores CORES. --outfile OUTFILE. --examples EXAMPLES Example directory from make_examples. --sample SAMPLE Sample name. --model {hybrid,pacbio,wes,wgs}. DeepVariant trained model to use, defaults to wgs. -h, --help. usage: dv_call_variants.py [--cores CORES] --outfile OUTFILE --examples. EXAMPLES --sample SAMPLE. [--model {hybrid,pacbio,wes,wgs}] [-h]. dv_call_variants.py: error: the following arguments are required: --outfile, --examples, --sample.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/627
https://github.com/google/deepvariant/issues/627:69,deployability,version,versions,69,"Sorry for my wrong posting of error messages. I have tried different versions of deepvariant in conda, and messed up the conda environment. . Actually I posted the deepvarint (1.3.0) error messages. : <(. Here is my attempts on the deepvariant 1.4.0: . `. $ conda create -n deepvar python=3.7.5. $ conda install -c bioconda deepvariant=1.4.0. $ conda activate deepvar. $ dv_call_variants.py -h . `. And I got the similar error messages:. Baseline DeepVariant arguments. File ""/tmp/Bazel.runfiles_j_t9fnwc/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 374. raise ValueError(f'Shape mismatch in {example_info_json} and '. ^. SyntaxError: invalid syntax. Wrapper arguments. usage: dv_call_variants.py [--cores CORES] --outfile OUTFILE --examples. EXAMPLES --sample SAMPLE. [--model {hybrid,pacbio,wes,wgs}] [-h]. DeepVariant call_variants wrapper. optional arguments:. --cores CORES. --outfile OUTFILE. --examples EXAMPLES Example directory from make_examples. --sample SAMPLE Sample name. --model {hybrid,pacbio,wes,wgs}. DeepVariant trained model to use, defaults to wgs. -h, --help. usage: dv_call_variants.py [--cores CORES] --outfile OUTFILE --examples. EXAMPLES --sample SAMPLE. [--model {hybrid,pacbio,wes,wgs}] [-h]. dv_call_variants.py: error: the following arguments are required: --outfile, --examples, --sample.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/627
https://github.com/google/deepvariant/issues/627:304,deployability,instal,install,304,"Sorry for my wrong posting of error messages. I have tried different versions of deepvariant in conda, and messed up the conda environment. . Actually I posted the deepvarint (1.3.0) error messages. : <(. Here is my attempts on the deepvariant 1.4.0: . `. $ conda create -n deepvar python=3.7.5. $ conda install -c bioconda deepvariant=1.4.0. $ conda activate deepvar. $ dv_call_variants.py -h . `. And I got the similar error messages:. Baseline DeepVariant arguments. File ""/tmp/Bazel.runfiles_j_t9fnwc/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 374. raise ValueError(f'Shape mismatch in {example_info_json} and '. ^. SyntaxError: invalid syntax. Wrapper arguments. usage: dv_call_variants.py [--cores CORES] --outfile OUTFILE --examples. EXAMPLES --sample SAMPLE. [--model {hybrid,pacbio,wes,wgs}] [-h]. DeepVariant call_variants wrapper. optional arguments:. --cores CORES. --outfile OUTFILE. --examples EXAMPLES Example directory from make_examples. --sample SAMPLE Sample name. --model {hybrid,pacbio,wes,wgs}. DeepVariant trained model to use, defaults to wgs. -h, --help. usage: dv_call_variants.py [--cores CORES] --outfile OUTFILE --examples. EXAMPLES --sample SAMPLE. [--model {hybrid,pacbio,wes,wgs}] [-h]. dv_call_variants.py: error: the following arguments are required: --outfile, --examples, --sample.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/627
https://github.com/google/deepvariant/issues/627:723,energy efficiency,core,cores,723,"Sorry for my wrong posting of error messages. I have tried different versions of deepvariant in conda, and messed up the conda environment. . Actually I posted the deepvarint (1.3.0) error messages. : <(. Here is my attempts on the deepvariant 1.4.0: . `. $ conda create -n deepvar python=3.7.5. $ conda install -c bioconda deepvariant=1.4.0. $ conda activate deepvar. $ dv_call_variants.py -h . `. And I got the similar error messages:. Baseline DeepVariant arguments. File ""/tmp/Bazel.runfiles_j_t9fnwc/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 374. raise ValueError(f'Shape mismatch in {example_info_json} and '. ^. SyntaxError: invalid syntax. Wrapper arguments. usage: dv_call_variants.py [--cores CORES] --outfile OUTFILE --examples. EXAMPLES --sample SAMPLE. [--model {hybrid,pacbio,wes,wgs}] [-h]. DeepVariant call_variants wrapper. optional arguments:. --cores CORES. --outfile OUTFILE. --examples EXAMPLES Example directory from make_examples. --sample SAMPLE Sample name. --model {hybrid,pacbio,wes,wgs}. DeepVariant trained model to use, defaults to wgs. -h, --help. usage: dv_call_variants.py [--cores CORES] --outfile OUTFILE --examples. EXAMPLES --sample SAMPLE. [--model {hybrid,pacbio,wes,wgs}] [-h]. dv_call_variants.py: error: the following arguments are required: --outfile, --examples, --sample.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/627
https://github.com/google/deepvariant/issues/627:729,energy efficiency,CORE,CORES,729,"Sorry for my wrong posting of error messages. I have tried different versions of deepvariant in conda, and messed up the conda environment. . Actually I posted the deepvarint (1.3.0) error messages. : <(. Here is my attempts on the deepvariant 1.4.0: . `. $ conda create -n deepvar python=3.7.5. $ conda install -c bioconda deepvariant=1.4.0. $ conda activate deepvar. $ dv_call_variants.py -h . `. And I got the similar error messages:. Baseline DeepVariant arguments. File ""/tmp/Bazel.runfiles_j_t9fnwc/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 374. raise ValueError(f'Shape mismatch in {example_info_json} and '. ^. SyntaxError: invalid syntax. Wrapper arguments. usage: dv_call_variants.py [--cores CORES] --outfile OUTFILE --examples. EXAMPLES --sample SAMPLE. [--model {hybrid,pacbio,wes,wgs}] [-h]. DeepVariant call_variants wrapper. optional arguments:. --cores CORES. --outfile OUTFILE. --examples EXAMPLES Example directory from make_examples. --sample SAMPLE Sample name. --model {hybrid,pacbio,wes,wgs}. DeepVariant trained model to use, defaults to wgs. -h, --help. usage: dv_call_variants.py [--cores CORES] --outfile OUTFILE --examples. EXAMPLES --sample SAMPLE. [--model {hybrid,pacbio,wes,wgs}] [-h]. dv_call_variants.py: error: the following arguments are required: --outfile, --examples, --sample.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/627
https://github.com/google/deepvariant/issues/627:795,energy efficiency,model,model,795,"Sorry for my wrong posting of error messages. I have tried different versions of deepvariant in conda, and messed up the conda environment. . Actually I posted the deepvarint (1.3.0) error messages. : <(. Here is my attempts on the deepvariant 1.4.0: . `. $ conda create -n deepvar python=3.7.5. $ conda install -c bioconda deepvariant=1.4.0. $ conda activate deepvar. $ dv_call_variants.py -h . `. And I got the similar error messages:. Baseline DeepVariant arguments. File ""/tmp/Bazel.runfiles_j_t9fnwc/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 374. raise ValueError(f'Shape mismatch in {example_info_json} and '. ^. SyntaxError: invalid syntax. Wrapper arguments. usage: dv_call_variants.py [--cores CORES] --outfile OUTFILE --examples. EXAMPLES --sample SAMPLE. [--model {hybrid,pacbio,wes,wgs}] [-h]. DeepVariant call_variants wrapper. optional arguments:. --cores CORES. --outfile OUTFILE. --examples EXAMPLES Example directory from make_examples. --sample SAMPLE Sample name. --model {hybrid,pacbio,wes,wgs}. DeepVariant trained model to use, defaults to wgs. -h, --help. usage: dv_call_variants.py [--cores CORES] --outfile OUTFILE --examples. EXAMPLES --sample SAMPLE. [--model {hybrid,pacbio,wes,wgs}] [-h]. dv_call_variants.py: error: the following arguments are required: --outfile, --examples, --sample.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/627
https://github.com/google/deepvariant/issues/627:890,energy efficiency,core,cores,890,"Sorry for my wrong posting of error messages. I have tried different versions of deepvariant in conda, and messed up the conda environment. . Actually I posted the deepvarint (1.3.0) error messages. : <(. Here is my attempts on the deepvariant 1.4.0: . `. $ conda create -n deepvar python=3.7.5. $ conda install -c bioconda deepvariant=1.4.0. $ conda activate deepvar. $ dv_call_variants.py -h . `. And I got the similar error messages:. Baseline DeepVariant arguments. File ""/tmp/Bazel.runfiles_j_t9fnwc/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 374. raise ValueError(f'Shape mismatch in {example_info_json} and '. ^. SyntaxError: invalid syntax. Wrapper arguments. usage: dv_call_variants.py [--cores CORES] --outfile OUTFILE --examples. EXAMPLES --sample SAMPLE. [--model {hybrid,pacbio,wes,wgs}] [-h]. DeepVariant call_variants wrapper. optional arguments:. --cores CORES. --outfile OUTFILE. --examples EXAMPLES Example directory from make_examples. --sample SAMPLE Sample name. --model {hybrid,pacbio,wes,wgs}. DeepVariant trained model to use, defaults to wgs. -h, --help. usage: dv_call_variants.py [--cores CORES] --outfile OUTFILE --examples. EXAMPLES --sample SAMPLE. [--model {hybrid,pacbio,wes,wgs}] [-h]. dv_call_variants.py: error: the following arguments are required: --outfile, --examples, --sample.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/627
https://github.com/google/deepvariant/issues/627:896,energy efficiency,CORE,CORES,896,"Sorry for my wrong posting of error messages. I have tried different versions of deepvariant in conda, and messed up the conda environment. . Actually I posted the deepvarint (1.3.0) error messages. : <(. Here is my attempts on the deepvariant 1.4.0: . `. $ conda create -n deepvar python=3.7.5. $ conda install -c bioconda deepvariant=1.4.0. $ conda activate deepvar. $ dv_call_variants.py -h . `. And I got the similar error messages:. Baseline DeepVariant arguments. File ""/tmp/Bazel.runfiles_j_t9fnwc/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 374. raise ValueError(f'Shape mismatch in {example_info_json} and '. ^. SyntaxError: invalid syntax. Wrapper arguments. usage: dv_call_variants.py [--cores CORES] --outfile OUTFILE --examples. EXAMPLES --sample SAMPLE. [--model {hybrid,pacbio,wes,wgs}] [-h]. DeepVariant call_variants wrapper. optional arguments:. --cores CORES. --outfile OUTFILE. --examples EXAMPLES Example directory from make_examples. --sample SAMPLE Sample name. --model {hybrid,pacbio,wes,wgs}. DeepVariant trained model to use, defaults to wgs. -h, --help. usage: dv_call_variants.py [--cores CORES] --outfile OUTFILE --examples. EXAMPLES --sample SAMPLE. [--model {hybrid,pacbio,wes,wgs}] [-h]. dv_call_variants.py: error: the following arguments are required: --outfile, --examples, --sample.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/627
https://github.com/google/deepvariant/issues/627:1011,energy efficiency,model,model,1011,"Sorry for my wrong posting of error messages. I have tried different versions of deepvariant in conda, and messed up the conda environment. . Actually I posted the deepvarint (1.3.0) error messages. : <(. Here is my attempts on the deepvariant 1.4.0: . `. $ conda create -n deepvar python=3.7.5. $ conda install -c bioconda deepvariant=1.4.0. $ conda activate deepvar. $ dv_call_variants.py -h . `. And I got the similar error messages:. Baseline DeepVariant arguments. File ""/tmp/Bazel.runfiles_j_t9fnwc/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 374. raise ValueError(f'Shape mismatch in {example_info_json} and '. ^. SyntaxError: invalid syntax. Wrapper arguments. usage: dv_call_variants.py [--cores CORES] --outfile OUTFILE --examples. EXAMPLES --sample SAMPLE. [--model {hybrid,pacbio,wes,wgs}] [-h]. DeepVariant call_variants wrapper. optional arguments:. --cores CORES. --outfile OUTFILE. --examples EXAMPLES Example directory from make_examples. --sample SAMPLE Sample name. --model {hybrid,pacbio,wes,wgs}. DeepVariant trained model to use, defaults to wgs. -h, --help. usage: dv_call_variants.py [--cores CORES] --outfile OUTFILE --examples. EXAMPLES --sample SAMPLE. [--model {hybrid,pacbio,wes,wgs}] [-h]. dv_call_variants.py: error: the following arguments are required: --outfile, --examples, --sample.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/627
https://github.com/google/deepvariant/issues/627:1062,energy efficiency,model,model,1062,"Sorry for my wrong posting of error messages. I have tried different versions of deepvariant in conda, and messed up the conda environment. . Actually I posted the deepvarint (1.3.0) error messages. : <(. Here is my attempts on the deepvariant 1.4.0: . `. $ conda create -n deepvar python=3.7.5. $ conda install -c bioconda deepvariant=1.4.0. $ conda activate deepvar. $ dv_call_variants.py -h . `. And I got the similar error messages:. Baseline DeepVariant arguments. File ""/tmp/Bazel.runfiles_j_t9fnwc/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 374. raise ValueError(f'Shape mismatch in {example_info_json} and '. ^. SyntaxError: invalid syntax. Wrapper arguments. usage: dv_call_variants.py [--cores CORES] --outfile OUTFILE --examples. EXAMPLES --sample SAMPLE. [--model {hybrid,pacbio,wes,wgs}] [-h]. DeepVariant call_variants wrapper. optional arguments:. --cores CORES. --outfile OUTFILE. --examples EXAMPLES Example directory from make_examples. --sample SAMPLE Sample name. --model {hybrid,pacbio,wes,wgs}. DeepVariant trained model to use, defaults to wgs. -h, --help. usage: dv_call_variants.py [--cores CORES] --outfile OUTFILE --examples. EXAMPLES --sample SAMPLE. [--model {hybrid,pacbio,wes,wgs}] [-h]. dv_call_variants.py: error: the following arguments are required: --outfile, --examples, --sample.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/627
https://github.com/google/deepvariant/issues/627:1135,energy efficiency,core,cores,1135,"Sorry for my wrong posting of error messages. I have tried different versions of deepvariant in conda, and messed up the conda environment. . Actually I posted the deepvarint (1.3.0) error messages. : <(. Here is my attempts on the deepvariant 1.4.0: . `. $ conda create -n deepvar python=3.7.5. $ conda install -c bioconda deepvariant=1.4.0. $ conda activate deepvar. $ dv_call_variants.py -h . `. And I got the similar error messages:. Baseline DeepVariant arguments. File ""/tmp/Bazel.runfiles_j_t9fnwc/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 374. raise ValueError(f'Shape mismatch in {example_info_json} and '. ^. SyntaxError: invalid syntax. Wrapper arguments. usage: dv_call_variants.py [--cores CORES] --outfile OUTFILE --examples. EXAMPLES --sample SAMPLE. [--model {hybrid,pacbio,wes,wgs}] [-h]. DeepVariant call_variants wrapper. optional arguments:. --cores CORES. --outfile OUTFILE. --examples EXAMPLES Example directory from make_examples. --sample SAMPLE Sample name. --model {hybrid,pacbio,wes,wgs}. DeepVariant trained model to use, defaults to wgs. -h, --help. usage: dv_call_variants.py [--cores CORES] --outfile OUTFILE --examples. EXAMPLES --sample SAMPLE. [--model {hybrid,pacbio,wes,wgs}] [-h]. dv_call_variants.py: error: the following arguments are required: --outfile, --examples, --sample.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/627
https://github.com/google/deepvariant/issues/627:1141,energy efficiency,CORE,CORES,1141,"Sorry for my wrong posting of error messages. I have tried different versions of deepvariant in conda, and messed up the conda environment. . Actually I posted the deepvarint (1.3.0) error messages. : <(. Here is my attempts on the deepvariant 1.4.0: . `. $ conda create -n deepvar python=3.7.5. $ conda install -c bioconda deepvariant=1.4.0. $ conda activate deepvar. $ dv_call_variants.py -h . `. And I got the similar error messages:. Baseline DeepVariant arguments. File ""/tmp/Bazel.runfiles_j_t9fnwc/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 374. raise ValueError(f'Shape mismatch in {example_info_json} and '. ^. SyntaxError: invalid syntax. Wrapper arguments. usage: dv_call_variants.py [--cores CORES] --outfile OUTFILE --examples. EXAMPLES --sample SAMPLE. [--model {hybrid,pacbio,wes,wgs}] [-h]. DeepVariant call_variants wrapper. optional arguments:. --cores CORES. --outfile OUTFILE. --examples EXAMPLES Example directory from make_examples. --sample SAMPLE Sample name. --model {hybrid,pacbio,wes,wgs}. DeepVariant trained model to use, defaults to wgs. -h, --help. usage: dv_call_variants.py [--cores CORES] --outfile OUTFILE --examples. EXAMPLES --sample SAMPLE. [--model {hybrid,pacbio,wes,wgs}] [-h]. dv_call_variants.py: error: the following arguments are required: --outfile, --examples, --sample.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/627
https://github.com/google/deepvariant/issues/627:1207,energy efficiency,model,model,1207,"Sorry for my wrong posting of error messages. I have tried different versions of deepvariant in conda, and messed up the conda environment. . Actually I posted the deepvarint (1.3.0) error messages. : <(. Here is my attempts on the deepvariant 1.4.0: . `. $ conda create -n deepvar python=3.7.5. $ conda install -c bioconda deepvariant=1.4.0. $ conda activate deepvar. $ dv_call_variants.py -h . `. And I got the similar error messages:. Baseline DeepVariant arguments. File ""/tmp/Bazel.runfiles_j_t9fnwc/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 374. raise ValueError(f'Shape mismatch in {example_info_json} and '. ^. SyntaxError: invalid syntax. Wrapper arguments. usage: dv_call_variants.py [--cores CORES] --outfile OUTFILE --examples. EXAMPLES --sample SAMPLE. [--model {hybrid,pacbio,wes,wgs}] [-h]. DeepVariant call_variants wrapper. optional arguments:. --cores CORES. --outfile OUTFILE. --examples EXAMPLES Example directory from make_examples. --sample SAMPLE Sample name. --model {hybrid,pacbio,wes,wgs}. DeepVariant trained model to use, defaults to wgs. -h, --help. usage: dv_call_variants.py [--cores CORES] --outfile OUTFILE --examples. EXAMPLES --sample SAMPLE. [--model {hybrid,pacbio,wes,wgs}] [-h]. dv_call_variants.py: error: the following arguments are required: --outfile, --examples, --sample.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/627
https://github.com/google/deepvariant/issues/627:36,integrability,messag,messages,36,"Sorry for my wrong posting of error messages. I have tried different versions of deepvariant in conda, and messed up the conda environment. . Actually I posted the deepvarint (1.3.0) error messages. : <(. Here is my attempts on the deepvariant 1.4.0: . `. $ conda create -n deepvar python=3.7.5. $ conda install -c bioconda deepvariant=1.4.0. $ conda activate deepvar. $ dv_call_variants.py -h . `. And I got the similar error messages:. Baseline DeepVariant arguments. File ""/tmp/Bazel.runfiles_j_t9fnwc/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 374. raise ValueError(f'Shape mismatch in {example_info_json} and '. ^. SyntaxError: invalid syntax. Wrapper arguments. usage: dv_call_variants.py [--cores CORES] --outfile OUTFILE --examples. EXAMPLES --sample SAMPLE. [--model {hybrid,pacbio,wes,wgs}] [-h]. DeepVariant call_variants wrapper. optional arguments:. --cores CORES. --outfile OUTFILE. --examples EXAMPLES Example directory from make_examples. --sample SAMPLE Sample name. --model {hybrid,pacbio,wes,wgs}. DeepVariant trained model to use, defaults to wgs. -h, --help. usage: dv_call_variants.py [--cores CORES] --outfile OUTFILE --examples. EXAMPLES --sample SAMPLE. [--model {hybrid,pacbio,wes,wgs}] [-h]. dv_call_variants.py: error: the following arguments are required: --outfile, --examples, --sample.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/627
https://github.com/google/deepvariant/issues/627:69,integrability,version,versions,69,"Sorry for my wrong posting of error messages. I have tried different versions of deepvariant in conda, and messed up the conda environment. . Actually I posted the deepvarint (1.3.0) error messages. : <(. Here is my attempts on the deepvariant 1.4.0: . `. $ conda create -n deepvar python=3.7.5. $ conda install -c bioconda deepvariant=1.4.0. $ conda activate deepvar. $ dv_call_variants.py -h . `. And I got the similar error messages:. Baseline DeepVariant arguments. File ""/tmp/Bazel.runfiles_j_t9fnwc/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 374. raise ValueError(f'Shape mismatch in {example_info_json} and '. ^. SyntaxError: invalid syntax. Wrapper arguments. usage: dv_call_variants.py [--cores CORES] --outfile OUTFILE --examples. EXAMPLES --sample SAMPLE. [--model {hybrid,pacbio,wes,wgs}] [-h]. DeepVariant call_variants wrapper. optional arguments:. --cores CORES. --outfile OUTFILE. --examples EXAMPLES Example directory from make_examples. --sample SAMPLE Sample name. --model {hybrid,pacbio,wes,wgs}. DeepVariant trained model to use, defaults to wgs. -h, --help. usage: dv_call_variants.py [--cores CORES] --outfile OUTFILE --examples. EXAMPLES --sample SAMPLE. [--model {hybrid,pacbio,wes,wgs}] [-h]. dv_call_variants.py: error: the following arguments are required: --outfile, --examples, --sample.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/627
https://github.com/google/deepvariant/issues/627:189,integrability,messag,messages,189,"Sorry for my wrong posting of error messages. I have tried different versions of deepvariant in conda, and messed up the conda environment. . Actually I posted the deepvarint (1.3.0) error messages. : <(. Here is my attempts on the deepvariant 1.4.0: . `. $ conda create -n deepvar python=3.7.5. $ conda install -c bioconda deepvariant=1.4.0. $ conda activate deepvar. $ dv_call_variants.py -h . `. And I got the similar error messages:. Baseline DeepVariant arguments. File ""/tmp/Bazel.runfiles_j_t9fnwc/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 374. raise ValueError(f'Shape mismatch in {example_info_json} and '. ^. SyntaxError: invalid syntax. Wrapper arguments. usage: dv_call_variants.py [--cores CORES] --outfile OUTFILE --examples. EXAMPLES --sample SAMPLE. [--model {hybrid,pacbio,wes,wgs}] [-h]. DeepVariant call_variants wrapper. optional arguments:. --cores CORES. --outfile OUTFILE. --examples EXAMPLES Example directory from make_examples. --sample SAMPLE Sample name. --model {hybrid,pacbio,wes,wgs}. DeepVariant trained model to use, defaults to wgs. -h, --help. usage: dv_call_variants.py [--cores CORES] --outfile OUTFILE --examples. EXAMPLES --sample SAMPLE. [--model {hybrid,pacbio,wes,wgs}] [-h]. dv_call_variants.py: error: the following arguments are required: --outfile, --examples, --sample.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/627
https://github.com/google/deepvariant/issues/627:427,integrability,messag,messages,427,"Sorry for my wrong posting of error messages. I have tried different versions of deepvariant in conda, and messed up the conda environment. . Actually I posted the deepvarint (1.3.0) error messages. : <(. Here is my attempts on the deepvariant 1.4.0: . `. $ conda create -n deepvar python=3.7.5. $ conda install -c bioconda deepvariant=1.4.0. $ conda activate deepvar. $ dv_call_variants.py -h . `. And I got the similar error messages:. Baseline DeepVariant arguments. File ""/tmp/Bazel.runfiles_j_t9fnwc/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 374. raise ValueError(f'Shape mismatch in {example_info_json} and '. ^. SyntaxError: invalid syntax. Wrapper arguments. usage: dv_call_variants.py [--cores CORES] --outfile OUTFILE --examples. EXAMPLES --sample SAMPLE. [--model {hybrid,pacbio,wes,wgs}] [-h]. DeepVariant call_variants wrapper. optional arguments:. --cores CORES. --outfile OUTFILE. --examples EXAMPLES Example directory from make_examples. --sample SAMPLE Sample name. --model {hybrid,pacbio,wes,wgs}. DeepVariant trained model to use, defaults to wgs. -h, --help. usage: dv_call_variants.py [--cores CORES] --outfile OUTFILE --examples. EXAMPLES --sample SAMPLE. [--model {hybrid,pacbio,wes,wgs}] [-h]. dv_call_variants.py: error: the following arguments are required: --outfile, --examples, --sample.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/627
https://github.com/google/deepvariant/issues/627:674,integrability,Wrap,Wrapper,674,"Sorry for my wrong posting of error messages. I have tried different versions of deepvariant in conda, and messed up the conda environment. . Actually I posted the deepvarint (1.3.0) error messages. : <(. Here is my attempts on the deepvariant 1.4.0: . `. $ conda create -n deepvar python=3.7.5. $ conda install -c bioconda deepvariant=1.4.0. $ conda activate deepvar. $ dv_call_variants.py -h . `. And I got the similar error messages:. Baseline DeepVariant arguments. File ""/tmp/Bazel.runfiles_j_t9fnwc/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 374. raise ValueError(f'Shape mismatch in {example_info_json} and '. ^. SyntaxError: invalid syntax. Wrapper arguments. usage: dv_call_variants.py [--cores CORES] --outfile OUTFILE --examples. EXAMPLES --sample SAMPLE. [--model {hybrid,pacbio,wes,wgs}] [-h]. DeepVariant call_variants wrapper. optional arguments:. --cores CORES. --outfile OUTFILE. --examples EXAMPLES Example directory from make_examples. --sample SAMPLE Sample name. --model {hybrid,pacbio,wes,wgs}. DeepVariant trained model to use, defaults to wgs. -h, --help. usage: dv_call_variants.py [--cores CORES] --outfile OUTFILE --examples. EXAMPLES --sample SAMPLE. [--model {hybrid,pacbio,wes,wgs}] [-h]. dv_call_variants.py: error: the following arguments are required: --outfile, --examples, --sample.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/627
https://github.com/google/deepvariant/issues/627:858,integrability,wrap,wrapper,858,"Sorry for my wrong posting of error messages. I have tried different versions of deepvariant in conda, and messed up the conda environment. . Actually I posted the deepvarint (1.3.0) error messages. : <(. Here is my attempts on the deepvariant 1.4.0: . `. $ conda create -n deepvar python=3.7.5. $ conda install -c bioconda deepvariant=1.4.0. $ conda activate deepvar. $ dv_call_variants.py -h . `. And I got the similar error messages:. Baseline DeepVariant arguments. File ""/tmp/Bazel.runfiles_j_t9fnwc/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 374. raise ValueError(f'Shape mismatch in {example_info_json} and '. ^. SyntaxError: invalid syntax. Wrapper arguments. usage: dv_call_variants.py [--cores CORES] --outfile OUTFILE --examples. EXAMPLES --sample SAMPLE. [--model {hybrid,pacbio,wes,wgs}] [-h]. DeepVariant call_variants wrapper. optional arguments:. --cores CORES. --outfile OUTFILE. --examples EXAMPLES Example directory from make_examples. --sample SAMPLE Sample name. --model {hybrid,pacbio,wes,wgs}. DeepVariant trained model to use, defaults to wgs. -h, --help. usage: dv_call_variants.py [--cores CORES] --outfile OUTFILE --examples. EXAMPLES --sample SAMPLE. [--model {hybrid,pacbio,wes,wgs}] [-h]. dv_call_variants.py: error: the following arguments are required: --outfile, --examples, --sample.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/627
https://github.com/google/deepvariant/issues/627:36,interoperability,messag,messages,36,"Sorry for my wrong posting of error messages. I have tried different versions of deepvariant in conda, and messed up the conda environment. . Actually I posted the deepvarint (1.3.0) error messages. : <(. Here is my attempts on the deepvariant 1.4.0: . `. $ conda create -n deepvar python=3.7.5. $ conda install -c bioconda deepvariant=1.4.0. $ conda activate deepvar. $ dv_call_variants.py -h . `. And I got the similar error messages:. Baseline DeepVariant arguments. File ""/tmp/Bazel.runfiles_j_t9fnwc/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 374. raise ValueError(f'Shape mismatch in {example_info_json} and '. ^. SyntaxError: invalid syntax. Wrapper arguments. usage: dv_call_variants.py [--cores CORES] --outfile OUTFILE --examples. EXAMPLES --sample SAMPLE. [--model {hybrid,pacbio,wes,wgs}] [-h]. DeepVariant call_variants wrapper. optional arguments:. --cores CORES. --outfile OUTFILE. --examples EXAMPLES Example directory from make_examples. --sample SAMPLE Sample name. --model {hybrid,pacbio,wes,wgs}. DeepVariant trained model to use, defaults to wgs. -h, --help. usage: dv_call_variants.py [--cores CORES] --outfile OUTFILE --examples. EXAMPLES --sample SAMPLE. [--model {hybrid,pacbio,wes,wgs}] [-h]. dv_call_variants.py: error: the following arguments are required: --outfile, --examples, --sample.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/627
https://github.com/google/deepvariant/issues/627:189,interoperability,messag,messages,189,"Sorry for my wrong posting of error messages. I have tried different versions of deepvariant in conda, and messed up the conda environment. . Actually I posted the deepvarint (1.3.0) error messages. : <(. Here is my attempts on the deepvariant 1.4.0: . `. $ conda create -n deepvar python=3.7.5. $ conda install -c bioconda deepvariant=1.4.0. $ conda activate deepvar. $ dv_call_variants.py -h . `. And I got the similar error messages:. Baseline DeepVariant arguments. File ""/tmp/Bazel.runfiles_j_t9fnwc/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 374. raise ValueError(f'Shape mismatch in {example_info_json} and '. ^. SyntaxError: invalid syntax. Wrapper arguments. usage: dv_call_variants.py [--cores CORES] --outfile OUTFILE --examples. EXAMPLES --sample SAMPLE. [--model {hybrid,pacbio,wes,wgs}] [-h]. DeepVariant call_variants wrapper. optional arguments:. --cores CORES. --outfile OUTFILE. --examples EXAMPLES Example directory from make_examples. --sample SAMPLE Sample name. --model {hybrid,pacbio,wes,wgs}. DeepVariant trained model to use, defaults to wgs. -h, --help. usage: dv_call_variants.py [--cores CORES] --outfile OUTFILE --examples. EXAMPLES --sample SAMPLE. [--model {hybrid,pacbio,wes,wgs}] [-h]. dv_call_variants.py: error: the following arguments are required: --outfile, --examples, --sample.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/627
https://github.com/google/deepvariant/issues/627:427,interoperability,messag,messages,427,"Sorry for my wrong posting of error messages. I have tried different versions of deepvariant in conda, and messed up the conda environment. . Actually I posted the deepvarint (1.3.0) error messages. : <(. Here is my attempts on the deepvariant 1.4.0: . `. $ conda create -n deepvar python=3.7.5. $ conda install -c bioconda deepvariant=1.4.0. $ conda activate deepvar. $ dv_call_variants.py -h . `. And I got the similar error messages:. Baseline DeepVariant arguments. File ""/tmp/Bazel.runfiles_j_t9fnwc/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 374. raise ValueError(f'Shape mismatch in {example_info_json} and '. ^. SyntaxError: invalid syntax. Wrapper arguments. usage: dv_call_variants.py [--cores CORES] --outfile OUTFILE --examples. EXAMPLES --sample SAMPLE. [--model {hybrid,pacbio,wes,wgs}] [-h]. DeepVariant call_variants wrapper. optional arguments:. --cores CORES. --outfile OUTFILE. --examples EXAMPLES Example directory from make_examples. --sample SAMPLE Sample name. --model {hybrid,pacbio,wes,wgs}. DeepVariant trained model to use, defaults to wgs. -h, --help. usage: dv_call_variants.py [--cores CORES] --outfile OUTFILE --examples. EXAMPLES --sample SAMPLE. [--model {hybrid,pacbio,wes,wgs}] [-h]. dv_call_variants.py: error: the following arguments are required: --outfile, --examples, --sample.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/627
https://github.com/google/deepvariant/issues/627:603,interoperability,mismatch,mismatch,603,"Sorry for my wrong posting of error messages. I have tried different versions of deepvariant in conda, and messed up the conda environment. . Actually I posted the deepvarint (1.3.0) error messages. : <(. Here is my attempts on the deepvariant 1.4.0: . `. $ conda create -n deepvar python=3.7.5. $ conda install -c bioconda deepvariant=1.4.0. $ conda activate deepvar. $ dv_call_variants.py -h . `. And I got the similar error messages:. Baseline DeepVariant arguments. File ""/tmp/Bazel.runfiles_j_t9fnwc/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 374. raise ValueError(f'Shape mismatch in {example_info_json} and '. ^. SyntaxError: invalid syntax. Wrapper arguments. usage: dv_call_variants.py [--cores CORES] --outfile OUTFILE --examples. EXAMPLES --sample SAMPLE. [--model {hybrid,pacbio,wes,wgs}] [-h]. DeepVariant call_variants wrapper. optional arguments:. --cores CORES. --outfile OUTFILE. --examples EXAMPLES Example directory from make_examples. --sample SAMPLE Sample name. --model {hybrid,pacbio,wes,wgs}. DeepVariant trained model to use, defaults to wgs. -h, --help. usage: dv_call_variants.py [--cores CORES] --outfile OUTFILE --examples. EXAMPLES --sample SAMPLE. [--model {hybrid,pacbio,wes,wgs}] [-h]. dv_call_variants.py: error: the following arguments are required: --outfile, --examples, --sample.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/627
https://github.com/google/deepvariant/issues/627:674,interoperability,Wrapper,Wrapper,674,"Sorry for my wrong posting of error messages. I have tried different versions of deepvariant in conda, and messed up the conda environment. . Actually I posted the deepvarint (1.3.0) error messages. : <(. Here is my attempts on the deepvariant 1.4.0: . `. $ conda create -n deepvar python=3.7.5. $ conda install -c bioconda deepvariant=1.4.0. $ conda activate deepvar. $ dv_call_variants.py -h . `. And I got the similar error messages:. Baseline DeepVariant arguments. File ""/tmp/Bazel.runfiles_j_t9fnwc/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 374. raise ValueError(f'Shape mismatch in {example_info_json} and '. ^. SyntaxError: invalid syntax. Wrapper arguments. usage: dv_call_variants.py [--cores CORES] --outfile OUTFILE --examples. EXAMPLES --sample SAMPLE. [--model {hybrid,pacbio,wes,wgs}] [-h]. DeepVariant call_variants wrapper. optional arguments:. --cores CORES. --outfile OUTFILE. --examples EXAMPLES Example directory from make_examples. --sample SAMPLE Sample name. --model {hybrid,pacbio,wes,wgs}. DeepVariant trained model to use, defaults to wgs. -h, --help. usage: dv_call_variants.py [--cores CORES] --outfile OUTFILE --examples. EXAMPLES --sample SAMPLE. [--model {hybrid,pacbio,wes,wgs}] [-h]. dv_call_variants.py: error: the following arguments are required: --outfile, --examples, --sample.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/627
