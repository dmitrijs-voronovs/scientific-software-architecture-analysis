id,quality_attribute,keyword,matched_word,match_idx,sentence,source,author,repo,version,wiki,url
https://github.com/google/deepvariant/issues/627:858,interoperability,wrapper,wrapper,858,"Sorry for my wrong posting of error messages. I have tried different versions of deepvariant in conda, and messed up the conda environment. . Actually I posted the deepvarint (1.3.0) error messages. : <(. Here is my attempts on the deepvariant 1.4.0: . `. $ conda create -n deepvar python=3.7.5. $ conda install -c bioconda deepvariant=1.4.0. $ conda activate deepvar. $ dv_call_variants.py -h . `. And I got the similar error messages:. Baseline DeepVariant arguments. File ""/tmp/Bazel.runfiles_j_t9fnwc/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 374. raise ValueError(f'Shape mismatch in {example_info_json} and '. ^. SyntaxError: invalid syntax. Wrapper arguments. usage: dv_call_variants.py [--cores CORES] --outfile OUTFILE --examples. EXAMPLES --sample SAMPLE. [--model {hybrid,pacbio,wes,wgs}] [-h]. DeepVariant call_variants wrapper. optional arguments:. --cores CORES. --outfile OUTFILE. --examples EXAMPLES Example directory from make_examples. --sample SAMPLE Sample name. --model {hybrid,pacbio,wes,wgs}. DeepVariant trained model to use, defaults to wgs. -h, --help. usage: dv_call_variants.py [--cores CORES] --outfile OUTFILE --examples. EXAMPLES --sample SAMPLE. [--model {hybrid,pacbio,wes,wgs}] [-h]. dv_call_variants.py: error: the following arguments are required: --outfile, --examples, --sample.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/627
https://github.com/google/deepvariant/issues/627:69,modifiability,version,versions,69,"Sorry for my wrong posting of error messages. I have tried different versions of deepvariant in conda, and messed up the conda environment. . Actually I posted the deepvarint (1.3.0) error messages. : <(. Here is my attempts on the deepvariant 1.4.0: . `. $ conda create -n deepvar python=3.7.5. $ conda install -c bioconda deepvariant=1.4.0. $ conda activate deepvar. $ dv_call_variants.py -h . `. And I got the similar error messages:. Baseline DeepVariant arguments. File ""/tmp/Bazel.runfiles_j_t9fnwc/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 374. raise ValueError(f'Shape mismatch in {example_info_json} and '. ^. SyntaxError: invalid syntax. Wrapper arguments. usage: dv_call_variants.py [--cores CORES] --outfile OUTFILE --examples. EXAMPLES --sample SAMPLE. [--model {hybrid,pacbio,wes,wgs}] [-h]. DeepVariant call_variants wrapper. optional arguments:. --cores CORES. --outfile OUTFILE. --examples EXAMPLES Example directory from make_examples. --sample SAMPLE Sample name. --model {hybrid,pacbio,wes,wgs}. DeepVariant trained model to use, defaults to wgs. -h, --help. usage: dv_call_variants.py [--cores CORES] --outfile OUTFILE --examples. EXAMPLES --sample SAMPLE. [--model {hybrid,pacbio,wes,wgs}] [-h]. dv_call_variants.py: error: the following arguments are required: --outfile, --examples, --sample.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/627
https://github.com/google/deepvariant/issues/627:809,modifiability,pac,pacbio,809,"Sorry for my wrong posting of error messages. I have tried different versions of deepvariant in conda, and messed up the conda environment. . Actually I posted the deepvarint (1.3.0) error messages. : <(. Here is my attempts on the deepvariant 1.4.0: . `. $ conda create -n deepvar python=3.7.5. $ conda install -c bioconda deepvariant=1.4.0. $ conda activate deepvar. $ dv_call_variants.py -h . `. And I got the similar error messages:. Baseline DeepVariant arguments. File ""/tmp/Bazel.runfiles_j_t9fnwc/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 374. raise ValueError(f'Shape mismatch in {example_info_json} and '. ^. SyntaxError: invalid syntax. Wrapper arguments. usage: dv_call_variants.py [--cores CORES] --outfile OUTFILE --examples. EXAMPLES --sample SAMPLE. [--model {hybrid,pacbio,wes,wgs}] [-h]. DeepVariant call_variants wrapper. optional arguments:. --cores CORES. --outfile OUTFILE. --examples EXAMPLES Example directory from make_examples. --sample SAMPLE Sample name. --model {hybrid,pacbio,wes,wgs}. DeepVariant trained model to use, defaults to wgs. -h, --help. usage: dv_call_variants.py [--cores CORES] --outfile OUTFILE --examples. EXAMPLES --sample SAMPLE. [--model {hybrid,pacbio,wes,wgs}] [-h]. dv_call_variants.py: error: the following arguments are required: --outfile, --examples, --sample.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/627
https://github.com/google/deepvariant/issues/627:1025,modifiability,pac,pacbio,1025,"Sorry for my wrong posting of error messages. I have tried different versions of deepvariant in conda, and messed up the conda environment. . Actually I posted the deepvarint (1.3.0) error messages. : <(. Here is my attempts on the deepvariant 1.4.0: . `. $ conda create -n deepvar python=3.7.5. $ conda install -c bioconda deepvariant=1.4.0. $ conda activate deepvar. $ dv_call_variants.py -h . `. And I got the similar error messages:. Baseline DeepVariant arguments. File ""/tmp/Bazel.runfiles_j_t9fnwc/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 374. raise ValueError(f'Shape mismatch in {example_info_json} and '. ^. SyntaxError: invalid syntax. Wrapper arguments. usage: dv_call_variants.py [--cores CORES] --outfile OUTFILE --examples. EXAMPLES --sample SAMPLE. [--model {hybrid,pacbio,wes,wgs}] [-h]. DeepVariant call_variants wrapper. optional arguments:. --cores CORES. --outfile OUTFILE. --examples EXAMPLES Example directory from make_examples. --sample SAMPLE Sample name. --model {hybrid,pacbio,wes,wgs}. DeepVariant trained model to use, defaults to wgs. -h, --help. usage: dv_call_variants.py [--cores CORES] --outfile OUTFILE --examples. EXAMPLES --sample SAMPLE. [--model {hybrid,pacbio,wes,wgs}] [-h]. dv_call_variants.py: error: the following arguments are required: --outfile, --examples, --sample.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/627
https://github.com/google/deepvariant/issues/627:1221,modifiability,pac,pacbio,1221,"Sorry for my wrong posting of error messages. I have tried different versions of deepvariant in conda, and messed up the conda environment. . Actually I posted the deepvarint (1.3.0) error messages. : <(. Here is my attempts on the deepvariant 1.4.0: . `. $ conda create -n deepvar python=3.7.5. $ conda install -c bioconda deepvariant=1.4.0. $ conda activate deepvar. $ dv_call_variants.py -h . `. And I got the similar error messages:. Baseline DeepVariant arguments. File ""/tmp/Bazel.runfiles_j_t9fnwc/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 374. raise ValueError(f'Shape mismatch in {example_info_json} and '. ^. SyntaxError: invalid syntax. Wrapper arguments. usage: dv_call_variants.py [--cores CORES] --outfile OUTFILE --examples. EXAMPLES --sample SAMPLE. [--model {hybrid,pacbio,wes,wgs}] [-h]. DeepVariant call_variants wrapper. optional arguments:. --cores CORES. --outfile OUTFILE. --examples EXAMPLES Example directory from make_examples. --sample SAMPLE Sample name. --model {hybrid,pacbio,wes,wgs}. DeepVariant trained model to use, defaults to wgs. -h, --help. usage: dv_call_variants.py [--cores CORES] --outfile OUTFILE --examples. EXAMPLES --sample SAMPLE. [--model {hybrid,pacbio,wes,wgs}] [-h]. dv_call_variants.py: error: the following arguments are required: --outfile, --examples, --sample.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/627
https://github.com/google/deepvariant/issues/627:30,performance,error,error,30,"Sorry for my wrong posting of error messages. I have tried different versions of deepvariant in conda, and messed up the conda environment. . Actually I posted the deepvarint (1.3.0) error messages. : <(. Here is my attempts on the deepvariant 1.4.0: . `. $ conda create -n deepvar python=3.7.5. $ conda install -c bioconda deepvariant=1.4.0. $ conda activate deepvar. $ dv_call_variants.py -h . `. And I got the similar error messages:. Baseline DeepVariant arguments. File ""/tmp/Bazel.runfiles_j_t9fnwc/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 374. raise ValueError(f'Shape mismatch in {example_info_json} and '. ^. SyntaxError: invalid syntax. Wrapper arguments. usage: dv_call_variants.py [--cores CORES] --outfile OUTFILE --examples. EXAMPLES --sample SAMPLE. [--model {hybrid,pacbio,wes,wgs}] [-h]. DeepVariant call_variants wrapper. optional arguments:. --cores CORES. --outfile OUTFILE. --examples EXAMPLES Example directory from make_examples. --sample SAMPLE Sample name. --model {hybrid,pacbio,wes,wgs}. DeepVariant trained model to use, defaults to wgs. -h, --help. usage: dv_call_variants.py [--cores CORES] --outfile OUTFILE --examples. EXAMPLES --sample SAMPLE. [--model {hybrid,pacbio,wes,wgs}] [-h]. dv_call_variants.py: error: the following arguments are required: --outfile, --examples, --sample.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/627
https://github.com/google/deepvariant/issues/627:183,performance,error,error,183,"Sorry for my wrong posting of error messages. I have tried different versions of deepvariant in conda, and messed up the conda environment. . Actually I posted the deepvarint (1.3.0) error messages. : <(. Here is my attempts on the deepvariant 1.4.0: . `. $ conda create -n deepvar python=3.7.5. $ conda install -c bioconda deepvariant=1.4.0. $ conda activate deepvar. $ dv_call_variants.py -h . `. And I got the similar error messages:. Baseline DeepVariant arguments. File ""/tmp/Bazel.runfiles_j_t9fnwc/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 374. raise ValueError(f'Shape mismatch in {example_info_json} and '. ^. SyntaxError: invalid syntax. Wrapper arguments. usage: dv_call_variants.py [--cores CORES] --outfile OUTFILE --examples. EXAMPLES --sample SAMPLE. [--model {hybrid,pacbio,wes,wgs}] [-h]. DeepVariant call_variants wrapper. optional arguments:. --cores CORES. --outfile OUTFILE. --examples EXAMPLES Example directory from make_examples. --sample SAMPLE Sample name. --model {hybrid,pacbio,wes,wgs}. DeepVariant trained model to use, defaults to wgs. -h, --help. usage: dv_call_variants.py [--cores CORES] --outfile OUTFILE --examples. EXAMPLES --sample SAMPLE. [--model {hybrid,pacbio,wes,wgs}] [-h]. dv_call_variants.py: error: the following arguments are required: --outfile, --examples, --sample.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/627
https://github.com/google/deepvariant/issues/627:421,performance,error,error,421,"Sorry for my wrong posting of error messages. I have tried different versions of deepvariant in conda, and messed up the conda environment. . Actually I posted the deepvarint (1.3.0) error messages. : <(. Here is my attempts on the deepvariant 1.4.0: . `. $ conda create -n deepvar python=3.7.5. $ conda install -c bioconda deepvariant=1.4.0. $ conda activate deepvar. $ dv_call_variants.py -h . `. And I got the similar error messages:. Baseline DeepVariant arguments. File ""/tmp/Bazel.runfiles_j_t9fnwc/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 374. raise ValueError(f'Shape mismatch in {example_info_json} and '. ^. SyntaxError: invalid syntax. Wrapper arguments. usage: dv_call_variants.py [--cores CORES] --outfile OUTFILE --examples. EXAMPLES --sample SAMPLE. [--model {hybrid,pacbio,wes,wgs}] [-h]. DeepVariant call_variants wrapper. optional arguments:. --cores CORES. --outfile OUTFILE. --examples EXAMPLES Example directory from make_examples. --sample SAMPLE Sample name. --model {hybrid,pacbio,wes,wgs}. DeepVariant trained model to use, defaults to wgs. -h, --help. usage: dv_call_variants.py [--cores CORES] --outfile OUTFILE --examples. EXAMPLES --sample SAMPLE. [--model {hybrid,pacbio,wes,wgs}] [-h]. dv_call_variants.py: error: the following arguments are required: --outfile, --examples, --sample.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/627
https://github.com/google/deepvariant/issues/627:1265,performance,error,error,1265,"Sorry for my wrong posting of error messages. I have tried different versions of deepvariant in conda, and messed up the conda environment. . Actually I posted the deepvarint (1.3.0) error messages. : <(. Here is my attempts on the deepvariant 1.4.0: . `. $ conda create -n deepvar python=3.7.5. $ conda install -c bioconda deepvariant=1.4.0. $ conda activate deepvar. $ dv_call_variants.py -h . `. And I got the similar error messages:. Baseline DeepVariant arguments. File ""/tmp/Bazel.runfiles_j_t9fnwc/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 374. raise ValueError(f'Shape mismatch in {example_info_json} and '. ^. SyntaxError: invalid syntax. Wrapper arguments. usage: dv_call_variants.py [--cores CORES] --outfile OUTFILE --examples. EXAMPLES --sample SAMPLE. [--model {hybrid,pacbio,wes,wgs}] [-h]. DeepVariant call_variants wrapper. optional arguments:. --cores CORES. --outfile OUTFILE. --examples EXAMPLES Example directory from make_examples. --sample SAMPLE Sample name. --model {hybrid,pacbio,wes,wgs}. DeepVariant trained model to use, defaults to wgs. -h, --help. usage: dv_call_variants.py [--cores CORES] --outfile OUTFILE --examples. EXAMPLES --sample SAMPLE. [--model {hybrid,pacbio,wes,wgs}] [-h]. dv_call_variants.py: error: the following arguments are required: --outfile, --examples, --sample.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/627
https://github.com/google/deepvariant/issues/627:30,safety,error,error,30,"Sorry for my wrong posting of error messages. I have tried different versions of deepvariant in conda, and messed up the conda environment. . Actually I posted the deepvarint (1.3.0) error messages. : <(. Here is my attempts on the deepvariant 1.4.0: . `. $ conda create -n deepvar python=3.7.5. $ conda install -c bioconda deepvariant=1.4.0. $ conda activate deepvar. $ dv_call_variants.py -h . `. And I got the similar error messages:. Baseline DeepVariant arguments. File ""/tmp/Bazel.runfiles_j_t9fnwc/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 374. raise ValueError(f'Shape mismatch in {example_info_json} and '. ^. SyntaxError: invalid syntax. Wrapper arguments. usage: dv_call_variants.py [--cores CORES] --outfile OUTFILE --examples. EXAMPLES --sample SAMPLE. [--model {hybrid,pacbio,wes,wgs}] [-h]. DeepVariant call_variants wrapper. optional arguments:. --cores CORES. --outfile OUTFILE. --examples EXAMPLES Example directory from make_examples. --sample SAMPLE Sample name. --model {hybrid,pacbio,wes,wgs}. DeepVariant trained model to use, defaults to wgs. -h, --help. usage: dv_call_variants.py [--cores CORES] --outfile OUTFILE --examples. EXAMPLES --sample SAMPLE. [--model {hybrid,pacbio,wes,wgs}] [-h]. dv_call_variants.py: error: the following arguments are required: --outfile, --examples, --sample.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/627
https://github.com/google/deepvariant/issues/627:183,safety,error,error,183,"Sorry for my wrong posting of error messages. I have tried different versions of deepvariant in conda, and messed up the conda environment. . Actually I posted the deepvarint (1.3.0) error messages. : <(. Here is my attempts on the deepvariant 1.4.0: . `. $ conda create -n deepvar python=3.7.5. $ conda install -c bioconda deepvariant=1.4.0. $ conda activate deepvar. $ dv_call_variants.py -h . `. And I got the similar error messages:. Baseline DeepVariant arguments. File ""/tmp/Bazel.runfiles_j_t9fnwc/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 374. raise ValueError(f'Shape mismatch in {example_info_json} and '. ^. SyntaxError: invalid syntax. Wrapper arguments. usage: dv_call_variants.py [--cores CORES] --outfile OUTFILE --examples. EXAMPLES --sample SAMPLE. [--model {hybrid,pacbio,wes,wgs}] [-h]. DeepVariant call_variants wrapper. optional arguments:. --cores CORES. --outfile OUTFILE. --examples EXAMPLES Example directory from make_examples. --sample SAMPLE Sample name. --model {hybrid,pacbio,wes,wgs}. DeepVariant trained model to use, defaults to wgs. -h, --help. usage: dv_call_variants.py [--cores CORES] --outfile OUTFILE --examples. EXAMPLES --sample SAMPLE. [--model {hybrid,pacbio,wes,wgs}] [-h]. dv_call_variants.py: error: the following arguments are required: --outfile, --examples, --sample.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/627
https://github.com/google/deepvariant/issues/627:421,safety,error,error,421,"Sorry for my wrong posting of error messages. I have tried different versions of deepvariant in conda, and messed up the conda environment. . Actually I posted the deepvarint (1.3.0) error messages. : <(. Here is my attempts on the deepvariant 1.4.0: . `. $ conda create -n deepvar python=3.7.5. $ conda install -c bioconda deepvariant=1.4.0. $ conda activate deepvar. $ dv_call_variants.py -h . `. And I got the similar error messages:. Baseline DeepVariant arguments. File ""/tmp/Bazel.runfiles_j_t9fnwc/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 374. raise ValueError(f'Shape mismatch in {example_info_json} and '. ^. SyntaxError: invalid syntax. Wrapper arguments. usage: dv_call_variants.py [--cores CORES] --outfile OUTFILE --examples. EXAMPLES --sample SAMPLE. [--model {hybrid,pacbio,wes,wgs}] [-h]. DeepVariant call_variants wrapper. optional arguments:. --cores CORES. --outfile OUTFILE. --examples EXAMPLES Example directory from make_examples. --sample SAMPLE Sample name. --model {hybrid,pacbio,wes,wgs}. DeepVariant trained model to use, defaults to wgs. -h, --help. usage: dv_call_variants.py [--cores CORES] --outfile OUTFILE --examples. EXAMPLES --sample SAMPLE. [--model {hybrid,pacbio,wes,wgs}] [-h]. dv_call_variants.py: error: the following arguments are required: --outfile, --examples, --sample.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/627
https://github.com/google/deepvariant/issues/627:1265,safety,error,error,1265,"Sorry for my wrong posting of error messages. I have tried different versions of deepvariant in conda, and messed up the conda environment. . Actually I posted the deepvarint (1.3.0) error messages. : <(. Here is my attempts on the deepvariant 1.4.0: . `. $ conda create -n deepvar python=3.7.5. $ conda install -c bioconda deepvariant=1.4.0. $ conda activate deepvar. $ dv_call_variants.py -h . `. And I got the similar error messages:. Baseline DeepVariant arguments. File ""/tmp/Bazel.runfiles_j_t9fnwc/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 374. raise ValueError(f'Shape mismatch in {example_info_json} and '. ^. SyntaxError: invalid syntax. Wrapper arguments. usage: dv_call_variants.py [--cores CORES] --outfile OUTFILE --examples. EXAMPLES --sample SAMPLE. [--model {hybrid,pacbio,wes,wgs}] [-h]. DeepVariant call_variants wrapper. optional arguments:. --cores CORES. --outfile OUTFILE. --examples EXAMPLES Example directory from make_examples. --sample SAMPLE Sample name. --model {hybrid,pacbio,wes,wgs}. DeepVariant trained model to use, defaults to wgs. -h, --help. usage: dv_call_variants.py [--cores CORES] --outfile OUTFILE --examples. EXAMPLES --sample SAMPLE. [--model {hybrid,pacbio,wes,wgs}] [-h]. dv_call_variants.py: error: the following arguments are required: --outfile, --examples, --sample.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/627
https://github.com/google/deepvariant/issues/627:795,security,model,model,795,"Sorry for my wrong posting of error messages. I have tried different versions of deepvariant in conda, and messed up the conda environment. . Actually I posted the deepvarint (1.3.0) error messages. : <(. Here is my attempts on the deepvariant 1.4.0: . `. $ conda create -n deepvar python=3.7.5. $ conda install -c bioconda deepvariant=1.4.0. $ conda activate deepvar. $ dv_call_variants.py -h . `. And I got the similar error messages:. Baseline DeepVariant arguments. File ""/tmp/Bazel.runfiles_j_t9fnwc/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 374. raise ValueError(f'Shape mismatch in {example_info_json} and '. ^. SyntaxError: invalid syntax. Wrapper arguments. usage: dv_call_variants.py [--cores CORES] --outfile OUTFILE --examples. EXAMPLES --sample SAMPLE. [--model {hybrid,pacbio,wes,wgs}] [-h]. DeepVariant call_variants wrapper. optional arguments:. --cores CORES. --outfile OUTFILE. --examples EXAMPLES Example directory from make_examples. --sample SAMPLE Sample name. --model {hybrid,pacbio,wes,wgs}. DeepVariant trained model to use, defaults to wgs. -h, --help. usage: dv_call_variants.py [--cores CORES] --outfile OUTFILE --examples. EXAMPLES --sample SAMPLE. [--model {hybrid,pacbio,wes,wgs}] [-h]. dv_call_variants.py: error: the following arguments are required: --outfile, --examples, --sample.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/627
https://github.com/google/deepvariant/issues/627:1011,security,model,model,1011,"Sorry for my wrong posting of error messages. I have tried different versions of deepvariant in conda, and messed up the conda environment. . Actually I posted the deepvarint (1.3.0) error messages. : <(. Here is my attempts on the deepvariant 1.4.0: . `. $ conda create -n deepvar python=3.7.5. $ conda install -c bioconda deepvariant=1.4.0. $ conda activate deepvar. $ dv_call_variants.py -h . `. And I got the similar error messages:. Baseline DeepVariant arguments. File ""/tmp/Bazel.runfiles_j_t9fnwc/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 374. raise ValueError(f'Shape mismatch in {example_info_json} and '. ^. SyntaxError: invalid syntax. Wrapper arguments. usage: dv_call_variants.py [--cores CORES] --outfile OUTFILE --examples. EXAMPLES --sample SAMPLE. [--model {hybrid,pacbio,wes,wgs}] [-h]. DeepVariant call_variants wrapper. optional arguments:. --cores CORES. --outfile OUTFILE. --examples EXAMPLES Example directory from make_examples. --sample SAMPLE Sample name. --model {hybrid,pacbio,wes,wgs}. DeepVariant trained model to use, defaults to wgs. -h, --help. usage: dv_call_variants.py [--cores CORES] --outfile OUTFILE --examples. EXAMPLES --sample SAMPLE. [--model {hybrid,pacbio,wes,wgs}] [-h]. dv_call_variants.py: error: the following arguments are required: --outfile, --examples, --sample.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/627
https://github.com/google/deepvariant/issues/627:1062,security,model,model,1062,"Sorry for my wrong posting of error messages. I have tried different versions of deepvariant in conda, and messed up the conda environment. . Actually I posted the deepvarint (1.3.0) error messages. : <(. Here is my attempts on the deepvariant 1.4.0: . `. $ conda create -n deepvar python=3.7.5. $ conda install -c bioconda deepvariant=1.4.0. $ conda activate deepvar. $ dv_call_variants.py -h . `. And I got the similar error messages:. Baseline DeepVariant arguments. File ""/tmp/Bazel.runfiles_j_t9fnwc/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 374. raise ValueError(f'Shape mismatch in {example_info_json} and '. ^. SyntaxError: invalid syntax. Wrapper arguments. usage: dv_call_variants.py [--cores CORES] --outfile OUTFILE --examples. EXAMPLES --sample SAMPLE. [--model {hybrid,pacbio,wes,wgs}] [-h]. DeepVariant call_variants wrapper. optional arguments:. --cores CORES. --outfile OUTFILE. --examples EXAMPLES Example directory from make_examples. --sample SAMPLE Sample name. --model {hybrid,pacbio,wes,wgs}. DeepVariant trained model to use, defaults to wgs. -h, --help. usage: dv_call_variants.py [--cores CORES] --outfile OUTFILE --examples. EXAMPLES --sample SAMPLE. [--model {hybrid,pacbio,wes,wgs}] [-h]. dv_call_variants.py: error: the following arguments are required: --outfile, --examples, --sample.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/627
https://github.com/google/deepvariant/issues/627:1207,security,model,model,1207,"Sorry for my wrong posting of error messages. I have tried different versions of deepvariant in conda, and messed up the conda environment. . Actually I posted the deepvarint (1.3.0) error messages. : <(. Here is my attempts on the deepvariant 1.4.0: . `. $ conda create -n deepvar python=3.7.5. $ conda install -c bioconda deepvariant=1.4.0. $ conda activate deepvar. $ dv_call_variants.py -h . `. And I got the similar error messages:. Baseline DeepVariant arguments. File ""/tmp/Bazel.runfiles_j_t9fnwc/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 374. raise ValueError(f'Shape mismatch in {example_info_json} and '. ^. SyntaxError: invalid syntax. Wrapper arguments. usage: dv_call_variants.py [--cores CORES] --outfile OUTFILE --examples. EXAMPLES --sample SAMPLE. [--model {hybrid,pacbio,wes,wgs}] [-h]. DeepVariant call_variants wrapper. optional arguments:. --cores CORES. --outfile OUTFILE. --examples EXAMPLES Example directory from make_examples. --sample SAMPLE Sample name. --model {hybrid,pacbio,wes,wgs}. DeepVariant trained model to use, defaults to wgs. -h, --help. usage: dv_call_variants.py [--cores CORES] --outfile OUTFILE --examples. EXAMPLES --sample SAMPLE. [--model {hybrid,pacbio,wes,wgs}] [-h]. dv_call_variants.py: error: the following arguments are required: --outfile, --examples, --sample.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/627
https://github.com/google/deepvariant/issues/627:30,usability,error,error,30,"Sorry for my wrong posting of error messages. I have tried different versions of deepvariant in conda, and messed up the conda environment. . Actually I posted the deepvarint (1.3.0) error messages. : <(. Here is my attempts on the deepvariant 1.4.0: . `. $ conda create -n deepvar python=3.7.5. $ conda install -c bioconda deepvariant=1.4.0. $ conda activate deepvar. $ dv_call_variants.py -h . `. And I got the similar error messages:. Baseline DeepVariant arguments. File ""/tmp/Bazel.runfiles_j_t9fnwc/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 374. raise ValueError(f'Shape mismatch in {example_info_json} and '. ^. SyntaxError: invalid syntax. Wrapper arguments. usage: dv_call_variants.py [--cores CORES] --outfile OUTFILE --examples. EXAMPLES --sample SAMPLE. [--model {hybrid,pacbio,wes,wgs}] [-h]. DeepVariant call_variants wrapper. optional arguments:. --cores CORES. --outfile OUTFILE. --examples EXAMPLES Example directory from make_examples. --sample SAMPLE Sample name. --model {hybrid,pacbio,wes,wgs}. DeepVariant trained model to use, defaults to wgs. -h, --help. usage: dv_call_variants.py [--cores CORES] --outfile OUTFILE --examples. EXAMPLES --sample SAMPLE. [--model {hybrid,pacbio,wes,wgs}] [-h]. dv_call_variants.py: error: the following arguments are required: --outfile, --examples, --sample.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/627
https://github.com/google/deepvariant/issues/627:183,usability,error,error,183,"Sorry for my wrong posting of error messages. I have tried different versions of deepvariant in conda, and messed up the conda environment. . Actually I posted the deepvarint (1.3.0) error messages. : <(. Here is my attempts on the deepvariant 1.4.0: . `. $ conda create -n deepvar python=3.7.5. $ conda install -c bioconda deepvariant=1.4.0. $ conda activate deepvar. $ dv_call_variants.py -h . `. And I got the similar error messages:. Baseline DeepVariant arguments. File ""/tmp/Bazel.runfiles_j_t9fnwc/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 374. raise ValueError(f'Shape mismatch in {example_info_json} and '. ^. SyntaxError: invalid syntax. Wrapper arguments. usage: dv_call_variants.py [--cores CORES] --outfile OUTFILE --examples. EXAMPLES --sample SAMPLE. [--model {hybrid,pacbio,wes,wgs}] [-h]. DeepVariant call_variants wrapper. optional arguments:. --cores CORES. --outfile OUTFILE. --examples EXAMPLES Example directory from make_examples. --sample SAMPLE Sample name. --model {hybrid,pacbio,wes,wgs}. DeepVariant trained model to use, defaults to wgs. -h, --help. usage: dv_call_variants.py [--cores CORES] --outfile OUTFILE --examples. EXAMPLES --sample SAMPLE. [--model {hybrid,pacbio,wes,wgs}] [-h]. dv_call_variants.py: error: the following arguments are required: --outfile, --examples, --sample.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/627
https://github.com/google/deepvariant/issues/627:421,usability,error,error,421,"Sorry for my wrong posting of error messages. I have tried different versions of deepvariant in conda, and messed up the conda environment. . Actually I posted the deepvarint (1.3.0) error messages. : <(. Here is my attempts on the deepvariant 1.4.0: . `. $ conda create -n deepvar python=3.7.5. $ conda install -c bioconda deepvariant=1.4.0. $ conda activate deepvar. $ dv_call_variants.py -h . `. And I got the similar error messages:. Baseline DeepVariant arguments. File ""/tmp/Bazel.runfiles_j_t9fnwc/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 374. raise ValueError(f'Shape mismatch in {example_info_json} and '. ^. SyntaxError: invalid syntax. Wrapper arguments. usage: dv_call_variants.py [--cores CORES] --outfile OUTFILE --examples. EXAMPLES --sample SAMPLE. [--model {hybrid,pacbio,wes,wgs}] [-h]. DeepVariant call_variants wrapper. optional arguments:. --cores CORES. --outfile OUTFILE. --examples EXAMPLES Example directory from make_examples. --sample SAMPLE Sample name. --model {hybrid,pacbio,wes,wgs}. DeepVariant trained model to use, defaults to wgs. -h, --help. usage: dv_call_variants.py [--cores CORES] --outfile OUTFILE --examples. EXAMPLES --sample SAMPLE. [--model {hybrid,pacbio,wes,wgs}] [-h]. dv_call_variants.py: error: the following arguments are required: --outfile, --examples, --sample.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/627
https://github.com/google/deepvariant/issues/627:1099,usability,help,help,1099,"Sorry for my wrong posting of error messages. I have tried different versions of deepvariant in conda, and messed up the conda environment. . Actually I posted the deepvarint (1.3.0) error messages. : <(. Here is my attempts on the deepvariant 1.4.0: . `. $ conda create -n deepvar python=3.7.5. $ conda install -c bioconda deepvariant=1.4.0. $ conda activate deepvar. $ dv_call_variants.py -h . `. And I got the similar error messages:. Baseline DeepVariant arguments. File ""/tmp/Bazel.runfiles_j_t9fnwc/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 374. raise ValueError(f'Shape mismatch in {example_info_json} and '. ^. SyntaxError: invalid syntax. Wrapper arguments. usage: dv_call_variants.py [--cores CORES] --outfile OUTFILE --examples. EXAMPLES --sample SAMPLE. [--model {hybrid,pacbio,wes,wgs}] [-h]. DeepVariant call_variants wrapper. optional arguments:. --cores CORES. --outfile OUTFILE. --examples EXAMPLES Example directory from make_examples. --sample SAMPLE Sample name. --model {hybrid,pacbio,wes,wgs}. DeepVariant trained model to use, defaults to wgs. -h, --help. usage: dv_call_variants.py [--cores CORES] --outfile OUTFILE --examples. EXAMPLES --sample SAMPLE. [--model {hybrid,pacbio,wes,wgs}] [-h]. dv_call_variants.py: error: the following arguments are required: --outfile, --examples, --sample.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/627
https://github.com/google/deepvariant/issues/627:1265,usability,error,error,1265,"Sorry for my wrong posting of error messages. I have tried different versions of deepvariant in conda, and messed up the conda environment. . Actually I posted the deepvarint (1.3.0) error messages. : <(. Here is my attempts on the deepvariant 1.4.0: . `. $ conda create -n deepvar python=3.7.5. $ conda install -c bioconda deepvariant=1.4.0. $ conda activate deepvar. $ dv_call_variants.py -h . `. And I got the similar error messages:. Baseline DeepVariant arguments. File ""/tmp/Bazel.runfiles_j_t9fnwc/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 374. raise ValueError(f'Shape mismatch in {example_info_json} and '. ^. SyntaxError: invalid syntax. Wrapper arguments. usage: dv_call_variants.py [--cores CORES] --outfile OUTFILE --examples. EXAMPLES --sample SAMPLE. [--model {hybrid,pacbio,wes,wgs}] [-h]. DeepVariant call_variants wrapper. optional arguments:. --cores CORES. --outfile OUTFILE. --examples EXAMPLES Example directory from make_examples. --sample SAMPLE Sample name. --model {hybrid,pacbio,wes,wgs}. DeepVariant trained model to use, defaults to wgs. -h, --help. usage: dv_call_variants.py [--cores CORES] --outfile OUTFILE --examples. EXAMPLES --sample SAMPLE. [--model {hybrid,pacbio,wes,wgs}] [-h]. dv_call_variants.py: error: the following arguments are required: --outfile, --examples, --sample.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/627
https://github.com/google/deepvariant/issues/627:380,availability,error,error,380,"Hi @richard-nm , thanks for the update. Given that our team didn't build or maintain the bioconda version, I'm not familiar with many things here. One question and one comment for you:. 1. Question: Where can I find documentation of this tool? For example, ""dv_call_variants.py"" isn't part of our GitHub repo, so I'm not familiar with that at all. 2. Comment: Purely based on the error message you posted:. ```. Baseline DeepVariant arguments. File ""/tmp/Bazel.runfiles_j_t9fnwc/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 374. raise ValueError(f'Shape mismatch in {example_info_json} and '. ^. SyntaxError: invalid syntax. ```. The corresponding line would be this one:. https://github.com/google/deepvariant/blob/r1.4/deepvariant/call_variants.py#L374. I'm not exactly why that would cause a SyntaxError though. It's a bit hard for me to figure out given that it's wrapped in something that I'm not familiar with. I did try to follow your steps above, but didn't succeed in installing:. ```. $ conda create -n deepvar python=3.7.5. $ conda install -c bioconda deepvariant=1.4.0. ```. gave me:. ```. UnsatisfiableError: The following specifications were found. to be incompatible with the existing python installation in your environment:. Specifications:. - deepvariant=1.4.0 -> python[version='>=3.6,<3.7.0a0']. Your python: python=3.7.5. ```. @richard-nm One more question for you -- is it possible to use Docker or Singularity? Which our team can support more easily as well. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/627
https://github.com/google/deepvariant/issues/627:32,deployability,updat,update,32,"Hi @richard-nm , thanks for the update. Given that our team didn't build or maintain the bioconda version, I'm not familiar with many things here. One question and one comment for you:. 1. Question: Where can I find documentation of this tool? For example, ""dv_call_variants.py"" isn't part of our GitHub repo, so I'm not familiar with that at all. 2. Comment: Purely based on the error message you posted:. ```. Baseline DeepVariant arguments. File ""/tmp/Bazel.runfiles_j_t9fnwc/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 374. raise ValueError(f'Shape mismatch in {example_info_json} and '. ^. SyntaxError: invalid syntax. ```. The corresponding line would be this one:. https://github.com/google/deepvariant/blob/r1.4/deepvariant/call_variants.py#L374. I'm not exactly why that would cause a SyntaxError though. It's a bit hard for me to figure out given that it's wrapped in something that I'm not familiar with. I did try to follow your steps above, but didn't succeed in installing:. ```. $ conda create -n deepvar python=3.7.5. $ conda install -c bioconda deepvariant=1.4.0. ```. gave me:. ```. UnsatisfiableError: The following specifications were found. to be incompatible with the existing python installation in your environment:. Specifications:. - deepvariant=1.4.0 -> python[version='>=3.6,<3.7.0a0']. Your python: python=3.7.5. ```. @richard-nm One more question for you -- is it possible to use Docker or Singularity? Which our team can support more easily as well. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/627
https://github.com/google/deepvariant/issues/627:67,deployability,build,build,67,"Hi @richard-nm , thanks for the update. Given that our team didn't build or maintain the bioconda version, I'm not familiar with many things here. One question and one comment for you:. 1. Question: Where can I find documentation of this tool? For example, ""dv_call_variants.py"" isn't part of our GitHub repo, so I'm not familiar with that at all. 2. Comment: Purely based on the error message you posted:. ```. Baseline DeepVariant arguments. File ""/tmp/Bazel.runfiles_j_t9fnwc/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 374. raise ValueError(f'Shape mismatch in {example_info_json} and '. ^. SyntaxError: invalid syntax. ```. The corresponding line would be this one:. https://github.com/google/deepvariant/blob/r1.4/deepvariant/call_variants.py#L374. I'm not exactly why that would cause a SyntaxError though. It's a bit hard for me to figure out given that it's wrapped in something that I'm not familiar with. I did try to follow your steps above, but didn't succeed in installing:. ```. $ conda create -n deepvar python=3.7.5. $ conda install -c bioconda deepvariant=1.4.0. ```. gave me:. ```. UnsatisfiableError: The following specifications were found. to be incompatible with the existing python installation in your environment:. Specifications:. - deepvariant=1.4.0 -> python[version='>=3.6,<3.7.0a0']. Your python: python=3.7.5. ```. @richard-nm One more question for you -- is it possible to use Docker or Singularity? Which our team can support more easily as well. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/627
https://github.com/google/deepvariant/issues/627:98,deployability,version,version,98,"Hi @richard-nm , thanks for the update. Given that our team didn't build or maintain the bioconda version, I'm not familiar with many things here. One question and one comment for you:. 1. Question: Where can I find documentation of this tool? For example, ""dv_call_variants.py"" isn't part of our GitHub repo, so I'm not familiar with that at all. 2. Comment: Purely based on the error message you posted:. ```. Baseline DeepVariant arguments. File ""/tmp/Bazel.runfiles_j_t9fnwc/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 374. raise ValueError(f'Shape mismatch in {example_info_json} and '. ^. SyntaxError: invalid syntax. ```. The corresponding line would be this one:. https://github.com/google/deepvariant/blob/r1.4/deepvariant/call_variants.py#L374. I'm not exactly why that would cause a SyntaxError though. It's a bit hard for me to figure out given that it's wrapped in something that I'm not familiar with. I did try to follow your steps above, but didn't succeed in installing:. ```. $ conda create -n deepvar python=3.7.5. $ conda install -c bioconda deepvariant=1.4.0. ```. gave me:. ```. UnsatisfiableError: The following specifications were found. to be incompatible with the existing python installation in your environment:. Specifications:. - deepvariant=1.4.0 -> python[version='>=3.6,<3.7.0a0']. Your python: python=3.7.5. ```. @richard-nm One more question for you -- is it possible to use Docker or Singularity? Which our team can support more easily as well. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/627
https://github.com/google/deepvariant/issues/627:1000,deployability,instal,installing,1000,"Hi @richard-nm , thanks for the update. Given that our team didn't build or maintain the bioconda version, I'm not familiar with many things here. One question and one comment for you:. 1. Question: Where can I find documentation of this tool? For example, ""dv_call_variants.py"" isn't part of our GitHub repo, so I'm not familiar with that at all. 2. Comment: Purely based on the error message you posted:. ```. Baseline DeepVariant arguments. File ""/tmp/Bazel.runfiles_j_t9fnwc/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 374. raise ValueError(f'Shape mismatch in {example_info_json} and '. ^. SyntaxError: invalid syntax. ```. The corresponding line would be this one:. https://github.com/google/deepvariant/blob/r1.4/deepvariant/call_variants.py#L374. I'm not exactly why that would cause a SyntaxError though. It's a bit hard for me to figure out given that it's wrapped in something that I'm not familiar with. I did try to follow your steps above, but didn't succeed in installing:. ```. $ conda create -n deepvar python=3.7.5. $ conda install -c bioconda deepvariant=1.4.0. ```. gave me:. ```. UnsatisfiableError: The following specifications were found. to be incompatible with the existing python installation in your environment:. Specifications:. - deepvariant=1.4.0 -> python[version='>=3.6,<3.7.0a0']. Your python: python=3.7.5. ```. @richard-nm One more question for you -- is it possible to use Docker or Singularity? Which our team can support more easily as well. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/627
https://github.com/google/deepvariant/issues/627:1066,deployability,instal,install,1066,"Hi @richard-nm , thanks for the update. Given that our team didn't build or maintain the bioconda version, I'm not familiar with many things here. One question and one comment for you:. 1. Question: Where can I find documentation of this tool? For example, ""dv_call_variants.py"" isn't part of our GitHub repo, so I'm not familiar with that at all. 2. Comment: Purely based on the error message you posted:. ```. Baseline DeepVariant arguments. File ""/tmp/Bazel.runfiles_j_t9fnwc/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 374. raise ValueError(f'Shape mismatch in {example_info_json} and '. ^. SyntaxError: invalid syntax. ```. The corresponding line would be this one:. https://github.com/google/deepvariant/blob/r1.4/deepvariant/call_variants.py#L374. I'm not exactly why that would cause a SyntaxError though. It's a bit hard for me to figure out given that it's wrapped in something that I'm not familiar with. I did try to follow your steps above, but didn't succeed in installing:. ```. $ conda create -n deepvar python=3.7.5. $ conda install -c bioconda deepvariant=1.4.0. ```. gave me:. ```. UnsatisfiableError: The following specifications were found. to be incompatible with the existing python installation in your environment:. Specifications:. - deepvariant=1.4.0 -> python[version='>=3.6,<3.7.0a0']. Your python: python=3.7.5. ```. @richard-nm One more question for you -- is it possible to use Docker or Singularity? Which our team can support more easily as well. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/627
https://github.com/google/deepvariant/issues/627:1230,deployability,instal,installation,1230,"Hi @richard-nm , thanks for the update. Given that our team didn't build or maintain the bioconda version, I'm not familiar with many things here. One question and one comment for you:. 1. Question: Where can I find documentation of this tool? For example, ""dv_call_variants.py"" isn't part of our GitHub repo, so I'm not familiar with that at all. 2. Comment: Purely based on the error message you posted:. ```. Baseline DeepVariant arguments. File ""/tmp/Bazel.runfiles_j_t9fnwc/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 374. raise ValueError(f'Shape mismatch in {example_info_json} and '. ^. SyntaxError: invalid syntax. ```. The corresponding line would be this one:. https://github.com/google/deepvariant/blob/r1.4/deepvariant/call_variants.py#L374. I'm not exactly why that would cause a SyntaxError though. It's a bit hard for me to figure out given that it's wrapped in something that I'm not familiar with. I did try to follow your steps above, but didn't succeed in installing:. ```. $ conda create -n deepvar python=3.7.5. $ conda install -c bioconda deepvariant=1.4.0. ```. gave me:. ```. UnsatisfiableError: The following specifications were found. to be incompatible with the existing python installation in your environment:. Specifications:. - deepvariant=1.4.0 -> python[version='>=3.6,<3.7.0a0']. Your python: python=3.7.5. ```. @richard-nm One more question for you -- is it possible to use Docker or Singularity? Which our team can support more easily as well. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/627
https://github.com/google/deepvariant/issues/627:1312,deployability,version,version,1312,"Hi @richard-nm , thanks for the update. Given that our team didn't build or maintain the bioconda version, I'm not familiar with many things here. One question and one comment for you:. 1. Question: Where can I find documentation of this tool? For example, ""dv_call_variants.py"" isn't part of our GitHub repo, so I'm not familiar with that at all. 2. Comment: Purely based on the error message you posted:. ```. Baseline DeepVariant arguments. File ""/tmp/Bazel.runfiles_j_t9fnwc/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 374. raise ValueError(f'Shape mismatch in {example_info_json} and '. ^. SyntaxError: invalid syntax. ```. The corresponding line would be this one:. https://github.com/google/deepvariant/blob/r1.4/deepvariant/call_variants.py#L374. I'm not exactly why that would cause a SyntaxError though. It's a bit hard for me to figure out given that it's wrapped in something that I'm not familiar with. I did try to follow your steps above, but didn't succeed in installing:. ```. $ conda create -n deepvar python=3.7.5. $ conda install -c bioconda deepvariant=1.4.0. ```. gave me:. ```. UnsatisfiableError: The following specifications were found. to be incompatible with the existing python installation in your environment:. Specifications:. - deepvariant=1.4.0 -> python[version='>=3.6,<3.7.0a0']. Your python: python=3.7.5. ```. @richard-nm One more question for you -- is it possible to use Docker or Singularity? Which our team can support more easily as well. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/627
https://github.com/google/deepvariant/issues/627:98,integrability,version,version,98,"Hi @richard-nm , thanks for the update. Given that our team didn't build or maintain the bioconda version, I'm not familiar with many things here. One question and one comment for you:. 1. Question: Where can I find documentation of this tool? For example, ""dv_call_variants.py"" isn't part of our GitHub repo, so I'm not familiar with that at all. 2. Comment: Purely based on the error message you posted:. ```. Baseline DeepVariant arguments. File ""/tmp/Bazel.runfiles_j_t9fnwc/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 374. raise ValueError(f'Shape mismatch in {example_info_json} and '. ^. SyntaxError: invalid syntax. ```. The corresponding line would be this one:. https://github.com/google/deepvariant/blob/r1.4/deepvariant/call_variants.py#L374. I'm not exactly why that would cause a SyntaxError though. It's a bit hard for me to figure out given that it's wrapped in something that I'm not familiar with. I did try to follow your steps above, but didn't succeed in installing:. ```. $ conda create -n deepvar python=3.7.5. $ conda install -c bioconda deepvariant=1.4.0. ```. gave me:. ```. UnsatisfiableError: The following specifications were found. to be incompatible with the existing python installation in your environment:. Specifications:. - deepvariant=1.4.0 -> python[version='>=3.6,<3.7.0a0']. Your python: python=3.7.5. ```. @richard-nm One more question for you -- is it possible to use Docker or Singularity? Which our team can support more easily as well. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/627
https://github.com/google/deepvariant/issues/627:386,integrability,messag,message,386,"Hi @richard-nm , thanks for the update. Given that our team didn't build or maintain the bioconda version, I'm not familiar with many things here. One question and one comment for you:. 1. Question: Where can I find documentation of this tool? For example, ""dv_call_variants.py"" isn't part of our GitHub repo, so I'm not familiar with that at all. 2. Comment: Purely based on the error message you posted:. ```. Baseline DeepVariant arguments. File ""/tmp/Bazel.runfiles_j_t9fnwc/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 374. raise ValueError(f'Shape mismatch in {example_info_json} and '. ^. SyntaxError: invalid syntax. ```. The corresponding line would be this one:. https://github.com/google/deepvariant/blob/r1.4/deepvariant/call_variants.py#L374. I'm not exactly why that would cause a SyntaxError though. It's a bit hard for me to figure out given that it's wrapped in something that I'm not familiar with. I did try to follow your steps above, but didn't succeed in installing:. ```. $ conda create -n deepvar python=3.7.5. $ conda install -c bioconda deepvariant=1.4.0. ```. gave me:. ```. UnsatisfiableError: The following specifications were found. to be incompatible with the existing python installation in your environment:. Specifications:. - deepvariant=1.4.0 -> python[version='>=3.6,<3.7.0a0']. Your python: python=3.7.5. ```. @richard-nm One more question for you -- is it possible to use Docker or Singularity? Which our team can support more easily as well. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/627
https://github.com/google/deepvariant/issues/627:891,integrability,wrap,wrapped,891,"Hi @richard-nm , thanks for the update. Given that our team didn't build or maintain the bioconda version, I'm not familiar with many things here. One question and one comment for you:. 1. Question: Where can I find documentation of this tool? For example, ""dv_call_variants.py"" isn't part of our GitHub repo, so I'm not familiar with that at all. 2. Comment: Purely based on the error message you posted:. ```. Baseline DeepVariant arguments. File ""/tmp/Bazel.runfiles_j_t9fnwc/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 374. raise ValueError(f'Shape mismatch in {example_info_json} and '. ^. SyntaxError: invalid syntax. ```. The corresponding line would be this one:. https://github.com/google/deepvariant/blob/r1.4/deepvariant/call_variants.py#L374. I'm not exactly why that would cause a SyntaxError though. It's a bit hard for me to figure out given that it's wrapped in something that I'm not familiar with. I did try to follow your steps above, but didn't succeed in installing:. ```. $ conda create -n deepvar python=3.7.5. $ conda install -c bioconda deepvariant=1.4.0. ```. gave me:. ```. UnsatisfiableError: The following specifications were found. to be incompatible with the existing python installation in your environment:. Specifications:. - deepvariant=1.4.0 -> python[version='>=3.6,<3.7.0a0']. Your python: python=3.7.5. ```. @richard-nm One more question for you -- is it possible to use Docker or Singularity? Which our team can support more easily as well. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/627
https://github.com/google/deepvariant/issues/627:1312,integrability,version,version,1312,"Hi @richard-nm , thanks for the update. Given that our team didn't build or maintain the bioconda version, I'm not familiar with many things here. One question and one comment for you:. 1. Question: Where can I find documentation of this tool? For example, ""dv_call_variants.py"" isn't part of our GitHub repo, so I'm not familiar with that at all. 2. Comment: Purely based on the error message you posted:. ```. Baseline DeepVariant arguments. File ""/tmp/Bazel.runfiles_j_t9fnwc/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 374. raise ValueError(f'Shape mismatch in {example_info_json} and '. ^. SyntaxError: invalid syntax. ```. The corresponding line would be this one:. https://github.com/google/deepvariant/blob/r1.4/deepvariant/call_variants.py#L374. I'm not exactly why that would cause a SyntaxError though. It's a bit hard for me to figure out given that it's wrapped in something that I'm not familiar with. I did try to follow your steps above, but didn't succeed in installing:. ```. $ conda create -n deepvar python=3.7.5. $ conda install -c bioconda deepvariant=1.4.0. ```. gave me:. ```. UnsatisfiableError: The following specifications were found. to be incompatible with the existing python installation in your environment:. Specifications:. - deepvariant=1.4.0 -> python[version='>=3.6,<3.7.0a0']. Your python: python=3.7.5. ```. @richard-nm One more question for you -- is it possible to use Docker or Singularity? Which our team can support more easily as well. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/627
https://github.com/google/deepvariant/issues/627:386,interoperability,messag,message,386,"Hi @richard-nm , thanks for the update. Given that our team didn't build or maintain the bioconda version, I'm not familiar with many things here. One question and one comment for you:. 1. Question: Where can I find documentation of this tool? For example, ""dv_call_variants.py"" isn't part of our GitHub repo, so I'm not familiar with that at all. 2. Comment: Purely based on the error message you posted:. ```. Baseline DeepVariant arguments. File ""/tmp/Bazel.runfiles_j_t9fnwc/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 374. raise ValueError(f'Shape mismatch in {example_info_json} and '. ^. SyntaxError: invalid syntax. ```. The corresponding line would be this one:. https://github.com/google/deepvariant/blob/r1.4/deepvariant/call_variants.py#L374. I'm not exactly why that would cause a SyntaxError though. It's a bit hard for me to figure out given that it's wrapped in something that I'm not familiar with. I did try to follow your steps above, but didn't succeed in installing:. ```. $ conda create -n deepvar python=3.7.5. $ conda install -c bioconda deepvariant=1.4.0. ```. gave me:. ```. UnsatisfiableError: The following specifications were found. to be incompatible with the existing python installation in your environment:. Specifications:. - deepvariant=1.4.0 -> python[version='>=3.6,<3.7.0a0']. Your python: python=3.7.5. ```. @richard-nm One more question for you -- is it possible to use Docker or Singularity? Which our team can support more easily as well. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/627
https://github.com/google/deepvariant/issues/627:577,interoperability,mismatch,mismatch,577,"Hi @richard-nm , thanks for the update. Given that our team didn't build or maintain the bioconda version, I'm not familiar with many things here. One question and one comment for you:. 1. Question: Where can I find documentation of this tool? For example, ""dv_call_variants.py"" isn't part of our GitHub repo, so I'm not familiar with that at all. 2. Comment: Purely based on the error message you posted:. ```. Baseline DeepVariant arguments. File ""/tmp/Bazel.runfiles_j_t9fnwc/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 374. raise ValueError(f'Shape mismatch in {example_info_json} and '. ^. SyntaxError: invalid syntax. ```. The corresponding line would be this one:. https://github.com/google/deepvariant/blob/r1.4/deepvariant/call_variants.py#L374. I'm not exactly why that would cause a SyntaxError though. It's a bit hard for me to figure out given that it's wrapped in something that I'm not familiar with. I did try to follow your steps above, but didn't succeed in installing:. ```. $ conda create -n deepvar python=3.7.5. $ conda install -c bioconda deepvariant=1.4.0. ```. gave me:. ```. UnsatisfiableError: The following specifications were found. to be incompatible with the existing python installation in your environment:. Specifications:. - deepvariant=1.4.0 -> python[version='>=3.6,<3.7.0a0']. Your python: python=3.7.5. ```. @richard-nm One more question for you -- is it possible to use Docker or Singularity? Which our team can support more easily as well. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/627
https://github.com/google/deepvariant/issues/627:1159,interoperability,specif,specifications,1159,"Hi @richard-nm , thanks for the update. Given that our team didn't build or maintain the bioconda version, I'm not familiar with many things here. One question and one comment for you:. 1. Question: Where can I find documentation of this tool? For example, ""dv_call_variants.py"" isn't part of our GitHub repo, so I'm not familiar with that at all. 2. Comment: Purely based on the error message you posted:. ```. Baseline DeepVariant arguments. File ""/tmp/Bazel.runfiles_j_t9fnwc/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 374. raise ValueError(f'Shape mismatch in {example_info_json} and '. ^. SyntaxError: invalid syntax. ```. The corresponding line would be this one:. https://github.com/google/deepvariant/blob/r1.4/deepvariant/call_variants.py#L374. I'm not exactly why that would cause a SyntaxError though. It's a bit hard for me to figure out given that it's wrapped in something that I'm not familiar with. I did try to follow your steps above, but didn't succeed in installing:. ```. $ conda create -n deepvar python=3.7.5. $ conda install -c bioconda deepvariant=1.4.0. ```. gave me:. ```. UnsatisfiableError: The following specifications were found. to be incompatible with the existing python installation in your environment:. Specifications:. - deepvariant=1.4.0 -> python[version='>=3.6,<3.7.0a0']. Your python: python=3.7.5. ```. @richard-nm One more question for you -- is it possible to use Docker or Singularity? Which our team can support more easily as well. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/627
https://github.com/google/deepvariant/issues/627:1192,interoperability,incompatib,incompatible,1192,"Hi @richard-nm , thanks for the update. Given that our team didn't build or maintain the bioconda version, I'm not familiar with many things here. One question and one comment for you:. 1. Question: Where can I find documentation of this tool? For example, ""dv_call_variants.py"" isn't part of our GitHub repo, so I'm not familiar with that at all. 2. Comment: Purely based on the error message you posted:. ```. Baseline DeepVariant arguments. File ""/tmp/Bazel.runfiles_j_t9fnwc/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 374. raise ValueError(f'Shape mismatch in {example_info_json} and '. ^. SyntaxError: invalid syntax. ```. The corresponding line would be this one:. https://github.com/google/deepvariant/blob/r1.4/deepvariant/call_variants.py#L374. I'm not exactly why that would cause a SyntaxError though. It's a bit hard for me to figure out given that it's wrapped in something that I'm not familiar with. I did try to follow your steps above, but didn't succeed in installing:. ```. $ conda create -n deepvar python=3.7.5. $ conda install -c bioconda deepvariant=1.4.0. ```. gave me:. ```. UnsatisfiableError: The following specifications were found. to be incompatible with the existing python installation in your environment:. Specifications:. - deepvariant=1.4.0 -> python[version='>=3.6,<3.7.0a0']. Your python: python=3.7.5. ```. @richard-nm One more question for you -- is it possible to use Docker or Singularity? Which our team can support more easily as well. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/627
https://github.com/google/deepvariant/issues/627:1265,interoperability,Specif,Specifications,1265,"Hi @richard-nm , thanks for the update. Given that our team didn't build or maintain the bioconda version, I'm not familiar with many things here. One question and one comment for you:. 1. Question: Where can I find documentation of this tool? For example, ""dv_call_variants.py"" isn't part of our GitHub repo, so I'm not familiar with that at all. 2. Comment: Purely based on the error message you posted:. ```. Baseline DeepVariant arguments. File ""/tmp/Bazel.runfiles_j_t9fnwc/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 374. raise ValueError(f'Shape mismatch in {example_info_json} and '. ^. SyntaxError: invalid syntax. ```. The corresponding line would be this one:. https://github.com/google/deepvariant/blob/r1.4/deepvariant/call_variants.py#L374. I'm not exactly why that would cause a SyntaxError though. It's a bit hard for me to figure out given that it's wrapped in something that I'm not familiar with. I did try to follow your steps above, but didn't succeed in installing:. ```. $ conda create -n deepvar python=3.7.5. $ conda install -c bioconda deepvariant=1.4.0. ```. gave me:. ```. UnsatisfiableError: The following specifications were found. to be incompatible with the existing python installation in your environment:. Specifications:. - deepvariant=1.4.0 -> python[version='>=3.6,<3.7.0a0']. Your python: python=3.7.5. ```. @richard-nm One more question for you -- is it possible to use Docker or Singularity? Which our team can support more easily as well. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/627
https://github.com/google/deepvariant/issues/627:76,modifiability,maintain,maintain,76,"Hi @richard-nm , thanks for the update. Given that our team didn't build or maintain the bioconda version, I'm not familiar with many things here. One question and one comment for you:. 1. Question: Where can I find documentation of this tool? For example, ""dv_call_variants.py"" isn't part of our GitHub repo, so I'm not familiar with that at all. 2. Comment: Purely based on the error message you posted:. ```. Baseline DeepVariant arguments. File ""/tmp/Bazel.runfiles_j_t9fnwc/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 374. raise ValueError(f'Shape mismatch in {example_info_json} and '. ^. SyntaxError: invalid syntax. ```. The corresponding line would be this one:. https://github.com/google/deepvariant/blob/r1.4/deepvariant/call_variants.py#L374. I'm not exactly why that would cause a SyntaxError though. It's a bit hard for me to figure out given that it's wrapped in something that I'm not familiar with. I did try to follow your steps above, but didn't succeed in installing:. ```. $ conda create -n deepvar python=3.7.5. $ conda install -c bioconda deepvariant=1.4.0. ```. gave me:. ```. UnsatisfiableError: The following specifications were found. to be incompatible with the existing python installation in your environment:. Specifications:. - deepvariant=1.4.0 -> python[version='>=3.6,<3.7.0a0']. Your python: python=3.7.5. ```. @richard-nm One more question for you -- is it possible to use Docker or Singularity? Which our team can support more easily as well. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/627
https://github.com/google/deepvariant/issues/627:98,modifiability,version,version,98,"Hi @richard-nm , thanks for the update. Given that our team didn't build or maintain the bioconda version, I'm not familiar with many things here. One question and one comment for you:. 1. Question: Where can I find documentation of this tool? For example, ""dv_call_variants.py"" isn't part of our GitHub repo, so I'm not familiar with that at all. 2. Comment: Purely based on the error message you posted:. ```. Baseline DeepVariant arguments. File ""/tmp/Bazel.runfiles_j_t9fnwc/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 374. raise ValueError(f'Shape mismatch in {example_info_json} and '. ^. SyntaxError: invalid syntax. ```. The corresponding line would be this one:. https://github.com/google/deepvariant/blob/r1.4/deepvariant/call_variants.py#L374. I'm not exactly why that would cause a SyntaxError though. It's a bit hard for me to figure out given that it's wrapped in something that I'm not familiar with. I did try to follow your steps above, but didn't succeed in installing:. ```. $ conda create -n deepvar python=3.7.5. $ conda install -c bioconda deepvariant=1.4.0. ```. gave me:. ```. UnsatisfiableError: The following specifications were found. to be incompatible with the existing python installation in your environment:. Specifications:. - deepvariant=1.4.0 -> python[version='>=3.6,<3.7.0a0']. Your python: python=3.7.5. ```. @richard-nm One more question for you -- is it possible to use Docker or Singularity? Which our team can support more easily as well. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/627
https://github.com/google/deepvariant/issues/627:1312,modifiability,version,version,1312,"Hi @richard-nm , thanks for the update. Given that our team didn't build or maintain the bioconda version, I'm not familiar with many things here. One question and one comment for you:. 1. Question: Where can I find documentation of this tool? For example, ""dv_call_variants.py"" isn't part of our GitHub repo, so I'm not familiar with that at all. 2. Comment: Purely based on the error message you posted:. ```. Baseline DeepVariant arguments. File ""/tmp/Bazel.runfiles_j_t9fnwc/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 374. raise ValueError(f'Shape mismatch in {example_info_json} and '. ^. SyntaxError: invalid syntax. ```. The corresponding line would be this one:. https://github.com/google/deepvariant/blob/r1.4/deepvariant/call_variants.py#L374. I'm not exactly why that would cause a SyntaxError though. It's a bit hard for me to figure out given that it's wrapped in something that I'm not familiar with. I did try to follow your steps above, but didn't succeed in installing:. ```. $ conda create -n deepvar python=3.7.5. $ conda install -c bioconda deepvariant=1.4.0. ```. gave me:. ```. UnsatisfiableError: The following specifications were found. to be incompatible with the existing python installation in your environment:. Specifications:. - deepvariant=1.4.0 -> python[version='>=3.6,<3.7.0a0']. Your python: python=3.7.5. ```. @richard-nm One more question for you -- is it possible to use Docker or Singularity? Which our team can support more easily as well. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/627
https://github.com/google/deepvariant/issues/627:380,performance,error,error,380,"Hi @richard-nm , thanks for the update. Given that our team didn't build or maintain the bioconda version, I'm not familiar with many things here. One question and one comment for you:. 1. Question: Where can I find documentation of this tool? For example, ""dv_call_variants.py"" isn't part of our GitHub repo, so I'm not familiar with that at all. 2. Comment: Purely based on the error message you posted:. ```. Baseline DeepVariant arguments. File ""/tmp/Bazel.runfiles_j_t9fnwc/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 374. raise ValueError(f'Shape mismatch in {example_info_json} and '. ^. SyntaxError: invalid syntax. ```. The corresponding line would be this one:. https://github.com/google/deepvariant/blob/r1.4/deepvariant/call_variants.py#L374. I'm not exactly why that would cause a SyntaxError though. It's a bit hard for me to figure out given that it's wrapped in something that I'm not familiar with. I did try to follow your steps above, but didn't succeed in installing:. ```. $ conda create -n deepvar python=3.7.5. $ conda install -c bioconda deepvariant=1.4.0. ```. gave me:. ```. UnsatisfiableError: The following specifications were found. to be incompatible with the existing python installation in your environment:. Specifications:. - deepvariant=1.4.0 -> python[version='>=3.6,<3.7.0a0']. Your python: python=3.7.5. ```. @richard-nm One more question for you -- is it possible to use Docker or Singularity? Which our team can support more easily as well. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/627
https://github.com/google/deepvariant/issues/627:32,safety,updat,update,32,"Hi @richard-nm , thanks for the update. Given that our team didn't build or maintain the bioconda version, I'm not familiar with many things here. One question and one comment for you:. 1. Question: Where can I find documentation of this tool? For example, ""dv_call_variants.py"" isn't part of our GitHub repo, so I'm not familiar with that at all. 2. Comment: Purely based on the error message you posted:. ```. Baseline DeepVariant arguments. File ""/tmp/Bazel.runfiles_j_t9fnwc/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 374. raise ValueError(f'Shape mismatch in {example_info_json} and '. ^. SyntaxError: invalid syntax. ```. The corresponding line would be this one:. https://github.com/google/deepvariant/blob/r1.4/deepvariant/call_variants.py#L374. I'm not exactly why that would cause a SyntaxError though. It's a bit hard for me to figure out given that it's wrapped in something that I'm not familiar with. I did try to follow your steps above, but didn't succeed in installing:. ```. $ conda create -n deepvar python=3.7.5. $ conda install -c bioconda deepvariant=1.4.0. ```. gave me:. ```. UnsatisfiableError: The following specifications were found. to be incompatible with the existing python installation in your environment:. Specifications:. - deepvariant=1.4.0 -> python[version='>=3.6,<3.7.0a0']. Your python: python=3.7.5. ```. @richard-nm One more question for you -- is it possible to use Docker or Singularity? Which our team can support more easily as well. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/627
https://github.com/google/deepvariant/issues/627:76,safety,maintain,maintain,76,"Hi @richard-nm , thanks for the update. Given that our team didn't build or maintain the bioconda version, I'm not familiar with many things here. One question and one comment for you:. 1. Question: Where can I find documentation of this tool? For example, ""dv_call_variants.py"" isn't part of our GitHub repo, so I'm not familiar with that at all. 2. Comment: Purely based on the error message you posted:. ```. Baseline DeepVariant arguments. File ""/tmp/Bazel.runfiles_j_t9fnwc/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 374. raise ValueError(f'Shape mismatch in {example_info_json} and '. ^. SyntaxError: invalid syntax. ```. The corresponding line would be this one:. https://github.com/google/deepvariant/blob/r1.4/deepvariant/call_variants.py#L374. I'm not exactly why that would cause a SyntaxError though. It's a bit hard for me to figure out given that it's wrapped in something that I'm not familiar with. I did try to follow your steps above, but didn't succeed in installing:. ```. $ conda create -n deepvar python=3.7.5. $ conda install -c bioconda deepvariant=1.4.0. ```. gave me:. ```. UnsatisfiableError: The following specifications were found. to be incompatible with the existing python installation in your environment:. Specifications:. - deepvariant=1.4.0 -> python[version='>=3.6,<3.7.0a0']. Your python: python=3.7.5. ```. @richard-nm One more question for you -- is it possible to use Docker or Singularity? Which our team can support more easily as well. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/627
https://github.com/google/deepvariant/issues/627:380,safety,error,error,380,"Hi @richard-nm , thanks for the update. Given that our team didn't build or maintain the bioconda version, I'm not familiar with many things here. One question and one comment for you:. 1. Question: Where can I find documentation of this tool? For example, ""dv_call_variants.py"" isn't part of our GitHub repo, so I'm not familiar with that at all. 2. Comment: Purely based on the error message you posted:. ```. Baseline DeepVariant arguments. File ""/tmp/Bazel.runfiles_j_t9fnwc/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 374. raise ValueError(f'Shape mismatch in {example_info_json} and '. ^. SyntaxError: invalid syntax. ```. The corresponding line would be this one:. https://github.com/google/deepvariant/blob/r1.4/deepvariant/call_variants.py#L374. I'm not exactly why that would cause a SyntaxError though. It's a bit hard for me to figure out given that it's wrapped in something that I'm not familiar with. I did try to follow your steps above, but didn't succeed in installing:. ```. $ conda create -n deepvar python=3.7.5. $ conda install -c bioconda deepvariant=1.4.0. ```. gave me:. ```. UnsatisfiableError: The following specifications were found. to be incompatible with the existing python installation in your environment:. Specifications:. - deepvariant=1.4.0 -> python[version='>=3.6,<3.7.0a0']. Your python: python=3.7.5. ```. @richard-nm One more question for you -- is it possible to use Docker or Singularity? Which our team can support more easily as well. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/627
https://github.com/google/deepvariant/issues/627:32,security,updat,update,32,"Hi @richard-nm , thanks for the update. Given that our team didn't build or maintain the bioconda version, I'm not familiar with many things here. One question and one comment for you:. 1. Question: Where can I find documentation of this tool? For example, ""dv_call_variants.py"" isn't part of our GitHub repo, so I'm not familiar with that at all. 2. Comment: Purely based on the error message you posted:. ```. Baseline DeepVariant arguments. File ""/tmp/Bazel.runfiles_j_t9fnwc/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 374. raise ValueError(f'Shape mismatch in {example_info_json} and '. ^. SyntaxError: invalid syntax. ```. The corresponding line would be this one:. https://github.com/google/deepvariant/blob/r1.4/deepvariant/call_variants.py#L374. I'm not exactly why that would cause a SyntaxError though. It's a bit hard for me to figure out given that it's wrapped in something that I'm not familiar with. I did try to follow your steps above, but didn't succeed in installing:. ```. $ conda create -n deepvar python=3.7.5. $ conda install -c bioconda deepvariant=1.4.0. ```. gave me:. ```. UnsatisfiableError: The following specifications were found. to be incompatible with the existing python installation in your environment:. Specifications:. - deepvariant=1.4.0 -> python[version='>=3.6,<3.7.0a0']. Your python: python=3.7.5. ```. @richard-nm One more question for you -- is it possible to use Docker or Singularity? Which our team can support more easily as well. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/627
https://github.com/google/deepvariant/issues/627:55,security,team,team,55,"Hi @richard-nm , thanks for the update. Given that our team didn't build or maintain the bioconda version, I'm not familiar with many things here. One question and one comment for you:. 1. Question: Where can I find documentation of this tool? For example, ""dv_call_variants.py"" isn't part of our GitHub repo, so I'm not familiar with that at all. 2. Comment: Purely based on the error message you posted:. ```. Baseline DeepVariant arguments. File ""/tmp/Bazel.runfiles_j_t9fnwc/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 374. raise ValueError(f'Shape mismatch in {example_info_json} and '. ^. SyntaxError: invalid syntax. ```. The corresponding line would be this one:. https://github.com/google/deepvariant/blob/r1.4/deepvariant/call_variants.py#L374. I'm not exactly why that would cause a SyntaxError though. It's a bit hard for me to figure out given that it's wrapped in something that I'm not familiar with. I did try to follow your steps above, but didn't succeed in installing:. ```. $ conda create -n deepvar python=3.7.5. $ conda install -c bioconda deepvariant=1.4.0. ```. gave me:. ```. UnsatisfiableError: The following specifications were found. to be incompatible with the existing python installation in your environment:. Specifications:. - deepvariant=1.4.0 -> python[version='>=3.6,<3.7.0a0']. Your python: python=3.7.5. ```. @richard-nm One more question for you -- is it possible to use Docker or Singularity? Which our team can support more easily as well. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/627
https://github.com/google/deepvariant/issues/627:1467,security,team,team,1467,"Hi @richard-nm , thanks for the update. Given that our team didn't build or maintain the bioconda version, I'm not familiar with many things here. One question and one comment for you:. 1. Question: Where can I find documentation of this tool? For example, ""dv_call_variants.py"" isn't part of our GitHub repo, so I'm not familiar with that at all. 2. Comment: Purely based on the error message you posted:. ```. Baseline DeepVariant arguments. File ""/tmp/Bazel.runfiles_j_t9fnwc/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 374. raise ValueError(f'Shape mismatch in {example_info_json} and '. ^. SyntaxError: invalid syntax. ```. The corresponding line would be this one:. https://github.com/google/deepvariant/blob/r1.4/deepvariant/call_variants.py#L374. I'm not exactly why that would cause a SyntaxError though. It's a bit hard for me to figure out given that it's wrapped in something that I'm not familiar with. I did try to follow your steps above, but didn't succeed in installing:. ```. $ conda create -n deepvar python=3.7.5. $ conda install -c bioconda deepvariant=1.4.0. ```. gave me:. ```. UnsatisfiableError: The following specifications were found. to be incompatible with the existing python installation in your environment:. Specifications:. - deepvariant=1.4.0 -> python[version='>=3.6,<3.7.0a0']. Your python: python=3.7.5. ```. @richard-nm One more question for you -- is it possible to use Docker or Singularity? Which our team can support more easily as well. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/627
https://github.com/google/deepvariant/issues/627:216,usability,document,documentation,216,"Hi @richard-nm , thanks for the update. Given that our team didn't build or maintain the bioconda version, I'm not familiar with many things here. One question and one comment for you:. 1. Question: Where can I find documentation of this tool? For example, ""dv_call_variants.py"" isn't part of our GitHub repo, so I'm not familiar with that at all. 2. Comment: Purely based on the error message you posted:. ```. Baseline DeepVariant arguments. File ""/tmp/Bazel.runfiles_j_t9fnwc/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 374. raise ValueError(f'Shape mismatch in {example_info_json} and '. ^. SyntaxError: invalid syntax. ```. The corresponding line would be this one:. https://github.com/google/deepvariant/blob/r1.4/deepvariant/call_variants.py#L374. I'm not exactly why that would cause a SyntaxError though. It's a bit hard for me to figure out given that it's wrapped in something that I'm not familiar with. I did try to follow your steps above, but didn't succeed in installing:. ```. $ conda create -n deepvar python=3.7.5. $ conda install -c bioconda deepvariant=1.4.0. ```. gave me:. ```. UnsatisfiableError: The following specifications were found. to be incompatible with the existing python installation in your environment:. Specifications:. - deepvariant=1.4.0 -> python[version='>=3.6,<3.7.0a0']. Your python: python=3.7.5. ```. @richard-nm One more question for you -- is it possible to use Docker or Singularity? Which our team can support more easily as well. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/627
https://github.com/google/deepvariant/issues/627:238,usability,tool,tool,238,"Hi @richard-nm , thanks for the update. Given that our team didn't build or maintain the bioconda version, I'm not familiar with many things here. One question and one comment for you:. 1. Question: Where can I find documentation of this tool? For example, ""dv_call_variants.py"" isn't part of our GitHub repo, so I'm not familiar with that at all. 2. Comment: Purely based on the error message you posted:. ```. Baseline DeepVariant arguments. File ""/tmp/Bazel.runfiles_j_t9fnwc/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 374. raise ValueError(f'Shape mismatch in {example_info_json} and '. ^. SyntaxError: invalid syntax. ```. The corresponding line would be this one:. https://github.com/google/deepvariant/blob/r1.4/deepvariant/call_variants.py#L374. I'm not exactly why that would cause a SyntaxError though. It's a bit hard for me to figure out given that it's wrapped in something that I'm not familiar with. I did try to follow your steps above, but didn't succeed in installing:. ```. $ conda create -n deepvar python=3.7.5. $ conda install -c bioconda deepvariant=1.4.0. ```. gave me:. ```. UnsatisfiableError: The following specifications were found. to be incompatible with the existing python installation in your environment:. Specifications:. - deepvariant=1.4.0 -> python[version='>=3.6,<3.7.0a0']. Your python: python=3.7.5. ```. @richard-nm One more question for you -- is it possible to use Docker or Singularity? Which our team can support more easily as well. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/627
https://github.com/google/deepvariant/issues/627:380,usability,error,error,380,"Hi @richard-nm , thanks for the update. Given that our team didn't build or maintain the bioconda version, I'm not familiar with many things here. One question and one comment for you:. 1. Question: Where can I find documentation of this tool? For example, ""dv_call_variants.py"" isn't part of our GitHub repo, so I'm not familiar with that at all. 2. Comment: Purely based on the error message you posted:. ```. Baseline DeepVariant arguments. File ""/tmp/Bazel.runfiles_j_t9fnwc/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 374. raise ValueError(f'Shape mismatch in {example_info_json} and '. ^. SyntaxError: invalid syntax. ```. The corresponding line would be this one:. https://github.com/google/deepvariant/blob/r1.4/deepvariant/call_variants.py#L374. I'm not exactly why that would cause a SyntaxError though. It's a bit hard for me to figure out given that it's wrapped in something that I'm not familiar with. I did try to follow your steps above, but didn't succeed in installing:. ```. $ conda create -n deepvar python=3.7.5. $ conda install -c bioconda deepvariant=1.4.0. ```. gave me:. ```. UnsatisfiableError: The following specifications were found. to be incompatible with the existing python installation in your environment:. Specifications:. - deepvariant=1.4.0 -> python[version='>=3.6,<3.7.0a0']. Your python: python=3.7.5. ```. @richard-nm One more question for you -- is it possible to use Docker or Singularity? Which our team can support more easily as well. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/627
https://github.com/google/deepvariant/issues/627:1476,usability,support,support,1476,"Hi @richard-nm , thanks for the update. Given that our team didn't build or maintain the bioconda version, I'm not familiar with many things here. One question and one comment for you:. 1. Question: Where can I find documentation of this tool? For example, ""dv_call_variants.py"" isn't part of our GitHub repo, so I'm not familiar with that at all. 2. Comment: Purely based on the error message you posted:. ```. Baseline DeepVariant arguments. File ""/tmp/Bazel.runfiles_j_t9fnwc/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 374. raise ValueError(f'Shape mismatch in {example_info_json} and '. ^. SyntaxError: invalid syntax. ```. The corresponding line would be this one:. https://github.com/google/deepvariant/blob/r1.4/deepvariant/call_variants.py#L374. I'm not exactly why that would cause a SyntaxError though. It's a bit hard for me to figure out given that it's wrapped in something that I'm not familiar with. I did try to follow your steps above, but didn't succeed in installing:. ```. $ conda create -n deepvar python=3.7.5. $ conda install -c bioconda deepvariant=1.4.0. ```. gave me:. ```. UnsatisfiableError: The following specifications were found. to be incompatible with the existing python installation in your environment:. Specifications:. - deepvariant=1.4.0 -> python[version='>=3.6,<3.7.0a0']. Your python: python=3.7.5. ```. @richard-nm One more question for you -- is it possible to use Docker or Singularity? Which our team can support more easily as well. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/627
https://github.com/google/deepvariant/issues/627:79,usability,close,close,79,"Hi @richard-nm , it seems like this thread has been inactive for a while. I'll close it now, but feel free to reopen to follow up if you like.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/627
https://github.com/google/deepvariant/issues/628:147,reliability,doe,doesn,147,It works when I add --pileup_image_height 100 and --pileup_image_width 99 after /opt/deepvariant/bin/make_examples.zip. But I was wandering why it doesn't choose right shape with previous command?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/628
https://github.com/google/deepvariant/issues/628:188,usability,command,command,188,It works when I add --pileup_image_height 100 and --pileup_image_width 99 after /opt/deepvariant/bin/make_examples.zip. But I was wandering why it doesn't choose right shape with previous command?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/628
https://github.com/google/deepvariant/issues/628:97,deployability,automat,automatically,97,"The shapes are different for pacbio model versus Illumina ""WGS"" model. Everything works together automatically if you use the run_deepvariant.py script, which is our recommended easiest way to run DeepVariant. But if you do prefer to use make_examples and call_variants separately, then you'll need to check that the parameters all make sense together. A good way to check this is to run a small run_deepvariant.py run or dry-run and see what parameters it passes to the make_examples and call_variants steps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/628
https://github.com/google/deepvariant/issues/628:36,energy efficiency,model,model,36,"The shapes are different for pacbio model versus Illumina ""WGS"" model. Everything works together automatically if you use the run_deepvariant.py script, which is our recommended easiest way to run DeepVariant. But if you do prefer to use make_examples and call_variants separately, then you'll need to check that the parameters all make sense together. A good way to check this is to run a small run_deepvariant.py run or dry-run and see what parameters it passes to the make_examples and call_variants steps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/628
https://github.com/google/deepvariant/issues/628:64,energy efficiency,model,model,64,"The shapes are different for pacbio model versus Illumina ""WGS"" model. Everything works together automatically if you use the run_deepvariant.py script, which is our recommended easiest way to run DeepVariant. But if you do prefer to use make_examples and call_variants separately, then you'll need to check that the parameters all make sense together. A good way to check this is to run a small run_deepvariant.py run or dry-run and see what parameters it passes to the make_examples and call_variants steps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/628
https://github.com/google/deepvariant/issues/628:29,modifiability,pac,pacbio,29,"The shapes are different for pacbio model versus Illumina ""WGS"" model. Everything works together automatically if you use the run_deepvariant.py script, which is our recommended easiest way to run DeepVariant. But if you do prefer to use make_examples and call_variants separately, then you'll need to check that the parameters all make sense together. A good way to check this is to run a small run_deepvariant.py run or dry-run and see what parameters it passes to the make_examples and call_variants steps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/628
https://github.com/google/deepvariant/issues/628:317,modifiability,paramet,parameters,317,"The shapes are different for pacbio model versus Illumina ""WGS"" model. Everything works together automatically if you use the run_deepvariant.py script, which is our recommended easiest way to run DeepVariant. But if you do prefer to use make_examples and call_variants separately, then you'll need to check that the parameters all make sense together. A good way to check this is to run a small run_deepvariant.py run or dry-run and see what parameters it passes to the make_examples and call_variants steps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/628
https://github.com/google/deepvariant/issues/628:443,modifiability,paramet,parameters,443,"The shapes are different for pacbio model versus Illumina ""WGS"" model. Everything works together automatically if you use the run_deepvariant.py script, which is our recommended easiest way to run DeepVariant. But if you do prefer to use make_examples and call_variants separately, then you'll need to check that the parameters all make sense together. A good way to check this is to run a small run_deepvariant.py run or dry-run and see what parameters it passes to the make_examples and call_variants steps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/628
https://github.com/google/deepvariant/issues/628:36,security,model,model,36,"The shapes are different for pacbio model versus Illumina ""WGS"" model. Everything works together automatically if you use the run_deepvariant.py script, which is our recommended easiest way to run DeepVariant. But if you do prefer to use make_examples and call_variants separately, then you'll need to check that the parameters all make sense together. A good way to check this is to run a small run_deepvariant.py run or dry-run and see what parameters it passes to the make_examples and call_variants steps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/628
https://github.com/google/deepvariant/issues/628:64,security,model,model,64,"The shapes are different for pacbio model versus Illumina ""WGS"" model. Everything works together automatically if you use the run_deepvariant.py script, which is our recommended easiest way to run DeepVariant. But if you do prefer to use make_examples and call_variants separately, then you'll need to check that the parameters all make sense together. A good way to check this is to run a small run_deepvariant.py run or dry-run and see what parameters it passes to the make_examples and call_variants steps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/628
https://github.com/google/deepvariant/issues/628:97,testability,automat,automatically,97,"The shapes are different for pacbio model versus Illumina ""WGS"" model. Everything works together automatically if you use the run_deepvariant.py script, which is our recommended easiest way to run DeepVariant. But if you do prefer to use make_examples and call_variants separately, then you'll need to check that the parameters all make sense together. A good way to check this is to run a small run_deepvariant.py run or dry-run and see what parameters it passes to the make_examples and call_variants steps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/628
https://github.com/google/deepvariant/issues/628:224,usability,prefer,prefer,224,"The shapes are different for pacbio model versus Illumina ""WGS"" model. Everything works together automatically if you use the run_deepvariant.py script, which is our recommended easiest way to run DeepVariant. But if you do prefer to use make_examples and call_variants separately, then you'll need to check that the parameters all make sense together. A good way to check this is to run a small run_deepvariant.py run or dry-run and see what parameters it passes to the make_examples and call_variants steps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/628
https://github.com/google/deepvariant/issues/629:99,availability,error,errors,99,"Hi @JosephLalli . At the time we were putting together the vg-giraffe paper, we noticed that Indel errors would occur because the positioning of read indel events in the CIGAR string were not always left-normalized. ABRA both fixed that issue and also seems to give some additional advantage by standardizing the representation before running the candidate generation in DeepVariant. Subsequent to that finding, both ourselves and the vg team built methods which left-normalize Indel CIGAR events during processing. We found that this reduced almost all, but not quite all, of the Indel performance difference with ABRA processed reads. Because we have built the DeepVariant vg pipeline to include read normalization with the --normalize_reads=true tag, we don't routinely benchmark the two methods against each other. Our last head-to-head benchmark on ABRA on and off on 35x HG003 was no_ABRA Indel F1=0.9953, with ABRA Indel F1=0.9958. @pichuan Has made [this linked gist](https://gist.github.com/pichuan/eedab4cf2e06fa7ceb2fad0f9b3f8066) that we use for efficient, single machine processing of vg giraffe+DeepVariant. We opted not to include the use of ABRA realignment in that pipeline. With the benchmark numbers available here, I hope this is sufficient information for you to decide whether you would like to include it or not. We continue to look at improvements in the Indel realignment/reassembly process to see if we can further improve in some of the manners that ABRA helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/629
https://github.com/google/deepvariant/issues/629:1219,availability,avail,available,1219,"Hi @JosephLalli . At the time we were putting together the vg-giraffe paper, we noticed that Indel errors would occur because the positioning of read indel events in the CIGAR string were not always left-normalized. ABRA both fixed that issue and also seems to give some additional advantage by standardizing the representation before running the candidate generation in DeepVariant. Subsequent to that finding, both ourselves and the vg team built methods which left-normalize Indel CIGAR events during processing. We found that this reduced almost all, but not quite all, of the Indel performance difference with ABRA processed reads. Because we have built the DeepVariant vg pipeline to include read normalization with the --normalize_reads=true tag, we don't routinely benchmark the two methods against each other. Our last head-to-head benchmark on ABRA on and off on 35x HG003 was no_ABRA Indel F1=0.9953, with ABRA Indel F1=0.9958. @pichuan Has made [this linked gist](https://gist.github.com/pichuan/eedab4cf2e06fa7ceb2fad0f9b3f8066) that we use for efficient, single machine processing of vg giraffe+DeepVariant. We opted not to include the use of ABRA realignment in that pipeline. With the benchmark numbers available here, I hope this is sufficient information for you to decide whether you would like to include it or not. We continue to look at improvements in the Indel realignment/reassembly process to see if we can further improve in some of the manners that ABRA helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/629
https://github.com/google/deepvariant/issues/629:678,deployability,pipelin,pipeline,678,"Hi @JosephLalli . At the time we were putting together the vg-giraffe paper, we noticed that Indel errors would occur because the positioning of read indel events in the CIGAR string were not always left-normalized. ABRA both fixed that issue and also seems to give some additional advantage by standardizing the representation before running the candidate generation in DeepVariant. Subsequent to that finding, both ourselves and the vg team built methods which left-normalize Indel CIGAR events during processing. We found that this reduced almost all, but not quite all, of the Indel performance difference with ABRA processed reads. Because we have built the DeepVariant vg pipeline to include read normalization with the --normalize_reads=true tag, we don't routinely benchmark the two methods against each other. Our last head-to-head benchmark on ABRA on and off on 35x HG003 was no_ABRA Indel F1=0.9953, with ABRA Indel F1=0.9958. @pichuan Has made [this linked gist](https://gist.github.com/pichuan/eedab4cf2e06fa7ceb2fad0f9b3f8066) that we use for efficient, single machine processing of vg giraffe+DeepVariant. We opted not to include the use of ABRA realignment in that pipeline. With the benchmark numbers available here, I hope this is sufficient information for you to decide whether you would like to include it or not. We continue to look at improvements in the Indel realignment/reassembly process to see if we can further improve in some of the manners that ABRA helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/629
https://github.com/google/deepvariant/issues/629:1182,deployability,pipelin,pipeline,1182,"Hi @JosephLalli . At the time we were putting together the vg-giraffe paper, we noticed that Indel errors would occur because the positioning of read indel events in the CIGAR string were not always left-normalized. ABRA both fixed that issue and also seems to give some additional advantage by standardizing the representation before running the candidate generation in DeepVariant. Subsequent to that finding, both ourselves and the vg team built methods which left-normalize Indel CIGAR events during processing. We found that this reduced almost all, but not quite all, of the Indel performance difference with ABRA processed reads. Because we have built the DeepVariant vg pipeline to include read normalization with the --normalize_reads=true tag, we don't routinely benchmark the two methods against each other. Our last head-to-head benchmark on ABRA on and off on 35x HG003 was no_ABRA Indel F1=0.9953, with ABRA Indel F1=0.9958. @pichuan Has made [this linked gist](https://gist.github.com/pichuan/eedab4cf2e06fa7ceb2fad0f9b3f8066) that we use for efficient, single machine processing of vg giraffe+DeepVariant. We opted not to include the use of ABRA realignment in that pipeline. With the benchmark numbers available here, I hope this is sufficient information for you to decide whether you would like to include it or not. We continue to look at improvements in the Indel realignment/reassembly process to see if we can further improve in some of the manners that ABRA helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/629
https://github.com/google/deepvariant/issues/629:1339,deployability,continu,continue,1339,"Hi @JosephLalli . At the time we were putting together the vg-giraffe paper, we noticed that Indel errors would occur because the positioning of read indel events in the CIGAR string were not always left-normalized. ABRA both fixed that issue and also seems to give some additional advantage by standardizing the representation before running the candidate generation in DeepVariant. Subsequent to that finding, both ourselves and the vg team built methods which left-normalize Indel CIGAR events during processing. We found that this reduced almost all, but not quite all, of the Indel performance difference with ABRA processed reads. Because we have built the DeepVariant vg pipeline to include read normalization with the --normalize_reads=true tag, we don't routinely benchmark the two methods against each other. Our last head-to-head benchmark on ABRA on and off on 35x HG003 was no_ABRA Indel F1=0.9953, with ABRA Indel F1=0.9958. @pichuan Has made [this linked gist](https://gist.github.com/pichuan/eedab4cf2e06fa7ceb2fad0f9b3f8066) that we use for efficient, single machine processing of vg giraffe+DeepVariant. We opted not to include the use of ABRA realignment in that pipeline. With the benchmark numbers available here, I hope this is sufficient information for you to decide whether you would like to include it or not. We continue to look at improvements in the Indel realignment/reassembly process to see if we can further improve in some of the manners that ABRA helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/629
https://github.com/google/deepvariant/issues/629:535,energy efficiency,reduc,reduced,535,"Hi @JosephLalli . At the time we were putting together the vg-giraffe paper, we noticed that Indel errors would occur because the positioning of read indel events in the CIGAR string were not always left-normalized. ABRA both fixed that issue and also seems to give some additional advantage by standardizing the representation before running the candidate generation in DeepVariant. Subsequent to that finding, both ourselves and the vg team built methods which left-normalize Indel CIGAR events during processing. We found that this reduced almost all, but not quite all, of the Indel performance difference with ABRA processed reads. Because we have built the DeepVariant vg pipeline to include read normalization with the --normalize_reads=true tag, we don't routinely benchmark the two methods against each other. Our last head-to-head benchmark on ABRA on and off on 35x HG003 was no_ABRA Indel F1=0.9953, with ABRA Indel F1=0.9958. @pichuan Has made [this linked gist](https://gist.github.com/pichuan/eedab4cf2e06fa7ceb2fad0f9b3f8066) that we use for efficient, single machine processing of vg giraffe+DeepVariant. We opted not to include the use of ABRA realignment in that pipeline. With the benchmark numbers available here, I hope this is sufficient information for you to decide whether you would like to include it or not. We continue to look at improvements in the Indel realignment/reassembly process to see if we can further improve in some of the manners that ABRA helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/629
https://github.com/google/deepvariant/issues/629:156,integrability,event,events,156,"Hi @JosephLalli . At the time we were putting together the vg-giraffe paper, we noticed that Indel errors would occur because the positioning of read indel events in the CIGAR string were not always left-normalized. ABRA both fixed that issue and also seems to give some additional advantage by standardizing the representation before running the candidate generation in DeepVariant. Subsequent to that finding, both ourselves and the vg team built methods which left-normalize Indel CIGAR events during processing. We found that this reduced almost all, but not quite all, of the Indel performance difference with ABRA processed reads. Because we have built the DeepVariant vg pipeline to include read normalization with the --normalize_reads=true tag, we don't routinely benchmark the two methods against each other. Our last head-to-head benchmark on ABRA on and off on 35x HG003 was no_ABRA Indel F1=0.9953, with ABRA Indel F1=0.9958. @pichuan Has made [this linked gist](https://gist.github.com/pichuan/eedab4cf2e06fa7ceb2fad0f9b3f8066) that we use for efficient, single machine processing of vg giraffe+DeepVariant. We opted not to include the use of ABRA realignment in that pipeline. With the benchmark numbers available here, I hope this is sufficient information for you to decide whether you would like to include it or not. We continue to look at improvements in the Indel realignment/reassembly process to see if we can further improve in some of the manners that ABRA helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/629
https://github.com/google/deepvariant/issues/629:295,integrability,standardiz,standardizing,295,"Hi @JosephLalli . At the time we were putting together the vg-giraffe paper, we noticed that Indel errors would occur because the positioning of read indel events in the CIGAR string were not always left-normalized. ABRA both fixed that issue and also seems to give some additional advantage by standardizing the representation before running the candidate generation in DeepVariant. Subsequent to that finding, both ourselves and the vg team built methods which left-normalize Indel CIGAR events during processing. We found that this reduced almost all, but not quite all, of the Indel performance difference with ABRA processed reads. Because we have built the DeepVariant vg pipeline to include read normalization with the --normalize_reads=true tag, we don't routinely benchmark the two methods against each other. Our last head-to-head benchmark on ABRA on and off on 35x HG003 was no_ABRA Indel F1=0.9953, with ABRA Indel F1=0.9958. @pichuan Has made [this linked gist](https://gist.github.com/pichuan/eedab4cf2e06fa7ceb2fad0f9b3f8066) that we use for efficient, single machine processing of vg giraffe+DeepVariant. We opted not to include the use of ABRA realignment in that pipeline. With the benchmark numbers available here, I hope this is sufficient information for you to decide whether you would like to include it or not. We continue to look at improvements in the Indel realignment/reassembly process to see if we can further improve in some of the manners that ABRA helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/629
https://github.com/google/deepvariant/issues/629:384,integrability,Sub,Subsequent,384,"Hi @JosephLalli . At the time we were putting together the vg-giraffe paper, we noticed that Indel errors would occur because the positioning of read indel events in the CIGAR string were not always left-normalized. ABRA both fixed that issue and also seems to give some additional advantage by standardizing the representation before running the candidate generation in DeepVariant. Subsequent to that finding, both ourselves and the vg team built methods which left-normalize Indel CIGAR events during processing. We found that this reduced almost all, but not quite all, of the Indel performance difference with ABRA processed reads. Because we have built the DeepVariant vg pipeline to include read normalization with the --normalize_reads=true tag, we don't routinely benchmark the two methods against each other. Our last head-to-head benchmark on ABRA on and off on 35x HG003 was no_ABRA Indel F1=0.9953, with ABRA Indel F1=0.9958. @pichuan Has made [this linked gist](https://gist.github.com/pichuan/eedab4cf2e06fa7ceb2fad0f9b3f8066) that we use for efficient, single machine processing of vg giraffe+DeepVariant. We opted not to include the use of ABRA realignment in that pipeline. With the benchmark numbers available here, I hope this is sufficient information for you to decide whether you would like to include it or not. We continue to look at improvements in the Indel realignment/reassembly process to see if we can further improve in some of the manners that ABRA helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/629
https://github.com/google/deepvariant/issues/629:490,integrability,event,events,490,"Hi @JosephLalli . At the time we were putting together the vg-giraffe paper, we noticed that Indel errors would occur because the positioning of read indel events in the CIGAR string were not always left-normalized. ABRA both fixed that issue and also seems to give some additional advantage by standardizing the representation before running the candidate generation in DeepVariant. Subsequent to that finding, both ourselves and the vg team built methods which left-normalize Indel CIGAR events during processing. We found that this reduced almost all, but not quite all, of the Indel performance difference with ABRA processed reads. Because we have built the DeepVariant vg pipeline to include read normalization with the --normalize_reads=true tag, we don't routinely benchmark the two methods against each other. Our last head-to-head benchmark on ABRA on and off on 35x HG003 was no_ABRA Indel F1=0.9953, with ABRA Indel F1=0.9958. @pichuan Has made [this linked gist](https://gist.github.com/pichuan/eedab4cf2e06fa7ceb2fad0f9b3f8066) that we use for efficient, single machine processing of vg giraffe+DeepVariant. We opted not to include the use of ABRA realignment in that pipeline. With the benchmark numbers available here, I hope this is sufficient information for you to decide whether you would like to include it or not. We continue to look at improvements in the Indel realignment/reassembly process to see if we can further improve in some of the manners that ABRA helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/629
https://github.com/google/deepvariant/issues/629:678,integrability,pipelin,pipeline,678,"Hi @JosephLalli . At the time we were putting together the vg-giraffe paper, we noticed that Indel errors would occur because the positioning of read indel events in the CIGAR string were not always left-normalized. ABRA both fixed that issue and also seems to give some additional advantage by standardizing the representation before running the candidate generation in DeepVariant. Subsequent to that finding, both ourselves and the vg team built methods which left-normalize Indel CIGAR events during processing. We found that this reduced almost all, but not quite all, of the Indel performance difference with ABRA processed reads. Because we have built the DeepVariant vg pipeline to include read normalization with the --normalize_reads=true tag, we don't routinely benchmark the two methods against each other. Our last head-to-head benchmark on ABRA on and off on 35x HG003 was no_ABRA Indel F1=0.9953, with ABRA Indel F1=0.9958. @pichuan Has made [this linked gist](https://gist.github.com/pichuan/eedab4cf2e06fa7ceb2fad0f9b3f8066) that we use for efficient, single machine processing of vg giraffe+DeepVariant. We opted not to include the use of ABRA realignment in that pipeline. With the benchmark numbers available here, I hope this is sufficient information for you to decide whether you would like to include it or not. We continue to look at improvements in the Indel realignment/reassembly process to see if we can further improve in some of the manners that ABRA helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/629
https://github.com/google/deepvariant/issues/629:763,integrability,rout,routinely,763,"Hi @JosephLalli . At the time we were putting together the vg-giraffe paper, we noticed that Indel errors would occur because the positioning of read indel events in the CIGAR string were not always left-normalized. ABRA both fixed that issue and also seems to give some additional advantage by standardizing the representation before running the candidate generation in DeepVariant. Subsequent to that finding, both ourselves and the vg team built methods which left-normalize Indel CIGAR events during processing. We found that this reduced almost all, but not quite all, of the Indel performance difference with ABRA processed reads. Because we have built the DeepVariant vg pipeline to include read normalization with the --normalize_reads=true tag, we don't routinely benchmark the two methods against each other. Our last head-to-head benchmark on ABRA on and off on 35x HG003 was no_ABRA Indel F1=0.9953, with ABRA Indel F1=0.9958. @pichuan Has made [this linked gist](https://gist.github.com/pichuan/eedab4cf2e06fa7ceb2fad0f9b3f8066) that we use for efficient, single machine processing of vg giraffe+DeepVariant. We opted not to include the use of ABRA realignment in that pipeline. With the benchmark numbers available here, I hope this is sufficient information for you to decide whether you would like to include it or not. We continue to look at improvements in the Indel realignment/reassembly process to see if we can further improve in some of the manners that ABRA helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/629
https://github.com/google/deepvariant/issues/629:1182,integrability,pipelin,pipeline,1182,"Hi @JosephLalli . At the time we were putting together the vg-giraffe paper, we noticed that Indel errors would occur because the positioning of read indel events in the CIGAR string were not always left-normalized. ABRA both fixed that issue and also seems to give some additional advantage by standardizing the representation before running the candidate generation in DeepVariant. Subsequent to that finding, both ourselves and the vg team built methods which left-normalize Indel CIGAR events during processing. We found that this reduced almost all, but not quite all, of the Indel performance difference with ABRA processed reads. Because we have built the DeepVariant vg pipeline to include read normalization with the --normalize_reads=true tag, we don't routinely benchmark the two methods against each other. Our last head-to-head benchmark on ABRA on and off on 35x HG003 was no_ABRA Indel F1=0.9953, with ABRA Indel F1=0.9958. @pichuan Has made [this linked gist](https://gist.github.com/pichuan/eedab4cf2e06fa7ceb2fad0f9b3f8066) that we use for efficient, single machine processing of vg giraffe+DeepVariant. We opted not to include the use of ABRA realignment in that pipeline. With the benchmark numbers available here, I hope this is sufficient information for you to decide whether you would like to include it or not. We continue to look at improvements in the Indel realignment/reassembly process to see if we can further improve in some of the manners that ABRA helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/629
https://github.com/google/deepvariant/issues/629:295,interoperability,standard,standardizing,295,"Hi @JosephLalli . At the time we were putting together the vg-giraffe paper, we noticed that Indel errors would occur because the positioning of read indel events in the CIGAR string were not always left-normalized. ABRA both fixed that issue and also seems to give some additional advantage by standardizing the representation before running the candidate generation in DeepVariant. Subsequent to that finding, both ourselves and the vg team built methods which left-normalize Indel CIGAR events during processing. We found that this reduced almost all, but not quite all, of the Indel performance difference with ABRA processed reads. Because we have built the DeepVariant vg pipeline to include read normalization with the --normalize_reads=true tag, we don't routinely benchmark the two methods against each other. Our last head-to-head benchmark on ABRA on and off on 35x HG003 was no_ABRA Indel F1=0.9953, with ABRA Indel F1=0.9958. @pichuan Has made [this linked gist](https://gist.github.com/pichuan/eedab4cf2e06fa7ceb2fad0f9b3f8066) that we use for efficient, single machine processing of vg giraffe+DeepVariant. We opted not to include the use of ABRA realignment in that pipeline. With the benchmark numbers available here, I hope this is sufficient information for you to decide whether you would like to include it or not. We continue to look at improvements in the Indel realignment/reassembly process to see if we can further improve in some of the manners that ABRA helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/629
https://github.com/google/deepvariant/issues/629:25,performance,time,time,25,"Hi @JosephLalli . At the time we were putting together the vg-giraffe paper, we noticed that Indel errors would occur because the positioning of read indel events in the CIGAR string were not always left-normalized. ABRA both fixed that issue and also seems to give some additional advantage by standardizing the representation before running the candidate generation in DeepVariant. Subsequent to that finding, both ourselves and the vg team built methods which left-normalize Indel CIGAR events during processing. We found that this reduced almost all, but not quite all, of the Indel performance difference with ABRA processed reads. Because we have built the DeepVariant vg pipeline to include read normalization with the --normalize_reads=true tag, we don't routinely benchmark the two methods against each other. Our last head-to-head benchmark on ABRA on and off on 35x HG003 was no_ABRA Indel F1=0.9953, with ABRA Indel F1=0.9958. @pichuan Has made [this linked gist](https://gist.github.com/pichuan/eedab4cf2e06fa7ceb2fad0f9b3f8066) that we use for efficient, single machine processing of vg giraffe+DeepVariant. We opted not to include the use of ABRA realignment in that pipeline. With the benchmark numbers available here, I hope this is sufficient information for you to decide whether you would like to include it or not. We continue to look at improvements in the Indel realignment/reassembly process to see if we can further improve in some of the manners that ABRA helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/629
https://github.com/google/deepvariant/issues/629:99,performance,error,errors,99,"Hi @JosephLalli . At the time we were putting together the vg-giraffe paper, we noticed that Indel errors would occur because the positioning of read indel events in the CIGAR string were not always left-normalized. ABRA both fixed that issue and also seems to give some additional advantage by standardizing the representation before running the candidate generation in DeepVariant. Subsequent to that finding, both ourselves and the vg team built methods which left-normalize Indel CIGAR events during processing. We found that this reduced almost all, but not quite all, of the Indel performance difference with ABRA processed reads. Because we have built the DeepVariant vg pipeline to include read normalization with the --normalize_reads=true tag, we don't routinely benchmark the two methods against each other. Our last head-to-head benchmark on ABRA on and off on 35x HG003 was no_ABRA Indel F1=0.9953, with ABRA Indel F1=0.9958. @pichuan Has made [this linked gist](https://gist.github.com/pichuan/eedab4cf2e06fa7ceb2fad0f9b3f8066) that we use for efficient, single machine processing of vg giraffe+DeepVariant. We opted not to include the use of ABRA realignment in that pipeline. With the benchmark numbers available here, I hope this is sufficient information for you to decide whether you would like to include it or not. We continue to look at improvements in the Indel realignment/reassembly process to see if we can further improve in some of the manners that ABRA helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/629
https://github.com/google/deepvariant/issues/629:587,performance,perform,performance,587,"Hi @JosephLalli . At the time we were putting together the vg-giraffe paper, we noticed that Indel errors would occur because the positioning of read indel events in the CIGAR string were not always left-normalized. ABRA both fixed that issue and also seems to give some additional advantage by standardizing the representation before running the candidate generation in DeepVariant. Subsequent to that finding, both ourselves and the vg team built methods which left-normalize Indel CIGAR events during processing. We found that this reduced almost all, but not quite all, of the Indel performance difference with ABRA processed reads. Because we have built the DeepVariant vg pipeline to include read normalization with the --normalize_reads=true tag, we don't routinely benchmark the two methods against each other. Our last head-to-head benchmark on ABRA on and off on 35x HG003 was no_ABRA Indel F1=0.9953, with ABRA Indel F1=0.9958. @pichuan Has made [this linked gist](https://gist.github.com/pichuan/eedab4cf2e06fa7ceb2fad0f9b3f8066) that we use for efficient, single machine processing of vg giraffe+DeepVariant. We opted not to include the use of ABRA realignment in that pipeline. With the benchmark numbers available here, I hope this is sufficient information for you to decide whether you would like to include it or not. We continue to look at improvements in the Indel realignment/reassembly process to see if we can further improve in some of the manners that ABRA helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/629
https://github.com/google/deepvariant/issues/629:1219,reliability,availab,available,1219,"Hi @JosephLalli . At the time we were putting together the vg-giraffe paper, we noticed that Indel errors would occur because the positioning of read indel events in the CIGAR string were not always left-normalized. ABRA both fixed that issue and also seems to give some additional advantage by standardizing the representation before running the candidate generation in DeepVariant. Subsequent to that finding, both ourselves and the vg team built methods which left-normalize Indel CIGAR events during processing. We found that this reduced almost all, but not quite all, of the Indel performance difference with ABRA processed reads. Because we have built the DeepVariant vg pipeline to include read normalization with the --normalize_reads=true tag, we don't routinely benchmark the two methods against each other. Our last head-to-head benchmark on ABRA on and off on 35x HG003 was no_ABRA Indel F1=0.9953, with ABRA Indel F1=0.9958. @pichuan Has made [this linked gist](https://gist.github.com/pichuan/eedab4cf2e06fa7ceb2fad0f9b3f8066) that we use for efficient, single machine processing of vg giraffe+DeepVariant. We opted not to include the use of ABRA realignment in that pipeline. With the benchmark numbers available here, I hope this is sufficient information for you to decide whether you would like to include it or not. We continue to look at improvements in the Indel realignment/reassembly process to see if we can further improve in some of the manners that ABRA helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/629
https://github.com/google/deepvariant/issues/629:99,safety,error,errors,99,"Hi @JosephLalli . At the time we were putting together the vg-giraffe paper, we noticed that Indel errors would occur because the positioning of read indel events in the CIGAR string were not always left-normalized. ABRA both fixed that issue and also seems to give some additional advantage by standardizing the representation before running the candidate generation in DeepVariant. Subsequent to that finding, both ourselves and the vg team built methods which left-normalize Indel CIGAR events during processing. We found that this reduced almost all, but not quite all, of the Indel performance difference with ABRA processed reads. Because we have built the DeepVariant vg pipeline to include read normalization with the --normalize_reads=true tag, we don't routinely benchmark the two methods against each other. Our last head-to-head benchmark on ABRA on and off on 35x HG003 was no_ABRA Indel F1=0.9953, with ABRA Indel F1=0.9958. @pichuan Has made [this linked gist](https://gist.github.com/pichuan/eedab4cf2e06fa7ceb2fad0f9b3f8066) that we use for efficient, single machine processing of vg giraffe+DeepVariant. We opted not to include the use of ABRA realignment in that pipeline. With the benchmark numbers available here, I hope this is sufficient information for you to decide whether you would like to include it or not. We continue to look at improvements in the Indel realignment/reassembly process to see if we can further improve in some of the manners that ABRA helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/629
https://github.com/google/deepvariant/issues/629:1219,safety,avail,available,1219,"Hi @JosephLalli . At the time we were putting together the vg-giraffe paper, we noticed that Indel errors would occur because the positioning of read indel events in the CIGAR string were not always left-normalized. ABRA both fixed that issue and also seems to give some additional advantage by standardizing the representation before running the candidate generation in DeepVariant. Subsequent to that finding, both ourselves and the vg team built methods which left-normalize Indel CIGAR events during processing. We found that this reduced almost all, but not quite all, of the Indel performance difference with ABRA processed reads. Because we have built the DeepVariant vg pipeline to include read normalization with the --normalize_reads=true tag, we don't routinely benchmark the two methods against each other. Our last head-to-head benchmark on ABRA on and off on 35x HG003 was no_ABRA Indel F1=0.9953, with ABRA Indel F1=0.9958. @pichuan Has made [this linked gist](https://gist.github.com/pichuan/eedab4cf2e06fa7ceb2fad0f9b3f8066) that we use for efficient, single machine processing of vg giraffe+DeepVariant. We opted not to include the use of ABRA realignment in that pipeline. With the benchmark numbers available here, I hope this is sufficient information for you to decide whether you would like to include it or not. We continue to look at improvements in the Indel realignment/reassembly process to see if we can further improve in some of the manners that ABRA helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/629
https://github.com/google/deepvariant/issues/629:438,security,team,team,438,"Hi @JosephLalli . At the time we were putting together the vg-giraffe paper, we noticed that Indel errors would occur because the positioning of read indel events in the CIGAR string were not always left-normalized. ABRA both fixed that issue and also seems to give some additional advantage by standardizing the representation before running the candidate generation in DeepVariant. Subsequent to that finding, both ourselves and the vg team built methods which left-normalize Indel CIGAR events during processing. We found that this reduced almost all, but not quite all, of the Indel performance difference with ABRA processed reads. Because we have built the DeepVariant vg pipeline to include read normalization with the --normalize_reads=true tag, we don't routinely benchmark the two methods against each other. Our last head-to-head benchmark on ABRA on and off on 35x HG003 was no_ABRA Indel F1=0.9953, with ABRA Indel F1=0.9958. @pichuan Has made [this linked gist](https://gist.github.com/pichuan/eedab4cf2e06fa7ceb2fad0f9b3f8066) that we use for efficient, single machine processing of vg giraffe+DeepVariant. We opted not to include the use of ABRA realignment in that pipeline. With the benchmark numbers available here, I hope this is sufficient information for you to decide whether you would like to include it or not. We continue to look at improvements in the Indel realignment/reassembly process to see if we can further improve in some of the manners that ABRA helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/629
https://github.com/google/deepvariant/issues/629:1219,security,availab,available,1219,"Hi @JosephLalli . At the time we were putting together the vg-giraffe paper, we noticed that Indel errors would occur because the positioning of read indel events in the CIGAR string were not always left-normalized. ABRA both fixed that issue and also seems to give some additional advantage by standardizing the representation before running the candidate generation in DeepVariant. Subsequent to that finding, both ourselves and the vg team built methods which left-normalize Indel CIGAR events during processing. We found that this reduced almost all, but not quite all, of the Indel performance difference with ABRA processed reads. Because we have built the DeepVariant vg pipeline to include read normalization with the --normalize_reads=true tag, we don't routinely benchmark the two methods against each other. Our last head-to-head benchmark on ABRA on and off on 35x HG003 was no_ABRA Indel F1=0.9953, with ABRA Indel F1=0.9958. @pichuan Has made [this linked gist](https://gist.github.com/pichuan/eedab4cf2e06fa7ceb2fad0f9b3f8066) that we use for efficient, single machine processing of vg giraffe+DeepVariant. We opted not to include the use of ABRA realignment in that pipeline. With the benchmark numbers available here, I hope this is sufficient information for you to decide whether you would like to include it or not. We continue to look at improvements in the Indel realignment/reassembly process to see if we can further improve in some of the manners that ABRA helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/629
https://github.com/google/deepvariant/issues/629:99,usability,error,errors,99,"Hi @JosephLalli . At the time we were putting together the vg-giraffe paper, we noticed that Indel errors would occur because the positioning of read indel events in the CIGAR string were not always left-normalized. ABRA both fixed that issue and also seems to give some additional advantage by standardizing the representation before running the candidate generation in DeepVariant. Subsequent to that finding, both ourselves and the vg team built methods which left-normalize Indel CIGAR events during processing. We found that this reduced almost all, but not quite all, of the Indel performance difference with ABRA processed reads. Because we have built the DeepVariant vg pipeline to include read normalization with the --normalize_reads=true tag, we don't routinely benchmark the two methods against each other. Our last head-to-head benchmark on ABRA on and off on 35x HG003 was no_ABRA Indel F1=0.9953, with ABRA Indel F1=0.9958. @pichuan Has made [this linked gist](https://gist.github.com/pichuan/eedab4cf2e06fa7ceb2fad0f9b3f8066) that we use for efficient, single machine processing of vg giraffe+DeepVariant. We opted not to include the use of ABRA realignment in that pipeline. With the benchmark numbers available here, I hope this is sufficient information for you to decide whether you would like to include it or not. We continue to look at improvements in the Indel realignment/reassembly process to see if we can further improve in some of the manners that ABRA helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/629
https://github.com/google/deepvariant/issues/629:587,usability,perform,performance,587,"Hi @JosephLalli . At the time we were putting together the vg-giraffe paper, we noticed that Indel errors would occur because the positioning of read indel events in the CIGAR string were not always left-normalized. ABRA both fixed that issue and also seems to give some additional advantage by standardizing the representation before running the candidate generation in DeepVariant. Subsequent to that finding, both ourselves and the vg team built methods which left-normalize Indel CIGAR events during processing. We found that this reduced almost all, but not quite all, of the Indel performance difference with ABRA processed reads. Because we have built the DeepVariant vg pipeline to include read normalization with the --normalize_reads=true tag, we don't routinely benchmark the two methods against each other. Our last head-to-head benchmark on ABRA on and off on 35x HG003 was no_ABRA Indel F1=0.9953, with ABRA Indel F1=0.9958. @pichuan Has made [this linked gist](https://gist.github.com/pichuan/eedab4cf2e06fa7ceb2fad0f9b3f8066) that we use for efficient, single machine processing of vg giraffe+DeepVariant. We opted not to include the use of ABRA realignment in that pipeline. With the benchmark numbers available here, I hope this is sufficient information for you to decide whether you would like to include it or not. We continue to look at improvements in the Indel realignment/reassembly process to see if we can further improve in some of the manners that ABRA helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/629
https://github.com/google/deepvariant/issues/629:1058,usability,efficien,efficient,1058,"Hi @JosephLalli . At the time we were putting together the vg-giraffe paper, we noticed that Indel errors would occur because the positioning of read indel events in the CIGAR string were not always left-normalized. ABRA both fixed that issue and also seems to give some additional advantage by standardizing the representation before running the candidate generation in DeepVariant. Subsequent to that finding, both ourselves and the vg team built methods which left-normalize Indel CIGAR events during processing. We found that this reduced almost all, but not quite all, of the Indel performance difference with ABRA processed reads. Because we have built the DeepVariant vg pipeline to include read normalization with the --normalize_reads=true tag, we don't routinely benchmark the two methods against each other. Our last head-to-head benchmark on ABRA on and off on 35x HG003 was no_ABRA Indel F1=0.9953, with ABRA Indel F1=0.9958. @pichuan Has made [this linked gist](https://gist.github.com/pichuan/eedab4cf2e06fa7ceb2fad0f9b3f8066) that we use for efficient, single machine processing of vg giraffe+DeepVariant. We opted not to include the use of ABRA realignment in that pipeline. With the benchmark numbers available here, I hope this is sufficient information for you to decide whether you would like to include it or not. We continue to look at improvements in the Indel realignment/reassembly process to see if we can further improve in some of the manners that ABRA helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/629
https://github.com/google/deepvariant/issues/629:1482,usability,help,helps,1482,"Hi @JosephLalli . At the time we were putting together the vg-giraffe paper, we noticed that Indel errors would occur because the positioning of read indel events in the CIGAR string were not always left-normalized. ABRA both fixed that issue and also seems to give some additional advantage by standardizing the representation before running the candidate generation in DeepVariant. Subsequent to that finding, both ourselves and the vg team built methods which left-normalize Indel CIGAR events during processing. We found that this reduced almost all, but not quite all, of the Indel performance difference with ABRA processed reads. Because we have built the DeepVariant vg pipeline to include read normalization with the --normalize_reads=true tag, we don't routinely benchmark the two methods against each other. Our last head-to-head benchmark on ABRA on and off on 35x HG003 was no_ABRA Indel F1=0.9953, with ABRA Indel F1=0.9958. @pichuan Has made [this linked gist](https://gist.github.com/pichuan/eedab4cf2e06fa7ceb2fad0f9b3f8066) that we use for efficient, single machine processing of vg giraffe+DeepVariant. We opted not to include the use of ABRA realignment in that pipeline. With the benchmark numbers available here, I hope this is sufficient information for you to decide whether you would like to include it or not. We continue to look at improvements in the Indel realignment/reassembly process to see if we can further improve in some of the manners that ABRA helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/629
https://github.com/google/deepvariant/issues/629:63,usability,close,close,63,"Hi @JosephLalli , let us know if you have more questions. I'll close this issue for now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/629
https://github.com/google/deepvariant/issues/629:31,modifiability,concern,concerns,31,"Yup, you guys addressed all my concerns. Thank you! I think optionally dropping the indel realignment could be very helpful for many labs.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/629
https://github.com/google/deepvariant/issues/629:31,testability,concern,concerns,31,"Yup, you guys addressed all my concerns. Thank you! I think optionally dropping the indel realignment could be very helpful for many labs.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/629
https://github.com/google/deepvariant/issues/629:116,usability,help,helpful,116,"Yup, you guys addressed all my concerns. Thank you! I think optionally dropping the indel realignment could be very helpful for many labs.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/629
https://github.com/google/deepvariant/issues/630:356,energy efficiency,estimat,estimation,356,"Hi @Wenfei-Xian . DeepVariant already sees the strand of the input reads as one of the input. For more information on the inputs to DeepVariant, see: [our blog looking through DeepVariant's eyes](https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/). DeepVariant will incorporate the strand information as a part of its estimation for the confidence in the variant call (e.g. in the QUAL field and the GQ value). The GQ value is the best-calibrated value to use for filtering or evaluation. DeepVariant doesn't annotate the supporting information in the VCF, so if you want to directly access the support for strand, you will need to run some other program to annotate the VCF with the BAM information. For strand, realignment is unlikely to have a noticeable effect, as it will change the coordinates of read positions (and when doing so may alter which reads overlap a variant position), but it will not alter the forward or reverse orientation of a read. Thank you,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/630
https://github.com/google/deepvariant/issues/630:502,integrability,filter,filtering,502,"Hi @Wenfei-Xian . DeepVariant already sees the strand of the input reads as one of the input. For more information on the inputs to DeepVariant, see: [our blog looking through DeepVariant's eyes](https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/). DeepVariant will incorporate the strand information as a part of its estimation for the confidence in the variant call (e.g. in the QUAL field and the GQ value). The GQ value is the best-calibrated value to use for filtering or evaluation. DeepVariant doesn't annotate the supporting information in the VCF, so if you want to directly access the support for strand, you will need to run some other program to annotate the VCF with the BAM information. For strand, realignment is unlikely to have a noticeable effect, as it will change the coordinates of read positions (and when doing so may alter which reads overlap a variant position), but it will not alter the forward or reverse orientation of a read. Thank you,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/630
https://github.com/google/deepvariant/issues/630:826,interoperability,coordinat,coordinates,826,"Hi @Wenfei-Xian . DeepVariant already sees the strand of the input reads as one of the input. For more information on the inputs to DeepVariant, see: [our blog looking through DeepVariant's eyes](https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/). DeepVariant will incorporate the strand information as a part of its estimation for the confidence in the variant call (e.g. in the QUAL field and the GQ value). The GQ value is the best-calibrated value to use for filtering or evaluation. DeepVariant doesn't annotate the supporting information in the VCF, so if you want to directly access the support for strand, you will need to run some other program to annotate the VCF with the BAM information. For strand, realignment is unlikely to have a noticeable effect, as it will change the coordinates of read positions (and when doing so may alter which reads overlap a variant position), but it will not alter the forward or reverse orientation of a read. Thank you,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/630
https://github.com/google/deepvariant/issues/630:539,reliability,doe,doesn,539,"Hi @Wenfei-Xian . DeepVariant already sees the strand of the input reads as one of the input. For more information on the inputs to DeepVariant, see: [our blog looking through DeepVariant's eyes](https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/). DeepVariant will incorporate the strand information as a part of its estimation for the confidence in the variant call (e.g. in the QUAL field and the GQ value). The GQ value is the best-calibrated value to use for filtering or evaluation. DeepVariant doesn't annotate the supporting information in the VCF, so if you want to directly access the support for strand, you will need to run some other program to annotate the VCF with the BAM information. For strand, realignment is unlikely to have a noticeable effect, as it will change the coordinates of read positions (and when doing so may alter which reads overlap a variant position), but it will not alter the forward or reverse orientation of a read. Thank you,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/630
https://github.com/google/deepvariant/issues/630:61,safety,input,input,61,"Hi @Wenfei-Xian . DeepVariant already sees the strand of the input reads as one of the input. For more information on the inputs to DeepVariant, see: [our blog looking through DeepVariant's eyes](https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/). DeepVariant will incorporate the strand information as a part of its estimation for the confidence in the variant call (e.g. in the QUAL field and the GQ value). The GQ value is the best-calibrated value to use for filtering or evaluation. DeepVariant doesn't annotate the supporting information in the VCF, so if you want to directly access the support for strand, you will need to run some other program to annotate the VCF with the BAM information. For strand, realignment is unlikely to have a noticeable effect, as it will change the coordinates of read positions (and when doing so may alter which reads overlap a variant position), but it will not alter the forward or reverse orientation of a read. Thank you,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/630
https://github.com/google/deepvariant/issues/630:87,safety,input,input,87,"Hi @Wenfei-Xian . DeepVariant already sees the strand of the input reads as one of the input. For more information on the inputs to DeepVariant, see: [our blog looking through DeepVariant's eyes](https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/). DeepVariant will incorporate the strand information as a part of its estimation for the confidence in the variant call (e.g. in the QUAL field and the GQ value). The GQ value is the best-calibrated value to use for filtering or evaluation. DeepVariant doesn't annotate the supporting information in the VCF, so if you want to directly access the support for strand, you will need to run some other program to annotate the VCF with the BAM information. For strand, realignment is unlikely to have a noticeable effect, as it will change the coordinates of read positions (and when doing so may alter which reads overlap a variant position), but it will not alter the forward or reverse orientation of a read. Thank you,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/630
https://github.com/google/deepvariant/issues/630:122,safety,input,inputs,122,"Hi @Wenfei-Xian . DeepVariant already sees the strand of the input reads as one of the input. For more information on the inputs to DeepVariant, see: [our blog looking through DeepVariant's eyes](https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/). DeepVariant will incorporate the strand information as a part of its estimation for the confidence in the variant call (e.g. in the QUAL field and the GQ value). The GQ value is the best-calibrated value to use for filtering or evaluation. DeepVariant doesn't annotate the supporting information in the VCF, so if you want to directly access the support for strand, you will need to run some other program to annotate the VCF with the BAM information. For strand, realignment is unlikely to have a noticeable effect, as it will change the coordinates of read positions (and when doing so may alter which reads overlap a variant position), but it will not alter the forward or reverse orientation of a read. Thank you,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/630
https://github.com/google/deepvariant/issues/630:622,security,access,access,622,"Hi @Wenfei-Xian . DeepVariant already sees the strand of the input reads as one of the input. For more information on the inputs to DeepVariant, see: [our blog looking through DeepVariant's eyes](https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/). DeepVariant will incorporate the strand information as a part of its estimation for the confidence in the variant call (e.g. in the QUAL field and the GQ value). The GQ value is the best-calibrated value to use for filtering or evaluation. DeepVariant doesn't annotate the supporting information in the VCF, so if you want to directly access the support for strand, you will need to run some other program to annotate the VCF with the BAM information. For strand, realignment is unlikely to have a noticeable effect, as it will change the coordinates of read positions (and when doing so may alter which reads overlap a variant position), but it will not alter the forward or reverse orientation of a read. Thank you,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/630
https://github.com/google/deepvariant/issues/630:61,usability,input,input,61,"Hi @Wenfei-Xian . DeepVariant already sees the strand of the input reads as one of the input. For more information on the inputs to DeepVariant, see: [our blog looking through DeepVariant's eyes](https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/). DeepVariant will incorporate the strand information as a part of its estimation for the confidence in the variant call (e.g. in the QUAL field and the GQ value). The GQ value is the best-calibrated value to use for filtering or evaluation. DeepVariant doesn't annotate the supporting information in the VCF, so if you want to directly access the support for strand, you will need to run some other program to annotate the VCF with the BAM information. For strand, realignment is unlikely to have a noticeable effect, as it will change the coordinates of read positions (and when doing so may alter which reads overlap a variant position), but it will not alter the forward or reverse orientation of a read. Thank you,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/630
https://github.com/google/deepvariant/issues/630:87,usability,input,input,87,"Hi @Wenfei-Xian . DeepVariant already sees the strand of the input reads as one of the input. For more information on the inputs to DeepVariant, see: [our blog looking through DeepVariant's eyes](https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/). DeepVariant will incorporate the strand information as a part of its estimation for the confidence in the variant call (e.g. in the QUAL field and the GQ value). The GQ value is the best-calibrated value to use for filtering or evaluation. DeepVariant doesn't annotate the supporting information in the VCF, so if you want to directly access the support for strand, you will need to run some other program to annotate the VCF with the BAM information. For strand, realignment is unlikely to have a noticeable effect, as it will change the coordinates of read positions (and when doing so may alter which reads overlap a variant position), but it will not alter the forward or reverse orientation of a read. Thank you,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/630
https://github.com/google/deepvariant/issues/630:122,usability,input,inputs,122,"Hi @Wenfei-Xian . DeepVariant already sees the strand of the input reads as one of the input. For more information on the inputs to DeepVariant, see: [our blog looking through DeepVariant's eyes](https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/). DeepVariant will incorporate the strand information as a part of its estimation for the confidence in the variant call (e.g. in the QUAL field and the GQ value). The GQ value is the best-calibrated value to use for filtering or evaluation. DeepVariant doesn't annotate the supporting information in the VCF, so if you want to directly access the support for strand, you will need to run some other program to annotate the VCF with the BAM information. For strand, realignment is unlikely to have a noticeable effect, as it will change the coordinates of read positions (and when doing so may alter which reads overlap a variant position), but it will not alter the forward or reverse orientation of a read. Thank you,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/630
https://github.com/google/deepvariant/issues/630:560,usability,support,supporting,560,"Hi @Wenfei-Xian . DeepVariant already sees the strand of the input reads as one of the input. For more information on the inputs to DeepVariant, see: [our blog looking through DeepVariant's eyes](https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/). DeepVariant will incorporate the strand information as a part of its estimation for the confidence in the variant call (e.g. in the QUAL field and the GQ value). The GQ value is the best-calibrated value to use for filtering or evaluation. DeepVariant doesn't annotate the supporting information in the VCF, so if you want to directly access the support for strand, you will need to run some other program to annotate the VCF with the BAM information. For strand, realignment is unlikely to have a noticeable effect, as it will change the coordinates of read positions (and when doing so may alter which reads overlap a variant position), but it will not alter the forward or reverse orientation of a read. Thank you,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/630
https://github.com/google/deepvariant/issues/630:633,usability,support,support,633,"Hi @Wenfei-Xian . DeepVariant already sees the strand of the input reads as one of the input. For more information on the inputs to DeepVariant, see: [our blog looking through DeepVariant's eyes](https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/). DeepVariant will incorporate the strand information as a part of its estimation for the confidence in the variant call (e.g. in the QUAL field and the GQ value). The GQ value is the best-calibrated value to use for filtering or evaluation. DeepVariant doesn't annotate the supporting information in the VCF, so if you want to directly access the support for strand, you will need to run some other program to annotate the VCF with the BAM information. For strand, realignment is unlikely to have a noticeable effect, as it will change the coordinates of read positions (and when doing so may alter which reads overlap a variant position), but it will not alter the forward or reverse orientation of a read. Thank you,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/630
https://github.com/google/deepvariant/issues/631:44,availability,consist,consistent,44,"Hi @crazysummerW . The message here is most consistent with reads that do not have quality values. Inspection of the GIAB file indicates that this is the case here - the reads have bases and not quality values. These GIAB files appear to be deposited in 2018. It seems likely to me that they are PacBio continuous long read (CLR) sequencing instead of Circular Consensus Sequencing (CCS). PacBio CLR sequencing has a higher base error rate and DeepVariant is not designed to process this older type of sequence data. . Mechanically, the reason this input file fails is that it lacks quality values, which DeepVariant expects. CLR sequencing does not generate quality values. For test data, I recommend using a more recent set of sequencing which use the CCS prep, e.g. the contents of: . https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/data/AshkenazimTrio/HG003_NA24149_father/PacBio_CCS_15kb_20kb_chemistry2/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/631
https://github.com/google/deepvariant/issues/631:429,availability,error,error,429,"Hi @crazysummerW . The message here is most consistent with reads that do not have quality values. Inspection of the GIAB file indicates that this is the case here - the reads have bases and not quality values. These GIAB files appear to be deposited in 2018. It seems likely to me that they are PacBio continuous long read (CLR) sequencing instead of Circular Consensus Sequencing (CCS). PacBio CLR sequencing has a higher base error rate and DeepVariant is not designed to process this older type of sequence data. . Mechanically, the reason this input file fails is that it lacks quality values, which DeepVariant expects. CLR sequencing does not generate quality values. For test data, I recommend using a more recent set of sequencing which use the CCS prep, e.g. the contents of: . https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/data/AshkenazimTrio/HG003_NA24149_father/PacBio_CCS_15kb_20kb_chemistry2/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/631
https://github.com/google/deepvariant/issues/631:303,deployability,continu,continuous,303,"Hi @crazysummerW . The message here is most consistent with reads that do not have quality values. Inspection of the GIAB file indicates that this is the case here - the reads have bases and not quality values. These GIAB files appear to be deposited in 2018. It seems likely to me that they are PacBio continuous long read (CLR) sequencing instead of Circular Consensus Sequencing (CCS). PacBio CLR sequencing has a higher base error rate and DeepVariant is not designed to process this older type of sequence data. . Mechanically, the reason this input file fails is that it lacks quality values, which DeepVariant expects. CLR sequencing does not generate quality values. For test data, I recommend using a more recent set of sequencing which use the CCS prep, e.g. the contents of: . https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/data/AshkenazimTrio/HG003_NA24149_father/PacBio_CCS_15kb_20kb_chemistry2/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/631
https://github.com/google/deepvariant/issues/631:560,deployability,fail,fails,560,"Hi @crazysummerW . The message here is most consistent with reads that do not have quality values. Inspection of the GIAB file indicates that this is the case here - the reads have bases and not quality values. These GIAB files appear to be deposited in 2018. It seems likely to me that they are PacBio continuous long read (CLR) sequencing instead of Circular Consensus Sequencing (CCS). PacBio CLR sequencing has a higher base error rate and DeepVariant is not designed to process this older type of sequence data. . Mechanically, the reason this input file fails is that it lacks quality values, which DeepVariant expects. CLR sequencing does not generate quality values. For test data, I recommend using a more recent set of sequencing which use the CCS prep, e.g. the contents of: . https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/data/AshkenazimTrio/HG003_NA24149_father/PacBio_CCS_15kb_20kb_chemistry2/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/631
https://github.com/google/deepvariant/issues/631:23,integrability,messag,message,23,"Hi @crazysummerW . The message here is most consistent with reads that do not have quality values. Inspection of the GIAB file indicates that this is the case here - the reads have bases and not quality values. These GIAB files appear to be deposited in 2018. It seems likely to me that they are PacBio continuous long read (CLR) sequencing instead of Circular Consensus Sequencing (CCS). PacBio CLR sequencing has a higher base error rate and DeepVariant is not designed to process this older type of sequence data. . Mechanically, the reason this input file fails is that it lacks quality values, which DeepVariant expects. CLR sequencing does not generate quality values. For test data, I recommend using a more recent set of sequencing which use the CCS prep, e.g. the contents of: . https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/data/AshkenazimTrio/HG003_NA24149_father/PacBio_CCS_15kb_20kb_chemistry2/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/631
https://github.com/google/deepvariant/issues/631:23,interoperability,messag,message,23,"Hi @crazysummerW . The message here is most consistent with reads that do not have quality values. Inspection of the GIAB file indicates that this is the case here - the reads have bases and not quality values. These GIAB files appear to be deposited in 2018. It seems likely to me that they are PacBio continuous long read (CLR) sequencing instead of Circular Consensus Sequencing (CCS). PacBio CLR sequencing has a higher base error rate and DeepVariant is not designed to process this older type of sequence data. . Mechanically, the reason this input file fails is that it lacks quality values, which DeepVariant expects. CLR sequencing does not generate quality values. For test data, I recommend using a more recent set of sequencing which use the CCS prep, e.g. the contents of: . https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/data/AshkenazimTrio/HG003_NA24149_father/PacBio_CCS_15kb_20kb_chemistry2/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/631
https://github.com/google/deepvariant/issues/631:296,modifiability,Pac,PacBio,296,"Hi @crazysummerW . The message here is most consistent with reads that do not have quality values. Inspection of the GIAB file indicates that this is the case here - the reads have bases and not quality values. These GIAB files appear to be deposited in 2018. It seems likely to me that they are PacBio continuous long read (CLR) sequencing instead of Circular Consensus Sequencing (CCS). PacBio CLR sequencing has a higher base error rate and DeepVariant is not designed to process this older type of sequence data. . Mechanically, the reason this input file fails is that it lacks quality values, which DeepVariant expects. CLR sequencing does not generate quality values. For test data, I recommend using a more recent set of sequencing which use the CCS prep, e.g. the contents of: . https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/data/AshkenazimTrio/HG003_NA24149_father/PacBio_CCS_15kb_20kb_chemistry2/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/631
https://github.com/google/deepvariant/issues/631:389,modifiability,Pac,PacBio,389,"Hi @crazysummerW . The message here is most consistent with reads that do not have quality values. Inspection of the GIAB file indicates that this is the case here - the reads have bases and not quality values. These GIAB files appear to be deposited in 2018. It seems likely to me that they are PacBio continuous long read (CLR) sequencing instead of Circular Consensus Sequencing (CCS). PacBio CLR sequencing has a higher base error rate and DeepVariant is not designed to process this older type of sequence data. . Mechanically, the reason this input file fails is that it lacks quality values, which DeepVariant expects. CLR sequencing does not generate quality values. For test data, I recommend using a more recent set of sequencing which use the CCS prep, e.g. the contents of: . https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/data/AshkenazimTrio/HG003_NA24149_father/PacBio_CCS_15kb_20kb_chemistry2/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/631
https://github.com/google/deepvariant/issues/631:429,performance,error,error,429,"Hi @crazysummerW . The message here is most consistent with reads that do not have quality values. Inspection of the GIAB file indicates that this is the case here - the reads have bases and not quality values. These GIAB files appear to be deposited in 2018. It seems likely to me that they are PacBio continuous long read (CLR) sequencing instead of Circular Consensus Sequencing (CCS). PacBio CLR sequencing has a higher base error rate and DeepVariant is not designed to process this older type of sequence data. . Mechanically, the reason this input file fails is that it lacks quality values, which DeepVariant expects. CLR sequencing does not generate quality values. For test data, I recommend using a more recent set of sequencing which use the CCS prep, e.g. the contents of: . https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/data/AshkenazimTrio/HG003_NA24149_father/PacBio_CCS_15kb_20kb_chemistry2/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/631
https://github.com/google/deepvariant/issues/631:773,performance,content,contents,773,"Hi @crazysummerW . The message here is most consistent with reads that do not have quality values. Inspection of the GIAB file indicates that this is the case here - the reads have bases and not quality values. These GIAB files appear to be deposited in 2018. It seems likely to me that they are PacBio continuous long read (CLR) sequencing instead of Circular Consensus Sequencing (CCS). PacBio CLR sequencing has a higher base error rate and DeepVariant is not designed to process this older type of sequence data. . Mechanically, the reason this input file fails is that it lacks quality values, which DeepVariant expects. CLR sequencing does not generate quality values. For test data, I recommend using a more recent set of sequencing which use the CCS prep, e.g. the contents of: . https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/data/AshkenazimTrio/HG003_NA24149_father/PacBio_CCS_15kb_20kb_chemistry2/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/631
https://github.com/google/deepvariant/issues/631:560,reliability,fail,fails,560,"Hi @crazysummerW . The message here is most consistent with reads that do not have quality values. Inspection of the GIAB file indicates that this is the case here - the reads have bases and not quality values. These GIAB files appear to be deposited in 2018. It seems likely to me that they are PacBio continuous long read (CLR) sequencing instead of Circular Consensus Sequencing (CCS). PacBio CLR sequencing has a higher base error rate and DeepVariant is not designed to process this older type of sequence data. . Mechanically, the reason this input file fails is that it lacks quality values, which DeepVariant expects. CLR sequencing does not generate quality values. For test data, I recommend using a more recent set of sequencing which use the CCS prep, e.g. the contents of: . https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/data/AshkenazimTrio/HG003_NA24149_father/PacBio_CCS_15kb_20kb_chemistry2/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/631
https://github.com/google/deepvariant/issues/631:641,reliability,doe,does,641,"Hi @crazysummerW . The message here is most consistent with reads that do not have quality values. Inspection of the GIAB file indicates that this is the case here - the reads have bases and not quality values. These GIAB files appear to be deposited in 2018. It seems likely to me that they are PacBio continuous long read (CLR) sequencing instead of Circular Consensus Sequencing (CCS). PacBio CLR sequencing has a higher base error rate and DeepVariant is not designed to process this older type of sequence data. . Mechanically, the reason this input file fails is that it lacks quality values, which DeepVariant expects. CLR sequencing does not generate quality values. For test data, I recommend using a more recent set of sequencing which use the CCS prep, e.g. the contents of: . https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/data/AshkenazimTrio/HG003_NA24149_father/PacBio_CCS_15kb_20kb_chemistry2/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/631
https://github.com/google/deepvariant/issues/631:429,safety,error,error,429,"Hi @crazysummerW . The message here is most consistent with reads that do not have quality values. Inspection of the GIAB file indicates that this is the case here - the reads have bases and not quality values. These GIAB files appear to be deposited in 2018. It seems likely to me that they are PacBio continuous long read (CLR) sequencing instead of Circular Consensus Sequencing (CCS). PacBio CLR sequencing has a higher base error rate and DeepVariant is not designed to process this older type of sequence data. . Mechanically, the reason this input file fails is that it lacks quality values, which DeepVariant expects. CLR sequencing does not generate quality values. For test data, I recommend using a more recent set of sequencing which use the CCS prep, e.g. the contents of: . https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/data/AshkenazimTrio/HG003_NA24149_father/PacBio_CCS_15kb_20kb_chemistry2/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/631
https://github.com/google/deepvariant/issues/631:549,safety,input,input,549,"Hi @crazysummerW . The message here is most consistent with reads that do not have quality values. Inspection of the GIAB file indicates that this is the case here - the reads have bases and not quality values. These GIAB files appear to be deposited in 2018. It seems likely to me that they are PacBio continuous long read (CLR) sequencing instead of Circular Consensus Sequencing (CCS). PacBio CLR sequencing has a higher base error rate and DeepVariant is not designed to process this older type of sequence data. . Mechanically, the reason this input file fails is that it lacks quality values, which DeepVariant expects. CLR sequencing does not generate quality values. For test data, I recommend using a more recent set of sequencing which use the CCS prep, e.g. the contents of: . https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/data/AshkenazimTrio/HG003_NA24149_father/PacBio_CCS_15kb_20kb_chemistry2/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/631
https://github.com/google/deepvariant/issues/631:679,safety,test,test,679,"Hi @crazysummerW . The message here is most consistent with reads that do not have quality values. Inspection of the GIAB file indicates that this is the case here - the reads have bases and not quality values. These GIAB files appear to be deposited in 2018. It seems likely to me that they are PacBio continuous long read (CLR) sequencing instead of Circular Consensus Sequencing (CCS). PacBio CLR sequencing has a higher base error rate and DeepVariant is not designed to process this older type of sequence data. . Mechanically, the reason this input file fails is that it lacks quality values, which DeepVariant expects. CLR sequencing does not generate quality values. For test data, I recommend using a more recent set of sequencing which use the CCS prep, e.g. the contents of: . https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/data/AshkenazimTrio/HG003_NA24149_father/PacBio_CCS_15kb_20kb_chemistry2/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/631
https://github.com/google/deepvariant/issues/631:679,testability,test,test,679,"Hi @crazysummerW . The message here is most consistent with reads that do not have quality values. Inspection of the GIAB file indicates that this is the case here - the reads have bases and not quality values. These GIAB files appear to be deposited in 2018. It seems likely to me that they are PacBio continuous long read (CLR) sequencing instead of Circular Consensus Sequencing (CCS). PacBio CLR sequencing has a higher base error rate and DeepVariant is not designed to process this older type of sequence data. . Mechanically, the reason this input file fails is that it lacks quality values, which DeepVariant expects. CLR sequencing does not generate quality values. For test data, I recommend using a more recent set of sequencing which use the CCS prep, e.g. the contents of: . https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/data/AshkenazimTrio/HG003_NA24149_father/PacBio_CCS_15kb_20kb_chemistry2/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/631
https://github.com/google/deepvariant/issues/631:800,testability,trace,trace,800,"Hi @crazysummerW . The message here is most consistent with reads that do not have quality values. Inspection of the GIAB file indicates that this is the case here - the reads have bases and not quality values. These GIAB files appear to be deposited in 2018. It seems likely to me that they are PacBio continuous long read (CLR) sequencing instead of Circular Consensus Sequencing (CCS). PacBio CLR sequencing has a higher base error rate and DeepVariant is not designed to process this older type of sequence data. . Mechanically, the reason this input file fails is that it lacks quality values, which DeepVariant expects. CLR sequencing does not generate quality values. For test data, I recommend using a more recent set of sequencing which use the CCS prep, e.g. the contents of: . https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/data/AshkenazimTrio/HG003_NA24149_father/PacBio_CCS_15kb_20kb_chemistry2/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/631
https://github.com/google/deepvariant/issues/631:44,usability,consist,consistent,44,"Hi @crazysummerW . The message here is most consistent with reads that do not have quality values. Inspection of the GIAB file indicates that this is the case here - the reads have bases and not quality values. These GIAB files appear to be deposited in 2018. It seems likely to me that they are PacBio continuous long read (CLR) sequencing instead of Circular Consensus Sequencing (CCS). PacBio CLR sequencing has a higher base error rate and DeepVariant is not designed to process this older type of sequence data. . Mechanically, the reason this input file fails is that it lacks quality values, which DeepVariant expects. CLR sequencing does not generate quality values. For test data, I recommend using a more recent set of sequencing which use the CCS prep, e.g. the contents of: . https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/data/AshkenazimTrio/HG003_NA24149_father/PacBio_CCS_15kb_20kb_chemistry2/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/631
https://github.com/google/deepvariant/issues/631:127,usability,indicat,indicates,127,"Hi @crazysummerW . The message here is most consistent with reads that do not have quality values. Inspection of the GIAB file indicates that this is the case here - the reads have bases and not quality values. These GIAB files appear to be deposited in 2018. It seems likely to me that they are PacBio continuous long read (CLR) sequencing instead of Circular Consensus Sequencing (CCS). PacBio CLR sequencing has a higher base error rate and DeepVariant is not designed to process this older type of sequence data. . Mechanically, the reason this input file fails is that it lacks quality values, which DeepVariant expects. CLR sequencing does not generate quality values. For test data, I recommend using a more recent set of sequencing which use the CCS prep, e.g. the contents of: . https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/data/AshkenazimTrio/HG003_NA24149_father/PacBio_CCS_15kb_20kb_chemistry2/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/631
https://github.com/google/deepvariant/issues/631:429,usability,error,error,429,"Hi @crazysummerW . The message here is most consistent with reads that do not have quality values. Inspection of the GIAB file indicates that this is the case here - the reads have bases and not quality values. These GIAB files appear to be deposited in 2018. It seems likely to me that they are PacBio continuous long read (CLR) sequencing instead of Circular Consensus Sequencing (CCS). PacBio CLR sequencing has a higher base error rate and DeepVariant is not designed to process this older type of sequence data. . Mechanically, the reason this input file fails is that it lacks quality values, which DeepVariant expects. CLR sequencing does not generate quality values. For test data, I recommend using a more recent set of sequencing which use the CCS prep, e.g. the contents of: . https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/data/AshkenazimTrio/HG003_NA24149_father/PacBio_CCS_15kb_20kb_chemistry2/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/631
https://github.com/google/deepvariant/issues/631:549,usability,input,input,549,"Hi @crazysummerW . The message here is most consistent with reads that do not have quality values. Inspection of the GIAB file indicates that this is the case here - the reads have bases and not quality values. These GIAB files appear to be deposited in 2018. It seems likely to me that they are PacBio continuous long read (CLR) sequencing instead of Circular Consensus Sequencing (CCS). PacBio CLR sequencing has a higher base error rate and DeepVariant is not designed to process this older type of sequence data. . Mechanically, the reason this input file fails is that it lacks quality values, which DeepVariant expects. CLR sequencing does not generate quality values. For test data, I recommend using a more recent set of sequencing which use the CCS prep, e.g. the contents of: . https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/data/AshkenazimTrio/HG003_NA24149_father/PacBio_CCS_15kb_20kb_chemistry2/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/631
https://github.com/google/deepvariant/issues/632:120,performance,content,content,120,"Hi @ivanwilliammd,. realtimegenomics - is a third party tool https://github.com/RealTimeGenomics/rtg-tools. What is the content of your ${PWD}/reference directory? .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/632
https://github.com/google/deepvariant/issues/632:56,usability,tool,tool,56,"Hi @ivanwilliammd,. realtimegenomics - is a third party tool https://github.com/RealTimeGenomics/rtg-tools. What is the content of your ${PWD}/reference directory? .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/632
https://github.com/google/deepvariant/issues/632:101,usability,tool,tools,101,"Hi @ivanwilliammd,. realtimegenomics - is a third party tool https://github.com/RealTimeGenomics/rtg-tools. What is the content of your ${PWD}/reference directory? .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/632
https://github.com/google/deepvariant/issues/632:953,availability,error,error,953,"Update : just now the ```rtg-tools format``` miraculously work using the following command. ```. docker run \. -v ""${PWD}/input"":""/input"" \. -v ""${PWD}/reference"":""/reference"" \. realtimegenomics/rtg-tools format \. -o /reference/GRCh38_no_alt_analysis_set.sdf ""/reference/GRCh38_no_alt_analysis_set.fasta"". ```. And this is the result . ```. Formatting FASTA data. Processing ""/reference/GRCh38_no_alt_analysis_set.fasta"". Detected: 'Human GRCh38 with UCSC naming', installing reference.txt. Input Data. Files : GRCh38_no_alt_analysis_set.fasta. Format : FASTA. Type : DNA. Number of sequences: 195. Total residues : 3099922541. Minimum length : 970. Mean length : 15897038. Maximum length : 248956422. Output Data. SDF-ID : 809c9a82-d8d5-477a-865b-772d28741815. Number of sequences: 195. Total residues : 3099922541. Minimum length : 970. Mean length : 15897038. Maximum length : 248956422. ```. However this ```rtg-tools mendelian``` still result in error of ```Error: An IO problem occurred: ""Not in GZIP format""``` when running the following command below. ```. docker run \. -v ""${PWD}/input"":""/input"" \. -v ""${PWD}/reference"":""/reference"" \. -v ""${PWD}/output"":""/output"" \. realtimegenomics/rtg-tools mendelian \. -i ""/output/HG002_trio_merged.vcf.gz"" \. -o ""/output/HG002_trio_annotated.output.vcf.gz"" \. --pedigree=/reference/trio.ped \. -t /reference/GRCh38_no_alt_analysis_set.sdf \. | tee output/deepvariant.input_rtg_output.txt. ```. Have tried looking the wrong data and it seems the following GLNexus VCF Merge command is giving corrupted ```HG002_trio_merged.vcf.gz```. ```. docker run \. -v ""${PWD}/output"":""/output"" \. quay.io/mlin/glnexus:v1.2.7 \. /usr/local/bin/glnexus_cli \. --config DeepVariant_unfiltered \. /output/HG002.g.vcf.gz \. /output/HG003.g.vcf.gz \. /output/HG004.g.vcf.gz \. | docker run -i google/deepvariant:deeptrio-""${BIN_VERSION}-gpu"" \. bcftools view - \. | docker run -i google/deepvariant:deeptrio-""${BIN_VERSION}-gpu"" \. bgzip -c > output/HG002_trio_merge",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/632
https://github.com/google/deepvariant/issues/632:965,availability,Error,Error,965,"Update : just now the ```rtg-tools format``` miraculously work using the following command. ```. docker run \. -v ""${PWD}/input"":""/input"" \. -v ""${PWD}/reference"":""/reference"" \. realtimegenomics/rtg-tools format \. -o /reference/GRCh38_no_alt_analysis_set.sdf ""/reference/GRCh38_no_alt_analysis_set.fasta"". ```. And this is the result . ```. Formatting FASTA data. Processing ""/reference/GRCh38_no_alt_analysis_set.fasta"". Detected: 'Human GRCh38 with UCSC naming', installing reference.txt. Input Data. Files : GRCh38_no_alt_analysis_set.fasta. Format : FASTA. Type : DNA. Number of sequences: 195. Total residues : 3099922541. Minimum length : 970. Mean length : 15897038. Maximum length : 248956422. Output Data. SDF-ID : 809c9a82-d8d5-477a-865b-772d28741815. Number of sequences: 195. Total residues : 3099922541. Minimum length : 970. Mean length : 15897038. Maximum length : 248956422. ```. However this ```rtg-tools mendelian``` still result in error of ```Error: An IO problem occurred: ""Not in GZIP format""``` when running the following command below. ```. docker run \. -v ""${PWD}/input"":""/input"" \. -v ""${PWD}/reference"":""/reference"" \. -v ""${PWD}/output"":""/output"" \. realtimegenomics/rtg-tools mendelian \. -i ""/output/HG002_trio_merged.vcf.gz"" \. -o ""/output/HG002_trio_annotated.output.vcf.gz"" \. --pedigree=/reference/trio.ped \. -t /reference/GRCh38_no_alt_analysis_set.sdf \. | tee output/deepvariant.input_rtg_output.txt. ```. Have tried looking the wrong data and it seems the following GLNexus VCF Merge command is giving corrupted ```HG002_trio_merged.vcf.gz```. ```. docker run \. -v ""${PWD}/output"":""/output"" \. quay.io/mlin/glnexus:v1.2.7 \. /usr/local/bin/glnexus_cli \. --config DeepVariant_unfiltered \. /output/HG002.g.vcf.gz \. /output/HG003.g.vcf.gz \. /output/HG004.g.vcf.gz \. | docker run -i google/deepvariant:deeptrio-""${BIN_VERSION}-gpu"" \. bcftools view - \. | docker run -i google/deepvariant:deeptrio-""${BIN_VERSION}-gpu"" \. bgzip -c > output/HG002_trio_merge",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/632
https://github.com/google/deepvariant/issues/632:0,deployability,Updat,Update,0,"Update : just now the ```rtg-tools format``` miraculously work using the following command. ```. docker run \. -v ""${PWD}/input"":""/input"" \. -v ""${PWD}/reference"":""/reference"" \. realtimegenomics/rtg-tools format \. -o /reference/GRCh38_no_alt_analysis_set.sdf ""/reference/GRCh38_no_alt_analysis_set.fasta"". ```. And this is the result . ```. Formatting FASTA data. Processing ""/reference/GRCh38_no_alt_analysis_set.fasta"". Detected: 'Human GRCh38 with UCSC naming', installing reference.txt. Input Data. Files : GRCh38_no_alt_analysis_set.fasta. Format : FASTA. Type : DNA. Number of sequences: 195. Total residues : 3099922541. Minimum length : 970. Mean length : 15897038. Maximum length : 248956422. Output Data. SDF-ID : 809c9a82-d8d5-477a-865b-772d28741815. Number of sequences: 195. Total residues : 3099922541. Minimum length : 970. Mean length : 15897038. Maximum length : 248956422. ```. However this ```rtg-tools mendelian``` still result in error of ```Error: An IO problem occurred: ""Not in GZIP format""``` when running the following command below. ```. docker run \. -v ""${PWD}/input"":""/input"" \. -v ""${PWD}/reference"":""/reference"" \. -v ""${PWD}/output"":""/output"" \. realtimegenomics/rtg-tools mendelian \. -i ""/output/HG002_trio_merged.vcf.gz"" \. -o ""/output/HG002_trio_annotated.output.vcf.gz"" \. --pedigree=/reference/trio.ped \. -t /reference/GRCh38_no_alt_analysis_set.sdf \. | tee output/deepvariant.input_rtg_output.txt. ```. Have tried looking the wrong data and it seems the following GLNexus VCF Merge command is giving corrupted ```HG002_trio_merged.vcf.gz```. ```. docker run \. -v ""${PWD}/output"":""/output"" \. quay.io/mlin/glnexus:v1.2.7 \. /usr/local/bin/glnexus_cli \. --config DeepVariant_unfiltered \. /output/HG002.g.vcf.gz \. /output/HG003.g.vcf.gz \. /output/HG004.g.vcf.gz \. | docker run -i google/deepvariant:deeptrio-""${BIN_VERSION}-gpu"" \. bcftools view - \. | docker run -i google/deepvariant:deeptrio-""${BIN_VERSION}-gpu"" \. bgzip -c > output/HG002_trio_merge",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/632
https://github.com/google/deepvariant/issues/632:467,deployability,instal,installing,467,"Update : just now the ```rtg-tools format``` miraculously work using the following command. ```. docker run \. -v ""${PWD}/input"":""/input"" \. -v ""${PWD}/reference"":""/reference"" \. realtimegenomics/rtg-tools format \. -o /reference/GRCh38_no_alt_analysis_set.sdf ""/reference/GRCh38_no_alt_analysis_set.fasta"". ```. And this is the result . ```. Formatting FASTA data. Processing ""/reference/GRCh38_no_alt_analysis_set.fasta"". Detected: 'Human GRCh38 with UCSC naming', installing reference.txt. Input Data. Files : GRCh38_no_alt_analysis_set.fasta. Format : FASTA. Type : DNA. Number of sequences: 195. Total residues : 3099922541. Minimum length : 970. Mean length : 15897038. Maximum length : 248956422. Output Data. SDF-ID : 809c9a82-d8d5-477a-865b-772d28741815. Number of sequences: 195. Total residues : 3099922541. Minimum length : 970. Mean length : 15897038. Maximum length : 248956422. ```. However this ```rtg-tools mendelian``` still result in error of ```Error: An IO problem occurred: ""Not in GZIP format""``` when running the following command below. ```. docker run \. -v ""${PWD}/input"":""/input"" \. -v ""${PWD}/reference"":""/reference"" \. -v ""${PWD}/output"":""/output"" \. realtimegenomics/rtg-tools mendelian \. -i ""/output/HG002_trio_merged.vcf.gz"" \. -o ""/output/HG002_trio_annotated.output.vcf.gz"" \. --pedigree=/reference/trio.ped \. -t /reference/GRCh38_no_alt_analysis_set.sdf \. | tee output/deepvariant.input_rtg_output.txt. ```. Have tried looking the wrong data and it seems the following GLNexus VCF Merge command is giving corrupted ```HG002_trio_merged.vcf.gz```. ```. docker run \. -v ""${PWD}/output"":""/output"" \. quay.io/mlin/glnexus:v1.2.7 \. /usr/local/bin/glnexus_cli \. --config DeepVariant_unfiltered \. /output/HG002.g.vcf.gz \. /output/HG003.g.vcf.gz \. /output/HG004.g.vcf.gz \. | docker run -i google/deepvariant:deeptrio-""${BIN_VERSION}-gpu"" \. bcftools view - \. | docker run -i google/deepvariant:deeptrio-""${BIN_VERSION}-gpu"" \. bgzip -c > output/HG002_trio_merge",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/632
https://github.com/google/deepvariant/issues/632:1871,energy efficiency,gpu,gpu,1871," now the ```rtg-tools format``` miraculously work using the following command. ```. docker run \. -v ""${PWD}/input"":""/input"" \. -v ""${PWD}/reference"":""/reference"" \. realtimegenomics/rtg-tools format \. -o /reference/GRCh38_no_alt_analysis_set.sdf ""/reference/GRCh38_no_alt_analysis_set.fasta"". ```. And this is the result . ```. Formatting FASTA data. Processing ""/reference/GRCh38_no_alt_analysis_set.fasta"". Detected: 'Human GRCh38 with UCSC naming', installing reference.txt. Input Data. Files : GRCh38_no_alt_analysis_set.fasta. Format : FASTA. Type : DNA. Number of sequences: 195. Total residues : 3099922541. Minimum length : 970. Mean length : 15897038. Maximum length : 248956422. Output Data. SDF-ID : 809c9a82-d8d5-477a-865b-772d28741815. Number of sequences: 195. Total residues : 3099922541. Minimum length : 970. Mean length : 15897038. Maximum length : 248956422. ```. However this ```rtg-tools mendelian``` still result in error of ```Error: An IO problem occurred: ""Not in GZIP format""``` when running the following command below. ```. docker run \. -v ""${PWD}/input"":""/input"" \. -v ""${PWD}/reference"":""/reference"" \. -v ""${PWD}/output"":""/output"" \. realtimegenomics/rtg-tools mendelian \. -i ""/output/HG002_trio_merged.vcf.gz"" \. -o ""/output/HG002_trio_annotated.output.vcf.gz"" \. --pedigree=/reference/trio.ped \. -t /reference/GRCh38_no_alt_analysis_set.sdf \. | tee output/deepvariant.input_rtg_output.txt. ```. Have tried looking the wrong data and it seems the following GLNexus VCF Merge command is giving corrupted ```HG002_trio_merged.vcf.gz```. ```. docker run \. -v ""${PWD}/output"":""/output"" \. quay.io/mlin/glnexus:v1.2.7 \. /usr/local/bin/glnexus_cli \. --config DeepVariant_unfiltered \. /output/HG002.g.vcf.gz \. /output/HG003.g.vcf.gz \. /output/HG004.g.vcf.gz \. | docker run -i google/deepvariant:deeptrio-""${BIN_VERSION}-gpu"" \. bcftools view - \. | docker run -i google/deepvariant:deeptrio-""${BIN_VERSION}-gpu"" \. bgzip -c > output/HG002_trio_merged.vcf.gz. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/632
https://github.com/google/deepvariant/issues/632:1958,energy efficiency,gpu,gpu,1958," now the ```rtg-tools format``` miraculously work using the following command. ```. docker run \. -v ""${PWD}/input"":""/input"" \. -v ""${PWD}/reference"":""/reference"" \. realtimegenomics/rtg-tools format \. -o /reference/GRCh38_no_alt_analysis_set.sdf ""/reference/GRCh38_no_alt_analysis_set.fasta"". ```. And this is the result . ```. Formatting FASTA data. Processing ""/reference/GRCh38_no_alt_analysis_set.fasta"". Detected: 'Human GRCh38 with UCSC naming', installing reference.txt. Input Data. Files : GRCh38_no_alt_analysis_set.fasta. Format : FASTA. Type : DNA. Number of sequences: 195. Total residues : 3099922541. Minimum length : 970. Mean length : 15897038. Maximum length : 248956422. Output Data. SDF-ID : 809c9a82-d8d5-477a-865b-772d28741815. Number of sequences: 195. Total residues : 3099922541. Minimum length : 970. Mean length : 15897038. Maximum length : 248956422. ```. However this ```rtg-tools mendelian``` still result in error of ```Error: An IO problem occurred: ""Not in GZIP format""``` when running the following command below. ```. docker run \. -v ""${PWD}/input"":""/input"" \. -v ""${PWD}/reference"":""/reference"" \. -v ""${PWD}/output"":""/output"" \. realtimegenomics/rtg-tools mendelian \. -i ""/output/HG002_trio_merged.vcf.gz"" \. -o ""/output/HG002_trio_annotated.output.vcf.gz"" \. --pedigree=/reference/trio.ped \. -t /reference/GRCh38_no_alt_analysis_set.sdf \. | tee output/deepvariant.input_rtg_output.txt. ```. Have tried looking the wrong data and it seems the following GLNexus VCF Merge command is giving corrupted ```HG002_trio_merged.vcf.gz```. ```. docker run \. -v ""${PWD}/output"":""/output"" \. quay.io/mlin/glnexus:v1.2.7 \. /usr/local/bin/glnexus_cli \. --config DeepVariant_unfiltered \. /output/HG002.g.vcf.gz \. /output/HG003.g.vcf.gz \. /output/HG004.g.vcf.gz \. | docker run -i google/deepvariant:deeptrio-""${BIN_VERSION}-gpu"" \. bcftools view - \. | docker run -i google/deepvariant:deeptrio-""${BIN_VERSION}-gpu"" \. bgzip -c > output/HG002_trio_merged.vcf.gz. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/632
https://github.com/google/deepvariant/issues/632:35,interoperability,format,format,35,"Update : just now the ```rtg-tools format``` miraculously work using the following command. ```. docker run \. -v ""${PWD}/input"":""/input"" \. -v ""${PWD}/reference"":""/reference"" \. realtimegenomics/rtg-tools format \. -o /reference/GRCh38_no_alt_analysis_set.sdf ""/reference/GRCh38_no_alt_analysis_set.fasta"". ```. And this is the result . ```. Formatting FASTA data. Processing ""/reference/GRCh38_no_alt_analysis_set.fasta"". Detected: 'Human GRCh38 with UCSC naming', installing reference.txt. Input Data. Files : GRCh38_no_alt_analysis_set.fasta. Format : FASTA. Type : DNA. Number of sequences: 195. Total residues : 3099922541. Minimum length : 970. Mean length : 15897038. Maximum length : 248956422. Output Data. SDF-ID : 809c9a82-d8d5-477a-865b-772d28741815. Number of sequences: 195. Total residues : 3099922541. Minimum length : 970. Mean length : 15897038. Maximum length : 248956422. ```. However this ```rtg-tools mendelian``` still result in error of ```Error: An IO problem occurred: ""Not in GZIP format""``` when running the following command below. ```. docker run \. -v ""${PWD}/input"":""/input"" \. -v ""${PWD}/reference"":""/reference"" \. -v ""${PWD}/output"":""/output"" \. realtimegenomics/rtg-tools mendelian \. -i ""/output/HG002_trio_merged.vcf.gz"" \. -o ""/output/HG002_trio_annotated.output.vcf.gz"" \. --pedigree=/reference/trio.ped \. -t /reference/GRCh38_no_alt_analysis_set.sdf \. | tee output/deepvariant.input_rtg_output.txt. ```. Have tried looking the wrong data and it seems the following GLNexus VCF Merge command is giving corrupted ```HG002_trio_merged.vcf.gz```. ```. docker run \. -v ""${PWD}/output"":""/output"" \. quay.io/mlin/glnexus:v1.2.7 \. /usr/local/bin/glnexus_cli \. --config DeepVariant_unfiltered \. /output/HG002.g.vcf.gz \. /output/HG003.g.vcf.gz \. /output/HG004.g.vcf.gz \. | docker run -i google/deepvariant:deeptrio-""${BIN_VERSION}-gpu"" \. bcftools view - \. | docker run -i google/deepvariant:deeptrio-""${BIN_VERSION}-gpu"" \. bgzip -c > output/HG002_trio_merge",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/632
https://github.com/google/deepvariant/issues/632:206,interoperability,format,format,206,"Update : just now the ```rtg-tools format``` miraculously work using the following command. ```. docker run \. -v ""${PWD}/input"":""/input"" \. -v ""${PWD}/reference"":""/reference"" \. realtimegenomics/rtg-tools format \. -o /reference/GRCh38_no_alt_analysis_set.sdf ""/reference/GRCh38_no_alt_analysis_set.fasta"". ```. And this is the result . ```. Formatting FASTA data. Processing ""/reference/GRCh38_no_alt_analysis_set.fasta"". Detected: 'Human GRCh38 with UCSC naming', installing reference.txt. Input Data. Files : GRCh38_no_alt_analysis_set.fasta. Format : FASTA. Type : DNA. Number of sequences: 195. Total residues : 3099922541. Minimum length : 970. Mean length : 15897038. Maximum length : 248956422. Output Data. SDF-ID : 809c9a82-d8d5-477a-865b-772d28741815. Number of sequences: 195. Total residues : 3099922541. Minimum length : 970. Mean length : 15897038. Maximum length : 248956422. ```. However this ```rtg-tools mendelian``` still result in error of ```Error: An IO problem occurred: ""Not in GZIP format""``` when running the following command below. ```. docker run \. -v ""${PWD}/input"":""/input"" \. -v ""${PWD}/reference"":""/reference"" \. -v ""${PWD}/output"":""/output"" \. realtimegenomics/rtg-tools mendelian \. -i ""/output/HG002_trio_merged.vcf.gz"" \. -o ""/output/HG002_trio_annotated.output.vcf.gz"" \. --pedigree=/reference/trio.ped \. -t /reference/GRCh38_no_alt_analysis_set.sdf \. | tee output/deepvariant.input_rtg_output.txt. ```. Have tried looking the wrong data and it seems the following GLNexus VCF Merge command is giving corrupted ```HG002_trio_merged.vcf.gz```. ```. docker run \. -v ""${PWD}/output"":""/output"" \. quay.io/mlin/glnexus:v1.2.7 \. /usr/local/bin/glnexus_cli \. --config DeepVariant_unfiltered \. /output/HG002.g.vcf.gz \. /output/HG003.g.vcf.gz \. /output/HG004.g.vcf.gz \. | docker run -i google/deepvariant:deeptrio-""${BIN_VERSION}-gpu"" \. bcftools view - \. | docker run -i google/deepvariant:deeptrio-""${BIN_VERSION}-gpu"" \. bgzip -c > output/HG002_trio_merge",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/632
https://github.com/google/deepvariant/issues/632:343,interoperability,Format,Formatting,343,"Update : just now the ```rtg-tools format``` miraculously work using the following command. ```. docker run \. -v ""${PWD}/input"":""/input"" \. -v ""${PWD}/reference"":""/reference"" \. realtimegenomics/rtg-tools format \. -o /reference/GRCh38_no_alt_analysis_set.sdf ""/reference/GRCh38_no_alt_analysis_set.fasta"". ```. And this is the result . ```. Formatting FASTA data. Processing ""/reference/GRCh38_no_alt_analysis_set.fasta"". Detected: 'Human GRCh38 with UCSC naming', installing reference.txt. Input Data. Files : GRCh38_no_alt_analysis_set.fasta. Format : FASTA. Type : DNA. Number of sequences: 195. Total residues : 3099922541. Minimum length : 970. Mean length : 15897038. Maximum length : 248956422. Output Data. SDF-ID : 809c9a82-d8d5-477a-865b-772d28741815. Number of sequences: 195. Total residues : 3099922541. Minimum length : 970. Mean length : 15897038. Maximum length : 248956422. ```. However this ```rtg-tools mendelian``` still result in error of ```Error: An IO problem occurred: ""Not in GZIP format""``` when running the following command below. ```. docker run \. -v ""${PWD}/input"":""/input"" \. -v ""${PWD}/reference"":""/reference"" \. -v ""${PWD}/output"":""/output"" \. realtimegenomics/rtg-tools mendelian \. -i ""/output/HG002_trio_merged.vcf.gz"" \. -o ""/output/HG002_trio_annotated.output.vcf.gz"" \. --pedigree=/reference/trio.ped \. -t /reference/GRCh38_no_alt_analysis_set.sdf \. | tee output/deepvariant.input_rtg_output.txt. ```. Have tried looking the wrong data and it seems the following GLNexus VCF Merge command is giving corrupted ```HG002_trio_merged.vcf.gz```. ```. docker run \. -v ""${PWD}/output"":""/output"" \. quay.io/mlin/glnexus:v1.2.7 \. /usr/local/bin/glnexus_cli \. --config DeepVariant_unfiltered \. /output/HG002.g.vcf.gz \. /output/HG003.g.vcf.gz \. /output/HG004.g.vcf.gz \. | docker run -i google/deepvariant:deeptrio-""${BIN_VERSION}-gpu"" \. bcftools view - \. | docker run -i google/deepvariant:deeptrio-""${BIN_VERSION}-gpu"" \. bgzip -c > output/HG002_trio_merge",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/632
https://github.com/google/deepvariant/issues/632:547,interoperability,Format,Format,547,"Update : just now the ```rtg-tools format``` miraculously work using the following command. ```. docker run \. -v ""${PWD}/input"":""/input"" \. -v ""${PWD}/reference"":""/reference"" \. realtimegenomics/rtg-tools format \. -o /reference/GRCh38_no_alt_analysis_set.sdf ""/reference/GRCh38_no_alt_analysis_set.fasta"". ```. And this is the result . ```. Formatting FASTA data. Processing ""/reference/GRCh38_no_alt_analysis_set.fasta"". Detected: 'Human GRCh38 with UCSC naming', installing reference.txt. Input Data. Files : GRCh38_no_alt_analysis_set.fasta. Format : FASTA. Type : DNA. Number of sequences: 195. Total residues : 3099922541. Minimum length : 970. Mean length : 15897038. Maximum length : 248956422. Output Data. SDF-ID : 809c9a82-d8d5-477a-865b-772d28741815. Number of sequences: 195. Total residues : 3099922541. Minimum length : 970. Mean length : 15897038. Maximum length : 248956422. ```. However this ```rtg-tools mendelian``` still result in error of ```Error: An IO problem occurred: ""Not in GZIP format""``` when running the following command below. ```. docker run \. -v ""${PWD}/input"":""/input"" \. -v ""${PWD}/reference"":""/reference"" \. -v ""${PWD}/output"":""/output"" \. realtimegenomics/rtg-tools mendelian \. -i ""/output/HG002_trio_merged.vcf.gz"" \. -o ""/output/HG002_trio_annotated.output.vcf.gz"" \. --pedigree=/reference/trio.ped \. -t /reference/GRCh38_no_alt_analysis_set.sdf \. | tee output/deepvariant.input_rtg_output.txt. ```. Have tried looking the wrong data and it seems the following GLNexus VCF Merge command is giving corrupted ```HG002_trio_merged.vcf.gz```. ```. docker run \. -v ""${PWD}/output"":""/output"" \. quay.io/mlin/glnexus:v1.2.7 \. /usr/local/bin/glnexus_cli \. --config DeepVariant_unfiltered \. /output/HG002.g.vcf.gz \. /output/HG003.g.vcf.gz \. /output/HG004.g.vcf.gz \. | docker run -i google/deepvariant:deeptrio-""${BIN_VERSION}-gpu"" \. bcftools view - \. | docker run -i google/deepvariant:deeptrio-""${BIN_VERSION}-gpu"" \. bgzip -c > output/HG002_trio_merge",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/632
https://github.com/google/deepvariant/issues/632:1009,interoperability,format,format,1009,"t now the ```rtg-tools format``` miraculously work using the following command. ```. docker run \. -v ""${PWD}/input"":""/input"" \. -v ""${PWD}/reference"":""/reference"" \. realtimegenomics/rtg-tools format \. -o /reference/GRCh38_no_alt_analysis_set.sdf ""/reference/GRCh38_no_alt_analysis_set.fasta"". ```. And this is the result . ```. Formatting FASTA data. Processing ""/reference/GRCh38_no_alt_analysis_set.fasta"". Detected: 'Human GRCh38 with UCSC naming', installing reference.txt. Input Data. Files : GRCh38_no_alt_analysis_set.fasta. Format : FASTA. Type : DNA. Number of sequences: 195. Total residues : 3099922541. Minimum length : 970. Mean length : 15897038. Maximum length : 248956422. Output Data. SDF-ID : 809c9a82-d8d5-477a-865b-772d28741815. Number of sequences: 195. Total residues : 3099922541. Minimum length : 970. Mean length : 15897038. Maximum length : 248956422. ```. However this ```rtg-tools mendelian``` still result in error of ```Error: An IO problem occurred: ""Not in GZIP format""``` when running the following command below. ```. docker run \. -v ""${PWD}/input"":""/input"" \. -v ""${PWD}/reference"":""/reference"" \. -v ""${PWD}/output"":""/output"" \. realtimegenomics/rtg-tools mendelian \. -i ""/output/HG002_trio_merged.vcf.gz"" \. -o ""/output/HG002_trio_annotated.output.vcf.gz"" \. --pedigree=/reference/trio.ped \. -t /reference/GRCh38_no_alt_analysis_set.sdf \. | tee output/deepvariant.input_rtg_output.txt. ```. Have tried looking the wrong data and it seems the following GLNexus VCF Merge command is giving corrupted ```HG002_trio_merged.vcf.gz```. ```. docker run \. -v ""${PWD}/output"":""/output"" \. quay.io/mlin/glnexus:v1.2.7 \. /usr/local/bin/glnexus_cli \. --config DeepVariant_unfiltered \. /output/HG002.g.vcf.gz \. /output/HG003.g.vcf.gz \. /output/HG004.g.vcf.gz \. | docker run -i google/deepvariant:deeptrio-""${BIN_VERSION}-gpu"" \. bcftools view - \. | docker run -i google/deepvariant:deeptrio-""${BIN_VERSION}-gpu"" \. bgzip -c > output/HG002_trio_merged.vcf.gz. ``",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/632
https://github.com/google/deepvariant/issues/632:953,performance,error,error,953,"Update : just now the ```rtg-tools format``` miraculously work using the following command. ```. docker run \. -v ""${PWD}/input"":""/input"" \. -v ""${PWD}/reference"":""/reference"" \. realtimegenomics/rtg-tools format \. -o /reference/GRCh38_no_alt_analysis_set.sdf ""/reference/GRCh38_no_alt_analysis_set.fasta"". ```. And this is the result . ```. Formatting FASTA data. Processing ""/reference/GRCh38_no_alt_analysis_set.fasta"". Detected: 'Human GRCh38 with UCSC naming', installing reference.txt. Input Data. Files : GRCh38_no_alt_analysis_set.fasta. Format : FASTA. Type : DNA. Number of sequences: 195. Total residues : 3099922541. Minimum length : 970. Mean length : 15897038. Maximum length : 248956422. Output Data. SDF-ID : 809c9a82-d8d5-477a-865b-772d28741815. Number of sequences: 195. Total residues : 3099922541. Minimum length : 970. Mean length : 15897038. Maximum length : 248956422. ```. However this ```rtg-tools mendelian``` still result in error of ```Error: An IO problem occurred: ""Not in GZIP format""``` when running the following command below. ```. docker run \. -v ""${PWD}/input"":""/input"" \. -v ""${PWD}/reference"":""/reference"" \. -v ""${PWD}/output"":""/output"" \. realtimegenomics/rtg-tools mendelian \. -i ""/output/HG002_trio_merged.vcf.gz"" \. -o ""/output/HG002_trio_annotated.output.vcf.gz"" \. --pedigree=/reference/trio.ped \. -t /reference/GRCh38_no_alt_analysis_set.sdf \. | tee output/deepvariant.input_rtg_output.txt. ```. Have tried looking the wrong data and it seems the following GLNexus VCF Merge command is giving corrupted ```HG002_trio_merged.vcf.gz```. ```. docker run \. -v ""${PWD}/output"":""/output"" \. quay.io/mlin/glnexus:v1.2.7 \. /usr/local/bin/glnexus_cli \. --config DeepVariant_unfiltered \. /output/HG002.g.vcf.gz \. /output/HG003.g.vcf.gz \. /output/HG004.g.vcf.gz \. | docker run -i google/deepvariant:deeptrio-""${BIN_VERSION}-gpu"" \. bcftools view - \. | docker run -i google/deepvariant:deeptrio-""${BIN_VERSION}-gpu"" \. bgzip -c > output/HG002_trio_merge",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/632
https://github.com/google/deepvariant/issues/632:965,performance,Error,Error,965,"Update : just now the ```rtg-tools format``` miraculously work using the following command. ```. docker run \. -v ""${PWD}/input"":""/input"" \. -v ""${PWD}/reference"":""/reference"" \. realtimegenomics/rtg-tools format \. -o /reference/GRCh38_no_alt_analysis_set.sdf ""/reference/GRCh38_no_alt_analysis_set.fasta"". ```. And this is the result . ```. Formatting FASTA data. Processing ""/reference/GRCh38_no_alt_analysis_set.fasta"". Detected: 'Human GRCh38 with UCSC naming', installing reference.txt. Input Data. Files : GRCh38_no_alt_analysis_set.fasta. Format : FASTA. Type : DNA. Number of sequences: 195. Total residues : 3099922541. Minimum length : 970. Mean length : 15897038. Maximum length : 248956422. Output Data. SDF-ID : 809c9a82-d8d5-477a-865b-772d28741815. Number of sequences: 195. Total residues : 3099922541. Minimum length : 970. Mean length : 15897038. Maximum length : 248956422. ```. However this ```rtg-tools mendelian``` still result in error of ```Error: An IO problem occurred: ""Not in GZIP format""``` when running the following command below. ```. docker run \. -v ""${PWD}/input"":""/input"" \. -v ""${PWD}/reference"":""/reference"" \. -v ""${PWD}/output"":""/output"" \. realtimegenomics/rtg-tools mendelian \. -i ""/output/HG002_trio_merged.vcf.gz"" \. -o ""/output/HG002_trio_annotated.output.vcf.gz"" \. --pedigree=/reference/trio.ped \. -t /reference/GRCh38_no_alt_analysis_set.sdf \. | tee output/deepvariant.input_rtg_output.txt. ```. Have tried looking the wrong data and it seems the following GLNexus VCF Merge command is giving corrupted ```HG002_trio_merged.vcf.gz```. ```. docker run \. -v ""${PWD}/output"":""/output"" \. quay.io/mlin/glnexus:v1.2.7 \. /usr/local/bin/glnexus_cli \. --config DeepVariant_unfiltered \. /output/HG002.g.vcf.gz \. /output/HG003.g.vcf.gz \. /output/HG004.g.vcf.gz \. | docker run -i google/deepvariant:deeptrio-""${BIN_VERSION}-gpu"" \. bcftools view - \. | docker run -i google/deepvariant:deeptrio-""${BIN_VERSION}-gpu"" \. bgzip -c > output/HG002_trio_merge",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/632
https://github.com/google/deepvariant/issues/632:1871,performance,gpu,gpu,1871," now the ```rtg-tools format``` miraculously work using the following command. ```. docker run \. -v ""${PWD}/input"":""/input"" \. -v ""${PWD}/reference"":""/reference"" \. realtimegenomics/rtg-tools format \. -o /reference/GRCh38_no_alt_analysis_set.sdf ""/reference/GRCh38_no_alt_analysis_set.fasta"". ```. And this is the result . ```. Formatting FASTA data. Processing ""/reference/GRCh38_no_alt_analysis_set.fasta"". Detected: 'Human GRCh38 with UCSC naming', installing reference.txt. Input Data. Files : GRCh38_no_alt_analysis_set.fasta. Format : FASTA. Type : DNA. Number of sequences: 195. Total residues : 3099922541. Minimum length : 970. Mean length : 15897038. Maximum length : 248956422. Output Data. SDF-ID : 809c9a82-d8d5-477a-865b-772d28741815. Number of sequences: 195. Total residues : 3099922541. Minimum length : 970. Mean length : 15897038. Maximum length : 248956422. ```. However this ```rtg-tools mendelian``` still result in error of ```Error: An IO problem occurred: ""Not in GZIP format""``` when running the following command below. ```. docker run \. -v ""${PWD}/input"":""/input"" \. -v ""${PWD}/reference"":""/reference"" \. -v ""${PWD}/output"":""/output"" \. realtimegenomics/rtg-tools mendelian \. -i ""/output/HG002_trio_merged.vcf.gz"" \. -o ""/output/HG002_trio_annotated.output.vcf.gz"" \. --pedigree=/reference/trio.ped \. -t /reference/GRCh38_no_alt_analysis_set.sdf \. | tee output/deepvariant.input_rtg_output.txt. ```. Have tried looking the wrong data and it seems the following GLNexus VCF Merge command is giving corrupted ```HG002_trio_merged.vcf.gz```. ```. docker run \. -v ""${PWD}/output"":""/output"" \. quay.io/mlin/glnexus:v1.2.7 \. /usr/local/bin/glnexus_cli \. --config DeepVariant_unfiltered \. /output/HG002.g.vcf.gz \. /output/HG003.g.vcf.gz \. /output/HG004.g.vcf.gz \. | docker run -i google/deepvariant:deeptrio-""${BIN_VERSION}-gpu"" \. bcftools view - \. | docker run -i google/deepvariant:deeptrio-""${BIN_VERSION}-gpu"" \. bgzip -c > output/HG002_trio_merged.vcf.gz. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/632
https://github.com/google/deepvariant/issues/632:1958,performance,gpu,gpu,1958," now the ```rtg-tools format``` miraculously work using the following command. ```. docker run \. -v ""${PWD}/input"":""/input"" \. -v ""${PWD}/reference"":""/reference"" \. realtimegenomics/rtg-tools format \. -o /reference/GRCh38_no_alt_analysis_set.sdf ""/reference/GRCh38_no_alt_analysis_set.fasta"". ```. And this is the result . ```. Formatting FASTA data. Processing ""/reference/GRCh38_no_alt_analysis_set.fasta"". Detected: 'Human GRCh38 with UCSC naming', installing reference.txt. Input Data. Files : GRCh38_no_alt_analysis_set.fasta. Format : FASTA. Type : DNA. Number of sequences: 195. Total residues : 3099922541. Minimum length : 970. Mean length : 15897038. Maximum length : 248956422. Output Data. SDF-ID : 809c9a82-d8d5-477a-865b-772d28741815. Number of sequences: 195. Total residues : 3099922541. Minimum length : 970. Mean length : 15897038. Maximum length : 248956422. ```. However this ```rtg-tools mendelian``` still result in error of ```Error: An IO problem occurred: ""Not in GZIP format""``` when running the following command below. ```. docker run \. -v ""${PWD}/input"":""/input"" \. -v ""${PWD}/reference"":""/reference"" \. -v ""${PWD}/output"":""/output"" \. realtimegenomics/rtg-tools mendelian \. -i ""/output/HG002_trio_merged.vcf.gz"" \. -o ""/output/HG002_trio_annotated.output.vcf.gz"" \. --pedigree=/reference/trio.ped \. -t /reference/GRCh38_no_alt_analysis_set.sdf \. | tee output/deepvariant.input_rtg_output.txt. ```. Have tried looking the wrong data and it seems the following GLNexus VCF Merge command is giving corrupted ```HG002_trio_merged.vcf.gz```. ```. docker run \. -v ""${PWD}/output"":""/output"" \. quay.io/mlin/glnexus:v1.2.7 \. /usr/local/bin/glnexus_cli \. --config DeepVariant_unfiltered \. /output/HG002.g.vcf.gz \. /output/HG003.g.vcf.gz \. /output/HG004.g.vcf.gz \. | docker run -i google/deepvariant:deeptrio-""${BIN_VERSION}-gpu"" \. bcftools view - \. | docker run -i google/deepvariant:deeptrio-""${BIN_VERSION}-gpu"" \. bgzip -c > output/HG002_trio_merged.vcf.gz. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/632
https://github.com/google/deepvariant/issues/632:0,safety,Updat,Update,0,"Update : just now the ```rtg-tools format``` miraculously work using the following command. ```. docker run \. -v ""${PWD}/input"":""/input"" \. -v ""${PWD}/reference"":""/reference"" \. realtimegenomics/rtg-tools format \. -o /reference/GRCh38_no_alt_analysis_set.sdf ""/reference/GRCh38_no_alt_analysis_set.fasta"". ```. And this is the result . ```. Formatting FASTA data. Processing ""/reference/GRCh38_no_alt_analysis_set.fasta"". Detected: 'Human GRCh38 with UCSC naming', installing reference.txt. Input Data. Files : GRCh38_no_alt_analysis_set.fasta. Format : FASTA. Type : DNA. Number of sequences: 195. Total residues : 3099922541. Minimum length : 970. Mean length : 15897038. Maximum length : 248956422. Output Data. SDF-ID : 809c9a82-d8d5-477a-865b-772d28741815. Number of sequences: 195. Total residues : 3099922541. Minimum length : 970. Mean length : 15897038. Maximum length : 248956422. ```. However this ```rtg-tools mendelian``` still result in error of ```Error: An IO problem occurred: ""Not in GZIP format""``` when running the following command below. ```. docker run \. -v ""${PWD}/input"":""/input"" \. -v ""${PWD}/reference"":""/reference"" \. -v ""${PWD}/output"":""/output"" \. realtimegenomics/rtg-tools mendelian \. -i ""/output/HG002_trio_merged.vcf.gz"" \. -o ""/output/HG002_trio_annotated.output.vcf.gz"" \. --pedigree=/reference/trio.ped \. -t /reference/GRCh38_no_alt_analysis_set.sdf \. | tee output/deepvariant.input_rtg_output.txt. ```. Have tried looking the wrong data and it seems the following GLNexus VCF Merge command is giving corrupted ```HG002_trio_merged.vcf.gz```. ```. docker run \. -v ""${PWD}/output"":""/output"" \. quay.io/mlin/glnexus:v1.2.7 \. /usr/local/bin/glnexus_cli \. --config DeepVariant_unfiltered \. /output/HG002.g.vcf.gz \. /output/HG003.g.vcf.gz \. /output/HG004.g.vcf.gz \. | docker run -i google/deepvariant:deeptrio-""${BIN_VERSION}-gpu"" \. bcftools view - \. | docker run -i google/deepvariant:deeptrio-""${BIN_VERSION}-gpu"" \. bgzip -c > output/HG002_trio_merge",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/632
https://github.com/google/deepvariant/issues/632:122,safety,input,input,122,"Update : just now the ```rtg-tools format``` miraculously work using the following command. ```. docker run \. -v ""${PWD}/input"":""/input"" \. -v ""${PWD}/reference"":""/reference"" \. realtimegenomics/rtg-tools format \. -o /reference/GRCh38_no_alt_analysis_set.sdf ""/reference/GRCh38_no_alt_analysis_set.fasta"". ```. And this is the result . ```. Formatting FASTA data. Processing ""/reference/GRCh38_no_alt_analysis_set.fasta"". Detected: 'Human GRCh38 with UCSC naming', installing reference.txt. Input Data. Files : GRCh38_no_alt_analysis_set.fasta. Format : FASTA. Type : DNA. Number of sequences: 195. Total residues : 3099922541. Minimum length : 970. Mean length : 15897038. Maximum length : 248956422. Output Data. SDF-ID : 809c9a82-d8d5-477a-865b-772d28741815. Number of sequences: 195. Total residues : 3099922541. Minimum length : 970. Mean length : 15897038. Maximum length : 248956422. ```. However this ```rtg-tools mendelian``` still result in error of ```Error: An IO problem occurred: ""Not in GZIP format""``` when running the following command below. ```. docker run \. -v ""${PWD}/input"":""/input"" \. -v ""${PWD}/reference"":""/reference"" \. -v ""${PWD}/output"":""/output"" \. realtimegenomics/rtg-tools mendelian \. -i ""/output/HG002_trio_merged.vcf.gz"" \. -o ""/output/HG002_trio_annotated.output.vcf.gz"" \. --pedigree=/reference/trio.ped \. -t /reference/GRCh38_no_alt_analysis_set.sdf \. | tee output/deepvariant.input_rtg_output.txt. ```. Have tried looking the wrong data and it seems the following GLNexus VCF Merge command is giving corrupted ```HG002_trio_merged.vcf.gz```. ```. docker run \. -v ""${PWD}/output"":""/output"" \. quay.io/mlin/glnexus:v1.2.7 \. /usr/local/bin/glnexus_cli \. --config DeepVariant_unfiltered \. /output/HG002.g.vcf.gz \. /output/HG003.g.vcf.gz \. /output/HG004.g.vcf.gz \. | docker run -i google/deepvariant:deeptrio-""${BIN_VERSION}-gpu"" \. bcftools view - \. | docker run -i google/deepvariant:deeptrio-""${BIN_VERSION}-gpu"" \. bgzip -c > output/HG002_trio_merge",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/632
https://github.com/google/deepvariant/issues/632:131,safety,input,input,131,"Update : just now the ```rtg-tools format``` miraculously work using the following command. ```. docker run \. -v ""${PWD}/input"":""/input"" \. -v ""${PWD}/reference"":""/reference"" \. realtimegenomics/rtg-tools format \. -o /reference/GRCh38_no_alt_analysis_set.sdf ""/reference/GRCh38_no_alt_analysis_set.fasta"". ```. And this is the result . ```. Formatting FASTA data. Processing ""/reference/GRCh38_no_alt_analysis_set.fasta"". Detected: 'Human GRCh38 with UCSC naming', installing reference.txt. Input Data. Files : GRCh38_no_alt_analysis_set.fasta. Format : FASTA. Type : DNA. Number of sequences: 195. Total residues : 3099922541. Minimum length : 970. Mean length : 15897038. Maximum length : 248956422. Output Data. SDF-ID : 809c9a82-d8d5-477a-865b-772d28741815. Number of sequences: 195. Total residues : 3099922541. Minimum length : 970. Mean length : 15897038. Maximum length : 248956422. ```. However this ```rtg-tools mendelian``` still result in error of ```Error: An IO problem occurred: ""Not in GZIP format""``` when running the following command below. ```. docker run \. -v ""${PWD}/input"":""/input"" \. -v ""${PWD}/reference"":""/reference"" \. -v ""${PWD}/output"":""/output"" \. realtimegenomics/rtg-tools mendelian \. -i ""/output/HG002_trio_merged.vcf.gz"" \. -o ""/output/HG002_trio_annotated.output.vcf.gz"" \. --pedigree=/reference/trio.ped \. -t /reference/GRCh38_no_alt_analysis_set.sdf \. | tee output/deepvariant.input_rtg_output.txt. ```. Have tried looking the wrong data and it seems the following GLNexus VCF Merge command is giving corrupted ```HG002_trio_merged.vcf.gz```. ```. docker run \. -v ""${PWD}/output"":""/output"" \. quay.io/mlin/glnexus:v1.2.7 \. /usr/local/bin/glnexus_cli \. --config DeepVariant_unfiltered \. /output/HG002.g.vcf.gz \. /output/HG003.g.vcf.gz \. /output/HG004.g.vcf.gz \. | docker run -i google/deepvariant:deeptrio-""${BIN_VERSION}-gpu"" \. bcftools view - \. | docker run -i google/deepvariant:deeptrio-""${BIN_VERSION}-gpu"" \. bgzip -c > output/HG002_trio_merge",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/632
https://github.com/google/deepvariant/issues/632:424,safety,Detect,Detected,424,"Update : just now the ```rtg-tools format``` miraculously work using the following command. ```. docker run \. -v ""${PWD}/input"":""/input"" \. -v ""${PWD}/reference"":""/reference"" \. realtimegenomics/rtg-tools format \. -o /reference/GRCh38_no_alt_analysis_set.sdf ""/reference/GRCh38_no_alt_analysis_set.fasta"". ```. And this is the result . ```. Formatting FASTA data. Processing ""/reference/GRCh38_no_alt_analysis_set.fasta"". Detected: 'Human GRCh38 with UCSC naming', installing reference.txt. Input Data. Files : GRCh38_no_alt_analysis_set.fasta. Format : FASTA. Type : DNA. Number of sequences: 195. Total residues : 3099922541. Minimum length : 970. Mean length : 15897038. Maximum length : 248956422. Output Data. SDF-ID : 809c9a82-d8d5-477a-865b-772d28741815. Number of sequences: 195. Total residues : 3099922541. Minimum length : 970. Mean length : 15897038. Maximum length : 248956422. ```. However this ```rtg-tools mendelian``` still result in error of ```Error: An IO problem occurred: ""Not in GZIP format""``` when running the following command below. ```. docker run \. -v ""${PWD}/input"":""/input"" \. -v ""${PWD}/reference"":""/reference"" \. -v ""${PWD}/output"":""/output"" \. realtimegenomics/rtg-tools mendelian \. -i ""/output/HG002_trio_merged.vcf.gz"" \. -o ""/output/HG002_trio_annotated.output.vcf.gz"" \. --pedigree=/reference/trio.ped \. -t /reference/GRCh38_no_alt_analysis_set.sdf \. | tee output/deepvariant.input_rtg_output.txt. ```. Have tried looking the wrong data and it seems the following GLNexus VCF Merge command is giving corrupted ```HG002_trio_merged.vcf.gz```. ```. docker run \. -v ""${PWD}/output"":""/output"" \. quay.io/mlin/glnexus:v1.2.7 \. /usr/local/bin/glnexus_cli \. --config DeepVariant_unfiltered \. /output/HG002.g.vcf.gz \. /output/HG003.g.vcf.gz \. /output/HG004.g.vcf.gz \. | docker run -i google/deepvariant:deeptrio-""${BIN_VERSION}-gpu"" \. bcftools view - \. | docker run -i google/deepvariant:deeptrio-""${BIN_VERSION}-gpu"" \. bgzip -c > output/HG002_trio_merge",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/632
https://github.com/google/deepvariant/issues/632:493,safety,Input,Input,493,"Update : just now the ```rtg-tools format``` miraculously work using the following command. ```. docker run \. -v ""${PWD}/input"":""/input"" \. -v ""${PWD}/reference"":""/reference"" \. realtimegenomics/rtg-tools format \. -o /reference/GRCh38_no_alt_analysis_set.sdf ""/reference/GRCh38_no_alt_analysis_set.fasta"". ```. And this is the result . ```. Formatting FASTA data. Processing ""/reference/GRCh38_no_alt_analysis_set.fasta"". Detected: 'Human GRCh38 with UCSC naming', installing reference.txt. Input Data. Files : GRCh38_no_alt_analysis_set.fasta. Format : FASTA. Type : DNA. Number of sequences: 195. Total residues : 3099922541. Minimum length : 970. Mean length : 15897038. Maximum length : 248956422. Output Data. SDF-ID : 809c9a82-d8d5-477a-865b-772d28741815. Number of sequences: 195. Total residues : 3099922541. Minimum length : 970. Mean length : 15897038. Maximum length : 248956422. ```. However this ```rtg-tools mendelian``` still result in error of ```Error: An IO problem occurred: ""Not in GZIP format""``` when running the following command below. ```. docker run \. -v ""${PWD}/input"":""/input"" \. -v ""${PWD}/reference"":""/reference"" \. -v ""${PWD}/output"":""/output"" \. realtimegenomics/rtg-tools mendelian \. -i ""/output/HG002_trio_merged.vcf.gz"" \. -o ""/output/HG002_trio_annotated.output.vcf.gz"" \. --pedigree=/reference/trio.ped \. -t /reference/GRCh38_no_alt_analysis_set.sdf \. | tee output/deepvariant.input_rtg_output.txt. ```. Have tried looking the wrong data and it seems the following GLNexus VCF Merge command is giving corrupted ```HG002_trio_merged.vcf.gz```. ```. docker run \. -v ""${PWD}/output"":""/output"" \. quay.io/mlin/glnexus:v1.2.7 \. /usr/local/bin/glnexus_cli \. --config DeepVariant_unfiltered \. /output/HG002.g.vcf.gz \. /output/HG003.g.vcf.gz \. /output/HG004.g.vcf.gz \. | docker run -i google/deepvariant:deeptrio-""${BIN_VERSION}-gpu"" \. bcftools view - \. | docker run -i google/deepvariant:deeptrio-""${BIN_VERSION}-gpu"" \. bgzip -c > output/HG002_trio_merge",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/632
https://github.com/google/deepvariant/issues/632:953,safety,error,error,953,"Update : just now the ```rtg-tools format``` miraculously work using the following command. ```. docker run \. -v ""${PWD}/input"":""/input"" \. -v ""${PWD}/reference"":""/reference"" \. realtimegenomics/rtg-tools format \. -o /reference/GRCh38_no_alt_analysis_set.sdf ""/reference/GRCh38_no_alt_analysis_set.fasta"". ```. And this is the result . ```. Formatting FASTA data. Processing ""/reference/GRCh38_no_alt_analysis_set.fasta"". Detected: 'Human GRCh38 with UCSC naming', installing reference.txt. Input Data. Files : GRCh38_no_alt_analysis_set.fasta. Format : FASTA. Type : DNA. Number of sequences: 195. Total residues : 3099922541. Minimum length : 970. Mean length : 15897038. Maximum length : 248956422. Output Data. SDF-ID : 809c9a82-d8d5-477a-865b-772d28741815. Number of sequences: 195. Total residues : 3099922541. Minimum length : 970. Mean length : 15897038. Maximum length : 248956422. ```. However this ```rtg-tools mendelian``` still result in error of ```Error: An IO problem occurred: ""Not in GZIP format""``` when running the following command below. ```. docker run \. -v ""${PWD}/input"":""/input"" \. -v ""${PWD}/reference"":""/reference"" \. -v ""${PWD}/output"":""/output"" \. realtimegenomics/rtg-tools mendelian \. -i ""/output/HG002_trio_merged.vcf.gz"" \. -o ""/output/HG002_trio_annotated.output.vcf.gz"" \. --pedigree=/reference/trio.ped \. -t /reference/GRCh38_no_alt_analysis_set.sdf \. | tee output/deepvariant.input_rtg_output.txt. ```. Have tried looking the wrong data and it seems the following GLNexus VCF Merge command is giving corrupted ```HG002_trio_merged.vcf.gz```. ```. docker run \. -v ""${PWD}/output"":""/output"" \. quay.io/mlin/glnexus:v1.2.7 \. /usr/local/bin/glnexus_cli \. --config DeepVariant_unfiltered \. /output/HG002.g.vcf.gz \. /output/HG003.g.vcf.gz \. /output/HG004.g.vcf.gz \. | docker run -i google/deepvariant:deeptrio-""${BIN_VERSION}-gpu"" \. bcftools view - \. | docker run -i google/deepvariant:deeptrio-""${BIN_VERSION}-gpu"" \. bgzip -c > output/HG002_trio_merge",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/632
https://github.com/google/deepvariant/issues/632:965,safety,Error,Error,965,"Update : just now the ```rtg-tools format``` miraculously work using the following command. ```. docker run \. -v ""${PWD}/input"":""/input"" \. -v ""${PWD}/reference"":""/reference"" \. realtimegenomics/rtg-tools format \. -o /reference/GRCh38_no_alt_analysis_set.sdf ""/reference/GRCh38_no_alt_analysis_set.fasta"". ```. And this is the result . ```. Formatting FASTA data. Processing ""/reference/GRCh38_no_alt_analysis_set.fasta"". Detected: 'Human GRCh38 with UCSC naming', installing reference.txt. Input Data. Files : GRCh38_no_alt_analysis_set.fasta. Format : FASTA. Type : DNA. Number of sequences: 195. Total residues : 3099922541. Minimum length : 970. Mean length : 15897038. Maximum length : 248956422. Output Data. SDF-ID : 809c9a82-d8d5-477a-865b-772d28741815. Number of sequences: 195. Total residues : 3099922541. Minimum length : 970. Mean length : 15897038. Maximum length : 248956422. ```. However this ```rtg-tools mendelian``` still result in error of ```Error: An IO problem occurred: ""Not in GZIP format""``` when running the following command below. ```. docker run \. -v ""${PWD}/input"":""/input"" \. -v ""${PWD}/reference"":""/reference"" \. -v ""${PWD}/output"":""/output"" \. realtimegenomics/rtg-tools mendelian \. -i ""/output/HG002_trio_merged.vcf.gz"" \. -o ""/output/HG002_trio_annotated.output.vcf.gz"" \. --pedigree=/reference/trio.ped \. -t /reference/GRCh38_no_alt_analysis_set.sdf \. | tee output/deepvariant.input_rtg_output.txt. ```. Have tried looking the wrong data and it seems the following GLNexus VCF Merge command is giving corrupted ```HG002_trio_merged.vcf.gz```. ```. docker run \. -v ""${PWD}/output"":""/output"" \. quay.io/mlin/glnexus:v1.2.7 \. /usr/local/bin/glnexus_cli \. --config DeepVariant_unfiltered \. /output/HG002.g.vcf.gz \. /output/HG003.g.vcf.gz \. /output/HG004.g.vcf.gz \. | docker run -i google/deepvariant:deeptrio-""${BIN_VERSION}-gpu"" \. bcftools view - \. | docker run -i google/deepvariant:deeptrio-""${BIN_VERSION}-gpu"" \. bgzip -c > output/HG002_trio_merge",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/632
https://github.com/google/deepvariant/issues/632:1092,safety,input,input,1092," now the ```rtg-tools format``` miraculously work using the following command. ```. docker run \. -v ""${PWD}/input"":""/input"" \. -v ""${PWD}/reference"":""/reference"" \. realtimegenomics/rtg-tools format \. -o /reference/GRCh38_no_alt_analysis_set.sdf ""/reference/GRCh38_no_alt_analysis_set.fasta"". ```. And this is the result . ```. Formatting FASTA data. Processing ""/reference/GRCh38_no_alt_analysis_set.fasta"". Detected: 'Human GRCh38 with UCSC naming', installing reference.txt. Input Data. Files : GRCh38_no_alt_analysis_set.fasta. Format : FASTA. Type : DNA. Number of sequences: 195. Total residues : 3099922541. Minimum length : 970. Mean length : 15897038. Maximum length : 248956422. Output Data. SDF-ID : 809c9a82-d8d5-477a-865b-772d28741815. Number of sequences: 195. Total residues : 3099922541. Minimum length : 970. Mean length : 15897038. Maximum length : 248956422. ```. However this ```rtg-tools mendelian``` still result in error of ```Error: An IO problem occurred: ""Not in GZIP format""``` when running the following command below. ```. docker run \. -v ""${PWD}/input"":""/input"" \. -v ""${PWD}/reference"":""/reference"" \. -v ""${PWD}/output"":""/output"" \. realtimegenomics/rtg-tools mendelian \. -i ""/output/HG002_trio_merged.vcf.gz"" \. -o ""/output/HG002_trio_annotated.output.vcf.gz"" \. --pedigree=/reference/trio.ped \. -t /reference/GRCh38_no_alt_analysis_set.sdf \. | tee output/deepvariant.input_rtg_output.txt. ```. Have tried looking the wrong data and it seems the following GLNexus VCF Merge command is giving corrupted ```HG002_trio_merged.vcf.gz```. ```. docker run \. -v ""${PWD}/output"":""/output"" \. quay.io/mlin/glnexus:v1.2.7 \. /usr/local/bin/glnexus_cli \. --config DeepVariant_unfiltered \. /output/HG002.g.vcf.gz \. /output/HG003.g.vcf.gz \. /output/HG004.g.vcf.gz \. | docker run -i google/deepvariant:deeptrio-""${BIN_VERSION}-gpu"" \. bcftools view - \. | docker run -i google/deepvariant:deeptrio-""${BIN_VERSION}-gpu"" \. bgzip -c > output/HG002_trio_merged.vcf.gz. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/632
https://github.com/google/deepvariant/issues/632:1101,safety,input,input,1101," now the ```rtg-tools format``` miraculously work using the following command. ```. docker run \. -v ""${PWD}/input"":""/input"" \. -v ""${PWD}/reference"":""/reference"" \. realtimegenomics/rtg-tools format \. -o /reference/GRCh38_no_alt_analysis_set.sdf ""/reference/GRCh38_no_alt_analysis_set.fasta"". ```. And this is the result . ```. Formatting FASTA data. Processing ""/reference/GRCh38_no_alt_analysis_set.fasta"". Detected: 'Human GRCh38 with UCSC naming', installing reference.txt. Input Data. Files : GRCh38_no_alt_analysis_set.fasta. Format : FASTA. Type : DNA. Number of sequences: 195. Total residues : 3099922541. Minimum length : 970. Mean length : 15897038. Maximum length : 248956422. Output Data. SDF-ID : 809c9a82-d8d5-477a-865b-772d28741815. Number of sequences: 195. Total residues : 3099922541. Minimum length : 970. Mean length : 15897038. Maximum length : 248956422. ```. However this ```rtg-tools mendelian``` still result in error of ```Error: An IO problem occurred: ""Not in GZIP format""``` when running the following command below. ```. docker run \. -v ""${PWD}/input"":""/input"" \. -v ""${PWD}/reference"":""/reference"" \. -v ""${PWD}/output"":""/output"" \. realtimegenomics/rtg-tools mendelian \. -i ""/output/HG002_trio_merged.vcf.gz"" \. -o ""/output/HG002_trio_annotated.output.vcf.gz"" \. --pedigree=/reference/trio.ped \. -t /reference/GRCh38_no_alt_analysis_set.sdf \. | tee output/deepvariant.input_rtg_output.txt. ```. Have tried looking the wrong data and it seems the following GLNexus VCF Merge command is giving corrupted ```HG002_trio_merged.vcf.gz```. ```. docker run \. -v ""${PWD}/output"":""/output"" \. quay.io/mlin/glnexus:v1.2.7 \. /usr/local/bin/glnexus_cli \. --config DeepVariant_unfiltered \. /output/HG002.g.vcf.gz \. /output/HG003.g.vcf.gz \. /output/HG004.g.vcf.gz \. | docker run -i google/deepvariant:deeptrio-""${BIN_VERSION}-gpu"" \. bcftools view - \. | docker run -i google/deepvariant:deeptrio-""${BIN_VERSION}-gpu"" \. bgzip -c > output/HG002_trio_merged.vcf.gz. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/632
https://github.com/google/deepvariant/issues/632:0,security,Updat,Update,0,"Update : just now the ```rtg-tools format``` miraculously work using the following command. ```. docker run \. -v ""${PWD}/input"":""/input"" \. -v ""${PWD}/reference"":""/reference"" \. realtimegenomics/rtg-tools format \. -o /reference/GRCh38_no_alt_analysis_set.sdf ""/reference/GRCh38_no_alt_analysis_set.fasta"". ```. And this is the result . ```. Formatting FASTA data. Processing ""/reference/GRCh38_no_alt_analysis_set.fasta"". Detected: 'Human GRCh38 with UCSC naming', installing reference.txt. Input Data. Files : GRCh38_no_alt_analysis_set.fasta. Format : FASTA. Type : DNA. Number of sequences: 195. Total residues : 3099922541. Minimum length : 970. Mean length : 15897038. Maximum length : 248956422. Output Data. SDF-ID : 809c9a82-d8d5-477a-865b-772d28741815. Number of sequences: 195. Total residues : 3099922541. Minimum length : 970. Mean length : 15897038. Maximum length : 248956422. ```. However this ```rtg-tools mendelian``` still result in error of ```Error: An IO problem occurred: ""Not in GZIP format""``` when running the following command below. ```. docker run \. -v ""${PWD}/input"":""/input"" \. -v ""${PWD}/reference"":""/reference"" \. -v ""${PWD}/output"":""/output"" \. realtimegenomics/rtg-tools mendelian \. -i ""/output/HG002_trio_merged.vcf.gz"" \. -o ""/output/HG002_trio_annotated.output.vcf.gz"" \. --pedigree=/reference/trio.ped \. -t /reference/GRCh38_no_alt_analysis_set.sdf \. | tee output/deepvariant.input_rtg_output.txt. ```. Have tried looking the wrong data and it seems the following GLNexus VCF Merge command is giving corrupted ```HG002_trio_merged.vcf.gz```. ```. docker run \. -v ""${PWD}/output"":""/output"" \. quay.io/mlin/glnexus:v1.2.7 \. /usr/local/bin/glnexus_cli \. --config DeepVariant_unfiltered \. /output/HG002.g.vcf.gz \. /output/HG003.g.vcf.gz \. /output/HG004.g.vcf.gz \. | docker run -i google/deepvariant:deeptrio-""${BIN_VERSION}-gpu"" \. bcftools view - \. | docker run -i google/deepvariant:deeptrio-""${BIN_VERSION}-gpu"" \. bgzip -c > output/HG002_trio_merge",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/632
https://github.com/google/deepvariant/issues/632:424,security,Detect,Detected,424,"Update : just now the ```rtg-tools format``` miraculously work using the following command. ```. docker run \. -v ""${PWD}/input"":""/input"" \. -v ""${PWD}/reference"":""/reference"" \. realtimegenomics/rtg-tools format \. -o /reference/GRCh38_no_alt_analysis_set.sdf ""/reference/GRCh38_no_alt_analysis_set.fasta"". ```. And this is the result . ```. Formatting FASTA data. Processing ""/reference/GRCh38_no_alt_analysis_set.fasta"". Detected: 'Human GRCh38 with UCSC naming', installing reference.txt. Input Data. Files : GRCh38_no_alt_analysis_set.fasta. Format : FASTA. Type : DNA. Number of sequences: 195. Total residues : 3099922541. Minimum length : 970. Mean length : 15897038. Maximum length : 248956422. Output Data. SDF-ID : 809c9a82-d8d5-477a-865b-772d28741815. Number of sequences: 195. Total residues : 3099922541. Minimum length : 970. Mean length : 15897038. Maximum length : 248956422. ```. However this ```rtg-tools mendelian``` still result in error of ```Error: An IO problem occurred: ""Not in GZIP format""``` when running the following command below. ```. docker run \. -v ""${PWD}/input"":""/input"" \. -v ""${PWD}/reference"":""/reference"" \. -v ""${PWD}/output"":""/output"" \. realtimegenomics/rtg-tools mendelian \. -i ""/output/HG002_trio_merged.vcf.gz"" \. -o ""/output/HG002_trio_annotated.output.vcf.gz"" \. --pedigree=/reference/trio.ped \. -t /reference/GRCh38_no_alt_analysis_set.sdf \. | tee output/deepvariant.input_rtg_output.txt. ```. Have tried looking the wrong data and it seems the following GLNexus VCF Merge command is giving corrupted ```HG002_trio_merged.vcf.gz```. ```. docker run \. -v ""${PWD}/output"":""/output"" \. quay.io/mlin/glnexus:v1.2.7 \. /usr/local/bin/glnexus_cli \. --config DeepVariant_unfiltered \. /output/HG002.g.vcf.gz \. /output/HG003.g.vcf.gz \. /output/HG004.g.vcf.gz \. | docker run -i google/deepvariant:deeptrio-""${BIN_VERSION}-gpu"" \. bcftools view - \. | docker run -i google/deepvariant:deeptrio-""${BIN_VERSION}-gpu"" \. bgzip -c > output/HG002_trio_merge",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/632
https://github.com/google/deepvariant/issues/632:29,usability,tool,tools,29,"Update : just now the ```rtg-tools format``` miraculously work using the following command. ```. docker run \. -v ""${PWD}/input"":""/input"" \. -v ""${PWD}/reference"":""/reference"" \. realtimegenomics/rtg-tools format \. -o /reference/GRCh38_no_alt_analysis_set.sdf ""/reference/GRCh38_no_alt_analysis_set.fasta"". ```. And this is the result . ```. Formatting FASTA data. Processing ""/reference/GRCh38_no_alt_analysis_set.fasta"". Detected: 'Human GRCh38 with UCSC naming', installing reference.txt. Input Data. Files : GRCh38_no_alt_analysis_set.fasta. Format : FASTA. Type : DNA. Number of sequences: 195. Total residues : 3099922541. Minimum length : 970. Mean length : 15897038. Maximum length : 248956422. Output Data. SDF-ID : 809c9a82-d8d5-477a-865b-772d28741815. Number of sequences: 195. Total residues : 3099922541. Minimum length : 970. Mean length : 15897038. Maximum length : 248956422. ```. However this ```rtg-tools mendelian``` still result in error of ```Error: An IO problem occurred: ""Not in GZIP format""``` when running the following command below. ```. docker run \. -v ""${PWD}/input"":""/input"" \. -v ""${PWD}/reference"":""/reference"" \. -v ""${PWD}/output"":""/output"" \. realtimegenomics/rtg-tools mendelian \. -i ""/output/HG002_trio_merged.vcf.gz"" \. -o ""/output/HG002_trio_annotated.output.vcf.gz"" \. --pedigree=/reference/trio.ped \. -t /reference/GRCh38_no_alt_analysis_set.sdf \. | tee output/deepvariant.input_rtg_output.txt. ```. Have tried looking the wrong data and it seems the following GLNexus VCF Merge command is giving corrupted ```HG002_trio_merged.vcf.gz```. ```. docker run \. -v ""${PWD}/output"":""/output"" \. quay.io/mlin/glnexus:v1.2.7 \. /usr/local/bin/glnexus_cli \. --config DeepVariant_unfiltered \. /output/HG002.g.vcf.gz \. /output/HG003.g.vcf.gz \. /output/HG004.g.vcf.gz \. | docker run -i google/deepvariant:deeptrio-""${BIN_VERSION}-gpu"" \. bcftools view - \. | docker run -i google/deepvariant:deeptrio-""${BIN_VERSION}-gpu"" \. bgzip -c > output/HG002_trio_merge",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/632
https://github.com/google/deepvariant/issues/632:83,usability,command,command,83,"Update : just now the ```rtg-tools format``` miraculously work using the following command. ```. docker run \. -v ""${PWD}/input"":""/input"" \. -v ""${PWD}/reference"":""/reference"" \. realtimegenomics/rtg-tools format \. -o /reference/GRCh38_no_alt_analysis_set.sdf ""/reference/GRCh38_no_alt_analysis_set.fasta"". ```. And this is the result . ```. Formatting FASTA data. Processing ""/reference/GRCh38_no_alt_analysis_set.fasta"". Detected: 'Human GRCh38 with UCSC naming', installing reference.txt. Input Data. Files : GRCh38_no_alt_analysis_set.fasta. Format : FASTA. Type : DNA. Number of sequences: 195. Total residues : 3099922541. Minimum length : 970. Mean length : 15897038. Maximum length : 248956422. Output Data. SDF-ID : 809c9a82-d8d5-477a-865b-772d28741815. Number of sequences: 195. Total residues : 3099922541. Minimum length : 970. Mean length : 15897038. Maximum length : 248956422. ```. However this ```rtg-tools mendelian``` still result in error of ```Error: An IO problem occurred: ""Not in GZIP format""``` when running the following command below. ```. docker run \. -v ""${PWD}/input"":""/input"" \. -v ""${PWD}/reference"":""/reference"" \. -v ""${PWD}/output"":""/output"" \. realtimegenomics/rtg-tools mendelian \. -i ""/output/HG002_trio_merged.vcf.gz"" \. -o ""/output/HG002_trio_annotated.output.vcf.gz"" \. --pedigree=/reference/trio.ped \. -t /reference/GRCh38_no_alt_analysis_set.sdf \. | tee output/deepvariant.input_rtg_output.txt. ```. Have tried looking the wrong data and it seems the following GLNexus VCF Merge command is giving corrupted ```HG002_trio_merged.vcf.gz```. ```. docker run \. -v ""${PWD}/output"":""/output"" \. quay.io/mlin/glnexus:v1.2.7 \. /usr/local/bin/glnexus_cli \. --config DeepVariant_unfiltered \. /output/HG002.g.vcf.gz \. /output/HG003.g.vcf.gz \. /output/HG004.g.vcf.gz \. | docker run -i google/deepvariant:deeptrio-""${BIN_VERSION}-gpu"" \. bcftools view - \. | docker run -i google/deepvariant:deeptrio-""${BIN_VERSION}-gpu"" \. bgzip -c > output/HG002_trio_merge",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/632
https://github.com/google/deepvariant/issues/632:122,usability,input,input,122,"Update : just now the ```rtg-tools format``` miraculously work using the following command. ```. docker run \. -v ""${PWD}/input"":""/input"" \. -v ""${PWD}/reference"":""/reference"" \. realtimegenomics/rtg-tools format \. -o /reference/GRCh38_no_alt_analysis_set.sdf ""/reference/GRCh38_no_alt_analysis_set.fasta"". ```. And this is the result . ```. Formatting FASTA data. Processing ""/reference/GRCh38_no_alt_analysis_set.fasta"". Detected: 'Human GRCh38 with UCSC naming', installing reference.txt. Input Data. Files : GRCh38_no_alt_analysis_set.fasta. Format : FASTA. Type : DNA. Number of sequences: 195. Total residues : 3099922541. Minimum length : 970. Mean length : 15897038. Maximum length : 248956422. Output Data. SDF-ID : 809c9a82-d8d5-477a-865b-772d28741815. Number of sequences: 195. Total residues : 3099922541. Minimum length : 970. Mean length : 15897038. Maximum length : 248956422. ```. However this ```rtg-tools mendelian``` still result in error of ```Error: An IO problem occurred: ""Not in GZIP format""``` when running the following command below. ```. docker run \. -v ""${PWD}/input"":""/input"" \. -v ""${PWD}/reference"":""/reference"" \. -v ""${PWD}/output"":""/output"" \. realtimegenomics/rtg-tools mendelian \. -i ""/output/HG002_trio_merged.vcf.gz"" \. -o ""/output/HG002_trio_annotated.output.vcf.gz"" \. --pedigree=/reference/trio.ped \. -t /reference/GRCh38_no_alt_analysis_set.sdf \. | tee output/deepvariant.input_rtg_output.txt. ```. Have tried looking the wrong data and it seems the following GLNexus VCF Merge command is giving corrupted ```HG002_trio_merged.vcf.gz```. ```. docker run \. -v ""${PWD}/output"":""/output"" \. quay.io/mlin/glnexus:v1.2.7 \. /usr/local/bin/glnexus_cli \. --config DeepVariant_unfiltered \. /output/HG002.g.vcf.gz \. /output/HG003.g.vcf.gz \. /output/HG004.g.vcf.gz \. | docker run -i google/deepvariant:deeptrio-""${BIN_VERSION}-gpu"" \. bcftools view - \. | docker run -i google/deepvariant:deeptrio-""${BIN_VERSION}-gpu"" \. bgzip -c > output/HG002_trio_merge",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/632
https://github.com/google/deepvariant/issues/632:131,usability,input,input,131,"Update : just now the ```rtg-tools format``` miraculously work using the following command. ```. docker run \. -v ""${PWD}/input"":""/input"" \. -v ""${PWD}/reference"":""/reference"" \. realtimegenomics/rtg-tools format \. -o /reference/GRCh38_no_alt_analysis_set.sdf ""/reference/GRCh38_no_alt_analysis_set.fasta"". ```. And this is the result . ```. Formatting FASTA data. Processing ""/reference/GRCh38_no_alt_analysis_set.fasta"". Detected: 'Human GRCh38 with UCSC naming', installing reference.txt. Input Data. Files : GRCh38_no_alt_analysis_set.fasta. Format : FASTA. Type : DNA. Number of sequences: 195. Total residues : 3099922541. Minimum length : 970. Mean length : 15897038. Maximum length : 248956422. Output Data. SDF-ID : 809c9a82-d8d5-477a-865b-772d28741815. Number of sequences: 195. Total residues : 3099922541. Minimum length : 970. Mean length : 15897038. Maximum length : 248956422. ```. However this ```rtg-tools mendelian``` still result in error of ```Error: An IO problem occurred: ""Not in GZIP format""``` when running the following command below. ```. docker run \. -v ""${PWD}/input"":""/input"" \. -v ""${PWD}/reference"":""/reference"" \. -v ""${PWD}/output"":""/output"" \. realtimegenomics/rtg-tools mendelian \. -i ""/output/HG002_trio_merged.vcf.gz"" \. -o ""/output/HG002_trio_annotated.output.vcf.gz"" \. --pedigree=/reference/trio.ped \. -t /reference/GRCh38_no_alt_analysis_set.sdf \. | tee output/deepvariant.input_rtg_output.txt. ```. Have tried looking the wrong data and it seems the following GLNexus VCF Merge command is giving corrupted ```HG002_trio_merged.vcf.gz```. ```. docker run \. -v ""${PWD}/output"":""/output"" \. quay.io/mlin/glnexus:v1.2.7 \. /usr/local/bin/glnexus_cli \. --config DeepVariant_unfiltered \. /output/HG002.g.vcf.gz \. /output/HG003.g.vcf.gz \. /output/HG004.g.vcf.gz \. | docker run -i google/deepvariant:deeptrio-""${BIN_VERSION}-gpu"" \. bcftools view - \. | docker run -i google/deepvariant:deeptrio-""${BIN_VERSION}-gpu"" \. bgzip -c > output/HG002_trio_merge",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/632
https://github.com/google/deepvariant/issues/632:200,usability,tool,tools,200,"Update : just now the ```rtg-tools format``` miraculously work using the following command. ```. docker run \. -v ""${PWD}/input"":""/input"" \. -v ""${PWD}/reference"":""/reference"" \. realtimegenomics/rtg-tools format \. -o /reference/GRCh38_no_alt_analysis_set.sdf ""/reference/GRCh38_no_alt_analysis_set.fasta"". ```. And this is the result . ```. Formatting FASTA data. Processing ""/reference/GRCh38_no_alt_analysis_set.fasta"". Detected: 'Human GRCh38 with UCSC naming', installing reference.txt. Input Data. Files : GRCh38_no_alt_analysis_set.fasta. Format : FASTA. Type : DNA. Number of sequences: 195. Total residues : 3099922541. Minimum length : 970. Mean length : 15897038. Maximum length : 248956422. Output Data. SDF-ID : 809c9a82-d8d5-477a-865b-772d28741815. Number of sequences: 195. Total residues : 3099922541. Minimum length : 970. Mean length : 15897038. Maximum length : 248956422. ```. However this ```rtg-tools mendelian``` still result in error of ```Error: An IO problem occurred: ""Not in GZIP format""``` when running the following command below. ```. docker run \. -v ""${PWD}/input"":""/input"" \. -v ""${PWD}/reference"":""/reference"" \. -v ""${PWD}/output"":""/output"" \. realtimegenomics/rtg-tools mendelian \. -i ""/output/HG002_trio_merged.vcf.gz"" \. -o ""/output/HG002_trio_annotated.output.vcf.gz"" \. --pedigree=/reference/trio.ped \. -t /reference/GRCh38_no_alt_analysis_set.sdf \. | tee output/deepvariant.input_rtg_output.txt. ```. Have tried looking the wrong data and it seems the following GLNexus VCF Merge command is giving corrupted ```HG002_trio_merged.vcf.gz```. ```. docker run \. -v ""${PWD}/output"":""/output"" \. quay.io/mlin/glnexus:v1.2.7 \. /usr/local/bin/glnexus_cli \. --config DeepVariant_unfiltered \. /output/HG002.g.vcf.gz \. /output/HG003.g.vcf.gz \. /output/HG004.g.vcf.gz \. | docker run -i google/deepvariant:deeptrio-""${BIN_VERSION}-gpu"" \. bcftools view - \. | docker run -i google/deepvariant:deeptrio-""${BIN_VERSION}-gpu"" \. bgzip -c > output/HG002_trio_merge",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/632
https://github.com/google/deepvariant/issues/632:493,usability,Input,Input,493,"Update : just now the ```rtg-tools format``` miraculously work using the following command. ```. docker run \. -v ""${PWD}/input"":""/input"" \. -v ""${PWD}/reference"":""/reference"" \. realtimegenomics/rtg-tools format \. -o /reference/GRCh38_no_alt_analysis_set.sdf ""/reference/GRCh38_no_alt_analysis_set.fasta"". ```. And this is the result . ```. Formatting FASTA data. Processing ""/reference/GRCh38_no_alt_analysis_set.fasta"". Detected: 'Human GRCh38 with UCSC naming', installing reference.txt. Input Data. Files : GRCh38_no_alt_analysis_set.fasta. Format : FASTA. Type : DNA. Number of sequences: 195. Total residues : 3099922541. Minimum length : 970. Mean length : 15897038. Maximum length : 248956422. Output Data. SDF-ID : 809c9a82-d8d5-477a-865b-772d28741815. Number of sequences: 195. Total residues : 3099922541. Minimum length : 970. Mean length : 15897038. Maximum length : 248956422. ```. However this ```rtg-tools mendelian``` still result in error of ```Error: An IO problem occurred: ""Not in GZIP format""``` when running the following command below. ```. docker run \. -v ""${PWD}/input"":""/input"" \. -v ""${PWD}/reference"":""/reference"" \. -v ""${PWD}/output"":""/output"" \. realtimegenomics/rtg-tools mendelian \. -i ""/output/HG002_trio_merged.vcf.gz"" \. -o ""/output/HG002_trio_annotated.output.vcf.gz"" \. --pedigree=/reference/trio.ped \. -t /reference/GRCh38_no_alt_analysis_set.sdf \. | tee output/deepvariant.input_rtg_output.txt. ```. Have tried looking the wrong data and it seems the following GLNexus VCF Merge command is giving corrupted ```HG002_trio_merged.vcf.gz```. ```. docker run \. -v ""${PWD}/output"":""/output"" \. quay.io/mlin/glnexus:v1.2.7 \. /usr/local/bin/glnexus_cli \. --config DeepVariant_unfiltered \. /output/HG002.g.vcf.gz \. /output/HG003.g.vcf.gz \. /output/HG004.g.vcf.gz \. | docker run -i google/deepvariant:deeptrio-""${BIN_VERSION}-gpu"" \. bcftools view - \. | docker run -i google/deepvariant:deeptrio-""${BIN_VERSION}-gpu"" \. bgzip -c > output/HG002_trio_merge",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/632
https://github.com/google/deepvariant/issues/632:630,usability,Minim,Minimum,630,"Update : just now the ```rtg-tools format``` miraculously work using the following command. ```. docker run \. -v ""${PWD}/input"":""/input"" \. -v ""${PWD}/reference"":""/reference"" \. realtimegenomics/rtg-tools format \. -o /reference/GRCh38_no_alt_analysis_set.sdf ""/reference/GRCh38_no_alt_analysis_set.fasta"". ```. And this is the result . ```. Formatting FASTA data. Processing ""/reference/GRCh38_no_alt_analysis_set.fasta"". Detected: 'Human GRCh38 with UCSC naming', installing reference.txt. Input Data. Files : GRCh38_no_alt_analysis_set.fasta. Format : FASTA. Type : DNA. Number of sequences: 195. Total residues : 3099922541. Minimum length : 970. Mean length : 15897038. Maximum length : 248956422. Output Data. SDF-ID : 809c9a82-d8d5-477a-865b-772d28741815. Number of sequences: 195. Total residues : 3099922541. Minimum length : 970. Mean length : 15897038. Maximum length : 248956422. ```. However this ```rtg-tools mendelian``` still result in error of ```Error: An IO problem occurred: ""Not in GZIP format""``` when running the following command below. ```. docker run \. -v ""${PWD}/input"":""/input"" \. -v ""${PWD}/reference"":""/reference"" \. -v ""${PWD}/output"":""/output"" \. realtimegenomics/rtg-tools mendelian \. -i ""/output/HG002_trio_merged.vcf.gz"" \. -o ""/output/HG002_trio_annotated.output.vcf.gz"" \. --pedigree=/reference/trio.ped \. -t /reference/GRCh38_no_alt_analysis_set.sdf \. | tee output/deepvariant.input_rtg_output.txt. ```. Have tried looking the wrong data and it seems the following GLNexus VCF Merge command is giving corrupted ```HG002_trio_merged.vcf.gz```. ```. docker run \. -v ""${PWD}/output"":""/output"" \. quay.io/mlin/glnexus:v1.2.7 \. /usr/local/bin/glnexus_cli \. --config DeepVariant_unfiltered \. /output/HG002.g.vcf.gz \. /output/HG003.g.vcf.gz \. /output/HG004.g.vcf.gz \. | docker run -i google/deepvariant:deeptrio-""${BIN_VERSION}-gpu"" \. bcftools view - \. | docker run -i google/deepvariant:deeptrio-""${BIN_VERSION}-gpu"" \. bgzip -c > output/HG002_trio_merge",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/632
https://github.com/google/deepvariant/issues/632:819,usability,Minim,Minimum,819,"Update : just now the ```rtg-tools format``` miraculously work using the following command. ```. docker run \. -v ""${PWD}/input"":""/input"" \. -v ""${PWD}/reference"":""/reference"" \. realtimegenomics/rtg-tools format \. -o /reference/GRCh38_no_alt_analysis_set.sdf ""/reference/GRCh38_no_alt_analysis_set.fasta"". ```. And this is the result . ```. Formatting FASTA data. Processing ""/reference/GRCh38_no_alt_analysis_set.fasta"". Detected: 'Human GRCh38 with UCSC naming', installing reference.txt. Input Data. Files : GRCh38_no_alt_analysis_set.fasta. Format : FASTA. Type : DNA. Number of sequences: 195. Total residues : 3099922541. Minimum length : 970. Mean length : 15897038. Maximum length : 248956422. Output Data. SDF-ID : 809c9a82-d8d5-477a-865b-772d28741815. Number of sequences: 195. Total residues : 3099922541. Minimum length : 970. Mean length : 15897038. Maximum length : 248956422. ```. However this ```rtg-tools mendelian``` still result in error of ```Error: An IO problem occurred: ""Not in GZIP format""``` when running the following command below. ```. docker run \. -v ""${PWD}/input"":""/input"" \. -v ""${PWD}/reference"":""/reference"" \. -v ""${PWD}/output"":""/output"" \. realtimegenomics/rtg-tools mendelian \. -i ""/output/HG002_trio_merged.vcf.gz"" \. -o ""/output/HG002_trio_annotated.output.vcf.gz"" \. --pedigree=/reference/trio.ped \. -t /reference/GRCh38_no_alt_analysis_set.sdf \. | tee output/deepvariant.input_rtg_output.txt. ```. Have tried looking the wrong data and it seems the following GLNexus VCF Merge command is giving corrupted ```HG002_trio_merged.vcf.gz```. ```. docker run \. -v ""${PWD}/output"":""/output"" \. quay.io/mlin/glnexus:v1.2.7 \. /usr/local/bin/glnexus_cli \. --config DeepVariant_unfiltered \. /output/HG002.g.vcf.gz \. /output/HG003.g.vcf.gz \. /output/HG004.g.vcf.gz \. | docker run -i google/deepvariant:deeptrio-""${BIN_VERSION}-gpu"" \. bcftools view - \. | docker run -i google/deepvariant:deeptrio-""${BIN_VERSION}-gpu"" \. bgzip -c > output/HG002_trio_merge",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/632
https://github.com/google/deepvariant/issues/632:918,usability,tool,tools,918,"Update : just now the ```rtg-tools format``` miraculously work using the following command. ```. docker run \. -v ""${PWD}/input"":""/input"" \. -v ""${PWD}/reference"":""/reference"" \. realtimegenomics/rtg-tools format \. -o /reference/GRCh38_no_alt_analysis_set.sdf ""/reference/GRCh38_no_alt_analysis_set.fasta"". ```. And this is the result . ```. Formatting FASTA data. Processing ""/reference/GRCh38_no_alt_analysis_set.fasta"". Detected: 'Human GRCh38 with UCSC naming', installing reference.txt. Input Data. Files : GRCh38_no_alt_analysis_set.fasta. Format : FASTA. Type : DNA. Number of sequences: 195. Total residues : 3099922541. Minimum length : 970. Mean length : 15897038. Maximum length : 248956422. Output Data. SDF-ID : 809c9a82-d8d5-477a-865b-772d28741815. Number of sequences: 195. Total residues : 3099922541. Minimum length : 970. Mean length : 15897038. Maximum length : 248956422. ```. However this ```rtg-tools mendelian``` still result in error of ```Error: An IO problem occurred: ""Not in GZIP format""``` when running the following command below. ```. docker run \. -v ""${PWD}/input"":""/input"" \. -v ""${PWD}/reference"":""/reference"" \. -v ""${PWD}/output"":""/output"" \. realtimegenomics/rtg-tools mendelian \. -i ""/output/HG002_trio_merged.vcf.gz"" \. -o ""/output/HG002_trio_annotated.output.vcf.gz"" \. --pedigree=/reference/trio.ped \. -t /reference/GRCh38_no_alt_analysis_set.sdf \. | tee output/deepvariant.input_rtg_output.txt. ```. Have tried looking the wrong data and it seems the following GLNexus VCF Merge command is giving corrupted ```HG002_trio_merged.vcf.gz```. ```. docker run \. -v ""${PWD}/output"":""/output"" \. quay.io/mlin/glnexus:v1.2.7 \. /usr/local/bin/glnexus_cli \. --config DeepVariant_unfiltered \. /output/HG002.g.vcf.gz \. /output/HG003.g.vcf.gz \. /output/HG004.g.vcf.gz \. | docker run -i google/deepvariant:deeptrio-""${BIN_VERSION}-gpu"" \. bcftools view - \. | docker run -i google/deepvariant:deeptrio-""${BIN_VERSION}-gpu"" \. bgzip -c > output/HG002_trio_merge",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/632
https://github.com/google/deepvariant/issues/632:953,usability,error,error,953,"Update : just now the ```rtg-tools format``` miraculously work using the following command. ```. docker run \. -v ""${PWD}/input"":""/input"" \. -v ""${PWD}/reference"":""/reference"" \. realtimegenomics/rtg-tools format \. -o /reference/GRCh38_no_alt_analysis_set.sdf ""/reference/GRCh38_no_alt_analysis_set.fasta"". ```. And this is the result . ```. Formatting FASTA data. Processing ""/reference/GRCh38_no_alt_analysis_set.fasta"". Detected: 'Human GRCh38 with UCSC naming', installing reference.txt. Input Data. Files : GRCh38_no_alt_analysis_set.fasta. Format : FASTA. Type : DNA. Number of sequences: 195. Total residues : 3099922541. Minimum length : 970. Mean length : 15897038. Maximum length : 248956422. Output Data. SDF-ID : 809c9a82-d8d5-477a-865b-772d28741815. Number of sequences: 195. Total residues : 3099922541. Minimum length : 970. Mean length : 15897038. Maximum length : 248956422. ```. However this ```rtg-tools mendelian``` still result in error of ```Error: An IO problem occurred: ""Not in GZIP format""``` when running the following command below. ```. docker run \. -v ""${PWD}/input"":""/input"" \. -v ""${PWD}/reference"":""/reference"" \. -v ""${PWD}/output"":""/output"" \. realtimegenomics/rtg-tools mendelian \. -i ""/output/HG002_trio_merged.vcf.gz"" \. -o ""/output/HG002_trio_annotated.output.vcf.gz"" \. --pedigree=/reference/trio.ped \. -t /reference/GRCh38_no_alt_analysis_set.sdf \. | tee output/deepvariant.input_rtg_output.txt. ```. Have tried looking the wrong data and it seems the following GLNexus VCF Merge command is giving corrupted ```HG002_trio_merged.vcf.gz```. ```. docker run \. -v ""${PWD}/output"":""/output"" \. quay.io/mlin/glnexus:v1.2.7 \. /usr/local/bin/glnexus_cli \. --config DeepVariant_unfiltered \. /output/HG002.g.vcf.gz \. /output/HG003.g.vcf.gz \. /output/HG004.g.vcf.gz \. | docker run -i google/deepvariant:deeptrio-""${BIN_VERSION}-gpu"" \. bcftools view - \. | docker run -i google/deepvariant:deeptrio-""${BIN_VERSION}-gpu"" \. bgzip -c > output/HG002_trio_merge",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/632
https://github.com/google/deepvariant/issues/632:965,usability,Error,Error,965,"Update : just now the ```rtg-tools format``` miraculously work using the following command. ```. docker run \. -v ""${PWD}/input"":""/input"" \. -v ""${PWD}/reference"":""/reference"" \. realtimegenomics/rtg-tools format \. -o /reference/GRCh38_no_alt_analysis_set.sdf ""/reference/GRCh38_no_alt_analysis_set.fasta"". ```. And this is the result . ```. Formatting FASTA data. Processing ""/reference/GRCh38_no_alt_analysis_set.fasta"". Detected: 'Human GRCh38 with UCSC naming', installing reference.txt. Input Data. Files : GRCh38_no_alt_analysis_set.fasta. Format : FASTA. Type : DNA. Number of sequences: 195. Total residues : 3099922541. Minimum length : 970. Mean length : 15897038. Maximum length : 248956422. Output Data. SDF-ID : 809c9a82-d8d5-477a-865b-772d28741815. Number of sequences: 195. Total residues : 3099922541. Minimum length : 970. Mean length : 15897038. Maximum length : 248956422. ```. However this ```rtg-tools mendelian``` still result in error of ```Error: An IO problem occurred: ""Not in GZIP format""``` when running the following command below. ```. docker run \. -v ""${PWD}/input"":""/input"" \. -v ""${PWD}/reference"":""/reference"" \. -v ""${PWD}/output"":""/output"" \. realtimegenomics/rtg-tools mendelian \. -i ""/output/HG002_trio_merged.vcf.gz"" \. -o ""/output/HG002_trio_annotated.output.vcf.gz"" \. --pedigree=/reference/trio.ped \. -t /reference/GRCh38_no_alt_analysis_set.sdf \. | tee output/deepvariant.input_rtg_output.txt. ```. Have tried looking the wrong data and it seems the following GLNexus VCF Merge command is giving corrupted ```HG002_trio_merged.vcf.gz```. ```. docker run \. -v ""${PWD}/output"":""/output"" \. quay.io/mlin/glnexus:v1.2.7 \. /usr/local/bin/glnexus_cli \. --config DeepVariant_unfiltered \. /output/HG002.g.vcf.gz \. /output/HG003.g.vcf.gz \. /output/HG004.g.vcf.gz \. | docker run -i google/deepvariant:deeptrio-""${BIN_VERSION}-gpu"" \. bcftools view - \. | docker run -i google/deepvariant:deeptrio-""${BIN_VERSION}-gpu"" \. bgzip -c > output/HG002_trio_merge",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/632
https://github.com/google/deepvariant/issues/632:1047,usability,command,command,1047," now the ```rtg-tools format``` miraculously work using the following command. ```. docker run \. -v ""${PWD}/input"":""/input"" \. -v ""${PWD}/reference"":""/reference"" \. realtimegenomics/rtg-tools format \. -o /reference/GRCh38_no_alt_analysis_set.sdf ""/reference/GRCh38_no_alt_analysis_set.fasta"". ```. And this is the result . ```. Formatting FASTA data. Processing ""/reference/GRCh38_no_alt_analysis_set.fasta"". Detected: 'Human GRCh38 with UCSC naming', installing reference.txt. Input Data. Files : GRCh38_no_alt_analysis_set.fasta. Format : FASTA. Type : DNA. Number of sequences: 195. Total residues : 3099922541. Minimum length : 970. Mean length : 15897038. Maximum length : 248956422. Output Data. SDF-ID : 809c9a82-d8d5-477a-865b-772d28741815. Number of sequences: 195. Total residues : 3099922541. Minimum length : 970. Mean length : 15897038. Maximum length : 248956422. ```. However this ```rtg-tools mendelian``` still result in error of ```Error: An IO problem occurred: ""Not in GZIP format""``` when running the following command below. ```. docker run \. -v ""${PWD}/input"":""/input"" \. -v ""${PWD}/reference"":""/reference"" \. -v ""${PWD}/output"":""/output"" \. realtimegenomics/rtg-tools mendelian \. -i ""/output/HG002_trio_merged.vcf.gz"" \. -o ""/output/HG002_trio_annotated.output.vcf.gz"" \. --pedigree=/reference/trio.ped \. -t /reference/GRCh38_no_alt_analysis_set.sdf \. | tee output/deepvariant.input_rtg_output.txt. ```. Have tried looking the wrong data and it seems the following GLNexus VCF Merge command is giving corrupted ```HG002_trio_merged.vcf.gz```. ```. docker run \. -v ""${PWD}/output"":""/output"" \. quay.io/mlin/glnexus:v1.2.7 \. /usr/local/bin/glnexus_cli \. --config DeepVariant_unfiltered \. /output/HG002.g.vcf.gz \. /output/HG003.g.vcf.gz \. /output/HG004.g.vcf.gz \. | docker run -i google/deepvariant:deeptrio-""${BIN_VERSION}-gpu"" \. bcftools view - \. | docker run -i google/deepvariant:deeptrio-""${BIN_VERSION}-gpu"" \. bgzip -c > output/HG002_trio_merged.vcf.gz. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/632
https://github.com/google/deepvariant/issues/632:1092,usability,input,input,1092," now the ```rtg-tools format``` miraculously work using the following command. ```. docker run \. -v ""${PWD}/input"":""/input"" \. -v ""${PWD}/reference"":""/reference"" \. realtimegenomics/rtg-tools format \. -o /reference/GRCh38_no_alt_analysis_set.sdf ""/reference/GRCh38_no_alt_analysis_set.fasta"". ```. And this is the result . ```. Formatting FASTA data. Processing ""/reference/GRCh38_no_alt_analysis_set.fasta"". Detected: 'Human GRCh38 with UCSC naming', installing reference.txt. Input Data. Files : GRCh38_no_alt_analysis_set.fasta. Format : FASTA. Type : DNA. Number of sequences: 195. Total residues : 3099922541. Minimum length : 970. Mean length : 15897038. Maximum length : 248956422. Output Data. SDF-ID : 809c9a82-d8d5-477a-865b-772d28741815. Number of sequences: 195. Total residues : 3099922541. Minimum length : 970. Mean length : 15897038. Maximum length : 248956422. ```. However this ```rtg-tools mendelian``` still result in error of ```Error: An IO problem occurred: ""Not in GZIP format""``` when running the following command below. ```. docker run \. -v ""${PWD}/input"":""/input"" \. -v ""${PWD}/reference"":""/reference"" \. -v ""${PWD}/output"":""/output"" \. realtimegenomics/rtg-tools mendelian \. -i ""/output/HG002_trio_merged.vcf.gz"" \. -o ""/output/HG002_trio_annotated.output.vcf.gz"" \. --pedigree=/reference/trio.ped \. -t /reference/GRCh38_no_alt_analysis_set.sdf \. | tee output/deepvariant.input_rtg_output.txt. ```. Have tried looking the wrong data and it seems the following GLNexus VCF Merge command is giving corrupted ```HG002_trio_merged.vcf.gz```. ```. docker run \. -v ""${PWD}/output"":""/output"" \. quay.io/mlin/glnexus:v1.2.7 \. /usr/local/bin/glnexus_cli \. --config DeepVariant_unfiltered \. /output/HG002.g.vcf.gz \. /output/HG003.g.vcf.gz \. /output/HG004.g.vcf.gz \. | docker run -i google/deepvariant:deeptrio-""${BIN_VERSION}-gpu"" \. bcftools view - \. | docker run -i google/deepvariant:deeptrio-""${BIN_VERSION}-gpu"" \. bgzip -c > output/HG002_trio_merged.vcf.gz. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/632
https://github.com/google/deepvariant/issues/632:1101,usability,input,input,1101," now the ```rtg-tools format``` miraculously work using the following command. ```. docker run \. -v ""${PWD}/input"":""/input"" \. -v ""${PWD}/reference"":""/reference"" \. realtimegenomics/rtg-tools format \. -o /reference/GRCh38_no_alt_analysis_set.sdf ""/reference/GRCh38_no_alt_analysis_set.fasta"". ```. And this is the result . ```. Formatting FASTA data. Processing ""/reference/GRCh38_no_alt_analysis_set.fasta"". Detected: 'Human GRCh38 with UCSC naming', installing reference.txt. Input Data. Files : GRCh38_no_alt_analysis_set.fasta. Format : FASTA. Type : DNA. Number of sequences: 195. Total residues : 3099922541. Minimum length : 970. Mean length : 15897038. Maximum length : 248956422. Output Data. SDF-ID : 809c9a82-d8d5-477a-865b-772d28741815. Number of sequences: 195. Total residues : 3099922541. Minimum length : 970. Mean length : 15897038. Maximum length : 248956422. ```. However this ```rtg-tools mendelian``` still result in error of ```Error: An IO problem occurred: ""Not in GZIP format""``` when running the following command below. ```. docker run \. -v ""${PWD}/input"":""/input"" \. -v ""${PWD}/reference"":""/reference"" \. -v ""${PWD}/output"":""/output"" \. realtimegenomics/rtg-tools mendelian \. -i ""/output/HG002_trio_merged.vcf.gz"" \. -o ""/output/HG002_trio_annotated.output.vcf.gz"" \. --pedigree=/reference/trio.ped \. -t /reference/GRCh38_no_alt_analysis_set.sdf \. | tee output/deepvariant.input_rtg_output.txt. ```. Have tried looking the wrong data and it seems the following GLNexus VCF Merge command is giving corrupted ```HG002_trio_merged.vcf.gz```. ```. docker run \. -v ""${PWD}/output"":""/output"" \. quay.io/mlin/glnexus:v1.2.7 \. /usr/local/bin/glnexus_cli \. --config DeepVariant_unfiltered \. /output/HG002.g.vcf.gz \. /output/HG003.g.vcf.gz \. /output/HG004.g.vcf.gz \. | docker run -i google/deepvariant:deeptrio-""${BIN_VERSION}-gpu"" \. bcftools view - \. | docker run -i google/deepvariant:deeptrio-""${BIN_VERSION}-gpu"" \. bgzip -c > output/HG002_trio_merged.vcf.gz. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/632
https://github.com/google/deepvariant/issues/632:1202,usability,tool,tools,1202," now the ```rtg-tools format``` miraculously work using the following command. ```. docker run \. -v ""${PWD}/input"":""/input"" \. -v ""${PWD}/reference"":""/reference"" \. realtimegenomics/rtg-tools format \. -o /reference/GRCh38_no_alt_analysis_set.sdf ""/reference/GRCh38_no_alt_analysis_set.fasta"". ```. And this is the result . ```. Formatting FASTA data. Processing ""/reference/GRCh38_no_alt_analysis_set.fasta"". Detected: 'Human GRCh38 with UCSC naming', installing reference.txt. Input Data. Files : GRCh38_no_alt_analysis_set.fasta. Format : FASTA. Type : DNA. Number of sequences: 195. Total residues : 3099922541. Minimum length : 970. Mean length : 15897038. Maximum length : 248956422. Output Data. SDF-ID : 809c9a82-d8d5-477a-865b-772d28741815. Number of sequences: 195. Total residues : 3099922541. Minimum length : 970. Mean length : 15897038. Maximum length : 248956422. ```. However this ```rtg-tools mendelian``` still result in error of ```Error: An IO problem occurred: ""Not in GZIP format""``` when running the following command below. ```. docker run \. -v ""${PWD}/input"":""/input"" \. -v ""${PWD}/reference"":""/reference"" \. -v ""${PWD}/output"":""/output"" \. realtimegenomics/rtg-tools mendelian \. -i ""/output/HG002_trio_merged.vcf.gz"" \. -o ""/output/HG002_trio_annotated.output.vcf.gz"" \. --pedigree=/reference/trio.ped \. -t /reference/GRCh38_no_alt_analysis_set.sdf \. | tee output/deepvariant.input_rtg_output.txt. ```. Have tried looking the wrong data and it seems the following GLNexus VCF Merge command is giving corrupted ```HG002_trio_merged.vcf.gz```. ```. docker run \. -v ""${PWD}/output"":""/output"" \. quay.io/mlin/glnexus:v1.2.7 \. /usr/local/bin/glnexus_cli \. --config DeepVariant_unfiltered \. /output/HG002.g.vcf.gz \. /output/HG003.g.vcf.gz \. /output/HG004.g.vcf.gz \. | docker run -i google/deepvariant:deeptrio-""${BIN_VERSION}-gpu"" \. bcftools view - \. | docker run -i google/deepvariant:deeptrio-""${BIN_VERSION}-gpu"" \. bgzip -c > output/HG002_trio_merged.vcf.gz. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/632
https://github.com/google/deepvariant/issues/632:1526,usability,command,command,1526," now the ```rtg-tools format``` miraculously work using the following command. ```. docker run \. -v ""${PWD}/input"":""/input"" \. -v ""${PWD}/reference"":""/reference"" \. realtimegenomics/rtg-tools format \. -o /reference/GRCh38_no_alt_analysis_set.sdf ""/reference/GRCh38_no_alt_analysis_set.fasta"". ```. And this is the result . ```. Formatting FASTA data. Processing ""/reference/GRCh38_no_alt_analysis_set.fasta"". Detected: 'Human GRCh38 with UCSC naming', installing reference.txt. Input Data. Files : GRCh38_no_alt_analysis_set.fasta. Format : FASTA. Type : DNA. Number of sequences: 195. Total residues : 3099922541. Minimum length : 970. Mean length : 15897038. Maximum length : 248956422. Output Data. SDF-ID : 809c9a82-d8d5-477a-865b-772d28741815. Number of sequences: 195. Total residues : 3099922541. Minimum length : 970. Mean length : 15897038. Maximum length : 248956422. ```. However this ```rtg-tools mendelian``` still result in error of ```Error: An IO problem occurred: ""Not in GZIP format""``` when running the following command below. ```. docker run \. -v ""${PWD}/input"":""/input"" \. -v ""${PWD}/reference"":""/reference"" \. -v ""${PWD}/output"":""/output"" \. realtimegenomics/rtg-tools mendelian \. -i ""/output/HG002_trio_merged.vcf.gz"" \. -o ""/output/HG002_trio_annotated.output.vcf.gz"" \. --pedigree=/reference/trio.ped \. -t /reference/GRCh38_no_alt_analysis_set.sdf \. | tee output/deepvariant.input_rtg_output.txt. ```. Have tried looking the wrong data and it seems the following GLNexus VCF Merge command is giving corrupted ```HG002_trio_merged.vcf.gz```. ```. docker run \. -v ""${PWD}/output"":""/output"" \. quay.io/mlin/glnexus:v1.2.7 \. /usr/local/bin/glnexus_cli \. --config DeepVariant_unfiltered \. /output/HG002.g.vcf.gz \. /output/HG003.g.vcf.gz \. /output/HG004.g.vcf.gz \. | docker run -i google/deepvariant:deeptrio-""${BIN_VERSION}-gpu"" \. bcftools view - \. | docker run -i google/deepvariant:deeptrio-""${BIN_VERSION}-gpu"" \. bgzip -c > output/HG002_trio_merged.vcf.gz. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/632
https://github.com/google/deepvariant/issues/632:68,availability,error,error,68,"Solve the problem --> the pipeline of using deeptrio bcftools cause error. Installing bcftools and using the following script works... I hope this help to improve the pipelining. ```. docker run \. -v ""${PWD}/output"":""/output"" \. quay.io/mlin/glnexus:v1.2.7 \. /usr/local/bin/glnexus_cli \. --config DeepVariant_unfiltered \. /output/HG002.g.vcf.gz \. /output/HG003.g.vcf.gz \. /output/HG004.g.vcf.gz \. | bcftools view -Oz -o ${PWD}/output/HG002_trio_merged.vcf.gz . ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/632
https://github.com/google/deepvariant/issues/632:26,deployability,pipelin,pipeline,26,"Solve the problem --> the pipeline of using deeptrio bcftools cause error. Installing bcftools and using the following script works... I hope this help to improve the pipelining. ```. docker run \. -v ""${PWD}/output"":""/output"" \. quay.io/mlin/glnexus:v1.2.7 \. /usr/local/bin/glnexus_cli \. --config DeepVariant_unfiltered \. /output/HG002.g.vcf.gz \. /output/HG003.g.vcf.gz \. /output/HG004.g.vcf.gz \. | bcftools view -Oz -o ${PWD}/output/HG002_trio_merged.vcf.gz . ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/632
https://github.com/google/deepvariant/issues/632:75,deployability,Instal,Installing,75,"Solve the problem --> the pipeline of using deeptrio bcftools cause error. Installing bcftools and using the following script works... I hope this help to improve the pipelining. ```. docker run \. -v ""${PWD}/output"":""/output"" \. quay.io/mlin/glnexus:v1.2.7 \. /usr/local/bin/glnexus_cli \. --config DeepVariant_unfiltered \. /output/HG002.g.vcf.gz \. /output/HG003.g.vcf.gz \. /output/HG004.g.vcf.gz \. | bcftools view -Oz -o ${PWD}/output/HG002_trio_merged.vcf.gz . ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/632
https://github.com/google/deepvariant/issues/632:167,deployability,pipelin,pipelining,167,"Solve the problem --> the pipeline of using deeptrio bcftools cause error. Installing bcftools and using the following script works... I hope this help to improve the pipelining. ```. docker run \. -v ""${PWD}/output"":""/output"" \. quay.io/mlin/glnexus:v1.2.7 \. /usr/local/bin/glnexus_cli \. --config DeepVariant_unfiltered \. /output/HG002.g.vcf.gz \. /output/HG003.g.vcf.gz \. /output/HG004.g.vcf.gz \. | bcftools view -Oz -o ${PWD}/output/HG002_trio_merged.vcf.gz . ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/632
https://github.com/google/deepvariant/issues/632:26,integrability,pipelin,pipeline,26,"Solve the problem --> the pipeline of using deeptrio bcftools cause error. Installing bcftools and using the following script works... I hope this help to improve the pipelining. ```. docker run \. -v ""${PWD}/output"":""/output"" \. quay.io/mlin/glnexus:v1.2.7 \. /usr/local/bin/glnexus_cli \. --config DeepVariant_unfiltered \. /output/HG002.g.vcf.gz \. /output/HG003.g.vcf.gz \. /output/HG004.g.vcf.gz \. | bcftools view -Oz -o ${PWD}/output/HG002_trio_merged.vcf.gz . ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/632
https://github.com/google/deepvariant/issues/632:167,integrability,pipelin,pipelining,167,"Solve the problem --> the pipeline of using deeptrio bcftools cause error. Installing bcftools and using the following script works... I hope this help to improve the pipelining. ```. docker run \. -v ""${PWD}/output"":""/output"" \. quay.io/mlin/glnexus:v1.2.7 \. /usr/local/bin/glnexus_cli \. --config DeepVariant_unfiltered \. /output/HG002.g.vcf.gz \. /output/HG003.g.vcf.gz \. /output/HG004.g.vcf.gz \. | bcftools view -Oz -o ${PWD}/output/HG002_trio_merged.vcf.gz . ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/632
https://github.com/google/deepvariant/issues/632:68,performance,error,error,68,"Solve the problem --> the pipeline of using deeptrio bcftools cause error. Installing bcftools and using the following script works... I hope this help to improve the pipelining. ```. docker run \. -v ""${PWD}/output"":""/output"" \. quay.io/mlin/glnexus:v1.2.7 \. /usr/local/bin/glnexus_cli \. --config DeepVariant_unfiltered \. /output/HG002.g.vcf.gz \. /output/HG003.g.vcf.gz \. /output/HG004.g.vcf.gz \. | bcftools view -Oz -o ${PWD}/output/HG002_trio_merged.vcf.gz . ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/632
https://github.com/google/deepvariant/issues/632:68,safety,error,error,68,"Solve the problem --> the pipeline of using deeptrio bcftools cause error. Installing bcftools and using the following script works... I hope this help to improve the pipelining. ```. docker run \. -v ""${PWD}/output"":""/output"" \. quay.io/mlin/glnexus:v1.2.7 \. /usr/local/bin/glnexus_cli \. --config DeepVariant_unfiltered \. /output/HG002.g.vcf.gz \. /output/HG003.g.vcf.gz \. /output/HG004.g.vcf.gz \. | bcftools view -Oz -o ${PWD}/output/HG002_trio_merged.vcf.gz . ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/632
https://github.com/google/deepvariant/issues/632:68,usability,error,error,68,"Solve the problem --> the pipeline of using deeptrio bcftools cause error. Installing bcftools and using the following script works... I hope this help to improve the pipelining. ```. docker run \. -v ""${PWD}/output"":""/output"" \. quay.io/mlin/glnexus:v1.2.7 \. /usr/local/bin/glnexus_cli \. --config DeepVariant_unfiltered \. /output/HG002.g.vcf.gz \. /output/HG003.g.vcf.gz \. /output/HG004.g.vcf.gz \. | bcftools view -Oz -o ${PWD}/output/HG002_trio_merged.vcf.gz . ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/632
https://github.com/google/deepvariant/issues/632:147,usability,help,help,147,"Solve the problem --> the pipeline of using deeptrio bcftools cause error. Installing bcftools and using the following script works... I hope this help to improve the pipelining. ```. docker run \. -v ""${PWD}/output"":""/output"" \. quay.io/mlin/glnexus:v1.2.7 \. /usr/local/bin/glnexus_cli \. --config DeepVariant_unfiltered \. /output/HG002.g.vcf.gz \. /output/HG003.g.vcf.gz \. /output/HG004.g.vcf.gz \. | bcftools view -Oz -o ${PWD}/output/HG002_trio_merged.vcf.gz . ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/632
https://github.com/google/deepvariant/issues/632:15,deployability,updat,update,15,Thanks for the update @ivanwilliammd !,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/632
https://github.com/google/deepvariant/issues/632:15,safety,updat,update,15,Thanks for the update @ivanwilliammd !,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/632
https://github.com/google/deepvariant/issues/632:15,security,updat,update,15,Thanks for the update @ivanwilliammd !,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/632
https://github.com/google/deepvariant/issues/633:440,availability,down,downstream,440,"Hello,. I think it'd be helpful if you can share individual gVCF calls for the problematic site before merging (hopefully with a fewer number of samples) so that we can try to reproduce it. As you said we'll have to ask GLnexus maintainers about this issue. I found this issue on the GLnexus repo https://github.com/dnanexus-rnd/GLnexus/issues/286 - I'd recommend adding a reproducible example there as well. About the PASS filter, if your downstream application explicitly requires having that filter value, I'd recommend using tools like `bcftools` to add it. I think `bcftools annotate --rename-annots` would work based on this page, using `FILTER/. PASS` as the mapping: https://samtools.github.io/bcftools/bcftools.html#annotate. Thank you. Best,. Ted",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/633
https://github.com/google/deepvariant/issues/633:424,integrability,filter,filter,424,"Hello,. I think it'd be helpful if you can share individual gVCF calls for the problematic site before merging (hopefully with a fewer number of samples) so that we can try to reproduce it. As you said we'll have to ask GLnexus maintainers about this issue. I found this issue on the GLnexus repo https://github.com/dnanexus-rnd/GLnexus/issues/286 - I'd recommend adding a reproducible example there as well. About the PASS filter, if your downstream application explicitly requires having that filter value, I'd recommend using tools like `bcftools` to add it. I think `bcftools annotate --rename-annots` would work based on this page, using `FILTER/. PASS` as the mapping: https://samtools.github.io/bcftools/bcftools.html#annotate. Thank you. Best,. Ted",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/633
https://github.com/google/deepvariant/issues/633:495,integrability,filter,filter,495,"Hello,. I think it'd be helpful if you can share individual gVCF calls for the problematic site before merging (hopefully with a fewer number of samples) so that we can try to reproduce it. As you said we'll have to ask GLnexus maintainers about this issue. I found this issue on the GLnexus repo https://github.com/dnanexus-rnd/GLnexus/issues/286 - I'd recommend adding a reproducible example there as well. About the PASS filter, if your downstream application explicitly requires having that filter value, I'd recommend using tools like `bcftools` to add it. I think `bcftools annotate --rename-annots` would work based on this page, using `FILTER/. PASS` as the mapping: https://samtools.github.io/bcftools/bcftools.html#annotate. Thank you. Best,. Ted",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/633
https://github.com/google/deepvariant/issues/633:644,integrability,FILTER,FILTER,644,"Hello,. I think it'd be helpful if you can share individual gVCF calls for the problematic site before merging (hopefully with a fewer number of samples) so that we can try to reproduce it. As you said we'll have to ask GLnexus maintainers about this issue. I found this issue on the GLnexus repo https://github.com/dnanexus-rnd/GLnexus/issues/286 - I'd recommend adding a reproducible example there as well. About the PASS filter, if your downstream application explicitly requires having that filter value, I'd recommend using tools like `bcftools` to add it. I think `bcftools annotate --rename-annots` would work based on this page, using `FILTER/. PASS` as the mapping: https://samtools.github.io/bcftools/bcftools.html#annotate. Thank you. Best,. Ted",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/633
https://github.com/google/deepvariant/issues/633:43,interoperability,share,share,43,"Hello,. I think it'd be helpful if you can share individual gVCF calls for the problematic site before merging (hopefully with a fewer number of samples) so that we can try to reproduce it. As you said we'll have to ask GLnexus maintainers about this issue. I found this issue on the GLnexus repo https://github.com/dnanexus-rnd/GLnexus/issues/286 - I'd recommend adding a reproducible example there as well. About the PASS filter, if your downstream application explicitly requires having that filter value, I'd recommend using tools like `bcftools` to add it. I think `bcftools annotate --rename-annots` would work based on this page, using `FILTER/. PASS` as the mapping: https://samtools.github.io/bcftools/bcftools.html#annotate. Thank you. Best,. Ted",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/633
https://github.com/google/deepvariant/issues/633:228,modifiability,maintain,maintainers,228,"Hello,. I think it'd be helpful if you can share individual gVCF calls for the problematic site before merging (hopefully with a fewer number of samples) so that we can try to reproduce it. As you said we'll have to ask GLnexus maintainers about this issue. I found this issue on the GLnexus repo https://github.com/dnanexus-rnd/GLnexus/issues/286 - I'd recommend adding a reproducible example there as well. About the PASS filter, if your downstream application explicitly requires having that filter value, I'd recommend using tools like `bcftools` to add it. I think `bcftools annotate --rename-annots` would work based on this page, using `FILTER/. PASS` as the mapping: https://samtools.github.io/bcftools/bcftools.html#annotate. Thank you. Best,. Ted",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/633
https://github.com/google/deepvariant/issues/633:228,safety,maintain,maintainers,228,"Hello,. I think it'd be helpful if you can share individual gVCF calls for the problematic site before merging (hopefully with a fewer number of samples) so that we can try to reproduce it. As you said we'll have to ask GLnexus maintainers about this issue. I found this issue on the GLnexus repo https://github.com/dnanexus-rnd/GLnexus/issues/286 - I'd recommend adding a reproducible example there as well. About the PASS filter, if your downstream application explicitly requires having that filter value, I'd recommend using tools like `bcftools` to add it. I think `bcftools annotate --rename-annots` would work based on this page, using `FILTER/. PASS` as the mapping: https://samtools.github.io/bcftools/bcftools.html#annotate. Thank you. Best,. Ted",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/633
https://github.com/google/deepvariant/issues/633:24,usability,help,helpful,24,"Hello,. I think it'd be helpful if you can share individual gVCF calls for the problematic site before merging (hopefully with a fewer number of samples) so that we can try to reproduce it. As you said we'll have to ask GLnexus maintainers about this issue. I found this issue on the GLnexus repo https://github.com/dnanexus-rnd/GLnexus/issues/286 - I'd recommend adding a reproducible example there as well. About the PASS filter, if your downstream application explicitly requires having that filter value, I'd recommend using tools like `bcftools` to add it. I think `bcftools annotate --rename-annots` would work based on this page, using `FILTER/. PASS` as the mapping: https://samtools.github.io/bcftools/bcftools.html#annotate. Thank you. Best,. Ted",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/633
https://github.com/google/deepvariant/issues/633:529,usability,tool,tools,529,"Hello,. I think it'd be helpful if you can share individual gVCF calls for the problematic site before merging (hopefully with a fewer number of samples) so that we can try to reproduce it. As you said we'll have to ask GLnexus maintainers about this issue. I found this issue on the GLnexus repo https://github.com/dnanexus-rnd/GLnexus/issues/286 - I'd recommend adding a reproducible example there as well. About the PASS filter, if your downstream application explicitly requires having that filter value, I'd recommend using tools like `bcftools` to add it. I think `bcftools annotate --rename-annots` would work based on this page, using `FILTER/. PASS` as the mapping: https://samtools.github.io/bcftools/bcftools.html#annotate. Thank you. Best,. Ted",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/633
https://github.com/google/deepvariant/issues/633:73,integrability,coupl,couple,73,"Hi Ted,. thank you for your reply! This is what the site looks like in a couple of gvcf files directly from deepvariant:. ```. 1	1318679	.	A	<*>	0	.	END=1319106	GT:GQ:MIN_DP:PL	0/0:1:0:0,0,0. 1	1319056	.	A	G,<*>	30.7	PASS	.	GT:GQ:DP:AD:VAF:PL	1/1:8:2:0,2,0:1,0:29,6,0,990,990,990. 1	1318679	.	A	<*>	0	.	END=1319087	GT:GQ:MIN_DP:PL	0/0:1:0:0,0,0. ```. Thank you for your help! All the best,. Daniel",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/633
https://github.com/google/deepvariant/issues/633:73,modifiability,coupl,couple,73,"Hi Ted,. thank you for your reply! This is what the site looks like in a couple of gvcf files directly from deepvariant:. ```. 1	1318679	.	A	<*>	0	.	END=1319106	GT:GQ:MIN_DP:PL	0/0:1:0:0,0,0. 1	1319056	.	A	G,<*>	30.7	PASS	.	GT:GQ:DP:AD:VAF:PL	1/1:8:2:0,2,0:1,0:29,6,0,990,990,990. 1	1318679	.	A	<*>	0	.	END=1319087	GT:GQ:MIN_DP:PL	0/0:1:0:0,0,0. ```. Thank you for your help! All the best,. Daniel",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/633
https://github.com/google/deepvariant/issues/633:73,testability,coupl,couple,73,"Hi Ted,. thank you for your reply! This is what the site looks like in a couple of gvcf files directly from deepvariant:. ```. 1	1318679	.	A	<*>	0	.	END=1319106	GT:GQ:MIN_DP:PL	0/0:1:0:0,0,0. 1	1319056	.	A	G,<*>	30.7	PASS	.	GT:GQ:DP:AD:VAF:PL	1/1:8:2:0,2,0:1,0:29,6,0,990,990,990. 1	1318679	.	A	<*>	0	.	END=1319087	GT:GQ:MIN_DP:PL	0/0:1:0:0,0,0. ```. Thank you for your help! All the best,. Daniel",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/633
https://github.com/google/deepvariant/issues/633:370,usability,help,help,370,"Hi Ted,. thank you for your reply! This is what the site looks like in a couple of gvcf files directly from deepvariant:. ```. 1	1318679	.	A	<*>	0	.	END=1319106	GT:GQ:MIN_DP:PL	0/0:1:0:0,0,0. 1	1319056	.	A	G,<*>	30.7	PASS	.	GT:GQ:DP:AD:VAF:PL	1/1:8:2:0,2,0:1,0:29,6,0,990,990,990. 1	1318679	.	A	<*>	0	.	END=1319087	GT:GQ:MIN_DP:PL	0/0:1:0:0,0,0. ```. Thank you for your help! All the best,. Daniel",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/633
https://github.com/google/deepvariant/issues/633:25,testability,understand,understand,25,"Hi @Dani-kolbe . Just to understand your desired behavior, do you want the final VCF calls for low evidence sites to be uniformly ./., or do you prefer all ./. listed as 0/0?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/633
https://github.com/google/deepvariant/issues/633:49,usability,behavi,behavior,49,"Hi @Dani-kolbe . Just to understand your desired behavior, do you want the final VCF calls for low evidence sites to be uniformly ./., or do you prefer all ./. listed as 0/0?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/633
https://github.com/google/deepvariant/issues/633:145,usability,prefer,prefer,145,"Hi @Dani-kolbe . Just to understand your desired behavior, do you want the final VCF calls for low evidence sites to be uniformly ./., or do you prefer all ./. listed as 0/0?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/633
https://github.com/google/deepvariant/issues/633:140,integrability,filter,filter,140,"Hi Andrew,. thank you again for your reply. I think ./. is fine - this will be handled as missing data by something like PLINK? And DV will filter out site with overall low confidence, right? Best wishes,. Daniel",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/633
https://github.com/google/deepvariant/issues/633:110,integrability,filter,filtering,110,I'm a bit uncertain about the status of this thread. It also seems like there was another thread asking about filtering. @Dani-kolbe Is there any questions we can still help with in this thread?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/633
https://github.com/google/deepvariant/issues/633:30,usability,statu,status,30,I'm a bit uncertain about the status of this thread. It also seems like there was another thread asking about filtering. @Dani-kolbe Is there any questions we can still help with in this thread?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/633
https://github.com/google/deepvariant/issues/633:169,usability,help,help,169,I'm a bit uncertain about the status of this thread. It also seems like there was another thread asking about filtering. @Dani-kolbe Is there any questions we can still help with in this thread?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/633
https://github.com/google/deepvariant/issues/634:79,availability,error,error,79,I have installed chardet 3.0.4 by using 'pip install chardet==3.04'. . But the error are still reported.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/634
https://github.com/google/deepvariant/issues/634:7,deployability,instal,installed,7,I have installed chardet 3.0.4 by using 'pip install chardet==3.04'. . But the error are still reported.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/634
https://github.com/google/deepvariant/issues/634:45,deployability,instal,install,45,I have installed chardet 3.0.4 by using 'pip install chardet==3.04'. . But the error are still reported.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/634
https://github.com/google/deepvariant/issues/634:79,performance,error,error,79,I have installed chardet 3.0.4 by using 'pip install chardet==3.04'. . But the error are still reported.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/634
https://github.com/google/deepvariant/issues/634:79,safety,error,error,79,I have installed chardet 3.0.4 by using 'pip install chardet==3.04'. . But the error are still reported.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/634
https://github.com/google/deepvariant/issues/634:79,usability,error,error,79,I have installed chardet 3.0.4 by using 'pip install chardet==3.04'. . But the error are still reported.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/634
https://github.com/google/deepvariant/issues/634:23,availability,error,error,23,"I have now solved this error. Firstly, input within shell:. > singularity shell deepvariant_1.5.0.sif. Then,. > pip install --upgrade pip. > pip install chardet==3.04",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/634
https://github.com/google/deepvariant/issues/634:116,deployability,instal,install,116,"I have now solved this error. Firstly, input within shell:. > singularity shell deepvariant_1.5.0.sif. Then,. > pip install --upgrade pip. > pip install chardet==3.04",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/634
https://github.com/google/deepvariant/issues/634:126,deployability,upgrad,upgrade,126,"I have now solved this error. Firstly, input within shell:. > singularity shell deepvariant_1.5.0.sif. Then,. > pip install --upgrade pip. > pip install chardet==3.04",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/634
https://github.com/google/deepvariant/issues/634:145,deployability,instal,install,145,"I have now solved this error. Firstly, input within shell:. > singularity shell deepvariant_1.5.0.sif. Then,. > pip install --upgrade pip. > pip install chardet==3.04",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/634
https://github.com/google/deepvariant/issues/634:126,modifiability,upgrad,upgrade,126,"I have now solved this error. Firstly, input within shell:. > singularity shell deepvariant_1.5.0.sif. Then,. > pip install --upgrade pip. > pip install chardet==3.04",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/634
https://github.com/google/deepvariant/issues/634:23,performance,error,error,23,"I have now solved this error. Firstly, input within shell:. > singularity shell deepvariant_1.5.0.sif. Then,. > pip install --upgrade pip. > pip install chardet==3.04",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/634
https://github.com/google/deepvariant/issues/634:23,safety,error,error,23,"I have now solved this error. Firstly, input within shell:. > singularity shell deepvariant_1.5.0.sif. Then,. > pip install --upgrade pip. > pip install chardet==3.04",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/634
https://github.com/google/deepvariant/issues/634:39,safety,input,input,39,"I have now solved this error. Firstly, input within shell:. > singularity shell deepvariant_1.5.0.sif. Then,. > pip install --upgrade pip. > pip install chardet==3.04",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/634
https://github.com/google/deepvariant/issues/634:23,usability,error,error,23,"I have now solved this error. Firstly, input within shell:. > singularity shell deepvariant_1.5.0.sif. Then,. > pip install --upgrade pip. > pip install chardet==3.04",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/634
https://github.com/google/deepvariant/issues/634:39,usability,input,input,39,"I have now solved this error. Firstly, input within shell:. > singularity shell deepvariant_1.5.0.sif. Then,. > pip install --upgrade pip. > pip install chardet==3.04",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/634
https://github.com/google/deepvariant/issues/634:90,deployability,contain,container,90,"@CWYuan08 Basically `singularity shell` drops you with a shell prompt within your running container. The container will require that pip update. Here is a [video link](https://www.youtube.com/watch?v=97VuBVnfcwg) describing this process. Once inside the container just run the pip install and then run DeepVariant from there, to use the updated environment.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/634
https://github.com/google/deepvariant/issues/634:105,deployability,contain,container,105,"@CWYuan08 Basically `singularity shell` drops you with a shell prompt within your running container. The container will require that pip update. Here is a [video link](https://www.youtube.com/watch?v=97VuBVnfcwg) describing this process. Once inside the container just run the pip install and then run DeepVariant from there, to use the updated environment.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/634
https://github.com/google/deepvariant/issues/634:137,deployability,updat,update,137,"@CWYuan08 Basically `singularity shell` drops you with a shell prompt within your running container. The container will require that pip update. Here is a [video link](https://www.youtube.com/watch?v=97VuBVnfcwg) describing this process. Once inside the container just run the pip install and then run DeepVariant from there, to use the updated environment.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/634
https://github.com/google/deepvariant/issues/634:254,deployability,contain,container,254,"@CWYuan08 Basically `singularity shell` drops you with a shell prompt within your running container. The container will require that pip update. Here is a [video link](https://www.youtube.com/watch?v=97VuBVnfcwg) describing this process. Once inside the container just run the pip install and then run DeepVariant from there, to use the updated environment.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/634
https://github.com/google/deepvariant/issues/634:281,deployability,instal,install,281,"@CWYuan08 Basically `singularity shell` drops you with a shell prompt within your running container. The container will require that pip update. Here is a [video link](https://www.youtube.com/watch?v=97VuBVnfcwg) describing this process. Once inside the container just run the pip install and then run DeepVariant from there, to use the updated environment.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/634
https://github.com/google/deepvariant/issues/634:337,deployability,updat,updated,337,"@CWYuan08 Basically `singularity shell` drops you with a shell prompt within your running container. The container will require that pip update. Here is a [video link](https://www.youtube.com/watch?v=97VuBVnfcwg) describing this process. Once inside the container just run the pip install and then run DeepVariant from there, to use the updated environment.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/634
https://github.com/google/deepvariant/issues/634:137,safety,updat,update,137,"@CWYuan08 Basically `singularity shell` drops you with a shell prompt within your running container. The container will require that pip update. Here is a [video link](https://www.youtube.com/watch?v=97VuBVnfcwg) describing this process. Once inside the container just run the pip install and then run DeepVariant from there, to use the updated environment.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/634
https://github.com/google/deepvariant/issues/634:337,safety,updat,updated,337,"@CWYuan08 Basically `singularity shell` drops you with a shell prompt within your running container. The container will require that pip update. Here is a [video link](https://www.youtube.com/watch?v=97VuBVnfcwg) describing this process. Once inside the container just run the pip install and then run DeepVariant from there, to use the updated environment.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/634
https://github.com/google/deepvariant/issues/634:137,security,updat,update,137,"@CWYuan08 Basically `singularity shell` drops you with a shell prompt within your running container. The container will require that pip update. Here is a [video link](https://www.youtube.com/watch?v=97VuBVnfcwg) describing this process. Once inside the container just run the pip install and then run DeepVariant from there, to use the updated environment.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/634
https://github.com/google/deepvariant/issues/634:337,security,updat,updated,337,"@CWYuan08 Basically `singularity shell` drops you with a shell prompt within your running container. The container will require that pip update. Here is a [video link](https://www.youtube.com/watch?v=97VuBVnfcwg) describing this process. Once inside the container just run the pip install and then run DeepVariant from there, to use the updated environment.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/634
https://github.com/google/deepvariant/issues/635:43,availability,operat,operation,43,"1. No - this should not be necessary. This operation is handled by DeepVariant. 2. There are two arguments being passed here: `--make_examples_extra_args=""split_skip_reads=true, channels=''`. - `--make_examples_extra_args=""split_skip_reads=true` - Turns on operation that splits reads. This is the equivalent of GATK SplitNCigarReads. - `channels=''` - Disables additional channels from being added. Currently we add an insert_size channel by default, but the RNA-seq model does not currently use this channel. 3. The RNAseq model can be applied in any region. However, if you look at our [preprint](https://www.biorxiv.org/content/10.1101/2022.10.16.512451v1.full), you will see that we restrict much of our analysis to CDS regions. The reason for this is that variant calling in CDS regions is a desirable use case where we can maintain relatively high accuracy in RNA-seq data. However, you can use a larger set of regions depending on what the goals of your analysis are. You can also apply filtering to increase precision in these regions (at a cost of reduced sensitivity). See the section `Selecting a cut-off for practical implementation` in our preprint for details on this idea. <img width=""633"" alt=""image"" src=""https://user-images.githubusercontent.com/1536935/234003967-3aa5abd4-f8f3-494d-8ada-79eef723548b.png"">. I recommend following along with our [case study](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-rnaseq-case-study.md) to get started.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/635
https://github.com/google/deepvariant/issues/635:257,availability,operat,operation,257,"1. No - this should not be necessary. This operation is handled by DeepVariant. 2. There are two arguments being passed here: `--make_examples_extra_args=""split_skip_reads=true, channels=''`. - `--make_examples_extra_args=""split_skip_reads=true` - Turns on operation that splits reads. This is the equivalent of GATK SplitNCigarReads. - `channels=''` - Disables additional channels from being added. Currently we add an insert_size channel by default, but the RNA-seq model does not currently use this channel. 3. The RNAseq model can be applied in any region. However, if you look at our [preprint](https://www.biorxiv.org/content/10.1101/2022.10.16.512451v1.full), you will see that we restrict much of our analysis to CDS regions. The reason for this is that variant calling in CDS regions is a desirable use case where we can maintain relatively high accuracy in RNA-seq data. However, you can use a larger set of regions depending on what the goals of your analysis are. You can also apply filtering to increase precision in these regions (at a cost of reduced sensitivity). See the section `Selecting a cut-off for practical implementation` in our preprint for details on this idea. <img width=""633"" alt=""image"" src=""https://user-images.githubusercontent.com/1536935/234003967-3aa5abd4-f8f3-494d-8ada-79eef723548b.png"">. I recommend following along with our [case study](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-rnaseq-case-study.md) to get started.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/635
https://github.com/google/deepvariant/issues/635:926,deployability,depend,depending,926,"1. No - this should not be necessary. This operation is handled by DeepVariant. 2. There are two arguments being passed here: `--make_examples_extra_args=""split_skip_reads=true, channels=''`. - `--make_examples_extra_args=""split_skip_reads=true` - Turns on operation that splits reads. This is the equivalent of GATK SplitNCigarReads. - `channels=''` - Disables additional channels from being added. Currently we add an insert_size channel by default, but the RNA-seq model does not currently use this channel. 3. The RNAseq model can be applied in any region. However, if you look at our [preprint](https://www.biorxiv.org/content/10.1101/2022.10.16.512451v1.full), you will see that we restrict much of our analysis to CDS regions. The reason for this is that variant calling in CDS regions is a desirable use case where we can maintain relatively high accuracy in RNA-seq data. However, you can use a larger set of regions depending on what the goals of your analysis are. You can also apply filtering to increase precision in these regions (at a cost of reduced sensitivity). See the section `Selecting a cut-off for practical implementation` in our preprint for details on this idea. <img width=""633"" alt=""image"" src=""https://user-images.githubusercontent.com/1536935/234003967-3aa5abd4-f8f3-494d-8ada-79eef723548b.png"">. I recommend following along with our [case study](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-rnaseq-case-study.md) to get started.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/635
https://github.com/google/deepvariant/issues/635:400,energy efficiency,Current,Currently,400,"1. No - this should not be necessary. This operation is handled by DeepVariant. 2. There are two arguments being passed here: `--make_examples_extra_args=""split_skip_reads=true, channels=''`. - `--make_examples_extra_args=""split_skip_reads=true` - Turns on operation that splits reads. This is the equivalent of GATK SplitNCigarReads. - `channels=''` - Disables additional channels from being added. Currently we add an insert_size channel by default, but the RNA-seq model does not currently use this channel. 3. The RNAseq model can be applied in any region. However, if you look at our [preprint](https://www.biorxiv.org/content/10.1101/2022.10.16.512451v1.full), you will see that we restrict much of our analysis to CDS regions. The reason for this is that variant calling in CDS regions is a desirable use case where we can maintain relatively high accuracy in RNA-seq data. However, you can use a larger set of regions depending on what the goals of your analysis are. You can also apply filtering to increase precision in these regions (at a cost of reduced sensitivity). See the section `Selecting a cut-off for practical implementation` in our preprint for details on this idea. <img width=""633"" alt=""image"" src=""https://user-images.githubusercontent.com/1536935/234003967-3aa5abd4-f8f3-494d-8ada-79eef723548b.png"">. I recommend following along with our [case study](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-rnaseq-case-study.md) to get started.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/635
https://github.com/google/deepvariant/issues/635:468,energy efficiency,model,model,468,"1. No - this should not be necessary. This operation is handled by DeepVariant. 2. There are two arguments being passed here: `--make_examples_extra_args=""split_skip_reads=true, channels=''`. - `--make_examples_extra_args=""split_skip_reads=true` - Turns on operation that splits reads. This is the equivalent of GATK SplitNCigarReads. - `channels=''` - Disables additional channels from being added. Currently we add an insert_size channel by default, but the RNA-seq model does not currently use this channel. 3. The RNAseq model can be applied in any region. However, if you look at our [preprint](https://www.biorxiv.org/content/10.1101/2022.10.16.512451v1.full), you will see that we restrict much of our analysis to CDS regions. The reason for this is that variant calling in CDS regions is a desirable use case where we can maintain relatively high accuracy in RNA-seq data. However, you can use a larger set of regions depending on what the goals of your analysis are. You can also apply filtering to increase precision in these regions (at a cost of reduced sensitivity). See the section `Selecting a cut-off for practical implementation` in our preprint for details on this idea. <img width=""633"" alt=""image"" src=""https://user-images.githubusercontent.com/1536935/234003967-3aa5abd4-f8f3-494d-8ada-79eef723548b.png"">. I recommend following along with our [case study](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-rnaseq-case-study.md) to get started.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/635
https://github.com/google/deepvariant/issues/635:483,energy efficiency,current,currently,483,"1. No - this should not be necessary. This operation is handled by DeepVariant. 2. There are two arguments being passed here: `--make_examples_extra_args=""split_skip_reads=true, channels=''`. - `--make_examples_extra_args=""split_skip_reads=true` - Turns on operation that splits reads. This is the equivalent of GATK SplitNCigarReads. - `channels=''` - Disables additional channels from being added. Currently we add an insert_size channel by default, but the RNA-seq model does not currently use this channel. 3. The RNAseq model can be applied in any region. However, if you look at our [preprint](https://www.biorxiv.org/content/10.1101/2022.10.16.512451v1.full), you will see that we restrict much of our analysis to CDS regions. The reason for this is that variant calling in CDS regions is a desirable use case where we can maintain relatively high accuracy in RNA-seq data. However, you can use a larger set of regions depending on what the goals of your analysis are. You can also apply filtering to increase precision in these regions (at a cost of reduced sensitivity). See the section `Selecting a cut-off for practical implementation` in our preprint for details on this idea. <img width=""633"" alt=""image"" src=""https://user-images.githubusercontent.com/1536935/234003967-3aa5abd4-f8f3-494d-8ada-79eef723548b.png"">. I recommend following along with our [case study](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-rnaseq-case-study.md) to get started.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/635
https://github.com/google/deepvariant/issues/635:525,energy efficiency,model,model,525,"1. No - this should not be necessary. This operation is handled by DeepVariant. 2. There are two arguments being passed here: `--make_examples_extra_args=""split_skip_reads=true, channels=''`. - `--make_examples_extra_args=""split_skip_reads=true` - Turns on operation that splits reads. This is the equivalent of GATK SplitNCigarReads. - `channels=''` - Disables additional channels from being added. Currently we add an insert_size channel by default, but the RNA-seq model does not currently use this channel. 3. The RNAseq model can be applied in any region. However, if you look at our [preprint](https://www.biorxiv.org/content/10.1101/2022.10.16.512451v1.full), you will see that we restrict much of our analysis to CDS regions. The reason for this is that variant calling in CDS regions is a desirable use case where we can maintain relatively high accuracy in RNA-seq data. However, you can use a larger set of regions depending on what the goals of your analysis are. You can also apply filtering to increase precision in these regions (at a cost of reduced sensitivity). See the section `Selecting a cut-off for practical implementation` in our preprint for details on this idea. <img width=""633"" alt=""image"" src=""https://user-images.githubusercontent.com/1536935/234003967-3aa5abd4-f8f3-494d-8ada-79eef723548b.png"">. I recommend following along with our [case study](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-rnaseq-case-study.md) to get started.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/635
https://github.com/google/deepvariant/issues/635:1058,energy efficiency,reduc,reduced,1058,"1. No - this should not be necessary. This operation is handled by DeepVariant. 2. There are two arguments being passed here: `--make_examples_extra_args=""split_skip_reads=true, channels=''`. - `--make_examples_extra_args=""split_skip_reads=true` - Turns on operation that splits reads. This is the equivalent of GATK SplitNCigarReads. - `channels=''` - Disables additional channels from being added. Currently we add an insert_size channel by default, but the RNA-seq model does not currently use this channel. 3. The RNAseq model can be applied in any region. However, if you look at our [preprint](https://www.biorxiv.org/content/10.1101/2022.10.16.512451v1.full), you will see that we restrict much of our analysis to CDS regions. The reason for this is that variant calling in CDS regions is a desirable use case where we can maintain relatively high accuracy in RNA-seq data. However, you can use a larger set of regions depending on what the goals of your analysis are. You can also apply filtering to increase precision in these regions (at a cost of reduced sensitivity). See the section `Selecting a cut-off for practical implementation` in our preprint for details on this idea. <img width=""633"" alt=""image"" src=""https://user-images.githubusercontent.com/1536935/234003967-3aa5abd4-f8f3-494d-8ada-79eef723548b.png"">. I recommend following along with our [case study](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-rnaseq-case-study.md) to get started.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/635
https://github.com/google/deepvariant/issues/635:926,integrability,depend,depending,926,"1. No - this should not be necessary. This operation is handled by DeepVariant. 2. There are two arguments being passed here: `--make_examples_extra_args=""split_skip_reads=true, channels=''`. - `--make_examples_extra_args=""split_skip_reads=true` - Turns on operation that splits reads. This is the equivalent of GATK SplitNCigarReads. - `channels=''` - Disables additional channels from being added. Currently we add an insert_size channel by default, but the RNA-seq model does not currently use this channel. 3. The RNAseq model can be applied in any region. However, if you look at our [preprint](https://www.biorxiv.org/content/10.1101/2022.10.16.512451v1.full), you will see that we restrict much of our analysis to CDS regions. The reason for this is that variant calling in CDS regions is a desirable use case where we can maintain relatively high accuracy in RNA-seq data. However, you can use a larger set of regions depending on what the goals of your analysis are. You can also apply filtering to increase precision in these regions (at a cost of reduced sensitivity). See the section `Selecting a cut-off for practical implementation` in our preprint for details on this idea. <img width=""633"" alt=""image"" src=""https://user-images.githubusercontent.com/1536935/234003967-3aa5abd4-f8f3-494d-8ada-79eef723548b.png"">. I recommend following along with our [case study](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-rnaseq-case-study.md) to get started.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/635
https://github.com/google/deepvariant/issues/635:995,integrability,filter,filtering,995,"1. No - this should not be necessary. This operation is handled by DeepVariant. 2. There are two arguments being passed here: `--make_examples_extra_args=""split_skip_reads=true, channels=''`. - `--make_examples_extra_args=""split_skip_reads=true` - Turns on operation that splits reads. This is the equivalent of GATK SplitNCigarReads. - `channels=''` - Disables additional channels from being added. Currently we add an insert_size channel by default, but the RNA-seq model does not currently use this channel. 3. The RNAseq model can be applied in any region. However, if you look at our [preprint](https://www.biorxiv.org/content/10.1101/2022.10.16.512451v1.full), you will see that we restrict much of our analysis to CDS regions. The reason for this is that variant calling in CDS regions is a desirable use case where we can maintain relatively high accuracy in RNA-seq data. However, you can use a larger set of regions depending on what the goals of your analysis are. You can also apply filtering to increase precision in these regions (at a cost of reduced sensitivity). See the section `Selecting a cut-off for practical implementation` in our preprint for details on this idea. <img width=""633"" alt=""image"" src=""https://user-images.githubusercontent.com/1536935/234003967-3aa5abd4-f8f3-494d-8ada-79eef723548b.png"">. I recommend following along with our [case study](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-rnaseq-case-study.md) to get started.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/635
https://github.com/google/deepvariant/issues/635:830,modifiability,maintain,maintain,830,"1. No - this should not be necessary. This operation is handled by DeepVariant. 2. There are two arguments being passed here: `--make_examples_extra_args=""split_skip_reads=true, channels=''`. - `--make_examples_extra_args=""split_skip_reads=true` - Turns on operation that splits reads. This is the equivalent of GATK SplitNCigarReads. - `channels=''` - Disables additional channels from being added. Currently we add an insert_size channel by default, but the RNA-seq model does not currently use this channel. 3. The RNAseq model can be applied in any region. However, if you look at our [preprint](https://www.biorxiv.org/content/10.1101/2022.10.16.512451v1.full), you will see that we restrict much of our analysis to CDS regions. The reason for this is that variant calling in CDS regions is a desirable use case where we can maintain relatively high accuracy in RNA-seq data. However, you can use a larger set of regions depending on what the goals of your analysis are. You can also apply filtering to increase precision in these regions (at a cost of reduced sensitivity). See the section `Selecting a cut-off for practical implementation` in our preprint for details on this idea. <img width=""633"" alt=""image"" src=""https://user-images.githubusercontent.com/1536935/234003967-3aa5abd4-f8f3-494d-8ada-79eef723548b.png"">. I recommend following along with our [case study](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-rnaseq-case-study.md) to get started.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/635
https://github.com/google/deepvariant/issues/635:926,modifiability,depend,depending,926,"1. No - this should not be necessary. This operation is handled by DeepVariant. 2. There are two arguments being passed here: `--make_examples_extra_args=""split_skip_reads=true, channels=''`. - `--make_examples_extra_args=""split_skip_reads=true` - Turns on operation that splits reads. This is the equivalent of GATK SplitNCigarReads. - `channels=''` - Disables additional channels from being added. Currently we add an insert_size channel by default, but the RNA-seq model does not currently use this channel. 3. The RNAseq model can be applied in any region. However, if you look at our [preprint](https://www.biorxiv.org/content/10.1101/2022.10.16.512451v1.full), you will see that we restrict much of our analysis to CDS regions. The reason for this is that variant calling in CDS regions is a desirable use case where we can maintain relatively high accuracy in RNA-seq data. However, you can use a larger set of regions depending on what the goals of your analysis are. You can also apply filtering to increase precision in these regions (at a cost of reduced sensitivity). See the section `Selecting a cut-off for practical implementation` in our preprint for details on this idea. <img width=""633"" alt=""image"" src=""https://user-images.githubusercontent.com/1536935/234003967-3aa5abd4-f8f3-494d-8ada-79eef723548b.png"">. I recommend following along with our [case study](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-rnaseq-case-study.md) to get started.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/635
https://github.com/google/deepvariant/issues/635:624,performance,content,content,624,"1. No - this should not be necessary. This operation is handled by DeepVariant. 2. There are two arguments being passed here: `--make_examples_extra_args=""split_skip_reads=true, channels=''`. - `--make_examples_extra_args=""split_skip_reads=true` - Turns on operation that splits reads. This is the equivalent of GATK SplitNCigarReads. - `channels=''` - Disables additional channels from being added. Currently we add an insert_size channel by default, but the RNA-seq model does not currently use this channel. 3. The RNAseq model can be applied in any region. However, if you look at our [preprint](https://www.biorxiv.org/content/10.1101/2022.10.16.512451v1.full), you will see that we restrict much of our analysis to CDS regions. The reason for this is that variant calling in CDS regions is a desirable use case where we can maintain relatively high accuracy in RNA-seq data. However, you can use a larger set of regions depending on what the goals of your analysis are. You can also apply filtering to increase precision in these regions (at a cost of reduced sensitivity). See the section `Selecting a cut-off for practical implementation` in our preprint for details on this idea. <img width=""633"" alt=""image"" src=""https://user-images.githubusercontent.com/1536935/234003967-3aa5abd4-f8f3-494d-8ada-79eef723548b.png"">. I recommend following along with our [case study](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-rnaseq-case-study.md) to get started.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/635
https://github.com/google/deepvariant/issues/635:474,reliability,doe,does,474,"1. No - this should not be necessary. This operation is handled by DeepVariant. 2. There are two arguments being passed here: `--make_examples_extra_args=""split_skip_reads=true, channels=''`. - `--make_examples_extra_args=""split_skip_reads=true` - Turns on operation that splits reads. This is the equivalent of GATK SplitNCigarReads. - `channels=''` - Disables additional channels from being added. Currently we add an insert_size channel by default, but the RNA-seq model does not currently use this channel. 3. The RNAseq model can be applied in any region. However, if you look at our [preprint](https://www.biorxiv.org/content/10.1101/2022.10.16.512451v1.full), you will see that we restrict much of our analysis to CDS regions. The reason for this is that variant calling in CDS regions is a desirable use case where we can maintain relatively high accuracy in RNA-seq data. However, you can use a larger set of regions depending on what the goals of your analysis are. You can also apply filtering to increase precision in these regions (at a cost of reduced sensitivity). See the section `Selecting a cut-off for practical implementation` in our preprint for details on this idea. <img width=""633"" alt=""image"" src=""https://user-images.githubusercontent.com/1536935/234003967-3aa5abd4-f8f3-494d-8ada-79eef723548b.png"">. I recommend following along with our [case study](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-rnaseq-case-study.md) to get started.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/635
https://github.com/google/deepvariant/issues/635:1121,reliability,pra,practical,1121,"1. No - this should not be necessary. This operation is handled by DeepVariant. 2. There are two arguments being passed here: `--make_examples_extra_args=""split_skip_reads=true, channels=''`. - `--make_examples_extra_args=""split_skip_reads=true` - Turns on operation that splits reads. This is the equivalent of GATK SplitNCigarReads. - `channels=''` - Disables additional channels from being added. Currently we add an insert_size channel by default, but the RNA-seq model does not currently use this channel. 3. The RNAseq model can be applied in any region. However, if you look at our [preprint](https://www.biorxiv.org/content/10.1101/2022.10.16.512451v1.full), you will see that we restrict much of our analysis to CDS regions. The reason for this is that variant calling in CDS regions is a desirable use case where we can maintain relatively high accuracy in RNA-seq data. However, you can use a larger set of regions depending on what the goals of your analysis are. You can also apply filtering to increase precision in these regions (at a cost of reduced sensitivity). See the section `Selecting a cut-off for practical implementation` in our preprint for details on this idea. <img width=""633"" alt=""image"" src=""https://user-images.githubusercontent.com/1536935/234003967-3aa5abd4-f8f3-494d-8ada-79eef723548b.png"">. I recommend following along with our [case study](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-rnaseq-case-study.md) to get started.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/635
https://github.com/google/deepvariant/issues/635:830,safety,maintain,maintain,830,"1. No - this should not be necessary. This operation is handled by DeepVariant. 2. There are two arguments being passed here: `--make_examples_extra_args=""split_skip_reads=true, channels=''`. - `--make_examples_extra_args=""split_skip_reads=true` - Turns on operation that splits reads. This is the equivalent of GATK SplitNCigarReads. - `channels=''` - Disables additional channels from being added. Currently we add an insert_size channel by default, but the RNA-seq model does not currently use this channel. 3. The RNAseq model can be applied in any region. However, if you look at our [preprint](https://www.biorxiv.org/content/10.1101/2022.10.16.512451v1.full), you will see that we restrict much of our analysis to CDS regions. The reason for this is that variant calling in CDS regions is a desirable use case where we can maintain relatively high accuracy in RNA-seq data. However, you can use a larger set of regions depending on what the goals of your analysis are. You can also apply filtering to increase precision in these regions (at a cost of reduced sensitivity). See the section `Selecting a cut-off for practical implementation` in our preprint for details on this idea. <img width=""633"" alt=""image"" src=""https://user-images.githubusercontent.com/1536935/234003967-3aa5abd4-f8f3-494d-8ada-79eef723548b.png"">. I recommend following along with our [case study](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-rnaseq-case-study.md) to get started.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/635
https://github.com/google/deepvariant/issues/635:926,safety,depend,depending,926,"1. No - this should not be necessary. This operation is handled by DeepVariant. 2. There are two arguments being passed here: `--make_examples_extra_args=""split_skip_reads=true, channels=''`. - `--make_examples_extra_args=""split_skip_reads=true` - Turns on operation that splits reads. This is the equivalent of GATK SplitNCigarReads. - `channels=''` - Disables additional channels from being added. Currently we add an insert_size channel by default, but the RNA-seq model does not currently use this channel. 3. The RNAseq model can be applied in any region. However, if you look at our [preprint](https://www.biorxiv.org/content/10.1101/2022.10.16.512451v1.full), you will see that we restrict much of our analysis to CDS regions. The reason for this is that variant calling in CDS regions is a desirable use case where we can maintain relatively high accuracy in RNA-seq data. However, you can use a larger set of regions depending on what the goals of your analysis are. You can also apply filtering to increase precision in these regions (at a cost of reduced sensitivity). See the section `Selecting a cut-off for practical implementation` in our preprint for details on this idea. <img width=""633"" alt=""image"" src=""https://user-images.githubusercontent.com/1536935/234003967-3aa5abd4-f8f3-494d-8ada-79eef723548b.png"">. I recommend following along with our [case study](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-rnaseq-case-study.md) to get started.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/635
https://github.com/google/deepvariant/issues/635:468,security,model,model,468,"1. No - this should not be necessary. This operation is handled by DeepVariant. 2. There are two arguments being passed here: `--make_examples_extra_args=""split_skip_reads=true, channels=''`. - `--make_examples_extra_args=""split_skip_reads=true` - Turns on operation that splits reads. This is the equivalent of GATK SplitNCigarReads. - `channels=''` - Disables additional channels from being added. Currently we add an insert_size channel by default, but the RNA-seq model does not currently use this channel. 3. The RNAseq model can be applied in any region. However, if you look at our [preprint](https://www.biorxiv.org/content/10.1101/2022.10.16.512451v1.full), you will see that we restrict much of our analysis to CDS regions. The reason for this is that variant calling in CDS regions is a desirable use case where we can maintain relatively high accuracy in RNA-seq data. However, you can use a larger set of regions depending on what the goals of your analysis are. You can also apply filtering to increase precision in these regions (at a cost of reduced sensitivity). See the section `Selecting a cut-off for practical implementation` in our preprint for details on this idea. <img width=""633"" alt=""image"" src=""https://user-images.githubusercontent.com/1536935/234003967-3aa5abd4-f8f3-494d-8ada-79eef723548b.png"">. I recommend following along with our [case study](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-rnaseq-case-study.md) to get started.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/635
https://github.com/google/deepvariant/issues/635:525,security,model,model,525,"1. No - this should not be necessary. This operation is handled by DeepVariant. 2. There are two arguments being passed here: `--make_examples_extra_args=""split_skip_reads=true, channels=''`. - `--make_examples_extra_args=""split_skip_reads=true` - Turns on operation that splits reads. This is the equivalent of GATK SplitNCigarReads. - `channels=''` - Disables additional channels from being added. Currently we add an insert_size channel by default, but the RNA-seq model does not currently use this channel. 3. The RNAseq model can be applied in any region. However, if you look at our [preprint](https://www.biorxiv.org/content/10.1101/2022.10.16.512451v1.full), you will see that we restrict much of our analysis to CDS regions. The reason for this is that variant calling in CDS regions is a desirable use case where we can maintain relatively high accuracy in RNA-seq data. However, you can use a larger set of regions depending on what the goals of your analysis are. You can also apply filtering to increase precision in these regions (at a cost of reduced sensitivity). See the section `Selecting a cut-off for practical implementation` in our preprint for details on this idea. <img width=""633"" alt=""image"" src=""https://user-images.githubusercontent.com/1536935/234003967-3aa5abd4-f8f3-494d-8ada-79eef723548b.png"">. I recommend following along with our [case study](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-rnaseq-case-study.md) to get started.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/635
https://github.com/google/deepvariant/issues/635:926,testability,depend,depending,926,"1. No - this should not be necessary. This operation is handled by DeepVariant. 2. There are two arguments being passed here: `--make_examples_extra_args=""split_skip_reads=true, channels=''`. - `--make_examples_extra_args=""split_skip_reads=true` - Turns on operation that splits reads. This is the equivalent of GATK SplitNCigarReads. - `channels=''` - Disables additional channels from being added. Currently we add an insert_size channel by default, but the RNA-seq model does not currently use this channel. 3. The RNAseq model can be applied in any region. However, if you look at our [preprint](https://www.biorxiv.org/content/10.1101/2022.10.16.512451v1.full), you will see that we restrict much of our analysis to CDS regions. The reason for this is that variant calling in CDS regions is a desirable use case where we can maintain relatively high accuracy in RNA-seq data. However, you can use a larger set of regions depending on what the goals of your analysis are. You can also apply filtering to increase precision in these regions (at a cost of reduced sensitivity). See the section `Selecting a cut-off for practical implementation` in our preprint for details on this idea. <img width=""633"" alt=""image"" src=""https://user-images.githubusercontent.com/1536935/234003967-3aa5abd4-f8f3-494d-8ada-79eef723548b.png"">. I recommend following along with our [case study](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-rnaseq-case-study.md) to get started.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/635
https://github.com/google/deepvariant/issues/635:1231,usability,user,user-images,1231,"1. No - this should not be necessary. This operation is handled by DeepVariant. 2. There are two arguments being passed here: `--make_examples_extra_args=""split_skip_reads=true, channels=''`. - `--make_examples_extra_args=""split_skip_reads=true` - Turns on operation that splits reads. This is the equivalent of GATK SplitNCigarReads. - `channels=''` - Disables additional channels from being added. Currently we add an insert_size channel by default, but the RNA-seq model does not currently use this channel. 3. The RNAseq model can be applied in any region. However, if you look at our [preprint](https://www.biorxiv.org/content/10.1101/2022.10.16.512451v1.full), you will see that we restrict much of our analysis to CDS regions. The reason for this is that variant calling in CDS regions is a desirable use case where we can maintain relatively high accuracy in RNA-seq data. However, you can use a larger set of regions depending on what the goals of your analysis are. You can also apply filtering to increase precision in these regions (at a cost of reduced sensitivity). See the section `Selecting a cut-off for practical implementation` in our preprint for details on this idea. <img width=""633"" alt=""image"" src=""https://user-images.githubusercontent.com/1536935/234003967-3aa5abd4-f8f3-494d-8ada-79eef723548b.png"">. I recommend following along with our [case study](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-rnaseq-case-study.md) to get started.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/635
https://github.com/google/deepvariant/issues/635:18,availability,error,error,18,"@crazysummerW the error seems to suggest that the contigs in your bam file do not match the contigs in the reference you provided. Can you double check that `$star_fasta` is pointing to the same file as `hg19.fasta`, and that the contigs present in both are the same? One possibility here could be that the `$star_fasta` provided here removed the `chr` prefix from the chromosomes.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/635
https://github.com/google/deepvariant/issues/635:18,performance,error,error,18,"@crazysummerW the error seems to suggest that the contigs in your bam file do not match the contigs in the reference you provided. Can you double check that `$star_fasta` is pointing to the same file as `hg19.fasta`, and that the contigs present in both are the same? One possibility here could be that the `$star_fasta` provided here removed the `chr` prefix from the chromosomes.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/635
https://github.com/google/deepvariant/issues/635:18,safety,error,error,18,"@crazysummerW the error seems to suggest that the contigs in your bam file do not match the contigs in the reference you provided. Can you double check that `$star_fasta` is pointing to the same file as `hg19.fasta`, and that the contigs present in both are the same? One possibility here could be that the `$star_fasta` provided here removed the `chr` prefix from the chromosomes.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/635
https://github.com/google/deepvariant/issues/635:18,usability,error,error,18,"@crazysummerW the error seems to suggest that the contigs in your bam file do not match the contigs in the reference you provided. Can you double check that `$star_fasta` is pointing to the same file as `hg19.fasta`, and that the contigs present in both are the same? One possibility here could be that the `$star_fasta` provided here removed the `chr` prefix from the chromosomes.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/635
https://github.com/google/deepvariant/issues/636:159,availability,sli,slim,159,"Hi Saurabh,. Currently the `run_deepvariant_keras` is experimental as we plan to develop this further in the future. The model for `run_deepvariant` is for tf-slim and would not simply extend to keras. For now, you should use `run_deepvariant` as that's the one we officially support.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/636
https://github.com/google/deepvariant/issues/636:13,energy efficiency,Current,Currently,13,"Hi Saurabh,. Currently the `run_deepvariant_keras` is experimental as we plan to develop this further in the future. The model for `run_deepvariant` is for tf-slim and would not simply extend to keras. For now, you should use `run_deepvariant` as that's the one we officially support.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/636
https://github.com/google/deepvariant/issues/636:121,energy efficiency,model,model,121,"Hi Saurabh,. Currently the `run_deepvariant_keras` is experimental as we plan to develop this further in the future. The model for `run_deepvariant` is for tf-slim and would not simply extend to keras. For now, you should use `run_deepvariant` as that's the one we officially support.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/636
https://github.com/google/deepvariant/issues/636:185,modifiability,exten,extend,185,"Hi Saurabh,. Currently the `run_deepvariant_keras` is experimental as we plan to develop this further in the future. The model for `run_deepvariant` is for tf-slim and would not simply extend to keras. For now, you should use `run_deepvariant` as that's the one we officially support.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/636
https://github.com/google/deepvariant/issues/636:159,reliability,sli,slim,159,"Hi Saurabh,. Currently the `run_deepvariant_keras` is experimental as we plan to develop this further in the future. The model for `run_deepvariant` is for tf-slim and would not simply extend to keras. For now, you should use `run_deepvariant` as that's the one we officially support.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/636
https://github.com/google/deepvariant/issues/636:121,security,model,model,121,"Hi Saurabh,. Currently the `run_deepvariant_keras` is experimental as we plan to develop this further in the future. The model for `run_deepvariant` is for tf-slim and would not simply extend to keras. For now, you should use `run_deepvariant` as that's the one we officially support.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/636
https://github.com/google/deepvariant/issues/636:73,testability,plan,plan,73,"Hi Saurabh,. Currently the `run_deepvariant_keras` is experimental as we plan to develop this further in the future. The model for `run_deepvariant` is for tf-slim and would not simply extend to keras. For now, you should use `run_deepvariant` as that's the one we officially support.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/636
https://github.com/google/deepvariant/issues/636:178,testability,simpl,simply,178,"Hi Saurabh,. Currently the `run_deepvariant_keras` is experimental as we plan to develop this further in the future. The model for `run_deepvariant` is for tf-slim and would not simply extend to keras. For now, you should use `run_deepvariant` as that's the one we officially support.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/636
https://github.com/google/deepvariant/issues/636:178,usability,simpl,simply,178,"Hi Saurabh,. Currently the `run_deepvariant_keras` is experimental as we plan to develop this further in the future. The model for `run_deepvariant` is for tf-slim and would not simply extend to keras. For now, you should use `run_deepvariant` as that's the one we officially support.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/636
https://github.com/google/deepvariant/issues/636:276,usability,support,support,276,"Hi Saurabh,. Currently the `run_deepvariant_keras` is experimental as we plan to develop this further in the future. The model for `run_deepvariant` is for tf-slim and would not simply extend to keras. For now, you should use `run_deepvariant` as that's the one we officially support.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/636
https://github.com/google/deepvariant/issues/636:107,energy efficiency,model,model,107,"Thanks for the clarification, Kishwar. Could you please let us know when we can expect the new keras-based model to be officially supported? . Thanks. Saurabh",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/636
https://github.com/google/deepvariant/issues/636:107,security,model,model,107,"Thanks for the clarification, Kishwar. Could you please let us know when we can expect the new keras-based model to be officially supported? . Thanks. Saurabh",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/636
https://github.com/google/deepvariant/issues/636:130,usability,support,supported,130,"Thanks for the clarification, Kishwar. Could you please let us know when we can expect the new keras-based model to be officially supported? . Thanks. Saurabh",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/636
https://github.com/google/deepvariant/issues/637:221,availability,state,stated,221,"Hi @Famingzhao ,. DeepVariant is trained for germline variant calling and somatic variant calling is not officially supported. For your use case, you have to run DeepVariant on tumor and normal independently. However, as stated before, somatic calling is not officially supported.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/637
https://github.com/google/deepvariant/issues/637:221,integrability,state,stated,221,"Hi @Famingzhao ,. DeepVariant is trained for germline variant calling and somatic variant calling is not officially supported. For your use case, you have to run DeepVariant on tumor and normal independently. However, as stated before, somatic calling is not officially supported.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/637
https://github.com/google/deepvariant/issues/637:116,usability,support,supported,116,"Hi @Famingzhao ,. DeepVariant is trained for germline variant calling and somatic variant calling is not officially supported. For your use case, you have to run DeepVariant on tumor and normal independently. However, as stated before, somatic calling is not officially supported.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/637
https://github.com/google/deepvariant/issues/637:270,usability,support,supported,270,"Hi @Famingzhao ,. DeepVariant is trained for germline variant calling and somatic variant calling is not officially supported. For your use case, you have to run DeepVariant on tumor and normal independently. However, as stated before, somatic calling is not officially supported.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/637
https://github.com/google/deepvariant/issues/638:96,availability,state,stated,96,"Hi @crazysummerW ,. Please see this issue: https://github.com/google/deepvariant/issues/569, as stated, this is our recommendation:. > We have a strong recommendation NOT to run Variant Score Quality Recalibration (VQSR). We observe large reductions in accuracy, particularly in recall of rare variants.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/638
https://github.com/google/deepvariant/issues/638:225,deployability,observ,observe,225,"Hi @crazysummerW ,. Please see this issue: https://github.com/google/deepvariant/issues/569, as stated, this is our recommendation:. > We have a strong recommendation NOT to run Variant Score Quality Recalibration (VQSR). We observe large reductions in accuracy, particularly in recall of rare variants.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/638
https://github.com/google/deepvariant/issues/638:239,energy efficiency,reduc,reductions,239,"Hi @crazysummerW ,. Please see this issue: https://github.com/google/deepvariant/issues/569, as stated, this is our recommendation:. > We have a strong recommendation NOT to run Variant Score Quality Recalibration (VQSR). We observe large reductions in accuracy, particularly in recall of rare variants.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/638
https://github.com/google/deepvariant/issues/638:96,integrability,state,stated,96,"Hi @crazysummerW ,. Please see this issue: https://github.com/google/deepvariant/issues/569, as stated, this is our recommendation:. > We have a strong recommendation NOT to run Variant Score Quality Recalibration (VQSR). We observe large reductions in accuracy, particularly in recall of rare variants.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/638
https://github.com/google/deepvariant/issues/638:225,testability,observ,observe,225,"Hi @crazysummerW ,. Please see this issue: https://github.com/google/deepvariant/issues/569, as stated, this is our recommendation:. > We have a strong recommendation NOT to run Variant Score Quality Recalibration (VQSR). We observe large reductions in accuracy, particularly in recall of rare variants.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/638
https://github.com/google/deepvariant/issues/638:43,safety,test,tested,43,hi，. Thanks for your reply. I mean BQSR. I tested on WES data. The result : . 1. deal with deduplication and BQSR：. ![1682582003166](https://user-images.githubusercontent.com/70870741/234796615-a5ce10c7-da7b-4542-8717-b2cde03fc478.jpg). 2. only deal with deduplication:. ![1682582061403](https://user-images.githubusercontent.com/70870741/234796818-f7bb9a58-dc50-4a96-8848-6bd785b0b136.jpg). Can I think BQSR processed results are better? .,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/638
https://github.com/google/deepvariant/issues/638:43,testability,test,tested,43,hi，. Thanks for your reply. I mean BQSR. I tested on WES data. The result : . 1. deal with deduplication and BQSR：. ![1682582003166](https://user-images.githubusercontent.com/70870741/234796615-a5ce10c7-da7b-4542-8717-b2cde03fc478.jpg). 2. only deal with deduplication:. ![1682582061403](https://user-images.githubusercontent.com/70870741/234796818-f7bb9a58-dc50-4a96-8848-6bd785b0b136.jpg). Can I think BQSR processed results are better? .,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/638
https://github.com/google/deepvariant/issues/638:141,usability,user,user-images,141,hi，. Thanks for your reply. I mean BQSR. I tested on WES data. The result : . 1. deal with deduplication and BQSR：. ![1682582003166](https://user-images.githubusercontent.com/70870741/234796615-a5ce10c7-da7b-4542-8717-b2cde03fc478.jpg). 2. only deal with deduplication:. ![1682582061403](https://user-images.githubusercontent.com/70870741/234796818-f7bb9a58-dc50-4a96-8848-6bd785b0b136.jpg). Can I think BQSR processed results are better? .,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/638
https://github.com/google/deepvariant/issues/638:296,usability,user,user-images,296,hi，. Thanks for your reply. I mean BQSR. I tested on WES data. The result : . 1. deal with deduplication and BQSR：. ![1682582003166](https://user-images.githubusercontent.com/70870741/234796615-a5ce10c7-da7b-4542-8717-b2cde03fc478.jpg). 2. only deal with deduplication:. ![1682582061403](https://user-images.githubusercontent.com/70870741/234796818-f7bb9a58-dc50-4a96-8848-6bd785b0b136.jpg). Can I think BQSR processed results are better? .,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/638
https://github.com/google/deepvariant/issues/638:105,availability,sli,slight,105,"Hi @crazysummerW ,. Sorry it was my mistake. I misread BQSR as VQSR. Previously our experiments showed a slight decrease in accuracy with BQSR, but, we haven't done that for the latest models. Your numbers do look slightly improved. Are you using the latest version of DeepVariant?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/638
https://github.com/google/deepvariant/issues/638:214,availability,sli,slightly,214,"Hi @crazysummerW ,. Sorry it was my mistake. I misread BQSR as VQSR. Previously our experiments showed a slight decrease in accuracy with BQSR, but, we haven't done that for the latest models. Your numbers do look slightly improved. Are you using the latest version of DeepVariant?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/638
https://github.com/google/deepvariant/issues/638:258,deployability,version,version,258,"Hi @crazysummerW ,. Sorry it was my mistake. I misread BQSR as VQSR. Previously our experiments showed a slight decrease in accuracy with BQSR, but, we haven't done that for the latest models. Your numbers do look slightly improved. Are you using the latest version of DeepVariant?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/638
https://github.com/google/deepvariant/issues/638:185,energy efficiency,model,models,185,"Hi @crazysummerW ,. Sorry it was my mistake. I misread BQSR as VQSR. Previously our experiments showed a slight decrease in accuracy with BQSR, but, we haven't done that for the latest models. Your numbers do look slightly improved. Are you using the latest version of DeepVariant?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/638
https://github.com/google/deepvariant/issues/638:258,integrability,version,version,258,"Hi @crazysummerW ,. Sorry it was my mistake. I misread BQSR as VQSR. Previously our experiments showed a slight decrease in accuracy with BQSR, but, we haven't done that for the latest models. Your numbers do look slightly improved. Are you using the latest version of DeepVariant?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/638
https://github.com/google/deepvariant/issues/638:258,modifiability,version,version,258,"Hi @crazysummerW ,. Sorry it was my mistake. I misread BQSR as VQSR. Previously our experiments showed a slight decrease in accuracy with BQSR, but, we haven't done that for the latest models. Your numbers do look slightly improved. Are you using the latest version of DeepVariant?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/638
https://github.com/google/deepvariant/issues/638:105,reliability,sli,slight,105,"Hi @crazysummerW ,. Sorry it was my mistake. I misread BQSR as VQSR. Previously our experiments showed a slight decrease in accuracy with BQSR, but, we haven't done that for the latest models. Your numbers do look slightly improved. Are you using the latest version of DeepVariant?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/638
https://github.com/google/deepvariant/issues/638:214,reliability,sli,slightly,214,"Hi @crazysummerW ,. Sorry it was my mistake. I misread BQSR as VQSR. Previously our experiments showed a slight decrease in accuracy with BQSR, but, we haven't done that for the latest models. Your numbers do look slightly improved. Are you using the latest version of DeepVariant?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/638
https://github.com/google/deepvariant/issues/638:185,security,model,models,185,"Hi @crazysummerW ,. Sorry it was my mistake. I misread BQSR as VQSR. Previously our experiments showed a slight decrease in accuracy with BQSR, but, we haven't done that for the latest models. Your numbers do look slightly improved. Are you using the latest version of DeepVariant?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/638
https://github.com/google/deepvariant/issues/638:55,energy efficiency,reduc,reduce,55,"hi，. I used DeepVariant1.4 and GATK.4.2.0.0. I want to reduce the running time of the process overall. So I want to confirm, is there no problem with data preprocessing without deduplication and BQSR for DeepVariant?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/638
https://github.com/google/deepvariant/issues/638:74,performance,time,time,74,"hi，. I used DeepVariant1.4 and GATK.4.2.0.0. I want to reduce the running time of the process overall. So I want to confirm, is there no problem with data preprocessing without deduplication and BQSR for DeepVariant?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/638
https://github.com/google/deepvariant/issues/638:116,usability,confirm,confirm,116,"hi，. I used DeepVariant1.4 and GATK.4.2.0.0. I want to reduce the running time of the process overall. So I want to confirm, is there no problem with data preprocessing without deduplication and BQSR for DeepVariant?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/638
https://github.com/google/deepvariant/issues/639:1519,deployability,observ,observes,1519,"frequency). The neural net assigns a genotype probability for all of these positions. The confidence that the neural network has in the variant call is represented in the GQ field (which is the phred-encoded probability that the assigned genotype is correct). When the probability for reference is >50% and 99% (GQ20), the genotype assigned is ./. when the probability for reference is >99% (GQ20+) the genotype assigned is 0/0. In your gVCF calls, only this row is a REF call made by the neural net:. ```. 1	69897	.	T	C,<*>	0.7	RefCall	.	GT:GQ:DP:AD:VAF:PL	./.:8:6:2,4,0:0.666667,0:0,8,13,990,990,990. ```. The second way that DeepVariant makes a no-call/reference-call is in the process of gVCF generation. The gVCF was designed as a way to encode the probability of a region with no variant call information is a reference, so that later in joint genotyping of many samples, you can potentially change the genotype call based on what you see in the population. In your gVCF calls all other REF calls fall into this category. DeepVariant makes a gVCF with REF-CALL blocks over stretches where it observes no candidates, with a heuristic logic based on coverage and support determining the probability of these calls. In calls of this nature, there is a different logic that does not apply the GQ20 threshold. In joint genotyping, GLnexus will use the probabilities to determine the VCF call. . In summary, a variant (0/1 or 1/1) will have a variant call if it is the most likely genotype at the position. A position will receive a 0/0 call if the model observed a site with >GQ20. It can make sense to filter DeepVariant results if you have a substantial preference for precision. In addition to higher overall accuracy, we try to make DeepVariant report well-calibrated confidence probabilities. If you need higher precision for the variant calls, filtering on the call GQ is the best field. This was a long answer. Please let me know what areas remain unclear after reading it. Thank you,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/639
https://github.com/google/deepvariant/issues/639:1560,deployability,log,logic,1560,"frequency). The neural net assigns a genotype probability for all of these positions. The confidence that the neural network has in the variant call is represented in the GQ field (which is the phred-encoded probability that the assigned genotype is correct). When the probability for reference is >50% and 99% (GQ20), the genotype assigned is ./. when the probability for reference is >99% (GQ20+) the genotype assigned is 0/0. In your gVCF calls, only this row is a REF call made by the neural net:. ```. 1	69897	.	T	C,<*>	0.7	RefCall	.	GT:GQ:DP:AD:VAF:PL	./.:8:6:2,4,0:0.666667,0:0,8,13,990,990,990. ```. The second way that DeepVariant makes a no-call/reference-call is in the process of gVCF generation. The gVCF was designed as a way to encode the probability of a region with no variant call information is a reference, so that later in joint genotyping of many samples, you can potentially change the genotype call based on what you see in the population. In your gVCF calls all other REF calls fall into this category. DeepVariant makes a gVCF with REF-CALL blocks over stretches where it observes no candidates, with a heuristic logic based on coverage and support determining the probability of these calls. In calls of this nature, there is a different logic that does not apply the GQ20 threshold. In joint genotyping, GLnexus will use the probabilities to determine the VCF call. . In summary, a variant (0/1 or 1/1) will have a variant call if it is the most likely genotype at the position. A position will receive a 0/0 call if the model observed a site with >GQ20. It can make sense to filter DeepVariant results if you have a substantial preference for precision. In addition to higher overall accuracy, we try to make DeepVariant report well-calibrated confidence probabilities. If you need higher precision for the variant calls, filtering on the call GQ is the best field. This was a long answer. Please let me know what areas remain unclear after reading it. Thank you,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/639
https://github.com/google/deepvariant/issues/639:1686,deployability,log,logic,1686,"frequency). The neural net assigns a genotype probability for all of these positions. The confidence that the neural network has in the variant call is represented in the GQ field (which is the phred-encoded probability that the assigned genotype is correct). When the probability for reference is >50% and 99% (GQ20), the genotype assigned is ./. when the probability for reference is >99% (GQ20+) the genotype assigned is 0/0. In your gVCF calls, only this row is a REF call made by the neural net:. ```. 1	69897	.	T	C,<*>	0.7	RefCall	.	GT:GQ:DP:AD:VAF:PL	./.:8:6:2,4,0:0.666667,0:0,8,13,990,990,990. ```. The second way that DeepVariant makes a no-call/reference-call is in the process of gVCF generation. The gVCF was designed as a way to encode the probability of a region with no variant call information is a reference, so that later in joint genotyping of many samples, you can potentially change the genotype call based on what you see in the population. In your gVCF calls all other REF calls fall into this category. DeepVariant makes a gVCF with REF-CALL blocks over stretches where it observes no candidates, with a heuristic logic based on coverage and support determining the probability of these calls. In calls of this nature, there is a different logic that does not apply the GQ20 threshold. In joint genotyping, GLnexus will use the probabilities to determine the VCF call. . In summary, a variant (0/1 or 1/1) will have a variant call if it is the most likely genotype at the position. A position will receive a 0/0 call if the model observed a site with >GQ20. It can make sense to filter DeepVariant results if you have a substantial preference for precision. In addition to higher overall accuracy, we try to make DeepVariant report well-calibrated confidence probabilities. If you need higher precision for the variant calls, filtering on the call GQ is the best field. This was a long answer. Please let me know what areas remain unclear after reading it. Thank you,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/639
https://github.com/google/deepvariant/issues/639:1976,deployability,observ,observed,1976,"frequency). The neural net assigns a genotype probability for all of these positions. The confidence that the neural network has in the variant call is represented in the GQ field (which is the phred-encoded probability that the assigned genotype is correct). When the probability for reference is >50% and 99% (GQ20), the genotype assigned is ./. when the probability for reference is >99% (GQ20+) the genotype assigned is 0/0. In your gVCF calls, only this row is a REF call made by the neural net:. ```. 1	69897	.	T	C,<*>	0.7	RefCall	.	GT:GQ:DP:AD:VAF:PL	./.:8:6:2,4,0:0.666667,0:0,8,13,990,990,990. ```. The second way that DeepVariant makes a no-call/reference-call is in the process of gVCF generation. The gVCF was designed as a way to encode the probability of a region with no variant call information is a reference, so that later in joint genotyping of many samples, you can potentially change the genotype call based on what you see in the population. In your gVCF calls all other REF calls fall into this category. DeepVariant makes a gVCF with REF-CALL blocks over stretches where it observes no candidates, with a heuristic logic based on coverage and support determining the probability of these calls. In calls of this nature, there is a different logic that does not apply the GQ20 threshold. In joint genotyping, GLnexus will use the probabilities to determine the VCF call. . In summary, a variant (0/1 or 1/1) will have a variant call if it is the most likely genotype at the position. A position will receive a 0/0 call if the model observed a site with >GQ20. It can make sense to filter DeepVariant results if you have a substantial preference for precision. In addition to higher overall accuracy, we try to make DeepVariant report well-calibrated confidence probabilities. If you need higher precision for the variant calls, filtering on the call GQ is the best field. This was a long answer. Please let me know what areas remain unclear after reading it. Thank you,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/639
https://github.com/google/deepvariant/issues/639:421,energy efficiency,frequenc,frequency,421,"Hi @Dani-kolbe . To answer this, we need to talk through how DeepVariant handles no-calls and how GLnexus then acts on that information. There are two different ways that DeepVariant can make a no-call/reference call for a position. DeepVariant first looks through the genome for any positions with some evidence that a variant may exist (for example any position where 2 reads support a variant and more than 12% allele frequency). The neural net assigns a genotype probability for all of these positions. The confidence that the neural network has in the variant call is represented in the GQ field (which is the phred-encoded probability that the assigned genotype is correct). When the probability for reference is >50% and 99% (GQ20), the genotype assigned is ./. when the probability for reference is >99% (GQ20+) the genotype assigned is 0/0. In your gVCF calls, only this row is a REF call made by the neural net:. ```. 1	69897	.	T	C,<*>	0.7	RefCall	.	GT:GQ:DP:AD:VAF:PL	./.:8:6:2,4,0:0.666667,0:0,8,13,990,990,990. ```. The second way that DeepVariant makes a no-call/reference-call is in the process of gVCF generation. The gVCF was designed as a way to encode the probability of a region with no variant call information is a reference, so that later in joint genotyping of many samples, you can potentially change the genotype call based on what you see in the population. In your gVCF calls all other REF calls fall into this category. DeepVariant makes a gVCF with REF-CALL blocks over stretches where it observes no candidates, with a heuristic logic based on coverage and support determining the probability of these calls. In calls of this nature, there is a different logic that does not apply the GQ20 threshold. In joint genotyping, GLnexus will use the probabilities to determine the VCF call. . In summary, a variant (0/1 or 1/1) will have a variant call if it is the most likely genotype at the position. A position will receive a 0/0 call if the model observed a site with >GQ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/639
https://github.com/google/deepvariant/issues/639:1970,energy efficiency,model,model,1970,"frequency). The neural net assigns a genotype probability for all of these positions. The confidence that the neural network has in the variant call is represented in the GQ field (which is the phred-encoded probability that the assigned genotype is correct). When the probability for reference is >50% and 99% (GQ20), the genotype assigned is ./. when the probability for reference is >99% (GQ20+) the genotype assigned is 0/0. In your gVCF calls, only this row is a REF call made by the neural net:. ```. 1	69897	.	T	C,<*>	0.7	RefCall	.	GT:GQ:DP:AD:VAF:PL	./.:8:6:2,4,0:0.666667,0:0,8,13,990,990,990. ```. The second way that DeepVariant makes a no-call/reference-call is in the process of gVCF generation. The gVCF was designed as a way to encode the probability of a region with no variant call information is a reference, so that later in joint genotyping of many samples, you can potentially change the genotype call based on what you see in the population. In your gVCF calls all other REF calls fall into this category. DeepVariant makes a gVCF with REF-CALL blocks over stretches where it observes no candidates, with a heuristic logic based on coverage and support determining the probability of these calls. In calls of this nature, there is a different logic that does not apply the GQ20 threshold. In joint genotyping, GLnexus will use the probabilities to determine the VCF call. . In summary, a variant (0/1 or 1/1) will have a variant call if it is the most likely genotype at the position. A position will receive a 0/0 call if the model observed a site with >GQ20. It can make sense to filter DeepVariant results if you have a substantial preference for precision. In addition to higher overall accuracy, we try to make DeepVariant report well-calibrated confidence probabilities. If you need higher precision for the variant calls, filtering on the call GQ is the best field. This was a long answer. Please let me know what areas remain unclear after reading it. Thank you,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/639
https://github.com/google/deepvariant/issues/639:2025,integrability,filter,filter,2025,"frequency). The neural net assigns a genotype probability for all of these positions. The confidence that the neural network has in the variant call is represented in the GQ field (which is the phred-encoded probability that the assigned genotype is correct). When the probability for reference is >50% and 99% (GQ20), the genotype assigned is ./. when the probability for reference is >99% (GQ20+) the genotype assigned is 0/0. In your gVCF calls, only this row is a REF call made by the neural net:. ```. 1	69897	.	T	C,<*>	0.7	RefCall	.	GT:GQ:DP:AD:VAF:PL	./.:8:6:2,4,0:0.666667,0:0,8,13,990,990,990. ```. The second way that DeepVariant makes a no-call/reference-call is in the process of gVCF generation. The gVCF was designed as a way to encode the probability of a region with no variant call information is a reference, so that later in joint genotyping of many samples, you can potentially change the genotype call based on what you see in the population. In your gVCF calls all other REF calls fall into this category. DeepVariant makes a gVCF with REF-CALL blocks over stretches where it observes no candidates, with a heuristic logic based on coverage and support determining the probability of these calls. In calls of this nature, there is a different logic that does not apply the GQ20 threshold. In joint genotyping, GLnexus will use the probabilities to determine the VCF call. . In summary, a variant (0/1 or 1/1) will have a variant call if it is the most likely genotype at the position. A position will receive a 0/0 call if the model observed a site with >GQ20. It can make sense to filter DeepVariant results if you have a substantial preference for precision. In addition to higher overall accuracy, we try to make DeepVariant report well-calibrated confidence probabilities. If you need higher precision for the variant calls, filtering on the call GQ is the best field. This was a long answer. Please let me know what areas remain unclear after reading it. Thank you,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/639
https://github.com/google/deepvariant/issues/639:2066,integrability,sub,substantial,2066,"frequency). The neural net assigns a genotype probability for all of these positions. The confidence that the neural network has in the variant call is represented in the GQ field (which is the phred-encoded probability that the assigned genotype is correct). When the probability for reference is >50% and 99% (GQ20), the genotype assigned is ./. when the probability for reference is >99% (GQ20+) the genotype assigned is 0/0. In your gVCF calls, only this row is a REF call made by the neural net:. ```. 1	69897	.	T	C,<*>	0.7	RefCall	.	GT:GQ:DP:AD:VAF:PL	./.:8:6:2,4,0:0.666667,0:0,8,13,990,990,990. ```. The second way that DeepVariant makes a no-call/reference-call is in the process of gVCF generation. The gVCF was designed as a way to encode the probability of a region with no variant call information is a reference, so that later in joint genotyping of many samples, you can potentially change the genotype call based on what you see in the population. In your gVCF calls all other REF calls fall into this category. DeepVariant makes a gVCF with REF-CALL blocks over stretches where it observes no candidates, with a heuristic logic based on coverage and support determining the probability of these calls. In calls of this nature, there is a different logic that does not apply the GQ20 threshold. In joint genotyping, GLnexus will use the probabilities to determine the VCF call. . In summary, a variant (0/1 or 1/1) will have a variant call if it is the most likely genotype at the position. A position will receive a 0/0 call if the model observed a site with >GQ20. It can make sense to filter DeepVariant results if you have a substantial preference for precision. In addition to higher overall accuracy, we try to make DeepVariant report well-calibrated confidence probabilities. If you need higher precision for the variant calls, filtering on the call GQ is the best field. This was a long answer. Please let me know what areas remain unclear after reading it. Thank you,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/639
https://github.com/google/deepvariant/issues/639:2272,integrability,filter,filtering,2272,"frequency). The neural net assigns a genotype probability for all of these positions. The confidence that the neural network has in the variant call is represented in the GQ field (which is the phred-encoded probability that the assigned genotype is correct). When the probability for reference is >50% and 99% (GQ20), the genotype assigned is ./. when the probability for reference is >99% (GQ20+) the genotype assigned is 0/0. In your gVCF calls, only this row is a REF call made by the neural net:. ```. 1	69897	.	T	C,<*>	0.7	RefCall	.	GT:GQ:DP:AD:VAF:PL	./.:8:6:2,4,0:0.666667,0:0,8,13,990,990,990. ```. The second way that DeepVariant makes a no-call/reference-call is in the process of gVCF generation. The gVCF was designed as a way to encode the probability of a region with no variant call information is a reference, so that later in joint genotyping of many samples, you can potentially change the genotype call based on what you see in the population. In your gVCF calls all other REF calls fall into this category. DeepVariant makes a gVCF with REF-CALL blocks over stretches where it observes no candidates, with a heuristic logic based on coverage and support determining the probability of these calls. In calls of this nature, there is a different logic that does not apply the GQ20 threshold. In joint genotyping, GLnexus will use the probabilities to determine the VCF call. . In summary, a variant (0/1 or 1/1) will have a variant call if it is the most likely genotype at the position. A position will receive a 0/0 call if the model observed a site with >GQ20. It can make sense to filter DeepVariant results if you have a substantial preference for precision. In addition to higher overall accuracy, we try to make DeepVariant report well-calibrated confidence probabilities. If you need higher precision for the variant calls, filtering on the call GQ is the best field. This was a long answer. Please let me know what areas remain unclear after reading it. Thank you,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/639
https://github.com/google/deepvariant/issues/639:538,performance,network,network,538,"Hi @Dani-kolbe . To answer this, we need to talk through how DeepVariant handles no-calls and how GLnexus then acts on that information. There are two different ways that DeepVariant can make a no-call/reference call for a position. DeepVariant first looks through the genome for any positions with some evidence that a variant may exist (for example any position where 2 reads support a variant and more than 12% allele frequency). The neural net assigns a genotype probability for all of these positions. The confidence that the neural network has in the variant call is represented in the GQ field (which is the phred-encoded probability that the assigned genotype is correct). When the probability for reference is >50% and 99% (GQ20), the genotype assigned is ./. when the probability for reference is >99% (GQ20+) the genotype assigned is 0/0. In your gVCF calls, only this row is a REF call made by the neural net:. ```. 1	69897	.	T	C,<*>	0.7	RefCall	.	GT:GQ:DP:AD:VAF:PL	./.:8:6:2,4,0:0.666667,0:0,8,13,990,990,990. ```. The second way that DeepVariant makes a no-call/reference-call is in the process of gVCF generation. The gVCF was designed as a way to encode the probability of a region with no variant call information is a reference, so that later in joint genotyping of many samples, you can potentially change the genotype call based on what you see in the population. In your gVCF calls all other REF calls fall into this category. DeepVariant makes a gVCF with REF-CALL blocks over stretches where it observes no candidates, with a heuristic logic based on coverage and support determining the probability of these calls. In calls of this nature, there is a different logic that does not apply the GQ20 threshold. In joint genotyping, GLnexus will use the probabilities to determine the VCF call. . In summary, a variant (0/1 or 1/1) will have a variant call if it is the most likely genotype at the position. A position will receive a 0/0 call if the model observed a site with >GQ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/639
https://github.com/google/deepvariant/issues/639:1697,reliability,doe,does,1697,"frequency). The neural net assigns a genotype probability for all of these positions. The confidence that the neural network has in the variant call is represented in the GQ field (which is the phred-encoded probability that the assigned genotype is correct). When the probability for reference is >50% and 99% (GQ20), the genotype assigned is ./. when the probability for reference is >99% (GQ20+) the genotype assigned is 0/0. In your gVCF calls, only this row is a REF call made by the neural net:. ```. 1	69897	.	T	C,<*>	0.7	RefCall	.	GT:GQ:DP:AD:VAF:PL	./.:8:6:2,4,0:0.666667,0:0,8,13,990,990,990. ```. The second way that DeepVariant makes a no-call/reference-call is in the process of gVCF generation. The gVCF was designed as a way to encode the probability of a region with no variant call information is a reference, so that later in joint genotyping of many samples, you can potentially change the genotype call based on what you see in the population. In your gVCF calls all other REF calls fall into this category. DeepVariant makes a gVCF with REF-CALL blocks over stretches where it observes no candidates, with a heuristic logic based on coverage and support determining the probability of these calls. In calls of this nature, there is a different logic that does not apply the GQ20 threshold. In joint genotyping, GLnexus will use the probabilities to determine the VCF call. . In summary, a variant (0/1 or 1/1) will have a variant call if it is the most likely genotype at the position. A position will receive a 0/0 call if the model observed a site with >GQ20. It can make sense to filter DeepVariant results if you have a substantial preference for precision. In addition to higher overall accuracy, we try to make DeepVariant report well-calibrated confidence probabilities. If you need higher precision for the variant calls, filtering on the call GQ is the best field. This was a long answer. Please let me know what areas remain unclear after reading it. Thank you,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/639
https://github.com/google/deepvariant/issues/639:1560,safety,log,logic,1560,"frequency). The neural net assigns a genotype probability for all of these positions. The confidence that the neural network has in the variant call is represented in the GQ field (which is the phred-encoded probability that the assigned genotype is correct). When the probability for reference is >50% and 99% (GQ20), the genotype assigned is ./. when the probability for reference is >99% (GQ20+) the genotype assigned is 0/0. In your gVCF calls, only this row is a REF call made by the neural net:. ```. 1	69897	.	T	C,<*>	0.7	RefCall	.	GT:GQ:DP:AD:VAF:PL	./.:8:6:2,4,0:0.666667,0:0,8,13,990,990,990. ```. The second way that DeepVariant makes a no-call/reference-call is in the process of gVCF generation. The gVCF was designed as a way to encode the probability of a region with no variant call information is a reference, so that later in joint genotyping of many samples, you can potentially change the genotype call based on what you see in the population. In your gVCF calls all other REF calls fall into this category. DeepVariant makes a gVCF with REF-CALL blocks over stretches where it observes no candidates, with a heuristic logic based on coverage and support determining the probability of these calls. In calls of this nature, there is a different logic that does not apply the GQ20 threshold. In joint genotyping, GLnexus will use the probabilities to determine the VCF call. . In summary, a variant (0/1 or 1/1) will have a variant call if it is the most likely genotype at the position. A position will receive a 0/0 call if the model observed a site with >GQ20. It can make sense to filter DeepVariant results if you have a substantial preference for precision. In addition to higher overall accuracy, we try to make DeepVariant report well-calibrated confidence probabilities. If you need higher precision for the variant calls, filtering on the call GQ is the best field. This was a long answer. Please let me know what areas remain unclear after reading it. Thank you,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/639
https://github.com/google/deepvariant/issues/639:1686,safety,log,logic,1686,"frequency). The neural net assigns a genotype probability for all of these positions. The confidence that the neural network has in the variant call is represented in the GQ field (which is the phred-encoded probability that the assigned genotype is correct). When the probability for reference is >50% and 99% (GQ20), the genotype assigned is ./. when the probability for reference is >99% (GQ20+) the genotype assigned is 0/0. In your gVCF calls, only this row is a REF call made by the neural net:. ```. 1	69897	.	T	C,<*>	0.7	RefCall	.	GT:GQ:DP:AD:VAF:PL	./.:8:6:2,4,0:0.666667,0:0,8,13,990,990,990. ```. The second way that DeepVariant makes a no-call/reference-call is in the process of gVCF generation. The gVCF was designed as a way to encode the probability of a region with no variant call information is a reference, so that later in joint genotyping of many samples, you can potentially change the genotype call based on what you see in the population. In your gVCF calls all other REF calls fall into this category. DeepVariant makes a gVCF with REF-CALL blocks over stretches where it observes no candidates, with a heuristic logic based on coverage and support determining the probability of these calls. In calls of this nature, there is a different logic that does not apply the GQ20 threshold. In joint genotyping, GLnexus will use the probabilities to determine the VCF call. . In summary, a variant (0/1 or 1/1) will have a variant call if it is the most likely genotype at the position. A position will receive a 0/0 call if the model observed a site with >GQ20. It can make sense to filter DeepVariant results if you have a substantial preference for precision. In addition to higher overall accuracy, we try to make DeepVariant report well-calibrated confidence probabilities. If you need higher precision for the variant calls, filtering on the call GQ is the best field. This was a long answer. Please let me know what areas remain unclear after reading it. Thank you,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/639
https://github.com/google/deepvariant/issues/639:538,security,network,network,538,"Hi @Dani-kolbe . To answer this, we need to talk through how DeepVariant handles no-calls and how GLnexus then acts on that information. There are two different ways that DeepVariant can make a no-call/reference call for a position. DeepVariant first looks through the genome for any positions with some evidence that a variant may exist (for example any position where 2 reads support a variant and more than 12% allele frequency). The neural net assigns a genotype probability for all of these positions. The confidence that the neural network has in the variant call is represented in the GQ field (which is the phred-encoded probability that the assigned genotype is correct). When the probability for reference is >50% and 99% (GQ20), the genotype assigned is ./. when the probability for reference is >99% (GQ20+) the genotype assigned is 0/0. In your gVCF calls, only this row is a REF call made by the neural net:. ```. 1	69897	.	T	C,<*>	0.7	RefCall	.	GT:GQ:DP:AD:VAF:PL	./.:8:6:2,4,0:0.666667,0:0,8,13,990,990,990. ```. The second way that DeepVariant makes a no-call/reference-call is in the process of gVCF generation. The gVCF was designed as a way to encode the probability of a region with no variant call information is a reference, so that later in joint genotyping of many samples, you can potentially change the genotype call based on what you see in the population. In your gVCF calls all other REF calls fall into this category. DeepVariant makes a gVCF with REF-CALL blocks over stretches where it observes no candidates, with a heuristic logic based on coverage and support determining the probability of these calls. In calls of this nature, there is a different logic that does not apply the GQ20 threshold. In joint genotyping, GLnexus will use the probabilities to determine the VCF call. . In summary, a variant (0/1 or 1/1) will have a variant call if it is the most likely genotype at the position. A position will receive a 0/0 call if the model observed a site with >GQ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/639
https://github.com/google/deepvariant/issues/639:1560,security,log,logic,1560,"frequency). The neural net assigns a genotype probability for all of these positions. The confidence that the neural network has in the variant call is represented in the GQ field (which is the phred-encoded probability that the assigned genotype is correct). When the probability for reference is >50% and 99% (GQ20), the genotype assigned is ./. when the probability for reference is >99% (GQ20+) the genotype assigned is 0/0. In your gVCF calls, only this row is a REF call made by the neural net:. ```. 1	69897	.	T	C,<*>	0.7	RefCall	.	GT:GQ:DP:AD:VAF:PL	./.:8:6:2,4,0:0.666667,0:0,8,13,990,990,990. ```. The second way that DeepVariant makes a no-call/reference-call is in the process of gVCF generation. The gVCF was designed as a way to encode the probability of a region with no variant call information is a reference, so that later in joint genotyping of many samples, you can potentially change the genotype call based on what you see in the population. In your gVCF calls all other REF calls fall into this category. DeepVariant makes a gVCF with REF-CALL blocks over stretches where it observes no candidates, with a heuristic logic based on coverage and support determining the probability of these calls. In calls of this nature, there is a different logic that does not apply the GQ20 threshold. In joint genotyping, GLnexus will use the probabilities to determine the VCF call. . In summary, a variant (0/1 or 1/1) will have a variant call if it is the most likely genotype at the position. A position will receive a 0/0 call if the model observed a site with >GQ20. It can make sense to filter DeepVariant results if you have a substantial preference for precision. In addition to higher overall accuracy, we try to make DeepVariant report well-calibrated confidence probabilities. If you need higher precision for the variant calls, filtering on the call GQ is the best field. This was a long answer. Please let me know what areas remain unclear after reading it. Thank you,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/639
https://github.com/google/deepvariant/issues/639:1686,security,log,logic,1686,"frequency). The neural net assigns a genotype probability for all of these positions. The confidence that the neural network has in the variant call is represented in the GQ field (which is the phred-encoded probability that the assigned genotype is correct). When the probability for reference is >50% and 99% (GQ20), the genotype assigned is ./. when the probability for reference is >99% (GQ20+) the genotype assigned is 0/0. In your gVCF calls, only this row is a REF call made by the neural net:. ```. 1	69897	.	T	C,<*>	0.7	RefCall	.	GT:GQ:DP:AD:VAF:PL	./.:8:6:2,4,0:0.666667,0:0,8,13,990,990,990. ```. The second way that DeepVariant makes a no-call/reference-call is in the process of gVCF generation. The gVCF was designed as a way to encode the probability of a region with no variant call information is a reference, so that later in joint genotyping of many samples, you can potentially change the genotype call based on what you see in the population. In your gVCF calls all other REF calls fall into this category. DeepVariant makes a gVCF with REF-CALL blocks over stretches where it observes no candidates, with a heuristic logic based on coverage and support determining the probability of these calls. In calls of this nature, there is a different logic that does not apply the GQ20 threshold. In joint genotyping, GLnexus will use the probabilities to determine the VCF call. . In summary, a variant (0/1 or 1/1) will have a variant call if it is the most likely genotype at the position. A position will receive a 0/0 call if the model observed a site with >GQ20. It can make sense to filter DeepVariant results if you have a substantial preference for precision. In addition to higher overall accuracy, we try to make DeepVariant report well-calibrated confidence probabilities. If you need higher precision for the variant calls, filtering on the call GQ is the best field. This was a long answer. Please let me know what areas remain unclear after reading it. Thank you,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/639
https://github.com/google/deepvariant/issues/639:1970,security,model,model,1970,"frequency). The neural net assigns a genotype probability for all of these positions. The confidence that the neural network has in the variant call is represented in the GQ field (which is the phred-encoded probability that the assigned genotype is correct). When the probability for reference is >50% and 99% (GQ20), the genotype assigned is ./. when the probability for reference is >99% (GQ20+) the genotype assigned is 0/0. In your gVCF calls, only this row is a REF call made by the neural net:. ```. 1	69897	.	T	C,<*>	0.7	RefCall	.	GT:GQ:DP:AD:VAF:PL	./.:8:6:2,4,0:0.666667,0:0,8,13,990,990,990. ```. The second way that DeepVariant makes a no-call/reference-call is in the process of gVCF generation. The gVCF was designed as a way to encode the probability of a region with no variant call information is a reference, so that later in joint genotyping of many samples, you can potentially change the genotype call based on what you see in the population. In your gVCF calls all other REF calls fall into this category. DeepVariant makes a gVCF with REF-CALL blocks over stretches where it observes no candidates, with a heuristic logic based on coverage and support determining the probability of these calls. In calls of this nature, there is a different logic that does not apply the GQ20 threshold. In joint genotyping, GLnexus will use the probabilities to determine the VCF call. . In summary, a variant (0/1 or 1/1) will have a variant call if it is the most likely genotype at the position. A position will receive a 0/0 call if the model observed a site with >GQ20. It can make sense to filter DeepVariant results if you have a substantial preference for precision. In addition to higher overall accuracy, we try to make DeepVariant report well-calibrated confidence probabilities. If you need higher precision for the variant calls, filtering on the call GQ is the best field. This was a long answer. Please let me know what areas remain unclear after reading it. Thank you,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/639
https://github.com/google/deepvariant/issues/639:1519,testability,observ,observes,1519,"frequency). The neural net assigns a genotype probability for all of these positions. The confidence that the neural network has in the variant call is represented in the GQ field (which is the phred-encoded probability that the assigned genotype is correct). When the probability for reference is >50% and 99% (GQ20), the genotype assigned is ./. when the probability for reference is >99% (GQ20+) the genotype assigned is 0/0. In your gVCF calls, only this row is a REF call made by the neural net:. ```. 1	69897	.	T	C,<*>	0.7	RefCall	.	GT:GQ:DP:AD:VAF:PL	./.:8:6:2,4,0:0.666667,0:0,8,13,990,990,990. ```. The second way that DeepVariant makes a no-call/reference-call is in the process of gVCF generation. The gVCF was designed as a way to encode the probability of a region with no variant call information is a reference, so that later in joint genotyping of many samples, you can potentially change the genotype call based on what you see in the population. In your gVCF calls all other REF calls fall into this category. DeepVariant makes a gVCF with REF-CALL blocks over stretches where it observes no candidates, with a heuristic logic based on coverage and support determining the probability of these calls. In calls of this nature, there is a different logic that does not apply the GQ20 threshold. In joint genotyping, GLnexus will use the probabilities to determine the VCF call. . In summary, a variant (0/1 or 1/1) will have a variant call if it is the most likely genotype at the position. A position will receive a 0/0 call if the model observed a site with >GQ20. It can make sense to filter DeepVariant results if you have a substantial preference for precision. In addition to higher overall accuracy, we try to make DeepVariant report well-calibrated confidence probabilities. If you need higher precision for the variant calls, filtering on the call GQ is the best field. This was a long answer. Please let me know what areas remain unclear after reading it. Thank you,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/639
https://github.com/google/deepvariant/issues/639:1560,testability,log,logic,1560,"frequency). The neural net assigns a genotype probability for all of these positions. The confidence that the neural network has in the variant call is represented in the GQ field (which is the phred-encoded probability that the assigned genotype is correct). When the probability for reference is >50% and 99% (GQ20), the genotype assigned is ./. when the probability for reference is >99% (GQ20+) the genotype assigned is 0/0. In your gVCF calls, only this row is a REF call made by the neural net:. ```. 1	69897	.	T	C,<*>	0.7	RefCall	.	GT:GQ:DP:AD:VAF:PL	./.:8:6:2,4,0:0.666667,0:0,8,13,990,990,990. ```. The second way that DeepVariant makes a no-call/reference-call is in the process of gVCF generation. The gVCF was designed as a way to encode the probability of a region with no variant call information is a reference, so that later in joint genotyping of many samples, you can potentially change the genotype call based on what you see in the population. In your gVCF calls all other REF calls fall into this category. DeepVariant makes a gVCF with REF-CALL blocks over stretches where it observes no candidates, with a heuristic logic based on coverage and support determining the probability of these calls. In calls of this nature, there is a different logic that does not apply the GQ20 threshold. In joint genotyping, GLnexus will use the probabilities to determine the VCF call. . In summary, a variant (0/1 or 1/1) will have a variant call if it is the most likely genotype at the position. A position will receive a 0/0 call if the model observed a site with >GQ20. It can make sense to filter DeepVariant results if you have a substantial preference for precision. In addition to higher overall accuracy, we try to make DeepVariant report well-calibrated confidence probabilities. If you need higher precision for the variant calls, filtering on the call GQ is the best field. This was a long answer. Please let me know what areas remain unclear after reading it. Thank you,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/639
https://github.com/google/deepvariant/issues/639:1575,testability,coverag,coverage,1575,"frequency). The neural net assigns a genotype probability for all of these positions. The confidence that the neural network has in the variant call is represented in the GQ field (which is the phred-encoded probability that the assigned genotype is correct). When the probability for reference is >50% and 99% (GQ20), the genotype assigned is ./. when the probability for reference is >99% (GQ20+) the genotype assigned is 0/0. In your gVCF calls, only this row is a REF call made by the neural net:. ```. 1	69897	.	T	C,<*>	0.7	RefCall	.	GT:GQ:DP:AD:VAF:PL	./.:8:6:2,4,0:0.666667,0:0,8,13,990,990,990. ```. The second way that DeepVariant makes a no-call/reference-call is in the process of gVCF generation. The gVCF was designed as a way to encode the probability of a region with no variant call information is a reference, so that later in joint genotyping of many samples, you can potentially change the genotype call based on what you see in the population. In your gVCF calls all other REF calls fall into this category. DeepVariant makes a gVCF with REF-CALL blocks over stretches where it observes no candidates, with a heuristic logic based on coverage and support determining the probability of these calls. In calls of this nature, there is a different logic that does not apply the GQ20 threshold. In joint genotyping, GLnexus will use the probabilities to determine the VCF call. . In summary, a variant (0/1 or 1/1) will have a variant call if it is the most likely genotype at the position. A position will receive a 0/0 call if the model observed a site with >GQ20. It can make sense to filter DeepVariant results if you have a substantial preference for precision. In addition to higher overall accuracy, we try to make DeepVariant report well-calibrated confidence probabilities. If you need higher precision for the variant calls, filtering on the call GQ is the best field. This was a long answer. Please let me know what areas remain unclear after reading it. Thank you,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/639
https://github.com/google/deepvariant/issues/639:1686,testability,log,logic,1686,"frequency). The neural net assigns a genotype probability for all of these positions. The confidence that the neural network has in the variant call is represented in the GQ field (which is the phred-encoded probability that the assigned genotype is correct). When the probability for reference is >50% and 99% (GQ20), the genotype assigned is ./. when the probability for reference is >99% (GQ20+) the genotype assigned is 0/0. In your gVCF calls, only this row is a REF call made by the neural net:. ```. 1	69897	.	T	C,<*>	0.7	RefCall	.	GT:GQ:DP:AD:VAF:PL	./.:8:6:2,4,0:0.666667,0:0,8,13,990,990,990. ```. The second way that DeepVariant makes a no-call/reference-call is in the process of gVCF generation. The gVCF was designed as a way to encode the probability of a region with no variant call information is a reference, so that later in joint genotyping of many samples, you can potentially change the genotype call based on what you see in the population. In your gVCF calls all other REF calls fall into this category. DeepVariant makes a gVCF with REF-CALL blocks over stretches where it observes no candidates, with a heuristic logic based on coverage and support determining the probability of these calls. In calls of this nature, there is a different logic that does not apply the GQ20 threshold. In joint genotyping, GLnexus will use the probabilities to determine the VCF call. . In summary, a variant (0/1 or 1/1) will have a variant call if it is the most likely genotype at the position. A position will receive a 0/0 call if the model observed a site with >GQ20. It can make sense to filter DeepVariant results if you have a substantial preference for precision. In addition to higher overall accuracy, we try to make DeepVariant report well-calibrated confidence probabilities. If you need higher precision for the variant calls, filtering on the call GQ is the best field. This was a long answer. Please let me know what areas remain unclear after reading it. Thank you,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/639
https://github.com/google/deepvariant/issues/639:1976,testability,observ,observed,1976,"frequency). The neural net assigns a genotype probability for all of these positions. The confidence that the neural network has in the variant call is represented in the GQ field (which is the phred-encoded probability that the assigned genotype is correct). When the probability for reference is >50% and 99% (GQ20), the genotype assigned is ./. when the probability for reference is >99% (GQ20+) the genotype assigned is 0/0. In your gVCF calls, only this row is a REF call made by the neural net:. ```. 1	69897	.	T	C,<*>	0.7	RefCall	.	GT:GQ:DP:AD:VAF:PL	./.:8:6:2,4,0:0.666667,0:0,8,13,990,990,990. ```. The second way that DeepVariant makes a no-call/reference-call is in the process of gVCF generation. The gVCF was designed as a way to encode the probability of a region with no variant call information is a reference, so that later in joint genotyping of many samples, you can potentially change the genotype call based on what you see in the population. In your gVCF calls all other REF calls fall into this category. DeepVariant makes a gVCF with REF-CALL blocks over stretches where it observes no candidates, with a heuristic logic based on coverage and support determining the probability of these calls. In calls of this nature, there is a different logic that does not apply the GQ20 threshold. In joint genotyping, GLnexus will use the probabilities to determine the VCF call. . In summary, a variant (0/1 or 1/1) will have a variant call if it is the most likely genotype at the position. A position will receive a 0/0 call if the model observed a site with >GQ20. It can make sense to filter DeepVariant results if you have a substantial preference for precision. In addition to higher overall accuracy, we try to make DeepVariant report well-calibrated confidence probabilities. If you need higher precision for the variant calls, filtering on the call GQ is the best field. This was a long answer. Please let me know what areas remain unclear after reading it. Thank you,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/639
https://github.com/google/deepvariant/issues/639:378,usability,support,support,378,"Hi @Dani-kolbe . To answer this, we need to talk through how DeepVariant handles no-calls and how GLnexus then acts on that information. There are two different ways that DeepVariant can make a no-call/reference call for a position. DeepVariant first looks through the genome for any positions with some evidence that a variant may exist (for example any position where 2 reads support a variant and more than 12% allele frequency). The neural net assigns a genotype probability for all of these positions. The confidence that the neural network has in the variant call is represented in the GQ field (which is the phred-encoded probability that the assigned genotype is correct). When the probability for reference is >50% and 99% (GQ20), the genotype assigned is ./. when the probability for reference is >99% (GQ20+) the genotype assigned is 0/0. In your gVCF calls, only this row is a REF call made by the neural net:. ```. 1	69897	.	T	C,<*>	0.7	RefCall	.	GT:GQ:DP:AD:VAF:PL	./.:8:6:2,4,0:0.666667,0:0,8,13,990,990,990. ```. The second way that DeepVariant makes a no-call/reference-call is in the process of gVCF generation. The gVCF was designed as a way to encode the probability of a region with no variant call information is a reference, so that later in joint genotyping of many samples, you can potentially change the genotype call based on what you see in the population. In your gVCF calls all other REF calls fall into this category. DeepVariant makes a gVCF with REF-CALL blocks over stretches where it observes no candidates, with a heuristic logic based on coverage and support determining the probability of these calls. In calls of this nature, there is a different logic that does not apply the GQ20 threshold. In joint genotyping, GLnexus will use the probabilities to determine the VCF call. . In summary, a variant (0/1 or 1/1) will have a variant call if it is the most likely genotype at the position. A position will receive a 0/0 call if the model observed a site with >GQ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/639
https://github.com/google/deepvariant/issues/639:1588,usability,support,support,1588,"frequency). The neural net assigns a genotype probability for all of these positions. The confidence that the neural network has in the variant call is represented in the GQ field (which is the phred-encoded probability that the assigned genotype is correct). When the probability for reference is >50% and 99% (GQ20), the genotype assigned is ./. when the probability for reference is >99% (GQ20+) the genotype assigned is 0/0. In your gVCF calls, only this row is a REF call made by the neural net:. ```. 1	69897	.	T	C,<*>	0.7	RefCall	.	GT:GQ:DP:AD:VAF:PL	./.:8:6:2,4,0:0.666667,0:0,8,13,990,990,990. ```. The second way that DeepVariant makes a no-call/reference-call is in the process of gVCF generation. The gVCF was designed as a way to encode the probability of a region with no variant call information is a reference, so that later in joint genotyping of many samples, you can potentially change the genotype call based on what you see in the population. In your gVCF calls all other REF calls fall into this category. DeepVariant makes a gVCF with REF-CALL blocks over stretches where it observes no candidates, with a heuristic logic based on coverage and support determining the probability of these calls. In calls of this nature, there is a different logic that does not apply the GQ20 threshold. In joint genotyping, GLnexus will use the probabilities to determine the VCF call. . In summary, a variant (0/1 or 1/1) will have a variant call if it is the most likely genotype at the position. A position will receive a 0/0 call if the model observed a site with >GQ20. It can make sense to filter DeepVariant results if you have a substantial preference for precision. In addition to higher overall accuracy, we try to make DeepVariant report well-calibrated confidence probabilities. If you need higher precision for the variant calls, filtering on the call GQ is the best field. This was a long answer. Please let me know what areas remain unclear after reading it. Thank you,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/639
https://github.com/google/deepvariant/issues/639:2078,usability,prefer,preference,2078,"frequency). The neural net assigns a genotype probability for all of these positions. The confidence that the neural network has in the variant call is represented in the GQ field (which is the phred-encoded probability that the assigned genotype is correct). When the probability for reference is >50% and 99% (GQ20), the genotype assigned is ./. when the probability for reference is >99% (GQ20+) the genotype assigned is 0/0. In your gVCF calls, only this row is a REF call made by the neural net:. ```. 1	69897	.	T	C,<*>	0.7	RefCall	.	GT:GQ:DP:AD:VAF:PL	./.:8:6:2,4,0:0.666667,0:0,8,13,990,990,990. ```. The second way that DeepVariant makes a no-call/reference-call is in the process of gVCF generation. The gVCF was designed as a way to encode the probability of a region with no variant call information is a reference, so that later in joint genotyping of many samples, you can potentially change the genotype call based on what you see in the population. In your gVCF calls all other REF calls fall into this category. DeepVariant makes a gVCF with REF-CALL blocks over stretches where it observes no candidates, with a heuristic logic based on coverage and support determining the probability of these calls. In calls of this nature, there is a different logic that does not apply the GQ20 threshold. In joint genotyping, GLnexus will use the probabilities to determine the VCF call. . In summary, a variant (0/1 or 1/1) will have a variant call if it is the most likely genotype at the position. A position will receive a 0/0 call if the model observed a site with >GQ20. It can make sense to filter DeepVariant results if you have a substantial preference for precision. In addition to higher overall accuracy, we try to make DeepVariant report well-calibrated confidence probabilities. If you need higher precision for the variant calls, filtering on the call GQ is the best field. This was a long answer. Please let me know what areas remain unclear after reading it. Thank you,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/639
https://github.com/google/deepvariant/issues/639:71,usability,help,helped,71,"Hi Andrew,. Thank you very much for your response, this has definitely helped clarify the situation! . Best wishes,. Daniel.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/639
https://github.com/google/deepvariant/issues/640:233,availability,error,error,233,Thanks for the question! I tried to run the quick start on a GCP VM but could not reproduce the issue. One hint from the [documentation](https://numpy.org/devdocs/user/troubleshooting-importerror.html#c-api-incompatibility) that the error links to is that it could have to do with numpy versions. Can you try `pip install numpy --upgrade` and then rerun?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:203,deployability,api,api-incompatibility,203,Thanks for the question! I tried to run the quick start on a GCP VM but could not reproduce the issue. One hint from the [documentation](https://numpy.org/devdocs/user/troubleshooting-importerror.html#c-api-incompatibility) that the error links to is that it could have to do with numpy versions. Can you try `pip install numpy --upgrade` and then rerun?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:287,deployability,version,versions,287,Thanks for the question! I tried to run the quick start on a GCP VM but could not reproduce the issue. One hint from the [documentation](https://numpy.org/devdocs/user/troubleshooting-importerror.html#c-api-incompatibility) that the error links to is that it could have to do with numpy versions. Can you try `pip install numpy --upgrade` and then rerun?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:314,deployability,instal,install,314,Thanks for the question! I tried to run the quick start on a GCP VM but could not reproduce the issue. One hint from the [documentation](https://numpy.org/devdocs/user/troubleshooting-importerror.html#c-api-incompatibility) that the error links to is that it could have to do with numpy versions. Can you try `pip install numpy --upgrade` and then rerun?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:330,deployability,upgrad,upgrade,330,Thanks for the question! I tried to run the quick start on a GCP VM but could not reproduce the issue. One hint from the [documentation](https://numpy.org/devdocs/user/troubleshooting-importerror.html#c-api-incompatibility) that the error links to is that it could have to do with numpy versions. Can you try `pip install numpy --upgrade` and then rerun?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:203,integrability,api,api-incompatibility,203,Thanks for the question! I tried to run the quick start on a GCP VM but could not reproduce the issue. One hint from the [documentation](https://numpy.org/devdocs/user/troubleshooting-importerror.html#c-api-incompatibility) that the error links to is that it could have to do with numpy versions. Can you try `pip install numpy --upgrade` and then rerun?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:287,integrability,version,versions,287,Thanks for the question! I tried to run the quick start on a GCP VM but could not reproduce the issue. One hint from the [documentation](https://numpy.org/devdocs/user/troubleshooting-importerror.html#c-api-incompatibility) that the error links to is that it could have to do with numpy versions. Can you try `pip install numpy --upgrade` and then rerun?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:203,interoperability,api,api-incompatibility,203,Thanks for the question! I tried to run the quick start on a GCP VM but could not reproduce the issue. One hint from the [documentation](https://numpy.org/devdocs/user/troubleshooting-importerror.html#c-api-incompatibility) that the error links to is that it could have to do with numpy versions. Can you try `pip install numpy --upgrade` and then rerun?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:287,modifiability,version,versions,287,Thanks for the question! I tried to run the quick start on a GCP VM but could not reproduce the issue. One hint from the [documentation](https://numpy.org/devdocs/user/troubleshooting-importerror.html#c-api-incompatibility) that the error links to is that it could have to do with numpy versions. Can you try `pip install numpy --upgrade` and then rerun?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:330,modifiability,upgrad,upgrade,330,Thanks for the question! I tried to run the quick start on a GCP VM but could not reproduce the issue. One hint from the [documentation](https://numpy.org/devdocs/user/troubleshooting-importerror.html#c-api-incompatibility) that the error links to is that it could have to do with numpy versions. Can you try `pip install numpy --upgrade` and then rerun?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:233,performance,error,error,233,Thanks for the question! I tried to run the quick start on a GCP VM but could not reproduce the issue. One hint from the [documentation](https://numpy.org/devdocs/user/troubleshooting-importerror.html#c-api-incompatibility) that the error links to is that it could have to do with numpy versions. Can you try `pip install numpy --upgrade` and then rerun?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:233,safety,error,error,233,Thanks for the question! I tried to run the quick start on a GCP VM but could not reproduce the issue. One hint from the [documentation](https://numpy.org/devdocs/user/troubleshooting-importerror.html#c-api-incompatibility) that the error links to is that it could have to do with numpy versions. Can you try `pip install numpy --upgrade` and then rerun?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:107,usability,hint,hint,107,Thanks for the question! I tried to run the quick start on a GCP VM but could not reproduce the issue. One hint from the [documentation](https://numpy.org/devdocs/user/troubleshooting-importerror.html#c-api-incompatibility) that the error links to is that it could have to do with numpy versions. Can you try `pip install numpy --upgrade` and then rerun?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:122,usability,document,documentation,122,Thanks for the question! I tried to run the quick start on a GCP VM but could not reproduce the issue. One hint from the [documentation](https://numpy.org/devdocs/user/troubleshooting-importerror.html#c-api-incompatibility) that the error links to is that it could have to do with numpy versions. Can you try `pip install numpy --upgrade` and then rerun?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:163,usability,user,user,163,Thanks for the question! I tried to run the quick start on a GCP VM but could not reproduce the issue. One hint from the [documentation](https://numpy.org/devdocs/user/troubleshooting-importerror.html#c-api-incompatibility) that the error links to is that it could have to do with numpy versions. Can you try `pip install numpy --upgrade` and then rerun?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:233,usability,error,error,233,Thanks for the question! I tried to run the quick start on a GCP VM but could not reproduce the issue. One hint from the [documentation](https://numpy.org/devdocs/user/troubleshooting-importerror.html#c-api-incompatibility) that the error links to is that it could have to do with numpy versions. Can you try `pip install numpy --upgrade` and then rerun?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:113,deployability,contain,container,113,"Thank you for the prompt reply! I am admittedly a little confused. I am running deep variant using a singularity container, so my local numpy version shouldn't matter right? I did try creating a container using 1.4.0 and it worked...",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:142,deployability,version,version,142,"Thank you for the prompt reply! I am admittedly a little confused. I am running deep variant using a singularity container, so my local numpy version shouldn't matter right? I did try creating a container using 1.4.0 and it worked...",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:195,deployability,contain,container,195,"Thank you for the prompt reply! I am admittedly a little confused. I am running deep variant using a singularity container, so my local numpy version shouldn't matter right? I did try creating a container using 1.4.0 and it worked...",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:142,integrability,version,version,142,"Thank you for the prompt reply! I am admittedly a little confused. I am running deep variant using a singularity container, so my local numpy version shouldn't matter right? I did try creating a container using 1.4.0 and it worked...",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:142,modifiability,version,version,142,"Thank you for the prompt reply! I am admittedly a little confused. I am running deep variant using a singularity container, so my local numpy version shouldn't matter right? I did try creating a container using 1.4.0 and it worked...",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:107,deployability,version,version,107,"I was curious so I also poked around. I also wasn't able to reproduce the issue. And I also agree that the version in Singularity container should be the one that's used. ```. $ singularity --version. singularity version 3.7.0. ```. ```. BIN_VERSION=""1.5.0"". singularity pull deepvariant.sif docker://google/deepvariant:""${BIN_VERSION}"". ```. ```. $ singularity exec deepvariant.sif pip freeze | grep numpy. numpy==1.24.2. ```. ```. $ singularity shell deepvariant.sif. Singularity> python -c ""import numpy; print(numpy.__version__)"". 1.24.2. ```. On this machine I'm on, I have an older version:. ```. $ python3 -c ""import numpy; print(numpy.__version__)"". 1.17.4. ```. But when running with Singularity, it seems to be seeing the one in the container:. ```. $ singularity exec --bind ""${INPUT_DIR}"":""/input"",""${OUTPUT_DIR}"":""/output"",/usr/lib/locale/:/usr/lib/locale/ deepvariant.sif python -c ""import numpy; print(numpy.__version__)"". 1.24.2. ```. I also don't have any answer to this.. :thinking: . @scfurl Glad to hear that at least 1.4.0 works for you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:130,deployability,contain,container,130,"I was curious so I also poked around. I also wasn't able to reproduce the issue. And I also agree that the version in Singularity container should be the one that's used. ```. $ singularity --version. singularity version 3.7.0. ```. ```. BIN_VERSION=""1.5.0"". singularity pull deepvariant.sif docker://google/deepvariant:""${BIN_VERSION}"". ```. ```. $ singularity exec deepvariant.sif pip freeze | grep numpy. numpy==1.24.2. ```. ```. $ singularity shell deepvariant.sif. Singularity> python -c ""import numpy; print(numpy.__version__)"". 1.24.2. ```. On this machine I'm on, I have an older version:. ```. $ python3 -c ""import numpy; print(numpy.__version__)"". 1.17.4. ```. But when running with Singularity, it seems to be seeing the one in the container:. ```. $ singularity exec --bind ""${INPUT_DIR}"":""/input"",""${OUTPUT_DIR}"":""/output"",/usr/lib/locale/:/usr/lib/locale/ deepvariant.sif python -c ""import numpy; print(numpy.__version__)"". 1.24.2. ```. I also don't have any answer to this.. :thinking: . @scfurl Glad to hear that at least 1.4.0 works for you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:192,deployability,version,version,192,"I was curious so I also poked around. I also wasn't able to reproduce the issue. And I also agree that the version in Singularity container should be the one that's used. ```. $ singularity --version. singularity version 3.7.0. ```. ```. BIN_VERSION=""1.5.0"". singularity pull deepvariant.sif docker://google/deepvariant:""${BIN_VERSION}"". ```. ```. $ singularity exec deepvariant.sif pip freeze | grep numpy. numpy==1.24.2. ```. ```. $ singularity shell deepvariant.sif. Singularity> python -c ""import numpy; print(numpy.__version__)"". 1.24.2. ```. On this machine I'm on, I have an older version:. ```. $ python3 -c ""import numpy; print(numpy.__version__)"". 1.17.4. ```. But when running with Singularity, it seems to be seeing the one in the container:. ```. $ singularity exec --bind ""${INPUT_DIR}"":""/input"",""${OUTPUT_DIR}"":""/output"",/usr/lib/locale/:/usr/lib/locale/ deepvariant.sif python -c ""import numpy; print(numpy.__version__)"". 1.24.2. ```. I also don't have any answer to this.. :thinking: . @scfurl Glad to hear that at least 1.4.0 works for you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:213,deployability,version,version,213,"I was curious so I also poked around. I also wasn't able to reproduce the issue. And I also agree that the version in Singularity container should be the one that's used. ```. $ singularity --version. singularity version 3.7.0. ```. ```. BIN_VERSION=""1.5.0"". singularity pull deepvariant.sif docker://google/deepvariant:""${BIN_VERSION}"". ```. ```. $ singularity exec deepvariant.sif pip freeze | grep numpy. numpy==1.24.2. ```. ```. $ singularity shell deepvariant.sif. Singularity> python -c ""import numpy; print(numpy.__version__)"". 1.24.2. ```. On this machine I'm on, I have an older version:. ```. $ python3 -c ""import numpy; print(numpy.__version__)"". 1.17.4. ```. But when running with Singularity, it seems to be seeing the one in the container:. ```. $ singularity exec --bind ""${INPUT_DIR}"":""/input"",""${OUTPUT_DIR}"":""/output"",/usr/lib/locale/:/usr/lib/locale/ deepvariant.sif python -c ""import numpy; print(numpy.__version__)"". 1.24.2. ```. I also don't have any answer to this.. :thinking: . @scfurl Glad to hear that at least 1.4.0 works for you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:588,deployability,version,version,588,"I was curious so I also poked around. I also wasn't able to reproduce the issue. And I also agree that the version in Singularity container should be the one that's used. ```. $ singularity --version. singularity version 3.7.0. ```. ```. BIN_VERSION=""1.5.0"". singularity pull deepvariant.sif docker://google/deepvariant:""${BIN_VERSION}"". ```. ```. $ singularity exec deepvariant.sif pip freeze | grep numpy. numpy==1.24.2. ```. ```. $ singularity shell deepvariant.sif. Singularity> python -c ""import numpy; print(numpy.__version__)"". 1.24.2. ```. On this machine I'm on, I have an older version:. ```. $ python3 -c ""import numpy; print(numpy.__version__)"". 1.17.4. ```. But when running with Singularity, it seems to be seeing the one in the container:. ```. $ singularity exec --bind ""${INPUT_DIR}"":""/input"",""${OUTPUT_DIR}"":""/output"",/usr/lib/locale/:/usr/lib/locale/ deepvariant.sif python -c ""import numpy; print(numpy.__version__)"". 1.24.2. ```. I also don't have any answer to this.. :thinking: . @scfurl Glad to hear that at least 1.4.0 works for you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:743,deployability,contain,container,743,"I was curious so I also poked around. I also wasn't able to reproduce the issue. And I also agree that the version in Singularity container should be the one that's used. ```. $ singularity --version. singularity version 3.7.0. ```. ```. BIN_VERSION=""1.5.0"". singularity pull deepvariant.sif docker://google/deepvariant:""${BIN_VERSION}"". ```. ```. $ singularity exec deepvariant.sif pip freeze | grep numpy. numpy==1.24.2. ```. ```. $ singularity shell deepvariant.sif. Singularity> python -c ""import numpy; print(numpy.__version__)"". 1.24.2. ```. On this machine I'm on, I have an older version:. ```. $ python3 -c ""import numpy; print(numpy.__version__)"". 1.17.4. ```. But when running with Singularity, it seems to be seeing the one in the container:. ```. $ singularity exec --bind ""${INPUT_DIR}"":""/input"",""${OUTPUT_DIR}"":""/output"",/usr/lib/locale/:/usr/lib/locale/ deepvariant.sif python -c ""import numpy; print(numpy.__version__)"". 1.24.2. ```. I also don't have any answer to this.. :thinking: . @scfurl Glad to hear that at least 1.4.0 works for you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:107,integrability,version,version,107,"I was curious so I also poked around. I also wasn't able to reproduce the issue. And I also agree that the version in Singularity container should be the one that's used. ```. $ singularity --version. singularity version 3.7.0. ```. ```. BIN_VERSION=""1.5.0"". singularity pull deepvariant.sif docker://google/deepvariant:""${BIN_VERSION}"". ```. ```. $ singularity exec deepvariant.sif pip freeze | grep numpy. numpy==1.24.2. ```. ```. $ singularity shell deepvariant.sif. Singularity> python -c ""import numpy; print(numpy.__version__)"". 1.24.2. ```. On this machine I'm on, I have an older version:. ```. $ python3 -c ""import numpy; print(numpy.__version__)"". 1.17.4. ```. But when running with Singularity, it seems to be seeing the one in the container:. ```. $ singularity exec --bind ""${INPUT_DIR}"":""/input"",""${OUTPUT_DIR}"":""/output"",/usr/lib/locale/:/usr/lib/locale/ deepvariant.sif python -c ""import numpy; print(numpy.__version__)"". 1.24.2. ```. I also don't have any answer to this.. :thinking: . @scfurl Glad to hear that at least 1.4.0 works for you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:192,integrability,version,version,192,"I was curious so I also poked around. I also wasn't able to reproduce the issue. And I also agree that the version in Singularity container should be the one that's used. ```. $ singularity --version. singularity version 3.7.0. ```. ```. BIN_VERSION=""1.5.0"". singularity pull deepvariant.sif docker://google/deepvariant:""${BIN_VERSION}"". ```. ```. $ singularity exec deepvariant.sif pip freeze | grep numpy. numpy==1.24.2. ```. ```. $ singularity shell deepvariant.sif. Singularity> python -c ""import numpy; print(numpy.__version__)"". 1.24.2. ```. On this machine I'm on, I have an older version:. ```. $ python3 -c ""import numpy; print(numpy.__version__)"". 1.17.4. ```. But when running with Singularity, it seems to be seeing the one in the container:. ```. $ singularity exec --bind ""${INPUT_DIR}"":""/input"",""${OUTPUT_DIR}"":""/output"",/usr/lib/locale/:/usr/lib/locale/ deepvariant.sif python -c ""import numpy; print(numpy.__version__)"". 1.24.2. ```. I also don't have any answer to this.. :thinking: . @scfurl Glad to hear that at least 1.4.0 works for you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:213,integrability,version,version,213,"I was curious so I also poked around. I also wasn't able to reproduce the issue. And I also agree that the version in Singularity container should be the one that's used. ```. $ singularity --version. singularity version 3.7.0. ```. ```. BIN_VERSION=""1.5.0"". singularity pull deepvariant.sif docker://google/deepvariant:""${BIN_VERSION}"". ```. ```. $ singularity exec deepvariant.sif pip freeze | grep numpy. numpy==1.24.2. ```. ```. $ singularity shell deepvariant.sif. Singularity> python -c ""import numpy; print(numpy.__version__)"". 1.24.2. ```. On this machine I'm on, I have an older version:. ```. $ python3 -c ""import numpy; print(numpy.__version__)"". 1.17.4. ```. But when running with Singularity, it seems to be seeing the one in the container:. ```. $ singularity exec --bind ""${INPUT_DIR}"":""/input"",""${OUTPUT_DIR}"":""/output"",/usr/lib/locale/:/usr/lib/locale/ deepvariant.sif python -c ""import numpy; print(numpy.__version__)"". 1.24.2. ```. I also don't have any answer to this.. :thinking: . @scfurl Glad to hear that at least 1.4.0 works for you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:588,integrability,version,version,588,"I was curious so I also poked around. I also wasn't able to reproduce the issue. And I also agree that the version in Singularity container should be the one that's used. ```. $ singularity --version. singularity version 3.7.0. ```. ```. BIN_VERSION=""1.5.0"". singularity pull deepvariant.sif docker://google/deepvariant:""${BIN_VERSION}"". ```. ```. $ singularity exec deepvariant.sif pip freeze | grep numpy. numpy==1.24.2. ```. ```. $ singularity shell deepvariant.sif. Singularity> python -c ""import numpy; print(numpy.__version__)"". 1.24.2. ```. On this machine I'm on, I have an older version:. ```. $ python3 -c ""import numpy; print(numpy.__version__)"". 1.17.4. ```. But when running with Singularity, it seems to be seeing the one in the container:. ```. $ singularity exec --bind ""${INPUT_DIR}"":""/input"",""${OUTPUT_DIR}"":""/output"",/usr/lib/locale/:/usr/lib/locale/ deepvariant.sif python -c ""import numpy; print(numpy.__version__)"". 1.24.2. ```. I also don't have any answer to this.. :thinking: . @scfurl Glad to hear that at least 1.4.0 works for you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:781,interoperability,bind,bind,781,"I was curious so I also poked around. I also wasn't able to reproduce the issue. And I also agree that the version in Singularity container should be the one that's used. ```. $ singularity --version. singularity version 3.7.0. ```. ```. BIN_VERSION=""1.5.0"". singularity pull deepvariant.sif docker://google/deepvariant:""${BIN_VERSION}"". ```. ```. $ singularity exec deepvariant.sif pip freeze | grep numpy. numpy==1.24.2. ```. ```. $ singularity shell deepvariant.sif. Singularity> python -c ""import numpy; print(numpy.__version__)"". 1.24.2. ```. On this machine I'm on, I have an older version:. ```. $ python3 -c ""import numpy; print(numpy.__version__)"". 1.17.4. ```. But when running with Singularity, it seems to be seeing the one in the container:. ```. $ singularity exec --bind ""${INPUT_DIR}"":""/input"",""${OUTPUT_DIR}"":""/output"",/usr/lib/locale/:/usr/lib/locale/ deepvariant.sif python -c ""import numpy; print(numpy.__version__)"". 1.24.2. ```. I also don't have any answer to this.. :thinking: . @scfurl Glad to hear that at least 1.4.0 works for you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:107,modifiability,version,version,107,"I was curious so I also poked around. I also wasn't able to reproduce the issue. And I also agree that the version in Singularity container should be the one that's used. ```. $ singularity --version. singularity version 3.7.0. ```. ```. BIN_VERSION=""1.5.0"". singularity pull deepvariant.sif docker://google/deepvariant:""${BIN_VERSION}"". ```. ```. $ singularity exec deepvariant.sif pip freeze | grep numpy. numpy==1.24.2. ```. ```. $ singularity shell deepvariant.sif. Singularity> python -c ""import numpy; print(numpy.__version__)"". 1.24.2. ```. On this machine I'm on, I have an older version:. ```. $ python3 -c ""import numpy; print(numpy.__version__)"". 1.17.4. ```. But when running with Singularity, it seems to be seeing the one in the container:. ```. $ singularity exec --bind ""${INPUT_DIR}"":""/input"",""${OUTPUT_DIR}"":""/output"",/usr/lib/locale/:/usr/lib/locale/ deepvariant.sif python -c ""import numpy; print(numpy.__version__)"". 1.24.2. ```. I also don't have any answer to this.. :thinking: . @scfurl Glad to hear that at least 1.4.0 works for you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:192,modifiability,version,version,192,"I was curious so I also poked around. I also wasn't able to reproduce the issue. And I also agree that the version in Singularity container should be the one that's used. ```. $ singularity --version. singularity version 3.7.0. ```. ```. BIN_VERSION=""1.5.0"". singularity pull deepvariant.sif docker://google/deepvariant:""${BIN_VERSION}"". ```. ```. $ singularity exec deepvariant.sif pip freeze | grep numpy. numpy==1.24.2. ```. ```. $ singularity shell deepvariant.sif. Singularity> python -c ""import numpy; print(numpy.__version__)"". 1.24.2. ```. On this machine I'm on, I have an older version:. ```. $ python3 -c ""import numpy; print(numpy.__version__)"". 1.17.4. ```. But when running with Singularity, it seems to be seeing the one in the container:. ```. $ singularity exec --bind ""${INPUT_DIR}"":""/input"",""${OUTPUT_DIR}"":""/output"",/usr/lib/locale/:/usr/lib/locale/ deepvariant.sif python -c ""import numpy; print(numpy.__version__)"". 1.24.2. ```. I also don't have any answer to this.. :thinking: . @scfurl Glad to hear that at least 1.4.0 works for you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:213,modifiability,version,version,213,"I was curious so I also poked around. I also wasn't able to reproduce the issue. And I also agree that the version in Singularity container should be the one that's used. ```. $ singularity --version. singularity version 3.7.0. ```. ```. BIN_VERSION=""1.5.0"". singularity pull deepvariant.sif docker://google/deepvariant:""${BIN_VERSION}"". ```. ```. $ singularity exec deepvariant.sif pip freeze | grep numpy. numpy==1.24.2. ```. ```. $ singularity shell deepvariant.sif. Singularity> python -c ""import numpy; print(numpy.__version__)"". 1.24.2. ```. On this machine I'm on, I have an older version:. ```. $ python3 -c ""import numpy; print(numpy.__version__)"". 1.17.4. ```. But when running with Singularity, it seems to be seeing the one in the container:. ```. $ singularity exec --bind ""${INPUT_DIR}"":""/input"",""${OUTPUT_DIR}"":""/output"",/usr/lib/locale/:/usr/lib/locale/ deepvariant.sif python -c ""import numpy; print(numpy.__version__)"". 1.24.2. ```. I also don't have any answer to this.. :thinking: . @scfurl Glad to hear that at least 1.4.0 works for you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:588,modifiability,version,version,588,"I was curious so I also poked around. I also wasn't able to reproduce the issue. And I also agree that the version in Singularity container should be the one that's used. ```. $ singularity --version. singularity version 3.7.0. ```. ```. BIN_VERSION=""1.5.0"". singularity pull deepvariant.sif docker://google/deepvariant:""${BIN_VERSION}"". ```. ```. $ singularity exec deepvariant.sif pip freeze | grep numpy. numpy==1.24.2. ```. ```. $ singularity shell deepvariant.sif. Singularity> python -c ""import numpy; print(numpy.__version__)"". 1.24.2. ```. On this machine I'm on, I have an older version:. ```. $ python3 -c ""import numpy; print(numpy.__version__)"". 1.17.4. ```. But when running with Singularity, it seems to be seeing the one in the container:. ```. $ singularity exec --bind ""${INPUT_DIR}"":""/input"",""${OUTPUT_DIR}"":""/output"",/usr/lib/locale/:/usr/lib/locale/ deepvariant.sif python -c ""import numpy; print(numpy.__version__)"". 1.24.2. ```. I also don't have any answer to this.. :thinking: . @scfurl Glad to hear that at least 1.4.0 works for you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:781,modifiability,bind,bind,781,"I was curious so I also poked around. I also wasn't able to reproduce the issue. And I also agree that the version in Singularity container should be the one that's used. ```. $ singularity --version. singularity version 3.7.0. ```. ```. BIN_VERSION=""1.5.0"". singularity pull deepvariant.sif docker://google/deepvariant:""${BIN_VERSION}"". ```. ```. $ singularity exec deepvariant.sif pip freeze | grep numpy. numpy==1.24.2. ```. ```. $ singularity shell deepvariant.sif. Singularity> python -c ""import numpy; print(numpy.__version__)"". 1.24.2. ```. On this machine I'm on, I have an older version:. ```. $ python3 -c ""import numpy; print(numpy.__version__)"". 1.17.4. ```. But when running with Singularity, it seems to be seeing the one in the container:. ```. $ singularity exec --bind ""${INPUT_DIR}"":""/input"",""${OUTPUT_DIR}"":""/output"",/usr/lib/locale/:/usr/lib/locale/ deepvariant.sif python -c ""import numpy; print(numpy.__version__)"". 1.24.2. ```. I also don't have any answer to this.. :thinking: . @scfurl Glad to hear that at least 1.4.0 works for you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:803,safety,input,input,803,"I was curious so I also poked around. I also wasn't able to reproduce the issue. And I also agree that the version in Singularity container should be the one that's used. ```. $ singularity --version. singularity version 3.7.0. ```. ```. BIN_VERSION=""1.5.0"". singularity pull deepvariant.sif docker://google/deepvariant:""${BIN_VERSION}"". ```. ```. $ singularity exec deepvariant.sif pip freeze | grep numpy. numpy==1.24.2. ```. ```. $ singularity shell deepvariant.sif. Singularity> python -c ""import numpy; print(numpy.__version__)"". 1.24.2. ```. On this machine I'm on, I have an older version:. ```. $ python3 -c ""import numpy; print(numpy.__version__)"". 1.17.4. ```. But when running with Singularity, it seems to be seeing the one in the container:. ```. $ singularity exec --bind ""${INPUT_DIR}"":""/input"",""${OUTPUT_DIR}"":""/output"",/usr/lib/locale/:/usr/lib/locale/ deepvariant.sif python -c ""import numpy; print(numpy.__version__)"". 1.24.2. ```. I also don't have any answer to this.. :thinking: . @scfurl Glad to hear that at least 1.4.0 works for you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:803,usability,input,input,803,"I was curious so I also poked around. I also wasn't able to reproduce the issue. And I also agree that the version in Singularity container should be the one that's used. ```. $ singularity --version. singularity version 3.7.0. ```. ```. BIN_VERSION=""1.5.0"". singularity pull deepvariant.sif docker://google/deepvariant:""${BIN_VERSION}"". ```. ```. $ singularity exec deepvariant.sif pip freeze | grep numpy. numpy==1.24.2. ```. ```. $ singularity shell deepvariant.sif. Singularity> python -c ""import numpy; print(numpy.__version__)"". 1.24.2. ```. On this machine I'm on, I have an older version:. ```. $ python3 -c ""import numpy; print(numpy.__version__)"". 1.17.4. ```. But when running with Singularity, it seems to be seeing the one in the container:. ```. $ singularity exec --bind ""${INPUT_DIR}"":""/input"",""${OUTPUT_DIR}"":""/output"",/usr/lib/locale/:/usr/lib/locale/ deepvariant.sif python -c ""import numpy; print(numpy.__version__)"". 1.24.2. ```. I also don't have any answer to this.. :thinking: . @scfurl Glad to hear that at least 1.4.0 works for you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:31,deployability,version,version,31,"@pichuan is correct. The numpy version being run is the one from inside the Singularity image. Based on the [Singularity source code](https://github.com/sylabs/singularity/blob/main/cmd/internal/cli/singularity.go#L596-L601) it is using the Go [os/exec](https://pkg.go.dev/os/exec#pkg-overview) package. An exec command will overwrite the whole current process to execute another program, which means the numpy version has to come from the image itself.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:411,deployability,version,version,411,"@pichuan is correct. The numpy version being run is the one from inside the Singularity image. Based on the [Singularity source code](https://github.com/sylabs/singularity/blob/main/cmd/internal/cli/singularity.go#L596-L601) it is using the Go [os/exec](https://pkg.go.dev/os/exec#pkg-overview) package. An exec command will overwrite the whole current process to execute another program, which means the numpy version has to come from the image itself.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:345,energy efficiency,current,current,345,"@pichuan is correct. The numpy version being run is the one from inside the Singularity image. Based on the [Singularity source code](https://github.com/sylabs/singularity/blob/main/cmd/internal/cli/singularity.go#L596-L601) it is using the Go [os/exec](https://pkg.go.dev/os/exec#pkg-overview) package. An exec command will overwrite the whole current process to execute another program, which means the numpy version has to come from the image itself.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:31,integrability,version,version,31,"@pichuan is correct. The numpy version being run is the one from inside the Singularity image. Based on the [Singularity source code](https://github.com/sylabs/singularity/blob/main/cmd/internal/cli/singularity.go#L596-L601) it is using the Go [os/exec](https://pkg.go.dev/os/exec#pkg-overview) package. An exec command will overwrite the whole current process to execute another program, which means the numpy version has to come from the image itself.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:411,integrability,version,version,411,"@pichuan is correct. The numpy version being run is the one from inside the Singularity image. Based on the [Singularity source code](https://github.com/sylabs/singularity/blob/main/cmd/internal/cli/singularity.go#L596-L601) it is using the Go [os/exec](https://pkg.go.dev/os/exec#pkg-overview) package. An exec command will overwrite the whole current process to execute another program, which means the numpy version has to come from the image itself.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:31,modifiability,version,version,31,"@pichuan is correct. The numpy version being run is the one from inside the Singularity image. Based on the [Singularity source code](https://github.com/sylabs/singularity/blob/main/cmd/internal/cli/singularity.go#L596-L601) it is using the Go [os/exec](https://pkg.go.dev/os/exec#pkg-overview) package. An exec command will overwrite the whole current process to execute another program, which means the numpy version has to come from the image itself.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:295,modifiability,pac,package,295,"@pichuan is correct. The numpy version being run is the one from inside the Singularity image. Based on the [Singularity source code](https://github.com/sylabs/singularity/blob/main/cmd/internal/cli/singularity.go#L596-L601) it is using the Go [os/exec](https://pkg.go.dev/os/exec#pkg-overview) package. An exec command will overwrite the whole current process to execute another program, which means the numpy version has to come from the image itself.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:411,modifiability,version,version,411,"@pichuan is correct. The numpy version being run is the one from inside the Singularity image. Based on the [Singularity source code](https://github.com/sylabs/singularity/blob/main/cmd/internal/cli/singularity.go#L596-L601) it is using the Go [os/exec](https://pkg.go.dev/os/exec#pkg-overview) package. An exec command will overwrite the whole current process to execute another program, which means the numpy version has to come from the image itself.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:312,usability,command,command,312,"@pichuan is correct. The numpy version being run is the one from inside the Singularity image. Based on the [Singularity source code](https://github.com/sylabs/singularity/blob/main/cmd/internal/cli/singularity.go#L596-L601) it is using the Go [os/exec](https://pkg.go.dev/os/exec#pkg-overview) package. An exec command will overwrite the whole current process to execute another program, which means the numpy version has to come from the image itself.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:138,deployability,contain,contains,138,I am running into the same issue when running on a small BAM file only. (I'm also using docker instead of singularity). The BAM file only contains this region: `chr22:0-40001`. Could the file size be a factor here?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:52,deployability,modul,module,52,"FYI:. The command output is. ```bash. RuntimeError: module compiled against API version 0x10 but this version of numpy is 0xf . Check the section C-API incompatibility at the Troubleshooting ImportError. section at https://numpy.org/devdocs/user/troubleshooting-importerror.html#c-api-incompatibility for indications on how to solve this problem . . Traceback (most recent call last): . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196, in <module> . app.run(main) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/absl_py/absl/app.py"", line 312, in run . _run_main(main, args) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/absl_py/absl/app.py"", line 258, in _run_main . sys.exit(main(argv)) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186, in main . make_examples_core.make_examples_runner(options) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2189, in make_examples_runner . region_example_shape = region_processor.writes_examples_in_region( . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1187, in writes_examples_in_region . for example in self.create_pileup_examples( . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1793, in create_pileup_examples . pileup_images = self.pic.create_pileup_images( . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 534, in create_pileup_images . pileup = _pileup_for_pair_of_alts(alts) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 486, in _pileup_for_pair_of_alts . ref_image = self.build_pileup( . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 435, in build_pileup . build_pileup_for_one_s",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:76,deployability,API,API,76,"FYI:. The command output is. ```bash. RuntimeError: module compiled against API version 0x10 but this version of numpy is 0xf . Check the section C-API incompatibility at the Troubleshooting ImportError. section at https://numpy.org/devdocs/user/troubleshooting-importerror.html#c-api-incompatibility for indications on how to solve this problem . . Traceback (most recent call last): . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196, in <module> . app.run(main) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/absl_py/absl/app.py"", line 312, in run . _run_main(main, args) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/absl_py/absl/app.py"", line 258, in _run_main . sys.exit(main(argv)) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186, in main . make_examples_core.make_examples_runner(options) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2189, in make_examples_runner . region_example_shape = region_processor.writes_examples_in_region( . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1187, in writes_examples_in_region . for example in self.create_pileup_examples( . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1793, in create_pileup_examples . pileup_images = self.pic.create_pileup_images( . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 534, in create_pileup_images . pileup = _pileup_for_pair_of_alts(alts) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 486, in _pileup_for_pair_of_alts . ref_image = self.build_pileup( . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 435, in build_pileup . build_pileup_for_one_s",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:80,deployability,version,version,80,"FYI:. The command output is. ```bash. RuntimeError: module compiled against API version 0x10 but this version of numpy is 0xf . Check the section C-API incompatibility at the Troubleshooting ImportError. section at https://numpy.org/devdocs/user/troubleshooting-importerror.html#c-api-incompatibility for indications on how to solve this problem . . Traceback (most recent call last): . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196, in <module> . app.run(main) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/absl_py/absl/app.py"", line 312, in run . _run_main(main, args) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/absl_py/absl/app.py"", line 258, in _run_main . sys.exit(main(argv)) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186, in main . make_examples_core.make_examples_runner(options) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2189, in make_examples_runner . region_example_shape = region_processor.writes_examples_in_region( . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1187, in writes_examples_in_region . for example in self.create_pileup_examples( . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1793, in create_pileup_examples . pileup_images = self.pic.create_pileup_images( . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 534, in create_pileup_images . pileup = _pileup_for_pair_of_alts(alts) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 486, in _pileup_for_pair_of_alts . ref_image = self.build_pileup( . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 435, in build_pileup . build_pileup_for_one_s",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:102,deployability,version,version,102,"FYI:. The command output is. ```bash. RuntimeError: module compiled against API version 0x10 but this version of numpy is 0xf . Check the section C-API incompatibility at the Troubleshooting ImportError. section at https://numpy.org/devdocs/user/troubleshooting-importerror.html#c-api-incompatibility for indications on how to solve this problem . . Traceback (most recent call last): . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196, in <module> . app.run(main) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/absl_py/absl/app.py"", line 312, in run . _run_main(main, args) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/absl_py/absl/app.py"", line 258, in _run_main . sys.exit(main(argv)) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186, in main . make_examples_core.make_examples_runner(options) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2189, in make_examples_runner . region_example_shape = region_processor.writes_examples_in_region( . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1187, in writes_examples_in_region . for example in self.create_pileup_examples( . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1793, in create_pileup_examples . pileup_images = self.pic.create_pileup_images( . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 534, in create_pileup_images . pileup = _pileup_for_pair_of_alts(alts) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 486, in _pileup_for_pair_of_alts . ref_image = self.build_pileup( . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 435, in build_pileup . build_pileup_for_one_s",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:148,deployability,API,API,148,"FYI:. The command output is. ```bash. RuntimeError: module compiled against API version 0x10 but this version of numpy is 0xf . Check the section C-API incompatibility at the Troubleshooting ImportError. section at https://numpy.org/devdocs/user/troubleshooting-importerror.html#c-api-incompatibility for indications on how to solve this problem . . Traceback (most recent call last): . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196, in <module> . app.run(main) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/absl_py/absl/app.py"", line 312, in run . _run_main(main, args) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/absl_py/absl/app.py"", line 258, in _run_main . sys.exit(main(argv)) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186, in main . make_examples_core.make_examples_runner(options) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2189, in make_examples_runner . region_example_shape = region_processor.writes_examples_in_region( . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1187, in writes_examples_in_region . for example in self.create_pileup_examples( . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1793, in create_pileup_examples . pileup_images = self.pic.create_pileup_images( . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 534, in create_pileup_images . pileup = _pileup_for_pair_of_alts(alts) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 486, in _pileup_for_pair_of_alts . ref_image = self.build_pileup( . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 435, in build_pileup . build_pileup_for_one_s",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:281,deployability,api,api-incompatibility,281,"FYI:. The command output is. ```bash. RuntimeError: module compiled against API version 0x10 but this version of numpy is 0xf . Check the section C-API incompatibility at the Troubleshooting ImportError. section at https://numpy.org/devdocs/user/troubleshooting-importerror.html#c-api-incompatibility for indications on how to solve this problem . . Traceback (most recent call last): . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196, in <module> . app.run(main) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/absl_py/absl/app.py"", line 312, in run . _run_main(main, args) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/absl_py/absl/app.py"", line 258, in _run_main . sys.exit(main(argv)) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186, in main . make_examples_core.make_examples_runner(options) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2189, in make_examples_runner . region_example_shape = region_processor.writes_examples_in_region( . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1187, in writes_examples_in_region . for example in self.create_pileup_examples( . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1793, in create_pileup_examples . pileup_images = self.pic.create_pileup_images( . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 534, in create_pileup_images . pileup = _pileup_for_pair_of_alts(alts) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 486, in _pileup_for_pair_of_alts . ref_image = self.build_pileup( . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 435, in build_pileup . build_pileup_for_one_s",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:499,deployability,modul,module,499,"FYI:. The command output is. ```bash. RuntimeError: module compiled against API version 0x10 but this version of numpy is 0xf . Check the section C-API incompatibility at the Troubleshooting ImportError. section at https://numpy.org/devdocs/user/troubleshooting-importerror.html#c-api-incompatibility for indications on how to solve this problem . . Traceback (most recent call last): . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196, in <module> . app.run(main) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/absl_py/absl/app.py"", line 312, in run . _run_main(main, args) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/absl_py/absl/app.py"", line 258, in _run_main . sys.exit(main(argv)) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186, in main . make_examples_core.make_examples_runner(options) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2189, in make_examples_runner . region_example_shape = region_processor.writes_examples_in_region( . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1187, in writes_examples_in_region . for example in self.create_pileup_examples( . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1793, in create_pileup_examples . pileup_images = self.pic.create_pileup_images( . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 534, in create_pileup_images . pileup = _pileup_for_pair_of_alts(alts) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 486, in _pileup_for_pair_of_alts . ref_image = self.build_pileup( . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 435, in build_pileup . build_pileup_for_one_s",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:2269,deployability,fail,failed,2269,"or example in self.create_pileup_examples( . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1793, in create_pileup_examples . pileup_images = self.pic.create_pileup_images( . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 534, in create_pileup_images . pileup = _pileup_for_pair_of_alts(alts) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 486, in _pileup_for_pair_of_alts . ref_image = self.build_pileup( . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 435, in build_pileup . build_pileup_for_one_sample(reads_for_samples[i], sample)) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 355, in build_pileup_for_one_sample . rows = ([self._encoder.encode_reference(refbases)] * . ImportError: numpy.core.multiarray failed to import . I0516 14:57:26.170286 139707932989248 make_examples_core.py:257] Task 0/2: Writing example info to ./make_examples.tfrecord-00000-of-00002.gz.example_info.json . I0516 14:57:26.170386 139707932989248 make_examples_core.py:2273] example_shape = None . I0516 14:57:26.170525 139707932989248 make_examples_core.py:2274] example_channels = [1, 2, 3, 4, 5, 6, 19] . I0516 14:57:26.170763 139707932989248 make_examples_core.py:257] Task 0/2: Found 0 candidate variants . I0516 14:57:26.170813 139707932989248 make_examples_core.py:257] Task 0/2: Created 0 examples . parallel: This job failed: . /opt/deepvariant/bin/make_examples --mode calling --ref genome.fasta --reads test.paired_end.sorted.cram --examples ./make_examples.tfrecord@2.gz --channels . insert_size --gvcf ./gvcf.tfrecord@2.gz --regions genome.bed --task 1 . ```. And the command is. ```bash. /opt/deepvariant/bin/run_deepvariant \. --ref=genome.fasta \. --reads=test.paired_end.sorted.cram \. --output_vcf=test_out.vcf.gz",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:2868,deployability,fail,failed,2868,"pileup_images( . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 534, in create_pileup_images . pileup = _pileup_for_pair_of_alts(alts) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 486, in _pileup_for_pair_of_alts . ref_image = self.build_pileup( . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 435, in build_pileup . build_pileup_for_one_sample(reads_for_samples[i], sample)) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 355, in build_pileup_for_one_sample . rows = ([self._encoder.encode_reference(refbases)] * . ImportError: numpy.core.multiarray failed to import . I0516 14:57:26.170286 139707932989248 make_examples_core.py:257] Task 0/2: Writing example info to ./make_examples.tfrecord-00000-of-00002.gz.example_info.json . I0516 14:57:26.170386 139707932989248 make_examples_core.py:2273] example_shape = None . I0516 14:57:26.170525 139707932989248 make_examples_core.py:2274] example_channels = [1, 2, 3, 4, 5, 6, 19] . I0516 14:57:26.170763 139707932989248 make_examples_core.py:257] Task 0/2: Found 0 candidate variants . I0516 14:57:26.170813 139707932989248 make_examples_core.py:257] Task 0/2: Created 0 examples . parallel: This job failed: . /opt/deepvariant/bin/make_examples --mode calling --ref genome.fasta --reads test.paired_end.sorted.cram --examples ./make_examples.tfrecord@2.gz --channels . insert_size --gvcf ./gvcf.tfrecord@2.gz --regions genome.bed --task 1 . ```. And the command is. ```bash. /opt/deepvariant/bin/run_deepvariant \. --ref=genome.fasta \. --reads=test.paired_end.sorted.cram \. --output_vcf=test_out.vcf.gz \. --output_gvcf=test_out.g.vcf.gz \. --model_type=WGS \. --regions genome.bed \. --intermediate_results_dir=. \. --num_shards=2. ```. Deepvariant version: 1.5.0 (the official docker container). Ran on Ubuntu 20.04 LTS",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:3420,deployability,version,version,3420,"pileup_images( . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 534, in create_pileup_images . pileup = _pileup_for_pair_of_alts(alts) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 486, in _pileup_for_pair_of_alts . ref_image = self.build_pileup( . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 435, in build_pileup . build_pileup_for_one_sample(reads_for_samples[i], sample)) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 355, in build_pileup_for_one_sample . rows = ([self._encoder.encode_reference(refbases)] * . ImportError: numpy.core.multiarray failed to import . I0516 14:57:26.170286 139707932989248 make_examples_core.py:257] Task 0/2: Writing example info to ./make_examples.tfrecord-00000-of-00002.gz.example_info.json . I0516 14:57:26.170386 139707932989248 make_examples_core.py:2273] example_shape = None . I0516 14:57:26.170525 139707932989248 make_examples_core.py:2274] example_channels = [1, 2, 3, 4, 5, 6, 19] . I0516 14:57:26.170763 139707932989248 make_examples_core.py:257] Task 0/2: Found 0 candidate variants . I0516 14:57:26.170813 139707932989248 make_examples_core.py:257] Task 0/2: Created 0 examples . parallel: This job failed: . /opt/deepvariant/bin/make_examples --mode calling --ref genome.fasta --reads test.paired_end.sorted.cram --examples ./make_examples.tfrecord@2.gz --channels . insert_size --gvcf ./gvcf.tfrecord@2.gz --regions genome.bed --task 1 . ```. And the command is. ```bash. /opt/deepvariant/bin/run_deepvariant \. --ref=genome.fasta \. --reads=test.paired_end.sorted.cram \. --output_vcf=test_out.vcf.gz \. --output_gvcf=test_out.g.vcf.gz \. --model_type=WGS \. --regions genome.bed \. --intermediate_results_dir=. \. --num_shards=2. ```. Deepvariant version: 1.5.0 (the official docker container). Ran on Ubuntu 20.04 LTS",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:3456,deployability,contain,container,3456,"pileup_images( . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 534, in create_pileup_images . pileup = _pileup_for_pair_of_alts(alts) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 486, in _pileup_for_pair_of_alts . ref_image = self.build_pileup( . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 435, in build_pileup . build_pileup_for_one_sample(reads_for_samples[i], sample)) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 355, in build_pileup_for_one_sample . rows = ([self._encoder.encode_reference(refbases)] * . ImportError: numpy.core.multiarray failed to import . I0516 14:57:26.170286 139707932989248 make_examples_core.py:257] Task 0/2: Writing example info to ./make_examples.tfrecord-00000-of-00002.gz.example_info.json . I0516 14:57:26.170386 139707932989248 make_examples_core.py:2273] example_shape = None . I0516 14:57:26.170525 139707932989248 make_examples_core.py:2274] example_channels = [1, 2, 3, 4, 5, 6, 19] . I0516 14:57:26.170763 139707932989248 make_examples_core.py:257] Task 0/2: Found 0 candidate variants . I0516 14:57:26.170813 139707932989248 make_examples_core.py:257] Task 0/2: Created 0 examples . parallel: This job failed: . /opt/deepvariant/bin/make_examples --mode calling --ref genome.fasta --reads test.paired_end.sorted.cram --examples ./make_examples.tfrecord@2.gz --channels . insert_size --gvcf ./gvcf.tfrecord@2.gz --regions genome.bed --task 1 . ```. And the command is. ```bash. /opt/deepvariant/bin/run_deepvariant \. --ref=genome.fasta \. --reads=test.paired_end.sorted.cram \. --output_vcf=test_out.vcf.gz \. --output_gvcf=test_out.g.vcf.gz \. --model_type=WGS \. --regions genome.bed \. --intermediate_results_dir=. \. --num_shards=2. ```. Deepvariant version: 1.5.0 (the official docker container). Ran on Ubuntu 20.04 LTS",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:2253,energy efficiency,core,core,2253,"les_in_region . for example in self.create_pileup_examples( . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1793, in create_pileup_examples . pileup_images = self.pic.create_pileup_images( . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 534, in create_pileup_images . pileup = _pileup_for_pair_of_alts(alts) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 486, in _pileup_for_pair_of_alts . ref_image = self.build_pileup( . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 435, in build_pileup . build_pileup_for_one_sample(reads_for_samples[i], sample)) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 355, in build_pileup_for_one_sample . rows = ([self._encoder.encode_reference(refbases)] * . ImportError: numpy.core.multiarray failed to import . I0516 14:57:26.170286 139707932989248 make_examples_core.py:257] Task 0/2: Writing example info to ./make_examples.tfrecord-00000-of-00002.gz.example_info.json . I0516 14:57:26.170386 139707932989248 make_examples_core.py:2273] example_shape = None . I0516 14:57:26.170525 139707932989248 make_examples_core.py:2274] example_channels = [1, 2, 3, 4, 5, 6, 19] . I0516 14:57:26.170763 139707932989248 make_examples_core.py:257] Task 0/2: Found 0 candidate variants . I0516 14:57:26.170813 139707932989248 make_examples_core.py:257] Task 0/2: Created 0 examples . parallel: This job failed: . /opt/deepvariant/bin/make_examples --mode calling --ref genome.fasta --reads test.paired_end.sorted.cram --examples ./make_examples.tfrecord@2.gz --channels . insert_size --gvcf ./gvcf.tfrecord@2.gz --regions genome.bed --task 1 . ```. And the command is. ```bash. /opt/deepvariant/bin/run_deepvariant \. --ref=genome.fasta \. --reads=test.paired_end.sorted.cram \. --output_vc",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:76,integrability,API,API,76,"FYI:. The command output is. ```bash. RuntimeError: module compiled against API version 0x10 but this version of numpy is 0xf . Check the section C-API incompatibility at the Troubleshooting ImportError. section at https://numpy.org/devdocs/user/troubleshooting-importerror.html#c-api-incompatibility for indications on how to solve this problem . . Traceback (most recent call last): . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196, in <module> . app.run(main) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/absl_py/absl/app.py"", line 312, in run . _run_main(main, args) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/absl_py/absl/app.py"", line 258, in _run_main . sys.exit(main(argv)) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186, in main . make_examples_core.make_examples_runner(options) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2189, in make_examples_runner . region_example_shape = region_processor.writes_examples_in_region( . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1187, in writes_examples_in_region . for example in self.create_pileup_examples( . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1793, in create_pileup_examples . pileup_images = self.pic.create_pileup_images( . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 534, in create_pileup_images . pileup = _pileup_for_pair_of_alts(alts) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 486, in _pileup_for_pair_of_alts . ref_image = self.build_pileup( . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 435, in build_pileup . build_pileup_for_one_s",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:80,integrability,version,version,80,"FYI:. The command output is. ```bash. RuntimeError: module compiled against API version 0x10 but this version of numpy is 0xf . Check the section C-API incompatibility at the Troubleshooting ImportError. section at https://numpy.org/devdocs/user/troubleshooting-importerror.html#c-api-incompatibility for indications on how to solve this problem . . Traceback (most recent call last): . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196, in <module> . app.run(main) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/absl_py/absl/app.py"", line 312, in run . _run_main(main, args) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/absl_py/absl/app.py"", line 258, in _run_main . sys.exit(main(argv)) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186, in main . make_examples_core.make_examples_runner(options) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2189, in make_examples_runner . region_example_shape = region_processor.writes_examples_in_region( . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1187, in writes_examples_in_region . for example in self.create_pileup_examples( . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1793, in create_pileup_examples . pileup_images = self.pic.create_pileup_images( . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 534, in create_pileup_images . pileup = _pileup_for_pair_of_alts(alts) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 486, in _pileup_for_pair_of_alts . ref_image = self.build_pileup( . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 435, in build_pileup . build_pileup_for_one_s",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:102,integrability,version,version,102,"FYI:. The command output is. ```bash. RuntimeError: module compiled against API version 0x10 but this version of numpy is 0xf . Check the section C-API incompatibility at the Troubleshooting ImportError. section at https://numpy.org/devdocs/user/troubleshooting-importerror.html#c-api-incompatibility for indications on how to solve this problem . . Traceback (most recent call last): . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196, in <module> . app.run(main) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/absl_py/absl/app.py"", line 312, in run . _run_main(main, args) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/absl_py/absl/app.py"", line 258, in _run_main . sys.exit(main(argv)) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186, in main . make_examples_core.make_examples_runner(options) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2189, in make_examples_runner . region_example_shape = region_processor.writes_examples_in_region( . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1187, in writes_examples_in_region . for example in self.create_pileup_examples( . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1793, in create_pileup_examples . pileup_images = self.pic.create_pileup_images( . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 534, in create_pileup_images . pileup = _pileup_for_pair_of_alts(alts) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 486, in _pileup_for_pair_of_alts . ref_image = self.build_pileup( . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 435, in build_pileup . build_pileup_for_one_s",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:148,integrability,API,API,148,"FYI:. The command output is. ```bash. RuntimeError: module compiled against API version 0x10 but this version of numpy is 0xf . Check the section C-API incompatibility at the Troubleshooting ImportError. section at https://numpy.org/devdocs/user/troubleshooting-importerror.html#c-api-incompatibility for indications on how to solve this problem . . Traceback (most recent call last): . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196, in <module> . app.run(main) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/absl_py/absl/app.py"", line 312, in run . _run_main(main, args) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/absl_py/absl/app.py"", line 258, in _run_main . sys.exit(main(argv)) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186, in main . make_examples_core.make_examples_runner(options) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2189, in make_examples_runner . region_example_shape = region_processor.writes_examples_in_region( . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1187, in writes_examples_in_region . for example in self.create_pileup_examples( . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1793, in create_pileup_examples . pileup_images = self.pic.create_pileup_images( . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 534, in create_pileup_images . pileup = _pileup_for_pair_of_alts(alts) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 486, in _pileup_for_pair_of_alts . ref_image = self.build_pileup( . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 435, in build_pileup . build_pileup_for_one_s",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:281,integrability,api,api-incompatibility,281,"FYI:. The command output is. ```bash. RuntimeError: module compiled against API version 0x10 but this version of numpy is 0xf . Check the section C-API incompatibility at the Troubleshooting ImportError. section at https://numpy.org/devdocs/user/troubleshooting-importerror.html#c-api-incompatibility for indications on how to solve this problem . . Traceback (most recent call last): . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196, in <module> . app.run(main) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/absl_py/absl/app.py"", line 312, in run . _run_main(main, args) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/absl_py/absl/app.py"", line 258, in _run_main . sys.exit(main(argv)) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186, in main . make_examples_core.make_examples_runner(options) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2189, in make_examples_runner . region_example_shape = region_processor.writes_examples_in_region( . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1187, in writes_examples_in_region . for example in self.create_pileup_examples( . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1793, in create_pileup_examples . pileup_images = self.pic.create_pileup_images( . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 534, in create_pileup_images . pileup = _pileup_for_pair_of_alts(alts) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 486, in _pileup_for_pair_of_alts . ref_image = self.build_pileup( . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 435, in build_pileup . build_pileup_for_one_s",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:3420,integrability,version,version,3420,"pileup_images( . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 534, in create_pileup_images . pileup = _pileup_for_pair_of_alts(alts) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 486, in _pileup_for_pair_of_alts . ref_image = self.build_pileup( . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 435, in build_pileup . build_pileup_for_one_sample(reads_for_samples[i], sample)) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 355, in build_pileup_for_one_sample . rows = ([self._encoder.encode_reference(refbases)] * . ImportError: numpy.core.multiarray failed to import . I0516 14:57:26.170286 139707932989248 make_examples_core.py:257] Task 0/2: Writing example info to ./make_examples.tfrecord-00000-of-00002.gz.example_info.json . I0516 14:57:26.170386 139707932989248 make_examples_core.py:2273] example_shape = None . I0516 14:57:26.170525 139707932989248 make_examples_core.py:2274] example_channels = [1, 2, 3, 4, 5, 6, 19] . I0516 14:57:26.170763 139707932989248 make_examples_core.py:257] Task 0/2: Found 0 candidate variants . I0516 14:57:26.170813 139707932989248 make_examples_core.py:257] Task 0/2: Created 0 examples . parallel: This job failed: . /opt/deepvariant/bin/make_examples --mode calling --ref genome.fasta --reads test.paired_end.sorted.cram --examples ./make_examples.tfrecord@2.gz --channels . insert_size --gvcf ./gvcf.tfrecord@2.gz --regions genome.bed --task 1 . ```. And the command is. ```bash. /opt/deepvariant/bin/run_deepvariant \. --ref=genome.fasta \. --reads=test.paired_end.sorted.cram \. --output_vcf=test_out.vcf.gz \. --output_gvcf=test_out.g.vcf.gz \. --model_type=WGS \. --regions genome.bed \. --intermediate_results_dir=. \. --num_shards=2. ```. Deepvariant version: 1.5.0 (the official docker container). Ran on Ubuntu 20.04 LTS",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:76,interoperability,API,API,76,"FYI:. The command output is. ```bash. RuntimeError: module compiled against API version 0x10 but this version of numpy is 0xf . Check the section C-API incompatibility at the Troubleshooting ImportError. section at https://numpy.org/devdocs/user/troubleshooting-importerror.html#c-api-incompatibility for indications on how to solve this problem . . Traceback (most recent call last): . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196, in <module> . app.run(main) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/absl_py/absl/app.py"", line 312, in run . _run_main(main, args) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/absl_py/absl/app.py"", line 258, in _run_main . sys.exit(main(argv)) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186, in main . make_examples_core.make_examples_runner(options) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2189, in make_examples_runner . region_example_shape = region_processor.writes_examples_in_region( . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1187, in writes_examples_in_region . for example in self.create_pileup_examples( . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1793, in create_pileup_examples . pileup_images = self.pic.create_pileup_images( . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 534, in create_pileup_images . pileup = _pileup_for_pair_of_alts(alts) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 486, in _pileup_for_pair_of_alts . ref_image = self.build_pileup( . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 435, in build_pileup . build_pileup_for_one_s",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:148,interoperability,API,API,148,"FYI:. The command output is. ```bash. RuntimeError: module compiled against API version 0x10 but this version of numpy is 0xf . Check the section C-API incompatibility at the Troubleshooting ImportError. section at https://numpy.org/devdocs/user/troubleshooting-importerror.html#c-api-incompatibility for indications on how to solve this problem . . Traceback (most recent call last): . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196, in <module> . app.run(main) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/absl_py/absl/app.py"", line 312, in run . _run_main(main, args) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/absl_py/absl/app.py"", line 258, in _run_main . sys.exit(main(argv)) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186, in main . make_examples_core.make_examples_runner(options) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2189, in make_examples_runner . region_example_shape = region_processor.writes_examples_in_region( . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1187, in writes_examples_in_region . for example in self.create_pileup_examples( . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1793, in create_pileup_examples . pileup_images = self.pic.create_pileup_images( . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 534, in create_pileup_images . pileup = _pileup_for_pair_of_alts(alts) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 486, in _pileup_for_pair_of_alts . ref_image = self.build_pileup( . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 435, in build_pileup . build_pileup_for_one_s",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:152,interoperability,incompatib,incompatibility,152,"FYI:. The command output is. ```bash. RuntimeError: module compiled against API version 0x10 but this version of numpy is 0xf . Check the section C-API incompatibility at the Troubleshooting ImportError. section at https://numpy.org/devdocs/user/troubleshooting-importerror.html#c-api-incompatibility for indications on how to solve this problem . . Traceback (most recent call last): . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196, in <module> . app.run(main) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/absl_py/absl/app.py"", line 312, in run . _run_main(main, args) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/absl_py/absl/app.py"", line 258, in _run_main . sys.exit(main(argv)) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186, in main . make_examples_core.make_examples_runner(options) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2189, in make_examples_runner . region_example_shape = region_processor.writes_examples_in_region( . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1187, in writes_examples_in_region . for example in self.create_pileup_examples( . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1793, in create_pileup_examples . pileup_images = self.pic.create_pileup_images( . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 534, in create_pileup_images . pileup = _pileup_for_pair_of_alts(alts) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 486, in _pileup_for_pair_of_alts . ref_image = self.build_pileup( . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 435, in build_pileup . build_pileup_for_one_s",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:281,interoperability,api,api-incompatibility,281,"FYI:. The command output is. ```bash. RuntimeError: module compiled against API version 0x10 but this version of numpy is 0xf . Check the section C-API incompatibility at the Troubleshooting ImportError. section at https://numpy.org/devdocs/user/troubleshooting-importerror.html#c-api-incompatibility for indications on how to solve this problem . . Traceback (most recent call last): . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196, in <module> . app.run(main) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/absl_py/absl/app.py"", line 312, in run . _run_main(main, args) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/absl_py/absl/app.py"", line 258, in _run_main . sys.exit(main(argv)) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186, in main . make_examples_core.make_examples_runner(options) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2189, in make_examples_runner . region_example_shape = region_processor.writes_examples_in_region( . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1187, in writes_examples_in_region . for example in self.create_pileup_examples( . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1793, in create_pileup_examples . pileup_images = self.pic.create_pileup_images( . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 534, in create_pileup_images . pileup = _pileup_for_pair_of_alts(alts) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 486, in _pileup_for_pair_of_alts . ref_image = self.build_pileup( . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 435, in build_pileup . build_pileup_for_one_s",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:52,modifiability,modul,module,52,"FYI:. The command output is. ```bash. RuntimeError: module compiled against API version 0x10 but this version of numpy is 0xf . Check the section C-API incompatibility at the Troubleshooting ImportError. section at https://numpy.org/devdocs/user/troubleshooting-importerror.html#c-api-incompatibility for indications on how to solve this problem . . Traceback (most recent call last): . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196, in <module> . app.run(main) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/absl_py/absl/app.py"", line 312, in run . _run_main(main, args) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/absl_py/absl/app.py"", line 258, in _run_main . sys.exit(main(argv)) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186, in main . make_examples_core.make_examples_runner(options) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2189, in make_examples_runner . region_example_shape = region_processor.writes_examples_in_region( . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1187, in writes_examples_in_region . for example in self.create_pileup_examples( . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1793, in create_pileup_examples . pileup_images = self.pic.create_pileup_images( . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 534, in create_pileup_images . pileup = _pileup_for_pair_of_alts(alts) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 486, in _pileup_for_pair_of_alts . ref_image = self.build_pileup( . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 435, in build_pileup . build_pileup_for_one_s",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:80,modifiability,version,version,80,"FYI:. The command output is. ```bash. RuntimeError: module compiled against API version 0x10 but this version of numpy is 0xf . Check the section C-API incompatibility at the Troubleshooting ImportError. section at https://numpy.org/devdocs/user/troubleshooting-importerror.html#c-api-incompatibility for indications on how to solve this problem . . Traceback (most recent call last): . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196, in <module> . app.run(main) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/absl_py/absl/app.py"", line 312, in run . _run_main(main, args) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/absl_py/absl/app.py"", line 258, in _run_main . sys.exit(main(argv)) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186, in main . make_examples_core.make_examples_runner(options) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2189, in make_examples_runner . region_example_shape = region_processor.writes_examples_in_region( . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1187, in writes_examples_in_region . for example in self.create_pileup_examples( . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1793, in create_pileup_examples . pileup_images = self.pic.create_pileup_images( . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 534, in create_pileup_images . pileup = _pileup_for_pair_of_alts(alts) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 486, in _pileup_for_pair_of_alts . ref_image = self.build_pileup( . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 435, in build_pileup . build_pileup_for_one_s",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:102,modifiability,version,version,102,"FYI:. The command output is. ```bash. RuntimeError: module compiled against API version 0x10 but this version of numpy is 0xf . Check the section C-API incompatibility at the Troubleshooting ImportError. section at https://numpy.org/devdocs/user/troubleshooting-importerror.html#c-api-incompatibility for indications on how to solve this problem . . Traceback (most recent call last): . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196, in <module> . app.run(main) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/absl_py/absl/app.py"", line 312, in run . _run_main(main, args) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/absl_py/absl/app.py"", line 258, in _run_main . sys.exit(main(argv)) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186, in main . make_examples_core.make_examples_runner(options) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2189, in make_examples_runner . region_example_shape = region_processor.writes_examples_in_region( . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1187, in writes_examples_in_region . for example in self.create_pileup_examples( . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1793, in create_pileup_examples . pileup_images = self.pic.create_pileup_images( . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 534, in create_pileup_images . pileup = _pileup_for_pair_of_alts(alts) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 486, in _pileup_for_pair_of_alts . ref_image = self.build_pileup( . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 435, in build_pileup . build_pileup_for_one_s",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:499,modifiability,modul,module,499,"FYI:. The command output is. ```bash. RuntimeError: module compiled against API version 0x10 but this version of numpy is 0xf . Check the section C-API incompatibility at the Troubleshooting ImportError. section at https://numpy.org/devdocs/user/troubleshooting-importerror.html#c-api-incompatibility for indications on how to solve this problem . . Traceback (most recent call last): . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196, in <module> . app.run(main) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/absl_py/absl/app.py"", line 312, in run . _run_main(main, args) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/absl_py/absl/app.py"", line 258, in _run_main . sys.exit(main(argv)) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186, in main . make_examples_core.make_examples_runner(options) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2189, in make_examples_runner . region_example_shape = region_processor.writes_examples_in_region( . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1187, in writes_examples_in_region . for example in self.create_pileup_examples( . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1793, in create_pileup_examples . pileup_images = self.pic.create_pileup_images( . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 534, in create_pileup_images . pileup = _pileup_for_pair_of_alts(alts) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 486, in _pileup_for_pair_of_alts . ref_image = self.build_pileup( . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 435, in build_pileup . build_pileup_for_one_s",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:3420,modifiability,version,version,3420,"pileup_images( . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 534, in create_pileup_images . pileup = _pileup_for_pair_of_alts(alts) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 486, in _pileup_for_pair_of_alts . ref_image = self.build_pileup( . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 435, in build_pileup . build_pileup_for_one_sample(reads_for_samples[i], sample)) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 355, in build_pileup_for_one_sample . rows = ([self._encoder.encode_reference(refbases)] * . ImportError: numpy.core.multiarray failed to import . I0516 14:57:26.170286 139707932989248 make_examples_core.py:257] Task 0/2: Writing example info to ./make_examples.tfrecord-00000-of-00002.gz.example_info.json . I0516 14:57:26.170386 139707932989248 make_examples_core.py:2273] example_shape = None . I0516 14:57:26.170525 139707932989248 make_examples_core.py:2274] example_channels = [1, 2, 3, 4, 5, 6, 19] . I0516 14:57:26.170763 139707932989248 make_examples_core.py:257] Task 0/2: Found 0 candidate variants . I0516 14:57:26.170813 139707932989248 make_examples_core.py:257] Task 0/2: Created 0 examples . parallel: This job failed: . /opt/deepvariant/bin/make_examples --mode calling --ref genome.fasta --reads test.paired_end.sorted.cram --examples ./make_examples.tfrecord@2.gz --channels . insert_size --gvcf ./gvcf.tfrecord@2.gz --regions genome.bed --task 1 . ```. And the command is. ```bash. /opt/deepvariant/bin/run_deepvariant \. --ref=genome.fasta \. --reads=test.paired_end.sorted.cram \. --output_vcf=test_out.vcf.gz \. --output_gvcf=test_out.g.vcf.gz \. --model_type=WGS \. --regions genome.bed \. --intermediate_results_dir=. \. --num_shards=2. ```. Deepvariant version: 1.5.0 (the official docker container). Ran on Ubuntu 20.04 LTS",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:2849,performance,parallel,parallel,2849,"pileup_images( . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 534, in create_pileup_images . pileup = _pileup_for_pair_of_alts(alts) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 486, in _pileup_for_pair_of_alts . ref_image = self.build_pileup( . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 435, in build_pileup . build_pileup_for_one_sample(reads_for_samples[i], sample)) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 355, in build_pileup_for_one_sample . rows = ([self._encoder.encode_reference(refbases)] * . ImportError: numpy.core.multiarray failed to import . I0516 14:57:26.170286 139707932989248 make_examples_core.py:257] Task 0/2: Writing example info to ./make_examples.tfrecord-00000-of-00002.gz.example_info.json . I0516 14:57:26.170386 139707932989248 make_examples_core.py:2273] example_shape = None . I0516 14:57:26.170525 139707932989248 make_examples_core.py:2274] example_channels = [1, 2, 3, 4, 5, 6, 19] . I0516 14:57:26.170763 139707932989248 make_examples_core.py:257] Task 0/2: Found 0 candidate variants . I0516 14:57:26.170813 139707932989248 make_examples_core.py:257] Task 0/2: Created 0 examples . parallel: This job failed: . /opt/deepvariant/bin/make_examples --mode calling --ref genome.fasta --reads test.paired_end.sorted.cram --examples ./make_examples.tfrecord@2.gz --channels . insert_size --gvcf ./gvcf.tfrecord@2.gz --regions genome.bed --task 1 . ```. And the command is. ```bash. /opt/deepvariant/bin/run_deepvariant \. --ref=genome.fasta \. --reads=test.paired_end.sorted.cram \. --output_vcf=test_out.vcf.gz \. --output_gvcf=test_out.g.vcf.gz \. --model_type=WGS \. --regions genome.bed \. --intermediate_results_dir=. \. --num_shards=2. ```. Deepvariant version: 1.5.0 (the official docker container). Ran on Ubuntu 20.04 LTS",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:2269,reliability,fail,failed,2269,"or example in self.create_pileup_examples( . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1793, in create_pileup_examples . pileup_images = self.pic.create_pileup_images( . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 534, in create_pileup_images . pileup = _pileup_for_pair_of_alts(alts) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 486, in _pileup_for_pair_of_alts . ref_image = self.build_pileup( . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 435, in build_pileup . build_pileup_for_one_sample(reads_for_samples[i], sample)) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 355, in build_pileup_for_one_sample . rows = ([self._encoder.encode_reference(refbases)] * . ImportError: numpy.core.multiarray failed to import . I0516 14:57:26.170286 139707932989248 make_examples_core.py:257] Task 0/2: Writing example info to ./make_examples.tfrecord-00000-of-00002.gz.example_info.json . I0516 14:57:26.170386 139707932989248 make_examples_core.py:2273] example_shape = None . I0516 14:57:26.170525 139707932989248 make_examples_core.py:2274] example_channels = [1, 2, 3, 4, 5, 6, 19] . I0516 14:57:26.170763 139707932989248 make_examples_core.py:257] Task 0/2: Found 0 candidate variants . I0516 14:57:26.170813 139707932989248 make_examples_core.py:257] Task 0/2: Created 0 examples . parallel: This job failed: . /opt/deepvariant/bin/make_examples --mode calling --ref genome.fasta --reads test.paired_end.sorted.cram --examples ./make_examples.tfrecord@2.gz --channels . insert_size --gvcf ./gvcf.tfrecord@2.gz --regions genome.bed --task 1 . ```. And the command is. ```bash. /opt/deepvariant/bin/run_deepvariant \. --ref=genome.fasta \. --reads=test.paired_end.sorted.cram \. --output_vcf=test_out.vcf.gz",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:2868,reliability,fail,failed,2868,"pileup_images( . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 534, in create_pileup_images . pileup = _pileup_for_pair_of_alts(alts) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 486, in _pileup_for_pair_of_alts . ref_image = self.build_pileup( . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 435, in build_pileup . build_pileup_for_one_sample(reads_for_samples[i], sample)) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 355, in build_pileup_for_one_sample . rows = ([self._encoder.encode_reference(refbases)] * . ImportError: numpy.core.multiarray failed to import . I0516 14:57:26.170286 139707932989248 make_examples_core.py:257] Task 0/2: Writing example info to ./make_examples.tfrecord-00000-of-00002.gz.example_info.json . I0516 14:57:26.170386 139707932989248 make_examples_core.py:2273] example_shape = None . I0516 14:57:26.170525 139707932989248 make_examples_core.py:2274] example_channels = [1, 2, 3, 4, 5, 6, 19] . I0516 14:57:26.170763 139707932989248 make_examples_core.py:257] Task 0/2: Found 0 candidate variants . I0516 14:57:26.170813 139707932989248 make_examples_core.py:257] Task 0/2: Created 0 examples . parallel: This job failed: . /opt/deepvariant/bin/make_examples --mode calling --ref genome.fasta --reads test.paired_end.sorted.cram --examples ./make_examples.tfrecord@2.gz --channels . insert_size --gvcf ./gvcf.tfrecord@2.gz --regions genome.bed --task 1 . ```. And the command is. ```bash. /opt/deepvariant/bin/run_deepvariant \. --ref=genome.fasta \. --reads=test.paired_end.sorted.cram \. --output_vcf=test_out.vcf.gz \. --output_gvcf=test_out.g.vcf.gz \. --model_type=WGS \. --regions genome.bed \. --intermediate_results_dir=. \. --num_shards=2. ```. Deepvariant version: 1.5.0 (the official docker container). Ran on Ubuntu 20.04 LTS",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:52,safety,modul,module,52,"FYI:. The command output is. ```bash. RuntimeError: module compiled against API version 0x10 but this version of numpy is 0xf . Check the section C-API incompatibility at the Troubleshooting ImportError. section at https://numpy.org/devdocs/user/troubleshooting-importerror.html#c-api-incompatibility for indications on how to solve this problem . . Traceback (most recent call last): . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196, in <module> . app.run(main) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/absl_py/absl/app.py"", line 312, in run . _run_main(main, args) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/absl_py/absl/app.py"", line 258, in _run_main . sys.exit(main(argv)) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186, in main . make_examples_core.make_examples_runner(options) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2189, in make_examples_runner . region_example_shape = region_processor.writes_examples_in_region( . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1187, in writes_examples_in_region . for example in self.create_pileup_examples( . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1793, in create_pileup_examples . pileup_images = self.pic.create_pileup_images( . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 534, in create_pileup_images . pileup = _pileup_for_pair_of_alts(alts) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 486, in _pileup_for_pair_of_alts . ref_image = self.build_pileup( . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 435, in build_pileup . build_pileup_for_one_s",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:499,safety,modul,module,499,"FYI:. The command output is. ```bash. RuntimeError: module compiled against API version 0x10 but this version of numpy is 0xf . Check the section C-API incompatibility at the Troubleshooting ImportError. section at https://numpy.org/devdocs/user/troubleshooting-importerror.html#c-api-incompatibility for indications on how to solve this problem . . Traceback (most recent call last): . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196, in <module> . app.run(main) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/absl_py/absl/app.py"", line 312, in run . _run_main(main, args) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/absl_py/absl/app.py"", line 258, in _run_main . sys.exit(main(argv)) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186, in main . make_examples_core.make_examples_runner(options) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2189, in make_examples_runner . region_example_shape = region_processor.writes_examples_in_region( . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1187, in writes_examples_in_region . for example in self.create_pileup_examples( . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1793, in create_pileup_examples . pileup_images = self.pic.create_pileup_images( . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 534, in create_pileup_images . pileup = _pileup_for_pair_of_alts(alts) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 486, in _pileup_for_pair_of_alts . ref_image = self.build_pileup( . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 435, in build_pileup . build_pileup_for_one_s",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:2955,safety,test,test,2955,"pileup_images( . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 534, in create_pileup_images . pileup = _pileup_for_pair_of_alts(alts) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 486, in _pileup_for_pair_of_alts . ref_image = self.build_pileup( . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 435, in build_pileup . build_pileup_for_one_sample(reads_for_samples[i], sample)) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 355, in build_pileup_for_one_sample . rows = ([self._encoder.encode_reference(refbases)] * . ImportError: numpy.core.multiarray failed to import . I0516 14:57:26.170286 139707932989248 make_examples_core.py:257] Task 0/2: Writing example info to ./make_examples.tfrecord-00000-of-00002.gz.example_info.json . I0516 14:57:26.170386 139707932989248 make_examples_core.py:2273] example_shape = None . I0516 14:57:26.170525 139707932989248 make_examples_core.py:2274] example_channels = [1, 2, 3, 4, 5, 6, 19] . I0516 14:57:26.170763 139707932989248 make_examples_core.py:257] Task 0/2: Found 0 candidate variants . I0516 14:57:26.170813 139707932989248 make_examples_core.py:257] Task 0/2: Created 0 examples . parallel: This job failed: . /opt/deepvariant/bin/make_examples --mode calling --ref genome.fasta --reads test.paired_end.sorted.cram --examples ./make_examples.tfrecord@2.gz --channels . insert_size --gvcf ./gvcf.tfrecord@2.gz --regions genome.bed --task 1 . ```. And the command is. ```bash. /opt/deepvariant/bin/run_deepvariant \. --ref=genome.fasta \. --reads=test.paired_end.sorted.cram \. --output_vcf=test_out.vcf.gz \. --output_gvcf=test_out.g.vcf.gz \. --model_type=WGS \. --regions genome.bed \. --intermediate_results_dir=. \. --num_shards=2. ```. Deepvariant version: 1.5.0 (the official docker container). Ran on Ubuntu 20.04 LTS",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:3213,safety,test,test,3213,"pileup_images( . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 534, in create_pileup_images . pileup = _pileup_for_pair_of_alts(alts) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 486, in _pileup_for_pair_of_alts . ref_image = self.build_pileup( . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 435, in build_pileup . build_pileup_for_one_sample(reads_for_samples[i], sample)) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 355, in build_pileup_for_one_sample . rows = ([self._encoder.encode_reference(refbases)] * . ImportError: numpy.core.multiarray failed to import . I0516 14:57:26.170286 139707932989248 make_examples_core.py:257] Task 0/2: Writing example info to ./make_examples.tfrecord-00000-of-00002.gz.example_info.json . I0516 14:57:26.170386 139707932989248 make_examples_core.py:2273] example_shape = None . I0516 14:57:26.170525 139707932989248 make_examples_core.py:2274] example_channels = [1, 2, 3, 4, 5, 6, 19] . I0516 14:57:26.170763 139707932989248 make_examples_core.py:257] Task 0/2: Found 0 candidate variants . I0516 14:57:26.170813 139707932989248 make_examples_core.py:257] Task 0/2: Created 0 examples . parallel: This job failed: . /opt/deepvariant/bin/make_examples --mode calling --ref genome.fasta --reads test.paired_end.sorted.cram --examples ./make_examples.tfrecord@2.gz --channels . insert_size --gvcf ./gvcf.tfrecord@2.gz --regions genome.bed --task 1 . ```. And the command is. ```bash. /opt/deepvariant/bin/run_deepvariant \. --ref=genome.fasta \. --reads=test.paired_end.sorted.cram \. --output_vcf=test_out.vcf.gz \. --output_gvcf=test_out.g.vcf.gz \. --model_type=WGS \. --regions genome.bed \. --intermediate_results_dir=. \. --num_shards=2. ```. Deepvariant version: 1.5.0 (the official docker container). Ran on Ubuntu 20.04 LTS",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:350,testability,Trace,Traceback,350,"FYI:. The command output is. ```bash. RuntimeError: module compiled against API version 0x10 but this version of numpy is 0xf . Check the section C-API incompatibility at the Troubleshooting ImportError. section at https://numpy.org/devdocs/user/troubleshooting-importerror.html#c-api-incompatibility for indications on how to solve this problem . . Traceback (most recent call last): . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196, in <module> . app.run(main) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/absl_py/absl/app.py"", line 312, in run . _run_main(main, args) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/absl_py/absl/app.py"", line 258, in _run_main . sys.exit(main(argv)) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186, in main . make_examples_core.make_examples_runner(options) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2189, in make_examples_runner . region_example_shape = region_processor.writes_examples_in_region( . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1187, in writes_examples_in_region . for example in self.create_pileup_examples( . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1793, in create_pileup_examples . pileup_images = self.pic.create_pileup_images( . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 534, in create_pileup_images . pileup = _pileup_for_pair_of_alts(alts) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 486, in _pileup_for_pair_of_alts . ref_image = self.build_pileup( . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 435, in build_pileup . build_pileup_for_one_s",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:2955,testability,test,test,2955,"pileup_images( . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 534, in create_pileup_images . pileup = _pileup_for_pair_of_alts(alts) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 486, in _pileup_for_pair_of_alts . ref_image = self.build_pileup( . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 435, in build_pileup . build_pileup_for_one_sample(reads_for_samples[i], sample)) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 355, in build_pileup_for_one_sample . rows = ([self._encoder.encode_reference(refbases)] * . ImportError: numpy.core.multiarray failed to import . I0516 14:57:26.170286 139707932989248 make_examples_core.py:257] Task 0/2: Writing example info to ./make_examples.tfrecord-00000-of-00002.gz.example_info.json . I0516 14:57:26.170386 139707932989248 make_examples_core.py:2273] example_shape = None . I0516 14:57:26.170525 139707932989248 make_examples_core.py:2274] example_channels = [1, 2, 3, 4, 5, 6, 19] . I0516 14:57:26.170763 139707932989248 make_examples_core.py:257] Task 0/2: Found 0 candidate variants . I0516 14:57:26.170813 139707932989248 make_examples_core.py:257] Task 0/2: Created 0 examples . parallel: This job failed: . /opt/deepvariant/bin/make_examples --mode calling --ref genome.fasta --reads test.paired_end.sorted.cram --examples ./make_examples.tfrecord@2.gz --channels . insert_size --gvcf ./gvcf.tfrecord@2.gz --regions genome.bed --task 1 . ```. And the command is. ```bash. /opt/deepvariant/bin/run_deepvariant \. --ref=genome.fasta \. --reads=test.paired_end.sorted.cram \. --output_vcf=test_out.vcf.gz \. --output_gvcf=test_out.g.vcf.gz \. --model_type=WGS \. --regions genome.bed \. --intermediate_results_dir=. \. --num_shards=2. ```. Deepvariant version: 1.5.0 (the official docker container). Ran on Ubuntu 20.04 LTS",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:3213,testability,test,test,3213,"pileup_images( . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 534, in create_pileup_images . pileup = _pileup_for_pair_of_alts(alts) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 486, in _pileup_for_pair_of_alts . ref_image = self.build_pileup( . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 435, in build_pileup . build_pileup_for_one_sample(reads_for_samples[i], sample)) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 355, in build_pileup_for_one_sample . rows = ([self._encoder.encode_reference(refbases)] * . ImportError: numpy.core.multiarray failed to import . I0516 14:57:26.170286 139707932989248 make_examples_core.py:257] Task 0/2: Writing example info to ./make_examples.tfrecord-00000-of-00002.gz.example_info.json . I0516 14:57:26.170386 139707932989248 make_examples_core.py:2273] example_shape = None . I0516 14:57:26.170525 139707932989248 make_examples_core.py:2274] example_channels = [1, 2, 3, 4, 5, 6, 19] . I0516 14:57:26.170763 139707932989248 make_examples_core.py:257] Task 0/2: Found 0 candidate variants . I0516 14:57:26.170813 139707932989248 make_examples_core.py:257] Task 0/2: Created 0 examples . parallel: This job failed: . /opt/deepvariant/bin/make_examples --mode calling --ref genome.fasta --reads test.paired_end.sorted.cram --examples ./make_examples.tfrecord@2.gz --channels . insert_size --gvcf ./gvcf.tfrecord@2.gz --regions genome.bed --task 1 . ```. And the command is. ```bash. /opt/deepvariant/bin/run_deepvariant \. --ref=genome.fasta \. --reads=test.paired_end.sorted.cram \. --output_vcf=test_out.vcf.gz \. --output_gvcf=test_out.g.vcf.gz \. --model_type=WGS \. --regions genome.bed \. --intermediate_results_dir=. \. --num_shards=2. ```. Deepvariant version: 1.5.0 (the official docker container). Ran on Ubuntu 20.04 LTS",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:10,usability,command,command,10,"FYI:. The command output is. ```bash. RuntimeError: module compiled against API version 0x10 but this version of numpy is 0xf . Check the section C-API incompatibility at the Troubleshooting ImportError. section at https://numpy.org/devdocs/user/troubleshooting-importerror.html#c-api-incompatibility for indications on how to solve this problem . . Traceback (most recent call last): . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196, in <module> . app.run(main) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/absl_py/absl/app.py"", line 312, in run . _run_main(main, args) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/absl_py/absl/app.py"", line 258, in _run_main . sys.exit(main(argv)) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186, in main . make_examples_core.make_examples_runner(options) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2189, in make_examples_runner . region_example_shape = region_processor.writes_examples_in_region( . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1187, in writes_examples_in_region . for example in self.create_pileup_examples( . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1793, in create_pileup_examples . pileup_images = self.pic.create_pileup_images( . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 534, in create_pileup_images . pileup = _pileup_for_pair_of_alts(alts) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 486, in _pileup_for_pair_of_alts . ref_image = self.build_pileup( . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 435, in build_pileup . build_pileup_for_one_s",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:241,usability,user,user,241,"FYI:. The command output is. ```bash. RuntimeError: module compiled against API version 0x10 but this version of numpy is 0xf . Check the section C-API incompatibility at the Troubleshooting ImportError. section at https://numpy.org/devdocs/user/troubleshooting-importerror.html#c-api-incompatibility for indications on how to solve this problem . . Traceback (most recent call last): . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196, in <module> . app.run(main) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/absl_py/absl/app.py"", line 312, in run . _run_main(main, args) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/absl_py/absl/app.py"", line 258, in _run_main . sys.exit(main(argv)) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186, in main . make_examples_core.make_examples_runner(options) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2189, in make_examples_runner . region_example_shape = region_processor.writes_examples_in_region( . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1187, in writes_examples_in_region . for example in self.create_pileup_examples( . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1793, in create_pileup_examples . pileup_images = self.pic.create_pileup_images( . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 534, in create_pileup_images . pileup = _pileup_for_pair_of_alts(alts) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 486, in _pileup_for_pair_of_alts . ref_image = self.build_pileup( . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 435, in build_pileup . build_pileup_for_one_s",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:305,usability,indicat,indications,305,"FYI:. The command output is. ```bash. RuntimeError: module compiled against API version 0x10 but this version of numpy is 0xf . Check the section C-API incompatibility at the Troubleshooting ImportError. section at https://numpy.org/devdocs/user/troubleshooting-importerror.html#c-api-incompatibility for indications on how to solve this problem . . Traceback (most recent call last): . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196, in <module> . app.run(main) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/absl_py/absl/app.py"", line 312, in run . _run_main(main, args) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/absl_py/absl/app.py"", line 258, in _run_main . sys.exit(main(argv)) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186, in main . make_examples_core.make_examples_runner(options) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2189, in make_examples_runner . region_example_shape = region_processor.writes_examples_in_region( . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1187, in writes_examples_in_region . for example in self.create_pileup_examples( . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1793, in create_pileup_examples . pileup_images = self.pic.create_pileup_images( . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 534, in create_pileup_images . pileup = _pileup_for_pair_of_alts(alts) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 486, in _pileup_for_pair_of_alts . ref_image = self.build_pileup( . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 435, in build_pileup . build_pileup_for_one_s",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:3122,usability,command,command,3122,"pileup_images( . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 534, in create_pileup_images . pileup = _pileup_for_pair_of_alts(alts) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 486, in _pileup_for_pair_of_alts . ref_image = self.build_pileup( . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 435, in build_pileup . build_pileup_for_one_sample(reads_for_samples[i], sample)) . File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 355, in build_pileup_for_one_sample . rows = ([self._encoder.encode_reference(refbases)] * . ImportError: numpy.core.multiarray failed to import . I0516 14:57:26.170286 139707932989248 make_examples_core.py:257] Task 0/2: Writing example info to ./make_examples.tfrecord-00000-of-00002.gz.example_info.json . I0516 14:57:26.170386 139707932989248 make_examples_core.py:2273] example_shape = None . I0516 14:57:26.170525 139707932989248 make_examples_core.py:2274] example_channels = [1, 2, 3, 4, 5, 6, 19] . I0516 14:57:26.170763 139707932989248 make_examples_core.py:257] Task 0/2: Found 0 candidate variants . I0516 14:57:26.170813 139707932989248 make_examples_core.py:257] Task 0/2: Created 0 examples . parallel: This job failed: . /opt/deepvariant/bin/make_examples --mode calling --ref genome.fasta --reads test.paired_end.sorted.cram --examples ./make_examples.tfrecord@2.gz --channels . insert_size --gvcf ./gvcf.tfrecord@2.gz --regions genome.bed --task 1 . ```. And the command is. ```bash. /opt/deepvariant/bin/run_deepvariant \. --ref=genome.fasta \. --reads=test.paired_end.sorted.cram \. --output_vcf=test_out.vcf.gz \. --output_gvcf=test_out.g.vcf.gz \. --model_type=WGS \. --regions genome.bed \. --intermediate_results_dir=. \. --num_shards=2. ```. Deepvariant version: 1.5.0 (the official docker container). Ran on Ubuntu 20.04 LTS",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:10,reliability,Doe,Does,10,@nvnieuwk Does it mean you only encounter this issue when running on a small region? Does the same setting work when you run something like the Quick Start?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:85,reliability,Doe,Does,85,@nvnieuwk Does it mean you only encounter this issue when running on a small region? Does the same setting work when you run something like the Quick Start?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:133,deployability,contain,container,133,"@nvnieuwk Are you sure you are running inside the Docker image? I listed all the files regarding the `/tmp` folder inside the Docker container (which all must be the same for all users), and I cannot find the ones shown above:. ```Bash. root@0f98b9adcd2d:/# /opt/deepvariant/bin/run_deepvariant --version. DeepVariant version 1.5.0. root@0f98b9adcd2d:/# ls /tmp/Bazel.runfiles_gd__toh4/. ls: cannot access '/tmp/Bazel.runfiles_gd__toh4/': No such file or directory. root@0f98b9adcd2d:/# find /tmp. /tmp. /tmp/tmpb20xyssf. /tmp/__pycache__. /tmp/__pycache__/__autograph_generated_file7y1wd6da.cpython-38.pyc. /tmp/__pycache__/__autograph_generated_fileilucivm5.cpython-38.pyc. /tmp/tmp0y_1vxbg. root@0f98b9adcd2d:/# find | grep bazel. ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazelize_command.py. ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazel_selected_py3.cpython-38.pyc. ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazelize_command.cpython-38.pyc. ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazel_selected_py3.py. root@0f98b9adcd2d:/#. ```. I remember seeing things like `/tmp/Bazel.runfiles_gd__toh4/` only when I compiled DeepVariant from scratch. It's a tricky task to get it compiled, so I just wanted to be sure you didn't mix up the session you might be running in.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:297,deployability,version,version,297,"@nvnieuwk Are you sure you are running inside the Docker image? I listed all the files regarding the `/tmp` folder inside the Docker container (which all must be the same for all users), and I cannot find the ones shown above:. ```Bash. root@0f98b9adcd2d:/# /opt/deepvariant/bin/run_deepvariant --version. DeepVariant version 1.5.0. root@0f98b9adcd2d:/# ls /tmp/Bazel.runfiles_gd__toh4/. ls: cannot access '/tmp/Bazel.runfiles_gd__toh4/': No such file or directory. root@0f98b9adcd2d:/# find /tmp. /tmp. /tmp/tmpb20xyssf. /tmp/__pycache__. /tmp/__pycache__/__autograph_generated_file7y1wd6da.cpython-38.pyc. /tmp/__pycache__/__autograph_generated_fileilucivm5.cpython-38.pyc. /tmp/tmp0y_1vxbg. root@0f98b9adcd2d:/# find | grep bazel. ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazelize_command.py. ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazel_selected_py3.cpython-38.pyc. ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazelize_command.cpython-38.pyc. ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazel_selected_py3.py. root@0f98b9adcd2d:/#. ```. I remember seeing things like `/tmp/Bazel.runfiles_gd__toh4/` only when I compiled DeepVariant from scratch. It's a tricky task to get it compiled, so I just wanted to be sure you didn't mix up the session you might be running in.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:318,deployability,version,version,318,"@nvnieuwk Are you sure you are running inside the Docker image? I listed all the files regarding the `/tmp` folder inside the Docker container (which all must be the same for all users), and I cannot find the ones shown above:. ```Bash. root@0f98b9adcd2d:/# /opt/deepvariant/bin/run_deepvariant --version. DeepVariant version 1.5.0. root@0f98b9adcd2d:/# ls /tmp/Bazel.runfiles_gd__toh4/. ls: cannot access '/tmp/Bazel.runfiles_gd__toh4/': No such file or directory. root@0f98b9adcd2d:/# find /tmp. /tmp. /tmp/tmpb20xyssf. /tmp/__pycache__. /tmp/__pycache__/__autograph_generated_file7y1wd6da.cpython-38.pyc. /tmp/__pycache__/__autograph_generated_fileilucivm5.cpython-38.pyc. /tmp/tmp0y_1vxbg. root@0f98b9adcd2d:/# find | grep bazel. ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazelize_command.py. ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazel_selected_py3.cpython-38.pyc. ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazelize_command.cpython-38.pyc. ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazel_selected_py3.py. root@0f98b9adcd2d:/#. ```. I remember seeing things like `/tmp/Bazel.runfiles_gd__toh4/` only when I compiled DeepVariant from scratch. It's a tricky task to get it compiled, so I just wanted to be sure you didn't mix up the session you might be running in.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:297,integrability,version,version,297,"@nvnieuwk Are you sure you are running inside the Docker image? I listed all the files regarding the `/tmp` folder inside the Docker container (which all must be the same for all users), and I cannot find the ones shown above:. ```Bash. root@0f98b9adcd2d:/# /opt/deepvariant/bin/run_deepvariant --version. DeepVariant version 1.5.0. root@0f98b9adcd2d:/# ls /tmp/Bazel.runfiles_gd__toh4/. ls: cannot access '/tmp/Bazel.runfiles_gd__toh4/': No such file or directory. root@0f98b9adcd2d:/# find /tmp. /tmp. /tmp/tmpb20xyssf. /tmp/__pycache__. /tmp/__pycache__/__autograph_generated_file7y1wd6da.cpython-38.pyc. /tmp/__pycache__/__autograph_generated_fileilucivm5.cpython-38.pyc. /tmp/tmp0y_1vxbg. root@0f98b9adcd2d:/# find | grep bazel. ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazelize_command.py. ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazel_selected_py3.cpython-38.pyc. ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazelize_command.cpython-38.pyc. ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazel_selected_py3.py. root@0f98b9adcd2d:/#. ```. I remember seeing things like `/tmp/Bazel.runfiles_gd__toh4/` only when I compiled DeepVariant from scratch. It's a tricky task to get it compiled, so I just wanted to be sure you didn't mix up the session you might be running in.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:318,integrability,version,version,318,"@nvnieuwk Are you sure you are running inside the Docker image? I listed all the files regarding the `/tmp` folder inside the Docker container (which all must be the same for all users), and I cannot find the ones shown above:. ```Bash. root@0f98b9adcd2d:/# /opt/deepvariant/bin/run_deepvariant --version. DeepVariant version 1.5.0. root@0f98b9adcd2d:/# ls /tmp/Bazel.runfiles_gd__toh4/. ls: cannot access '/tmp/Bazel.runfiles_gd__toh4/': No such file or directory. root@0f98b9adcd2d:/# find /tmp. /tmp. /tmp/tmpb20xyssf. /tmp/__pycache__. /tmp/__pycache__/__autograph_generated_file7y1wd6da.cpython-38.pyc. /tmp/__pycache__/__autograph_generated_fileilucivm5.cpython-38.pyc. /tmp/tmp0y_1vxbg. root@0f98b9adcd2d:/# find | grep bazel. ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazelize_command.py. ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazel_selected_py3.cpython-38.pyc. ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazelize_command.cpython-38.pyc. ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazel_selected_py3.py. root@0f98b9adcd2d:/#. ```. I remember seeing things like `/tmp/Bazel.runfiles_gd__toh4/` only when I compiled DeepVariant from scratch. It's a tricky task to get it compiled, so I just wanted to be sure you didn't mix up the session you might be running in.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:297,modifiability,version,version,297,"@nvnieuwk Are you sure you are running inside the Docker image? I listed all the files regarding the `/tmp` folder inside the Docker container (which all must be the same for all users), and I cannot find the ones shown above:. ```Bash. root@0f98b9adcd2d:/# /opt/deepvariant/bin/run_deepvariant --version. DeepVariant version 1.5.0. root@0f98b9adcd2d:/# ls /tmp/Bazel.runfiles_gd__toh4/. ls: cannot access '/tmp/Bazel.runfiles_gd__toh4/': No such file or directory. root@0f98b9adcd2d:/# find /tmp. /tmp. /tmp/tmpb20xyssf. /tmp/__pycache__. /tmp/__pycache__/__autograph_generated_file7y1wd6da.cpython-38.pyc. /tmp/__pycache__/__autograph_generated_fileilucivm5.cpython-38.pyc. /tmp/tmp0y_1vxbg. root@0f98b9adcd2d:/# find | grep bazel. ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazelize_command.py. ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazel_selected_py3.cpython-38.pyc. ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazelize_command.cpython-38.pyc. ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazel_selected_py3.py. root@0f98b9adcd2d:/#. ```. I remember seeing things like `/tmp/Bazel.runfiles_gd__toh4/` only when I compiled DeepVariant from scratch. It's a tricky task to get it compiled, so I just wanted to be sure you didn't mix up the session you might be running in.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:318,modifiability,version,version,318,"@nvnieuwk Are you sure you are running inside the Docker image? I listed all the files regarding the `/tmp` folder inside the Docker container (which all must be the same for all users), and I cannot find the ones shown above:. ```Bash. root@0f98b9adcd2d:/# /opt/deepvariant/bin/run_deepvariant --version. DeepVariant version 1.5.0. root@0f98b9adcd2d:/# ls /tmp/Bazel.runfiles_gd__toh4/. ls: cannot access '/tmp/Bazel.runfiles_gd__toh4/': No such file or directory. root@0f98b9adcd2d:/# find /tmp. /tmp. /tmp/tmpb20xyssf. /tmp/__pycache__. /tmp/__pycache__/__autograph_generated_file7y1wd6da.cpython-38.pyc. /tmp/__pycache__/__autograph_generated_fileilucivm5.cpython-38.pyc. /tmp/tmp0y_1vxbg. root@0f98b9adcd2d:/# find | grep bazel. ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazelize_command.py. ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazel_selected_py3.cpython-38.pyc. ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazelize_command.cpython-38.pyc. ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazel_selected_py3.py. root@0f98b9adcd2d:/#. ```. I remember seeing things like `/tmp/Bazel.runfiles_gd__toh4/` only when I compiled DeepVariant from scratch. It's a tricky task to get it compiled, so I just wanted to be sure you didn't mix up the session you might be running in.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:765,modifiability,pac,packages,765,"@nvnieuwk Are you sure you are running inside the Docker image? I listed all the files regarding the `/tmp` folder inside the Docker container (which all must be the same for all users), and I cannot find the ones shown above:. ```Bash. root@0f98b9adcd2d:/# /opt/deepvariant/bin/run_deepvariant --version. DeepVariant version 1.5.0. root@0f98b9adcd2d:/# ls /tmp/Bazel.runfiles_gd__toh4/. ls: cannot access '/tmp/Bazel.runfiles_gd__toh4/': No such file or directory. root@0f98b9adcd2d:/# find /tmp. /tmp. /tmp/tmpb20xyssf. /tmp/__pycache__. /tmp/__pycache__/__autograph_generated_file7y1wd6da.cpython-38.pyc. /tmp/__pycache__/__autograph_generated_fileilucivm5.cpython-38.pyc. /tmp/tmp0y_1vxbg. root@0f98b9adcd2d:/# find | grep bazel. ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazelize_command.py. ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazel_selected_py3.cpython-38.pyc. ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazelize_command.cpython-38.pyc. ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazel_selected_py3.py. root@0f98b9adcd2d:/#. ```. I remember seeing things like `/tmp/Bazel.runfiles_gd__toh4/` only when I compiled DeepVariant from scratch. It's a tricky task to get it compiled, so I just wanted to be sure you didn't mix up the session you might be running in.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:840,modifiability,pac,packages,840,"@nvnieuwk Are you sure you are running inside the Docker image? I listed all the files regarding the `/tmp` folder inside the Docker container (which all must be the same for all users), and I cannot find the ones shown above:. ```Bash. root@0f98b9adcd2d:/# /opt/deepvariant/bin/run_deepvariant --version. DeepVariant version 1.5.0. root@0f98b9adcd2d:/# ls /tmp/Bazel.runfiles_gd__toh4/. ls: cannot access '/tmp/Bazel.runfiles_gd__toh4/': No such file or directory. root@0f98b9adcd2d:/# find /tmp. /tmp. /tmp/tmpb20xyssf. /tmp/__pycache__. /tmp/__pycache__/__autograph_generated_file7y1wd6da.cpython-38.pyc. /tmp/__pycache__/__autograph_generated_fileilucivm5.cpython-38.pyc. /tmp/tmp0y_1vxbg. root@0f98b9adcd2d:/# find | grep bazel. ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazelize_command.py. ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazel_selected_py3.cpython-38.pyc. ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazelize_command.cpython-38.pyc. ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazel_selected_py3.py. root@0f98b9adcd2d:/#. ```. I remember seeing things like `/tmp/Bazel.runfiles_gd__toh4/` only when I compiled DeepVariant from scratch. It's a tricky task to get it compiled, so I just wanted to be sure you didn't mix up the session you might be running in.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:941,modifiability,pac,packages,941,"@nvnieuwk Are you sure you are running inside the Docker image? I listed all the files regarding the `/tmp` folder inside the Docker container (which all must be the same for all users), and I cannot find the ones shown above:. ```Bash. root@0f98b9adcd2d:/# /opt/deepvariant/bin/run_deepvariant --version. DeepVariant version 1.5.0. root@0f98b9adcd2d:/# ls /tmp/Bazel.runfiles_gd__toh4/. ls: cannot access '/tmp/Bazel.runfiles_gd__toh4/': No such file or directory. root@0f98b9adcd2d:/# find /tmp. /tmp. /tmp/tmpb20xyssf. /tmp/__pycache__. /tmp/__pycache__/__autograph_generated_file7y1wd6da.cpython-38.pyc. /tmp/__pycache__/__autograph_generated_fileilucivm5.cpython-38.pyc. /tmp/tmp0y_1vxbg. root@0f98b9adcd2d:/# find | grep bazel. ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazelize_command.py. ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazel_selected_py3.cpython-38.pyc. ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazelize_command.cpython-38.pyc. ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazel_selected_py3.py. root@0f98b9adcd2d:/#. ```. I remember seeing things like `/tmp/Bazel.runfiles_gd__toh4/` only when I compiled DeepVariant from scratch. It's a tricky task to get it compiled, so I just wanted to be sure you didn't mix up the session you might be running in.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:1040,modifiability,pac,packages,1040,"@nvnieuwk Are you sure you are running inside the Docker image? I listed all the files regarding the `/tmp` folder inside the Docker container (which all must be the same for all users), and I cannot find the ones shown above:. ```Bash. root@0f98b9adcd2d:/# /opt/deepvariant/bin/run_deepvariant --version. DeepVariant version 1.5.0. root@0f98b9adcd2d:/# ls /tmp/Bazel.runfiles_gd__toh4/. ls: cannot access '/tmp/Bazel.runfiles_gd__toh4/': No such file or directory. root@0f98b9adcd2d:/# find /tmp. /tmp. /tmp/tmpb20xyssf. /tmp/__pycache__. /tmp/__pycache__/__autograph_generated_file7y1wd6da.cpython-38.pyc. /tmp/__pycache__/__autograph_generated_fileilucivm5.cpython-38.pyc. /tmp/tmp0y_1vxbg. root@0f98b9adcd2d:/# find | grep bazel. ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazelize_command.py. ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazel_selected_py3.cpython-38.pyc. ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazelize_command.cpython-38.pyc. ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazel_selected_py3.py. root@0f98b9adcd2d:/#. ```. I remember seeing things like `/tmp/Bazel.runfiles_gd__toh4/` only when I compiled DeepVariant from scratch. It's a tricky task to get it compiled, so I just wanted to be sure you didn't mix up the session you might be running in.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:779,safety,test,testing,779,"@nvnieuwk Are you sure you are running inside the Docker image? I listed all the files regarding the `/tmp` folder inside the Docker container (which all must be the same for all users), and I cannot find the ones shown above:. ```Bash. root@0f98b9adcd2d:/# /opt/deepvariant/bin/run_deepvariant --version. DeepVariant version 1.5.0. root@0f98b9adcd2d:/# ls /tmp/Bazel.runfiles_gd__toh4/. ls: cannot access '/tmp/Bazel.runfiles_gd__toh4/': No such file or directory. root@0f98b9adcd2d:/# find /tmp. /tmp. /tmp/tmpb20xyssf. /tmp/__pycache__. /tmp/__pycache__/__autograph_generated_file7y1wd6da.cpython-38.pyc. /tmp/__pycache__/__autograph_generated_fileilucivm5.cpython-38.pyc. /tmp/tmp0y_1vxbg. root@0f98b9adcd2d:/# find | grep bazel. ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazelize_command.py. ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazel_selected_py3.cpython-38.pyc. ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazelize_command.cpython-38.pyc. ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazel_selected_py3.py. root@0f98b9adcd2d:/#. ```. I remember seeing things like `/tmp/Bazel.runfiles_gd__toh4/` only when I compiled DeepVariant from scratch. It's a tricky task to get it compiled, so I just wanted to be sure you didn't mix up the session you might be running in.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:854,safety,test,testing,854,"@nvnieuwk Are you sure you are running inside the Docker image? I listed all the files regarding the `/tmp` folder inside the Docker container (which all must be the same for all users), and I cannot find the ones shown above:. ```Bash. root@0f98b9adcd2d:/# /opt/deepvariant/bin/run_deepvariant --version. DeepVariant version 1.5.0. root@0f98b9adcd2d:/# ls /tmp/Bazel.runfiles_gd__toh4/. ls: cannot access '/tmp/Bazel.runfiles_gd__toh4/': No such file or directory. root@0f98b9adcd2d:/# find /tmp. /tmp. /tmp/tmpb20xyssf. /tmp/__pycache__. /tmp/__pycache__/__autograph_generated_file7y1wd6da.cpython-38.pyc. /tmp/__pycache__/__autograph_generated_fileilucivm5.cpython-38.pyc. /tmp/tmp0y_1vxbg. root@0f98b9adcd2d:/# find | grep bazel. ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazelize_command.py. ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazel_selected_py3.cpython-38.pyc. ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazelize_command.cpython-38.pyc. ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazel_selected_py3.py. root@0f98b9adcd2d:/#. ```. I remember seeing things like `/tmp/Bazel.runfiles_gd__toh4/` only when I compiled DeepVariant from scratch. It's a tricky task to get it compiled, so I just wanted to be sure you didn't mix up the session you might be running in.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:955,safety,test,testing,955,"@nvnieuwk Are you sure you are running inside the Docker image? I listed all the files regarding the `/tmp` folder inside the Docker container (which all must be the same for all users), and I cannot find the ones shown above:. ```Bash. root@0f98b9adcd2d:/# /opt/deepvariant/bin/run_deepvariant --version. DeepVariant version 1.5.0. root@0f98b9adcd2d:/# ls /tmp/Bazel.runfiles_gd__toh4/. ls: cannot access '/tmp/Bazel.runfiles_gd__toh4/': No such file or directory. root@0f98b9adcd2d:/# find /tmp. /tmp. /tmp/tmpb20xyssf. /tmp/__pycache__. /tmp/__pycache__/__autograph_generated_file7y1wd6da.cpython-38.pyc. /tmp/__pycache__/__autograph_generated_fileilucivm5.cpython-38.pyc. /tmp/tmp0y_1vxbg. root@0f98b9adcd2d:/# find | grep bazel. ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazelize_command.py. ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazel_selected_py3.cpython-38.pyc. ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazelize_command.cpython-38.pyc. ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazel_selected_py3.py. root@0f98b9adcd2d:/#. ```. I remember seeing things like `/tmp/Bazel.runfiles_gd__toh4/` only when I compiled DeepVariant from scratch. It's a tricky task to get it compiled, so I just wanted to be sure you didn't mix up the session you might be running in.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:1054,safety,test,testing,1054,"@nvnieuwk Are you sure you are running inside the Docker image? I listed all the files regarding the `/tmp` folder inside the Docker container (which all must be the same for all users), and I cannot find the ones shown above:. ```Bash. root@0f98b9adcd2d:/# /opt/deepvariant/bin/run_deepvariant --version. DeepVariant version 1.5.0. root@0f98b9adcd2d:/# ls /tmp/Bazel.runfiles_gd__toh4/. ls: cannot access '/tmp/Bazel.runfiles_gd__toh4/': No such file or directory. root@0f98b9adcd2d:/# find /tmp. /tmp. /tmp/tmpb20xyssf. /tmp/__pycache__. /tmp/__pycache__/__autograph_generated_file7y1wd6da.cpython-38.pyc. /tmp/__pycache__/__autograph_generated_fileilucivm5.cpython-38.pyc. /tmp/tmp0y_1vxbg. root@0f98b9adcd2d:/# find | grep bazel. ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazelize_command.py. ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazel_selected_py3.cpython-38.pyc. ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazelize_command.cpython-38.pyc. ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazel_selected_py3.py. root@0f98b9adcd2d:/#. ```. I remember seeing things like `/tmp/Bazel.runfiles_gd__toh4/` only when I compiled DeepVariant from scratch. It's a tricky task to get it compiled, so I just wanted to be sure you didn't mix up the session you might be running in.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:1115,safety,reme,remember,1115,"@nvnieuwk Are you sure you are running inside the Docker image? I listed all the files regarding the `/tmp` folder inside the Docker container (which all must be the same for all users), and I cannot find the ones shown above:. ```Bash. root@0f98b9adcd2d:/# /opt/deepvariant/bin/run_deepvariant --version. DeepVariant version 1.5.0. root@0f98b9adcd2d:/# ls /tmp/Bazel.runfiles_gd__toh4/. ls: cannot access '/tmp/Bazel.runfiles_gd__toh4/': No such file or directory. root@0f98b9adcd2d:/# find /tmp. /tmp. /tmp/tmpb20xyssf. /tmp/__pycache__. /tmp/__pycache__/__autograph_generated_file7y1wd6da.cpython-38.pyc. /tmp/__pycache__/__autograph_generated_fileilucivm5.cpython-38.pyc. /tmp/tmp0y_1vxbg. root@0f98b9adcd2d:/# find | grep bazel. ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazelize_command.py. ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazel_selected_py3.cpython-38.pyc. ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazelize_command.cpython-38.pyc. ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazel_selected_py3.py. root@0f98b9adcd2d:/#. ```. I remember seeing things like `/tmp/Bazel.runfiles_gd__toh4/` only when I compiled DeepVariant from scratch. It's a tricky task to get it compiled, so I just wanted to be sure you didn't mix up the session you might be running in.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:399,security,access,access,399,"@nvnieuwk Are you sure you are running inside the Docker image? I listed all the files regarding the `/tmp` folder inside the Docker container (which all must be the same for all users), and I cannot find the ones shown above:. ```Bash. root@0f98b9adcd2d:/# /opt/deepvariant/bin/run_deepvariant --version. DeepVariant version 1.5.0. root@0f98b9adcd2d:/# ls /tmp/Bazel.runfiles_gd__toh4/. ls: cannot access '/tmp/Bazel.runfiles_gd__toh4/': No such file or directory. root@0f98b9adcd2d:/# find /tmp. /tmp. /tmp/tmpb20xyssf. /tmp/__pycache__. /tmp/__pycache__/__autograph_generated_file7y1wd6da.cpython-38.pyc. /tmp/__pycache__/__autograph_generated_fileilucivm5.cpython-38.pyc. /tmp/tmp0y_1vxbg. root@0f98b9adcd2d:/# find | grep bazel. ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazelize_command.py. ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazel_selected_py3.cpython-38.pyc. ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazelize_command.cpython-38.pyc. ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazel_selected_py3.py. root@0f98b9adcd2d:/#. ```. I remember seeing things like `/tmp/Bazel.runfiles_gd__toh4/` only when I compiled DeepVariant from scratch. It's a tricky task to get it compiled, so I just wanted to be sure you didn't mix up the session you might be running in.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:1311,security,session,session,1311,"@nvnieuwk Are you sure you are running inside the Docker image? I listed all the files regarding the `/tmp` folder inside the Docker container (which all must be the same for all users), and I cannot find the ones shown above:. ```Bash. root@0f98b9adcd2d:/# /opt/deepvariant/bin/run_deepvariant --version. DeepVariant version 1.5.0. root@0f98b9adcd2d:/# ls /tmp/Bazel.runfiles_gd__toh4/. ls: cannot access '/tmp/Bazel.runfiles_gd__toh4/': No such file or directory. root@0f98b9adcd2d:/# find /tmp. /tmp. /tmp/tmpb20xyssf. /tmp/__pycache__. /tmp/__pycache__/__autograph_generated_file7y1wd6da.cpython-38.pyc. /tmp/__pycache__/__autograph_generated_fileilucivm5.cpython-38.pyc. /tmp/tmp0y_1vxbg. root@0f98b9adcd2d:/# find | grep bazel. ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazelize_command.py. ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazel_selected_py3.cpython-38.pyc. ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazelize_command.cpython-38.pyc. ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazel_selected_py3.py. root@0f98b9adcd2d:/#. ```. I remember seeing things like `/tmp/Bazel.runfiles_gd__toh4/` only when I compiled DeepVariant from scratch. It's a tricky task to get it compiled, so I just wanted to be sure you didn't mix up the session you might be running in.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:779,testability,test,testing,779,"@nvnieuwk Are you sure you are running inside the Docker image? I listed all the files regarding the `/tmp` folder inside the Docker container (which all must be the same for all users), and I cannot find the ones shown above:. ```Bash. root@0f98b9adcd2d:/# /opt/deepvariant/bin/run_deepvariant --version. DeepVariant version 1.5.0. root@0f98b9adcd2d:/# ls /tmp/Bazel.runfiles_gd__toh4/. ls: cannot access '/tmp/Bazel.runfiles_gd__toh4/': No such file or directory. root@0f98b9adcd2d:/# find /tmp. /tmp. /tmp/tmpb20xyssf. /tmp/__pycache__. /tmp/__pycache__/__autograph_generated_file7y1wd6da.cpython-38.pyc. /tmp/__pycache__/__autograph_generated_fileilucivm5.cpython-38.pyc. /tmp/tmp0y_1vxbg. root@0f98b9adcd2d:/# find | grep bazel. ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazelize_command.py. ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazel_selected_py3.cpython-38.pyc. ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazelize_command.cpython-38.pyc. ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazel_selected_py3.py. root@0f98b9adcd2d:/#. ```. I remember seeing things like `/tmp/Bazel.runfiles_gd__toh4/` only when I compiled DeepVariant from scratch. It's a tricky task to get it compiled, so I just wanted to be sure you didn't mix up the session you might be running in.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:854,testability,test,testing,854,"@nvnieuwk Are you sure you are running inside the Docker image? I listed all the files regarding the `/tmp` folder inside the Docker container (which all must be the same for all users), and I cannot find the ones shown above:. ```Bash. root@0f98b9adcd2d:/# /opt/deepvariant/bin/run_deepvariant --version. DeepVariant version 1.5.0. root@0f98b9adcd2d:/# ls /tmp/Bazel.runfiles_gd__toh4/. ls: cannot access '/tmp/Bazel.runfiles_gd__toh4/': No such file or directory. root@0f98b9adcd2d:/# find /tmp. /tmp. /tmp/tmpb20xyssf. /tmp/__pycache__. /tmp/__pycache__/__autograph_generated_file7y1wd6da.cpython-38.pyc. /tmp/__pycache__/__autograph_generated_fileilucivm5.cpython-38.pyc. /tmp/tmp0y_1vxbg. root@0f98b9adcd2d:/# find | grep bazel. ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazelize_command.py. ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazel_selected_py3.cpython-38.pyc. ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazelize_command.cpython-38.pyc. ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazel_selected_py3.py. root@0f98b9adcd2d:/#. ```. I remember seeing things like `/tmp/Bazel.runfiles_gd__toh4/` only when I compiled DeepVariant from scratch. It's a tricky task to get it compiled, so I just wanted to be sure you didn't mix up the session you might be running in.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:955,testability,test,testing,955,"@nvnieuwk Are you sure you are running inside the Docker image? I listed all the files regarding the `/tmp` folder inside the Docker container (which all must be the same for all users), and I cannot find the ones shown above:. ```Bash. root@0f98b9adcd2d:/# /opt/deepvariant/bin/run_deepvariant --version. DeepVariant version 1.5.0. root@0f98b9adcd2d:/# ls /tmp/Bazel.runfiles_gd__toh4/. ls: cannot access '/tmp/Bazel.runfiles_gd__toh4/': No such file or directory. root@0f98b9adcd2d:/# find /tmp. /tmp. /tmp/tmpb20xyssf. /tmp/__pycache__. /tmp/__pycache__/__autograph_generated_file7y1wd6da.cpython-38.pyc. /tmp/__pycache__/__autograph_generated_fileilucivm5.cpython-38.pyc. /tmp/tmp0y_1vxbg. root@0f98b9adcd2d:/# find | grep bazel. ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazelize_command.py. ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazel_selected_py3.cpython-38.pyc. ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazelize_command.cpython-38.pyc. ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazel_selected_py3.py. root@0f98b9adcd2d:/#. ```. I remember seeing things like `/tmp/Bazel.runfiles_gd__toh4/` only when I compiled DeepVariant from scratch. It's a tricky task to get it compiled, so I just wanted to be sure you didn't mix up the session you might be running in.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:1054,testability,test,testing,1054,"@nvnieuwk Are you sure you are running inside the Docker image? I listed all the files regarding the `/tmp` folder inside the Docker container (which all must be the same for all users), and I cannot find the ones shown above:. ```Bash. root@0f98b9adcd2d:/# /opt/deepvariant/bin/run_deepvariant --version. DeepVariant version 1.5.0. root@0f98b9adcd2d:/# ls /tmp/Bazel.runfiles_gd__toh4/. ls: cannot access '/tmp/Bazel.runfiles_gd__toh4/': No such file or directory. root@0f98b9adcd2d:/# find /tmp. /tmp. /tmp/tmpb20xyssf. /tmp/__pycache__. /tmp/__pycache__/__autograph_generated_file7y1wd6da.cpython-38.pyc. /tmp/__pycache__/__autograph_generated_fileilucivm5.cpython-38.pyc. /tmp/tmp0y_1vxbg. root@0f98b9adcd2d:/# find | grep bazel. ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazelize_command.py. ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazel_selected_py3.cpython-38.pyc. ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazelize_command.cpython-38.pyc. ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazel_selected_py3.py. root@0f98b9adcd2d:/#. ```. I remember seeing things like `/tmp/Bazel.runfiles_gd__toh4/` only when I compiled DeepVariant from scratch. It's a tricky task to get it compiled, so I just wanted to be sure you didn't mix up the session you might be running in.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:179,usability,user,users,179,"@nvnieuwk Are you sure you are running inside the Docker image? I listed all the files regarding the `/tmp` folder inside the Docker container (which all must be the same for all users), and I cannot find the ones shown above:. ```Bash. root@0f98b9adcd2d:/# /opt/deepvariant/bin/run_deepvariant --version. DeepVariant version 1.5.0. root@0f98b9adcd2d:/# ls /tmp/Bazel.runfiles_gd__toh4/. ls: cannot access '/tmp/Bazel.runfiles_gd__toh4/': No such file or directory. root@0f98b9adcd2d:/# find /tmp. /tmp. /tmp/tmpb20xyssf. /tmp/__pycache__. /tmp/__pycache__/__autograph_generated_file7y1wd6da.cpython-38.pyc. /tmp/__pycache__/__autograph_generated_fileilucivm5.cpython-38.pyc. /tmp/tmp0y_1vxbg. root@0f98b9adcd2d:/# find | grep bazel. ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazelize_command.py. ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazel_selected_py3.cpython-38.pyc. ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazelize_command.cpython-38.pyc. ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazel_selected_py3.py. root@0f98b9adcd2d:/#. ```. I remember seeing things like `/tmp/Bazel.runfiles_gd__toh4/` only when I compiled DeepVariant from scratch. It's a tricky task to get it compiled, so I just wanted to be sure you didn't mix up the session you might be running in.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:330,availability,error,error,330,"> @nvnieuwk Does it mean you only encounter this issue when running on a small region? Does the same setting work when you run something like the Quick Start? Yes I've tried it with a CRAM file that contains the whole chromosome 21 and this works, but when I use a BAM with the subset `chr22:0-40001`, I get the earlier mentioned error",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:199,deployability,contain,contains,199,"> @nvnieuwk Does it mean you only encounter this issue when running on a small region? Does the same setting work when you run something like the Quick Start? Yes I've tried it with a CRAM file that contains the whole chromosome 21 and this works, but when I use a BAM with the subset `chr22:0-40001`, I get the earlier mentioned error",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:278,integrability,sub,subset,278,"> @nvnieuwk Does it mean you only encounter this issue when running on a small region? Does the same setting work when you run something like the Quick Start? Yes I've tried it with a CRAM file that contains the whole chromosome 21 and this works, but when I use a BAM with the subset `chr22:0-40001`, I get the earlier mentioned error",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:330,performance,error,error,330,"> @nvnieuwk Does it mean you only encounter this issue when running on a small region? Does the same setting work when you run something like the Quick Start? Yes I've tried it with a CRAM file that contains the whole chromosome 21 and this works, but when I use a BAM with the subset `chr22:0-40001`, I get the earlier mentioned error",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:12,reliability,Doe,Does,12,"> @nvnieuwk Does it mean you only encounter this issue when running on a small region? Does the same setting work when you run something like the Quick Start? Yes I've tried it with a CRAM file that contains the whole chromosome 21 and this works, but when I use a BAM with the subset `chr22:0-40001`, I get the earlier mentioned error",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:87,reliability,Doe,Does,87,"> @nvnieuwk Does it mean you only encounter this issue when running on a small region? Does the same setting work when you run something like the Quick Start? Yes I've tried it with a CRAM file that contains the whole chromosome 21 and this works, but when I use a BAM with the subset `chr22:0-40001`, I get the earlier mentioned error",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:330,safety,error,error,330,"> @nvnieuwk Does it mean you only encounter this issue when running on a small region? Does the same setting work when you run something like the Quick Start? Yes I've tried it with a CRAM file that contains the whole chromosome 21 and this works, but when I use a BAM with the subset `chr22:0-40001`, I get the earlier mentioned error",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:330,usability,error,error,330,"> @nvnieuwk Does it mean you only encounter this issue when running on a small region? Does the same setting work when you run something like the Quick Start? Yes I've tried it with a CRAM file that contains the whole chromosome 21 and this works, but when I use a BAM with the subset `chr22:0-40001`, I get the earlier mentioned error",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:135,deployability,contain,container,135,"> @nvnieuwk Are you sure you are running inside the Docker image? I listed all the files regarding the `/tmp` folder inside the Docker container (which all must be the same for all users), and I cannot find the ones shown above:. > . > ```shell. > root@0f98b9adcd2d:/# /opt/deepvariant/bin/run_deepvariant --version. > DeepVariant version 1.5.0. > root@0f98b9adcd2d:/# ls /tmp/Bazel.runfiles_gd__toh4/. > ls: cannot access '/tmp/Bazel.runfiles_gd__toh4/': No such file or directory. > root@0f98b9adcd2d:/# find /tmp. > /tmp. > /tmp/tmpb20xyssf. > /tmp/__pycache__. > /tmp/__pycache__/__autograph_generated_file7y1wd6da.cpython-38.pyc. > /tmp/__pycache__/__autograph_generated_fileilucivm5.cpython-38.pyc. > /tmp/tmp0y_1vxbg. > root@0f98b9adcd2d:/# find | grep bazel. > ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazelize_command.py. > ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazel_selected_py3.cpython-38.pyc. > ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazelize_command.cpython-38.pyc. > ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazel_selected_py3.py. > root@0f98b9adcd2d:/#. > ```. > . > I remember seeing things like `/tmp/Bazel.runfiles_gd__toh4/` only when I compiled DeepVariant from scratch. It's a tricky task to get it compiled, so I just wanted to be sure you didn't mix up the session you might be running in. I've been running all my tests using a Nextflow pipeline. The command Nextflow uses to run the docker containers looks like this:. ```. docker run -i --cpu-shares 2048 --memory 3072m -u $(id -u) -e ""HOME=${HOME}"" -v /etc/passwd:/etc/passwd:ro -v /etc/shadow:/etc/shadow:ro -v /etc/group:/etc/group:ro -v $HOME:$HOME -v /tmp/tmppixeopws:/tmp/tmppixeopws -w ""$PWD"" --platform linux/x86_64 --name $NXF_BOXID docker.io/google/deepvariant:1.5.0 /bin/bash -ue /tmp/tmppixeopws/16/280c239bb924cbde08365e5fbf5788/.command.sh. ```. I've never had any problems that it would take some files from the current system ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:308,deployability,version,version,308,"> @nvnieuwk Are you sure you are running inside the Docker image? I listed all the files regarding the `/tmp` folder inside the Docker container (which all must be the same for all users), and I cannot find the ones shown above:. > . > ```shell. > root@0f98b9adcd2d:/# /opt/deepvariant/bin/run_deepvariant --version. > DeepVariant version 1.5.0. > root@0f98b9adcd2d:/# ls /tmp/Bazel.runfiles_gd__toh4/. > ls: cannot access '/tmp/Bazel.runfiles_gd__toh4/': No such file or directory. > root@0f98b9adcd2d:/# find /tmp. > /tmp. > /tmp/tmpb20xyssf. > /tmp/__pycache__. > /tmp/__pycache__/__autograph_generated_file7y1wd6da.cpython-38.pyc. > /tmp/__pycache__/__autograph_generated_fileilucivm5.cpython-38.pyc. > /tmp/tmp0y_1vxbg. > root@0f98b9adcd2d:/# find | grep bazel. > ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazelize_command.py. > ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazel_selected_py3.cpython-38.pyc. > ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazelize_command.cpython-38.pyc. > ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazel_selected_py3.py. > root@0f98b9adcd2d:/#. > ```. > . > I remember seeing things like `/tmp/Bazel.runfiles_gd__toh4/` only when I compiled DeepVariant from scratch. It's a tricky task to get it compiled, so I just wanted to be sure you didn't mix up the session you might be running in. I've been running all my tests using a Nextflow pipeline. The command Nextflow uses to run the docker containers looks like this:. ```. docker run -i --cpu-shares 2048 --memory 3072m -u $(id -u) -e ""HOME=${HOME}"" -v /etc/passwd:/etc/passwd:ro -v /etc/shadow:/etc/shadow:ro -v /etc/group:/etc/group:ro -v $HOME:$HOME -v /tmp/tmppixeopws:/tmp/tmppixeopws -w ""$PWD"" --platform linux/x86_64 --name $NXF_BOXID docker.io/google/deepvariant:1.5.0 /bin/bash -ue /tmp/tmppixeopws/16/280c239bb924cbde08365e5fbf5788/.command.sh. ```. I've never had any problems that it would take some files from the current system ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:331,deployability,version,version,331,"> @nvnieuwk Are you sure you are running inside the Docker image? I listed all the files regarding the `/tmp` folder inside the Docker container (which all must be the same for all users), and I cannot find the ones shown above:. > . > ```shell. > root@0f98b9adcd2d:/# /opt/deepvariant/bin/run_deepvariant --version. > DeepVariant version 1.5.0. > root@0f98b9adcd2d:/# ls /tmp/Bazel.runfiles_gd__toh4/. > ls: cannot access '/tmp/Bazel.runfiles_gd__toh4/': No such file or directory. > root@0f98b9adcd2d:/# find /tmp. > /tmp. > /tmp/tmpb20xyssf. > /tmp/__pycache__. > /tmp/__pycache__/__autograph_generated_file7y1wd6da.cpython-38.pyc. > /tmp/__pycache__/__autograph_generated_fileilucivm5.cpython-38.pyc. > /tmp/tmp0y_1vxbg. > root@0f98b9adcd2d:/# find | grep bazel. > ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazelize_command.py. > ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazel_selected_py3.cpython-38.pyc. > ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazelize_command.cpython-38.pyc. > ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazel_selected_py3.py. > root@0f98b9adcd2d:/#. > ```. > . > I remember seeing things like `/tmp/Bazel.runfiles_gd__toh4/` only when I compiled DeepVariant from scratch. It's a tricky task to get it compiled, so I just wanted to be sure you didn't mix up the session you might be running in. I've been running all my tests using a Nextflow pipeline. The command Nextflow uses to run the docker containers looks like this:. ```. docker run -i --cpu-shares 2048 --memory 3072m -u $(id -u) -e ""HOME=${HOME}"" -v /etc/passwd:/etc/passwd:ro -v /etc/shadow:/etc/shadow:ro -v /etc/group:/etc/group:ro -v $HOME:$HOME -v /tmp/tmppixeopws:/tmp/tmppixeopws -w ""$PWD"" --platform linux/x86_64 --name $NXF_BOXID docker.io/google/deepvariant:1.5.0 /bin/bash -ue /tmp/tmppixeopws/16/280c239bb924cbde08365e5fbf5788/.command.sh. ```. I've never had any problems that it would take some files from the current system ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:1443,deployability,pipelin,pipeline,1443," all the files regarding the `/tmp` folder inside the Docker container (which all must be the same for all users), and I cannot find the ones shown above:. > . > ```shell. > root@0f98b9adcd2d:/# /opt/deepvariant/bin/run_deepvariant --version. > DeepVariant version 1.5.0. > root@0f98b9adcd2d:/# ls /tmp/Bazel.runfiles_gd__toh4/. > ls: cannot access '/tmp/Bazel.runfiles_gd__toh4/': No such file or directory. > root@0f98b9adcd2d:/# find /tmp. > /tmp. > /tmp/tmpb20xyssf. > /tmp/__pycache__. > /tmp/__pycache__/__autograph_generated_file7y1wd6da.cpython-38.pyc. > /tmp/__pycache__/__autograph_generated_fileilucivm5.cpython-38.pyc. > /tmp/tmp0y_1vxbg. > root@0f98b9adcd2d:/# find | grep bazel. > ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazelize_command.py. > ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazel_selected_py3.cpython-38.pyc. > ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazelize_command.cpython-38.pyc. > ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazel_selected_py3.py. > root@0f98b9adcd2d:/#. > ```. > . > I remember seeing things like `/tmp/Bazel.runfiles_gd__toh4/` only when I compiled DeepVariant from scratch. It's a tricky task to get it compiled, so I just wanted to be sure you didn't mix up the session you might be running in. I've been running all my tests using a Nextflow pipeline. The command Nextflow uses to run the docker containers looks like this:. ```. docker run -i --cpu-shares 2048 --memory 3072m -u $(id -u) -e ""HOME=${HOME}"" -v /etc/passwd:/etc/passwd:ro -v /etc/shadow:/etc/shadow:ro -v /etc/group:/etc/group:ro -v $HOME:$HOME -v /tmp/tmppixeopws:/tmp/tmppixeopws -w ""$PWD"" --platform linux/x86_64 --name $NXF_BOXID docker.io/google/deepvariant:1.5.0 /bin/bash -ue /tmp/tmppixeopws/16/280c239bb924cbde08365e5fbf5788/.command.sh. ```. I've never had any problems that it would take some files from the current system with the exception of the ones in the volumes (which it's supposed to use)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:1497,deployability,contain,containers,1497," all the files regarding the `/tmp` folder inside the Docker container (which all must be the same for all users), and I cannot find the ones shown above:. > . > ```shell. > root@0f98b9adcd2d:/# /opt/deepvariant/bin/run_deepvariant --version. > DeepVariant version 1.5.0. > root@0f98b9adcd2d:/# ls /tmp/Bazel.runfiles_gd__toh4/. > ls: cannot access '/tmp/Bazel.runfiles_gd__toh4/': No such file or directory. > root@0f98b9adcd2d:/# find /tmp. > /tmp. > /tmp/tmpb20xyssf. > /tmp/__pycache__. > /tmp/__pycache__/__autograph_generated_file7y1wd6da.cpython-38.pyc. > /tmp/__pycache__/__autograph_generated_fileilucivm5.cpython-38.pyc. > /tmp/tmp0y_1vxbg. > root@0f98b9adcd2d:/# find | grep bazel. > ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazelize_command.py. > ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazel_selected_py3.cpython-38.pyc. > ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazelize_command.cpython-38.pyc. > ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazel_selected_py3.py. > root@0f98b9adcd2d:/#. > ```. > . > I remember seeing things like `/tmp/Bazel.runfiles_gd__toh4/` only when I compiled DeepVariant from scratch. It's a tricky task to get it compiled, so I just wanted to be sure you didn't mix up the session you might be running in. I've been running all my tests using a Nextflow pipeline. The command Nextflow uses to run the docker containers looks like this:. ```. docker run -i --cpu-shares 2048 --memory 3072m -u $(id -u) -e ""HOME=${HOME}"" -v /etc/passwd:/etc/passwd:ro -v /etc/shadow:/etc/shadow:ro -v /etc/group:/etc/group:ro -v $HOME:$HOME -v /tmp/tmppixeopws:/tmp/tmppixeopws -w ""$PWD"" --platform linux/x86_64 --name $NXF_BOXID docker.io/google/deepvariant:1.5.0 /bin/bash -ue /tmp/tmppixeopws/16/280c239bb924cbde08365e5fbf5788/.command.sh. ```. I've never had any problems that it would take some files from the current system with the exception of the ones in the volumes (which it's supposed to use)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:1547,energy efficiency,cpu,cpu-shares,1547," all the files regarding the `/tmp` folder inside the Docker container (which all must be the same for all users), and I cannot find the ones shown above:. > . > ```shell. > root@0f98b9adcd2d:/# /opt/deepvariant/bin/run_deepvariant --version. > DeepVariant version 1.5.0. > root@0f98b9adcd2d:/# ls /tmp/Bazel.runfiles_gd__toh4/. > ls: cannot access '/tmp/Bazel.runfiles_gd__toh4/': No such file or directory. > root@0f98b9adcd2d:/# find /tmp. > /tmp. > /tmp/tmpb20xyssf. > /tmp/__pycache__. > /tmp/__pycache__/__autograph_generated_file7y1wd6da.cpython-38.pyc. > /tmp/__pycache__/__autograph_generated_fileilucivm5.cpython-38.pyc. > /tmp/tmp0y_1vxbg. > root@0f98b9adcd2d:/# find | grep bazel. > ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazelize_command.py. > ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazel_selected_py3.cpython-38.pyc. > ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazelize_command.cpython-38.pyc. > ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazel_selected_py3.py. > root@0f98b9adcd2d:/#. > ```. > . > I remember seeing things like `/tmp/Bazel.runfiles_gd__toh4/` only when I compiled DeepVariant from scratch. It's a tricky task to get it compiled, so I just wanted to be sure you didn't mix up the session you might be running in. I've been running all my tests using a Nextflow pipeline. The command Nextflow uses to run the docker containers looks like this:. ```. docker run -i --cpu-shares 2048 --memory 3072m -u $(id -u) -e ""HOME=${HOME}"" -v /etc/passwd:/etc/passwd:ro -v /etc/shadow:/etc/shadow:ro -v /etc/group:/etc/group:ro -v $HOME:$HOME -v /tmp/tmppixeopws:/tmp/tmppixeopws -w ""$PWD"" --platform linux/x86_64 --name $NXF_BOXID docker.io/google/deepvariant:1.5.0 /bin/bash -ue /tmp/tmppixeopws/16/280c239bb924cbde08365e5fbf5788/.command.sh. ```. I've never had any problems that it would take some files from the current system with the exception of the ones in the volumes (which it's supposed to use)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:1985,energy efficiency,current,current,1985," all the files regarding the `/tmp` folder inside the Docker container (which all must be the same for all users), and I cannot find the ones shown above:. > . > ```shell. > root@0f98b9adcd2d:/# /opt/deepvariant/bin/run_deepvariant --version. > DeepVariant version 1.5.0. > root@0f98b9adcd2d:/# ls /tmp/Bazel.runfiles_gd__toh4/. > ls: cannot access '/tmp/Bazel.runfiles_gd__toh4/': No such file or directory. > root@0f98b9adcd2d:/# find /tmp. > /tmp. > /tmp/tmpb20xyssf. > /tmp/__pycache__. > /tmp/__pycache__/__autograph_generated_file7y1wd6da.cpython-38.pyc. > /tmp/__pycache__/__autograph_generated_fileilucivm5.cpython-38.pyc. > /tmp/tmp0y_1vxbg. > root@0f98b9adcd2d:/# find | grep bazel. > ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazelize_command.py. > ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazel_selected_py3.cpython-38.pyc. > ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazelize_command.cpython-38.pyc. > ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazel_selected_py3.py. > root@0f98b9adcd2d:/#. > ```. > . > I remember seeing things like `/tmp/Bazel.runfiles_gd__toh4/` only when I compiled DeepVariant from scratch. It's a tricky task to get it compiled, so I just wanted to be sure you didn't mix up the session you might be running in. I've been running all my tests using a Nextflow pipeline. The command Nextflow uses to run the docker containers looks like this:. ```. docker run -i --cpu-shares 2048 --memory 3072m -u $(id -u) -e ""HOME=${HOME}"" -v /etc/passwd:/etc/passwd:ro -v /etc/shadow:/etc/shadow:ro -v /etc/group:/etc/group:ro -v $HOME:$HOME -v /tmp/tmppixeopws:/tmp/tmppixeopws -w ""$PWD"" --platform linux/x86_64 --name $NXF_BOXID docker.io/google/deepvariant:1.5.0 /bin/bash -ue /tmp/tmppixeopws/16/280c239bb924cbde08365e5fbf5788/.command.sh. ```. I've never had any problems that it would take some files from the current system with the exception of the ones in the volumes (which it's supposed to use)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:308,integrability,version,version,308,"> @nvnieuwk Are you sure you are running inside the Docker image? I listed all the files regarding the `/tmp` folder inside the Docker container (which all must be the same for all users), and I cannot find the ones shown above:. > . > ```shell. > root@0f98b9adcd2d:/# /opt/deepvariant/bin/run_deepvariant --version. > DeepVariant version 1.5.0. > root@0f98b9adcd2d:/# ls /tmp/Bazel.runfiles_gd__toh4/. > ls: cannot access '/tmp/Bazel.runfiles_gd__toh4/': No such file or directory. > root@0f98b9adcd2d:/# find /tmp. > /tmp. > /tmp/tmpb20xyssf. > /tmp/__pycache__. > /tmp/__pycache__/__autograph_generated_file7y1wd6da.cpython-38.pyc. > /tmp/__pycache__/__autograph_generated_fileilucivm5.cpython-38.pyc. > /tmp/tmp0y_1vxbg. > root@0f98b9adcd2d:/# find | grep bazel. > ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazelize_command.py. > ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazel_selected_py3.cpython-38.pyc. > ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazelize_command.cpython-38.pyc. > ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazel_selected_py3.py. > root@0f98b9adcd2d:/#. > ```. > . > I remember seeing things like `/tmp/Bazel.runfiles_gd__toh4/` only when I compiled DeepVariant from scratch. It's a tricky task to get it compiled, so I just wanted to be sure you didn't mix up the session you might be running in. I've been running all my tests using a Nextflow pipeline. The command Nextflow uses to run the docker containers looks like this:. ```. docker run -i --cpu-shares 2048 --memory 3072m -u $(id -u) -e ""HOME=${HOME}"" -v /etc/passwd:/etc/passwd:ro -v /etc/shadow:/etc/shadow:ro -v /etc/group:/etc/group:ro -v $HOME:$HOME -v /tmp/tmppixeopws:/tmp/tmppixeopws -w ""$PWD"" --platform linux/x86_64 --name $NXF_BOXID docker.io/google/deepvariant:1.5.0 /bin/bash -ue /tmp/tmppixeopws/16/280c239bb924cbde08365e5fbf5788/.command.sh. ```. I've never had any problems that it would take some files from the current system ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:331,integrability,version,version,331,"> @nvnieuwk Are you sure you are running inside the Docker image? I listed all the files regarding the `/tmp` folder inside the Docker container (which all must be the same for all users), and I cannot find the ones shown above:. > . > ```shell. > root@0f98b9adcd2d:/# /opt/deepvariant/bin/run_deepvariant --version. > DeepVariant version 1.5.0. > root@0f98b9adcd2d:/# ls /tmp/Bazel.runfiles_gd__toh4/. > ls: cannot access '/tmp/Bazel.runfiles_gd__toh4/': No such file or directory. > root@0f98b9adcd2d:/# find /tmp. > /tmp. > /tmp/tmpb20xyssf. > /tmp/__pycache__. > /tmp/__pycache__/__autograph_generated_file7y1wd6da.cpython-38.pyc. > /tmp/__pycache__/__autograph_generated_fileilucivm5.cpython-38.pyc. > /tmp/tmp0y_1vxbg. > root@0f98b9adcd2d:/# find | grep bazel. > ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazelize_command.py. > ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazel_selected_py3.cpython-38.pyc. > ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazelize_command.cpython-38.pyc. > ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazel_selected_py3.py. > root@0f98b9adcd2d:/#. > ```. > . > I remember seeing things like `/tmp/Bazel.runfiles_gd__toh4/` only when I compiled DeepVariant from scratch. It's a tricky task to get it compiled, so I just wanted to be sure you didn't mix up the session you might be running in. I've been running all my tests using a Nextflow pipeline. The command Nextflow uses to run the docker containers looks like this:. ```. docker run -i --cpu-shares 2048 --memory 3072m -u $(id -u) -e ""HOME=${HOME}"" -v /etc/passwd:/etc/passwd:ro -v /etc/shadow:/etc/shadow:ro -v /etc/group:/etc/group:ro -v $HOME:$HOME -v /tmp/tmppixeopws:/tmp/tmppixeopws -w ""$PWD"" --platform linux/x86_64 --name $NXF_BOXID docker.io/google/deepvariant:1.5.0 /bin/bash -ue /tmp/tmppixeopws/16/280c239bb924cbde08365e5fbf5788/.command.sh. ```. I've never had any problems that it would take some files from the current system ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:1443,integrability,pipelin,pipeline,1443," all the files regarding the `/tmp` folder inside the Docker container (which all must be the same for all users), and I cannot find the ones shown above:. > . > ```shell. > root@0f98b9adcd2d:/# /opt/deepvariant/bin/run_deepvariant --version. > DeepVariant version 1.5.0. > root@0f98b9adcd2d:/# ls /tmp/Bazel.runfiles_gd__toh4/. > ls: cannot access '/tmp/Bazel.runfiles_gd__toh4/': No such file or directory. > root@0f98b9adcd2d:/# find /tmp. > /tmp. > /tmp/tmpb20xyssf. > /tmp/__pycache__. > /tmp/__pycache__/__autograph_generated_file7y1wd6da.cpython-38.pyc. > /tmp/__pycache__/__autograph_generated_fileilucivm5.cpython-38.pyc. > /tmp/tmp0y_1vxbg. > root@0f98b9adcd2d:/# find | grep bazel. > ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazelize_command.py. > ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazel_selected_py3.cpython-38.pyc. > ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazelize_command.cpython-38.pyc. > ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazel_selected_py3.py. > root@0f98b9adcd2d:/#. > ```. > . > I remember seeing things like `/tmp/Bazel.runfiles_gd__toh4/` only when I compiled DeepVariant from scratch. It's a tricky task to get it compiled, so I just wanted to be sure you didn't mix up the session you might be running in. I've been running all my tests using a Nextflow pipeline. The command Nextflow uses to run the docker containers looks like this:. ```. docker run -i --cpu-shares 2048 --memory 3072m -u $(id -u) -e ""HOME=${HOME}"" -v /etc/passwd:/etc/passwd:ro -v /etc/shadow:/etc/shadow:ro -v /etc/group:/etc/group:ro -v $HOME:$HOME -v /tmp/tmppixeopws:/tmp/tmppixeopws -w ""$PWD"" --platform linux/x86_64 --name $NXF_BOXID docker.io/google/deepvariant:1.5.0 /bin/bash -ue /tmp/tmppixeopws/16/280c239bb924cbde08365e5fbf5788/.command.sh. ```. I've never had any problems that it would take some files from the current system with the exception of the ones in the volumes (which it's supposed to use)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:1551,interoperability,share,shares,1551," all the files regarding the `/tmp` folder inside the Docker container (which all must be the same for all users), and I cannot find the ones shown above:. > . > ```shell. > root@0f98b9adcd2d:/# /opt/deepvariant/bin/run_deepvariant --version. > DeepVariant version 1.5.0. > root@0f98b9adcd2d:/# ls /tmp/Bazel.runfiles_gd__toh4/. > ls: cannot access '/tmp/Bazel.runfiles_gd__toh4/': No such file or directory. > root@0f98b9adcd2d:/# find /tmp. > /tmp. > /tmp/tmpb20xyssf. > /tmp/__pycache__. > /tmp/__pycache__/__autograph_generated_file7y1wd6da.cpython-38.pyc. > /tmp/__pycache__/__autograph_generated_fileilucivm5.cpython-38.pyc. > /tmp/tmp0y_1vxbg. > root@0f98b9adcd2d:/# find | grep bazel. > ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazelize_command.py. > ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazel_selected_py3.cpython-38.pyc. > ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazelize_command.cpython-38.pyc. > ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazel_selected_py3.py. > root@0f98b9adcd2d:/#. > ```. > . > I remember seeing things like `/tmp/Bazel.runfiles_gd__toh4/` only when I compiled DeepVariant from scratch. It's a tricky task to get it compiled, so I just wanted to be sure you didn't mix up the session you might be running in. I've been running all my tests using a Nextflow pipeline. The command Nextflow uses to run the docker containers looks like this:. ```. docker run -i --cpu-shares 2048 --memory 3072m -u $(id -u) -e ""HOME=${HOME}"" -v /etc/passwd:/etc/passwd:ro -v /etc/shadow:/etc/shadow:ro -v /etc/group:/etc/group:ro -v $HOME:$HOME -v /tmp/tmppixeopws:/tmp/tmppixeopws -w ""$PWD"" --platform linux/x86_64 --name $NXF_BOXID docker.io/google/deepvariant:1.5.0 /bin/bash -ue /tmp/tmppixeopws/16/280c239bb924cbde08365e5fbf5788/.command.sh. ```. I've never had any problems that it would take some files from the current system with the exception of the ones in the volumes (which it's supposed to use)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:1760,interoperability,platform,platform,1760," all the files regarding the `/tmp` folder inside the Docker container (which all must be the same for all users), and I cannot find the ones shown above:. > . > ```shell. > root@0f98b9adcd2d:/# /opt/deepvariant/bin/run_deepvariant --version. > DeepVariant version 1.5.0. > root@0f98b9adcd2d:/# ls /tmp/Bazel.runfiles_gd__toh4/. > ls: cannot access '/tmp/Bazel.runfiles_gd__toh4/': No such file or directory. > root@0f98b9adcd2d:/# find /tmp. > /tmp. > /tmp/tmpb20xyssf. > /tmp/__pycache__. > /tmp/__pycache__/__autograph_generated_file7y1wd6da.cpython-38.pyc. > /tmp/__pycache__/__autograph_generated_fileilucivm5.cpython-38.pyc. > /tmp/tmp0y_1vxbg. > root@0f98b9adcd2d:/# find | grep bazel. > ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazelize_command.py. > ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazel_selected_py3.cpython-38.pyc. > ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazelize_command.cpython-38.pyc. > ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazel_selected_py3.py. > root@0f98b9adcd2d:/#. > ```. > . > I remember seeing things like `/tmp/Bazel.runfiles_gd__toh4/` only when I compiled DeepVariant from scratch. It's a tricky task to get it compiled, so I just wanted to be sure you didn't mix up the session you might be running in. I've been running all my tests using a Nextflow pipeline. The command Nextflow uses to run the docker containers looks like this:. ```. docker run -i --cpu-shares 2048 --memory 3072m -u $(id -u) -e ""HOME=${HOME}"" -v /etc/passwd:/etc/passwd:ro -v /etc/shadow:/etc/shadow:ro -v /etc/group:/etc/group:ro -v $HOME:$HOME -v /tmp/tmppixeopws:/tmp/tmppixeopws -w ""$PWD"" --platform linux/x86_64 --name $NXF_BOXID docker.io/google/deepvariant:1.5.0 /bin/bash -ue /tmp/tmppixeopws/16/280c239bb924cbde08365e5fbf5788/.command.sh. ```. I've never had any problems that it would take some files from the current system with the exception of the ones in the volumes (which it's supposed to use)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:308,modifiability,version,version,308,"> @nvnieuwk Are you sure you are running inside the Docker image? I listed all the files regarding the `/tmp` folder inside the Docker container (which all must be the same for all users), and I cannot find the ones shown above:. > . > ```shell. > root@0f98b9adcd2d:/# /opt/deepvariant/bin/run_deepvariant --version. > DeepVariant version 1.5.0. > root@0f98b9adcd2d:/# ls /tmp/Bazel.runfiles_gd__toh4/. > ls: cannot access '/tmp/Bazel.runfiles_gd__toh4/': No such file or directory. > root@0f98b9adcd2d:/# find /tmp. > /tmp. > /tmp/tmpb20xyssf. > /tmp/__pycache__. > /tmp/__pycache__/__autograph_generated_file7y1wd6da.cpython-38.pyc. > /tmp/__pycache__/__autograph_generated_fileilucivm5.cpython-38.pyc. > /tmp/tmp0y_1vxbg. > root@0f98b9adcd2d:/# find | grep bazel. > ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazelize_command.py. > ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazel_selected_py3.cpython-38.pyc. > ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazelize_command.cpython-38.pyc. > ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazel_selected_py3.py. > root@0f98b9adcd2d:/#. > ```. > . > I remember seeing things like `/tmp/Bazel.runfiles_gd__toh4/` only when I compiled DeepVariant from scratch. It's a tricky task to get it compiled, so I just wanted to be sure you didn't mix up the session you might be running in. I've been running all my tests using a Nextflow pipeline. The command Nextflow uses to run the docker containers looks like this:. ```. docker run -i --cpu-shares 2048 --memory 3072m -u $(id -u) -e ""HOME=${HOME}"" -v /etc/passwd:/etc/passwd:ro -v /etc/shadow:/etc/shadow:ro -v /etc/group:/etc/group:ro -v $HOME:$HOME -v /tmp/tmppixeopws:/tmp/tmppixeopws -w ""$PWD"" --platform linux/x86_64 --name $NXF_BOXID docker.io/google/deepvariant:1.5.0 /bin/bash -ue /tmp/tmppixeopws/16/280c239bb924cbde08365e5fbf5788/.command.sh. ```. I've never had any problems that it would take some files from the current system ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:331,modifiability,version,version,331,"> @nvnieuwk Are you sure you are running inside the Docker image? I listed all the files regarding the `/tmp` folder inside the Docker container (which all must be the same for all users), and I cannot find the ones shown above:. > . > ```shell. > root@0f98b9adcd2d:/# /opt/deepvariant/bin/run_deepvariant --version. > DeepVariant version 1.5.0. > root@0f98b9adcd2d:/# ls /tmp/Bazel.runfiles_gd__toh4/. > ls: cannot access '/tmp/Bazel.runfiles_gd__toh4/': No such file or directory. > root@0f98b9adcd2d:/# find /tmp. > /tmp. > /tmp/tmpb20xyssf. > /tmp/__pycache__. > /tmp/__pycache__/__autograph_generated_file7y1wd6da.cpython-38.pyc. > /tmp/__pycache__/__autograph_generated_fileilucivm5.cpython-38.pyc. > /tmp/tmp0y_1vxbg. > root@0f98b9adcd2d:/# find | grep bazel. > ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazelize_command.py. > ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazel_selected_py3.cpython-38.pyc. > ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazelize_command.cpython-38.pyc. > ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazel_selected_py3.py. > root@0f98b9adcd2d:/#. > ```. > . > I remember seeing things like `/tmp/Bazel.runfiles_gd__toh4/` only when I compiled DeepVariant from scratch. It's a tricky task to get it compiled, so I just wanted to be sure you didn't mix up the session you might be running in. I've been running all my tests using a Nextflow pipeline. The command Nextflow uses to run the docker containers looks like this:. ```. docker run -i --cpu-shares 2048 --memory 3072m -u $(id -u) -e ""HOME=${HOME}"" -v /etc/passwd:/etc/passwd:ro -v /etc/shadow:/etc/shadow:ro -v /etc/group:/etc/group:ro -v $HOME:$HOME -v /tmp/tmppixeopws:/tmp/tmppixeopws -w ""$PWD"" --platform linux/x86_64 --name $NXF_BOXID docker.io/google/deepvariant:1.5.0 /bin/bash -ue /tmp/tmppixeopws/16/280c239bb924cbde08365e5fbf5788/.command.sh. ```. I've never had any problems that it would take some files from the current system ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:800,modifiability,pac,packages,800,"> @nvnieuwk Are you sure you are running inside the Docker image? I listed all the files regarding the `/tmp` folder inside the Docker container (which all must be the same for all users), and I cannot find the ones shown above:. > . > ```shell. > root@0f98b9adcd2d:/# /opt/deepvariant/bin/run_deepvariant --version. > DeepVariant version 1.5.0. > root@0f98b9adcd2d:/# ls /tmp/Bazel.runfiles_gd__toh4/. > ls: cannot access '/tmp/Bazel.runfiles_gd__toh4/': No such file or directory. > root@0f98b9adcd2d:/# find /tmp. > /tmp. > /tmp/tmpb20xyssf. > /tmp/__pycache__. > /tmp/__pycache__/__autograph_generated_file7y1wd6da.cpython-38.pyc. > /tmp/__pycache__/__autograph_generated_fileilucivm5.cpython-38.pyc. > /tmp/tmp0y_1vxbg. > root@0f98b9adcd2d:/# find | grep bazel. > ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazelize_command.py. > ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazel_selected_py3.cpython-38.pyc. > ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazelize_command.cpython-38.pyc. > ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazel_selected_py3.py. > root@0f98b9adcd2d:/#. > ```. > . > I remember seeing things like `/tmp/Bazel.runfiles_gd__toh4/` only when I compiled DeepVariant from scratch. It's a tricky task to get it compiled, so I just wanted to be sure you didn't mix up the session you might be running in. I've been running all my tests using a Nextflow pipeline. The command Nextflow uses to run the docker containers looks like this:. ```. docker run -i --cpu-shares 2048 --memory 3072m -u $(id -u) -e ""HOME=${HOME}"" -v /etc/passwd:/etc/passwd:ro -v /etc/shadow:/etc/shadow:ro -v /etc/group:/etc/group:ro -v $HOME:$HOME -v /tmp/tmppixeopws:/tmp/tmppixeopws -w ""$PWD"" --platform linux/x86_64 --name $NXF_BOXID docker.io/google/deepvariant:1.5.0 /bin/bash -ue /tmp/tmppixeopws/16/280c239bb924cbde08365e5fbf5788/.command.sh. ```. I've never had any problems that it would take some files from the current system ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:877,modifiability,pac,packages,877,"> @nvnieuwk Are you sure you are running inside the Docker image? I listed all the files regarding the `/tmp` folder inside the Docker container (which all must be the same for all users), and I cannot find the ones shown above:. > . > ```shell. > root@0f98b9adcd2d:/# /opt/deepvariant/bin/run_deepvariant --version. > DeepVariant version 1.5.0. > root@0f98b9adcd2d:/# ls /tmp/Bazel.runfiles_gd__toh4/. > ls: cannot access '/tmp/Bazel.runfiles_gd__toh4/': No such file or directory. > root@0f98b9adcd2d:/# find /tmp. > /tmp. > /tmp/tmpb20xyssf. > /tmp/__pycache__. > /tmp/__pycache__/__autograph_generated_file7y1wd6da.cpython-38.pyc. > /tmp/__pycache__/__autograph_generated_fileilucivm5.cpython-38.pyc. > /tmp/tmp0y_1vxbg. > root@0f98b9adcd2d:/# find | grep bazel. > ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazelize_command.py. > ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazel_selected_py3.cpython-38.pyc. > ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazelize_command.cpython-38.pyc. > ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazel_selected_py3.py. > root@0f98b9adcd2d:/#. > ```. > . > I remember seeing things like `/tmp/Bazel.runfiles_gd__toh4/` only when I compiled DeepVariant from scratch. It's a tricky task to get it compiled, so I just wanted to be sure you didn't mix up the session you might be running in. I've been running all my tests using a Nextflow pipeline. The command Nextflow uses to run the docker containers looks like this:. ```. docker run -i --cpu-shares 2048 --memory 3072m -u $(id -u) -e ""HOME=${HOME}"" -v /etc/passwd:/etc/passwd:ro -v /etc/shadow:/etc/shadow:ro -v /etc/group:/etc/group:ro -v $HOME:$HOME -v /tmp/tmppixeopws:/tmp/tmppixeopws -w ""$PWD"" --platform linux/x86_64 --name $NXF_BOXID docker.io/google/deepvariant:1.5.0 /bin/bash -ue /tmp/tmppixeopws/16/280c239bb924cbde08365e5fbf5788/.command.sh. ```. I've never had any problems that it would take some files from the current system ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:980,modifiability,pac,packages,980,"> @nvnieuwk Are you sure you are running inside the Docker image? I listed all the files regarding the `/tmp` folder inside the Docker container (which all must be the same for all users), and I cannot find the ones shown above:. > . > ```shell. > root@0f98b9adcd2d:/# /opt/deepvariant/bin/run_deepvariant --version. > DeepVariant version 1.5.0. > root@0f98b9adcd2d:/# ls /tmp/Bazel.runfiles_gd__toh4/. > ls: cannot access '/tmp/Bazel.runfiles_gd__toh4/': No such file or directory. > root@0f98b9adcd2d:/# find /tmp. > /tmp. > /tmp/tmpb20xyssf. > /tmp/__pycache__. > /tmp/__pycache__/__autograph_generated_file7y1wd6da.cpython-38.pyc. > /tmp/__pycache__/__autograph_generated_fileilucivm5.cpython-38.pyc. > /tmp/tmp0y_1vxbg. > root@0f98b9adcd2d:/# find | grep bazel. > ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazelize_command.py. > ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazel_selected_py3.cpython-38.pyc. > ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazelize_command.cpython-38.pyc. > ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazel_selected_py3.py. > root@0f98b9adcd2d:/#. > ```. > . > I remember seeing things like `/tmp/Bazel.runfiles_gd__toh4/` only when I compiled DeepVariant from scratch. It's a tricky task to get it compiled, so I just wanted to be sure you didn't mix up the session you might be running in. I've been running all my tests using a Nextflow pipeline. The command Nextflow uses to run the docker containers looks like this:. ```. docker run -i --cpu-shares 2048 --memory 3072m -u $(id -u) -e ""HOME=${HOME}"" -v /etc/passwd:/etc/passwd:ro -v /etc/shadow:/etc/shadow:ro -v /etc/group:/etc/group:ro -v $HOME:$HOME -v /tmp/tmppixeopws:/tmp/tmppixeopws -w ""$PWD"" --platform linux/x86_64 --name $NXF_BOXID docker.io/google/deepvariant:1.5.0 /bin/bash -ue /tmp/tmppixeopws/16/280c239bb924cbde08365e5fbf5788/.command.sh. ```. I've never had any problems that it would take some files from the current system ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:1081,modifiability,pac,packages,1081," all the files regarding the `/tmp` folder inside the Docker container (which all must be the same for all users), and I cannot find the ones shown above:. > . > ```shell. > root@0f98b9adcd2d:/# /opt/deepvariant/bin/run_deepvariant --version. > DeepVariant version 1.5.0. > root@0f98b9adcd2d:/# ls /tmp/Bazel.runfiles_gd__toh4/. > ls: cannot access '/tmp/Bazel.runfiles_gd__toh4/': No such file or directory. > root@0f98b9adcd2d:/# find /tmp. > /tmp. > /tmp/tmpb20xyssf. > /tmp/__pycache__. > /tmp/__pycache__/__autograph_generated_file7y1wd6da.cpython-38.pyc. > /tmp/__pycache__/__autograph_generated_fileilucivm5.cpython-38.pyc. > /tmp/tmp0y_1vxbg. > root@0f98b9adcd2d:/# find | grep bazel. > ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazelize_command.py. > ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazel_selected_py3.cpython-38.pyc. > ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazelize_command.cpython-38.pyc. > ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazel_selected_py3.py. > root@0f98b9adcd2d:/#. > ```. > . > I remember seeing things like `/tmp/Bazel.runfiles_gd__toh4/` only when I compiled DeepVariant from scratch. It's a tricky task to get it compiled, so I just wanted to be sure you didn't mix up the session you might be running in. I've been running all my tests using a Nextflow pipeline. The command Nextflow uses to run the docker containers looks like this:. ```. docker run -i --cpu-shares 2048 --memory 3072m -u $(id -u) -e ""HOME=${HOME}"" -v /etc/passwd:/etc/passwd:ro -v /etc/shadow:/etc/shadow:ro -v /etc/group:/etc/group:ro -v $HOME:$HOME -v /tmp/tmppixeopws:/tmp/tmppixeopws -w ""$PWD"" --platform linux/x86_64 --name $NXF_BOXID docker.io/google/deepvariant:1.5.0 /bin/bash -ue /tmp/tmppixeopws/16/280c239bb924cbde08365e5fbf5788/.command.sh. ```. I've never had any problems that it would take some files from the current system with the exception of the ones in the volumes (which it's supposed to use)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:1547,performance,cpu,cpu-shares,1547," all the files regarding the `/tmp` folder inside the Docker container (which all must be the same for all users), and I cannot find the ones shown above:. > . > ```shell. > root@0f98b9adcd2d:/# /opt/deepvariant/bin/run_deepvariant --version. > DeepVariant version 1.5.0. > root@0f98b9adcd2d:/# ls /tmp/Bazel.runfiles_gd__toh4/. > ls: cannot access '/tmp/Bazel.runfiles_gd__toh4/': No such file or directory. > root@0f98b9adcd2d:/# find /tmp. > /tmp. > /tmp/tmpb20xyssf. > /tmp/__pycache__. > /tmp/__pycache__/__autograph_generated_file7y1wd6da.cpython-38.pyc. > /tmp/__pycache__/__autograph_generated_fileilucivm5.cpython-38.pyc. > /tmp/tmp0y_1vxbg. > root@0f98b9adcd2d:/# find | grep bazel. > ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazelize_command.py. > ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazel_selected_py3.cpython-38.pyc. > ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazelize_command.cpython-38.pyc. > ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazel_selected_py3.py. > root@0f98b9adcd2d:/#. > ```. > . > I remember seeing things like `/tmp/Bazel.runfiles_gd__toh4/` only when I compiled DeepVariant from scratch. It's a tricky task to get it compiled, so I just wanted to be sure you didn't mix up the session you might be running in. I've been running all my tests using a Nextflow pipeline. The command Nextflow uses to run the docker containers looks like this:. ```. docker run -i --cpu-shares 2048 --memory 3072m -u $(id -u) -e ""HOME=${HOME}"" -v /etc/passwd:/etc/passwd:ro -v /etc/shadow:/etc/shadow:ro -v /etc/group:/etc/group:ro -v $HOME:$HOME -v /tmp/tmppixeopws:/tmp/tmppixeopws -w ""$PWD"" --platform linux/x86_64 --name $NXF_BOXID docker.io/google/deepvariant:1.5.0 /bin/bash -ue /tmp/tmppixeopws/16/280c239bb924cbde08365e5fbf5788/.command.sh. ```. I've never had any problems that it would take some files from the current system with the exception of the ones in the volumes (which it's supposed to use)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:1565,performance,memor,memory,1565," all the files regarding the `/tmp` folder inside the Docker container (which all must be the same for all users), and I cannot find the ones shown above:. > . > ```shell. > root@0f98b9adcd2d:/# /opt/deepvariant/bin/run_deepvariant --version. > DeepVariant version 1.5.0. > root@0f98b9adcd2d:/# ls /tmp/Bazel.runfiles_gd__toh4/. > ls: cannot access '/tmp/Bazel.runfiles_gd__toh4/': No such file or directory. > root@0f98b9adcd2d:/# find /tmp. > /tmp. > /tmp/tmpb20xyssf. > /tmp/__pycache__. > /tmp/__pycache__/__autograph_generated_file7y1wd6da.cpython-38.pyc. > /tmp/__pycache__/__autograph_generated_fileilucivm5.cpython-38.pyc. > /tmp/tmp0y_1vxbg. > root@0f98b9adcd2d:/# find | grep bazel. > ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazelize_command.py. > ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazel_selected_py3.cpython-38.pyc. > ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazelize_command.cpython-38.pyc. > ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazel_selected_py3.py. > root@0f98b9adcd2d:/#. > ```. > . > I remember seeing things like `/tmp/Bazel.runfiles_gd__toh4/` only when I compiled DeepVariant from scratch. It's a tricky task to get it compiled, so I just wanted to be sure you didn't mix up the session you might be running in. I've been running all my tests using a Nextflow pipeline. The command Nextflow uses to run the docker containers looks like this:. ```. docker run -i --cpu-shares 2048 --memory 3072m -u $(id -u) -e ""HOME=${HOME}"" -v /etc/passwd:/etc/passwd:ro -v /etc/shadow:/etc/shadow:ro -v /etc/group:/etc/group:ro -v $HOME:$HOME -v /tmp/tmppixeopws:/tmp/tmppixeopws -w ""$PWD"" --platform linux/x86_64 --name $NXF_BOXID docker.io/google/deepvariant:1.5.0 /bin/bash -ue /tmp/tmppixeopws/16/280c239bb924cbde08365e5fbf5788/.command.sh. ```. I've never had any problems that it would take some files from the current system with the exception of the ones in the volumes (which it's supposed to use)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:814,safety,test,testing,814,"> @nvnieuwk Are you sure you are running inside the Docker image? I listed all the files regarding the `/tmp` folder inside the Docker container (which all must be the same for all users), and I cannot find the ones shown above:. > . > ```shell. > root@0f98b9adcd2d:/# /opt/deepvariant/bin/run_deepvariant --version. > DeepVariant version 1.5.0. > root@0f98b9adcd2d:/# ls /tmp/Bazel.runfiles_gd__toh4/. > ls: cannot access '/tmp/Bazel.runfiles_gd__toh4/': No such file or directory. > root@0f98b9adcd2d:/# find /tmp. > /tmp. > /tmp/tmpb20xyssf. > /tmp/__pycache__. > /tmp/__pycache__/__autograph_generated_file7y1wd6da.cpython-38.pyc. > /tmp/__pycache__/__autograph_generated_fileilucivm5.cpython-38.pyc. > /tmp/tmp0y_1vxbg. > root@0f98b9adcd2d:/# find | grep bazel. > ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazelize_command.py. > ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazel_selected_py3.cpython-38.pyc. > ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazelize_command.cpython-38.pyc. > ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazel_selected_py3.py. > root@0f98b9adcd2d:/#. > ```. > . > I remember seeing things like `/tmp/Bazel.runfiles_gd__toh4/` only when I compiled DeepVariant from scratch. It's a tricky task to get it compiled, so I just wanted to be sure you didn't mix up the session you might be running in. I've been running all my tests using a Nextflow pipeline. The command Nextflow uses to run the docker containers looks like this:. ```. docker run -i --cpu-shares 2048 --memory 3072m -u $(id -u) -e ""HOME=${HOME}"" -v /etc/passwd:/etc/passwd:ro -v /etc/shadow:/etc/shadow:ro -v /etc/group:/etc/group:ro -v $HOME:$HOME -v /tmp/tmppixeopws:/tmp/tmppixeopws -w ""$PWD"" --platform linux/x86_64 --name $NXF_BOXID docker.io/google/deepvariant:1.5.0 /bin/bash -ue /tmp/tmppixeopws/16/280c239bb924cbde08365e5fbf5788/.command.sh. ```. I've never had any problems that it would take some files from the current system ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:891,safety,test,testing,891,"> @nvnieuwk Are you sure you are running inside the Docker image? I listed all the files regarding the `/tmp` folder inside the Docker container (which all must be the same for all users), and I cannot find the ones shown above:. > . > ```shell. > root@0f98b9adcd2d:/# /opt/deepvariant/bin/run_deepvariant --version. > DeepVariant version 1.5.0. > root@0f98b9adcd2d:/# ls /tmp/Bazel.runfiles_gd__toh4/. > ls: cannot access '/tmp/Bazel.runfiles_gd__toh4/': No such file or directory. > root@0f98b9adcd2d:/# find /tmp. > /tmp. > /tmp/tmpb20xyssf. > /tmp/__pycache__. > /tmp/__pycache__/__autograph_generated_file7y1wd6da.cpython-38.pyc. > /tmp/__pycache__/__autograph_generated_fileilucivm5.cpython-38.pyc. > /tmp/tmp0y_1vxbg. > root@0f98b9adcd2d:/# find | grep bazel. > ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazelize_command.py. > ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazel_selected_py3.cpython-38.pyc. > ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazelize_command.cpython-38.pyc. > ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazel_selected_py3.py. > root@0f98b9adcd2d:/#. > ```. > . > I remember seeing things like `/tmp/Bazel.runfiles_gd__toh4/` only when I compiled DeepVariant from scratch. It's a tricky task to get it compiled, so I just wanted to be sure you didn't mix up the session you might be running in. I've been running all my tests using a Nextflow pipeline. The command Nextflow uses to run the docker containers looks like this:. ```. docker run -i --cpu-shares 2048 --memory 3072m -u $(id -u) -e ""HOME=${HOME}"" -v /etc/passwd:/etc/passwd:ro -v /etc/shadow:/etc/shadow:ro -v /etc/group:/etc/group:ro -v $HOME:$HOME -v /tmp/tmppixeopws:/tmp/tmppixeopws -w ""$PWD"" --platform linux/x86_64 --name $NXF_BOXID docker.io/google/deepvariant:1.5.0 /bin/bash -ue /tmp/tmppixeopws/16/280c239bb924cbde08365e5fbf5788/.command.sh. ```. I've never had any problems that it would take some files from the current system ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:994,safety,test,testing,994,"> @nvnieuwk Are you sure you are running inside the Docker image? I listed all the files regarding the `/tmp` folder inside the Docker container (which all must be the same for all users), and I cannot find the ones shown above:. > . > ```shell. > root@0f98b9adcd2d:/# /opt/deepvariant/bin/run_deepvariant --version. > DeepVariant version 1.5.0. > root@0f98b9adcd2d:/# ls /tmp/Bazel.runfiles_gd__toh4/. > ls: cannot access '/tmp/Bazel.runfiles_gd__toh4/': No such file or directory. > root@0f98b9adcd2d:/# find /tmp. > /tmp. > /tmp/tmpb20xyssf. > /tmp/__pycache__. > /tmp/__pycache__/__autograph_generated_file7y1wd6da.cpython-38.pyc. > /tmp/__pycache__/__autograph_generated_fileilucivm5.cpython-38.pyc. > /tmp/tmp0y_1vxbg. > root@0f98b9adcd2d:/# find | grep bazel. > ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazelize_command.py. > ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazel_selected_py3.cpython-38.pyc. > ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazelize_command.cpython-38.pyc. > ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazel_selected_py3.py. > root@0f98b9adcd2d:/#. > ```. > . > I remember seeing things like `/tmp/Bazel.runfiles_gd__toh4/` only when I compiled DeepVariant from scratch. It's a tricky task to get it compiled, so I just wanted to be sure you didn't mix up the session you might be running in. I've been running all my tests using a Nextflow pipeline. The command Nextflow uses to run the docker containers looks like this:. ```. docker run -i --cpu-shares 2048 --memory 3072m -u $(id -u) -e ""HOME=${HOME}"" -v /etc/passwd:/etc/passwd:ro -v /etc/shadow:/etc/shadow:ro -v /etc/group:/etc/group:ro -v $HOME:$HOME -v /tmp/tmppixeopws:/tmp/tmppixeopws -w ""$PWD"" --platform linux/x86_64 --name $NXF_BOXID docker.io/google/deepvariant:1.5.0 /bin/bash -ue /tmp/tmppixeopws/16/280c239bb924cbde08365e5fbf5788/.command.sh. ```. I've never had any problems that it would take some files from the current system ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:1095,safety,test,testing,1095," all the files regarding the `/tmp` folder inside the Docker container (which all must be the same for all users), and I cannot find the ones shown above:. > . > ```shell. > root@0f98b9adcd2d:/# /opt/deepvariant/bin/run_deepvariant --version. > DeepVariant version 1.5.0. > root@0f98b9adcd2d:/# ls /tmp/Bazel.runfiles_gd__toh4/. > ls: cannot access '/tmp/Bazel.runfiles_gd__toh4/': No such file or directory. > root@0f98b9adcd2d:/# find /tmp. > /tmp. > /tmp/tmpb20xyssf. > /tmp/__pycache__. > /tmp/__pycache__/__autograph_generated_file7y1wd6da.cpython-38.pyc. > /tmp/__pycache__/__autograph_generated_fileilucivm5.cpython-38.pyc. > /tmp/tmp0y_1vxbg. > root@0f98b9adcd2d:/# find | grep bazel. > ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazelize_command.py. > ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazel_selected_py3.cpython-38.pyc. > ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazelize_command.cpython-38.pyc. > ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazel_selected_py3.py. > root@0f98b9adcd2d:/#. > ```. > . > I remember seeing things like `/tmp/Bazel.runfiles_gd__toh4/` only when I compiled DeepVariant from scratch. It's a tricky task to get it compiled, so I just wanted to be sure you didn't mix up the session you might be running in. I've been running all my tests using a Nextflow pipeline. The command Nextflow uses to run the docker containers looks like this:. ```. docker run -i --cpu-shares 2048 --memory 3072m -u $(id -u) -e ""HOME=${HOME}"" -v /etc/passwd:/etc/passwd:ro -v /etc/shadow:/etc/shadow:ro -v /etc/group:/etc/group:ro -v $HOME:$HOME -v /tmp/tmppixeopws:/tmp/tmppixeopws -w ""$PWD"" --platform linux/x86_64 --name $NXF_BOXID docker.io/google/deepvariant:1.5.0 /bin/bash -ue /tmp/tmppixeopws/16/280c239bb924cbde08365e5fbf5788/.command.sh. ```. I've never had any problems that it would take some files from the current system with the exception of the ones in the volumes (which it's supposed to use)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:1166,safety,reme,remember,1166," all the files regarding the `/tmp` folder inside the Docker container (which all must be the same for all users), and I cannot find the ones shown above:. > . > ```shell. > root@0f98b9adcd2d:/# /opt/deepvariant/bin/run_deepvariant --version. > DeepVariant version 1.5.0. > root@0f98b9adcd2d:/# ls /tmp/Bazel.runfiles_gd__toh4/. > ls: cannot access '/tmp/Bazel.runfiles_gd__toh4/': No such file or directory. > root@0f98b9adcd2d:/# find /tmp. > /tmp. > /tmp/tmpb20xyssf. > /tmp/__pycache__. > /tmp/__pycache__/__autograph_generated_file7y1wd6da.cpython-38.pyc. > /tmp/__pycache__/__autograph_generated_fileilucivm5.cpython-38.pyc. > /tmp/tmp0y_1vxbg. > root@0f98b9adcd2d:/# find | grep bazel. > ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazelize_command.py. > ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazel_selected_py3.cpython-38.pyc. > ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazelize_command.cpython-38.pyc. > ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazel_selected_py3.py. > root@0f98b9adcd2d:/#. > ```. > . > I remember seeing things like `/tmp/Bazel.runfiles_gd__toh4/` only when I compiled DeepVariant from scratch. It's a tricky task to get it compiled, so I just wanted to be sure you didn't mix up the session you might be running in. I've been running all my tests using a Nextflow pipeline. The command Nextflow uses to run the docker containers looks like this:. ```. docker run -i --cpu-shares 2048 --memory 3072m -u $(id -u) -e ""HOME=${HOME}"" -v /etc/passwd:/etc/passwd:ro -v /etc/shadow:/etc/shadow:ro -v /etc/group:/etc/group:ro -v $HOME:$HOME -v /tmp/tmppixeopws:/tmp/tmppixeopws -w ""$PWD"" --platform linux/x86_64 --name $NXF_BOXID docker.io/google/deepvariant:1.5.0 /bin/bash -ue /tmp/tmppixeopws/16/280c239bb924cbde08365e5fbf5788/.command.sh. ```. I've never had any problems that it would take some files from the current system with the exception of the ones in the volumes (which it's supposed to use)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:1420,safety,test,tests,1420," all the files regarding the `/tmp` folder inside the Docker container (which all must be the same for all users), and I cannot find the ones shown above:. > . > ```shell. > root@0f98b9adcd2d:/# /opt/deepvariant/bin/run_deepvariant --version. > DeepVariant version 1.5.0. > root@0f98b9adcd2d:/# ls /tmp/Bazel.runfiles_gd__toh4/. > ls: cannot access '/tmp/Bazel.runfiles_gd__toh4/': No such file or directory. > root@0f98b9adcd2d:/# find /tmp. > /tmp. > /tmp/tmpb20xyssf. > /tmp/__pycache__. > /tmp/__pycache__/__autograph_generated_file7y1wd6da.cpython-38.pyc. > /tmp/__pycache__/__autograph_generated_fileilucivm5.cpython-38.pyc. > /tmp/tmp0y_1vxbg. > root@0f98b9adcd2d:/# find | grep bazel. > ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazelize_command.py. > ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazel_selected_py3.cpython-38.pyc. > ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazelize_command.cpython-38.pyc. > ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazel_selected_py3.py. > root@0f98b9adcd2d:/#. > ```. > . > I remember seeing things like `/tmp/Bazel.runfiles_gd__toh4/` only when I compiled DeepVariant from scratch. It's a tricky task to get it compiled, so I just wanted to be sure you didn't mix up the session you might be running in. I've been running all my tests using a Nextflow pipeline. The command Nextflow uses to run the docker containers looks like this:. ```. docker run -i --cpu-shares 2048 --memory 3072m -u $(id -u) -e ""HOME=${HOME}"" -v /etc/passwd:/etc/passwd:ro -v /etc/shadow:/etc/shadow:ro -v /etc/group:/etc/group:ro -v $HOME:$HOME -v /tmp/tmppixeopws:/tmp/tmppixeopws -w ""$PWD"" --platform linux/x86_64 --name $NXF_BOXID docker.io/google/deepvariant:1.5.0 /bin/bash -ue /tmp/tmppixeopws/16/280c239bb924cbde08365e5fbf5788/.command.sh. ```. I've never had any problems that it would take some files from the current system with the exception of the ones in the volumes (which it's supposed to use)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:2009,safety,except,exception,2009," all the files regarding the `/tmp` folder inside the Docker container (which all must be the same for all users), and I cannot find the ones shown above:. > . > ```shell. > root@0f98b9adcd2d:/# /opt/deepvariant/bin/run_deepvariant --version. > DeepVariant version 1.5.0. > root@0f98b9adcd2d:/# ls /tmp/Bazel.runfiles_gd__toh4/. > ls: cannot access '/tmp/Bazel.runfiles_gd__toh4/': No such file or directory. > root@0f98b9adcd2d:/# find /tmp. > /tmp. > /tmp/tmpb20xyssf. > /tmp/__pycache__. > /tmp/__pycache__/__autograph_generated_file7y1wd6da.cpython-38.pyc. > /tmp/__pycache__/__autograph_generated_fileilucivm5.cpython-38.pyc. > /tmp/tmp0y_1vxbg. > root@0f98b9adcd2d:/# find | grep bazel. > ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazelize_command.py. > ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazel_selected_py3.cpython-38.pyc. > ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazelize_command.cpython-38.pyc. > ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazel_selected_py3.py. > root@0f98b9adcd2d:/#. > ```. > . > I remember seeing things like `/tmp/Bazel.runfiles_gd__toh4/` only when I compiled DeepVariant from scratch. It's a tricky task to get it compiled, so I just wanted to be sure you didn't mix up the session you might be running in. I've been running all my tests using a Nextflow pipeline. The command Nextflow uses to run the docker containers looks like this:. ```. docker run -i --cpu-shares 2048 --memory 3072m -u $(id -u) -e ""HOME=${HOME}"" -v /etc/passwd:/etc/passwd:ro -v /etc/shadow:/etc/shadow:ro -v /etc/group:/etc/group:ro -v $HOME:$HOME -v /tmp/tmppixeopws:/tmp/tmppixeopws -w ""$PWD"" --platform linux/x86_64 --name $NXF_BOXID docker.io/google/deepvariant:1.5.0 /bin/bash -ue /tmp/tmppixeopws/16/280c239bb924cbde08365e5fbf5788/.command.sh. ```. I've never had any problems that it would take some files from the current system with the exception of the ones in the volumes (which it's supposed to use)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:416,security,access,access,416,"> @nvnieuwk Are you sure you are running inside the Docker image? I listed all the files regarding the `/tmp` folder inside the Docker container (which all must be the same for all users), and I cannot find the ones shown above:. > . > ```shell. > root@0f98b9adcd2d:/# /opt/deepvariant/bin/run_deepvariant --version. > DeepVariant version 1.5.0. > root@0f98b9adcd2d:/# ls /tmp/Bazel.runfiles_gd__toh4/. > ls: cannot access '/tmp/Bazel.runfiles_gd__toh4/': No such file or directory. > root@0f98b9adcd2d:/# find /tmp. > /tmp. > /tmp/tmpb20xyssf. > /tmp/__pycache__. > /tmp/__pycache__/__autograph_generated_file7y1wd6da.cpython-38.pyc. > /tmp/__pycache__/__autograph_generated_fileilucivm5.cpython-38.pyc. > /tmp/tmp0y_1vxbg. > root@0f98b9adcd2d:/# find | grep bazel. > ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazelize_command.py. > ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazel_selected_py3.cpython-38.pyc. > ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazelize_command.cpython-38.pyc. > ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazel_selected_py3.py. > root@0f98b9adcd2d:/#. > ```. > . > I remember seeing things like `/tmp/Bazel.runfiles_gd__toh4/` only when I compiled DeepVariant from scratch. It's a tricky task to get it compiled, so I just wanted to be sure you didn't mix up the session you might be running in. I've been running all my tests using a Nextflow pipeline. The command Nextflow uses to run the docker containers looks like this:. ```. docker run -i --cpu-shares 2048 --memory 3072m -u $(id -u) -e ""HOME=${HOME}"" -v /etc/passwd:/etc/passwd:ro -v /etc/shadow:/etc/shadow:ro -v /etc/group:/etc/group:ro -v $HOME:$HOME -v /tmp/tmppixeopws:/tmp/tmppixeopws -w ""$PWD"" --platform linux/x86_64 --name $NXF_BOXID docker.io/google/deepvariant:1.5.0 /bin/bash -ue /tmp/tmppixeopws/16/280c239bb924cbde08365e5fbf5788/.command.sh. ```. I've never had any problems that it would take some files from the current system ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:1362,security,session,session,1362," all the files regarding the `/tmp` folder inside the Docker container (which all must be the same for all users), and I cannot find the ones shown above:. > . > ```shell. > root@0f98b9adcd2d:/# /opt/deepvariant/bin/run_deepvariant --version. > DeepVariant version 1.5.0. > root@0f98b9adcd2d:/# ls /tmp/Bazel.runfiles_gd__toh4/. > ls: cannot access '/tmp/Bazel.runfiles_gd__toh4/': No such file or directory. > root@0f98b9adcd2d:/# find /tmp. > /tmp. > /tmp/tmpb20xyssf. > /tmp/__pycache__. > /tmp/__pycache__/__autograph_generated_file7y1wd6da.cpython-38.pyc. > /tmp/__pycache__/__autograph_generated_fileilucivm5.cpython-38.pyc. > /tmp/tmp0y_1vxbg. > root@0f98b9adcd2d:/# find | grep bazel. > ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazelize_command.py. > ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazel_selected_py3.cpython-38.pyc. > ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazelize_command.cpython-38.pyc. > ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazel_selected_py3.py. > root@0f98b9adcd2d:/#. > ```. > . > I remember seeing things like `/tmp/Bazel.runfiles_gd__toh4/` only when I compiled DeepVariant from scratch. It's a tricky task to get it compiled, so I just wanted to be sure you didn't mix up the session you might be running in. I've been running all my tests using a Nextflow pipeline. The command Nextflow uses to run the docker containers looks like this:. ```. docker run -i --cpu-shares 2048 --memory 3072m -u $(id -u) -e ""HOME=${HOME}"" -v /etc/passwd:/etc/passwd:ro -v /etc/shadow:/etc/shadow:ro -v /etc/group:/etc/group:ro -v $HOME:$HOME -v /tmp/tmppixeopws:/tmp/tmppixeopws -w ""$PWD"" --platform linux/x86_64 --name $NXF_BOXID docker.io/google/deepvariant:1.5.0 /bin/bash -ue /tmp/tmppixeopws/16/280c239bb924cbde08365e5fbf5788/.command.sh. ```. I've never had any problems that it would take some files from the current system with the exception of the ones in the volumes (which it's supposed to use)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:814,testability,test,testing,814,"> @nvnieuwk Are you sure you are running inside the Docker image? I listed all the files regarding the `/tmp` folder inside the Docker container (which all must be the same for all users), and I cannot find the ones shown above:. > . > ```shell. > root@0f98b9adcd2d:/# /opt/deepvariant/bin/run_deepvariant --version. > DeepVariant version 1.5.0. > root@0f98b9adcd2d:/# ls /tmp/Bazel.runfiles_gd__toh4/. > ls: cannot access '/tmp/Bazel.runfiles_gd__toh4/': No such file or directory. > root@0f98b9adcd2d:/# find /tmp. > /tmp. > /tmp/tmpb20xyssf. > /tmp/__pycache__. > /tmp/__pycache__/__autograph_generated_file7y1wd6da.cpython-38.pyc. > /tmp/__pycache__/__autograph_generated_fileilucivm5.cpython-38.pyc. > /tmp/tmp0y_1vxbg. > root@0f98b9adcd2d:/# find | grep bazel. > ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazelize_command.py. > ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazel_selected_py3.cpython-38.pyc. > ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazelize_command.cpython-38.pyc. > ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazel_selected_py3.py. > root@0f98b9adcd2d:/#. > ```. > . > I remember seeing things like `/tmp/Bazel.runfiles_gd__toh4/` only when I compiled DeepVariant from scratch. It's a tricky task to get it compiled, so I just wanted to be sure you didn't mix up the session you might be running in. I've been running all my tests using a Nextflow pipeline. The command Nextflow uses to run the docker containers looks like this:. ```. docker run -i --cpu-shares 2048 --memory 3072m -u $(id -u) -e ""HOME=${HOME}"" -v /etc/passwd:/etc/passwd:ro -v /etc/shadow:/etc/shadow:ro -v /etc/group:/etc/group:ro -v $HOME:$HOME -v /tmp/tmppixeopws:/tmp/tmppixeopws -w ""$PWD"" --platform linux/x86_64 --name $NXF_BOXID docker.io/google/deepvariant:1.5.0 /bin/bash -ue /tmp/tmppixeopws/16/280c239bb924cbde08365e5fbf5788/.command.sh. ```. I've never had any problems that it would take some files from the current system ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:891,testability,test,testing,891,"> @nvnieuwk Are you sure you are running inside the Docker image? I listed all the files regarding the `/tmp` folder inside the Docker container (which all must be the same for all users), and I cannot find the ones shown above:. > . > ```shell. > root@0f98b9adcd2d:/# /opt/deepvariant/bin/run_deepvariant --version. > DeepVariant version 1.5.0. > root@0f98b9adcd2d:/# ls /tmp/Bazel.runfiles_gd__toh4/. > ls: cannot access '/tmp/Bazel.runfiles_gd__toh4/': No such file or directory. > root@0f98b9adcd2d:/# find /tmp. > /tmp. > /tmp/tmpb20xyssf. > /tmp/__pycache__. > /tmp/__pycache__/__autograph_generated_file7y1wd6da.cpython-38.pyc. > /tmp/__pycache__/__autograph_generated_fileilucivm5.cpython-38.pyc. > /tmp/tmp0y_1vxbg. > root@0f98b9adcd2d:/# find | grep bazel. > ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazelize_command.py. > ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazel_selected_py3.cpython-38.pyc. > ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazelize_command.cpython-38.pyc. > ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazel_selected_py3.py. > root@0f98b9adcd2d:/#. > ```. > . > I remember seeing things like `/tmp/Bazel.runfiles_gd__toh4/` only when I compiled DeepVariant from scratch. It's a tricky task to get it compiled, so I just wanted to be sure you didn't mix up the session you might be running in. I've been running all my tests using a Nextflow pipeline. The command Nextflow uses to run the docker containers looks like this:. ```. docker run -i --cpu-shares 2048 --memory 3072m -u $(id -u) -e ""HOME=${HOME}"" -v /etc/passwd:/etc/passwd:ro -v /etc/shadow:/etc/shadow:ro -v /etc/group:/etc/group:ro -v $HOME:$HOME -v /tmp/tmppixeopws:/tmp/tmppixeopws -w ""$PWD"" --platform linux/x86_64 --name $NXF_BOXID docker.io/google/deepvariant:1.5.0 /bin/bash -ue /tmp/tmppixeopws/16/280c239bb924cbde08365e5fbf5788/.command.sh. ```. I've never had any problems that it would take some files from the current system ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:994,testability,test,testing,994,"> @nvnieuwk Are you sure you are running inside the Docker image? I listed all the files regarding the `/tmp` folder inside the Docker container (which all must be the same for all users), and I cannot find the ones shown above:. > . > ```shell. > root@0f98b9adcd2d:/# /opt/deepvariant/bin/run_deepvariant --version. > DeepVariant version 1.5.0. > root@0f98b9adcd2d:/# ls /tmp/Bazel.runfiles_gd__toh4/. > ls: cannot access '/tmp/Bazel.runfiles_gd__toh4/': No such file or directory. > root@0f98b9adcd2d:/# find /tmp. > /tmp. > /tmp/tmpb20xyssf. > /tmp/__pycache__. > /tmp/__pycache__/__autograph_generated_file7y1wd6da.cpython-38.pyc. > /tmp/__pycache__/__autograph_generated_fileilucivm5.cpython-38.pyc. > /tmp/tmp0y_1vxbg. > root@0f98b9adcd2d:/# find | grep bazel. > ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazelize_command.py. > ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazel_selected_py3.cpython-38.pyc. > ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazelize_command.cpython-38.pyc. > ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazel_selected_py3.py. > root@0f98b9adcd2d:/#. > ```. > . > I remember seeing things like `/tmp/Bazel.runfiles_gd__toh4/` only when I compiled DeepVariant from scratch. It's a tricky task to get it compiled, so I just wanted to be sure you didn't mix up the session you might be running in. I've been running all my tests using a Nextflow pipeline. The command Nextflow uses to run the docker containers looks like this:. ```. docker run -i --cpu-shares 2048 --memory 3072m -u $(id -u) -e ""HOME=${HOME}"" -v /etc/passwd:/etc/passwd:ro -v /etc/shadow:/etc/shadow:ro -v /etc/group:/etc/group:ro -v $HOME:$HOME -v /tmp/tmppixeopws:/tmp/tmppixeopws -w ""$PWD"" --platform linux/x86_64 --name $NXF_BOXID docker.io/google/deepvariant:1.5.0 /bin/bash -ue /tmp/tmppixeopws/16/280c239bb924cbde08365e5fbf5788/.command.sh. ```. I've never had any problems that it would take some files from the current system ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:1095,testability,test,testing,1095," all the files regarding the `/tmp` folder inside the Docker container (which all must be the same for all users), and I cannot find the ones shown above:. > . > ```shell. > root@0f98b9adcd2d:/# /opt/deepvariant/bin/run_deepvariant --version. > DeepVariant version 1.5.0. > root@0f98b9adcd2d:/# ls /tmp/Bazel.runfiles_gd__toh4/. > ls: cannot access '/tmp/Bazel.runfiles_gd__toh4/': No such file or directory. > root@0f98b9adcd2d:/# find /tmp. > /tmp. > /tmp/tmpb20xyssf. > /tmp/__pycache__. > /tmp/__pycache__/__autograph_generated_file7y1wd6da.cpython-38.pyc. > /tmp/__pycache__/__autograph_generated_fileilucivm5.cpython-38.pyc. > /tmp/tmp0y_1vxbg. > root@0f98b9adcd2d:/# find | grep bazel. > ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazelize_command.py. > ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazel_selected_py3.cpython-38.pyc. > ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazelize_command.cpython-38.pyc. > ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazel_selected_py3.py. > root@0f98b9adcd2d:/#. > ```. > . > I remember seeing things like `/tmp/Bazel.runfiles_gd__toh4/` only when I compiled DeepVariant from scratch. It's a tricky task to get it compiled, so I just wanted to be sure you didn't mix up the session you might be running in. I've been running all my tests using a Nextflow pipeline. The command Nextflow uses to run the docker containers looks like this:. ```. docker run -i --cpu-shares 2048 --memory 3072m -u $(id -u) -e ""HOME=${HOME}"" -v /etc/passwd:/etc/passwd:ro -v /etc/shadow:/etc/shadow:ro -v /etc/group:/etc/group:ro -v $HOME:$HOME -v /tmp/tmppixeopws:/tmp/tmppixeopws -w ""$PWD"" --platform linux/x86_64 --name $NXF_BOXID docker.io/google/deepvariant:1.5.0 /bin/bash -ue /tmp/tmppixeopws/16/280c239bb924cbde08365e5fbf5788/.command.sh. ```. I've never had any problems that it would take some files from the current system with the exception of the ones in the volumes (which it's supposed to use)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:1420,testability,test,tests,1420," all the files regarding the `/tmp` folder inside the Docker container (which all must be the same for all users), and I cannot find the ones shown above:. > . > ```shell. > root@0f98b9adcd2d:/# /opt/deepvariant/bin/run_deepvariant --version. > DeepVariant version 1.5.0. > root@0f98b9adcd2d:/# ls /tmp/Bazel.runfiles_gd__toh4/. > ls: cannot access '/tmp/Bazel.runfiles_gd__toh4/': No such file or directory. > root@0f98b9adcd2d:/# find /tmp. > /tmp. > /tmp/tmpb20xyssf. > /tmp/__pycache__. > /tmp/__pycache__/__autograph_generated_file7y1wd6da.cpython-38.pyc. > /tmp/__pycache__/__autograph_generated_fileilucivm5.cpython-38.pyc. > /tmp/tmp0y_1vxbg. > root@0f98b9adcd2d:/# find | grep bazel. > ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazelize_command.py. > ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazel_selected_py3.cpython-38.pyc. > ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazelize_command.cpython-38.pyc. > ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazel_selected_py3.py. > root@0f98b9adcd2d:/#. > ```. > . > I remember seeing things like `/tmp/Bazel.runfiles_gd__toh4/` only when I compiled DeepVariant from scratch. It's a tricky task to get it compiled, so I just wanted to be sure you didn't mix up the session you might be running in. I've been running all my tests using a Nextflow pipeline. The command Nextflow uses to run the docker containers looks like this:. ```. docker run -i --cpu-shares 2048 --memory 3072m -u $(id -u) -e ""HOME=${HOME}"" -v /etc/passwd:/etc/passwd:ro -v /etc/shadow:/etc/shadow:ro -v /etc/group:/etc/group:ro -v $HOME:$HOME -v /tmp/tmppixeopws:/tmp/tmppixeopws -w ""$PWD"" --platform linux/x86_64 --name $NXF_BOXID docker.io/google/deepvariant:1.5.0 /bin/bash -ue /tmp/tmppixeopws/16/280c239bb924cbde08365e5fbf5788/.command.sh. ```. I've never had any problems that it would take some files from the current system with the exception of the ones in the volumes (which it's supposed to use)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:181,usability,user,users,181,"> @nvnieuwk Are you sure you are running inside the Docker image? I listed all the files regarding the `/tmp` folder inside the Docker container (which all must be the same for all users), and I cannot find the ones shown above:. > . > ```shell. > root@0f98b9adcd2d:/# /opt/deepvariant/bin/run_deepvariant --version. > DeepVariant version 1.5.0. > root@0f98b9adcd2d:/# ls /tmp/Bazel.runfiles_gd__toh4/. > ls: cannot access '/tmp/Bazel.runfiles_gd__toh4/': No such file or directory. > root@0f98b9adcd2d:/# find /tmp. > /tmp. > /tmp/tmpb20xyssf. > /tmp/__pycache__. > /tmp/__pycache__/__autograph_generated_file7y1wd6da.cpython-38.pyc. > /tmp/__pycache__/__autograph_generated_fileilucivm5.cpython-38.pyc. > /tmp/tmp0y_1vxbg. > root@0f98b9adcd2d:/# find | grep bazel. > ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazelize_command.py. > ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazel_selected_py3.cpython-38.pyc. > ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazelize_command.cpython-38.pyc. > ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazel_selected_py3.py. > root@0f98b9adcd2d:/#. > ```. > . > I remember seeing things like `/tmp/Bazel.runfiles_gd__toh4/` only when I compiled DeepVariant from scratch. It's a tricky task to get it compiled, so I just wanted to be sure you didn't mix up the session you might be running in. I've been running all my tests using a Nextflow pipeline. The command Nextflow uses to run the docker containers looks like this:. ```. docker run -i --cpu-shares 2048 --memory 3072m -u $(id -u) -e ""HOME=${HOME}"" -v /etc/passwd:/etc/passwd:ro -v /etc/shadow:/etc/shadow:ro -v /etc/group:/etc/group:ro -v $HOME:$HOME -v /tmp/tmppixeopws:/tmp/tmppixeopws -w ""$PWD"" --platform linux/x86_64 --name $NXF_BOXID docker.io/google/deepvariant:1.5.0 /bin/bash -ue /tmp/tmppixeopws/16/280c239bb924cbde08365e5fbf5788/.command.sh. ```. I've never had any problems that it would take some files from the current system ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:1457,usability,command,command,1457," all the files regarding the `/tmp` folder inside the Docker container (which all must be the same for all users), and I cannot find the ones shown above:. > . > ```shell. > root@0f98b9adcd2d:/# /opt/deepvariant/bin/run_deepvariant --version. > DeepVariant version 1.5.0. > root@0f98b9adcd2d:/# ls /tmp/Bazel.runfiles_gd__toh4/. > ls: cannot access '/tmp/Bazel.runfiles_gd__toh4/': No such file or directory. > root@0f98b9adcd2d:/# find /tmp. > /tmp. > /tmp/tmpb20xyssf. > /tmp/__pycache__. > /tmp/__pycache__/__autograph_generated_file7y1wd6da.cpython-38.pyc. > /tmp/__pycache__/__autograph_generated_fileilucivm5.cpython-38.pyc. > /tmp/tmp0y_1vxbg. > root@0f98b9adcd2d:/# find | grep bazel. > ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazelize_command.py. > ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazel_selected_py3.cpython-38.pyc. > ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazelize_command.cpython-38.pyc. > ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazel_selected_py3.py. > root@0f98b9adcd2d:/#. > ```. > . > I remember seeing things like `/tmp/Bazel.runfiles_gd__toh4/` only when I compiled DeepVariant from scratch. It's a tricky task to get it compiled, so I just wanted to be sure you didn't mix up the session you might be running in. I've been running all my tests using a Nextflow pipeline. The command Nextflow uses to run the docker containers looks like this:. ```. docker run -i --cpu-shares 2048 --memory 3072m -u $(id -u) -e ""HOME=${HOME}"" -v /etc/passwd:/etc/passwd:ro -v /etc/shadow:/etc/shadow:ro -v /etc/group:/etc/group:ro -v $HOME:$HOME -v /tmp/tmppixeopws:/tmp/tmppixeopws -w ""$PWD"" --platform linux/x86_64 --name $NXF_BOXID docker.io/google/deepvariant:1.5.0 /bin/bash -ue /tmp/tmppixeopws/16/280c239bb924cbde08365e5fbf5788/.command.sh. ```. I've never had any problems that it would take some files from the current system with the exception of the ones in the volumes (which it's supposed to use)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:1565,usability,memor,memory,1565," all the files regarding the `/tmp` folder inside the Docker container (which all must be the same for all users), and I cannot find the ones shown above:. > . > ```shell. > root@0f98b9adcd2d:/# /opt/deepvariant/bin/run_deepvariant --version. > DeepVariant version 1.5.0. > root@0f98b9adcd2d:/# ls /tmp/Bazel.runfiles_gd__toh4/. > ls: cannot access '/tmp/Bazel.runfiles_gd__toh4/': No such file or directory. > root@0f98b9adcd2d:/# find /tmp. > /tmp. > /tmp/tmpb20xyssf. > /tmp/__pycache__. > /tmp/__pycache__/__autograph_generated_file7y1wd6da.cpython-38.pyc. > /tmp/__pycache__/__autograph_generated_fileilucivm5.cpython-38.pyc. > /tmp/tmp0y_1vxbg. > root@0f98b9adcd2d:/# find | grep bazel. > ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazelize_command.py. > ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazel_selected_py3.cpython-38.pyc. > ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazelize_command.cpython-38.pyc. > ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazel_selected_py3.py. > root@0f98b9adcd2d:/#. > ```. > . > I remember seeing things like `/tmp/Bazel.runfiles_gd__toh4/` only when I compiled DeepVariant from scratch. It's a tricky task to get it compiled, so I just wanted to be sure you didn't mix up the session you might be running in. I've been running all my tests using a Nextflow pipeline. The command Nextflow uses to run the docker containers looks like this:. ```. docker run -i --cpu-shares 2048 --memory 3072m -u $(id -u) -e ""HOME=${HOME}"" -v /etc/passwd:/etc/passwd:ro -v /etc/shadow:/etc/shadow:ro -v /etc/group:/etc/group:ro -v $HOME:$HOME -v /tmp/tmppixeopws:/tmp/tmppixeopws -w ""$PWD"" --platform linux/x86_64 --name $NXF_BOXID docker.io/google/deepvariant:1.5.0 /bin/bash -ue /tmp/tmppixeopws/16/280c239bb924cbde08365e5fbf5788/.command.sh. ```. I've never had any problems that it would take some files from the current system with the exception of the ones in the volumes (which it's supposed to use)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:1901,usability,command,command,1901," all the files regarding the `/tmp` folder inside the Docker container (which all must be the same for all users), and I cannot find the ones shown above:. > . > ```shell. > root@0f98b9adcd2d:/# /opt/deepvariant/bin/run_deepvariant --version. > DeepVariant version 1.5.0. > root@0f98b9adcd2d:/# ls /tmp/Bazel.runfiles_gd__toh4/. > ls: cannot access '/tmp/Bazel.runfiles_gd__toh4/': No such file or directory. > root@0f98b9adcd2d:/# find /tmp. > /tmp. > /tmp/tmpb20xyssf. > /tmp/__pycache__. > /tmp/__pycache__/__autograph_generated_file7y1wd6da.cpython-38.pyc. > /tmp/__pycache__/__autograph_generated_fileilucivm5.cpython-38.pyc. > /tmp/tmp0y_1vxbg. > root@0f98b9adcd2d:/# find | grep bazel. > ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazelize_command.py. > ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazel_selected_py3.cpython-38.pyc. > ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazelize_command.cpython-38.pyc. > ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazel_selected_py3.py. > root@0f98b9adcd2d:/#. > ```. > . > I remember seeing things like `/tmp/Bazel.runfiles_gd__toh4/` only when I compiled DeepVariant from scratch. It's a tricky task to get it compiled, so I just wanted to be sure you didn't mix up the session you might be running in. I've been running all my tests using a Nextflow pipeline. The command Nextflow uses to run the docker containers looks like this:. ```. docker run -i --cpu-shares 2048 --memory 3072m -u $(id -u) -e ""HOME=${HOME}"" -v /etc/passwd:/etc/passwd:ro -v /etc/shadow:/etc/shadow:ro -v /etc/group:/etc/group:ro -v $HOME:$HOME -v /tmp/tmppixeopws:/tmp/tmppixeopws -w ""$PWD"" --platform linux/x86_64 --name $NXF_BOXID docker.io/google/deepvariant:1.5.0 /bin/bash -ue /tmp/tmppixeopws/16/280c239bb924cbde08365e5fbf5788/.command.sh. ```. I've never had any problems that it would take some files from the current system with the exception of the ones in the volumes (which it's supposed to use)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:67,availability,error,error,67,"@nvnieuwk from my understanding, I would be surprised if the numpy error is related to the region. I feel that might be a red herring. Unfortunately I don't have a clear answer for you because I can't reproduce your exact setting. If you think it can still be related to the small region, the next thing I'd suggest you try is: Start from the setting that worked before (CRAM file that contains the whole chromosome 21). First confirm that still works, and just change one thing: restrict to a smaller region and see if that still works or fails with the numpy error again. Sorry that I don't have better suggestions for now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:561,availability,error,error,561,"@nvnieuwk from my understanding, I would be surprised if the numpy error is related to the region. I feel that might be a red herring. Unfortunately I don't have a clear answer for you because I can't reproduce your exact setting. If you think it can still be related to the small region, the next thing I'd suggest you try is: Start from the setting that worked before (CRAM file that contains the whole chromosome 21). First confirm that still works, and just change one thing: restrict to a smaller region and see if that still works or fails with the numpy error again. Sorry that I don't have better suggestions for now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:386,deployability,contain,contains,386,"@nvnieuwk from my understanding, I would be surprised if the numpy error is related to the region. I feel that might be a red herring. Unfortunately I don't have a clear answer for you because I can't reproduce your exact setting. If you think it can still be related to the small region, the next thing I'd suggest you try is: Start from the setting that worked before (CRAM file that contains the whole chromosome 21). First confirm that still works, and just change one thing: restrict to a smaller region and see if that still works or fails with the numpy error again. Sorry that I don't have better suggestions for now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:540,deployability,fail,fails,540,"@nvnieuwk from my understanding, I would be surprised if the numpy error is related to the region. I feel that might be a red herring. Unfortunately I don't have a clear answer for you because I can't reproduce your exact setting. If you think it can still be related to the small region, the next thing I'd suggest you try is: Start from the setting that worked before (CRAM file that contains the whole chromosome 21). First confirm that still works, and just change one thing: restrict to a smaller region and see if that still works or fails with the numpy error again. Sorry that I don't have better suggestions for now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:67,performance,error,error,67,"@nvnieuwk from my understanding, I would be surprised if the numpy error is related to the region. I feel that might be a red herring. Unfortunately I don't have a clear answer for you because I can't reproduce your exact setting. If you think it can still be related to the small region, the next thing I'd suggest you try is: Start from the setting that worked before (CRAM file that contains the whole chromosome 21). First confirm that still works, and just change one thing: restrict to a smaller region and see if that still works or fails with the numpy error again. Sorry that I don't have better suggestions for now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:561,performance,error,error,561,"@nvnieuwk from my understanding, I would be surprised if the numpy error is related to the region. I feel that might be a red herring. Unfortunately I don't have a clear answer for you because I can't reproduce your exact setting. If you think it can still be related to the small region, the next thing I'd suggest you try is: Start from the setting that worked before (CRAM file that contains the whole chromosome 21). First confirm that still works, and just change one thing: restrict to a smaller region and see if that still works or fails with the numpy error again. Sorry that I don't have better suggestions for now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:540,reliability,fail,fails,540,"@nvnieuwk from my understanding, I would be surprised if the numpy error is related to the region. I feel that might be a red herring. Unfortunately I don't have a clear answer for you because I can't reproduce your exact setting. If you think it can still be related to the small region, the next thing I'd suggest you try is: Start from the setting that worked before (CRAM file that contains the whole chromosome 21). First confirm that still works, and just change one thing: restrict to a smaller region and see if that still works or fails with the numpy error again. Sorry that I don't have better suggestions for now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:67,safety,error,error,67,"@nvnieuwk from my understanding, I would be surprised if the numpy error is related to the region. I feel that might be a red herring. Unfortunately I don't have a clear answer for you because I can't reproduce your exact setting. If you think it can still be related to the small region, the next thing I'd suggest you try is: Start from the setting that worked before (CRAM file that contains the whole chromosome 21). First confirm that still works, and just change one thing: restrict to a smaller region and see if that still works or fails with the numpy error again. Sorry that I don't have better suggestions for now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:561,safety,error,error,561,"@nvnieuwk from my understanding, I would be surprised if the numpy error is related to the region. I feel that might be a red herring. Unfortunately I don't have a clear answer for you because I can't reproduce your exact setting. If you think it can still be related to the small region, the next thing I'd suggest you try is: Start from the setting that worked before (CRAM file that contains the whole chromosome 21). First confirm that still works, and just change one thing: restrict to a smaller region and see if that still works or fails with the numpy error again. Sorry that I don't have better suggestions for now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:18,testability,understand,understanding,18,"@nvnieuwk from my understanding, I would be surprised if the numpy error is related to the region. I feel that might be a red herring. Unfortunately I don't have a clear answer for you because I can't reproduce your exact setting. If you think it can still be related to the small region, the next thing I'd suggest you try is: Start from the setting that worked before (CRAM file that contains the whole chromosome 21). First confirm that still works, and just change one thing: restrict to a smaller region and see if that still works or fails with the numpy error again. Sorry that I don't have better suggestions for now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:67,usability,error,error,67,"@nvnieuwk from my understanding, I would be surprised if the numpy error is related to the region. I feel that might be a red herring. Unfortunately I don't have a clear answer for you because I can't reproduce your exact setting. If you think it can still be related to the small region, the next thing I'd suggest you try is: Start from the setting that worked before (CRAM file that contains the whole chromosome 21). First confirm that still works, and just change one thing: restrict to a smaller region and see if that still works or fails with the numpy error again. Sorry that I don't have better suggestions for now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:164,usability,clear,clear,164,"@nvnieuwk from my understanding, I would be surprised if the numpy error is related to the region. I feel that might be a red herring. Unfortunately I don't have a clear answer for you because I can't reproduce your exact setting. If you think it can still be related to the small region, the next thing I'd suggest you try is: Start from the setting that worked before (CRAM file that contains the whole chromosome 21). First confirm that still works, and just change one thing: restrict to a smaller region and see if that still works or fails with the numpy error again. Sorry that I don't have better suggestions for now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:427,usability,confirm,confirm,427,"@nvnieuwk from my understanding, I would be surprised if the numpy error is related to the region. I feel that might be a red herring. Unfortunately I don't have a clear answer for you because I can't reproduce your exact setting. If you think it can still be related to the small region, the next thing I'd suggest you try is: Start from the setting that worked before (CRAM file that contains the whole chromosome 21). First confirm that still works, and just change one thing: restrict to a smaller region and see if that still works or fails with the numpy error again. Sorry that I don't have better suggestions for now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:561,usability,error,error,561,"@nvnieuwk from my understanding, I would be surprised if the numpy error is related to the region. I feel that might be a red herring. Unfortunately I don't have a clear answer for you because I can't reproduce your exact setting. If you think it can still be related to the small region, the next thing I'd suggest you try is: Start from the setting that worked before (CRAM file that contains the whole chromosome 21). First confirm that still works, and just change one thing: restrict to a smaller region and see if that still works or fails with the numpy error again. Sorry that I don't have better suggestions for now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:164,integrability,repositor,repository,164,"Thank you for the suggestions, I've tried running the chr21 CRAM in the same setup and they didn't work here either. Something must be wrong with the setup of that repository, I'll check the differences with the repo where deepvariant worked :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:164,interoperability,repositor,repository,164,"Thank you for the suggestions, I've tried running the chr21 CRAM in the same setup and they didn't work here either. Something must be wrong with the setup of that repository, I'll check the differences with the repo where deepvariant worked :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:372,availability,replic,replicating,372,"Hi Nicolas,. Aside from Pi-Chuan's recommendation -- which I think is very good to try -- see if also doing these two steps yield any results (which might make the region issue go away naturally):. 1) To minimize the number of working variables, first let's try Docker by itself. This would be without NextFlow, just by using the command-line (in a terminal Bash session) replicating exactly the [DeepVariant QuickStart](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-quick-start.md). If it does work, then the next step should be easier to configure. 2) If step 1 works, then you can customize a NextFlow pipeline based on the configuration of step one, using [containerOptions](https://www.nextflow.io/docs/latest/process.html#containeroptions), which I'm sure you already know or a custom process. I am assuming your NextFlow implementation allows you to create custom pipelines, which do not add any default volume mounts or environment variables. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:624,deployability,pipelin,pipeline,624,"Hi Nicolas,. Aside from Pi-Chuan's recommendation -- which I think is very good to try -- see if also doing these two steps yield any results (which might make the region issue go away naturally):. 1) To minimize the number of working variables, first let's try Docker by itself. This would be without NextFlow, just by using the command-line (in a terminal Bash session) replicating exactly the [DeepVariant QuickStart](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-quick-start.md). If it does work, then the next step should be easier to configure. 2) If step 1 works, then you can customize a NextFlow pipeline based on the configuration of step one, using [containerOptions](https://www.nextflow.io/docs/latest/process.html#containeroptions), which I'm sure you already know or a custom process. I am assuming your NextFlow implementation allows you to create custom pipelines, which do not add any default volume mounts or environment variables. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:646,deployability,configurat,configuration,646,"Hi Nicolas,. Aside from Pi-Chuan's recommendation -- which I think is very good to try -- see if also doing these two steps yield any results (which might make the region issue go away naturally):. 1) To minimize the number of working variables, first let's try Docker by itself. This would be without NextFlow, just by using the command-line (in a terminal Bash session) replicating exactly the [DeepVariant QuickStart](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-quick-start.md). If it does work, then the next step should be easier to configure. 2) If step 1 works, then you can customize a NextFlow pipeline based on the configuration of step one, using [containerOptions](https://www.nextflow.io/docs/latest/process.html#containeroptions), which I'm sure you already know or a custom process. I am assuming your NextFlow implementation allows you to create custom pipelines, which do not add any default volume mounts or environment variables. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:680,deployability,contain,containerOptions,680,"Hi Nicolas,. Aside from Pi-Chuan's recommendation -- which I think is very good to try -- see if also doing these two steps yield any results (which might make the region issue go away naturally):. 1) To minimize the number of working variables, first let's try Docker by itself. This would be without NextFlow, just by using the command-line (in a terminal Bash session) replicating exactly the [DeepVariant QuickStart](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-quick-start.md). If it does work, then the next step should be easier to configure. 2) If step 1 works, then you can customize a NextFlow pipeline based on the configuration of step one, using [containerOptions](https://www.nextflow.io/docs/latest/process.html#containeroptions), which I'm sure you already know or a custom process. I am assuming your NextFlow implementation allows you to create custom pipelines, which do not add any default volume mounts or environment variables. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:747,deployability,contain,containeroptions,747,"Hi Nicolas,. Aside from Pi-Chuan's recommendation -- which I think is very good to try -- see if also doing these two steps yield any results (which might make the region issue go away naturally):. 1) To minimize the number of working variables, first let's try Docker by itself. This would be without NextFlow, just by using the command-line (in a terminal Bash session) replicating exactly the [DeepVariant QuickStart](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-quick-start.md). If it does work, then the next step should be easier to configure. 2) If step 1 works, then you can customize a NextFlow pipeline based on the configuration of step one, using [containerOptions](https://www.nextflow.io/docs/latest/process.html#containeroptions), which I'm sure you already know or a custom process. I am assuming your NextFlow implementation allows you to create custom pipelines, which do not add any default volume mounts or environment variables. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:890,deployability,pipelin,pipelines,890,"Hi Nicolas,. Aside from Pi-Chuan's recommendation -- which I think is very good to try -- see if also doing these two steps yield any results (which might make the region issue go away naturally):. 1) To minimize the number of working variables, first let's try Docker by itself. This would be without NextFlow, just by using the command-line (in a terminal Bash session) replicating exactly the [DeepVariant QuickStart](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-quick-start.md). If it does work, then the next step should be easier to configure. 2) If step 1 works, then you can customize a NextFlow pipeline based on the configuration of step one, using [containerOptions](https://www.nextflow.io/docs/latest/process.html#containeroptions), which I'm sure you already know or a custom process. I am assuming your NextFlow implementation allows you to create custom pipelines, which do not add any default volume mounts or environment variables. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:559,integrability,configur,configure,559,"Hi Nicolas,. Aside from Pi-Chuan's recommendation -- which I think is very good to try -- see if also doing these two steps yield any results (which might make the region issue go away naturally):. 1) To minimize the number of working variables, first let's try Docker by itself. This would be without NextFlow, just by using the command-line (in a terminal Bash session) replicating exactly the [DeepVariant QuickStart](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-quick-start.md). If it does work, then the next step should be easier to configure. 2) If step 1 works, then you can customize a NextFlow pipeline based on the configuration of step one, using [containerOptions](https://www.nextflow.io/docs/latest/process.html#containeroptions), which I'm sure you already know or a custom process. I am assuming your NextFlow implementation allows you to create custom pipelines, which do not add any default volume mounts or environment variables. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:624,integrability,pipelin,pipeline,624,"Hi Nicolas,. Aside from Pi-Chuan's recommendation -- which I think is very good to try -- see if also doing these two steps yield any results (which might make the region issue go away naturally):. 1) To minimize the number of working variables, first let's try Docker by itself. This would be without NextFlow, just by using the command-line (in a terminal Bash session) replicating exactly the [DeepVariant QuickStart](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-quick-start.md). If it does work, then the next step should be easier to configure. 2) If step 1 works, then you can customize a NextFlow pipeline based on the configuration of step one, using [containerOptions](https://www.nextflow.io/docs/latest/process.html#containeroptions), which I'm sure you already know or a custom process. I am assuming your NextFlow implementation allows you to create custom pipelines, which do not add any default volume mounts or environment variables. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:646,integrability,configur,configuration,646,"Hi Nicolas,. Aside from Pi-Chuan's recommendation -- which I think is very good to try -- see if also doing these two steps yield any results (which might make the region issue go away naturally):. 1) To minimize the number of working variables, first let's try Docker by itself. This would be without NextFlow, just by using the command-line (in a terminal Bash session) replicating exactly the [DeepVariant QuickStart](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-quick-start.md). If it does work, then the next step should be easier to configure. 2) If step 1 works, then you can customize a NextFlow pipeline based on the configuration of step one, using [containerOptions](https://www.nextflow.io/docs/latest/process.html#containeroptions), which I'm sure you already know or a custom process. I am assuming your NextFlow implementation allows you to create custom pipelines, which do not add any default volume mounts or environment variables. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:890,integrability,pipelin,pipelines,890,"Hi Nicolas,. Aside from Pi-Chuan's recommendation -- which I think is very good to try -- see if also doing these two steps yield any results (which might make the region issue go away naturally):. 1) To minimize the number of working variables, first let's try Docker by itself. This would be without NextFlow, just by using the command-line (in a terminal Bash session) replicating exactly the [DeepVariant QuickStart](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-quick-start.md). If it does work, then the next step should be easier to configure. 2) If step 1 works, then you can customize a NextFlow pipeline based on the configuration of step one, using [containerOptions](https://www.nextflow.io/docs/latest/process.html#containeroptions), which I'm sure you already know or a custom process. I am assuming your NextFlow implementation allows you to create custom pipelines, which do not add any default volume mounts or environment variables. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:235,modifiability,variab,variables,235,"Hi Nicolas,. Aside from Pi-Chuan's recommendation -- which I think is very good to try -- see if also doing these two steps yield any results (which might make the region issue go away naturally):. 1) To minimize the number of working variables, first let's try Docker by itself. This would be without NextFlow, just by using the command-line (in a terminal Bash session) replicating exactly the [DeepVariant QuickStart](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-quick-start.md). If it does work, then the next step should be easier to configure. 2) If step 1 works, then you can customize a NextFlow pipeline based on the configuration of step one, using [containerOptions](https://www.nextflow.io/docs/latest/process.html#containeroptions), which I'm sure you already know or a custom process. I am assuming your NextFlow implementation allows you to create custom pipelines, which do not add any default volume mounts or environment variables. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:559,modifiability,configur,configure,559,"Hi Nicolas,. Aside from Pi-Chuan's recommendation -- which I think is very good to try -- see if also doing these two steps yield any results (which might make the region issue go away naturally):. 1) To minimize the number of working variables, first let's try Docker by itself. This would be without NextFlow, just by using the command-line (in a terminal Bash session) replicating exactly the [DeepVariant QuickStart](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-quick-start.md). If it does work, then the next step should be easier to configure. 2) If step 1 works, then you can customize a NextFlow pipeline based on the configuration of step one, using [containerOptions](https://www.nextflow.io/docs/latest/process.html#containeroptions), which I'm sure you already know or a custom process. I am assuming your NextFlow implementation allows you to create custom pipelines, which do not add any default volume mounts or environment variables. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:646,modifiability,configur,configuration,646,"Hi Nicolas,. Aside from Pi-Chuan's recommendation -- which I think is very good to try -- see if also doing these two steps yield any results (which might make the region issue go away naturally):. 1) To minimize the number of working variables, first let's try Docker by itself. This would be without NextFlow, just by using the command-line (in a terminal Bash session) replicating exactly the [DeepVariant QuickStart](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-quick-start.md). If it does work, then the next step should be easier to configure. 2) If step 1 works, then you can customize a NextFlow pipeline based on the configuration of step one, using [containerOptions](https://www.nextflow.io/docs/latest/process.html#containeroptions), which I'm sure you already know or a custom process. I am assuming your NextFlow implementation allows you to create custom pipelines, which do not add any default volume mounts or environment variables. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:959,modifiability,variab,variables,959,"Hi Nicolas,. Aside from Pi-Chuan's recommendation -- which I think is very good to try -- see if also doing these two steps yield any results (which might make the region issue go away naturally):. 1) To minimize the number of working variables, first let's try Docker by itself. This would be without NextFlow, just by using the command-line (in a terminal Bash session) replicating exactly the [DeepVariant QuickStart](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-quick-start.md). If it does work, then the next step should be easier to configure. 2) If step 1 works, then you can customize a NextFlow pipeline based on the configuration of step one, using [containerOptions](https://www.nextflow.io/docs/latest/process.html#containeroptions), which I'm sure you already know or a custom process. I am assuming your NextFlow implementation allows you to create custom pipelines, which do not add any default volume mounts or environment variables. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:509,reliability,doe,does,509,"Hi Nicolas,. Aside from Pi-Chuan's recommendation -- which I think is very good to try -- see if also doing these two steps yield any results (which might make the region issue go away naturally):. 1) To minimize the number of working variables, first let's try Docker by itself. This would be without NextFlow, just by using the command-line (in a terminal Bash session) replicating exactly the [DeepVariant QuickStart](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-quick-start.md). If it does work, then the next step should be easier to configure. 2) If step 1 works, then you can customize a NextFlow pipeline based on the configuration of step one, using [containerOptions](https://www.nextflow.io/docs/latest/process.html#containeroptions), which I'm sure you already know or a custom process. I am assuming your NextFlow implementation allows you to create custom pipelines, which do not add any default volume mounts or environment variables. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:330,security,command-lin,command-line,330,"Hi Nicolas,. Aside from Pi-Chuan's recommendation -- which I think is very good to try -- see if also doing these two steps yield any results (which might make the region issue go away naturally):. 1) To minimize the number of working variables, first let's try Docker by itself. This would be without NextFlow, just by using the command-line (in a terminal Bash session) replicating exactly the [DeepVariant QuickStart](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-quick-start.md). If it does work, then the next step should be easier to configure. 2) If step 1 works, then you can customize a NextFlow pipeline based on the configuration of step one, using [containerOptions](https://www.nextflow.io/docs/latest/process.html#containeroptions), which I'm sure you already know or a custom process. I am assuming your NextFlow implementation allows you to create custom pipelines, which do not add any default volume mounts or environment variables. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:363,security,session,session,363,"Hi Nicolas,. Aside from Pi-Chuan's recommendation -- which I think is very good to try -- see if also doing these two steps yield any results (which might make the region issue go away naturally):. 1) To minimize the number of working variables, first let's try Docker by itself. This would be without NextFlow, just by using the command-line (in a terminal Bash session) replicating exactly the [DeepVariant QuickStart](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-quick-start.md). If it does work, then the next step should be easier to configure. 2) If step 1 works, then you can customize a NextFlow pipeline based on the configuration of step one, using [containerOptions](https://www.nextflow.io/docs/latest/process.html#containeroptions), which I'm sure you already know or a custom process. I am assuming your NextFlow implementation allows you to create custom pipelines, which do not add any default volume mounts or environment variables. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:559,security,configur,configure,559,"Hi Nicolas,. Aside from Pi-Chuan's recommendation -- which I think is very good to try -- see if also doing these two steps yield any results (which might make the region issue go away naturally):. 1) To minimize the number of working variables, first let's try Docker by itself. This would be without NextFlow, just by using the command-line (in a terminal Bash session) replicating exactly the [DeepVariant QuickStart](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-quick-start.md). If it does work, then the next step should be easier to configure. 2) If step 1 works, then you can customize a NextFlow pipeline based on the configuration of step one, using [containerOptions](https://www.nextflow.io/docs/latest/process.html#containeroptions), which I'm sure you already know or a custom process. I am assuming your NextFlow implementation allows you to create custom pipelines, which do not add any default volume mounts or environment variables. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:646,security,configur,configuration,646,"Hi Nicolas,. Aside from Pi-Chuan's recommendation -- which I think is very good to try -- see if also doing these two steps yield any results (which might make the region issue go away naturally):. 1) To minimize the number of working variables, first let's try Docker by itself. This would be without NextFlow, just by using the command-line (in a terminal Bash session) replicating exactly the [DeepVariant QuickStart](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-quick-start.md). If it does work, then the next step should be easier to configure. 2) If step 1 works, then you can customize a NextFlow pipeline based on the configuration of step one, using [containerOptions](https://www.nextflow.io/docs/latest/process.html#containeroptions), which I'm sure you already know or a custom process. I am assuming your NextFlow implementation allows you to create custom pipelines, which do not add any default volume mounts or environment variables. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:204,usability,minim,minimize,204,"Hi Nicolas,. Aside from Pi-Chuan's recommendation -- which I think is very good to try -- see if also doing these two steps yield any results (which might make the region issue go away naturally):. 1) To minimize the number of working variables, first let's try Docker by itself. This would be without NextFlow, just by using the command-line (in a terminal Bash session) replicating exactly the [DeepVariant QuickStart](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-quick-start.md). If it does work, then the next step should be easier to configure. 2) If step 1 works, then you can customize a NextFlow pipeline based on the configuration of step one, using [containerOptions](https://www.nextflow.io/docs/latest/process.html#containeroptions), which I'm sure you already know or a custom process. I am assuming your NextFlow implementation allows you to create custom pipelines, which do not add any default volume mounts or environment variables. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:330,usability,command,command-line,330,"Hi Nicolas,. Aside from Pi-Chuan's recommendation -- which I think is very good to try -- see if also doing these two steps yield any results (which might make the region issue go away naturally):. 1) To minimize the number of working variables, first let's try Docker by itself. This would be without NextFlow, just by using the command-line (in a terminal Bash session) replicating exactly the [DeepVariant QuickStart](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-quick-start.md). If it does work, then the next step should be easier to configure. 2) If step 1 works, then you can customize a NextFlow pipeline based on the configuration of step one, using [containerOptions](https://www.nextflow.io/docs/latest/process.html#containeroptions), which I'm sure you already know or a custom process. I am assuming your NextFlow implementation allows you to create custom pipelines, which do not add any default volume mounts or environment variables. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:603,usability,custom,customize,603,"Hi Nicolas,. Aside from Pi-Chuan's recommendation -- which I think is very good to try -- see if also doing these two steps yield any results (which might make the region issue go away naturally):. 1) To minimize the number of working variables, first let's try Docker by itself. This would be without NextFlow, just by using the command-line (in a terminal Bash session) replicating exactly the [DeepVariant QuickStart](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-quick-start.md). If it does work, then the next step should be easier to configure. 2) If step 1 works, then you can customize a NextFlow pipeline based on the configuration of step one, using [containerOptions](https://www.nextflow.io/docs/latest/process.html#containeroptions), which I'm sure you already know or a custom process. I am assuming your NextFlow implementation allows you to create custom pipelines, which do not add any default volume mounts or environment variables. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:803,usability,custom,custom,803,"Hi Nicolas,. Aside from Pi-Chuan's recommendation -- which I think is very good to try -- see if also doing these two steps yield any results (which might make the region issue go away naturally):. 1) To minimize the number of working variables, first let's try Docker by itself. This would be without NextFlow, just by using the command-line (in a terminal Bash session) replicating exactly the [DeepVariant QuickStart](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-quick-start.md). If it does work, then the next step should be easier to configure. 2) If step 1 works, then you can customize a NextFlow pipeline based on the configuration of step one, using [containerOptions](https://www.nextflow.io/docs/latest/process.html#containeroptions), which I'm sure you already know or a custom process. I am assuming your NextFlow implementation allows you to create custom pipelines, which do not add any default volume mounts or environment variables. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:883,usability,custom,custom,883,"Hi Nicolas,. Aside from Pi-Chuan's recommendation -- which I think is very good to try -- see if also doing these two steps yield any results (which might make the region issue go away naturally):. 1) To minimize the number of working variables, first let's try Docker by itself. This would be without NextFlow, just by using the command-line (in a terminal Bash session) replicating exactly the [DeepVariant QuickStart](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-quick-start.md). If it does work, then the next step should be easier to configure. 2) If step 1 works, then you can customize a NextFlow pipeline based on the configuration of step one, using [containerOptions](https://www.nextflow.io/docs/latest/process.html#containeroptions), which I'm sure you already know or a custom process. I am assuming your NextFlow implementation allows you to create custom pipelines, which do not add any default volume mounts or environment variables. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/640:129,usability,close,close,129,"Hi @nvnieuwk , it seems like you're able to find a working setup now. It seems like @scfurl also found that 1.4.0 worked. . I'll close this issue for now given that we don't have a reproducible setting on our side to fix the issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640
https://github.com/google/deepvariant/issues/641:147,deployability,releas,releases,147,"Thanks for the question! We recommend you use DeepVariant v1.5.0 to get a model trained with Revio data. See https://github.com/google/deepvariant/releases/tag/v1.5.0 . ""Incorporated PacBio Revio training data in DeepVariant PacBio model. In our evaluations this single model performs well on both Sequel II and Revio datatypes. Please use DeepVariant v1.5 and later for Revio data.""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/641
https://github.com/google/deepvariant/issues/641:74,energy efficiency,model,model,74,"Thanks for the question! We recommend you use DeepVariant v1.5.0 to get a model trained with Revio data. See https://github.com/google/deepvariant/releases/tag/v1.5.0 . ""Incorporated PacBio Revio training data in DeepVariant PacBio model. In our evaluations this single model performs well on both Sequel II and Revio datatypes. Please use DeepVariant v1.5 and later for Revio data.""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/641
https://github.com/google/deepvariant/issues/641:232,energy efficiency,model,model,232,"Thanks for the question! We recommend you use DeepVariant v1.5.0 to get a model trained with Revio data. See https://github.com/google/deepvariant/releases/tag/v1.5.0 . ""Incorporated PacBio Revio training data in DeepVariant PacBio model. In our evaluations this single model performs well on both Sequel II and Revio datatypes. Please use DeepVariant v1.5 and later for Revio data.""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/641
https://github.com/google/deepvariant/issues/641:270,energy efficiency,model,model,270,"Thanks for the question! We recommend you use DeepVariant v1.5.0 to get a model trained with Revio data. See https://github.com/google/deepvariant/releases/tag/v1.5.0 . ""Incorporated PacBio Revio training data in DeepVariant PacBio model. In our evaluations this single model performs well on both Sequel II and Revio datatypes. Please use DeepVariant v1.5 and later for Revio data.""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/641
https://github.com/google/deepvariant/issues/641:183,modifiability,Pac,PacBio,183,"Thanks for the question! We recommend you use DeepVariant v1.5.0 to get a model trained with Revio data. See https://github.com/google/deepvariant/releases/tag/v1.5.0 . ""Incorporated PacBio Revio training data in DeepVariant PacBio model. In our evaluations this single model performs well on both Sequel II and Revio datatypes. Please use DeepVariant v1.5 and later for Revio data.""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/641
https://github.com/google/deepvariant/issues/641:225,modifiability,Pac,PacBio,225,"Thanks for the question! We recommend you use DeepVariant v1.5.0 to get a model trained with Revio data. See https://github.com/google/deepvariant/releases/tag/v1.5.0 . ""Incorporated PacBio Revio training data in DeepVariant PacBio model. In our evaluations this single model performs well on both Sequel II and Revio datatypes. Please use DeepVariant v1.5 and later for Revio data.""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/641
https://github.com/google/deepvariant/issues/641:276,performance,perform,performs,276,"Thanks for the question! We recommend you use DeepVariant v1.5.0 to get a model trained with Revio data. See https://github.com/google/deepvariant/releases/tag/v1.5.0 . ""Incorporated PacBio Revio training data in DeepVariant PacBio model. In our evaluations this single model performs well on both Sequel II and Revio datatypes. Please use DeepVariant v1.5 and later for Revio data.""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/641
https://github.com/google/deepvariant/issues/641:74,security,model,model,74,"Thanks for the question! We recommend you use DeepVariant v1.5.0 to get a model trained with Revio data. See https://github.com/google/deepvariant/releases/tag/v1.5.0 . ""Incorporated PacBio Revio training data in DeepVariant PacBio model. In our evaluations this single model performs well on both Sequel II and Revio datatypes. Please use DeepVariant v1.5 and later for Revio data.""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/641
https://github.com/google/deepvariant/issues/641:232,security,model,model,232,"Thanks for the question! We recommend you use DeepVariant v1.5.0 to get a model trained with Revio data. See https://github.com/google/deepvariant/releases/tag/v1.5.0 . ""Incorporated PacBio Revio training data in DeepVariant PacBio model. In our evaluations this single model performs well on both Sequel II and Revio datatypes. Please use DeepVariant v1.5 and later for Revio data.""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/641
https://github.com/google/deepvariant/issues/641:270,security,model,model,270,"Thanks for the question! We recommend you use DeepVariant v1.5.0 to get a model trained with Revio data. See https://github.com/google/deepvariant/releases/tag/v1.5.0 . ""Incorporated PacBio Revio training data in DeepVariant PacBio model. In our evaluations this single model performs well on both Sequel II and Revio datatypes. Please use DeepVariant v1.5 and later for Revio data.""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/641
https://github.com/google/deepvariant/issues/641:276,usability,perform,performs,276,"Thanks for the question! We recommend you use DeepVariant v1.5.0 to get a model trained with Revio data. See https://github.com/google/deepvariant/releases/tag/v1.5.0 . ""Incorporated PacBio Revio training data in DeepVariant PacBio model. In our evaluations this single model performs well on both Sequel II and Revio datatypes. Please use DeepVariant v1.5 and later for Revio data.""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/641
https://github.com/google/deepvariant/issues/642:2717,availability,error,error,2717,".g. Q10 is 90% confidence in call, Q20 is 99% confidence, Q30 99.9% confidence, and so on). **--filter-name LowQ --filter ""DP <200 "" --filter-name LowD** This field implies to me you are doing panel sequencing or deep exome sequencing, as this is a high value. The sample info should have DP present and it will not differ between GATK and DeepVariant output. You can use the same filter. . **--filter ""DP> 10000 "" --filter-name HigD** This is again the DP field, which is present in the call file and can also be used. **MQ<40.0 "" --filter-name SedT** This filters low mappability regions. This is actually a fairly high value, which means this will aggressively filter low mappability regions. MQ is seen by DeepVariant and used in calling to determine the variant call and confidence, so it will be reflected in the GQ value. However, MQ is not reported into the VCF, so this filter cannot be used for filtering without additional annotation of the VCF. If you want to filter out low mappability regions, I would recommend intersecting with the high mappability BED file from genome in a bottle. **--filter ""SOR > 50.0 "" --filter-name LowSOR** This filter relates to the strand of reads over a variant. Strand bias is something which DeepVariant can see in its inputs and will use this information to determine variant calls and assign confidence to those calls. However, strand bias is not reported a a separate value in the VCF, so this filter cannot be used for filtering without additional annotation of the VCF. I would say that use of this additional filter isn't needed. **--filter ""AN <150 "" --filter-name LowMiss""** As I understand this filter, it quantifies the number of different alleles identified at a position. I suppose this is some measure of the error rate around a position. If I understand the annotation correctly, DeepVariant should never produce value which come remotely close to this value in the cohort. You can exclude this filter as I don't think it would ever trigger.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/642
https://github.com/google/deepvariant/issues/642:642,energy efficiency,Predict,Predicted,642,"Hi @wangsb111 . For filtering, it remains the case that filtering on the sample GQ field is the best way to increase specificity while losing the smallest number of true variants. . Are you able to run the same postprocess tool you are using on the GLnexus calls? If so, here are some thoughts on your parameters:. **-filter ""QUAL <50.0"" --filter-name LowQ** The QUAL field can be used. DeepVariant is much more conservative and realistic about the QUAL values. Filtering at 50 will filter too many real calls. Figure 1 in our [DeepVariant-GLnexus paper](https://academic.oup.com/bioinformatics/article/36/24/5582/6064144) has a chart of the Predicted and Real qualities for GATK and DeepVariant. Matching the predicted-real accuracy by quality between the two would put the comparable QUAL threshold at 12-17. Note that DeepVariant's GQ values are well-calibrated over the range of 0-60, so you can map them directly onto your desired specificity (e.g. Q10 is 90% confidence in call, Q20 is 99% confidence, Q30 99.9% confidence, and so on). **--filter-name LowQ --filter ""DP <200 "" --filter-name LowD** This field implies to me you are doing panel sequencing or deep exome sequencing, as this is a high value. The sample info should have DP present and it will not differ between GATK and DeepVariant output. You can use the same filter. . **--filter ""DP> 10000 "" --filter-name HigD** This is again the DP field, which is present in the call file and can also be used. **MQ<40.0 "" --filter-name SedT** This filters low mappability regions. This is actually a fairly high value, which means this will aggressively filter low mappability regions. MQ is seen by DeepVariant and used in calling to determine the variant call and confidence, so it will be reflected in the GQ value. However, MQ is not reported into the VCF, so this filter cannot be used for filtering without additional annotation of the VCF. If you want to filter out low mappability regions, I would recommend intersecting with the hi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/642
https://github.com/google/deepvariant/issues/642:710,energy efficiency,predict,predicted-real,710,"Hi @wangsb111 . For filtering, it remains the case that filtering on the sample GQ field is the best way to increase specificity while losing the smallest number of true variants. . Are you able to run the same postprocess tool you are using on the GLnexus calls? If so, here are some thoughts on your parameters:. **-filter ""QUAL <50.0"" --filter-name LowQ** The QUAL field can be used. DeepVariant is much more conservative and realistic about the QUAL values. Filtering at 50 will filter too many real calls. Figure 1 in our [DeepVariant-GLnexus paper](https://academic.oup.com/bioinformatics/article/36/24/5582/6064144) has a chart of the Predicted and Real qualities for GATK and DeepVariant. Matching the predicted-real accuracy by quality between the two would put the comparable QUAL threshold at 12-17. Note that DeepVariant's GQ values are well-calibrated over the range of 0-60, so you can map them directly onto your desired specificity (e.g. Q10 is 90% confidence in call, Q20 is 99% confidence, Q30 99.9% confidence, and so on). **--filter-name LowQ --filter ""DP <200 "" --filter-name LowD** This field implies to me you are doing panel sequencing or deep exome sequencing, as this is a high value. The sample info should have DP present and it will not differ between GATK and DeepVariant output. You can use the same filter. . **--filter ""DP> 10000 "" --filter-name HigD** This is again the DP field, which is present in the call file and can also be used. **MQ<40.0 "" --filter-name SedT** This filters low mappability regions. This is actually a fairly high value, which means this will aggressively filter low mappability regions. MQ is seen by DeepVariant and used in calling to determine the variant call and confidence, so it will be reflected in the GQ value. However, MQ is not reported into the VCF, so this filter cannot be used for filtering without additional annotation of the VCF. If you want to filter out low mappability regions, I would recommend intersecting with the hi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/642
https://github.com/google/deepvariant/issues/642:2702,energy efficiency,measur,measure,2702,".g. Q10 is 90% confidence in call, Q20 is 99% confidence, Q30 99.9% confidence, and so on). **--filter-name LowQ --filter ""DP <200 "" --filter-name LowD** This field implies to me you are doing panel sequencing or deep exome sequencing, as this is a high value. The sample info should have DP present and it will not differ between GATK and DeepVariant output. You can use the same filter. . **--filter ""DP> 10000 "" --filter-name HigD** This is again the DP field, which is present in the call file and can also be used. **MQ<40.0 "" --filter-name SedT** This filters low mappability regions. This is actually a fairly high value, which means this will aggressively filter low mappability regions. MQ is seen by DeepVariant and used in calling to determine the variant call and confidence, so it will be reflected in the GQ value. However, MQ is not reported into the VCF, so this filter cannot be used for filtering without additional annotation of the VCF. If you want to filter out low mappability regions, I would recommend intersecting with the high mappability BED file from genome in a bottle. **--filter ""SOR > 50.0 "" --filter-name LowSOR** This filter relates to the strand of reads over a variant. Strand bias is something which DeepVariant can see in its inputs and will use this information to determine variant calls and assign confidence to those calls. However, strand bias is not reported a a separate value in the VCF, so this filter cannot be used for filtering without additional annotation of the VCF. I would say that use of this additional filter isn't needed. **--filter ""AN <150 "" --filter-name LowMiss""** As I understand this filter, it quantifies the number of different alleles identified at a position. I suppose this is some measure of the error rate around a position. If I understand the annotation correctly, DeepVariant should never produce value which come remotely close to this value in the cohort. You can exclude this filter as I don't think it would ever trigger.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/642
https://github.com/google/deepvariant/issues/642:20,integrability,filter,filtering,20,"Hi @wangsb111 . For filtering, it remains the case that filtering on the sample GQ field is the best way to increase specificity while losing the smallest number of true variants. . Are you able to run the same postprocess tool you are using on the GLnexus calls? If so, here are some thoughts on your parameters:. **-filter ""QUAL <50.0"" --filter-name LowQ** The QUAL field can be used. DeepVariant is much more conservative and realistic about the QUAL values. Filtering at 50 will filter too many real calls. Figure 1 in our [DeepVariant-GLnexus paper](https://academic.oup.com/bioinformatics/article/36/24/5582/6064144) has a chart of the Predicted and Real qualities for GATK and DeepVariant. Matching the predicted-real accuracy by quality between the two would put the comparable QUAL threshold at 12-17. Note that DeepVariant's GQ values are well-calibrated over the range of 0-60, so you can map them directly onto your desired specificity (e.g. Q10 is 90% confidence in call, Q20 is 99% confidence, Q30 99.9% confidence, and so on). **--filter-name LowQ --filter ""DP <200 "" --filter-name LowD** This field implies to me you are doing panel sequencing or deep exome sequencing, as this is a high value. The sample info should have DP present and it will not differ between GATK and DeepVariant output. You can use the same filter. . **--filter ""DP> 10000 "" --filter-name HigD** This is again the DP field, which is present in the call file and can also be used. **MQ<40.0 "" --filter-name SedT** This filters low mappability regions. This is actually a fairly high value, which means this will aggressively filter low mappability regions. MQ is seen by DeepVariant and used in calling to determine the variant call and confidence, so it will be reflected in the GQ value. However, MQ is not reported into the VCF, so this filter cannot be used for filtering without additional annotation of the VCF. If you want to filter out low mappability regions, I would recommend intersecting with the hi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/642
https://github.com/google/deepvariant/issues/642:56,integrability,filter,filtering,56,"Hi @wangsb111 . For filtering, it remains the case that filtering on the sample GQ field is the best way to increase specificity while losing the smallest number of true variants. . Are you able to run the same postprocess tool you are using on the GLnexus calls? If so, here are some thoughts on your parameters:. **-filter ""QUAL <50.0"" --filter-name LowQ** The QUAL field can be used. DeepVariant is much more conservative and realistic about the QUAL values. Filtering at 50 will filter too many real calls. Figure 1 in our [DeepVariant-GLnexus paper](https://academic.oup.com/bioinformatics/article/36/24/5582/6064144) has a chart of the Predicted and Real qualities for GATK and DeepVariant. Matching the predicted-real accuracy by quality between the two would put the comparable QUAL threshold at 12-17. Note that DeepVariant's GQ values are well-calibrated over the range of 0-60, so you can map them directly onto your desired specificity (e.g. Q10 is 90% confidence in call, Q20 is 99% confidence, Q30 99.9% confidence, and so on). **--filter-name LowQ --filter ""DP <200 "" --filter-name LowD** This field implies to me you are doing panel sequencing or deep exome sequencing, as this is a high value. The sample info should have DP present and it will not differ between GATK and DeepVariant output. You can use the same filter. . **--filter ""DP> 10000 "" --filter-name HigD** This is again the DP field, which is present in the call file and can also be used. **MQ<40.0 "" --filter-name SedT** This filters low mappability regions. This is actually a fairly high value, which means this will aggressively filter low mappability regions. MQ is seen by DeepVariant and used in calling to determine the variant call and confidence, so it will be reflected in the GQ value. However, MQ is not reported into the VCF, so this filter cannot be used for filtering without additional annotation of the VCF. If you want to filter out low mappability regions, I would recommend intersecting with the hi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/642
https://github.com/google/deepvariant/issues/642:318,integrability,filter,filter,318,"Hi @wangsb111 . For filtering, it remains the case that filtering on the sample GQ field is the best way to increase specificity while losing the smallest number of true variants. . Are you able to run the same postprocess tool you are using on the GLnexus calls? If so, here are some thoughts on your parameters:. **-filter ""QUAL <50.0"" --filter-name LowQ** The QUAL field can be used. DeepVariant is much more conservative and realistic about the QUAL values. Filtering at 50 will filter too many real calls. Figure 1 in our [DeepVariant-GLnexus paper](https://academic.oup.com/bioinformatics/article/36/24/5582/6064144) has a chart of the Predicted and Real qualities for GATK and DeepVariant. Matching the predicted-real accuracy by quality between the two would put the comparable QUAL threshold at 12-17. Note that DeepVariant's GQ values are well-calibrated over the range of 0-60, so you can map them directly onto your desired specificity (e.g. Q10 is 90% confidence in call, Q20 is 99% confidence, Q30 99.9% confidence, and so on). **--filter-name LowQ --filter ""DP <200 "" --filter-name LowD** This field implies to me you are doing panel sequencing or deep exome sequencing, as this is a high value. The sample info should have DP present and it will not differ between GATK and DeepVariant output. You can use the same filter. . **--filter ""DP> 10000 "" --filter-name HigD** This is again the DP field, which is present in the call file and can also be used. **MQ<40.0 "" --filter-name SedT** This filters low mappability regions. This is actually a fairly high value, which means this will aggressively filter low mappability regions. MQ is seen by DeepVariant and used in calling to determine the variant call and confidence, so it will be reflected in the GQ value. However, MQ is not reported into the VCF, so this filter cannot be used for filtering without additional annotation of the VCF. If you want to filter out low mappability regions, I would recommend intersecting with the hi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/642
https://github.com/google/deepvariant/issues/642:340,integrability,filter,filter-name,340,"Hi @wangsb111 . For filtering, it remains the case that filtering on the sample GQ field is the best way to increase specificity while losing the smallest number of true variants. . Are you able to run the same postprocess tool you are using on the GLnexus calls? If so, here are some thoughts on your parameters:. **-filter ""QUAL <50.0"" --filter-name LowQ** The QUAL field can be used. DeepVariant is much more conservative and realistic about the QUAL values. Filtering at 50 will filter too many real calls. Figure 1 in our [DeepVariant-GLnexus paper](https://academic.oup.com/bioinformatics/article/36/24/5582/6064144) has a chart of the Predicted and Real qualities for GATK and DeepVariant. Matching the predicted-real accuracy by quality between the two would put the comparable QUAL threshold at 12-17. Note that DeepVariant's GQ values are well-calibrated over the range of 0-60, so you can map them directly onto your desired specificity (e.g. Q10 is 90% confidence in call, Q20 is 99% confidence, Q30 99.9% confidence, and so on). **--filter-name LowQ --filter ""DP <200 "" --filter-name LowD** This field implies to me you are doing panel sequencing or deep exome sequencing, as this is a high value. The sample info should have DP present and it will not differ between GATK and DeepVariant output. You can use the same filter. . **--filter ""DP> 10000 "" --filter-name HigD** This is again the DP field, which is present in the call file and can also be used. **MQ<40.0 "" --filter-name SedT** This filters low mappability regions. This is actually a fairly high value, which means this will aggressively filter low mappability regions. MQ is seen by DeepVariant and used in calling to determine the variant call and confidence, so it will be reflected in the GQ value. However, MQ is not reported into the VCF, so this filter cannot be used for filtering without additional annotation of the VCF. If you want to filter out low mappability regions, I would recommend intersecting with the hi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/642
https://github.com/google/deepvariant/issues/642:462,integrability,Filter,Filtering,462,"Hi @wangsb111 . For filtering, it remains the case that filtering on the sample GQ field is the best way to increase specificity while losing the smallest number of true variants. . Are you able to run the same postprocess tool you are using on the GLnexus calls? If so, here are some thoughts on your parameters:. **-filter ""QUAL <50.0"" --filter-name LowQ** The QUAL field can be used. DeepVariant is much more conservative and realistic about the QUAL values. Filtering at 50 will filter too many real calls. Figure 1 in our [DeepVariant-GLnexus paper](https://academic.oup.com/bioinformatics/article/36/24/5582/6064144) has a chart of the Predicted and Real qualities for GATK and DeepVariant. Matching the predicted-real accuracy by quality between the two would put the comparable QUAL threshold at 12-17. Note that DeepVariant's GQ values are well-calibrated over the range of 0-60, so you can map them directly onto your desired specificity (e.g. Q10 is 90% confidence in call, Q20 is 99% confidence, Q30 99.9% confidence, and so on). **--filter-name LowQ --filter ""DP <200 "" --filter-name LowD** This field implies to me you are doing panel sequencing or deep exome sequencing, as this is a high value. The sample info should have DP present and it will not differ between GATK and DeepVariant output. You can use the same filter. . **--filter ""DP> 10000 "" --filter-name HigD** This is again the DP field, which is present in the call file and can also be used. **MQ<40.0 "" --filter-name SedT** This filters low mappability regions. This is actually a fairly high value, which means this will aggressively filter low mappability regions. MQ is seen by DeepVariant and used in calling to determine the variant call and confidence, so it will be reflected in the GQ value. However, MQ is not reported into the VCF, so this filter cannot be used for filtering without additional annotation of the VCF. If you want to filter out low mappability regions, I would recommend intersecting with the hi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/642
https://github.com/google/deepvariant/issues/642:483,integrability,filter,filter,483,"Hi @wangsb111 . For filtering, it remains the case that filtering on the sample GQ field is the best way to increase specificity while losing the smallest number of true variants. . Are you able to run the same postprocess tool you are using on the GLnexus calls? If so, here are some thoughts on your parameters:. **-filter ""QUAL <50.0"" --filter-name LowQ** The QUAL field can be used. DeepVariant is much more conservative and realistic about the QUAL values. Filtering at 50 will filter too many real calls. Figure 1 in our [DeepVariant-GLnexus paper](https://academic.oup.com/bioinformatics/article/36/24/5582/6064144) has a chart of the Predicted and Real qualities for GATK and DeepVariant. Matching the predicted-real accuracy by quality between the two would put the comparable QUAL threshold at 12-17. Note that DeepVariant's GQ values are well-calibrated over the range of 0-60, so you can map them directly onto your desired specificity (e.g. Q10 is 90% confidence in call, Q20 is 99% confidence, Q30 99.9% confidence, and so on). **--filter-name LowQ --filter ""DP <200 "" --filter-name LowD** This field implies to me you are doing panel sequencing or deep exome sequencing, as this is a high value. The sample info should have DP present and it will not differ between GATK and DeepVariant output. You can use the same filter. . **--filter ""DP> 10000 "" --filter-name HigD** This is again the DP field, which is present in the call file and can also be used. **MQ<40.0 "" --filter-name SedT** This filters low mappability regions. This is actually a fairly high value, which means this will aggressively filter low mappability regions. MQ is seen by DeepVariant and used in calling to determine the variant call and confidence, so it will be reflected in the GQ value. However, MQ is not reported into the VCF, so this filter cannot be used for filtering without additional annotation of the VCF. If you want to filter out low mappability regions, I would recommend intersecting with the hi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/642
https://github.com/google/deepvariant/issues/642:1046,integrability,filter,filter-name,1046,"hat filtering on the sample GQ field is the best way to increase specificity while losing the smallest number of true variants. . Are you able to run the same postprocess tool you are using on the GLnexus calls? If so, here are some thoughts on your parameters:. **-filter ""QUAL <50.0"" --filter-name LowQ** The QUAL field can be used. DeepVariant is much more conservative and realistic about the QUAL values. Filtering at 50 will filter too many real calls. Figure 1 in our [DeepVariant-GLnexus paper](https://academic.oup.com/bioinformatics/article/36/24/5582/6064144) has a chart of the Predicted and Real qualities for GATK and DeepVariant. Matching the predicted-real accuracy by quality between the two would put the comparable QUAL threshold at 12-17. Note that DeepVariant's GQ values are well-calibrated over the range of 0-60, so you can map them directly onto your desired specificity (e.g. Q10 is 90% confidence in call, Q20 is 99% confidence, Q30 99.9% confidence, and so on). **--filter-name LowQ --filter ""DP <200 "" --filter-name LowD** This field implies to me you are doing panel sequencing or deep exome sequencing, as this is a high value. The sample info should have DP present and it will not differ between GATK and DeepVariant output. You can use the same filter. . **--filter ""DP> 10000 "" --filter-name HigD** This is again the DP field, which is present in the call file and can also be used. **MQ<40.0 "" --filter-name SedT** This filters low mappability regions. This is actually a fairly high value, which means this will aggressively filter low mappability regions. MQ is seen by DeepVariant and used in calling to determine the variant call and confidence, so it will be reflected in the GQ value. However, MQ is not reported into the VCF, so this filter cannot be used for filtering without additional annotation of the VCF. If you want to filter out low mappability regions, I would recommend intersecting with the high mappability BED file from genome in a bottle. **-",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/642
https://github.com/google/deepvariant/issues/642:1065,integrability,filter,filter,1065," the sample GQ field is the best way to increase specificity while losing the smallest number of true variants. . Are you able to run the same postprocess tool you are using on the GLnexus calls? If so, here are some thoughts on your parameters:. **-filter ""QUAL <50.0"" --filter-name LowQ** The QUAL field can be used. DeepVariant is much more conservative and realistic about the QUAL values. Filtering at 50 will filter too many real calls. Figure 1 in our [DeepVariant-GLnexus paper](https://academic.oup.com/bioinformatics/article/36/24/5582/6064144) has a chart of the Predicted and Real qualities for GATK and DeepVariant. Matching the predicted-real accuracy by quality between the two would put the comparable QUAL threshold at 12-17. Note that DeepVariant's GQ values are well-calibrated over the range of 0-60, so you can map them directly onto your desired specificity (e.g. Q10 is 90% confidence in call, Q20 is 99% confidence, Q30 99.9% confidence, and so on). **--filter-name LowQ --filter ""DP <200 "" --filter-name LowD** This field implies to me you are doing panel sequencing or deep exome sequencing, as this is a high value. The sample info should have DP present and it will not differ between GATK and DeepVariant output. You can use the same filter. . **--filter ""DP> 10000 "" --filter-name HigD** This is again the DP field, which is present in the call file and can also be used. **MQ<40.0 "" --filter-name SedT** This filters low mappability regions. This is actually a fairly high value, which means this will aggressively filter low mappability regions. MQ is seen by DeepVariant and used in calling to determine the variant call and confidence, so it will be reflected in the GQ value. However, MQ is not reported into the VCF, so this filter cannot be used for filtering without additional annotation of the VCF. If you want to filter out low mappability regions, I would recommend intersecting with the high mappability BED file from genome in a bottle. **--filter ""SOR > 5",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/642
https://github.com/google/deepvariant/issues/642:1085,integrability,filter,filter-name,1085," the best way to increase specificity while losing the smallest number of true variants. . Are you able to run the same postprocess tool you are using on the GLnexus calls? If so, here are some thoughts on your parameters:. **-filter ""QUAL <50.0"" --filter-name LowQ** The QUAL field can be used. DeepVariant is much more conservative and realistic about the QUAL values. Filtering at 50 will filter too many real calls. Figure 1 in our [DeepVariant-GLnexus paper](https://academic.oup.com/bioinformatics/article/36/24/5582/6064144) has a chart of the Predicted and Real qualities for GATK and DeepVariant. Matching the predicted-real accuracy by quality between the two would put the comparable QUAL threshold at 12-17. Note that DeepVariant's GQ values are well-calibrated over the range of 0-60, so you can map them directly onto your desired specificity (e.g. Q10 is 90% confidence in call, Q20 is 99% confidence, Q30 99.9% confidence, and so on). **--filter-name LowQ --filter ""DP <200 "" --filter-name LowD** This field implies to me you are doing panel sequencing or deep exome sequencing, as this is a high value. The sample info should have DP present and it will not differ between GATK and DeepVariant output. You can use the same filter. . **--filter ""DP> 10000 "" --filter-name HigD** This is again the DP field, which is present in the call file and can also be used. **MQ<40.0 "" --filter-name SedT** This filters low mappability regions. This is actually a fairly high value, which means this will aggressively filter low mappability regions. MQ is seen by DeepVariant and used in calling to determine the variant call and confidence, so it will be reflected in the GQ value. However, MQ is not reported into the VCF, so this filter cannot be used for filtering without additional annotation of the VCF. If you want to filter out low mappability regions, I would recommend intersecting with the high mappability BED file from genome in a bottle. **--filter ""SOR > 50.0 "" --filter-name Low",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/642
https://github.com/google/deepvariant/issues/642:1331,integrability,filter,filter,1331,".0"" --filter-name LowQ** The QUAL field can be used. DeepVariant is much more conservative and realistic about the QUAL values. Filtering at 50 will filter too many real calls. Figure 1 in our [DeepVariant-GLnexus paper](https://academic.oup.com/bioinformatics/article/36/24/5582/6064144) has a chart of the Predicted and Real qualities for GATK and DeepVariant. Matching the predicted-real accuracy by quality between the two would put the comparable QUAL threshold at 12-17. Note that DeepVariant's GQ values are well-calibrated over the range of 0-60, so you can map them directly onto your desired specificity (e.g. Q10 is 90% confidence in call, Q20 is 99% confidence, Q30 99.9% confidence, and so on). **--filter-name LowQ --filter ""DP <200 "" --filter-name LowD** This field implies to me you are doing panel sequencing or deep exome sequencing, as this is a high value. The sample info should have DP present and it will not differ between GATK and DeepVariant output. You can use the same filter. . **--filter ""DP> 10000 "" --filter-name HigD** This is again the DP field, which is present in the call file and can also be used. **MQ<40.0 "" --filter-name SedT** This filters low mappability regions. This is actually a fairly high value, which means this will aggressively filter low mappability regions. MQ is seen by DeepVariant and used in calling to determine the variant call and confidence, so it will be reflected in the GQ value. However, MQ is not reported into the VCF, so this filter cannot be used for filtering without additional annotation of the VCF. If you want to filter out low mappability regions, I would recommend intersecting with the high mappability BED file from genome in a bottle. **--filter ""SOR > 50.0 "" --filter-name LowSOR** This filter relates to the strand of reads over a variant. Strand bias is something which DeepVariant can see in its inputs and will use this information to determine variant calls and assign confidence to those calls. However, strand bi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/642
https://github.com/google/deepvariant/issues/642:1345,integrability,filter,filter,1345,"ame LowQ** The QUAL field can be used. DeepVariant is much more conservative and realistic about the QUAL values. Filtering at 50 will filter too many real calls. Figure 1 in our [DeepVariant-GLnexus paper](https://academic.oup.com/bioinformatics/article/36/24/5582/6064144) has a chart of the Predicted and Real qualities for GATK and DeepVariant. Matching the predicted-real accuracy by quality between the two would put the comparable QUAL threshold at 12-17. Note that DeepVariant's GQ values are well-calibrated over the range of 0-60, so you can map them directly onto your desired specificity (e.g. Q10 is 90% confidence in call, Q20 is 99% confidence, Q30 99.9% confidence, and so on). **--filter-name LowQ --filter ""DP <200 "" --filter-name LowD** This field implies to me you are doing panel sequencing or deep exome sequencing, as this is a high value. The sample info should have DP present and it will not differ between GATK and DeepVariant output. You can use the same filter. . **--filter ""DP> 10000 "" --filter-name HigD** This is again the DP field, which is present in the call file and can also be used. **MQ<40.0 "" --filter-name SedT** This filters low mappability regions. This is actually a fairly high value, which means this will aggressively filter low mappability regions. MQ is seen by DeepVariant and used in calling to determine the variant call and confidence, so it will be reflected in the GQ value. However, MQ is not reported into the VCF, so this filter cannot be used for filtering without additional annotation of the VCF. If you want to filter out low mappability regions, I would recommend intersecting with the high mappability BED file from genome in a bottle. **--filter ""SOR > 50.0 "" --filter-name LowSOR** This filter relates to the strand of reads over a variant. Strand bias is something which DeepVariant can see in its inputs and will use this information to determine variant calls and assign confidence to those calls. However, strand bias is not repo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/642
https://github.com/google/deepvariant/issues/642:1367,integrability,filter,filter-name,1367," can be used. DeepVariant is much more conservative and realistic about the QUAL values. Filtering at 50 will filter too many real calls. Figure 1 in our [DeepVariant-GLnexus paper](https://academic.oup.com/bioinformatics/article/36/24/5582/6064144) has a chart of the Predicted and Real qualities for GATK and DeepVariant. Matching the predicted-real accuracy by quality between the two would put the comparable QUAL threshold at 12-17. Note that DeepVariant's GQ values are well-calibrated over the range of 0-60, so you can map them directly onto your desired specificity (e.g. Q10 is 90% confidence in call, Q20 is 99% confidence, Q30 99.9% confidence, and so on). **--filter-name LowQ --filter ""DP <200 "" --filter-name LowD** This field implies to me you are doing panel sequencing or deep exome sequencing, as this is a high value. The sample info should have DP present and it will not differ between GATK and DeepVariant output. You can use the same filter. . **--filter ""DP> 10000 "" --filter-name HigD** This is again the DP field, which is present in the call file and can also be used. **MQ<40.0 "" --filter-name SedT** This filters low mappability regions. This is actually a fairly high value, which means this will aggressively filter low mappability regions. MQ is seen by DeepVariant and used in calling to determine the variant call and confidence, so it will be reflected in the GQ value. However, MQ is not reported into the VCF, so this filter cannot be used for filtering without additional annotation of the VCF. If you want to filter out low mappability regions, I would recommend intersecting with the high mappability BED file from genome in a bottle. **--filter ""SOR > 50.0 "" --filter-name LowSOR** This filter relates to the strand of reads over a variant. Strand bias is something which DeepVariant can see in its inputs and will use this information to determine variant calls and assign confidence to those calls. However, strand bias is not reported a a separate value i",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/642
https://github.com/google/deepvariant/issues/642:1484,integrability,filter,filter-name,1484,"too many real calls. Figure 1 in our [DeepVariant-GLnexus paper](https://academic.oup.com/bioinformatics/article/36/24/5582/6064144) has a chart of the Predicted and Real qualities for GATK and DeepVariant. Matching the predicted-real accuracy by quality between the two would put the comparable QUAL threshold at 12-17. Note that DeepVariant's GQ values are well-calibrated over the range of 0-60, so you can map them directly onto your desired specificity (e.g. Q10 is 90% confidence in call, Q20 is 99% confidence, Q30 99.9% confidence, and so on). **--filter-name LowQ --filter ""DP <200 "" --filter-name LowD** This field implies to me you are doing panel sequencing or deep exome sequencing, as this is a high value. The sample info should have DP present and it will not differ between GATK and DeepVariant output. You can use the same filter. . **--filter ""DP> 10000 "" --filter-name HigD** This is again the DP field, which is present in the call file and can also be used. **MQ<40.0 "" --filter-name SedT** This filters low mappability regions. This is actually a fairly high value, which means this will aggressively filter low mappability regions. MQ is seen by DeepVariant and used in calling to determine the variant call and confidence, so it will be reflected in the GQ value. However, MQ is not reported into the VCF, so this filter cannot be used for filtering without additional annotation of the VCF. If you want to filter out low mappability regions, I would recommend intersecting with the high mappability BED file from genome in a bottle. **--filter ""SOR > 50.0 "" --filter-name LowSOR** This filter relates to the strand of reads over a variant. Strand bias is something which DeepVariant can see in its inputs and will use this information to determine variant calls and assign confidence to those calls. However, strand bias is not reported a a separate value in the VCF, so this filter cannot be used for filtering without additional annotation of the VCF. I would say that use",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/642
https://github.com/google/deepvariant/issues/642:1508,integrability,filter,filters,1508,"igure 1 in our [DeepVariant-GLnexus paper](https://academic.oup.com/bioinformatics/article/36/24/5582/6064144) has a chart of the Predicted and Real qualities for GATK and DeepVariant. Matching the predicted-real accuracy by quality between the two would put the comparable QUAL threshold at 12-17. Note that DeepVariant's GQ values are well-calibrated over the range of 0-60, so you can map them directly onto your desired specificity (e.g. Q10 is 90% confidence in call, Q20 is 99% confidence, Q30 99.9% confidence, and so on). **--filter-name LowQ --filter ""DP <200 "" --filter-name LowD** This field implies to me you are doing panel sequencing or deep exome sequencing, as this is a high value. The sample info should have DP present and it will not differ between GATK and DeepVariant output. You can use the same filter. . **--filter ""DP> 10000 "" --filter-name HigD** This is again the DP field, which is present in the call file and can also be used. **MQ<40.0 "" --filter-name SedT** This filters low mappability regions. This is actually a fairly high value, which means this will aggressively filter low mappability regions. MQ is seen by DeepVariant and used in calling to determine the variant call and confidence, so it will be reflected in the GQ value. However, MQ is not reported into the VCF, so this filter cannot be used for filtering without additional annotation of the VCF. If you want to filter out low mappability regions, I would recommend intersecting with the high mappability BED file from genome in a bottle. **--filter ""SOR > 50.0 "" --filter-name LowSOR** This filter relates to the strand of reads over a variant. Strand bias is something which DeepVariant can see in its inputs and will use this information to determine variant calls and assign confidence to those calls. However, strand bias is not reported a a separate value in the VCF, so this filter cannot be used for filtering without additional annotation of the VCF. I would say that use of this additional fi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/642
https://github.com/google/deepvariant/issues/642:1614,integrability,filter,filter,1614,"4144) has a chart of the Predicted and Real qualities for GATK and DeepVariant. Matching the predicted-real accuracy by quality between the two would put the comparable QUAL threshold at 12-17. Note that DeepVariant's GQ values are well-calibrated over the range of 0-60, so you can map them directly onto your desired specificity (e.g. Q10 is 90% confidence in call, Q20 is 99% confidence, Q30 99.9% confidence, and so on). **--filter-name LowQ --filter ""DP <200 "" --filter-name LowD** This field implies to me you are doing panel sequencing or deep exome sequencing, as this is a high value. The sample info should have DP present and it will not differ between GATK and DeepVariant output. You can use the same filter. . **--filter ""DP> 10000 "" --filter-name HigD** This is again the DP field, which is present in the call file and can also be used. **MQ<40.0 "" --filter-name SedT** This filters low mappability regions. This is actually a fairly high value, which means this will aggressively filter low mappability regions. MQ is seen by DeepVariant and used in calling to determine the variant call and confidence, so it will be reflected in the GQ value. However, MQ is not reported into the VCF, so this filter cannot be used for filtering without additional annotation of the VCF. If you want to filter out low mappability regions, I would recommend intersecting with the high mappability BED file from genome in a bottle. **--filter ""SOR > 50.0 "" --filter-name LowSOR** This filter relates to the strand of reads over a variant. Strand bias is something which DeepVariant can see in its inputs and will use this information to determine variant calls and assign confidence to those calls. However, strand bias is not reported a a separate value in the VCF, so this filter cannot be used for filtering without additional annotation of the VCF. I would say that use of this additional filter isn't needed. **--filter ""AN <150 "" --filter-name LowMiss""** As I understand this filter, it quantif",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/642
https://github.com/google/deepvariant/issues/642:1829,integrability,filter,filter,1829,"'s GQ values are well-calibrated over the range of 0-60, so you can map them directly onto your desired specificity (e.g. Q10 is 90% confidence in call, Q20 is 99% confidence, Q30 99.9% confidence, and so on). **--filter-name LowQ --filter ""DP <200 "" --filter-name LowD** This field implies to me you are doing panel sequencing or deep exome sequencing, as this is a high value. The sample info should have DP present and it will not differ between GATK and DeepVariant output. You can use the same filter. . **--filter ""DP> 10000 "" --filter-name HigD** This is again the DP field, which is present in the call file and can also be used. **MQ<40.0 "" --filter-name SedT** This filters low mappability regions. This is actually a fairly high value, which means this will aggressively filter low mappability regions. MQ is seen by DeepVariant and used in calling to determine the variant call and confidence, so it will be reflected in the GQ value. However, MQ is not reported into the VCF, so this filter cannot be used for filtering without additional annotation of the VCF. If you want to filter out low mappability regions, I would recommend intersecting with the high mappability BED file from genome in a bottle. **--filter ""SOR > 50.0 "" --filter-name LowSOR** This filter relates to the strand of reads over a variant. Strand bias is something which DeepVariant can see in its inputs and will use this information to determine variant calls and assign confidence to those calls. However, strand bias is not reported a a separate value in the VCF, so this filter cannot be used for filtering without additional annotation of the VCF. I would say that use of this additional filter isn't needed. **--filter ""AN <150 "" --filter-name LowMiss""** As I understand this filter, it quantifies the number of different alleles identified at a position. I suppose this is some measure of the error rate around a position. If I understand the annotation correctly, DeepVariant should never produce value whic",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/642
https://github.com/google/deepvariant/issues/642:1855,integrability,filter,filtering,1855,"ated over the range of 0-60, so you can map them directly onto your desired specificity (e.g. Q10 is 90% confidence in call, Q20 is 99% confidence, Q30 99.9% confidence, and so on). **--filter-name LowQ --filter ""DP <200 "" --filter-name LowD** This field implies to me you are doing panel sequencing or deep exome sequencing, as this is a high value. The sample info should have DP present and it will not differ between GATK and DeepVariant output. You can use the same filter. . **--filter ""DP> 10000 "" --filter-name HigD** This is again the DP field, which is present in the call file and can also be used. **MQ<40.0 "" --filter-name SedT** This filters low mappability regions. This is actually a fairly high value, which means this will aggressively filter low mappability regions. MQ is seen by DeepVariant and used in calling to determine the variant call and confidence, so it will be reflected in the GQ value. However, MQ is not reported into the VCF, so this filter cannot be used for filtering without additional annotation of the VCF. If you want to filter out low mappability regions, I would recommend intersecting with the high mappability BED file from genome in a bottle. **--filter ""SOR > 50.0 "" --filter-name LowSOR** This filter relates to the strand of reads over a variant. Strand bias is something which DeepVariant can see in its inputs and will use this information to determine variant calls and assign confidence to those calls. However, strand bias is not reported a a separate value in the VCF, so this filter cannot be used for filtering without additional annotation of the VCF. I would say that use of this additional filter isn't needed. **--filter ""AN <150 "" --filter-name LowMiss""** As I understand this filter, it quantifies the number of different alleles identified at a position. I suppose this is some measure of the error rate around a position. If I understand the annotation correctly, DeepVariant should never produce value which come remotely close to thi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/642
https://github.com/google/deepvariant/issues/642:1922,integrability,filter,filter,1922,"ur desired specificity (e.g. Q10 is 90% confidence in call, Q20 is 99% confidence, Q30 99.9% confidence, and so on). **--filter-name LowQ --filter ""DP <200 "" --filter-name LowD** This field implies to me you are doing panel sequencing or deep exome sequencing, as this is a high value. The sample info should have DP present and it will not differ between GATK and DeepVariant output. You can use the same filter. . **--filter ""DP> 10000 "" --filter-name HigD** This is again the DP field, which is present in the call file and can also be used. **MQ<40.0 "" --filter-name SedT** This filters low mappability regions. This is actually a fairly high value, which means this will aggressively filter low mappability regions. MQ is seen by DeepVariant and used in calling to determine the variant call and confidence, so it will be reflected in the GQ value. However, MQ is not reported into the VCF, so this filter cannot be used for filtering without additional annotation of the VCF. If you want to filter out low mappability regions, I would recommend intersecting with the high mappability BED file from genome in a bottle. **--filter ""SOR > 50.0 "" --filter-name LowSOR** This filter relates to the strand of reads over a variant. Strand bias is something which DeepVariant can see in its inputs and will use this information to determine variant calls and assign confidence to those calls. However, strand bias is not reported a a separate value in the VCF, so this filter cannot be used for filtering without additional annotation of the VCF. I would say that use of this additional filter isn't needed. **--filter ""AN <150 "" --filter-name LowMiss""** As I understand this filter, it quantifies the number of different alleles identified at a position. I suppose this is some measure of the error rate around a position. If I understand the annotation correctly, DeepVariant should never produce value which come remotely close to this value in the cohort. You can exclude this filter as I don't thi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/642
https://github.com/google/deepvariant/issues/642:2053,integrability,filter,filter,2053,".g. Q10 is 90% confidence in call, Q20 is 99% confidence, Q30 99.9% confidence, and so on). **--filter-name LowQ --filter ""DP <200 "" --filter-name LowD** This field implies to me you are doing panel sequencing or deep exome sequencing, as this is a high value. The sample info should have DP present and it will not differ between GATK and DeepVariant output. You can use the same filter. . **--filter ""DP> 10000 "" --filter-name HigD** This is again the DP field, which is present in the call file and can also be used. **MQ<40.0 "" --filter-name SedT** This filters low mappability regions. This is actually a fairly high value, which means this will aggressively filter low mappability regions. MQ is seen by DeepVariant and used in calling to determine the variant call and confidence, so it will be reflected in the GQ value. However, MQ is not reported into the VCF, so this filter cannot be used for filtering without additional annotation of the VCF. If you want to filter out low mappability regions, I would recommend intersecting with the high mappability BED file from genome in a bottle. **--filter ""SOR > 50.0 "" --filter-name LowSOR** This filter relates to the strand of reads over a variant. Strand bias is something which DeepVariant can see in its inputs and will use this information to determine variant calls and assign confidence to those calls. However, strand bias is not reported a a separate value in the VCF, so this filter cannot be used for filtering without additional annotation of the VCF. I would say that use of this additional filter isn't needed. **--filter ""AN <150 "" --filter-name LowMiss""** As I understand this filter, it quantifies the number of different alleles identified at a position. I suppose this is some measure of the error rate around a position. If I understand the annotation correctly, DeepVariant should never produce value which come remotely close to this value in the cohort. You can exclude this filter as I don't think it would ever trigger.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/642
https://github.com/google/deepvariant/issues/642:2076,integrability,filter,filter-name,2076,".g. Q10 is 90% confidence in call, Q20 is 99% confidence, Q30 99.9% confidence, and so on). **--filter-name LowQ --filter ""DP <200 "" --filter-name LowD** This field implies to me you are doing panel sequencing or deep exome sequencing, as this is a high value. The sample info should have DP present and it will not differ between GATK and DeepVariant output. You can use the same filter. . **--filter ""DP> 10000 "" --filter-name HigD** This is again the DP field, which is present in the call file and can also be used. **MQ<40.0 "" --filter-name SedT** This filters low mappability regions. This is actually a fairly high value, which means this will aggressively filter low mappability regions. MQ is seen by DeepVariant and used in calling to determine the variant call and confidence, so it will be reflected in the GQ value. However, MQ is not reported into the VCF, so this filter cannot be used for filtering without additional annotation of the VCF. If you want to filter out low mappability regions, I would recommend intersecting with the high mappability BED file from genome in a bottle. **--filter ""SOR > 50.0 "" --filter-name LowSOR** This filter relates to the strand of reads over a variant. Strand bias is something which DeepVariant can see in its inputs and will use this information to determine variant calls and assign confidence to those calls. However, strand bias is not reported a a separate value in the VCF, so this filter cannot be used for filtering without additional annotation of the VCF. I would say that use of this additional filter isn't needed. **--filter ""AN <150 "" --filter-name LowMiss""** As I understand this filter, it quantifies the number of different alleles identified at a position. I suppose this is some measure of the error rate around a position. If I understand the annotation correctly, DeepVariant should never produce value which come remotely close to this value in the cohort. You can exclude this filter as I don't think it would ever trigger.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/642
https://github.com/google/deepvariant/issues/642:2102,integrability,filter,filter,2102,".g. Q10 is 90% confidence in call, Q20 is 99% confidence, Q30 99.9% confidence, and so on). **--filter-name LowQ --filter ""DP <200 "" --filter-name LowD** This field implies to me you are doing panel sequencing or deep exome sequencing, as this is a high value. The sample info should have DP present and it will not differ between GATK and DeepVariant output. You can use the same filter. . **--filter ""DP> 10000 "" --filter-name HigD** This is again the DP field, which is present in the call file and can also be used. **MQ<40.0 "" --filter-name SedT** This filters low mappability regions. This is actually a fairly high value, which means this will aggressively filter low mappability regions. MQ is seen by DeepVariant and used in calling to determine the variant call and confidence, so it will be reflected in the GQ value. However, MQ is not reported into the VCF, so this filter cannot be used for filtering without additional annotation of the VCF. If you want to filter out low mappability regions, I would recommend intersecting with the high mappability BED file from genome in a bottle. **--filter ""SOR > 50.0 "" --filter-name LowSOR** This filter relates to the strand of reads over a variant. Strand bias is something which DeepVariant can see in its inputs and will use this information to determine variant calls and assign confidence to those calls. However, strand bias is not reported a a separate value in the VCF, so this filter cannot be used for filtering without additional annotation of the VCF. I would say that use of this additional filter isn't needed. **--filter ""AN <150 "" --filter-name LowMiss""** As I understand this filter, it quantifies the number of different alleles identified at a position. I suppose this is some measure of the error rate around a position. If I understand the annotation correctly, DeepVariant should never produce value which come remotely close to this value in the cohort. You can exclude this filter as I don't think it would ever trigger.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/642
https://github.com/google/deepvariant/issues/642:2392,integrability,filter,filter,2392,".g. Q10 is 90% confidence in call, Q20 is 99% confidence, Q30 99.9% confidence, and so on). **--filter-name LowQ --filter ""DP <200 "" --filter-name LowD** This field implies to me you are doing panel sequencing or deep exome sequencing, as this is a high value. The sample info should have DP present and it will not differ between GATK and DeepVariant output. You can use the same filter. . **--filter ""DP> 10000 "" --filter-name HigD** This is again the DP field, which is present in the call file and can also be used. **MQ<40.0 "" --filter-name SedT** This filters low mappability regions. This is actually a fairly high value, which means this will aggressively filter low mappability regions. MQ is seen by DeepVariant and used in calling to determine the variant call and confidence, so it will be reflected in the GQ value. However, MQ is not reported into the VCF, so this filter cannot be used for filtering without additional annotation of the VCF. If you want to filter out low mappability regions, I would recommend intersecting with the high mappability BED file from genome in a bottle. **--filter ""SOR > 50.0 "" --filter-name LowSOR** This filter relates to the strand of reads over a variant. Strand bias is something which DeepVariant can see in its inputs and will use this information to determine variant calls and assign confidence to those calls. However, strand bias is not reported a a separate value in the VCF, so this filter cannot be used for filtering without additional annotation of the VCF. I would say that use of this additional filter isn't needed. **--filter ""AN <150 "" --filter-name LowMiss""** As I understand this filter, it quantifies the number of different alleles identified at a position. I suppose this is some measure of the error rate around a position. If I understand the annotation correctly, DeepVariant should never produce value which come remotely close to this value in the cohort. You can exclude this filter as I don't think it would ever trigger.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/642
https://github.com/google/deepvariant/issues/642:2418,integrability,filter,filtering,2418,".g. Q10 is 90% confidence in call, Q20 is 99% confidence, Q30 99.9% confidence, and so on). **--filter-name LowQ --filter ""DP <200 "" --filter-name LowD** This field implies to me you are doing panel sequencing or deep exome sequencing, as this is a high value. The sample info should have DP present and it will not differ between GATK and DeepVariant output. You can use the same filter. . **--filter ""DP> 10000 "" --filter-name HigD** This is again the DP field, which is present in the call file and can also be used. **MQ<40.0 "" --filter-name SedT** This filters low mappability regions. This is actually a fairly high value, which means this will aggressively filter low mappability regions. MQ is seen by DeepVariant and used in calling to determine the variant call and confidence, so it will be reflected in the GQ value. However, MQ is not reported into the VCF, so this filter cannot be used for filtering without additional annotation of the VCF. If you want to filter out low mappability regions, I would recommend intersecting with the high mappability BED file from genome in a bottle. **--filter ""SOR > 50.0 "" --filter-name LowSOR** This filter relates to the strand of reads over a variant. Strand bias is something which DeepVariant can see in its inputs and will use this information to determine variant calls and assign confidence to those calls. However, strand bias is not reported a a separate value in the VCF, so this filter cannot be used for filtering without additional annotation of the VCF. I would say that use of this additional filter isn't needed. **--filter ""AN <150 "" --filter-name LowMiss""** As I understand this filter, it quantifies the number of different alleles identified at a position. I suppose this is some measure of the error rate around a position. If I understand the annotation correctly, DeepVariant should never produce value which come remotely close to this value in the cohort. You can exclude this filter as I don't think it would ever trigger.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/642
https://github.com/google/deepvariant/issues/642:2510,integrability,filter,filter,2510,".g. Q10 is 90% confidence in call, Q20 is 99% confidence, Q30 99.9% confidence, and so on). **--filter-name LowQ --filter ""DP <200 "" --filter-name LowD** This field implies to me you are doing panel sequencing or deep exome sequencing, as this is a high value. The sample info should have DP present and it will not differ between GATK and DeepVariant output. You can use the same filter. . **--filter ""DP> 10000 "" --filter-name HigD** This is again the DP field, which is present in the call file and can also be used. **MQ<40.0 "" --filter-name SedT** This filters low mappability regions. This is actually a fairly high value, which means this will aggressively filter low mappability regions. MQ is seen by DeepVariant and used in calling to determine the variant call and confidence, so it will be reflected in the GQ value. However, MQ is not reported into the VCF, so this filter cannot be used for filtering without additional annotation of the VCF. If you want to filter out low mappability regions, I would recommend intersecting with the high mappability BED file from genome in a bottle. **--filter ""SOR > 50.0 "" --filter-name LowSOR** This filter relates to the strand of reads over a variant. Strand bias is something which DeepVariant can see in its inputs and will use this information to determine variant calls and assign confidence to those calls. However, strand bias is not reported a a separate value in the VCF, so this filter cannot be used for filtering without additional annotation of the VCF. I would say that use of this additional filter isn't needed. **--filter ""AN <150 "" --filter-name LowMiss""** As I understand this filter, it quantifies the number of different alleles identified at a position. I suppose this is some measure of the error rate around a position. If I understand the annotation correctly, DeepVariant should never produce value which come remotely close to this value in the cohort. You can exclude this filter as I don't think it would ever trigger.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/642
https://github.com/google/deepvariant/issues/642:2535,integrability,filter,filter,2535,".g. Q10 is 90% confidence in call, Q20 is 99% confidence, Q30 99.9% confidence, and so on). **--filter-name LowQ --filter ""DP <200 "" --filter-name LowD** This field implies to me you are doing panel sequencing or deep exome sequencing, as this is a high value. The sample info should have DP present and it will not differ between GATK and DeepVariant output. You can use the same filter. . **--filter ""DP> 10000 "" --filter-name HigD** This is again the DP field, which is present in the call file and can also be used. **MQ<40.0 "" --filter-name SedT** This filters low mappability regions. This is actually a fairly high value, which means this will aggressively filter low mappability regions. MQ is seen by DeepVariant and used in calling to determine the variant call and confidence, so it will be reflected in the GQ value. However, MQ is not reported into the VCF, so this filter cannot be used for filtering without additional annotation of the VCF. If you want to filter out low mappability regions, I would recommend intersecting with the high mappability BED file from genome in a bottle. **--filter ""SOR > 50.0 "" --filter-name LowSOR** This filter relates to the strand of reads over a variant. Strand bias is something which DeepVariant can see in its inputs and will use this information to determine variant calls and assign confidence to those calls. However, strand bias is not reported a a separate value in the VCF, so this filter cannot be used for filtering without additional annotation of the VCF. I would say that use of this additional filter isn't needed. **--filter ""AN <150 "" --filter-name LowMiss""** As I understand this filter, it quantifies the number of different alleles identified at a position. I suppose this is some measure of the error rate around a position. If I understand the annotation correctly, DeepVariant should never produce value which come remotely close to this value in the cohort. You can exclude this filter as I don't think it would ever trigger.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/642
https://github.com/google/deepvariant/issues/642:2555,integrability,filter,filter-name,2555,".g. Q10 is 90% confidence in call, Q20 is 99% confidence, Q30 99.9% confidence, and so on). **--filter-name LowQ --filter ""DP <200 "" --filter-name LowD** This field implies to me you are doing panel sequencing or deep exome sequencing, as this is a high value. The sample info should have DP present and it will not differ between GATK and DeepVariant output. You can use the same filter. . **--filter ""DP> 10000 "" --filter-name HigD** This is again the DP field, which is present in the call file and can also be used. **MQ<40.0 "" --filter-name SedT** This filters low mappability regions. This is actually a fairly high value, which means this will aggressively filter low mappability regions. MQ is seen by DeepVariant and used in calling to determine the variant call and confidence, so it will be reflected in the GQ value. However, MQ is not reported into the VCF, so this filter cannot be used for filtering without additional annotation of the VCF. If you want to filter out low mappability regions, I would recommend intersecting with the high mappability BED file from genome in a bottle. **--filter ""SOR > 50.0 "" --filter-name LowSOR** This filter relates to the strand of reads over a variant. Strand bias is something which DeepVariant can see in its inputs and will use this information to determine variant calls and assign confidence to those calls. However, strand bias is not reported a a separate value in the VCF, so this filter cannot be used for filtering without additional annotation of the VCF. I would say that use of this additional filter isn't needed. **--filter ""AN <150 "" --filter-name LowMiss""** As I understand this filter, it quantifies the number of different alleles identified at a position. I suppose this is some measure of the error rate around a position. If I understand the annotation correctly, DeepVariant should never produce value which come remotely close to this value in the cohort. You can exclude this filter as I don't think it would ever trigger.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/642
https://github.com/google/deepvariant/issues/642:2599,integrability,filter,filter,2599,".g. Q10 is 90% confidence in call, Q20 is 99% confidence, Q30 99.9% confidence, and so on). **--filter-name LowQ --filter ""DP <200 "" --filter-name LowD** This field implies to me you are doing panel sequencing or deep exome sequencing, as this is a high value. The sample info should have DP present and it will not differ between GATK and DeepVariant output. You can use the same filter. . **--filter ""DP> 10000 "" --filter-name HigD** This is again the DP field, which is present in the call file and can also be used. **MQ<40.0 "" --filter-name SedT** This filters low mappability regions. This is actually a fairly high value, which means this will aggressively filter low mappability regions. MQ is seen by DeepVariant and used in calling to determine the variant call and confidence, so it will be reflected in the GQ value. However, MQ is not reported into the VCF, so this filter cannot be used for filtering without additional annotation of the VCF. If you want to filter out low mappability regions, I would recommend intersecting with the high mappability BED file from genome in a bottle. **--filter ""SOR > 50.0 "" --filter-name LowSOR** This filter relates to the strand of reads over a variant. Strand bias is something which DeepVariant can see in its inputs and will use this information to determine variant calls and assign confidence to those calls. However, strand bias is not reported a a separate value in the VCF, so this filter cannot be used for filtering without additional annotation of the VCF. I would say that use of this additional filter isn't needed. **--filter ""AN <150 "" --filter-name LowMiss""** As I understand this filter, it quantifies the number of different alleles identified at a position. I suppose this is some measure of the error rate around a position. If I understand the annotation correctly, DeepVariant should never produce value which come remotely close to this value in the cohort. You can exclude this filter as I don't think it would ever trigger.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/642
https://github.com/google/deepvariant/issues/642:2904,integrability,filter,filter,2904,".g. Q10 is 90% confidence in call, Q20 is 99% confidence, Q30 99.9% confidence, and so on). **--filter-name LowQ --filter ""DP <200 "" --filter-name LowD** This field implies to me you are doing panel sequencing or deep exome sequencing, as this is a high value. The sample info should have DP present and it will not differ between GATK and DeepVariant output. You can use the same filter. . **--filter ""DP> 10000 "" --filter-name HigD** This is again the DP field, which is present in the call file and can also be used. **MQ<40.0 "" --filter-name SedT** This filters low mappability regions. This is actually a fairly high value, which means this will aggressively filter low mappability regions. MQ is seen by DeepVariant and used in calling to determine the variant call and confidence, so it will be reflected in the GQ value. However, MQ is not reported into the VCF, so this filter cannot be used for filtering without additional annotation of the VCF. If you want to filter out low mappability regions, I would recommend intersecting with the high mappability BED file from genome in a bottle. **--filter ""SOR > 50.0 "" --filter-name LowSOR** This filter relates to the strand of reads over a variant. Strand bias is something which DeepVariant can see in its inputs and will use this information to determine variant calls and assign confidence to those calls. However, strand bias is not reported a a separate value in the VCF, so this filter cannot be used for filtering without additional annotation of the VCF. I would say that use of this additional filter isn't needed. **--filter ""AN <150 "" --filter-name LowMiss""** As I understand this filter, it quantifies the number of different alleles identified at a position. I suppose this is some measure of the error rate around a position. If I understand the annotation correctly, DeepVariant should never produce value which come remotely close to this value in the cohort. You can exclude this filter as I don't think it would ever trigger.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/642
https://github.com/google/deepvariant/issues/642:117,interoperability,specif,specificity,117,"Hi @wangsb111 . For filtering, it remains the case that filtering on the sample GQ field is the best way to increase specificity while losing the smallest number of true variants. . Are you able to run the same postprocess tool you are using on the GLnexus calls? If so, here are some thoughts on your parameters:. **-filter ""QUAL <50.0"" --filter-name LowQ** The QUAL field can be used. DeepVariant is much more conservative and realistic about the QUAL values. Filtering at 50 will filter too many real calls. Figure 1 in our [DeepVariant-GLnexus paper](https://academic.oup.com/bioinformatics/article/36/24/5582/6064144) has a chart of the Predicted and Real qualities for GATK and DeepVariant. Matching the predicted-real accuracy by quality between the two would put the comparable QUAL threshold at 12-17. Note that DeepVariant's GQ values are well-calibrated over the range of 0-60, so you can map them directly onto your desired specificity (e.g. Q10 is 90% confidence in call, Q20 is 99% confidence, Q30 99.9% confidence, and so on). **--filter-name LowQ --filter ""DP <200 "" --filter-name LowD** This field implies to me you are doing panel sequencing or deep exome sequencing, as this is a high value. The sample info should have DP present and it will not differ between GATK and DeepVariant output. You can use the same filter. . **--filter ""DP> 10000 "" --filter-name HigD** This is again the DP field, which is present in the call file and can also be used. **MQ<40.0 "" --filter-name SedT** This filters low mappability regions. This is actually a fairly high value, which means this will aggressively filter low mappability regions. MQ is seen by DeepVariant and used in calling to determine the variant call and confidence, so it will be reflected in the GQ value. However, MQ is not reported into the VCF, so this filter cannot be used for filtering without additional annotation of the VCF. If you want to filter out low mappability regions, I would recommend intersecting with the hi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/642
https://github.com/google/deepvariant/issues/642:936,interoperability,specif,specificity,936,"Hi @wangsb111 . For filtering, it remains the case that filtering on the sample GQ field is the best way to increase specificity while losing the smallest number of true variants. . Are you able to run the same postprocess tool you are using on the GLnexus calls? If so, here are some thoughts on your parameters:. **-filter ""QUAL <50.0"" --filter-name LowQ** The QUAL field can be used. DeepVariant is much more conservative and realistic about the QUAL values. Filtering at 50 will filter too many real calls. Figure 1 in our [DeepVariant-GLnexus paper](https://academic.oup.com/bioinformatics/article/36/24/5582/6064144) has a chart of the Predicted and Real qualities for GATK and DeepVariant. Matching the predicted-real accuracy by quality between the two would put the comparable QUAL threshold at 12-17. Note that DeepVariant's GQ values are well-calibrated over the range of 0-60, so you can map them directly onto your desired specificity (e.g. Q10 is 90% confidence in call, Q20 is 99% confidence, Q30 99.9% confidence, and so on). **--filter-name LowQ --filter ""DP <200 "" --filter-name LowD** This field implies to me you are doing panel sequencing or deep exome sequencing, as this is a high value. The sample info should have DP present and it will not differ between GATK and DeepVariant output. You can use the same filter. . **--filter ""DP> 10000 "" --filter-name HigD** This is again the DP field, which is present in the call file and can also be used. **MQ<40.0 "" --filter-name SedT** This filters low mappability regions. This is actually a fairly high value, which means this will aggressively filter low mappability regions. MQ is seen by DeepVariant and used in calling to determine the variant call and confidence, so it will be reflected in the GQ value. However, MQ is not reported into the VCF, so this filter cannot be used for filtering without additional annotation of the VCF. If you want to filter out low mappability regions, I would recommend intersecting with the hi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/642
https://github.com/google/deepvariant/issues/642:302,modifiability,paramet,parameters,302,"Hi @wangsb111 . For filtering, it remains the case that filtering on the sample GQ field is the best way to increase specificity while losing the smallest number of true variants. . Are you able to run the same postprocess tool you are using on the GLnexus calls? If so, here are some thoughts on your parameters:. **-filter ""QUAL <50.0"" --filter-name LowQ** The QUAL field can be used. DeepVariant is much more conservative and realistic about the QUAL values. Filtering at 50 will filter too many real calls. Figure 1 in our [DeepVariant-GLnexus paper](https://academic.oup.com/bioinformatics/article/36/24/5582/6064144) has a chart of the Predicted and Real qualities for GATK and DeepVariant. Matching the predicted-real accuracy by quality between the two would put the comparable QUAL threshold at 12-17. Note that DeepVariant's GQ values are well-calibrated over the range of 0-60, so you can map them directly onto your desired specificity (e.g. Q10 is 90% confidence in call, Q20 is 99% confidence, Q30 99.9% confidence, and so on). **--filter-name LowQ --filter ""DP <200 "" --filter-name LowD** This field implies to me you are doing panel sequencing or deep exome sequencing, as this is a high value. The sample info should have DP present and it will not differ between GATK and DeepVariant output. You can use the same filter. . **--filter ""DP> 10000 "" --filter-name HigD** This is again the DP field, which is present in the call file and can also be used. **MQ<40.0 "" --filter-name SedT** This filters low mappability regions. This is actually a fairly high value, which means this will aggressively filter low mappability regions. MQ is seen by DeepVariant and used in calling to determine the variant call and confidence, so it will be reflected in the GQ value. However, MQ is not reported into the VCF, so this filter cannot be used for filtering without additional annotation of the VCF. If you want to filter out low mappability regions, I would recommend intersecting with the hi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/642
https://github.com/google/deepvariant/issues/642:2717,performance,error,error,2717,".g. Q10 is 90% confidence in call, Q20 is 99% confidence, Q30 99.9% confidence, and so on). **--filter-name LowQ --filter ""DP <200 "" --filter-name LowD** This field implies to me you are doing panel sequencing or deep exome sequencing, as this is a high value. The sample info should have DP present and it will not differ between GATK and DeepVariant output. You can use the same filter. . **--filter ""DP> 10000 "" --filter-name HigD** This is again the DP field, which is present in the call file and can also be used. **MQ<40.0 "" --filter-name SedT** This filters low mappability regions. This is actually a fairly high value, which means this will aggressively filter low mappability regions. MQ is seen by DeepVariant and used in calling to determine the variant call and confidence, so it will be reflected in the GQ value. However, MQ is not reported into the VCF, so this filter cannot be used for filtering without additional annotation of the VCF. If you want to filter out low mappability regions, I would recommend intersecting with the high mappability BED file from genome in a bottle. **--filter ""SOR > 50.0 "" --filter-name LowSOR** This filter relates to the strand of reads over a variant. Strand bias is something which DeepVariant can see in its inputs and will use this information to determine variant calls and assign confidence to those calls. However, strand bias is not reported a a separate value in the VCF, so this filter cannot be used for filtering without additional annotation of the VCF. I would say that use of this additional filter isn't needed. **--filter ""AN <150 "" --filter-name LowMiss""** As I understand this filter, it quantifies the number of different alleles identified at a position. I suppose this is some measure of the error rate around a position. If I understand the annotation correctly, DeepVariant should never produce value which come remotely close to this value in the cohort. You can exclude this filter as I don't think it would ever trigger.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/642
https://github.com/google/deepvariant/issues/642:642,safety,Predict,Predicted,642,"Hi @wangsb111 . For filtering, it remains the case that filtering on the sample GQ field is the best way to increase specificity while losing the smallest number of true variants. . Are you able to run the same postprocess tool you are using on the GLnexus calls? If so, here are some thoughts on your parameters:. **-filter ""QUAL <50.0"" --filter-name LowQ** The QUAL field can be used. DeepVariant is much more conservative and realistic about the QUAL values. Filtering at 50 will filter too many real calls. Figure 1 in our [DeepVariant-GLnexus paper](https://academic.oup.com/bioinformatics/article/36/24/5582/6064144) has a chart of the Predicted and Real qualities for GATK and DeepVariant. Matching the predicted-real accuracy by quality between the two would put the comparable QUAL threshold at 12-17. Note that DeepVariant's GQ values are well-calibrated over the range of 0-60, so you can map them directly onto your desired specificity (e.g. Q10 is 90% confidence in call, Q20 is 99% confidence, Q30 99.9% confidence, and so on). **--filter-name LowQ --filter ""DP <200 "" --filter-name LowD** This field implies to me you are doing panel sequencing or deep exome sequencing, as this is a high value. The sample info should have DP present and it will not differ between GATK and DeepVariant output. You can use the same filter. . **--filter ""DP> 10000 "" --filter-name HigD** This is again the DP field, which is present in the call file and can also be used. **MQ<40.0 "" --filter-name SedT** This filters low mappability regions. This is actually a fairly high value, which means this will aggressively filter low mappability regions. MQ is seen by DeepVariant and used in calling to determine the variant call and confidence, so it will be reflected in the GQ value. However, MQ is not reported into the VCF, so this filter cannot be used for filtering without additional annotation of the VCF. If you want to filter out low mappability regions, I would recommend intersecting with the hi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/642
https://github.com/google/deepvariant/issues/642:710,safety,predict,predicted-real,710,"Hi @wangsb111 . For filtering, it remains the case that filtering on the sample GQ field is the best way to increase specificity while losing the smallest number of true variants. . Are you able to run the same postprocess tool you are using on the GLnexus calls? If so, here are some thoughts on your parameters:. **-filter ""QUAL <50.0"" --filter-name LowQ** The QUAL field can be used. DeepVariant is much more conservative and realistic about the QUAL values. Filtering at 50 will filter too many real calls. Figure 1 in our [DeepVariant-GLnexus paper](https://academic.oup.com/bioinformatics/article/36/24/5582/6064144) has a chart of the Predicted and Real qualities for GATK and DeepVariant. Matching the predicted-real accuracy by quality between the two would put the comparable QUAL threshold at 12-17. Note that DeepVariant's GQ values are well-calibrated over the range of 0-60, so you can map them directly onto your desired specificity (e.g. Q10 is 90% confidence in call, Q20 is 99% confidence, Q30 99.9% confidence, and so on). **--filter-name LowQ --filter ""DP <200 "" --filter-name LowD** This field implies to me you are doing panel sequencing or deep exome sequencing, as this is a high value. The sample info should have DP present and it will not differ between GATK and DeepVariant output. You can use the same filter. . **--filter ""DP> 10000 "" --filter-name HigD** This is again the DP field, which is present in the call file and can also be used. **MQ<40.0 "" --filter-name SedT** This filters low mappability regions. This is actually a fairly high value, which means this will aggressively filter low mappability regions. MQ is seen by DeepVariant and used in calling to determine the variant call and confidence, so it will be reflected in the GQ value. However, MQ is not reported into the VCF, so this filter cannot be used for filtering without additional annotation of the VCF. If you want to filter out low mappability regions, I would recommend intersecting with the hi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/642
https://github.com/google/deepvariant/issues/642:2214,safety,input,inputs,2214,".g. Q10 is 90% confidence in call, Q20 is 99% confidence, Q30 99.9% confidence, and so on). **--filter-name LowQ --filter ""DP <200 "" --filter-name LowD** This field implies to me you are doing panel sequencing or deep exome sequencing, as this is a high value. The sample info should have DP present and it will not differ between GATK and DeepVariant output. You can use the same filter. . **--filter ""DP> 10000 "" --filter-name HigD** This is again the DP field, which is present in the call file and can also be used. **MQ<40.0 "" --filter-name SedT** This filters low mappability regions. This is actually a fairly high value, which means this will aggressively filter low mappability regions. MQ is seen by DeepVariant and used in calling to determine the variant call and confidence, so it will be reflected in the GQ value. However, MQ is not reported into the VCF, so this filter cannot be used for filtering without additional annotation of the VCF. If you want to filter out low mappability regions, I would recommend intersecting with the high mappability BED file from genome in a bottle. **--filter ""SOR > 50.0 "" --filter-name LowSOR** This filter relates to the strand of reads over a variant. Strand bias is something which DeepVariant can see in its inputs and will use this information to determine variant calls and assign confidence to those calls. However, strand bias is not reported a a separate value in the VCF, so this filter cannot be used for filtering without additional annotation of the VCF. I would say that use of this additional filter isn't needed. **--filter ""AN <150 "" --filter-name LowMiss""** As I understand this filter, it quantifies the number of different alleles identified at a position. I suppose this is some measure of the error rate around a position. If I understand the annotation correctly, DeepVariant should never produce value which come remotely close to this value in the cohort. You can exclude this filter as I don't think it would ever trigger.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/642
https://github.com/google/deepvariant/issues/642:2717,safety,error,error,2717,".g. Q10 is 90% confidence in call, Q20 is 99% confidence, Q30 99.9% confidence, and so on). **--filter-name LowQ --filter ""DP <200 "" --filter-name LowD** This field implies to me you are doing panel sequencing or deep exome sequencing, as this is a high value. The sample info should have DP present and it will not differ between GATK and DeepVariant output. You can use the same filter. . **--filter ""DP> 10000 "" --filter-name HigD** This is again the DP field, which is present in the call file and can also be used. **MQ<40.0 "" --filter-name SedT** This filters low mappability regions. This is actually a fairly high value, which means this will aggressively filter low mappability regions. MQ is seen by DeepVariant and used in calling to determine the variant call and confidence, so it will be reflected in the GQ value. However, MQ is not reported into the VCF, so this filter cannot be used for filtering without additional annotation of the VCF. If you want to filter out low mappability regions, I would recommend intersecting with the high mappability BED file from genome in a bottle. **--filter ""SOR > 50.0 "" --filter-name LowSOR** This filter relates to the strand of reads over a variant. Strand bias is something which DeepVariant can see in its inputs and will use this information to determine variant calls and assign confidence to those calls. However, strand bias is not reported a a separate value in the VCF, so this filter cannot be used for filtering without additional annotation of the VCF. I would say that use of this additional filter isn't needed. **--filter ""AN <150 "" --filter-name LowMiss""** As I understand this filter, it quantifies the number of different alleles identified at a position. I suppose this is some measure of the error rate around a position. If I understand the annotation correctly, DeepVariant should never produce value which come remotely close to this value in the cohort. You can exclude this filter as I don't think it would ever trigger.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/642
https://github.com/google/deepvariant/issues/642:2653,security,ident,identified,2653,".g. Q10 is 90% confidence in call, Q20 is 99% confidence, Q30 99.9% confidence, and so on). **--filter-name LowQ --filter ""DP <200 "" --filter-name LowD** This field implies to me you are doing panel sequencing or deep exome sequencing, as this is a high value. The sample info should have DP present and it will not differ between GATK and DeepVariant output. You can use the same filter. . **--filter ""DP> 10000 "" --filter-name HigD** This is again the DP field, which is present in the call file and can also be used. **MQ<40.0 "" --filter-name SedT** This filters low mappability regions. This is actually a fairly high value, which means this will aggressively filter low mappability regions. MQ is seen by DeepVariant and used in calling to determine the variant call and confidence, so it will be reflected in the GQ value. However, MQ is not reported into the VCF, so this filter cannot be used for filtering without additional annotation of the VCF. If you want to filter out low mappability regions, I would recommend intersecting with the high mappability BED file from genome in a bottle. **--filter ""SOR > 50.0 "" --filter-name LowSOR** This filter relates to the strand of reads over a variant. Strand bias is something which DeepVariant can see in its inputs and will use this information to determine variant calls and assign confidence to those calls. However, strand bias is not reported a a separate value in the VCF, so this filter cannot be used for filtering without additional annotation of the VCF. I would say that use of this additional filter isn't needed. **--filter ""AN <150 "" --filter-name LowMiss""** As I understand this filter, it quantifies the number of different alleles identified at a position. I suppose this is some measure of the error rate around a position. If I understand the annotation correctly, DeepVariant should never produce value which come remotely close to this value in the cohort. You can exclude this filter as I don't think it would ever trigger.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/642
https://github.com/google/deepvariant/issues/642:2583,testability,understand,understand,2583,".g. Q10 is 90% confidence in call, Q20 is 99% confidence, Q30 99.9% confidence, and so on). **--filter-name LowQ --filter ""DP <200 "" --filter-name LowD** This field implies to me you are doing panel sequencing or deep exome sequencing, as this is a high value. The sample info should have DP present and it will not differ between GATK and DeepVariant output. You can use the same filter. . **--filter ""DP> 10000 "" --filter-name HigD** This is again the DP field, which is present in the call file and can also be used. **MQ<40.0 "" --filter-name SedT** This filters low mappability regions. This is actually a fairly high value, which means this will aggressively filter low mappability regions. MQ is seen by DeepVariant and used in calling to determine the variant call and confidence, so it will be reflected in the GQ value. However, MQ is not reported into the VCF, so this filter cannot be used for filtering without additional annotation of the VCF. If you want to filter out low mappability regions, I would recommend intersecting with the high mappability BED file from genome in a bottle. **--filter ""SOR > 50.0 "" --filter-name LowSOR** This filter relates to the strand of reads over a variant. Strand bias is something which DeepVariant can see in its inputs and will use this information to determine variant calls and assign confidence to those calls. However, strand bias is not reported a a separate value in the VCF, so this filter cannot be used for filtering without additional annotation of the VCF. I would say that use of this additional filter isn't needed. **--filter ""AN <150 "" --filter-name LowMiss""** As I understand this filter, it quantifies the number of different alleles identified at a position. I suppose this is some measure of the error rate around a position. If I understand the annotation correctly, DeepVariant should never produce value which come remotely close to this value in the cohort. You can exclude this filter as I don't think it would ever trigger.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/642
https://github.com/google/deepvariant/issues/642:2752,testability,understand,understand,2752,".g. Q10 is 90% confidence in call, Q20 is 99% confidence, Q30 99.9% confidence, and so on). **--filter-name LowQ --filter ""DP <200 "" --filter-name LowD** This field implies to me you are doing panel sequencing or deep exome sequencing, as this is a high value. The sample info should have DP present and it will not differ between GATK and DeepVariant output. You can use the same filter. . **--filter ""DP> 10000 "" --filter-name HigD** This is again the DP field, which is present in the call file and can also be used. **MQ<40.0 "" --filter-name SedT** This filters low mappability regions. This is actually a fairly high value, which means this will aggressively filter low mappability regions. MQ is seen by DeepVariant and used in calling to determine the variant call and confidence, so it will be reflected in the GQ value. However, MQ is not reported into the VCF, so this filter cannot be used for filtering without additional annotation of the VCF. If you want to filter out low mappability regions, I would recommend intersecting with the high mappability BED file from genome in a bottle. **--filter ""SOR > 50.0 "" --filter-name LowSOR** This filter relates to the strand of reads over a variant. Strand bias is something which DeepVariant can see in its inputs and will use this information to determine variant calls and assign confidence to those calls. However, strand bias is not reported a a separate value in the VCF, so this filter cannot be used for filtering without additional annotation of the VCF. I would say that use of this additional filter isn't needed. **--filter ""AN <150 "" --filter-name LowMiss""** As I understand this filter, it quantifies the number of different alleles identified at a position. I suppose this is some measure of the error rate around a position. If I understand the annotation correctly, DeepVariant should never produce value which come remotely close to this value in the cohort. You can exclude this filter as I don't think it would ever trigger.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/642
https://github.com/google/deepvariant/issues/642:223,usability,tool,tool,223,"Hi @wangsb111 . For filtering, it remains the case that filtering on the sample GQ field is the best way to increase specificity while losing the smallest number of true variants. . Are you able to run the same postprocess tool you are using on the GLnexus calls? If so, here are some thoughts on your parameters:. **-filter ""QUAL <50.0"" --filter-name LowQ** The QUAL field can be used. DeepVariant is much more conservative and realistic about the QUAL values. Filtering at 50 will filter too many real calls. Figure 1 in our [DeepVariant-GLnexus paper](https://academic.oup.com/bioinformatics/article/36/24/5582/6064144) has a chart of the Predicted and Real qualities for GATK and DeepVariant. Matching the predicted-real accuracy by quality between the two would put the comparable QUAL threshold at 12-17. Note that DeepVariant's GQ values are well-calibrated over the range of 0-60, so you can map them directly onto your desired specificity (e.g. Q10 is 90% confidence in call, Q20 is 99% confidence, Q30 99.9% confidence, and so on). **--filter-name LowQ --filter ""DP <200 "" --filter-name LowD** This field implies to me you are doing panel sequencing or deep exome sequencing, as this is a high value. The sample info should have DP present and it will not differ between GATK and DeepVariant output. You can use the same filter. . **--filter ""DP> 10000 "" --filter-name HigD** This is again the DP field, which is present in the call file and can also be used. **MQ<40.0 "" --filter-name SedT** This filters low mappability regions. This is actually a fairly high value, which means this will aggressively filter low mappability regions. MQ is seen by DeepVariant and used in calling to determine the variant call and confidence, so it will be reflected in the GQ value. However, MQ is not reported into the VCF, so this filter cannot be used for filtering without additional annotation of the VCF. If you want to filter out low mappability regions, I would recommend intersecting with the hi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/642
https://github.com/google/deepvariant/issues/642:2214,usability,input,inputs,2214,".g. Q10 is 90% confidence in call, Q20 is 99% confidence, Q30 99.9% confidence, and so on). **--filter-name LowQ --filter ""DP <200 "" --filter-name LowD** This field implies to me you are doing panel sequencing or deep exome sequencing, as this is a high value. The sample info should have DP present and it will not differ between GATK and DeepVariant output. You can use the same filter. . **--filter ""DP> 10000 "" --filter-name HigD** This is again the DP field, which is present in the call file and can also be used. **MQ<40.0 "" --filter-name SedT** This filters low mappability regions. This is actually a fairly high value, which means this will aggressively filter low mappability regions. MQ is seen by DeepVariant and used in calling to determine the variant call and confidence, so it will be reflected in the GQ value. However, MQ is not reported into the VCF, so this filter cannot be used for filtering without additional annotation of the VCF. If you want to filter out low mappability regions, I would recommend intersecting with the high mappability BED file from genome in a bottle. **--filter ""SOR > 50.0 "" --filter-name LowSOR** This filter relates to the strand of reads over a variant. Strand bias is something which DeepVariant can see in its inputs and will use this information to determine variant calls and assign confidence to those calls. However, strand bias is not reported a a separate value in the VCF, so this filter cannot be used for filtering without additional annotation of the VCF. I would say that use of this additional filter isn't needed. **--filter ""AN <150 "" --filter-name LowMiss""** As I understand this filter, it quantifies the number of different alleles identified at a position. I suppose this is some measure of the error rate around a position. If I understand the annotation correctly, DeepVariant should never produce value which come remotely close to this value in the cohort. You can exclude this filter as I don't think it would ever trigger.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/642
https://github.com/google/deepvariant/issues/642:2717,usability,error,error,2717,".g. Q10 is 90% confidence in call, Q20 is 99% confidence, Q30 99.9% confidence, and so on). **--filter-name LowQ --filter ""DP <200 "" --filter-name LowD** This field implies to me you are doing panel sequencing or deep exome sequencing, as this is a high value. The sample info should have DP present and it will not differ between GATK and DeepVariant output. You can use the same filter. . **--filter ""DP> 10000 "" --filter-name HigD** This is again the DP field, which is present in the call file and can also be used. **MQ<40.0 "" --filter-name SedT** This filters low mappability regions. This is actually a fairly high value, which means this will aggressively filter low mappability regions. MQ is seen by DeepVariant and used in calling to determine the variant call and confidence, so it will be reflected in the GQ value. However, MQ is not reported into the VCF, so this filter cannot be used for filtering without additional annotation of the VCF. If you want to filter out low mappability regions, I would recommend intersecting with the high mappability BED file from genome in a bottle. **--filter ""SOR > 50.0 "" --filter-name LowSOR** This filter relates to the strand of reads over a variant. Strand bias is something which DeepVariant can see in its inputs and will use this information to determine variant calls and assign confidence to those calls. However, strand bias is not reported a a separate value in the VCF, so this filter cannot be used for filtering without additional annotation of the VCF. I would say that use of this additional filter isn't needed. **--filter ""AN <150 "" --filter-name LowMiss""** As I understand this filter, it quantifies the number of different alleles identified at a position. I suppose this is some measure of the error rate around a position. If I understand the annotation correctly, DeepVariant should never produce value which come remotely close to this value in the cohort. You can exclude this filter as I don't think it would ever trigger.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/642
https://github.com/google/deepvariant/issues/642:2848,usability,close,close,2848,".g. Q10 is 90% confidence in call, Q20 is 99% confidence, Q30 99.9% confidence, and so on). **--filter-name LowQ --filter ""DP <200 "" --filter-name LowD** This field implies to me you are doing panel sequencing or deep exome sequencing, as this is a high value. The sample info should have DP present and it will not differ between GATK and DeepVariant output. You can use the same filter. . **--filter ""DP> 10000 "" --filter-name HigD** This is again the DP field, which is present in the call file and can also be used. **MQ<40.0 "" --filter-name SedT** This filters low mappability regions. This is actually a fairly high value, which means this will aggressively filter low mappability regions. MQ is seen by DeepVariant and used in calling to determine the variant call and confidence, so it will be reflected in the GQ value. However, MQ is not reported into the VCF, so this filter cannot be used for filtering without additional annotation of the VCF. If you want to filter out low mappability regions, I would recommend intersecting with the high mappability BED file from genome in a bottle. **--filter ""SOR > 50.0 "" --filter-name LowSOR** This filter relates to the strand of reads over a variant. Strand bias is something which DeepVariant can see in its inputs and will use this information to determine variant calls and assign confidence to those calls. However, strand bias is not reported a a separate value in the VCF, so this filter cannot be used for filtering without additional annotation of the VCF. I would say that use of this additional filter isn't needed. **--filter ""AN <150 "" --filter-name LowMiss""** As I understand this filter, it quantifies the number of different alleles identified at a position. I suppose this is some measure of the error rate around a position. If I understand the annotation correctly, DeepVariant should never produce value which come remotely close to this value in the cohort. You can exclude this filter as I don't think it would ever trigger.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/642
https://github.com/google/deepvariant/issues/643:745,availability,avail,available,745,"Thanks for the question. Yes that is definitely some kind of bug, but I'm not able to reproduce it. It looks like something wrong with the rendering in vega-lite. How does this report look to you?: https://42basepairs.com/browse/gs/deepvariant/visual_reports/DeepVariant/1.5.0/WGS. This one has proper minus signs when I look at it. I'm asking because the report is effectively a website, so rendering can show up differently on different computers. A few things you could try:. 1. Open the HTML report in a different web browser (Chrome, Firefox, Safari, etc.). 2. Click the (...) in the top right corner of the report and go to ""Open in Vega Editor"". 3. Play with any language settings on the computer or web browser in case it's the alphabet available that is missing the ""-"" character. If none of those solve the issue, then I'm also happy to take a look at your report, if you can share it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/643
https://github.com/google/deepvariant/issues/643:886,interoperability,share,share,886,"Thanks for the question. Yes that is definitely some kind of bug, but I'm not able to reproduce it. It looks like something wrong with the rendering in vega-lite. How does this report look to you?: https://42basepairs.com/browse/gs/deepvariant/visual_reports/DeepVariant/1.5.0/WGS. This one has proper minus signs when I look at it. I'm asking because the report is effectively a website, so rendering can show up differently on different computers. A few things you could try:. 1. Open the HTML report in a different web browser (Chrome, Firefox, Safari, etc.). 2. Click the (...) in the top right corner of the report and go to ""Open in Vega Editor"". 3. Play with any language settings on the computer or web browser in case it's the alphabet available that is missing the ""-"" character. If none of those solve the issue, then I'm also happy to take a look at your report, if you can share it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/643
https://github.com/google/deepvariant/issues/643:167,reliability,doe,does,167,"Thanks for the question. Yes that is definitely some kind of bug, but I'm not able to reproduce it. It looks like something wrong with the rendering in vega-lite. How does this report look to you?: https://42basepairs.com/browse/gs/deepvariant/visual_reports/DeepVariant/1.5.0/WGS. This one has proper minus signs when I look at it. I'm asking because the report is effectively a website, so rendering can show up differently on different computers. A few things you could try:. 1. Open the HTML report in a different web browser (Chrome, Firefox, Safari, etc.). 2. Click the (...) in the top right corner of the report and go to ""Open in Vega Editor"". 3. Play with any language settings on the computer or web browser in case it's the alphabet available that is missing the ""-"" character. If none of those solve the issue, then I'm also happy to take a look at your report, if you can share it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/643
https://github.com/google/deepvariant/issues/643:745,reliability,availab,available,745,"Thanks for the question. Yes that is definitely some kind of bug, but I'm not able to reproduce it. It looks like something wrong with the rendering in vega-lite. How does this report look to you?: https://42basepairs.com/browse/gs/deepvariant/visual_reports/DeepVariant/1.5.0/WGS. This one has proper minus signs when I look at it. I'm asking because the report is effectively a website, so rendering can show up differently on different computers. A few things you could try:. 1. Open the HTML report in a different web browser (Chrome, Firefox, Safari, etc.). 2. Click the (...) in the top right corner of the report and go to ""Open in Vega Editor"". 3. Play with any language settings on the computer or web browser in case it's the alphabet available that is missing the ""-"" character. If none of those solve the issue, then I'm also happy to take a look at your report, if you can share it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/643
https://github.com/google/deepvariant/issues/643:745,safety,avail,available,745,"Thanks for the question. Yes that is definitely some kind of bug, but I'm not able to reproduce it. It looks like something wrong with the rendering in vega-lite. How does this report look to you?: https://42basepairs.com/browse/gs/deepvariant/visual_reports/DeepVariant/1.5.0/WGS. This one has proper minus signs when I look at it. I'm asking because the report is effectively a website, so rendering can show up differently on different computers. A few things you could try:. 1. Open the HTML report in a different web browser (Chrome, Firefox, Safari, etc.). 2. Click the (...) in the top right corner of the report and go to ""Open in Vega Editor"". 3. Play with any language settings on the computer or web browser in case it's the alphabet available that is missing the ""-"" character. If none of those solve the issue, then I'm also happy to take a look at your report, if you can share it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/643
https://github.com/google/deepvariant/issues/643:308,security,sign,signs,308,"Thanks for the question. Yes that is definitely some kind of bug, but I'm not able to reproduce it. It looks like something wrong with the rendering in vega-lite. How does this report look to you?: https://42basepairs.com/browse/gs/deepvariant/visual_reports/DeepVariant/1.5.0/WGS. This one has proper minus signs when I look at it. I'm asking because the report is effectively a website, so rendering can show up differently on different computers. A few things you could try:. 1. Open the HTML report in a different web browser (Chrome, Firefox, Safari, etc.). 2. Click the (...) in the top right corner of the report and go to ""Open in Vega Editor"". 3. Play with any language settings on the computer or web browser in case it's the alphabet available that is missing the ""-"" character. If none of those solve the issue, then I'm also happy to take a look at your report, if you can share it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/643
https://github.com/google/deepvariant/issues/643:745,security,availab,available,745,"Thanks for the question. Yes that is definitely some kind of bug, but I'm not able to reproduce it. It looks like something wrong with the rendering in vega-lite. How does this report look to you?: https://42basepairs.com/browse/gs/deepvariant/visual_reports/DeepVariant/1.5.0/WGS. This one has proper minus signs when I look at it. I'm asking because the report is effectively a website, so rendering can show up differently on different computers. A few things you could try:. 1. Open the HTML report in a different web browser (Chrome, Firefox, Safari, etc.). 2. Click the (...) in the top right corner of the report and go to ""Open in Vega Editor"". 3. Play with any language settings on the computer or web browser in case it's the alphabet available that is missing the ""-"" character. If none of those solve the issue, then I'm also happy to take a look at your report, if you can share it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/643
https://github.com/google/deepvariant/issues/643:366,usability,effectiv,effectively,366,"Thanks for the question. Yes that is definitely some kind of bug, but I'm not able to reproduce it. It looks like something wrong with the rendering in vega-lite. How does this report look to you?: https://42basepairs.com/browse/gs/deepvariant/visual_reports/DeepVariant/1.5.0/WGS. This one has proper minus signs when I look at it. I'm asking because the report is effectively a website, so rendering can show up differently on different computers. A few things you could try:. 1. Open the HTML report in a different web browser (Chrome, Firefox, Safari, etc.). 2. Click the (...) in the top right corner of the report and go to ""Open in Vega Editor"". 3. Play with any language settings on the computer or web browser in case it's the alphabet available that is missing the ""-"" character. If none of those solve the issue, then I'm also happy to take a look at your report, if you can share it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/643
https://github.com/google/deepvariant/issues/643:386,interoperability,specif,specificity,386,"Hi Maria,. Thanks for your kind help and time. Your linked report looks OK and shows minus symbols in chrome or safari (OSX 13.3.1 / M2) while mine are bad and were obtained on two different ubuntu22 servers with the same docker image. By contrast, the same report looks OK on the ubuntu server itself (open via VNC and using Mozilla), which points in the direction of a ubuntu 22 font specificity in my servers as you suggested. I attach here one such report for your review, the data is not confidential and comes from a SRA genome. [WT_REP1.visual_report.html.zip](https://github.com/google/deepvariant/files/11404181/WT_REP1.visual_report.html.zip). best regards",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/643
https://github.com/google/deepvariant/issues/643:41,performance,time,time,41,"Hi Maria,. Thanks for your kind help and time. Your linked report looks OK and shows minus symbols in chrome or safari (OSX 13.3.1 / M2) while mine are bad and were obtained on two different ubuntu22 servers with the same docker image. By contrast, the same report looks OK on the ubuntu server itself (open via VNC and using Mozilla), which points in the direction of a ubuntu 22 font specificity in my servers as you suggested. I attach here one such report for your review, the data is not confidential and comes from a SRA genome. [WT_REP1.visual_report.html.zip](https://github.com/google/deepvariant/files/11404181/WT_REP1.visual_report.html.zip). best regards",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/643
https://github.com/google/deepvariant/issues/643:469,safety,review,review,469,"Hi Maria,. Thanks for your kind help and time. Your linked report looks OK and shows minus symbols in chrome or safari (OSX 13.3.1 / M2) while mine are bad and were obtained on two different ubuntu22 servers with the same docker image. By contrast, the same report looks OK on the ubuntu server itself (open via VNC and using Mozilla), which points in the direction of a ubuntu 22 font specificity in my servers as you suggested. I attach here one such report for your review, the data is not confidential and comes from a SRA genome. [WT_REP1.visual_report.html.zip](https://github.com/google/deepvariant/files/11404181/WT_REP1.visual_report.html.zip). best regards",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/643
https://github.com/google/deepvariant/issues/643:493,security,confidential,confidential,493,"Hi Maria,. Thanks for your kind help and time. Your linked report looks OK and shows minus symbols in chrome or safari (OSX 13.3.1 / M2) while mine are bad and were obtained on two different ubuntu22 servers with the same docker image. By contrast, the same report looks OK on the ubuntu server itself (open via VNC and using Mozilla), which points in the direction of a ubuntu 22 font specificity in my servers as you suggested. I attach here one such report for your review, the data is not confidential and comes from a SRA genome. [WT_REP1.visual_report.html.zip](https://github.com/google/deepvariant/files/11404181/WT_REP1.visual_report.html.zip). best regards",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/643
https://github.com/google/deepvariant/issues/643:469,testability,review,review,469,"Hi Maria,. Thanks for your kind help and time. Your linked report looks OK and shows minus symbols in chrome or safari (OSX 13.3.1 / M2) while mine are bad and were obtained on two different ubuntu22 servers with the same docker image. By contrast, the same report looks OK on the ubuntu server itself (open via VNC and using Mozilla), which points in the direction of a ubuntu 22 font specificity in my servers as you suggested. I attach here one such report for your review, the data is not confidential and comes from a SRA genome. [WT_REP1.visual_report.html.zip](https://github.com/google/deepvariant/files/11404181/WT_REP1.visual_report.html.zip). best regards",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/643
https://github.com/google/deepvariant/issues/643:32,usability,help,help,32,"Hi Maria,. Thanks for your kind help and time. Your linked report looks OK and shows minus symbols in chrome or safari (OSX 13.3.1 / M2) while mine are bad and were obtained on two different ubuntu22 servers with the same docker image. By contrast, the same report looks OK on the ubuntu server itself (open via VNC and using Mozilla), which points in the direction of a ubuntu 22 font specificity in my servers as you suggested. I attach here one such report for your review, the data is not confidential and comes from a SRA genome. [WT_REP1.visual_report.html.zip](https://github.com/google/deepvariant/files/11404181/WT_REP1.visual_report.html.zip). best regards",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/643
https://github.com/google/deepvariant/issues/643:144,interoperability,specif,specify,144,"Okay it looks like it's the text encoding. Looking at the report you attached, I was able to fix it by changing the `vega@5` import (line 9) to specify `charset=""UTF-8""`:. ```. <script type=""text/javascript"" charset=""UTF-8"" src=""https://storage.googleapis.com/deepvariant/lib/vega/vega@5""></script>. ```. Does that solve it on your end too?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/643
https://github.com/google/deepvariant/issues/643:305,reliability,Doe,Does,305,"Okay it looks like it's the text encoding. Looking at the report you attached, I was able to fix it by changing the `vega@5` import (line 9) to specify `charset=""UTF-8""`:. ```. <script type=""text/javascript"" charset=""UTF-8"" src=""https://storage.googleapis.com/deepvariant/lib/vega/vega@5""></script>. ```. Does that solve it on your end too?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/643
https://github.com/google/deepvariant/issues/643:36,reliability,doe,does,36,"And when you make the change above, does that look okay on the Ubuntu 22 server?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/643
https://github.com/google/deepvariant/issues/644:10,usability,tool,tool,10,We made a tool for that called show_examples! Documentation here: https://github.com/google/deepvariant/blob/r1.5/docs/show-examples.md,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/644
https://github.com/google/deepvariant/issues/644:46,usability,Document,Documentation,46,We made a tool for that called show_examples! Documentation here: https://github.com/google/deepvariant/blob/r1.5/docs/show-examples.md,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/644
https://github.com/google/deepvariant/issues/645:35,deployability,depend,dependent,35,"@Dani-kolbe how you filter will be dependent on the type of analysis and how strict you want to be. Can you clarify - are all of these variants listed considered false positives or just the first two (low coverage) sites? `GQ` assesses genotype quality. It will be correlated with depth, but is also impacted by other factors, and as you see the `GQ` can still be high even for some variants with low depth. Can you apply filteres that target low depth or missingness? For example:. * Filter sites with high missing rates (e.g. > 20% of samples with a `./.`). * Filter sites where the average depth per sample is < 5. * Do you expect higher coverage at these sites based on your WES setup? You can also consider filtering for sites that your WES assay is expected to capture.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/645
https://github.com/google/deepvariant/issues/645:20,integrability,filter,filter,20,"@Dani-kolbe how you filter will be dependent on the type of analysis and how strict you want to be. Can you clarify - are all of these variants listed considered false positives or just the first two (low coverage) sites? `GQ` assesses genotype quality. It will be correlated with depth, but is also impacted by other factors, and as you see the `GQ` can still be high even for some variants with low depth. Can you apply filteres that target low depth or missingness? For example:. * Filter sites with high missing rates (e.g. > 20% of samples with a `./.`). * Filter sites where the average depth per sample is < 5. * Do you expect higher coverage at these sites based on your WES setup? You can also consider filtering for sites that your WES assay is expected to capture.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/645
https://github.com/google/deepvariant/issues/645:35,integrability,depend,dependent,35,"@Dani-kolbe how you filter will be dependent on the type of analysis and how strict you want to be. Can you clarify - are all of these variants listed considered false positives or just the first two (low coverage) sites? `GQ` assesses genotype quality. It will be correlated with depth, but is also impacted by other factors, and as you see the `GQ` can still be high even for some variants with low depth. Can you apply filteres that target low depth or missingness? For example:. * Filter sites with high missing rates (e.g. > 20% of samples with a `./.`). * Filter sites where the average depth per sample is < 5. * Do you expect higher coverage at these sites based on your WES setup? You can also consider filtering for sites that your WES assay is expected to capture.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/645
https://github.com/google/deepvariant/issues/645:422,integrability,filter,filteres,422,"@Dani-kolbe how you filter will be dependent on the type of analysis and how strict you want to be. Can you clarify - are all of these variants listed considered false positives or just the first two (low coverage) sites? `GQ` assesses genotype quality. It will be correlated with depth, but is also impacted by other factors, and as you see the `GQ` can still be high even for some variants with low depth. Can you apply filteres that target low depth or missingness? For example:. * Filter sites with high missing rates (e.g. > 20% of samples with a `./.`). * Filter sites where the average depth per sample is < 5. * Do you expect higher coverage at these sites based on your WES setup? You can also consider filtering for sites that your WES assay is expected to capture.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/645
https://github.com/google/deepvariant/issues/645:485,integrability,Filter,Filter,485,"@Dani-kolbe how you filter will be dependent on the type of analysis and how strict you want to be. Can you clarify - are all of these variants listed considered false positives or just the first two (low coverage) sites? `GQ` assesses genotype quality. It will be correlated with depth, but is also impacted by other factors, and as you see the `GQ` can still be high even for some variants with low depth. Can you apply filteres that target low depth or missingness? For example:. * Filter sites with high missing rates (e.g. > 20% of samples with a `./.`). * Filter sites where the average depth per sample is < 5. * Do you expect higher coverage at these sites based on your WES setup? You can also consider filtering for sites that your WES assay is expected to capture.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/645
https://github.com/google/deepvariant/issues/645:562,integrability,Filter,Filter,562,"@Dani-kolbe how you filter will be dependent on the type of analysis and how strict you want to be. Can you clarify - are all of these variants listed considered false positives or just the first two (low coverage) sites? `GQ` assesses genotype quality. It will be correlated with depth, but is also impacted by other factors, and as you see the `GQ` can still be high even for some variants with low depth. Can you apply filteres that target low depth or missingness? For example:. * Filter sites with high missing rates (e.g. > 20% of samples with a `./.`). * Filter sites where the average depth per sample is < 5. * Do you expect higher coverage at these sites based on your WES setup? You can also consider filtering for sites that your WES assay is expected to capture.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/645
https://github.com/google/deepvariant/issues/645:712,integrability,filter,filtering,712,"@Dani-kolbe how you filter will be dependent on the type of analysis and how strict you want to be. Can you clarify - are all of these variants listed considered false positives or just the first two (low coverage) sites? `GQ` assesses genotype quality. It will be correlated with depth, but is also impacted by other factors, and as you see the `GQ` can still be high even for some variants with low depth. Can you apply filteres that target low depth or missingness? For example:. * Filter sites with high missing rates (e.g. > 20% of samples with a `./.`). * Filter sites where the average depth per sample is < 5. * Do you expect higher coverage at these sites based on your WES setup? You can also consider filtering for sites that your WES assay is expected to capture.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/645
https://github.com/google/deepvariant/issues/645:35,modifiability,depend,dependent,35,"@Dani-kolbe how you filter will be dependent on the type of analysis and how strict you want to be. Can you clarify - are all of these variants listed considered false positives or just the first two (low coverage) sites? `GQ` assesses genotype quality. It will be correlated with depth, but is also impacted by other factors, and as you see the `GQ` can still be high even for some variants with low depth. Can you apply filteres that target low depth or missingness? For example:. * Filter sites with high missing rates (e.g. > 20% of samples with a `./.`). * Filter sites where the average depth per sample is < 5. * Do you expect higher coverage at these sites based on your WES setup? You can also consider filtering for sites that your WES assay is expected to capture.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/645
https://github.com/google/deepvariant/issues/645:35,safety,depend,dependent,35,"@Dani-kolbe how you filter will be dependent on the type of analysis and how strict you want to be. Can you clarify - are all of these variants listed considered false positives or just the first two (low coverage) sites? `GQ` assesses genotype quality. It will be correlated with depth, but is also impacted by other factors, and as you see the `GQ` can still be high even for some variants with low depth. Can you apply filteres that target low depth or missingness? For example:. * Filter sites with high missing rates (e.g. > 20% of samples with a `./.`). * Filter sites where the average depth per sample is < 5. * Do you expect higher coverage at these sites based on your WES setup? You can also consider filtering for sites that your WES assay is expected to capture.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/645
https://github.com/google/deepvariant/issues/645:227,security,assess,assesses,227,"@Dani-kolbe how you filter will be dependent on the type of analysis and how strict you want to be. Can you clarify - are all of these variants listed considered false positives or just the first two (low coverage) sites? `GQ` assesses genotype quality. It will be correlated with depth, but is also impacted by other factors, and as you see the `GQ` can still be high even for some variants with low depth. Can you apply filteres that target low depth or missingness? For example:. * Filter sites with high missing rates (e.g. > 20% of samples with a `./.`). * Filter sites where the average depth per sample is < 5. * Do you expect higher coverage at these sites based on your WES setup? You can also consider filtering for sites that your WES assay is expected to capture.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/645
https://github.com/google/deepvariant/issues/645:35,testability,depend,dependent,35,"@Dani-kolbe how you filter will be dependent on the type of analysis and how strict you want to be. Can you clarify - are all of these variants listed considered false positives or just the first two (low coverage) sites? `GQ` assesses genotype quality. It will be correlated with depth, but is also impacted by other factors, and as you see the `GQ` can still be high even for some variants with low depth. Can you apply filteres that target low depth or missingness? For example:. * Filter sites with high missing rates (e.g. > 20% of samples with a `./.`). * Filter sites where the average depth per sample is < 5. * Do you expect higher coverage at these sites based on your WES setup? You can also consider filtering for sites that your WES assay is expected to capture.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/645
https://github.com/google/deepvariant/issues/645:205,testability,coverag,coverage,205,"@Dani-kolbe how you filter will be dependent on the type of analysis and how strict you want to be. Can you clarify - are all of these variants listed considered false positives or just the first two (low coverage) sites? `GQ` assesses genotype quality. It will be correlated with depth, but is also impacted by other factors, and as you see the `GQ` can still be high even for some variants with low depth. Can you apply filteres that target low depth or missingness? For example:. * Filter sites with high missing rates (e.g. > 20% of samples with a `./.`). * Filter sites where the average depth per sample is < 5. * Do you expect higher coverage at these sites based on your WES setup? You can also consider filtering for sites that your WES assay is expected to capture.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/645
https://github.com/google/deepvariant/issues/645:641,testability,coverag,coverage,641,"@Dani-kolbe how you filter will be dependent on the type of analysis and how strict you want to be. Can you clarify - are all of these variants listed considered false positives or just the first two (low coverage) sites? `GQ` assesses genotype quality. It will be correlated with depth, but is also impacted by other factors, and as you see the `GQ` can still be high even for some variants with low depth. Can you apply filteres that target low depth or missingness? For example:. * Filter sites with high missing rates (e.g. > 20% of samples with a `./.`). * Filter sites where the average depth per sample is < 5. * Do you expect higher coverage at these sites based on your WES setup? You can also consider filtering for sites that your WES assay is expected to capture.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/645
https://github.com/google/deepvariant/issues/645:244,integrability,filter,filtering,244,"Hi Daniel! Thank you for your reply. I will have a look and get back here with a response. On a side not, is there:. - a way to add fields such as average depth, allele number and other typical vcf info fields using DV? - and can I then do the filtering you mentioned using GLnexus, or should I do it on the merged vcf using something like vcftools? Sorry if this is very basic... Thank you! Best,. Daniel",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/645
https://github.com/google/deepvariant/issues/645:17,integrability,filter,filtering,17,"@Dani-kolbe this filtering process is beyond the scope of DeepVariant, I'm not sure whether GLnexus will be helpful here. Personally, I would use `bcftools`. I believe something like this would help with filtering sites with high missing rates:. ```. bcftools filter --include ""F_PASS(GT!='mis') > 0.05"". ```. See the bcftools manual and specifically the [expressions](https://samtools.github.io/bcftools/bcftools.html#expressions) section for more details.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/645
https://github.com/google/deepvariant/issues/645:204,integrability,filter,filtering,204,"@Dani-kolbe this filtering process is beyond the scope of DeepVariant, I'm not sure whether GLnexus will be helpful here. Personally, I would use `bcftools`. I believe something like this would help with filtering sites with high missing rates:. ```. bcftools filter --include ""F_PASS(GT!='mis') > 0.05"". ```. See the bcftools manual and specifically the [expressions](https://samtools.github.io/bcftools/bcftools.html#expressions) section for more details.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/645
https://github.com/google/deepvariant/issues/645:260,integrability,filter,filter,260,"@Dani-kolbe this filtering process is beyond the scope of DeepVariant, I'm not sure whether GLnexus will be helpful here. Personally, I would use `bcftools`. I believe something like this would help with filtering sites with high missing rates:. ```. bcftools filter --include ""F_PASS(GT!='mis') > 0.05"". ```. See the bcftools manual and specifically the [expressions](https://samtools.github.io/bcftools/bcftools.html#expressions) section for more details.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/645
https://github.com/google/deepvariant/issues/645:338,interoperability,specif,specifically,338,"@Dani-kolbe this filtering process is beyond the scope of DeepVariant, I'm not sure whether GLnexus will be helpful here. Personally, I would use `bcftools`. I believe something like this would help with filtering sites with high missing rates:. ```. bcftools filter --include ""F_PASS(GT!='mis') > 0.05"". ```. See the bcftools manual and specifically the [expressions](https://samtools.github.io/bcftools/bcftools.html#expressions) section for more details.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/645
https://github.com/google/deepvariant/issues/645:108,usability,help,helpful,108,"@Dani-kolbe this filtering process is beyond the scope of DeepVariant, I'm not sure whether GLnexus will be helpful here. Personally, I would use `bcftools`. I believe something like this would help with filtering sites with high missing rates:. ```. bcftools filter --include ""F_PASS(GT!='mis') > 0.05"". ```. See the bcftools manual and specifically the [expressions](https://samtools.github.io/bcftools/bcftools.html#expressions) section for more details.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/645
https://github.com/google/deepvariant/issues/645:122,usability,Person,Personally,122,"@Dani-kolbe this filtering process is beyond the scope of DeepVariant, I'm not sure whether GLnexus will be helpful here. Personally, I would use `bcftools`. I believe something like this would help with filtering sites with high missing rates:. ```. bcftools filter --include ""F_PASS(GT!='mis') > 0.05"". ```. See the bcftools manual and specifically the [expressions](https://samtools.github.io/bcftools/bcftools.html#expressions) section for more details.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/645
https://github.com/google/deepvariant/issues/645:194,usability,help,help,194,"@Dani-kolbe this filtering process is beyond the scope of DeepVariant, I'm not sure whether GLnexus will be helpful here. Personally, I would use `bcftools`. I believe something like this would help with filtering sites with high missing rates:. ```. bcftools filter --include ""F_PASS(GT!='mis') > 0.05"". ```. See the bcftools manual and specifically the [expressions](https://samtools.github.io/bcftools/bcftools.html#expressions) section for more details.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/645
https://github.com/google/deepvariant/issues/646:37,availability,avail,available,37,@yangyxt how much memory do you have available on this machine? Is it possible to increase the amount of memory?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/646
https://github.com/google/deepvariant/issues/646:18,performance,memor,memory,18,@yangyxt how much memory do you have available on this machine? Is it possible to increase the amount of memory?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/646
https://github.com/google/deepvariant/issues/646:105,performance,memor,memory,105,@yangyxt how much memory do you have available on this machine? Is it possible to increase the amount of memory?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/646
https://github.com/google/deepvariant/issues/646:37,reliability,availab,available,37,@yangyxt how much memory do you have available on this machine? Is it possible to increase the amount of memory?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/646
https://github.com/google/deepvariant/issues/646:37,safety,avail,available,37,@yangyxt how much memory do you have available on this machine? Is it possible to increase the amount of memory?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/646
https://github.com/google/deepvariant/issues/646:37,security,availab,available,37,@yangyxt how much memory do you have available on this machine? Is it possible to increase the amount of memory?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/646
https://github.com/google/deepvariant/issues/646:18,usability,memor,memory,18,@yangyxt how much memory do you have available on this machine? Is it possible to increase the amount of memory?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/646
https://github.com/google/deepvariant/issues/646:105,usability,memor,memory,105,@yangyxt how much memory do you have available on this machine? Is it possible to increase the amount of memory?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/646
https://github.com/google/deepvariant/issues/646:65,availability,cluster,cluster,65,"@danielecook Thanks for the swift response. I ran this on an HPC cluster using the PBS scheduler. I allocated 200 GB for this job, and according to ""qstat"", this job only uses 20GB at maximum. I'm preferred to believe that this issue is not due to the lack of sufficient memory for DeepVariant or DeepTrio. . Here is a screenshot of the finished job status in the qstat table:. ![image](https://github.com/google/deepvariant/assets/40780228/9b7fe09d-fb20-4f8a-9ff9-972e539aa6a1).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/646
https://github.com/google/deepvariant/issues/646:65,deployability,cluster,cluster,65,"@danielecook Thanks for the swift response. I ran this on an HPC cluster using the PBS scheduler. I allocated 200 GB for this job, and according to ""qstat"", this job only uses 20GB at maximum. I'm preferred to believe that this issue is not due to the lack of sufficient memory for DeepVariant or DeepTrio. . Here is a screenshot of the finished job status in the qstat table:. ![image](https://github.com/google/deepvariant/assets/40780228/9b7fe09d-fb20-4f8a-9ff9-972e539aa6a1).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/646
https://github.com/google/deepvariant/issues/646:87,energy efficiency,schedul,scheduler,87,"@danielecook Thanks for the swift response. I ran this on an HPC cluster using the PBS scheduler. I allocated 200 GB for this job, and according to ""qstat"", this job only uses 20GB at maximum. I'm preferred to believe that this issue is not due to the lack of sufficient memory for DeepVariant or DeepTrio. . Here is a screenshot of the finished job status in the qstat table:. ![image](https://github.com/google/deepvariant/assets/40780228/9b7fe09d-fb20-4f8a-9ff9-972e539aa6a1).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/646
https://github.com/google/deepvariant/issues/646:100,energy efficiency,alloc,allocated,100,"@danielecook Thanks for the swift response. I ran this on an HPC cluster using the PBS scheduler. I allocated 200 GB for this job, and according to ""qstat"", this job only uses 20GB at maximum. I'm preferred to believe that this issue is not due to the lack of sufficient memory for DeepVariant or DeepTrio. . Here is a screenshot of the finished job status in the qstat table:. ![image](https://github.com/google/deepvariant/assets/40780228/9b7fe09d-fb20-4f8a-9ff9-972e539aa6a1).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/646
https://github.com/google/deepvariant/issues/646:87,performance,schedul,scheduler,87,"@danielecook Thanks for the swift response. I ran this on an HPC cluster using the PBS scheduler. I allocated 200 GB for this job, and according to ""qstat"", this job only uses 20GB at maximum. I'm preferred to believe that this issue is not due to the lack of sufficient memory for DeepVariant or DeepTrio. . Here is a screenshot of the finished job status in the qstat table:. ![image](https://github.com/google/deepvariant/assets/40780228/9b7fe09d-fb20-4f8a-9ff9-972e539aa6a1).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/646
https://github.com/google/deepvariant/issues/646:271,performance,memor,memory,271,"@danielecook Thanks for the swift response. I ran this on an HPC cluster using the PBS scheduler. I allocated 200 GB for this job, and according to ""qstat"", this job only uses 20GB at maximum. I'm preferred to believe that this issue is not due to the lack of sufficient memory for DeepVariant or DeepTrio. . Here is a screenshot of the finished job status in the qstat table:. ![image](https://github.com/google/deepvariant/assets/40780228/9b7fe09d-fb20-4f8a-9ff9-972e539aa6a1).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/646
https://github.com/google/deepvariant/issues/646:197,usability,prefer,preferred,197,"@danielecook Thanks for the swift response. I ran this on an HPC cluster using the PBS scheduler. I allocated 200 GB for this job, and according to ""qstat"", this job only uses 20GB at maximum. I'm preferred to believe that this issue is not due to the lack of sufficient memory for DeepVariant or DeepTrio. . Here is a screenshot of the finished job status in the qstat table:. ![image](https://github.com/google/deepvariant/assets/40780228/9b7fe09d-fb20-4f8a-9ff9-972e539aa6a1).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/646
https://github.com/google/deepvariant/issues/646:271,usability,memor,memory,271,"@danielecook Thanks for the swift response. I ran this on an HPC cluster using the PBS scheduler. I allocated 200 GB for this job, and according to ""qstat"", this job only uses 20GB at maximum. I'm preferred to believe that this issue is not due to the lack of sufficient memory for DeepVariant or DeepTrio. . Here is a screenshot of the finished job status in the qstat table:. ![image](https://github.com/google/deepvariant/assets/40780228/9b7fe09d-fb20-4f8a-9ff9-972e539aa6a1).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/646
https://github.com/google/deepvariant/issues/646:350,usability,statu,status,350,"@danielecook Thanks for the swift response. I ran this on an HPC cluster using the PBS scheduler. I allocated 200 GB for this job, and according to ""qstat"", this job only uses 20GB at maximum. I'm preferred to believe that this issue is not due to the lack of sufficient memory for DeepVariant or DeepTrio. . Here is a screenshot of the finished job status in the qstat table:. ![image](https://github.com/google/deepvariant/assets/40780228/9b7fe09d-fb20-4f8a-9ff9-972e539aa6a1).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/646
https://github.com/google/deepvariant/issues/646:199,availability,error,error,199,"Hi @yangyxt ,. In your log, I saw this line which is a bit strange:. ```. Line 770: In function call_deeptrio_per_pair: Tue May 9 05:42:52 HKT 2023: Failed on running singularity DeepTrio. Quit with error. ```. I tried to search for that function in our r1.4 codebase (which is the version you mentioned you're using):. ```. git clone https://github.com/google/deepvariant.git. cd deepvariant/. git checkout r1.4. find . -type f -exec grep -H call_deeptrio_per_pair {} \;. ```. And I can't find that function in our codebase. Can you confirm that you're running our version, or maybe you're running a modified version from somewhere else?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/646
https://github.com/google/deepvariant/issues/646:23,deployability,log,log,23,"Hi @yangyxt ,. In your log, I saw this line which is a bit strange:. ```. Line 770: In function call_deeptrio_per_pair: Tue May 9 05:42:52 HKT 2023: Failed on running singularity DeepTrio. Quit with error. ```. I tried to search for that function in our r1.4 codebase (which is the version you mentioned you're using):. ```. git clone https://github.com/google/deepvariant.git. cd deepvariant/. git checkout r1.4. find . -type f -exec grep -H call_deeptrio_per_pair {} \;. ```. And I can't find that function in our codebase. Can you confirm that you're running our version, or maybe you're running a modified version from somewhere else?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/646
https://github.com/google/deepvariant/issues/646:149,deployability,Fail,Failed,149,"Hi @yangyxt ,. In your log, I saw this line which is a bit strange:. ```. Line 770: In function call_deeptrio_per_pair: Tue May 9 05:42:52 HKT 2023: Failed on running singularity DeepTrio. Quit with error. ```. I tried to search for that function in our r1.4 codebase (which is the version you mentioned you're using):. ```. git clone https://github.com/google/deepvariant.git. cd deepvariant/. git checkout r1.4. find . -type f -exec grep -H call_deeptrio_per_pair {} \;. ```. And I can't find that function in our codebase. Can you confirm that you're running our version, or maybe you're running a modified version from somewhere else?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/646
https://github.com/google/deepvariant/issues/646:282,deployability,version,version,282,"Hi @yangyxt ,. In your log, I saw this line which is a bit strange:. ```. Line 770: In function call_deeptrio_per_pair: Tue May 9 05:42:52 HKT 2023: Failed on running singularity DeepTrio. Quit with error. ```. I tried to search for that function in our r1.4 codebase (which is the version you mentioned you're using):. ```. git clone https://github.com/google/deepvariant.git. cd deepvariant/. git checkout r1.4. find . -type f -exec grep -H call_deeptrio_per_pair {} \;. ```. And I can't find that function in our codebase. Can you confirm that you're running our version, or maybe you're running a modified version from somewhere else?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/646
https://github.com/google/deepvariant/issues/646:566,deployability,version,version,566,"Hi @yangyxt ,. In your log, I saw this line which is a bit strange:. ```. Line 770: In function call_deeptrio_per_pair: Tue May 9 05:42:52 HKT 2023: Failed on running singularity DeepTrio. Quit with error. ```. I tried to search for that function in our r1.4 codebase (which is the version you mentioned you're using):. ```. git clone https://github.com/google/deepvariant.git. cd deepvariant/. git checkout r1.4. find . -type f -exec grep -H call_deeptrio_per_pair {} \;. ```. And I can't find that function in our codebase. Can you confirm that you're running our version, or maybe you're running a modified version from somewhere else?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/646
https://github.com/google/deepvariant/issues/646:610,deployability,version,version,610,"Hi @yangyxt ,. In your log, I saw this line which is a bit strange:. ```. Line 770: In function call_deeptrio_per_pair: Tue May 9 05:42:52 HKT 2023: Failed on running singularity DeepTrio. Quit with error. ```. I tried to search for that function in our r1.4 codebase (which is the version you mentioned you're using):. ```. git clone https://github.com/google/deepvariant.git. cd deepvariant/. git checkout r1.4. find . -type f -exec grep -H call_deeptrio_per_pair {} \;. ```. And I can't find that function in our codebase. Can you confirm that you're running our version, or maybe you're running a modified version from somewhere else?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/646
https://github.com/google/deepvariant/issues/646:282,integrability,version,version,282,"Hi @yangyxt ,. In your log, I saw this line which is a bit strange:. ```. Line 770: In function call_deeptrio_per_pair: Tue May 9 05:42:52 HKT 2023: Failed on running singularity DeepTrio. Quit with error. ```. I tried to search for that function in our r1.4 codebase (which is the version you mentioned you're using):. ```. git clone https://github.com/google/deepvariant.git. cd deepvariant/. git checkout r1.4. find . -type f -exec grep -H call_deeptrio_per_pair {} \;. ```. And I can't find that function in our codebase. Can you confirm that you're running our version, or maybe you're running a modified version from somewhere else?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/646
https://github.com/google/deepvariant/issues/646:566,integrability,version,version,566,"Hi @yangyxt ,. In your log, I saw this line which is a bit strange:. ```. Line 770: In function call_deeptrio_per_pair: Tue May 9 05:42:52 HKT 2023: Failed on running singularity DeepTrio. Quit with error. ```. I tried to search for that function in our r1.4 codebase (which is the version you mentioned you're using):. ```. git clone https://github.com/google/deepvariant.git. cd deepvariant/. git checkout r1.4. find . -type f -exec grep -H call_deeptrio_per_pair {} \;. ```. And I can't find that function in our codebase. Can you confirm that you're running our version, or maybe you're running a modified version from somewhere else?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/646
https://github.com/google/deepvariant/issues/646:610,integrability,version,version,610,"Hi @yangyxt ,. In your log, I saw this line which is a bit strange:. ```. Line 770: In function call_deeptrio_per_pair: Tue May 9 05:42:52 HKT 2023: Failed on running singularity DeepTrio. Quit with error. ```. I tried to search for that function in our r1.4 codebase (which is the version you mentioned you're using):. ```. git clone https://github.com/google/deepvariant.git. cd deepvariant/. git checkout r1.4. find . -type f -exec grep -H call_deeptrio_per_pair {} \;. ```. And I can't find that function in our codebase. Can you confirm that you're running our version, or maybe you're running a modified version from somewhere else?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/646
https://github.com/google/deepvariant/issues/646:282,modifiability,version,version,282,"Hi @yangyxt ,. In your log, I saw this line which is a bit strange:. ```. Line 770: In function call_deeptrio_per_pair: Tue May 9 05:42:52 HKT 2023: Failed on running singularity DeepTrio. Quit with error. ```. I tried to search for that function in our r1.4 codebase (which is the version you mentioned you're using):. ```. git clone https://github.com/google/deepvariant.git. cd deepvariant/. git checkout r1.4. find . -type f -exec grep -H call_deeptrio_per_pair {} \;. ```. And I can't find that function in our codebase. Can you confirm that you're running our version, or maybe you're running a modified version from somewhere else?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/646
https://github.com/google/deepvariant/issues/646:566,modifiability,version,version,566,"Hi @yangyxt ,. In your log, I saw this line which is a bit strange:. ```. Line 770: In function call_deeptrio_per_pair: Tue May 9 05:42:52 HKT 2023: Failed on running singularity DeepTrio. Quit with error. ```. I tried to search for that function in our r1.4 codebase (which is the version you mentioned you're using):. ```. git clone https://github.com/google/deepvariant.git. cd deepvariant/. git checkout r1.4. find . -type f -exec grep -H call_deeptrio_per_pair {} \;. ```. And I can't find that function in our codebase. Can you confirm that you're running our version, or maybe you're running a modified version from somewhere else?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/646
https://github.com/google/deepvariant/issues/646:610,modifiability,version,version,610,"Hi @yangyxt ,. In your log, I saw this line which is a bit strange:. ```. Line 770: In function call_deeptrio_per_pair: Tue May 9 05:42:52 HKT 2023: Failed on running singularity DeepTrio. Quit with error. ```. I tried to search for that function in our r1.4 codebase (which is the version you mentioned you're using):. ```. git clone https://github.com/google/deepvariant.git. cd deepvariant/. git checkout r1.4. find . -type f -exec grep -H call_deeptrio_per_pair {} \;. ```. And I can't find that function in our codebase. Can you confirm that you're running our version, or maybe you're running a modified version from somewhere else?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/646
https://github.com/google/deepvariant/issues/646:199,performance,error,error,199,"Hi @yangyxt ,. In your log, I saw this line which is a bit strange:. ```. Line 770: In function call_deeptrio_per_pair: Tue May 9 05:42:52 HKT 2023: Failed on running singularity DeepTrio. Quit with error. ```. I tried to search for that function in our r1.4 codebase (which is the version you mentioned you're using):. ```. git clone https://github.com/google/deepvariant.git. cd deepvariant/. git checkout r1.4. find . -type f -exec grep -H call_deeptrio_per_pair {} \;. ```. And I can't find that function in our codebase. Can you confirm that you're running our version, or maybe you're running a modified version from somewhere else?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/646
https://github.com/google/deepvariant/issues/646:149,reliability,Fail,Failed,149,"Hi @yangyxt ,. In your log, I saw this line which is a bit strange:. ```. Line 770: In function call_deeptrio_per_pair: Tue May 9 05:42:52 HKT 2023: Failed on running singularity DeepTrio. Quit with error. ```. I tried to search for that function in our r1.4 codebase (which is the version you mentioned you're using):. ```. git clone https://github.com/google/deepvariant.git. cd deepvariant/. git checkout r1.4. find . -type f -exec grep -H call_deeptrio_per_pair {} \;. ```. And I can't find that function in our codebase. Can you confirm that you're running our version, or maybe you're running a modified version from somewhere else?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/646
https://github.com/google/deepvariant/issues/646:23,safety,log,log,23,"Hi @yangyxt ,. In your log, I saw this line which is a bit strange:. ```. Line 770: In function call_deeptrio_per_pair: Tue May 9 05:42:52 HKT 2023: Failed on running singularity DeepTrio. Quit with error. ```. I tried to search for that function in our r1.4 codebase (which is the version you mentioned you're using):. ```. git clone https://github.com/google/deepvariant.git. cd deepvariant/. git checkout r1.4. find . -type f -exec grep -H call_deeptrio_per_pair {} \;. ```. And I can't find that function in our codebase. Can you confirm that you're running our version, or maybe you're running a modified version from somewhere else?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/646
https://github.com/google/deepvariant/issues/646:199,safety,error,error,199,"Hi @yangyxt ,. In your log, I saw this line which is a bit strange:. ```. Line 770: In function call_deeptrio_per_pair: Tue May 9 05:42:52 HKT 2023: Failed on running singularity DeepTrio. Quit with error. ```. I tried to search for that function in our r1.4 codebase (which is the version you mentioned you're using):. ```. git clone https://github.com/google/deepvariant.git. cd deepvariant/. git checkout r1.4. find . -type f -exec grep -H call_deeptrio_per_pair {} \;. ```. And I can't find that function in our codebase. Can you confirm that you're running our version, or maybe you're running a modified version from somewhere else?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/646
https://github.com/google/deepvariant/issues/646:23,security,log,log,23,"Hi @yangyxt ,. In your log, I saw this line which is a bit strange:. ```. Line 770: In function call_deeptrio_per_pair: Tue May 9 05:42:52 HKT 2023: Failed on running singularity DeepTrio. Quit with error. ```. I tried to search for that function in our r1.4 codebase (which is the version you mentioned you're using):. ```. git clone https://github.com/google/deepvariant.git. cd deepvariant/. git checkout r1.4. find . -type f -exec grep -H call_deeptrio_per_pair {} \;. ```. And I can't find that function in our codebase. Can you confirm that you're running our version, or maybe you're running a modified version from somewhere else?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/646
https://github.com/google/deepvariant/issues/646:601,security,modif,modified,601,"Hi @yangyxt ,. In your log, I saw this line which is a bit strange:. ```. Line 770: In function call_deeptrio_per_pair: Tue May 9 05:42:52 HKT 2023: Failed on running singularity DeepTrio. Quit with error. ```. I tried to search for that function in our r1.4 codebase (which is the version you mentioned you're using):. ```. git clone https://github.com/google/deepvariant.git. cd deepvariant/. git checkout r1.4. find . -type f -exec grep -H call_deeptrio_per_pair {} \;. ```. And I can't find that function in our codebase. Can you confirm that you're running our version, or maybe you're running a modified version from somewhere else?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/646
https://github.com/google/deepvariant/issues/646:23,testability,log,log,23,"Hi @yangyxt ,. In your log, I saw this line which is a bit strange:. ```. Line 770: In function call_deeptrio_per_pair: Tue May 9 05:42:52 HKT 2023: Failed on running singularity DeepTrio. Quit with error. ```. I tried to search for that function in our r1.4 codebase (which is the version you mentioned you're using):. ```. git clone https://github.com/google/deepvariant.git. cd deepvariant/. git checkout r1.4. find . -type f -exec grep -H call_deeptrio_per_pair {} \;. ```. And I can't find that function in our codebase. Can you confirm that you're running our version, or maybe you're running a modified version from somewhere else?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/646
https://github.com/google/deepvariant/issues/646:199,usability,error,error,199,"Hi @yangyxt ,. In your log, I saw this line which is a bit strange:. ```. Line 770: In function call_deeptrio_per_pair: Tue May 9 05:42:52 HKT 2023: Failed on running singularity DeepTrio. Quit with error. ```. I tried to search for that function in our r1.4 codebase (which is the version you mentioned you're using):. ```. git clone https://github.com/google/deepvariant.git. cd deepvariant/. git checkout r1.4. find . -type f -exec grep -H call_deeptrio_per_pair {} \;. ```. And I can't find that function in our codebase. Can you confirm that you're running our version, or maybe you're running a modified version from somewhere else?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/646
https://github.com/google/deepvariant/issues/646:534,usability,confirm,confirm,534,"Hi @yangyxt ,. In your log, I saw this line which is a bit strange:. ```. Line 770: In function call_deeptrio_per_pair: Tue May 9 05:42:52 HKT 2023: Failed on running singularity DeepTrio. Quit with error. ```. I tried to search for that function in our r1.4 codebase (which is the version you mentioned you're using):. ```. git clone https://github.com/google/deepvariant.git. cd deepvariant/. git checkout r1.4. find . -type f -exec grep -H call_deeptrio_per_pair {} \;. ```. And I can't find that function in our codebase. Can you confirm that you're running our version, or maybe you're running a modified version from somewhere else?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/646
https://github.com/google/deepvariant/issues/646:19,deployability,log,log,19,"hi @pichuan , this log line is generated from my bash function wrapper running singularity version of DeepTrio. I confirm that I am using the official docker image (converted to SIF image with Singularity) .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/646
https://github.com/google/deepvariant/issues/646:91,deployability,version,version,91,"hi @pichuan , this log line is generated from my bash function wrapper running singularity version of DeepTrio. I confirm that I am using the official docker image (converted to SIF image with Singularity) .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/646
https://github.com/google/deepvariant/issues/646:63,integrability,wrap,wrapper,63,"hi @pichuan , this log line is generated from my bash function wrapper running singularity version of DeepTrio. I confirm that I am using the official docker image (converted to SIF image with Singularity) .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/646
https://github.com/google/deepvariant/issues/646:91,integrability,version,version,91,"hi @pichuan , this log line is generated from my bash function wrapper running singularity version of DeepTrio. I confirm that I am using the official docker image (converted to SIF image with Singularity) .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/646
https://github.com/google/deepvariant/issues/646:63,interoperability,wrapper,wrapper,63,"hi @pichuan , this log line is generated from my bash function wrapper running singularity version of DeepTrio. I confirm that I am using the official docker image (converted to SIF image with Singularity) .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/646
https://github.com/google/deepvariant/issues/646:91,modifiability,version,version,91,"hi @pichuan , this log line is generated from my bash function wrapper running singularity version of DeepTrio. I confirm that I am using the official docker image (converted to SIF image with Singularity) .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/646
https://github.com/google/deepvariant/issues/646:19,safety,log,log,19,"hi @pichuan , this log line is generated from my bash function wrapper running singularity version of DeepTrio. I confirm that I am using the official docker image (converted to SIF image with Singularity) .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/646
https://github.com/google/deepvariant/issues/646:19,security,log,log,19,"hi @pichuan , this log line is generated from my bash function wrapper running singularity version of DeepTrio. I confirm that I am using the official docker image (converted to SIF image with Singularity) .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/646
https://github.com/google/deepvariant/issues/646:19,testability,log,log,19,"hi @pichuan , this log line is generated from my bash function wrapper running singularity version of DeepTrio. I confirm that I am using the official docker image (converted to SIF image with Singularity) .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/646
https://github.com/google/deepvariant/issues/646:114,usability,confirm,confirm,114,"hi @pichuan , this log line is generated from my bash function wrapper running singularity version of DeepTrio. I confirm that I am using the official docker image (converted to SIF image with Singularity) .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/646
https://github.com/google/deepvariant/issues/646:8,interoperability,share,share,8,Can you share the command of how you convert to SIF? Thanks.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/646
https://github.com/google/deepvariant/issues/646:18,usability,command,command,18,Can you share the command of how you convert to SIF? Thanks.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/646
https://github.com/google/deepvariant/issues/646:984,availability,error,error,984,"Actually I have a suggestion:. Can you directly run the failed command. `./opt/deepvariant/bin/deeptrio/make_examples --mode calling --ref /paedyl01/disk1/yangyxt/indexed_genome/ucsc.hg19.fasta --reads_parent1 /paedyl01/disk1/yangyxt/wesplus/50_samples_20220304/aligned_results/A210126.deduped.bam --reads /paedyl01/disk1/yangyxt/wesplus/50_samples_20220304/aligned_results/A210124.deduped.bam --examples /paedyl01/disk1/yangyxt/test_tmp/singularity_inter_aRMiURV3spzCOiBE1ojvo_fj2J05oPk1/make_examples.tfrecord@6.gz --sample_name A210124 --sample_name_parent1 A210126 --channels insert_size --gvcf /paedyl01/disk1/yangyxt/test_tmp/singularity_inter_aRMiURV3spzCOiBE1ojvo_fj2J05oPk1/gvcf.tfrecord@6.gz --normalize_reads --pileup_image_height_child 100 --pileup_image_height_parent 100 --regions /paedyl01/disk1/yangyxt/indexed_genome/hg19/ucsc.hg19.no_dad.bed --task 0`. Without using the one-step run_deeptrio. I now remember that the way I wrote that script can make the underlying error unclear, and as a result hard to debug when an error occurs. If you directly run the failed job, you should get more helpful error message. Sorry for the inconvenience. I believe I improved that for run_deepvariant.py a while ago but haven't done the same for run_deeptrio.py.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/646
https://github.com/google/deepvariant/issues/646:1037,availability,error,error,1037,"Actually I have a suggestion:. Can you directly run the failed command. `./opt/deepvariant/bin/deeptrio/make_examples --mode calling --ref /paedyl01/disk1/yangyxt/indexed_genome/ucsc.hg19.fasta --reads_parent1 /paedyl01/disk1/yangyxt/wesplus/50_samples_20220304/aligned_results/A210126.deduped.bam --reads /paedyl01/disk1/yangyxt/wesplus/50_samples_20220304/aligned_results/A210124.deduped.bam --examples /paedyl01/disk1/yangyxt/test_tmp/singularity_inter_aRMiURV3spzCOiBE1ojvo_fj2J05oPk1/make_examples.tfrecord@6.gz --sample_name A210124 --sample_name_parent1 A210126 --channels insert_size --gvcf /paedyl01/disk1/yangyxt/test_tmp/singularity_inter_aRMiURV3spzCOiBE1ojvo_fj2J05oPk1/gvcf.tfrecord@6.gz --normalize_reads --pileup_image_height_child 100 --pileup_image_height_parent 100 --regions /paedyl01/disk1/yangyxt/indexed_genome/hg19/ucsc.hg19.no_dad.bed --task 0`. Without using the one-step run_deeptrio. I now remember that the way I wrote that script can make the underlying error unclear, and as a result hard to debug when an error occurs. If you directly run the failed job, you should get more helpful error message. Sorry for the inconvenience. I believe I improved that for run_deepvariant.py a while ago but haven't done the same for run_deeptrio.py.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/646
https://github.com/google/deepvariant/issues/646:1115,availability,error,error,1115,"Actually I have a suggestion:. Can you directly run the failed command. `./opt/deepvariant/bin/deeptrio/make_examples --mode calling --ref /paedyl01/disk1/yangyxt/indexed_genome/ucsc.hg19.fasta --reads_parent1 /paedyl01/disk1/yangyxt/wesplus/50_samples_20220304/aligned_results/A210126.deduped.bam --reads /paedyl01/disk1/yangyxt/wesplus/50_samples_20220304/aligned_results/A210124.deduped.bam --examples /paedyl01/disk1/yangyxt/test_tmp/singularity_inter_aRMiURV3spzCOiBE1ojvo_fj2J05oPk1/make_examples.tfrecord@6.gz --sample_name A210124 --sample_name_parent1 A210126 --channels insert_size --gvcf /paedyl01/disk1/yangyxt/test_tmp/singularity_inter_aRMiURV3spzCOiBE1ojvo_fj2J05oPk1/gvcf.tfrecord@6.gz --normalize_reads --pileup_image_height_child 100 --pileup_image_height_parent 100 --regions /paedyl01/disk1/yangyxt/indexed_genome/hg19/ucsc.hg19.no_dad.bed --task 0`. Without using the one-step run_deeptrio. I now remember that the way I wrote that script can make the underlying error unclear, and as a result hard to debug when an error occurs. If you directly run the failed job, you should get more helpful error message. Sorry for the inconvenience. I believe I improved that for run_deepvariant.py a while ago but haven't done the same for run_deeptrio.py.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/646
https://github.com/google/deepvariant/issues/646:56,deployability,fail,failed,56,"Actually I have a suggestion:. Can you directly run the failed command. `./opt/deepvariant/bin/deeptrio/make_examples --mode calling --ref /paedyl01/disk1/yangyxt/indexed_genome/ucsc.hg19.fasta --reads_parent1 /paedyl01/disk1/yangyxt/wesplus/50_samples_20220304/aligned_results/A210126.deduped.bam --reads /paedyl01/disk1/yangyxt/wesplus/50_samples_20220304/aligned_results/A210124.deduped.bam --examples /paedyl01/disk1/yangyxt/test_tmp/singularity_inter_aRMiURV3spzCOiBE1ojvo_fj2J05oPk1/make_examples.tfrecord@6.gz --sample_name A210124 --sample_name_parent1 A210126 --channels insert_size --gvcf /paedyl01/disk1/yangyxt/test_tmp/singularity_inter_aRMiURV3spzCOiBE1ojvo_fj2J05oPk1/gvcf.tfrecord@6.gz --normalize_reads --pileup_image_height_child 100 --pileup_image_height_parent 100 --regions /paedyl01/disk1/yangyxt/indexed_genome/hg19/ucsc.hg19.no_dad.bed --task 0`. Without using the one-step run_deeptrio. I now remember that the way I wrote that script can make the underlying error unclear, and as a result hard to debug when an error occurs. If you directly run the failed job, you should get more helpful error message. Sorry for the inconvenience. I believe I improved that for run_deepvariant.py a while ago but haven't done the same for run_deeptrio.py.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/646
https://github.com/google/deepvariant/issues/646:1075,deployability,fail,failed,1075,"Actually I have a suggestion:. Can you directly run the failed command. `./opt/deepvariant/bin/deeptrio/make_examples --mode calling --ref /paedyl01/disk1/yangyxt/indexed_genome/ucsc.hg19.fasta --reads_parent1 /paedyl01/disk1/yangyxt/wesplus/50_samples_20220304/aligned_results/A210126.deduped.bam --reads /paedyl01/disk1/yangyxt/wesplus/50_samples_20220304/aligned_results/A210124.deduped.bam --examples /paedyl01/disk1/yangyxt/test_tmp/singularity_inter_aRMiURV3spzCOiBE1ojvo_fj2J05oPk1/make_examples.tfrecord@6.gz --sample_name A210124 --sample_name_parent1 A210126 --channels insert_size --gvcf /paedyl01/disk1/yangyxt/test_tmp/singularity_inter_aRMiURV3spzCOiBE1ojvo_fj2J05oPk1/gvcf.tfrecord@6.gz --normalize_reads --pileup_image_height_child 100 --pileup_image_height_parent 100 --regions /paedyl01/disk1/yangyxt/indexed_genome/hg19/ucsc.hg19.no_dad.bed --task 0`. Without using the one-step run_deeptrio. I now remember that the way I wrote that script can make the underlying error unclear, and as a result hard to debug when an error occurs. If you directly run the failed job, you should get more helpful error message. Sorry for the inconvenience. I believe I improved that for run_deepvariant.py a while ago but haven't done the same for run_deeptrio.py.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/646
https://github.com/google/deepvariant/issues/646:1121,integrability,messag,message,1121,"Actually I have a suggestion:. Can you directly run the failed command. `./opt/deepvariant/bin/deeptrio/make_examples --mode calling --ref /paedyl01/disk1/yangyxt/indexed_genome/ucsc.hg19.fasta --reads_parent1 /paedyl01/disk1/yangyxt/wesplus/50_samples_20220304/aligned_results/A210126.deduped.bam --reads /paedyl01/disk1/yangyxt/wesplus/50_samples_20220304/aligned_results/A210124.deduped.bam --examples /paedyl01/disk1/yangyxt/test_tmp/singularity_inter_aRMiURV3spzCOiBE1ojvo_fj2J05oPk1/make_examples.tfrecord@6.gz --sample_name A210124 --sample_name_parent1 A210126 --channels insert_size --gvcf /paedyl01/disk1/yangyxt/test_tmp/singularity_inter_aRMiURV3spzCOiBE1ojvo_fj2J05oPk1/gvcf.tfrecord@6.gz --normalize_reads --pileup_image_height_child 100 --pileup_image_height_parent 100 --regions /paedyl01/disk1/yangyxt/indexed_genome/hg19/ucsc.hg19.no_dad.bed --task 0`. Without using the one-step run_deeptrio. I now remember that the way I wrote that script can make the underlying error unclear, and as a result hard to debug when an error occurs. If you directly run the failed job, you should get more helpful error message. Sorry for the inconvenience. I believe I improved that for run_deepvariant.py a while ago but haven't done the same for run_deeptrio.py.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/646
https://github.com/google/deepvariant/issues/646:1121,interoperability,messag,message,1121,"Actually I have a suggestion:. Can you directly run the failed command. `./opt/deepvariant/bin/deeptrio/make_examples --mode calling --ref /paedyl01/disk1/yangyxt/indexed_genome/ucsc.hg19.fasta --reads_parent1 /paedyl01/disk1/yangyxt/wesplus/50_samples_20220304/aligned_results/A210126.deduped.bam --reads /paedyl01/disk1/yangyxt/wesplus/50_samples_20220304/aligned_results/A210124.deduped.bam --examples /paedyl01/disk1/yangyxt/test_tmp/singularity_inter_aRMiURV3spzCOiBE1ojvo_fj2J05oPk1/make_examples.tfrecord@6.gz --sample_name A210124 --sample_name_parent1 A210126 --channels insert_size --gvcf /paedyl01/disk1/yangyxt/test_tmp/singularity_inter_aRMiURV3spzCOiBE1ojvo_fj2J05oPk1/gvcf.tfrecord@6.gz --normalize_reads --pileup_image_height_child 100 --pileup_image_height_parent 100 --regions /paedyl01/disk1/yangyxt/indexed_genome/hg19/ucsc.hg19.no_dad.bed --task 0`. Without using the one-step run_deeptrio. I now remember that the way I wrote that script can make the underlying error unclear, and as a result hard to debug when an error occurs. If you directly run the failed job, you should get more helpful error message. Sorry for the inconvenience. I believe I improved that for run_deepvariant.py a while ago but haven't done the same for run_deeptrio.py.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/646
https://github.com/google/deepvariant/issues/646:984,performance,error,error,984,"Actually I have a suggestion:. Can you directly run the failed command. `./opt/deepvariant/bin/deeptrio/make_examples --mode calling --ref /paedyl01/disk1/yangyxt/indexed_genome/ucsc.hg19.fasta --reads_parent1 /paedyl01/disk1/yangyxt/wesplus/50_samples_20220304/aligned_results/A210126.deduped.bam --reads /paedyl01/disk1/yangyxt/wesplus/50_samples_20220304/aligned_results/A210124.deduped.bam --examples /paedyl01/disk1/yangyxt/test_tmp/singularity_inter_aRMiURV3spzCOiBE1ojvo_fj2J05oPk1/make_examples.tfrecord@6.gz --sample_name A210124 --sample_name_parent1 A210126 --channels insert_size --gvcf /paedyl01/disk1/yangyxt/test_tmp/singularity_inter_aRMiURV3spzCOiBE1ojvo_fj2J05oPk1/gvcf.tfrecord@6.gz --normalize_reads --pileup_image_height_child 100 --pileup_image_height_parent 100 --regions /paedyl01/disk1/yangyxt/indexed_genome/hg19/ucsc.hg19.no_dad.bed --task 0`. Without using the one-step run_deeptrio. I now remember that the way I wrote that script can make the underlying error unclear, and as a result hard to debug when an error occurs. If you directly run the failed job, you should get more helpful error message. Sorry for the inconvenience. I believe I improved that for run_deepvariant.py a while ago but haven't done the same for run_deeptrio.py.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/646
https://github.com/google/deepvariant/issues/646:1037,performance,error,error,1037,"Actually I have a suggestion:. Can you directly run the failed command. `./opt/deepvariant/bin/deeptrio/make_examples --mode calling --ref /paedyl01/disk1/yangyxt/indexed_genome/ucsc.hg19.fasta --reads_parent1 /paedyl01/disk1/yangyxt/wesplus/50_samples_20220304/aligned_results/A210126.deduped.bam --reads /paedyl01/disk1/yangyxt/wesplus/50_samples_20220304/aligned_results/A210124.deduped.bam --examples /paedyl01/disk1/yangyxt/test_tmp/singularity_inter_aRMiURV3spzCOiBE1ojvo_fj2J05oPk1/make_examples.tfrecord@6.gz --sample_name A210124 --sample_name_parent1 A210126 --channels insert_size --gvcf /paedyl01/disk1/yangyxt/test_tmp/singularity_inter_aRMiURV3spzCOiBE1ojvo_fj2J05oPk1/gvcf.tfrecord@6.gz --normalize_reads --pileup_image_height_child 100 --pileup_image_height_parent 100 --regions /paedyl01/disk1/yangyxt/indexed_genome/hg19/ucsc.hg19.no_dad.bed --task 0`. Without using the one-step run_deeptrio. I now remember that the way I wrote that script can make the underlying error unclear, and as a result hard to debug when an error occurs. If you directly run the failed job, you should get more helpful error message. Sorry for the inconvenience. I believe I improved that for run_deepvariant.py a while ago but haven't done the same for run_deeptrio.py.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/646
https://github.com/google/deepvariant/issues/646:1115,performance,error,error,1115,"Actually I have a suggestion:. Can you directly run the failed command. `./opt/deepvariant/bin/deeptrio/make_examples --mode calling --ref /paedyl01/disk1/yangyxt/indexed_genome/ucsc.hg19.fasta --reads_parent1 /paedyl01/disk1/yangyxt/wesplus/50_samples_20220304/aligned_results/A210126.deduped.bam --reads /paedyl01/disk1/yangyxt/wesplus/50_samples_20220304/aligned_results/A210124.deduped.bam --examples /paedyl01/disk1/yangyxt/test_tmp/singularity_inter_aRMiURV3spzCOiBE1ojvo_fj2J05oPk1/make_examples.tfrecord@6.gz --sample_name A210124 --sample_name_parent1 A210126 --channels insert_size --gvcf /paedyl01/disk1/yangyxt/test_tmp/singularity_inter_aRMiURV3spzCOiBE1ojvo_fj2J05oPk1/gvcf.tfrecord@6.gz --normalize_reads --pileup_image_height_child 100 --pileup_image_height_parent 100 --regions /paedyl01/disk1/yangyxt/indexed_genome/hg19/ucsc.hg19.no_dad.bed --task 0`. Without using the one-step run_deeptrio. I now remember that the way I wrote that script can make the underlying error unclear, and as a result hard to debug when an error occurs. If you directly run the failed job, you should get more helpful error message. Sorry for the inconvenience. I believe I improved that for run_deepvariant.py a while ago but haven't done the same for run_deeptrio.py.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/646
https://github.com/google/deepvariant/issues/646:56,reliability,fail,failed,56,"Actually I have a suggestion:. Can you directly run the failed command. `./opt/deepvariant/bin/deeptrio/make_examples --mode calling --ref /paedyl01/disk1/yangyxt/indexed_genome/ucsc.hg19.fasta --reads_parent1 /paedyl01/disk1/yangyxt/wesplus/50_samples_20220304/aligned_results/A210126.deduped.bam --reads /paedyl01/disk1/yangyxt/wesplus/50_samples_20220304/aligned_results/A210124.deduped.bam --examples /paedyl01/disk1/yangyxt/test_tmp/singularity_inter_aRMiURV3spzCOiBE1ojvo_fj2J05oPk1/make_examples.tfrecord@6.gz --sample_name A210124 --sample_name_parent1 A210126 --channels insert_size --gvcf /paedyl01/disk1/yangyxt/test_tmp/singularity_inter_aRMiURV3spzCOiBE1ojvo_fj2J05oPk1/gvcf.tfrecord@6.gz --normalize_reads --pileup_image_height_child 100 --pileup_image_height_parent 100 --regions /paedyl01/disk1/yangyxt/indexed_genome/hg19/ucsc.hg19.no_dad.bed --task 0`. Without using the one-step run_deeptrio. I now remember that the way I wrote that script can make the underlying error unclear, and as a result hard to debug when an error occurs. If you directly run the failed job, you should get more helpful error message. Sorry for the inconvenience. I believe I improved that for run_deepvariant.py a while ago but haven't done the same for run_deeptrio.py.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/646
https://github.com/google/deepvariant/issues/646:1075,reliability,fail,failed,1075,"Actually I have a suggestion:. Can you directly run the failed command. `./opt/deepvariant/bin/deeptrio/make_examples --mode calling --ref /paedyl01/disk1/yangyxt/indexed_genome/ucsc.hg19.fasta --reads_parent1 /paedyl01/disk1/yangyxt/wesplus/50_samples_20220304/aligned_results/A210126.deduped.bam --reads /paedyl01/disk1/yangyxt/wesplus/50_samples_20220304/aligned_results/A210124.deduped.bam --examples /paedyl01/disk1/yangyxt/test_tmp/singularity_inter_aRMiURV3spzCOiBE1ojvo_fj2J05oPk1/make_examples.tfrecord@6.gz --sample_name A210124 --sample_name_parent1 A210126 --channels insert_size --gvcf /paedyl01/disk1/yangyxt/test_tmp/singularity_inter_aRMiURV3spzCOiBE1ojvo_fj2J05oPk1/gvcf.tfrecord@6.gz --normalize_reads --pileup_image_height_child 100 --pileup_image_height_parent 100 --regions /paedyl01/disk1/yangyxt/indexed_genome/hg19/ucsc.hg19.no_dad.bed --task 0`. Without using the one-step run_deeptrio. I now remember that the way I wrote that script can make the underlying error unclear, and as a result hard to debug when an error occurs. If you directly run the failed job, you should get more helpful error message. Sorry for the inconvenience. I believe I improved that for run_deepvariant.py a while ago but haven't done the same for run_deeptrio.py.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/646
https://github.com/google/deepvariant/issues/646:918,safety,reme,remember,918,"Actually I have a suggestion:. Can you directly run the failed command. `./opt/deepvariant/bin/deeptrio/make_examples --mode calling --ref /paedyl01/disk1/yangyxt/indexed_genome/ucsc.hg19.fasta --reads_parent1 /paedyl01/disk1/yangyxt/wesplus/50_samples_20220304/aligned_results/A210126.deduped.bam --reads /paedyl01/disk1/yangyxt/wesplus/50_samples_20220304/aligned_results/A210124.deduped.bam --examples /paedyl01/disk1/yangyxt/test_tmp/singularity_inter_aRMiURV3spzCOiBE1ojvo_fj2J05oPk1/make_examples.tfrecord@6.gz --sample_name A210124 --sample_name_parent1 A210126 --channels insert_size --gvcf /paedyl01/disk1/yangyxt/test_tmp/singularity_inter_aRMiURV3spzCOiBE1ojvo_fj2J05oPk1/gvcf.tfrecord@6.gz --normalize_reads --pileup_image_height_child 100 --pileup_image_height_parent 100 --regions /paedyl01/disk1/yangyxt/indexed_genome/hg19/ucsc.hg19.no_dad.bed --task 0`. Without using the one-step run_deeptrio. I now remember that the way I wrote that script can make the underlying error unclear, and as a result hard to debug when an error occurs. If you directly run the failed job, you should get more helpful error message. Sorry for the inconvenience. I believe I improved that for run_deepvariant.py a while ago but haven't done the same for run_deeptrio.py.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/646
https://github.com/google/deepvariant/issues/646:984,safety,error,error,984,"Actually I have a suggestion:. Can you directly run the failed command. `./opt/deepvariant/bin/deeptrio/make_examples --mode calling --ref /paedyl01/disk1/yangyxt/indexed_genome/ucsc.hg19.fasta --reads_parent1 /paedyl01/disk1/yangyxt/wesplus/50_samples_20220304/aligned_results/A210126.deduped.bam --reads /paedyl01/disk1/yangyxt/wesplus/50_samples_20220304/aligned_results/A210124.deduped.bam --examples /paedyl01/disk1/yangyxt/test_tmp/singularity_inter_aRMiURV3spzCOiBE1ojvo_fj2J05oPk1/make_examples.tfrecord@6.gz --sample_name A210124 --sample_name_parent1 A210126 --channels insert_size --gvcf /paedyl01/disk1/yangyxt/test_tmp/singularity_inter_aRMiURV3spzCOiBE1ojvo_fj2J05oPk1/gvcf.tfrecord@6.gz --normalize_reads --pileup_image_height_child 100 --pileup_image_height_parent 100 --regions /paedyl01/disk1/yangyxt/indexed_genome/hg19/ucsc.hg19.no_dad.bed --task 0`. Without using the one-step run_deeptrio. I now remember that the way I wrote that script can make the underlying error unclear, and as a result hard to debug when an error occurs. If you directly run the failed job, you should get more helpful error message. Sorry for the inconvenience. I believe I improved that for run_deepvariant.py a while ago but haven't done the same for run_deeptrio.py.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/646
https://github.com/google/deepvariant/issues/646:1037,safety,error,error,1037,"Actually I have a suggestion:. Can you directly run the failed command. `./opt/deepvariant/bin/deeptrio/make_examples --mode calling --ref /paedyl01/disk1/yangyxt/indexed_genome/ucsc.hg19.fasta --reads_parent1 /paedyl01/disk1/yangyxt/wesplus/50_samples_20220304/aligned_results/A210126.deduped.bam --reads /paedyl01/disk1/yangyxt/wesplus/50_samples_20220304/aligned_results/A210124.deduped.bam --examples /paedyl01/disk1/yangyxt/test_tmp/singularity_inter_aRMiURV3spzCOiBE1ojvo_fj2J05oPk1/make_examples.tfrecord@6.gz --sample_name A210124 --sample_name_parent1 A210126 --channels insert_size --gvcf /paedyl01/disk1/yangyxt/test_tmp/singularity_inter_aRMiURV3spzCOiBE1ojvo_fj2J05oPk1/gvcf.tfrecord@6.gz --normalize_reads --pileup_image_height_child 100 --pileup_image_height_parent 100 --regions /paedyl01/disk1/yangyxt/indexed_genome/hg19/ucsc.hg19.no_dad.bed --task 0`. Without using the one-step run_deeptrio. I now remember that the way I wrote that script can make the underlying error unclear, and as a result hard to debug when an error occurs. If you directly run the failed job, you should get more helpful error message. Sorry for the inconvenience. I believe I improved that for run_deepvariant.py a while ago but haven't done the same for run_deeptrio.py.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/646
https://github.com/google/deepvariant/issues/646:1115,safety,error,error,1115,"Actually I have a suggestion:. Can you directly run the failed command. `./opt/deepvariant/bin/deeptrio/make_examples --mode calling --ref /paedyl01/disk1/yangyxt/indexed_genome/ucsc.hg19.fasta --reads_parent1 /paedyl01/disk1/yangyxt/wesplus/50_samples_20220304/aligned_results/A210126.deduped.bam --reads /paedyl01/disk1/yangyxt/wesplus/50_samples_20220304/aligned_results/A210124.deduped.bam --examples /paedyl01/disk1/yangyxt/test_tmp/singularity_inter_aRMiURV3spzCOiBE1ojvo_fj2J05oPk1/make_examples.tfrecord@6.gz --sample_name A210124 --sample_name_parent1 A210126 --channels insert_size --gvcf /paedyl01/disk1/yangyxt/test_tmp/singularity_inter_aRMiURV3spzCOiBE1ojvo_fj2J05oPk1/gvcf.tfrecord@6.gz --normalize_reads --pileup_image_height_child 100 --pileup_image_height_parent 100 --regions /paedyl01/disk1/yangyxt/indexed_genome/hg19/ucsc.hg19.no_dad.bed --task 0`. Without using the one-step run_deeptrio. I now remember that the way I wrote that script can make the underlying error unclear, and as a result hard to debug when an error occurs. If you directly run the failed job, you should get more helpful error message. Sorry for the inconvenience. I believe I improved that for run_deepvariant.py a while ago but haven't done the same for run_deeptrio.py.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/646
https://github.com/google/deepvariant/issues/646:63,usability,command,command,63,"Actually I have a suggestion:. Can you directly run the failed command. `./opt/deepvariant/bin/deeptrio/make_examples --mode calling --ref /paedyl01/disk1/yangyxt/indexed_genome/ucsc.hg19.fasta --reads_parent1 /paedyl01/disk1/yangyxt/wesplus/50_samples_20220304/aligned_results/A210126.deduped.bam --reads /paedyl01/disk1/yangyxt/wesplus/50_samples_20220304/aligned_results/A210124.deduped.bam --examples /paedyl01/disk1/yangyxt/test_tmp/singularity_inter_aRMiURV3spzCOiBE1ojvo_fj2J05oPk1/make_examples.tfrecord@6.gz --sample_name A210124 --sample_name_parent1 A210126 --channels insert_size --gvcf /paedyl01/disk1/yangyxt/test_tmp/singularity_inter_aRMiURV3spzCOiBE1ojvo_fj2J05oPk1/gvcf.tfrecord@6.gz --normalize_reads --pileup_image_height_child 100 --pileup_image_height_parent 100 --regions /paedyl01/disk1/yangyxt/indexed_genome/hg19/ucsc.hg19.no_dad.bed --task 0`. Without using the one-step run_deeptrio. I now remember that the way I wrote that script can make the underlying error unclear, and as a result hard to debug when an error occurs. If you directly run the failed job, you should get more helpful error message. Sorry for the inconvenience. I believe I improved that for run_deepvariant.py a while ago but haven't done the same for run_deeptrio.py.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/646
https://github.com/google/deepvariant/issues/646:984,usability,error,error,984,"Actually I have a suggestion:. Can you directly run the failed command. `./opt/deepvariant/bin/deeptrio/make_examples --mode calling --ref /paedyl01/disk1/yangyxt/indexed_genome/ucsc.hg19.fasta --reads_parent1 /paedyl01/disk1/yangyxt/wesplus/50_samples_20220304/aligned_results/A210126.deduped.bam --reads /paedyl01/disk1/yangyxt/wesplus/50_samples_20220304/aligned_results/A210124.deduped.bam --examples /paedyl01/disk1/yangyxt/test_tmp/singularity_inter_aRMiURV3spzCOiBE1ojvo_fj2J05oPk1/make_examples.tfrecord@6.gz --sample_name A210124 --sample_name_parent1 A210126 --channels insert_size --gvcf /paedyl01/disk1/yangyxt/test_tmp/singularity_inter_aRMiURV3spzCOiBE1ojvo_fj2J05oPk1/gvcf.tfrecord@6.gz --normalize_reads --pileup_image_height_child 100 --pileup_image_height_parent 100 --regions /paedyl01/disk1/yangyxt/indexed_genome/hg19/ucsc.hg19.no_dad.bed --task 0`. Without using the one-step run_deeptrio. I now remember that the way I wrote that script can make the underlying error unclear, and as a result hard to debug when an error occurs. If you directly run the failed job, you should get more helpful error message. Sorry for the inconvenience. I believe I improved that for run_deepvariant.py a while ago but haven't done the same for run_deeptrio.py.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/646
https://github.com/google/deepvariant/issues/646:1037,usability,error,error,1037,"Actually I have a suggestion:. Can you directly run the failed command. `./opt/deepvariant/bin/deeptrio/make_examples --mode calling --ref /paedyl01/disk1/yangyxt/indexed_genome/ucsc.hg19.fasta --reads_parent1 /paedyl01/disk1/yangyxt/wesplus/50_samples_20220304/aligned_results/A210126.deduped.bam --reads /paedyl01/disk1/yangyxt/wesplus/50_samples_20220304/aligned_results/A210124.deduped.bam --examples /paedyl01/disk1/yangyxt/test_tmp/singularity_inter_aRMiURV3spzCOiBE1ojvo_fj2J05oPk1/make_examples.tfrecord@6.gz --sample_name A210124 --sample_name_parent1 A210126 --channels insert_size --gvcf /paedyl01/disk1/yangyxt/test_tmp/singularity_inter_aRMiURV3spzCOiBE1ojvo_fj2J05oPk1/gvcf.tfrecord@6.gz --normalize_reads --pileup_image_height_child 100 --pileup_image_height_parent 100 --regions /paedyl01/disk1/yangyxt/indexed_genome/hg19/ucsc.hg19.no_dad.bed --task 0`. Without using the one-step run_deeptrio. I now remember that the way I wrote that script can make the underlying error unclear, and as a result hard to debug when an error occurs. If you directly run the failed job, you should get more helpful error message. Sorry for the inconvenience. I believe I improved that for run_deepvariant.py a while ago but haven't done the same for run_deeptrio.py.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/646
https://github.com/google/deepvariant/issues/646:1107,usability,help,helpful,1107,"Actually I have a suggestion:. Can you directly run the failed command. `./opt/deepvariant/bin/deeptrio/make_examples --mode calling --ref /paedyl01/disk1/yangyxt/indexed_genome/ucsc.hg19.fasta --reads_parent1 /paedyl01/disk1/yangyxt/wesplus/50_samples_20220304/aligned_results/A210126.deduped.bam --reads /paedyl01/disk1/yangyxt/wesplus/50_samples_20220304/aligned_results/A210124.deduped.bam --examples /paedyl01/disk1/yangyxt/test_tmp/singularity_inter_aRMiURV3spzCOiBE1ojvo_fj2J05oPk1/make_examples.tfrecord@6.gz --sample_name A210124 --sample_name_parent1 A210126 --channels insert_size --gvcf /paedyl01/disk1/yangyxt/test_tmp/singularity_inter_aRMiURV3spzCOiBE1ojvo_fj2J05oPk1/gvcf.tfrecord@6.gz --normalize_reads --pileup_image_height_child 100 --pileup_image_height_parent 100 --regions /paedyl01/disk1/yangyxt/indexed_genome/hg19/ucsc.hg19.no_dad.bed --task 0`. Without using the one-step run_deeptrio. I now remember that the way I wrote that script can make the underlying error unclear, and as a result hard to debug when an error occurs. If you directly run the failed job, you should get more helpful error message. Sorry for the inconvenience. I believe I improved that for run_deepvariant.py a while ago but haven't done the same for run_deeptrio.py.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/646
https://github.com/google/deepvariant/issues/646:1115,usability,error,error,1115,"Actually I have a suggestion:. Can you directly run the failed command. `./opt/deepvariant/bin/deeptrio/make_examples --mode calling --ref /paedyl01/disk1/yangyxt/indexed_genome/ucsc.hg19.fasta --reads_parent1 /paedyl01/disk1/yangyxt/wesplus/50_samples_20220304/aligned_results/A210126.deduped.bam --reads /paedyl01/disk1/yangyxt/wesplus/50_samples_20220304/aligned_results/A210124.deduped.bam --examples /paedyl01/disk1/yangyxt/test_tmp/singularity_inter_aRMiURV3spzCOiBE1ojvo_fj2J05oPk1/make_examples.tfrecord@6.gz --sample_name A210124 --sample_name_parent1 A210126 --channels insert_size --gvcf /paedyl01/disk1/yangyxt/test_tmp/singularity_inter_aRMiURV3spzCOiBE1ojvo_fj2J05oPk1/gvcf.tfrecord@6.gz --normalize_reads --pileup_image_height_child 100 --pileup_image_height_parent 100 --regions /paedyl01/disk1/yangyxt/indexed_genome/hg19/ucsc.hg19.no_dad.bed --task 0`. Without using the one-step run_deeptrio. I now remember that the way I wrote that script can make the underlying error unclear, and as a result hard to debug when an error occurs. If you directly run the failed job, you should get more helpful error message. Sorry for the inconvenience. I believe I improved that for run_deepvariant.py a while ago but haven't done the same for run_deeptrio.py.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/646
https://github.com/google/deepvariant/issues/646:141,deployability,version,version,141,Hello @pichuan I will try your suggestion and let you know the feedback. As to the SIF conversion its just simple command using singularity (version 3.7.2) pull docker://google/deepvariant:1.4 (Now I use 1.5).,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/646
https://github.com/google/deepvariant/issues/646:141,integrability,version,version,141,Hello @pichuan I will try your suggestion and let you know the feedback. As to the SIF conversion its just simple command using singularity (version 3.7.2) pull docker://google/deepvariant:1.4 (Now I use 1.5).,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/646
https://github.com/google/deepvariant/issues/646:87,interoperability,convers,conversion,87,Hello @pichuan I will try your suggestion and let you know the feedback. As to the SIF conversion its just simple command using singularity (version 3.7.2) pull docker://google/deepvariant:1.4 (Now I use 1.5).,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/646
https://github.com/google/deepvariant/issues/646:141,modifiability,version,version,141,Hello @pichuan I will try your suggestion and let you know the feedback. As to the SIF conversion its just simple command using singularity (version 3.7.2) pull docker://google/deepvariant:1.4 (Now I use 1.5).,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/646
https://github.com/google/deepvariant/issues/646:107,testability,simpl,simple,107,Hello @pichuan I will try your suggestion and let you know the feedback. As to the SIF conversion its just simple command using singularity (version 3.7.2) pull docker://google/deepvariant:1.4 (Now I use 1.5).,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/646
https://github.com/google/deepvariant/issues/646:63,usability,feedback,feedback,63,Hello @pichuan I will try your suggestion and let you know the feedback. As to the SIF conversion its just simple command using singularity (version 3.7.2) pull docker://google/deepvariant:1.4 (Now I use 1.5).,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/646
https://github.com/google/deepvariant/issues/646:107,usability,simpl,simple,107,Hello @pichuan I will try your suggestion and let you know the feedback. As to the SIF conversion its just simple command using singularity (version 3.7.2) pull docker://google/deepvariant:1.4 (Now I use 1.5).,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/646
https://github.com/google/deepvariant/issues/646:114,usability,command,command,114,Hello @pichuan I will try your suggestion and let you know the feedback. As to the SIF conversion its just simple command using singularity (version 3.7.2) pull docker://google/deepvariant:1.4 (Now I use 1.5).,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/646
https://github.com/google/deepvariant/issues/646:26,usability,command,command,26,"Hi @pichuan, I tried your command and it returns with successful return code. I dont know what went wrong if I use run_deeptrio.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/646
https://github.com/google/deepvariant/issues/646:362,availability,error,error,362,"Hi @yangyxt ,. I see... For now, to help you continue your work, I suggest using run_deeptrio with the `--dry_run=true` flag to get the list of commands, and run the commands there individually for now. Sorry for the inconvenience. The goal for having the one-step run_deeptrio.py script was to make things easier. But I need to do some improvements to make the error reporting better in the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/646
https://github.com/google/deepvariant/issues/646:45,deployability,continu,continue,45,"Hi @yangyxt ,. I see... For now, to help you continue your work, I suggest using run_deeptrio with the `--dry_run=true` flag to get the list of commands, and run the commands there individually for now. Sorry for the inconvenience. The goal for having the one-step run_deeptrio.py script was to make things easier. But I need to do some improvements to make the error reporting better in the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/646
https://github.com/google/deepvariant/issues/646:397,deployability,releas,release,397,"Hi @yangyxt ,. I see... For now, to help you continue your work, I suggest using run_deeptrio with the `--dry_run=true` flag to get the list of commands, and run the commands there individually for now. Sorry for the inconvenience. The goal for having the one-step run_deeptrio.py script was to make things easier. But I need to do some improvements to make the error reporting better in the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/646
https://github.com/google/deepvariant/issues/646:362,performance,error,error,362,"Hi @yangyxt ,. I see... For now, to help you continue your work, I suggest using run_deeptrio with the `--dry_run=true` flag to get the list of commands, and run the commands there individually for now. Sorry for the inconvenience. The goal for having the one-step run_deeptrio.py script was to make things easier. But I need to do some improvements to make the error reporting better in the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/646
https://github.com/google/deepvariant/issues/646:362,safety,error,error,362,"Hi @yangyxt ,. I see... For now, to help you continue your work, I suggest using run_deeptrio with the `--dry_run=true` flag to get the list of commands, and run the commands there individually for now. Sorry for the inconvenience. The goal for having the one-step run_deeptrio.py script was to make things easier. But I need to do some improvements to make the error reporting better in the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/646
https://github.com/google/deepvariant/issues/646:36,usability,help,help,36,"Hi @yangyxt ,. I see... For now, to help you continue your work, I suggest using run_deeptrio with the `--dry_run=true` flag to get the list of commands, and run the commands there individually for now. Sorry for the inconvenience. The goal for having the one-step run_deeptrio.py script was to make things easier. But I need to do some improvements to make the error reporting better in the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/646
https://github.com/google/deepvariant/issues/646:144,usability,command,commands,144,"Hi @yangyxt ,. I see... For now, to help you continue your work, I suggest using run_deeptrio with the `--dry_run=true` flag to get the list of commands, and run the commands there individually for now. Sorry for the inconvenience. The goal for having the one-step run_deeptrio.py script was to make things easier. But I need to do some improvements to make the error reporting better in the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/646
https://github.com/google/deepvariant/issues/646:166,usability,command,commands,166,"Hi @yangyxt ,. I see... For now, to help you continue your work, I suggest using run_deeptrio with the `--dry_run=true` flag to get the list of commands, and run the commands there individually for now. Sorry for the inconvenience. The goal for having the one-step run_deeptrio.py script was to make things easier. But I need to do some improvements to make the error reporting better in the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/646
https://github.com/google/deepvariant/issues/646:362,usability,error,error,362,"Hi @yangyxt ,. I see... For now, to help you continue your work, I suggest using run_deeptrio with the `--dry_run=true` flag to get the list of commands, and run the commands there individually for now. Sorry for the inconvenience. The goal for having the one-step run_deeptrio.py script was to make things easier. But I need to do some improvements to make the error reporting better in the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/646
https://github.com/google/deepvariant/issues/646:78,availability,error,errors,78,"I'll close this issue for now. @yangyxt if you are able to find more detailed errors, feel free to update and reopen.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/646
https://github.com/google/deepvariant/issues/646:99,deployability,updat,update,99,"I'll close this issue for now. @yangyxt if you are able to find more detailed errors, feel free to update and reopen.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/646
https://github.com/google/deepvariant/issues/646:78,performance,error,errors,78,"I'll close this issue for now. @yangyxt if you are able to find more detailed errors, feel free to update and reopen.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/646
https://github.com/google/deepvariant/issues/646:78,safety,error,errors,78,"I'll close this issue for now. @yangyxt if you are able to find more detailed errors, feel free to update and reopen.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/646
https://github.com/google/deepvariant/issues/646:99,safety,updat,update,99,"I'll close this issue for now. @yangyxt if you are able to find more detailed errors, feel free to update and reopen.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/646
https://github.com/google/deepvariant/issues/646:99,security,updat,update,99,"I'll close this issue for now. @yangyxt if you are able to find more detailed errors, feel free to update and reopen.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/646
https://github.com/google/deepvariant/issues/646:5,usability,close,close,5,"I'll close this issue for now. @yangyxt if you are able to find more detailed errors, feel free to update and reopen.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/646
https://github.com/google/deepvariant/issues/646:78,usability,error,errors,78,"I'll close this issue for now. @yangyxt if you are able to find more detailed errors, feel free to update and reopen.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/646
https://github.com/google/deepvariant/issues/649:44,deployability,releas,release,44,"@kiranpatil222 you are in luck. Our [v1.4.0 release](https://github.com/google/deepvariant/releases/tag/v1.4.0) introduced 'direct phasing,' which means you no longer need to run whatshap. . See the [PacBio case study](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-pacbio-model-case-study.md) for further details.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/649
https://github.com/google/deepvariant/issues/649:91,deployability,releas,releases,91,"@kiranpatil222 you are in luck. Our [v1.4.0 release](https://github.com/google/deepvariant/releases/tag/v1.4.0) introduced 'direct phasing,' which means you no longer need to run whatshap. . See the [PacBio case study](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-pacbio-model-case-study.md) for further details.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/649
https://github.com/google/deepvariant/issues/649:291,energy efficiency,model,model-case-study,291,"@kiranpatil222 you are in luck. Our [v1.4.0 release](https://github.com/google/deepvariant/releases/tag/v1.4.0) introduced 'direct phasing,' which means you no longer need to run whatshap. . See the [PacBio case study](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-pacbio-model-case-study.md) for further details.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/649
https://github.com/google/deepvariant/issues/649:200,modifiability,Pac,PacBio,200,"@kiranpatil222 you are in luck. Our [v1.4.0 release](https://github.com/google/deepvariant/releases/tag/v1.4.0) introduced 'direct phasing,' which means you no longer need to run whatshap. . See the [PacBio case study](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-pacbio-model-case-study.md) for further details.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/649
https://github.com/google/deepvariant/issues/649:284,modifiability,pac,pacbio-model-case-study,284,"@kiranpatil222 you are in luck. Our [v1.4.0 release](https://github.com/google/deepvariant/releases/tag/v1.4.0) introduced 'direct phasing,' which means you no longer need to run whatshap. . See the [PacBio case study](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-pacbio-model-case-study.md) for further details.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/649
https://github.com/google/deepvariant/issues/649:291,security,model,model-case-study,291,"@kiranpatil222 you are in luck. Our [v1.4.0 release](https://github.com/google/deepvariant/releases/tag/v1.4.0) introduced 'direct phasing,' which means you no longer need to run whatshap. . See the [PacBio case study](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-pacbio-model-case-study.md) for further details.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/649
https://github.com/google/deepvariant/issues/649:187,deployability,version,version,187,"@danielecook Thanks a lot for for clarification. One more quick question outa topic, As you said in v1.4.0 it no longer need Whatshap, How to check my Singularity (SIF) DeepVariant image version the one i am using ?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/649
https://github.com/google/deepvariant/issues/649:78,integrability,topic,topic,78,"@danielecook Thanks a lot for for clarification. One more quick question outa topic, As you said in v1.4.0 it no longer need Whatshap, How to check my Singularity (SIF) DeepVariant image version the one i am using ?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/649
https://github.com/google/deepvariant/issues/649:187,integrability,version,version,187,"@danielecook Thanks a lot for for clarification. One more quick question outa topic, As you said in v1.4.0 it no longer need Whatshap, How to check my Singularity (SIF) DeepVariant image version the one i am using ?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/649
https://github.com/google/deepvariant/issues/649:187,modifiability,version,version,187,"@danielecook Thanks a lot for for clarification. One more quick question outa topic, As you said in v1.4.0 it no longer need Whatshap, How to check my Singularity (SIF) DeepVariant image version the one i am using ?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/649
https://github.com/google/deepvariant/issues/649:470,deployability,log,log,470,"Hi, . I'm sure I'm doing something wrong, but couldn't figure out where that is: the variants I get are not directly phased. I followed the PacBio case study, using the 1.5.0 docker. . Here's the command. ```bash. $ ./run_deepvariant \. --model_type PACBIO \. --ref /shuang_data/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa \. --reads /shuang_data/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf /tmp/only_vcf.vcf.gz \. --regions chr20. ```. What's strange in the log are the following lines. ```. ... I0514 01:48:09.263085 140121429608256 make_examples_core.py:257] 62017 candidates (71070 examples) [121.51s elapsed]. I0514 01:48:23.145400 140121429608256 make_examples_core.py:257] Skip phasing: len(candidates[main_sample]) is 7088. I0514 01:52:54.732045 140121429608256 make_examples_core.py:257] 67303 candidates (76952 examples) [285.47s elapsed]. ... I0514 02:28:55.286200 140121429608256 make_examples_core.py:257] 166834 candidates (181516 examples) [142.03s elapsed]. I0514 02:29:01.686342 140121429608256 make_examples_core.py:257] Skip phasing: len(candidates[main_sample]) is 5953. I0514 02:31:24.326849 140121429608256 make_examples_core.py:257] 172413 candidates (187997 examples) [149.04s elapsed]. I0514 02:31:26.924692 140121429608256 make_examples_core.py:257] Skip phasing: len(candidates[main_sample]) is 7496. I0514 02:35:53.029588 140121429608256 make_examples_core.py:257] 178012 candidates (194646 examples) [268.70s elapsed]. .... ```. And there's also the relevant command information from the log below (formatted for human consumption). ```. time seq 0 0 | parallel -q --halt 2 \. --line-buffer /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""/shuang_data/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa"" \. --reads ""/shuang_data/HG003.GRCh38.chr20.pFDA_truthv2.bam"" \. --examples ""/tmp/tmpouwynlb0/make_examples.tfrecord@1.gz"" \. --add_hp_channel \. --alt_aligned_pileup ""diff_channels"" \. --max_reads_per_partition ""600"" \. --min_mapping_quality ""1",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/649
https://github.com/google/deepvariant/issues/649:1528,deployability,log,log,1528,"140121429608256 make_examples_core.py:257] 62017 candidates (71070 examples) [121.51s elapsed]. I0514 01:48:23.145400 140121429608256 make_examples_core.py:257] Skip phasing: len(candidates[main_sample]) is 7088. I0514 01:52:54.732045 140121429608256 make_examples_core.py:257] 67303 candidates (76952 examples) [285.47s elapsed]. ... I0514 02:28:55.286200 140121429608256 make_examples_core.py:257] 166834 candidates (181516 examples) [142.03s elapsed]. I0514 02:29:01.686342 140121429608256 make_examples_core.py:257] Skip phasing: len(candidates[main_sample]) is 5953. I0514 02:31:24.326849 140121429608256 make_examples_core.py:257] 172413 candidates (187997 examples) [149.04s elapsed]. I0514 02:31:26.924692 140121429608256 make_examples_core.py:257] Skip phasing: len(candidates[main_sample]) is 7496. I0514 02:35:53.029588 140121429608256 make_examples_core.py:257] 178012 candidates (194646 examples) [268.70s elapsed]. .... ```. And there's also the relevant command information from the log below (formatted for human consumption). ```. time seq 0 0 | parallel -q --halt 2 \. --line-buffer /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""/shuang_data/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa"" \. --reads ""/shuang_data/HG003.GRCh38.chr20.pFDA_truthv2.bam"" \. --examples ""/tmp/tmpouwynlb0/make_examples.tfrecord@1.gz"" \. --add_hp_channel \. --alt_aligned_pileup ""diff_channels"" \. --max_reads_per_partition ""600"" \. --min_mapping_quality ""1"" \. --parse_sam_aux_fields \. --partition_size ""25000"" \. --phase_reads \. --pileup_image_width ""199"" \. --norealign_reads \. --regions ""chr20"" \. --sort_by_haplotypes \. --track_ref_reads \. --vsc_min_fraction_indels ""0.12"" \. --task {}. ... ```. Here's a snapshot of the output. ```. chr20 64506 . C CTCCAT 44.3 PASS . GT:GQ:DP:AD:VAF:PL 0/1:35:44:20,20:0.454545:44,0,35. chr20 68303 . T C 45.4 PASS . GT:GQ:DP:AD:VAF:PL 0/1:45:42:21,21:0.5:45,0,59. chr20 72765 . T TA 21.8 PASS . GT:GQ:DP:AD:VAF:PL 0/1:22:32:19,12:0.375:21,0,",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/649
https://github.com/google/deepvariant/issues/649:1624,integrability,buffer,buffer,1624,"0514 01:48:23.145400 140121429608256 make_examples_core.py:257] Skip phasing: len(candidates[main_sample]) is 7088. I0514 01:52:54.732045 140121429608256 make_examples_core.py:257] 67303 candidates (76952 examples) [285.47s elapsed]. ... I0514 02:28:55.286200 140121429608256 make_examples_core.py:257] 166834 candidates (181516 examples) [142.03s elapsed]. I0514 02:29:01.686342 140121429608256 make_examples_core.py:257] Skip phasing: len(candidates[main_sample]) is 5953. I0514 02:31:24.326849 140121429608256 make_examples_core.py:257] 172413 candidates (187997 examples) [149.04s elapsed]. I0514 02:31:26.924692 140121429608256 make_examples_core.py:257] Skip phasing: len(candidates[main_sample]) is 7496. I0514 02:35:53.029588 140121429608256 make_examples_core.py:257] 178012 candidates (194646 examples) [268.70s elapsed]. .... ```. And there's also the relevant command information from the log below (formatted for human consumption). ```. time seq 0 0 | parallel -q --halt 2 \. --line-buffer /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""/shuang_data/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa"" \. --reads ""/shuang_data/HG003.GRCh38.chr20.pFDA_truthv2.bam"" \. --examples ""/tmp/tmpouwynlb0/make_examples.tfrecord@1.gz"" \. --add_hp_channel \. --alt_aligned_pileup ""diff_channels"" \. --max_reads_per_partition ""600"" \. --min_mapping_quality ""1"" \. --parse_sam_aux_fields \. --partition_size ""25000"" \. --phase_reads \. --pileup_image_width ""199"" \. --norealign_reads \. --regions ""chr20"" \. --sort_by_haplotypes \. --track_ref_reads \. --vsc_min_fraction_indels ""0.12"" \. --task {}. ... ```. Here's a snapshot of the output. ```. chr20 64506 . C CTCCAT 44.3 PASS . GT:GQ:DP:AD:VAF:PL 0/1:35:44:20,20:0.454545:44,0,35. chr20 68303 . T C 45.4 PASS . GT:GQ:DP:AD:VAF:PL 0/1:45:42:21,21:0.5:45,0,59. chr20 72765 . T TA 21.8 PASS . GT:GQ:DP:AD:VAF:PL 0/1:22:32:19,12:0.375:21,0,40. chr20 81154 . T G 64.2 PASS . GT:GQ:DP:AD:VAF:PL 0/1:63:39:14,25:0.641026:64,0,68. chr20 8209",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/649
https://github.com/google/deepvariant/issues/649:1539,interoperability,format,formatted,1539,"6 make_examples_core.py:257] 62017 candidates (71070 examples) [121.51s elapsed]. I0514 01:48:23.145400 140121429608256 make_examples_core.py:257] Skip phasing: len(candidates[main_sample]) is 7088. I0514 01:52:54.732045 140121429608256 make_examples_core.py:257] 67303 candidates (76952 examples) [285.47s elapsed]. ... I0514 02:28:55.286200 140121429608256 make_examples_core.py:257] 166834 candidates (181516 examples) [142.03s elapsed]. I0514 02:29:01.686342 140121429608256 make_examples_core.py:257] Skip phasing: len(candidates[main_sample]) is 5953. I0514 02:31:24.326849 140121429608256 make_examples_core.py:257] 172413 candidates (187997 examples) [149.04s elapsed]. I0514 02:31:26.924692 140121429608256 make_examples_core.py:257] Skip phasing: len(candidates[main_sample]) is 7496. I0514 02:35:53.029588 140121429608256 make_examples_core.py:257] 178012 candidates (194646 examples) [268.70s elapsed]. .... ```. And there's also the relevant command information from the log below (formatted for human consumption). ```. time seq 0 0 | parallel -q --halt 2 \. --line-buffer /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""/shuang_data/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa"" \. --reads ""/shuang_data/HG003.GRCh38.chr20.pFDA_truthv2.bam"" \. --examples ""/tmp/tmpouwynlb0/make_examples.tfrecord@1.gz"" \. --add_hp_channel \. --alt_aligned_pileup ""diff_channels"" \. --max_reads_per_partition ""600"" \. --min_mapping_quality ""1"" \. --parse_sam_aux_fields \. --partition_size ""25000"" \. --phase_reads \. --pileup_image_width ""199"" \. --norealign_reads \. --regions ""chr20"" \. --sort_by_haplotypes \. --track_ref_reads \. --vsc_min_fraction_indels ""0.12"" \. --task {}. ... ```. Here's a snapshot of the output. ```. chr20 64506 . C CTCCAT 44.3 PASS . GT:GQ:DP:AD:VAF:PL 0/1:35:44:20,20:0.454545:44,0,35. chr20 68303 . T C 45.4 PASS . GT:GQ:DP:AD:VAF:PL 0/1:45:42:21,21:0.5:45,0,59. chr20 72765 . T TA 21.8 PASS . GT:GQ:DP:AD:VAF:PL 0/1:22:32:19,12:0.375:21,0,40. chr20 8115",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/649
https://github.com/google/deepvariant/issues/649:140,modifiability,Pac,PacBio,140,"Hi, . I'm sure I'm doing something wrong, but couldn't figure out where that is: the variants I get are not directly phased. I followed the PacBio case study, using the 1.5.0 docker. . Here's the command. ```bash. $ ./run_deepvariant \. --model_type PACBIO \. --ref /shuang_data/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa \. --reads /shuang_data/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf /tmp/only_vcf.vcf.gz \. --regions chr20. ```. What's strange in the log are the following lines. ```. ... I0514 01:48:09.263085 140121429608256 make_examples_core.py:257] 62017 candidates (71070 examples) [121.51s elapsed]. I0514 01:48:23.145400 140121429608256 make_examples_core.py:257] Skip phasing: len(candidates[main_sample]) is 7088. I0514 01:52:54.732045 140121429608256 make_examples_core.py:257] 67303 candidates (76952 examples) [285.47s elapsed]. ... I0514 02:28:55.286200 140121429608256 make_examples_core.py:257] 166834 candidates (181516 examples) [142.03s elapsed]. I0514 02:29:01.686342 140121429608256 make_examples_core.py:257] Skip phasing: len(candidates[main_sample]) is 5953. I0514 02:31:24.326849 140121429608256 make_examples_core.py:257] 172413 candidates (187997 examples) [149.04s elapsed]. I0514 02:31:26.924692 140121429608256 make_examples_core.py:257] Skip phasing: len(candidates[main_sample]) is 7496. I0514 02:35:53.029588 140121429608256 make_examples_core.py:257] 178012 candidates (194646 examples) [268.70s elapsed]. .... ```. And there's also the relevant command information from the log below (formatted for human consumption). ```. time seq 0 0 | parallel -q --halt 2 \. --line-buffer /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""/shuang_data/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa"" \. --reads ""/shuang_data/HG003.GRCh38.chr20.pFDA_truthv2.bam"" \. --examples ""/tmp/tmpouwynlb0/make_examples.tfrecord@1.gz"" \. --add_hp_channel \. --alt_aligned_pileup ""diff_channels"" \. --max_reads_per_partition ""600"" \. --min_mapping_quality ""1",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/649
https://github.com/google/deepvariant/issues/649:250,modifiability,PAC,PACBIO,250,"Hi, . I'm sure I'm doing something wrong, but couldn't figure out where that is: the variants I get are not directly phased. I followed the PacBio case study, using the 1.5.0 docker. . Here's the command. ```bash. $ ./run_deepvariant \. --model_type PACBIO \. --ref /shuang_data/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa \. --reads /shuang_data/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf /tmp/only_vcf.vcf.gz \. --regions chr20. ```. What's strange in the log are the following lines. ```. ... I0514 01:48:09.263085 140121429608256 make_examples_core.py:257] 62017 candidates (71070 examples) [121.51s elapsed]. I0514 01:48:23.145400 140121429608256 make_examples_core.py:257] Skip phasing: len(candidates[main_sample]) is 7088. I0514 01:52:54.732045 140121429608256 make_examples_core.py:257] 67303 candidates (76952 examples) [285.47s elapsed]. ... I0514 02:28:55.286200 140121429608256 make_examples_core.py:257] 166834 candidates (181516 examples) [142.03s elapsed]. I0514 02:29:01.686342 140121429608256 make_examples_core.py:257] Skip phasing: len(candidates[main_sample]) is 5953. I0514 02:31:24.326849 140121429608256 make_examples_core.py:257] 172413 candidates (187997 examples) [149.04s elapsed]. I0514 02:31:26.924692 140121429608256 make_examples_core.py:257] Skip phasing: len(candidates[main_sample]) is 7496. I0514 02:35:53.029588 140121429608256 make_examples_core.py:257] 178012 candidates (194646 examples) [268.70s elapsed]. .... ```. And there's also the relevant command information from the log below (formatted for human consumption). ```. time seq 0 0 | parallel -q --halt 2 \. --line-buffer /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""/shuang_data/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa"" \. --reads ""/shuang_data/HG003.GRCh38.chr20.pFDA_truthv2.bam"" \. --examples ""/tmp/tmpouwynlb0/make_examples.tfrecord@1.gz"" \. --add_hp_channel \. --alt_aligned_pileup ""diff_channels"" \. --max_reads_per_partition ""600"" \. --min_mapping_quality ""1",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/649
https://github.com/google/deepvariant/issues/649:3113,modifiability,Pac,PacBio,3113,"4692 140121429608256 make_examples_core.py:257] Skip phasing: len(candidates[main_sample]) is 7496. I0514 02:35:53.029588 140121429608256 make_examples_core.py:257] 178012 candidates (194646 examples) [268.70s elapsed]. .... ```. And there's also the relevant command information from the log below (formatted for human consumption). ```. time seq 0 0 | parallel -q --halt 2 \. --line-buffer /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""/shuang_data/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa"" \. --reads ""/shuang_data/HG003.GRCh38.chr20.pFDA_truthv2.bam"" \. --examples ""/tmp/tmpouwynlb0/make_examples.tfrecord@1.gz"" \. --add_hp_channel \. --alt_aligned_pileup ""diff_channels"" \. --max_reads_per_partition ""600"" \. --min_mapping_quality ""1"" \. --parse_sam_aux_fields \. --partition_size ""25000"" \. --phase_reads \. --pileup_image_width ""199"" \. --norealign_reads \. --regions ""chr20"" \. --sort_by_haplotypes \. --track_ref_reads \. --vsc_min_fraction_indels ""0.12"" \. --task {}. ... ```. Here's a snapshot of the output. ```. chr20 64506 . C CTCCAT 44.3 PASS . GT:GQ:DP:AD:VAF:PL 0/1:35:44:20,20:0.454545:44,0,35. chr20 68303 . T C 45.4 PASS . GT:GQ:DP:AD:VAF:PL 0/1:45:42:21,21:0.5:45,0,59. chr20 72765 . T TA 21.8 PASS . GT:GQ:DP:AD:VAF:PL 0/1:22:32:19,12:0.375:21,0,40. chr20 81154 . T G 64.2 PASS . GT:GQ:DP:AD:VAF:PL 0/1:63:39:14,25:0.641026:64,0,68. chr20 82090 . C A 64 PASS . GT:GQ:DP:AD:VAF:PL 0/1:64:37:23,14:0.378378:64,0,77. chr20 82719 . C T 60.9 PASS . GT:GQ:DP:AD:VAF:PL 0/1:60:39:23,16:0.410256:60,0,66. chr20 83158 . C T 62.9 PASS . GT:GQ:DP:AD:VAF:PL 0/1:63:38:22,16:0.421053:62,0,76. chr20 84647 . G T 61.8 PASS . GT:GQ:DP:AD:VAF:PL 0/1:60:41:22,19:0.463415:61,0,66. chr20 85259 . G A 66.9 PASS . GT:GQ:DP:AD:VAF:PL 1/1:63:43:0,43:1:66,64,0. chr20 85729 . G A 59.5 PASS . GT:GQ:DP:AD:VAF:PL 1/1:56:44:0,44:1:59,59,0. ```. The linked PacBio case study also doesn't mention anything about phasing, or any related metric. Could you please help? Thank you! Steve.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/649
https://github.com/google/deepvariant/issues/649:1578,performance,time,time,1578,"andidates (71070 examples) [121.51s elapsed]. I0514 01:48:23.145400 140121429608256 make_examples_core.py:257] Skip phasing: len(candidates[main_sample]) is 7088. I0514 01:52:54.732045 140121429608256 make_examples_core.py:257] 67303 candidates (76952 examples) [285.47s elapsed]. ... I0514 02:28:55.286200 140121429608256 make_examples_core.py:257] 166834 candidates (181516 examples) [142.03s elapsed]. I0514 02:29:01.686342 140121429608256 make_examples_core.py:257] Skip phasing: len(candidates[main_sample]) is 5953. I0514 02:31:24.326849 140121429608256 make_examples_core.py:257] 172413 candidates (187997 examples) [149.04s elapsed]. I0514 02:31:26.924692 140121429608256 make_examples_core.py:257] Skip phasing: len(candidates[main_sample]) is 7496. I0514 02:35:53.029588 140121429608256 make_examples_core.py:257] 178012 candidates (194646 examples) [268.70s elapsed]. .... ```. And there's also the relevant command information from the log below (formatted for human consumption). ```. time seq 0 0 | parallel -q --halt 2 \. --line-buffer /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""/shuang_data/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa"" \. --reads ""/shuang_data/HG003.GRCh38.chr20.pFDA_truthv2.bam"" \. --examples ""/tmp/tmpouwynlb0/make_examples.tfrecord@1.gz"" \. --add_hp_channel \. --alt_aligned_pileup ""diff_channels"" \. --max_reads_per_partition ""600"" \. --min_mapping_quality ""1"" \. --parse_sam_aux_fields \. --partition_size ""25000"" \. --phase_reads \. --pileup_image_width ""199"" \. --norealign_reads \. --regions ""chr20"" \. --sort_by_haplotypes \. --track_ref_reads \. --vsc_min_fraction_indels ""0.12"" \. --task {}. ... ```. Here's a snapshot of the output. ```. chr20 64506 . C CTCCAT 44.3 PASS . GT:GQ:DP:AD:VAF:PL 0/1:35:44:20,20:0.454545:44,0,35. chr20 68303 . T C 45.4 PASS . GT:GQ:DP:AD:VAF:PL 0/1:45:42:21,21:0.5:45,0,59. chr20 72765 . T TA 21.8 PASS . GT:GQ:DP:AD:VAF:PL 0/1:22:32:19,12:0.375:21,0,40. chr20 81154 . T G 64.2 PASS . GT:GQ:DP:AD:VAF:",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/649
https://github.com/google/deepvariant/issues/649:1593,performance,parallel,parallel,1593,"examples) [121.51s elapsed]. I0514 01:48:23.145400 140121429608256 make_examples_core.py:257] Skip phasing: len(candidates[main_sample]) is 7088. I0514 01:52:54.732045 140121429608256 make_examples_core.py:257] 67303 candidates (76952 examples) [285.47s elapsed]. ... I0514 02:28:55.286200 140121429608256 make_examples_core.py:257] 166834 candidates (181516 examples) [142.03s elapsed]. I0514 02:29:01.686342 140121429608256 make_examples_core.py:257] Skip phasing: len(candidates[main_sample]) is 5953. I0514 02:31:24.326849 140121429608256 make_examples_core.py:257] 172413 candidates (187997 examples) [149.04s elapsed]. I0514 02:31:26.924692 140121429608256 make_examples_core.py:257] Skip phasing: len(candidates[main_sample]) is 7496. I0514 02:35:53.029588 140121429608256 make_examples_core.py:257] 178012 candidates (194646 examples) [268.70s elapsed]. .... ```. And there's also the relevant command information from the log below (formatted for human consumption). ```. time seq 0 0 | parallel -q --halt 2 \. --line-buffer /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""/shuang_data/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa"" \. --reads ""/shuang_data/HG003.GRCh38.chr20.pFDA_truthv2.bam"" \. --examples ""/tmp/tmpouwynlb0/make_examples.tfrecord@1.gz"" \. --add_hp_channel \. --alt_aligned_pileup ""diff_channels"" \. --max_reads_per_partition ""600"" \. --min_mapping_quality ""1"" \. --parse_sam_aux_fields \. --partition_size ""25000"" \. --phase_reads \. --pileup_image_width ""199"" \. --norealign_reads \. --regions ""chr20"" \. --sort_by_haplotypes \. --track_ref_reads \. --vsc_min_fraction_indels ""0.12"" \. --task {}. ... ```. Here's a snapshot of the output. ```. chr20 64506 . C CTCCAT 44.3 PASS . GT:GQ:DP:AD:VAF:PL 0/1:35:44:20,20:0.454545:44,0,35. chr20 68303 . T C 45.4 PASS . GT:GQ:DP:AD:VAF:PL 0/1:45:42:21,21:0.5:45,0,59. chr20 72765 . T TA 21.8 PASS . GT:GQ:DP:AD:VAF:PL 0/1:22:32:19,12:0.375:21,0,40. chr20 81154 . T G 64.2 PASS . GT:GQ:DP:AD:VAF:PL 0/1:63:39:14,2",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/649
https://github.com/google/deepvariant/issues/649:3136,reliability,doe,doesn,3136,"4692 140121429608256 make_examples_core.py:257] Skip phasing: len(candidates[main_sample]) is 7496. I0514 02:35:53.029588 140121429608256 make_examples_core.py:257] 178012 candidates (194646 examples) [268.70s elapsed]. .... ```. And there's also the relevant command information from the log below (formatted for human consumption). ```. time seq 0 0 | parallel -q --halt 2 \. --line-buffer /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""/shuang_data/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa"" \. --reads ""/shuang_data/HG003.GRCh38.chr20.pFDA_truthv2.bam"" \. --examples ""/tmp/tmpouwynlb0/make_examples.tfrecord@1.gz"" \. --add_hp_channel \. --alt_aligned_pileup ""diff_channels"" \. --max_reads_per_partition ""600"" \. --min_mapping_quality ""1"" \. --parse_sam_aux_fields \. --partition_size ""25000"" \. --phase_reads \. --pileup_image_width ""199"" \. --norealign_reads \. --regions ""chr20"" \. --sort_by_haplotypes \. --track_ref_reads \. --vsc_min_fraction_indels ""0.12"" \. --task {}. ... ```. Here's a snapshot of the output. ```. chr20 64506 . C CTCCAT 44.3 PASS . GT:GQ:DP:AD:VAF:PL 0/1:35:44:20,20:0.454545:44,0,35. chr20 68303 . T C 45.4 PASS . GT:GQ:DP:AD:VAF:PL 0/1:45:42:21,21:0.5:45,0,59. chr20 72765 . T TA 21.8 PASS . GT:GQ:DP:AD:VAF:PL 0/1:22:32:19,12:0.375:21,0,40. chr20 81154 . T G 64.2 PASS . GT:GQ:DP:AD:VAF:PL 0/1:63:39:14,25:0.641026:64,0,68. chr20 82090 . C A 64 PASS . GT:GQ:DP:AD:VAF:PL 0/1:64:37:23,14:0.378378:64,0,77. chr20 82719 . C T 60.9 PASS . GT:GQ:DP:AD:VAF:PL 0/1:60:39:23,16:0.410256:60,0,66. chr20 83158 . C T 62.9 PASS . GT:GQ:DP:AD:VAF:PL 0/1:63:38:22,16:0.421053:62,0,76. chr20 84647 . G T 61.8 PASS . GT:GQ:DP:AD:VAF:PL 0/1:60:41:22,19:0.463415:61,0,66. chr20 85259 . G A 66.9 PASS . GT:GQ:DP:AD:VAF:PL 1/1:63:43:0,43:1:66,64,0. chr20 85729 . G A 59.5 PASS . GT:GQ:DP:AD:VAF:PL 1/1:56:44:0,44:1:59,59,0. ```. The linked PacBio case study also doesn't mention anything about phasing, or any related metric. Could you please help? Thank you! Steve.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/649
https://github.com/google/deepvariant/issues/649:470,safety,log,log,470,"Hi, . I'm sure I'm doing something wrong, but couldn't figure out where that is: the variants I get are not directly phased. I followed the PacBio case study, using the 1.5.0 docker. . Here's the command. ```bash. $ ./run_deepvariant \. --model_type PACBIO \. --ref /shuang_data/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa \. --reads /shuang_data/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf /tmp/only_vcf.vcf.gz \. --regions chr20. ```. What's strange in the log are the following lines. ```. ... I0514 01:48:09.263085 140121429608256 make_examples_core.py:257] 62017 candidates (71070 examples) [121.51s elapsed]. I0514 01:48:23.145400 140121429608256 make_examples_core.py:257] Skip phasing: len(candidates[main_sample]) is 7088. I0514 01:52:54.732045 140121429608256 make_examples_core.py:257] 67303 candidates (76952 examples) [285.47s elapsed]. ... I0514 02:28:55.286200 140121429608256 make_examples_core.py:257] 166834 candidates (181516 examples) [142.03s elapsed]. I0514 02:29:01.686342 140121429608256 make_examples_core.py:257] Skip phasing: len(candidates[main_sample]) is 5953. I0514 02:31:24.326849 140121429608256 make_examples_core.py:257] 172413 candidates (187997 examples) [149.04s elapsed]. I0514 02:31:26.924692 140121429608256 make_examples_core.py:257] Skip phasing: len(candidates[main_sample]) is 7496. I0514 02:35:53.029588 140121429608256 make_examples_core.py:257] 178012 candidates (194646 examples) [268.70s elapsed]. .... ```. And there's also the relevant command information from the log below (formatted for human consumption). ```. time seq 0 0 | parallel -q --halt 2 \. --line-buffer /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""/shuang_data/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa"" \. --reads ""/shuang_data/HG003.GRCh38.chr20.pFDA_truthv2.bam"" \. --examples ""/tmp/tmpouwynlb0/make_examples.tfrecord@1.gz"" \. --add_hp_channel \. --alt_aligned_pileup ""diff_channels"" \. --max_reads_per_partition ""600"" \. --min_mapping_quality ""1",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/649
https://github.com/google/deepvariant/issues/649:1528,safety,log,log,1528,"140121429608256 make_examples_core.py:257] 62017 candidates (71070 examples) [121.51s elapsed]. I0514 01:48:23.145400 140121429608256 make_examples_core.py:257] Skip phasing: len(candidates[main_sample]) is 7088. I0514 01:52:54.732045 140121429608256 make_examples_core.py:257] 67303 candidates (76952 examples) [285.47s elapsed]. ... I0514 02:28:55.286200 140121429608256 make_examples_core.py:257] 166834 candidates (181516 examples) [142.03s elapsed]. I0514 02:29:01.686342 140121429608256 make_examples_core.py:257] Skip phasing: len(candidates[main_sample]) is 5953. I0514 02:31:24.326849 140121429608256 make_examples_core.py:257] 172413 candidates (187997 examples) [149.04s elapsed]. I0514 02:31:26.924692 140121429608256 make_examples_core.py:257] Skip phasing: len(candidates[main_sample]) is 7496. I0514 02:35:53.029588 140121429608256 make_examples_core.py:257] 178012 candidates (194646 examples) [268.70s elapsed]. .... ```. And there's also the relevant command information from the log below (formatted for human consumption). ```. time seq 0 0 | parallel -q --halt 2 \. --line-buffer /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""/shuang_data/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa"" \. --reads ""/shuang_data/HG003.GRCh38.chr20.pFDA_truthv2.bam"" \. --examples ""/tmp/tmpouwynlb0/make_examples.tfrecord@1.gz"" \. --add_hp_channel \. --alt_aligned_pileup ""diff_channels"" \. --max_reads_per_partition ""600"" \. --min_mapping_quality ""1"" \. --parse_sam_aux_fields \. --partition_size ""25000"" \. --phase_reads \. --pileup_image_width ""199"" \. --norealign_reads \. --regions ""chr20"" \. --sort_by_haplotypes \. --track_ref_reads \. --vsc_min_fraction_indels ""0.12"" \. --task {}. ... ```. Here's a snapshot of the output. ```. chr20 64506 . C CTCCAT 44.3 PASS . GT:GQ:DP:AD:VAF:PL 0/1:35:44:20,20:0.454545:44,0,35. chr20 68303 . T C 45.4 PASS . GT:GQ:DP:AD:VAF:PL 0/1:45:42:21,21:0.5:45,0,59. chr20 72765 . T TA 21.8 PASS . GT:GQ:DP:AD:VAF:PL 0/1:22:32:19,12:0.375:21,0,",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/649
https://github.com/google/deepvariant/issues/649:470,security,log,log,470,"Hi, . I'm sure I'm doing something wrong, but couldn't figure out where that is: the variants I get are not directly phased. I followed the PacBio case study, using the 1.5.0 docker. . Here's the command. ```bash. $ ./run_deepvariant \. --model_type PACBIO \. --ref /shuang_data/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa \. --reads /shuang_data/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf /tmp/only_vcf.vcf.gz \. --regions chr20. ```. What's strange in the log are the following lines. ```. ... I0514 01:48:09.263085 140121429608256 make_examples_core.py:257] 62017 candidates (71070 examples) [121.51s elapsed]. I0514 01:48:23.145400 140121429608256 make_examples_core.py:257] Skip phasing: len(candidates[main_sample]) is 7088. I0514 01:52:54.732045 140121429608256 make_examples_core.py:257] 67303 candidates (76952 examples) [285.47s elapsed]. ... I0514 02:28:55.286200 140121429608256 make_examples_core.py:257] 166834 candidates (181516 examples) [142.03s elapsed]. I0514 02:29:01.686342 140121429608256 make_examples_core.py:257] Skip phasing: len(candidates[main_sample]) is 5953. I0514 02:31:24.326849 140121429608256 make_examples_core.py:257] 172413 candidates (187997 examples) [149.04s elapsed]. I0514 02:31:26.924692 140121429608256 make_examples_core.py:257] Skip phasing: len(candidates[main_sample]) is 7496. I0514 02:35:53.029588 140121429608256 make_examples_core.py:257] 178012 candidates (194646 examples) [268.70s elapsed]. .... ```. And there's also the relevant command information from the log below (formatted for human consumption). ```. time seq 0 0 | parallel -q --halt 2 \. --line-buffer /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""/shuang_data/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa"" \. --reads ""/shuang_data/HG003.GRCh38.chr20.pFDA_truthv2.bam"" \. --examples ""/tmp/tmpouwynlb0/make_examples.tfrecord@1.gz"" \. --add_hp_channel \. --alt_aligned_pileup ""diff_channels"" \. --max_reads_per_partition ""600"" \. --min_mapping_quality ""1",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/649
https://github.com/google/deepvariant/issues/649:1528,security,log,log,1528,"140121429608256 make_examples_core.py:257] 62017 candidates (71070 examples) [121.51s elapsed]. I0514 01:48:23.145400 140121429608256 make_examples_core.py:257] Skip phasing: len(candidates[main_sample]) is 7088. I0514 01:52:54.732045 140121429608256 make_examples_core.py:257] 67303 candidates (76952 examples) [285.47s elapsed]. ... I0514 02:28:55.286200 140121429608256 make_examples_core.py:257] 166834 candidates (181516 examples) [142.03s elapsed]. I0514 02:29:01.686342 140121429608256 make_examples_core.py:257] Skip phasing: len(candidates[main_sample]) is 5953. I0514 02:31:24.326849 140121429608256 make_examples_core.py:257] 172413 candidates (187997 examples) [149.04s elapsed]. I0514 02:31:26.924692 140121429608256 make_examples_core.py:257] Skip phasing: len(candidates[main_sample]) is 7496. I0514 02:35:53.029588 140121429608256 make_examples_core.py:257] 178012 candidates (194646 examples) [268.70s elapsed]. .... ```. And there's also the relevant command information from the log below (formatted for human consumption). ```. time seq 0 0 | parallel -q --halt 2 \. --line-buffer /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""/shuang_data/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa"" \. --reads ""/shuang_data/HG003.GRCh38.chr20.pFDA_truthv2.bam"" \. --examples ""/tmp/tmpouwynlb0/make_examples.tfrecord@1.gz"" \. --add_hp_channel \. --alt_aligned_pileup ""diff_channels"" \. --max_reads_per_partition ""600"" \. --min_mapping_quality ""1"" \. --parse_sam_aux_fields \. --partition_size ""25000"" \. --phase_reads \. --pileup_image_width ""199"" \. --norealign_reads \. --regions ""chr20"" \. --sort_by_haplotypes \. --track_ref_reads \. --vsc_min_fraction_indels ""0.12"" \. --task {}. ... ```. Here's a snapshot of the output. ```. chr20 64506 . C CTCCAT 44.3 PASS . GT:GQ:DP:AD:VAF:PL 0/1:35:44:20,20:0.454545:44,0,35. chr20 68303 . T C 45.4 PASS . GT:GQ:DP:AD:VAF:PL 0/1:45:42:21,21:0.5:45,0,59. chr20 72765 . T TA 21.8 PASS . GT:GQ:DP:AD:VAF:PL 0/1:22:32:19,12:0.375:21,0,",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/649
https://github.com/google/deepvariant/issues/649:470,testability,log,log,470,"Hi, . I'm sure I'm doing something wrong, but couldn't figure out where that is: the variants I get are not directly phased. I followed the PacBio case study, using the 1.5.0 docker. . Here's the command. ```bash. $ ./run_deepvariant \. --model_type PACBIO \. --ref /shuang_data/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa \. --reads /shuang_data/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf /tmp/only_vcf.vcf.gz \. --regions chr20. ```. What's strange in the log are the following lines. ```. ... I0514 01:48:09.263085 140121429608256 make_examples_core.py:257] 62017 candidates (71070 examples) [121.51s elapsed]. I0514 01:48:23.145400 140121429608256 make_examples_core.py:257] Skip phasing: len(candidates[main_sample]) is 7088. I0514 01:52:54.732045 140121429608256 make_examples_core.py:257] 67303 candidates (76952 examples) [285.47s elapsed]. ... I0514 02:28:55.286200 140121429608256 make_examples_core.py:257] 166834 candidates (181516 examples) [142.03s elapsed]. I0514 02:29:01.686342 140121429608256 make_examples_core.py:257] Skip phasing: len(candidates[main_sample]) is 5953. I0514 02:31:24.326849 140121429608256 make_examples_core.py:257] 172413 candidates (187997 examples) [149.04s elapsed]. I0514 02:31:26.924692 140121429608256 make_examples_core.py:257] Skip phasing: len(candidates[main_sample]) is 7496. I0514 02:35:53.029588 140121429608256 make_examples_core.py:257] 178012 candidates (194646 examples) [268.70s elapsed]. .... ```. And there's also the relevant command information from the log below (formatted for human consumption). ```. time seq 0 0 | parallel -q --halt 2 \. --line-buffer /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""/shuang_data/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa"" \. --reads ""/shuang_data/HG003.GRCh38.chr20.pFDA_truthv2.bam"" \. --examples ""/tmp/tmpouwynlb0/make_examples.tfrecord@1.gz"" \. --add_hp_channel \. --alt_aligned_pileup ""diff_channels"" \. --max_reads_per_partition ""600"" \. --min_mapping_quality ""1",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/649
https://github.com/google/deepvariant/issues/649:1528,testability,log,log,1528,"140121429608256 make_examples_core.py:257] 62017 candidates (71070 examples) [121.51s elapsed]. I0514 01:48:23.145400 140121429608256 make_examples_core.py:257] Skip phasing: len(candidates[main_sample]) is 7088. I0514 01:52:54.732045 140121429608256 make_examples_core.py:257] 67303 candidates (76952 examples) [285.47s elapsed]. ... I0514 02:28:55.286200 140121429608256 make_examples_core.py:257] 166834 candidates (181516 examples) [142.03s elapsed]. I0514 02:29:01.686342 140121429608256 make_examples_core.py:257] Skip phasing: len(candidates[main_sample]) is 5953. I0514 02:31:24.326849 140121429608256 make_examples_core.py:257] 172413 candidates (187997 examples) [149.04s elapsed]. I0514 02:31:26.924692 140121429608256 make_examples_core.py:257] Skip phasing: len(candidates[main_sample]) is 7496. I0514 02:35:53.029588 140121429608256 make_examples_core.py:257] 178012 candidates (194646 examples) [268.70s elapsed]. .... ```. And there's also the relevant command information from the log below (formatted for human consumption). ```. time seq 0 0 | parallel -q --halt 2 \. --line-buffer /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""/shuang_data/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa"" \. --reads ""/shuang_data/HG003.GRCh38.chr20.pFDA_truthv2.bam"" \. --examples ""/tmp/tmpouwynlb0/make_examples.tfrecord@1.gz"" \. --add_hp_channel \. --alt_aligned_pileup ""diff_channels"" \. --max_reads_per_partition ""600"" \. --min_mapping_quality ""1"" \. --parse_sam_aux_fields \. --partition_size ""25000"" \. --phase_reads \. --pileup_image_width ""199"" \. --norealign_reads \. --regions ""chr20"" \. --sort_by_haplotypes \. --track_ref_reads \. --vsc_min_fraction_indels ""0.12"" \. --task {}. ... ```. Here's a snapshot of the output. ```. chr20 64506 . C CTCCAT 44.3 PASS . GT:GQ:DP:AD:VAF:PL 0/1:35:44:20,20:0.454545:44,0,35. chr20 68303 . T C 45.4 PASS . GT:GQ:DP:AD:VAF:PL 0/1:45:42:21,21:0.5:45,0,59. chr20 72765 . T TA 21.8 PASS . GT:GQ:DP:AD:VAF:PL 0/1:22:32:19,12:0.375:21,0,",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/649
https://github.com/google/deepvariant/issues/649:196,usability,command,command,196,"Hi, . I'm sure I'm doing something wrong, but couldn't figure out where that is: the variants I get are not directly phased. I followed the PacBio case study, using the 1.5.0 docker. . Here's the command. ```bash. $ ./run_deepvariant \. --model_type PACBIO \. --ref /shuang_data/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa \. --reads /shuang_data/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf /tmp/only_vcf.vcf.gz \. --regions chr20. ```. What's strange in the log are the following lines. ```. ... I0514 01:48:09.263085 140121429608256 make_examples_core.py:257] 62017 candidates (71070 examples) [121.51s elapsed]. I0514 01:48:23.145400 140121429608256 make_examples_core.py:257] Skip phasing: len(candidates[main_sample]) is 7088. I0514 01:52:54.732045 140121429608256 make_examples_core.py:257] 67303 candidates (76952 examples) [285.47s elapsed]. ... I0514 02:28:55.286200 140121429608256 make_examples_core.py:257] 166834 candidates (181516 examples) [142.03s elapsed]. I0514 02:29:01.686342 140121429608256 make_examples_core.py:257] Skip phasing: len(candidates[main_sample]) is 5953. I0514 02:31:24.326849 140121429608256 make_examples_core.py:257] 172413 candidates (187997 examples) [149.04s elapsed]. I0514 02:31:26.924692 140121429608256 make_examples_core.py:257] Skip phasing: len(candidates[main_sample]) is 7496. I0514 02:35:53.029588 140121429608256 make_examples_core.py:257] 178012 candidates (194646 examples) [268.70s elapsed]. .... ```. And there's also the relevant command information from the log below (formatted for human consumption). ```. time seq 0 0 | parallel -q --halt 2 \. --line-buffer /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""/shuang_data/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa"" \. --reads ""/shuang_data/HG003.GRCh38.chr20.pFDA_truthv2.bam"" \. --examples ""/tmp/tmpouwynlb0/make_examples.tfrecord@1.gz"" \. --add_hp_channel \. --alt_aligned_pileup ""diff_channels"" \. --max_reads_per_partition ""600"" \. --min_mapping_quality ""1",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/649
https://github.com/google/deepvariant/issues/649:1499,usability,command,command,1499," ... I0514 01:48:09.263085 140121429608256 make_examples_core.py:257] 62017 candidates (71070 examples) [121.51s elapsed]. I0514 01:48:23.145400 140121429608256 make_examples_core.py:257] Skip phasing: len(candidates[main_sample]) is 7088. I0514 01:52:54.732045 140121429608256 make_examples_core.py:257] 67303 candidates (76952 examples) [285.47s elapsed]. ... I0514 02:28:55.286200 140121429608256 make_examples_core.py:257] 166834 candidates (181516 examples) [142.03s elapsed]. I0514 02:29:01.686342 140121429608256 make_examples_core.py:257] Skip phasing: len(candidates[main_sample]) is 5953. I0514 02:31:24.326849 140121429608256 make_examples_core.py:257] 172413 candidates (187997 examples) [149.04s elapsed]. I0514 02:31:26.924692 140121429608256 make_examples_core.py:257] Skip phasing: len(candidates[main_sample]) is 7496. I0514 02:35:53.029588 140121429608256 make_examples_core.py:257] 178012 candidates (194646 examples) [268.70s elapsed]. .... ```. And there's also the relevant command information from the log below (formatted for human consumption). ```. time seq 0 0 | parallel -q --halt 2 \. --line-buffer /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""/shuang_data/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa"" \. --reads ""/shuang_data/HG003.GRCh38.chr20.pFDA_truthv2.bam"" \. --examples ""/tmp/tmpouwynlb0/make_examples.tfrecord@1.gz"" \. --add_hp_channel \. --alt_aligned_pileup ""diff_channels"" \. --max_reads_per_partition ""600"" \. --min_mapping_quality ""1"" \. --parse_sam_aux_fields \. --partition_size ""25000"" \. --phase_reads \. --pileup_image_width ""199"" \. --norealign_reads \. --regions ""chr20"" \. --sort_by_haplotypes \. --track_ref_reads \. --vsc_min_fraction_indels ""0.12"" \. --task {}. ... ```. Here's a snapshot of the output. ```. chr20 64506 . C CTCCAT 44.3 PASS . GT:GQ:DP:AD:VAF:PL 0/1:35:44:20,20:0.454545:44,0,35. chr20 68303 . T C 45.4 PASS . GT:GQ:DP:AD:VAF:PL 0/1:45:42:21,21:0.5:45,0,59. chr20 72765 . T TA 21.8 PASS . GT:GQ:DP:AD:VAF:PL ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/649
https://github.com/google/deepvariant/issues/649:3216,usability,help,help,3216,"4692 140121429608256 make_examples_core.py:257] Skip phasing: len(candidates[main_sample]) is 7496. I0514 02:35:53.029588 140121429608256 make_examples_core.py:257] 178012 candidates (194646 examples) [268.70s elapsed]. .... ```. And there's also the relevant command information from the log below (formatted for human consumption). ```. time seq 0 0 | parallel -q --halt 2 \. --line-buffer /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""/shuang_data/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa"" \. --reads ""/shuang_data/HG003.GRCh38.chr20.pFDA_truthv2.bam"" \. --examples ""/tmp/tmpouwynlb0/make_examples.tfrecord@1.gz"" \. --add_hp_channel \. --alt_aligned_pileup ""diff_channels"" \. --max_reads_per_partition ""600"" \. --min_mapping_quality ""1"" \. --parse_sam_aux_fields \. --partition_size ""25000"" \. --phase_reads \. --pileup_image_width ""199"" \. --norealign_reads \. --regions ""chr20"" \. --sort_by_haplotypes \. --track_ref_reads \. --vsc_min_fraction_indels ""0.12"" \. --task {}. ... ```. Here's a snapshot of the output. ```. chr20 64506 . C CTCCAT 44.3 PASS . GT:GQ:DP:AD:VAF:PL 0/1:35:44:20,20:0.454545:44,0,35. chr20 68303 . T C 45.4 PASS . GT:GQ:DP:AD:VAF:PL 0/1:45:42:21,21:0.5:45,0,59. chr20 72765 . T TA 21.8 PASS . GT:GQ:DP:AD:VAF:PL 0/1:22:32:19,12:0.375:21,0,40. chr20 81154 . T G 64.2 PASS . GT:GQ:DP:AD:VAF:PL 0/1:63:39:14,25:0.641026:64,0,68. chr20 82090 . C A 64 PASS . GT:GQ:DP:AD:VAF:PL 0/1:64:37:23,14:0.378378:64,0,77. chr20 82719 . C T 60.9 PASS . GT:GQ:DP:AD:VAF:PL 0/1:60:39:23,16:0.410256:60,0,66. chr20 83158 . C T 62.9 PASS . GT:GQ:DP:AD:VAF:PL 0/1:63:38:22,16:0.421053:62,0,76. chr20 84647 . G T 61.8 PASS . GT:GQ:DP:AD:VAF:PL 0/1:60:41:22,19:0.463415:61,0,66. chr20 85259 . G A 66.9 PASS . GT:GQ:DP:AD:VAF:PL 1/1:63:43:0,43:1:66,64,0. chr20 85729 . G A 59.5 PASS . GT:GQ:DP:AD:VAF:PL 1/1:56:44:0,44:1:59,59,0. ```. The linked PacBio case study also doesn't mention anything about phasing, or any related metric. Could you please help? Thank you! Steve.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/649
https://github.com/google/deepvariant/issues/649:62,deployability,log,log,62,"Hi @SHuang-Broad , thanks for your question. This line in the log indicates that you are running with our internal phasing:. ```. I0514 02:29:01.686342 140121429608256 make_examples_core.py:257] Skip phasing: len(candidates[main_sample]) is 5953. ```. (In this case, the phasing algorithm skips some windows based on some threshold). Given that you're using run_deepvariant, you can see that in r1.5, our run_deepvariant.py enables `phase_reads` for the PacBio mode. https://github.com/google/deepvariant/blob/r1.5/scripts/run_deepvariant.py#L247-L260. (Other than `phase_reads`, a few other flags are also necessary. For example: `track_ref_reads`, `sort_by_haplotypes`, `parse_sam_aux_fields`, And `partition_size` to cover enough candidates.). If you're running make_examples separately, please make sure to add these flags. You can use the `--dry_run` flag of run_deepvariant to figure out which flags to add. Note that our phasing is intended for use in DeepVariant accuracy only (so that our users don't need to add an extra step like before). We don't intend for our phasing results to be used like a standalone phasing tool. That is why we didn't explicitly expose the phased results. If you're not seeing the accuracy we reported, or the accuracy you see on other samples look unexpected, please let us know! By the way, if you really want to double check the phasing results, you can save the make_examples results using the --intermediate_results_dir flag. And then use our https://github.com/google/deepvariant/blob/r1.5/docs/show-examples.md tools to visualize the images. You should able to see a channel with the sorted HP values. Note that not all windows would be phased, but if you look through a few of them you should be able to check. If you're curious about the phasing algorithm itself, @akolesnikov and @kishwarshafin are working on writing up a bit more of the details. That might take a while to finish, but we can let you know when we have some more details to share.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/649
https://github.com/google/deepvariant/issues/649:1988,interoperability,share,share,1988,"Hi @SHuang-Broad , thanks for your question. This line in the log indicates that you are running with our internal phasing:. ```. I0514 02:29:01.686342 140121429608256 make_examples_core.py:257] Skip phasing: len(candidates[main_sample]) is 5953. ```. (In this case, the phasing algorithm skips some windows based on some threshold). Given that you're using run_deepvariant, you can see that in r1.5, our run_deepvariant.py enables `phase_reads` for the PacBio mode. https://github.com/google/deepvariant/blob/r1.5/scripts/run_deepvariant.py#L247-L260. (Other than `phase_reads`, a few other flags are also necessary. For example: `track_ref_reads`, `sort_by_haplotypes`, `parse_sam_aux_fields`, And `partition_size` to cover enough candidates.). If you're running make_examples separately, please make sure to add these flags. You can use the `--dry_run` flag of run_deepvariant to figure out which flags to add. Note that our phasing is intended for use in DeepVariant accuracy only (so that our users don't need to add an extra step like before). We don't intend for our phasing results to be used like a standalone phasing tool. That is why we didn't explicitly expose the phased results. If you're not seeing the accuracy we reported, or the accuracy you see on other samples look unexpected, please let us know! By the way, if you really want to double check the phasing results, you can save the make_examples results using the --intermediate_results_dir flag. And then use our https://github.com/google/deepvariant/blob/r1.5/docs/show-examples.md tools to visualize the images. You should able to see a channel with the sorted HP values. Note that not all windows would be phased, but if you look through a few of them you should be able to check. If you're curious about the phasing algorithm itself, @akolesnikov and @kishwarshafin are working on writing up a bit more of the details. That might take a while to finish, but we can let you know when we have some more details to share.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/649
https://github.com/google/deepvariant/issues/649:454,modifiability,Pac,PacBio,454,"Hi @SHuang-Broad , thanks for your question. This line in the log indicates that you are running with our internal phasing:. ```. I0514 02:29:01.686342 140121429608256 make_examples_core.py:257] Skip phasing: len(candidates[main_sample]) is 5953. ```. (In this case, the phasing algorithm skips some windows based on some threshold). Given that you're using run_deepvariant, you can see that in r1.5, our run_deepvariant.py enables `phase_reads` for the PacBio mode. https://github.com/google/deepvariant/blob/r1.5/scripts/run_deepvariant.py#L247-L260. (Other than `phase_reads`, a few other flags are also necessary. For example: `track_ref_reads`, `sort_by_haplotypes`, `parse_sam_aux_fields`, And `partition_size` to cover enough candidates.). If you're running make_examples separately, please make sure to add these flags. You can use the `--dry_run` flag of run_deepvariant to figure out which flags to add. Note that our phasing is intended for use in DeepVariant accuracy only (so that our users don't need to add an extra step like before). We don't intend for our phasing results to be used like a standalone phasing tool. That is why we didn't explicitly expose the phased results. If you're not seeing the accuracy we reported, or the accuracy you see on other samples look unexpected, please let us know! By the way, if you really want to double check the phasing results, you can save the make_examples results using the --intermediate_results_dir flag. And then use our https://github.com/google/deepvariant/blob/r1.5/docs/show-examples.md tools to visualize the images. You should able to see a channel with the sorted HP values. Note that not all windows would be phased, but if you look through a few of them you should be able to check. If you're curious about the phasing algorithm itself, @akolesnikov and @kishwarshafin are working on writing up a bit more of the details. That might take a while to finish, but we can let you know when we have some more details to share.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/649
https://github.com/google/deepvariant/issues/649:62,safety,log,log,62,"Hi @SHuang-Broad , thanks for your question. This line in the log indicates that you are running with our internal phasing:. ```. I0514 02:29:01.686342 140121429608256 make_examples_core.py:257] Skip phasing: len(candidates[main_sample]) is 5953. ```. (In this case, the phasing algorithm skips some windows based on some threshold). Given that you're using run_deepvariant, you can see that in r1.5, our run_deepvariant.py enables `phase_reads` for the PacBio mode. https://github.com/google/deepvariant/blob/r1.5/scripts/run_deepvariant.py#L247-L260. (Other than `phase_reads`, a few other flags are also necessary. For example: `track_ref_reads`, `sort_by_haplotypes`, `parse_sam_aux_fields`, And `partition_size` to cover enough candidates.). If you're running make_examples separately, please make sure to add these flags. You can use the `--dry_run` flag of run_deepvariant to figure out which flags to add. Note that our phasing is intended for use in DeepVariant accuracy only (so that our users don't need to add an extra step like before). We don't intend for our phasing results to be used like a standalone phasing tool. That is why we didn't explicitly expose the phased results. If you're not seeing the accuracy we reported, or the accuracy you see on other samples look unexpected, please let us know! By the way, if you really want to double check the phasing results, you can save the make_examples results using the --intermediate_results_dir flag. And then use our https://github.com/google/deepvariant/blob/r1.5/docs/show-examples.md tools to visualize the images. You should able to see a channel with the sorted HP values. Note that not all windows would be phased, but if you look through a few of them you should be able to check. If you're curious about the phasing algorithm itself, @akolesnikov and @kishwarshafin are working on writing up a bit more of the details. That might take a while to finish, but we can let you know when we have some more details to share.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/649
https://github.com/google/deepvariant/issues/649:62,security,log,log,62,"Hi @SHuang-Broad , thanks for your question. This line in the log indicates that you are running with our internal phasing:. ```. I0514 02:29:01.686342 140121429608256 make_examples_core.py:257] Skip phasing: len(candidates[main_sample]) is 5953. ```. (In this case, the phasing algorithm skips some windows based on some threshold). Given that you're using run_deepvariant, you can see that in r1.5, our run_deepvariant.py enables `phase_reads` for the PacBio mode. https://github.com/google/deepvariant/blob/r1.5/scripts/run_deepvariant.py#L247-L260. (Other than `phase_reads`, a few other flags are also necessary. For example: `track_ref_reads`, `sort_by_haplotypes`, `parse_sam_aux_fields`, And `partition_size` to cover enough candidates.). If you're running make_examples separately, please make sure to add these flags. You can use the `--dry_run` flag of run_deepvariant to figure out which flags to add. Note that our phasing is intended for use in DeepVariant accuracy only (so that our users don't need to add an extra step like before). We don't intend for our phasing results to be used like a standalone phasing tool. That is why we didn't explicitly expose the phased results. If you're not seeing the accuracy we reported, or the accuracy you see on other samples look unexpected, please let us know! By the way, if you really want to double check the phasing results, you can save the make_examples results using the --intermediate_results_dir flag. And then use our https://github.com/google/deepvariant/blob/r1.5/docs/show-examples.md tools to visualize the images. You should able to see a channel with the sorted HP values. Note that not all windows would be phased, but if you look through a few of them you should be able to check. If you're curious about the phasing algorithm itself, @akolesnikov and @kishwarshafin are working on writing up a bit more of the details. That might take a while to finish, but we can let you know when we have some more details to share.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/649
https://github.com/google/deepvariant/issues/649:1166,security,expos,expose,1166,"Hi @SHuang-Broad , thanks for your question. This line in the log indicates that you are running with our internal phasing:. ```. I0514 02:29:01.686342 140121429608256 make_examples_core.py:257] Skip phasing: len(candidates[main_sample]) is 5953. ```. (In this case, the phasing algorithm skips some windows based on some threshold). Given that you're using run_deepvariant, you can see that in r1.5, our run_deepvariant.py enables `phase_reads` for the PacBio mode. https://github.com/google/deepvariant/blob/r1.5/scripts/run_deepvariant.py#L247-L260. (Other than `phase_reads`, a few other flags are also necessary. For example: `track_ref_reads`, `sort_by_haplotypes`, `parse_sam_aux_fields`, And `partition_size` to cover enough candidates.). If you're running make_examples separately, please make sure to add these flags. You can use the `--dry_run` flag of run_deepvariant to figure out which flags to add. Note that our phasing is intended for use in DeepVariant accuracy only (so that our users don't need to add an extra step like before). We don't intend for our phasing results to be used like a standalone phasing tool. That is why we didn't explicitly expose the phased results. If you're not seeing the accuracy we reported, or the accuracy you see on other samples look unexpected, please let us know! By the way, if you really want to double check the phasing results, you can save the make_examples results using the --intermediate_results_dir flag. And then use our https://github.com/google/deepvariant/blob/r1.5/docs/show-examples.md tools to visualize the images. You should able to see a channel with the sorted HP values. Note that not all windows would be phased, but if you look through a few of them you should be able to check. If you're curious about the phasing algorithm itself, @akolesnikov and @kishwarshafin are working on writing up a bit more of the details. That might take a while to finish, but we can let you know when we have some more details to share.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/649
https://github.com/google/deepvariant/issues/649:62,testability,log,log,62,"Hi @SHuang-Broad , thanks for your question. This line in the log indicates that you are running with our internal phasing:. ```. I0514 02:29:01.686342 140121429608256 make_examples_core.py:257] Skip phasing: len(candidates[main_sample]) is 5953. ```. (In this case, the phasing algorithm skips some windows based on some threshold). Given that you're using run_deepvariant, you can see that in r1.5, our run_deepvariant.py enables `phase_reads` for the PacBio mode. https://github.com/google/deepvariant/blob/r1.5/scripts/run_deepvariant.py#L247-L260. (Other than `phase_reads`, a few other flags are also necessary. For example: `track_ref_reads`, `sort_by_haplotypes`, `parse_sam_aux_fields`, And `partition_size` to cover enough candidates.). If you're running make_examples separately, please make sure to add these flags. You can use the `--dry_run` flag of run_deepvariant to figure out which flags to add. Note that our phasing is intended for use in DeepVariant accuracy only (so that our users don't need to add an extra step like before). We don't intend for our phasing results to be used like a standalone phasing tool. That is why we didn't explicitly expose the phased results. If you're not seeing the accuracy we reported, or the accuracy you see on other samples look unexpected, please let us know! By the way, if you really want to double check the phasing results, you can save the make_examples results using the --intermediate_results_dir flag. And then use our https://github.com/google/deepvariant/blob/r1.5/docs/show-examples.md tools to visualize the images. You should able to see a channel with the sorted HP values. Note that not all windows would be phased, but if you look through a few of them you should be able to check. If you're curious about the phasing algorithm itself, @akolesnikov and @kishwarshafin are working on writing up a bit more of the details. That might take a while to finish, but we can let you know when we have some more details to share.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/649
https://github.com/google/deepvariant/issues/649:66,usability,indicat,indicates,66,"Hi @SHuang-Broad , thanks for your question. This line in the log indicates that you are running with our internal phasing:. ```. I0514 02:29:01.686342 140121429608256 make_examples_core.py:257] Skip phasing: len(candidates[main_sample]) is 5953. ```. (In this case, the phasing algorithm skips some windows based on some threshold). Given that you're using run_deepvariant, you can see that in r1.5, our run_deepvariant.py enables `phase_reads` for the PacBio mode. https://github.com/google/deepvariant/blob/r1.5/scripts/run_deepvariant.py#L247-L260. (Other than `phase_reads`, a few other flags are also necessary. For example: `track_ref_reads`, `sort_by_haplotypes`, `parse_sam_aux_fields`, And `partition_size` to cover enough candidates.). If you're running make_examples separately, please make sure to add these flags. You can use the `--dry_run` flag of run_deepvariant to figure out which flags to add. Note that our phasing is intended for use in DeepVariant accuracy only (so that our users don't need to add an extra step like before). We don't intend for our phasing results to be used like a standalone phasing tool. That is why we didn't explicitly expose the phased results. If you're not seeing the accuracy we reported, or the accuracy you see on other samples look unexpected, please let us know! By the way, if you really want to double check the phasing results, you can save the make_examples results using the --intermediate_results_dir flag. And then use our https://github.com/google/deepvariant/blob/r1.5/docs/show-examples.md tools to visualize the images. You should able to see a channel with the sorted HP values. Note that not all windows would be phased, but if you look through a few of them you should be able to check. If you're curious about the phasing algorithm itself, @akolesnikov and @kishwarshafin are working on writing up a bit more of the details. That might take a while to finish, but we can let you know when we have some more details to share.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/649
https://github.com/google/deepvariant/issues/649:998,usability,user,users,998,"Hi @SHuang-Broad , thanks for your question. This line in the log indicates that you are running with our internal phasing:. ```. I0514 02:29:01.686342 140121429608256 make_examples_core.py:257] Skip phasing: len(candidates[main_sample]) is 5953. ```. (In this case, the phasing algorithm skips some windows based on some threshold). Given that you're using run_deepvariant, you can see that in r1.5, our run_deepvariant.py enables `phase_reads` for the PacBio mode. https://github.com/google/deepvariant/blob/r1.5/scripts/run_deepvariant.py#L247-L260. (Other than `phase_reads`, a few other flags are also necessary. For example: `track_ref_reads`, `sort_by_haplotypes`, `parse_sam_aux_fields`, And `partition_size` to cover enough candidates.). If you're running make_examples separately, please make sure to add these flags. You can use the `--dry_run` flag of run_deepvariant to figure out which flags to add. Note that our phasing is intended for use in DeepVariant accuracy only (so that our users don't need to add an extra step like before). We don't intend for our phasing results to be used like a standalone phasing tool. That is why we didn't explicitly expose the phased results. If you're not seeing the accuracy we reported, or the accuracy you see on other samples look unexpected, please let us know! By the way, if you really want to double check the phasing results, you can save the make_examples results using the --intermediate_results_dir flag. And then use our https://github.com/google/deepvariant/blob/r1.5/docs/show-examples.md tools to visualize the images. You should able to see a channel with the sorted HP values. Note that not all windows would be phased, but if you look through a few of them you should be able to check. If you're curious about the phasing algorithm itself, @akolesnikov and @kishwarshafin are working on writing up a bit more of the details. That might take a while to finish, but we can let you know when we have some more details to share.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/649
https://github.com/google/deepvariant/issues/649:1127,usability,tool,tool,1127,"Hi @SHuang-Broad , thanks for your question. This line in the log indicates that you are running with our internal phasing:. ```. I0514 02:29:01.686342 140121429608256 make_examples_core.py:257] Skip phasing: len(candidates[main_sample]) is 5953. ```. (In this case, the phasing algorithm skips some windows based on some threshold). Given that you're using run_deepvariant, you can see that in r1.5, our run_deepvariant.py enables `phase_reads` for the PacBio mode. https://github.com/google/deepvariant/blob/r1.5/scripts/run_deepvariant.py#L247-L260. (Other than `phase_reads`, a few other flags are also necessary. For example: `track_ref_reads`, `sort_by_haplotypes`, `parse_sam_aux_fields`, And `partition_size` to cover enough candidates.). If you're running make_examples separately, please make sure to add these flags. You can use the `--dry_run` flag of run_deepvariant to figure out which flags to add. Note that our phasing is intended for use in DeepVariant accuracy only (so that our users don't need to add an extra step like before). We don't intend for our phasing results to be used like a standalone phasing tool. That is why we didn't explicitly expose the phased results. If you're not seeing the accuracy we reported, or the accuracy you see on other samples look unexpected, please let us know! By the way, if you really want to double check the phasing results, you can save the make_examples results using the --intermediate_results_dir flag. And then use our https://github.com/google/deepvariant/blob/r1.5/docs/show-examples.md tools to visualize the images. You should able to see a channel with the sorted HP values. Note that not all windows would be phased, but if you look through a few of them you should be able to check. If you're curious about the phasing algorithm itself, @akolesnikov and @kishwarshafin are working on writing up a bit more of the details. That might take a while to finish, but we can let you know when we have some more details to share.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/649
https://github.com/google/deepvariant/issues/649:1555,usability,tool,tools,1555,"Hi @SHuang-Broad , thanks for your question. This line in the log indicates that you are running with our internal phasing:. ```. I0514 02:29:01.686342 140121429608256 make_examples_core.py:257] Skip phasing: len(candidates[main_sample]) is 5953. ```. (In this case, the phasing algorithm skips some windows based on some threshold). Given that you're using run_deepvariant, you can see that in r1.5, our run_deepvariant.py enables `phase_reads` for the PacBio mode. https://github.com/google/deepvariant/blob/r1.5/scripts/run_deepvariant.py#L247-L260. (Other than `phase_reads`, a few other flags are also necessary. For example: `track_ref_reads`, `sort_by_haplotypes`, `parse_sam_aux_fields`, And `partition_size` to cover enough candidates.). If you're running make_examples separately, please make sure to add these flags. You can use the `--dry_run` flag of run_deepvariant to figure out which flags to add. Note that our phasing is intended for use in DeepVariant accuracy only (so that our users don't need to add an extra step like before). We don't intend for our phasing results to be used like a standalone phasing tool. That is why we didn't explicitly expose the phased results. If you're not seeing the accuracy we reported, or the accuracy you see on other samples look unexpected, please let us know! By the way, if you really want to double check the phasing results, you can save the make_examples results using the --intermediate_results_dir flag. And then use our https://github.com/google/deepvariant/blob/r1.5/docs/show-examples.md tools to visualize the images. You should able to see a channel with the sorted HP values. Note that not all windows would be phased, but if you look through a few of them you should be able to check. If you're curious about the phasing algorithm itself, @akolesnikov and @kishwarshafin are working on writing up a bit more of the details. That might take a while to finish, but we can let you know when we have some more details to share.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/649
https://github.com/google/deepvariant/issues/649:1564,usability,visual,visualize,1564,"Hi @SHuang-Broad , thanks for your question. This line in the log indicates that you are running with our internal phasing:. ```. I0514 02:29:01.686342 140121429608256 make_examples_core.py:257] Skip phasing: len(candidates[main_sample]) is 5953. ```. (In this case, the phasing algorithm skips some windows based on some threshold). Given that you're using run_deepvariant, you can see that in r1.5, our run_deepvariant.py enables `phase_reads` for the PacBio mode. https://github.com/google/deepvariant/blob/r1.5/scripts/run_deepvariant.py#L247-L260. (Other than `phase_reads`, a few other flags are also necessary. For example: `track_ref_reads`, `sort_by_haplotypes`, `parse_sam_aux_fields`, And `partition_size` to cover enough candidates.). If you're running make_examples separately, please make sure to add these flags. You can use the `--dry_run` flag of run_deepvariant to figure out which flags to add. Note that our phasing is intended for use in DeepVariant accuracy only (so that our users don't need to add an extra step like before). We don't intend for our phasing results to be used like a standalone phasing tool. That is why we didn't explicitly expose the phased results. If you're not seeing the accuracy we reported, or the accuracy you see on other samples look unexpected, please let us know! By the way, if you really want to double check the phasing results, you can save the make_examples results using the --intermediate_results_dir flag. And then use our https://github.com/google/deepvariant/blob/r1.5/docs/show-examples.md tools to visualize the images. You should able to see a channel with the sorted HP values. Note that not all windows would be phased, but if you look through a few of them you should be able to check. If you're curious about the phasing algorithm itself, @akolesnikov and @kishwarshafin are working on writing up a bit more of the details. That might take a while to finish, but we can let you know when we have some more details to share.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/649
https://github.com/google/deepvariant/issues/649:100,deployability,version,version,100,"Hello Pi-Chuan, thanks for answer above. Just to make it clear to me... I obtained a VCF file using version 1.4.0, but I cannot see the phase data (e.g., ""|"") in my sample. Does this mean that I need to call variants again using version 1.5.0 and the ""--phase_reads"" flag? Or does DeepVariant not explicitly provide this data, requiring me to run Whatshap + DeepVariant to visualize it?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/649
https://github.com/google/deepvariant/issues/649:229,deployability,version,version,229,"Hello Pi-Chuan, thanks for answer above. Just to make it clear to me... I obtained a VCF file using version 1.4.0, but I cannot see the phase data (e.g., ""|"") in my sample. Does this mean that I need to call variants again using version 1.5.0 and the ""--phase_reads"" flag? Or does DeepVariant not explicitly provide this data, requiring me to run Whatshap + DeepVariant to visualize it?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/649
https://github.com/google/deepvariant/issues/649:100,integrability,version,version,100,"Hello Pi-Chuan, thanks for answer above. Just to make it clear to me... I obtained a VCF file using version 1.4.0, but I cannot see the phase data (e.g., ""|"") in my sample. Does this mean that I need to call variants again using version 1.5.0 and the ""--phase_reads"" flag? Or does DeepVariant not explicitly provide this data, requiring me to run Whatshap + DeepVariant to visualize it?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/649
https://github.com/google/deepvariant/issues/649:229,integrability,version,version,229,"Hello Pi-Chuan, thanks for answer above. Just to make it clear to me... I obtained a VCF file using version 1.4.0, but I cannot see the phase data (e.g., ""|"") in my sample. Does this mean that I need to call variants again using version 1.5.0 and the ""--phase_reads"" flag? Or does DeepVariant not explicitly provide this data, requiring me to run Whatshap + DeepVariant to visualize it?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/649
https://github.com/google/deepvariant/issues/649:100,modifiability,version,version,100,"Hello Pi-Chuan, thanks for answer above. Just to make it clear to me... I obtained a VCF file using version 1.4.0, but I cannot see the phase data (e.g., ""|"") in my sample. Does this mean that I need to call variants again using version 1.5.0 and the ""--phase_reads"" flag? Or does DeepVariant not explicitly provide this data, requiring me to run Whatshap + DeepVariant to visualize it?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/649
https://github.com/google/deepvariant/issues/649:229,modifiability,version,version,229,"Hello Pi-Chuan, thanks for answer above. Just to make it clear to me... I obtained a VCF file using version 1.4.0, but I cannot see the phase data (e.g., ""|"") in my sample. Does this mean that I need to call variants again using version 1.5.0 and the ""--phase_reads"" flag? Or does DeepVariant not explicitly provide this data, requiring me to run Whatshap + DeepVariant to visualize it?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/649
https://github.com/google/deepvariant/issues/649:173,reliability,Doe,Does,173,"Hello Pi-Chuan, thanks for answer above. Just to make it clear to me... I obtained a VCF file using version 1.4.0, but I cannot see the phase data (e.g., ""|"") in my sample. Does this mean that I need to call variants again using version 1.5.0 and the ""--phase_reads"" flag? Or does DeepVariant not explicitly provide this data, requiring me to run Whatshap + DeepVariant to visualize it?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/649
https://github.com/google/deepvariant/issues/649:276,reliability,doe,does,276,"Hello Pi-Chuan, thanks for answer above. Just to make it clear to me... I obtained a VCF file using version 1.4.0, but I cannot see the phase data (e.g., ""|"") in my sample. Does this mean that I need to call variants again using version 1.5.0 and the ""--phase_reads"" flag? Or does DeepVariant not explicitly provide this data, requiring me to run Whatshap + DeepVariant to visualize it?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/649
https://github.com/google/deepvariant/issues/649:57,usability,clear,clear,57,"Hello Pi-Chuan, thanks for answer above. Just to make it clear to me... I obtained a VCF file using version 1.4.0, but I cannot see the phase data (e.g., ""|"") in my sample. Does this mean that I need to call variants again using version 1.5.0 and the ""--phase_reads"" flag? Or does DeepVariant not explicitly provide this data, requiring me to run Whatshap + DeepVariant to visualize it?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/649
https://github.com/google/deepvariant/issues/649:373,usability,visual,visualize,373,"Hello Pi-Chuan, thanks for answer above. Just to make it clear to me... I obtained a VCF file using version 1.4.0, but I cannot see the phase data (e.g., ""|"") in my sample. Does this mean that I need to call variants again using version 1.5.0 and the ""--phase_reads"" flag? Or does DeepVariant not explicitly provide this data, requiring me to run Whatshap + DeepVariant to visualize it?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/649
https://github.com/google/deepvariant/issues/649:516,availability,avail,available,516,"Thanks for the explanations, @pichuan ! Just to re-iterate to make sure I can understand, . - it is not expected that DV 1.4.0 and 1.5.0 will output phased variants, i.e. the GT field should still look like 0/1 not 0|1. . - Instead, phasing is done internally to DV (like the PEPPER step), so that accuracy can benefit. . - Therefore we should still run a standalone phasing tool (e.g. whatshap, margin phase) to phase the outputs of DV 1.4.0 and 1.5.0, if phasing is needed. - And internal phasing results are also available if one follows your suggestion above (i.e. peeking under the hood). Is this correct? Also, looking forward to the new doc explaining the phasing! Thanks,. Steve.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/649
https://github.com/google/deepvariant/issues/649:516,reliability,availab,available,516,"Thanks for the explanations, @pichuan ! Just to re-iterate to make sure I can understand, . - it is not expected that DV 1.4.0 and 1.5.0 will output phased variants, i.e. the GT field should still look like 0/1 not 0|1. . - Instead, phasing is done internally to DV (like the PEPPER step), so that accuracy can benefit. . - Therefore we should still run a standalone phasing tool (e.g. whatshap, margin phase) to phase the outputs of DV 1.4.0 and 1.5.0, if phasing is needed. - And internal phasing results are also available if one follows your suggestion above (i.e. peeking under the hood). Is this correct? Also, looking forward to the new doc explaining the phasing! Thanks,. Steve.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/649
https://github.com/google/deepvariant/issues/649:516,safety,avail,available,516,"Thanks for the explanations, @pichuan ! Just to re-iterate to make sure I can understand, . - it is not expected that DV 1.4.0 and 1.5.0 will output phased variants, i.e. the GT field should still look like 0/1 not 0|1. . - Instead, phasing is done internally to DV (like the PEPPER step), so that accuracy can benefit. . - Therefore we should still run a standalone phasing tool (e.g. whatshap, margin phase) to phase the outputs of DV 1.4.0 and 1.5.0, if phasing is needed. - And internal phasing results are also available if one follows your suggestion above (i.e. peeking under the hood). Is this correct? Also, looking forward to the new doc explaining the phasing! Thanks,. Steve.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/649
https://github.com/google/deepvariant/issues/649:516,security,availab,available,516,"Thanks for the explanations, @pichuan ! Just to re-iterate to make sure I can understand, . - it is not expected that DV 1.4.0 and 1.5.0 will output phased variants, i.e. the GT field should still look like 0/1 not 0|1. . - Instead, phasing is done internally to DV (like the PEPPER step), so that accuracy can benefit. . - Therefore we should still run a standalone phasing tool (e.g. whatshap, margin phase) to phase the outputs of DV 1.4.0 and 1.5.0, if phasing is needed. - And internal phasing results are also available if one follows your suggestion above (i.e. peeking under the hood). Is this correct? Also, looking forward to the new doc explaining the phasing! Thanks,. Steve.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/649
https://github.com/google/deepvariant/issues/649:78,testability,understand,understand,78,"Thanks for the explanations, @pichuan ! Just to re-iterate to make sure I can understand, . - it is not expected that DV 1.4.0 and 1.5.0 will output phased variants, i.e. the GT field should still look like 0/1 not 0|1. . - Instead, phasing is done internally to DV (like the PEPPER step), so that accuracy can benefit. . - Therefore we should still run a standalone phasing tool (e.g. whatshap, margin phase) to phase the outputs of DV 1.4.0 and 1.5.0, if phasing is needed. - And internal phasing results are also available if one follows your suggestion above (i.e. peeking under the hood). Is this correct? Also, looking forward to the new doc explaining the phasing! Thanks,. Steve.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/649
https://github.com/google/deepvariant/issues/649:375,usability,tool,tool,375,"Thanks for the explanations, @pichuan ! Just to re-iterate to make sure I can understand, . - it is not expected that DV 1.4.0 and 1.5.0 will output phased variants, i.e. the GT field should still look like 0/1 not 0|1. . - Instead, phasing is done internally to DV (like the PEPPER step), so that accuracy can benefit. . - Therefore we should still run a standalone phasing tool (e.g. whatshap, margin phase) to phase the outputs of DV 1.4.0 and 1.5.0, if phasing is needed. - And internal phasing results are also available if one follows your suggestion above (i.e. peeking under the hood). Is this correct? Also, looking forward to the new doc explaining the phasing! Thanks,. Steve.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/649
https://github.com/google/deepvariant/issues/649:46,deployability,automat,automated,46,"@SHuang-Broad ,. You are correct. The phasing automated within DeepVariant is actually the haplotagging step that was manually done with whatshap before with `whatshap haplotag`. So, it's read haplotagging that is automated not variant phasing. If you want to phase the variants, then you need to run either whatshap or margin on the output of DeepVariant to obtain the phased variants.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/649
https://github.com/google/deepvariant/issues/649:214,deployability,automat,automated,214,"@SHuang-Broad ,. You are correct. The phasing automated within DeepVariant is actually the haplotagging step that was manually done with whatshap before with `whatshap haplotag`. So, it's read haplotagging that is automated not variant phasing. If you want to phase the variants, then you need to run either whatshap or margin on the output of DeepVariant to obtain the phased variants.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/649
https://github.com/google/deepvariant/issues/649:46,testability,automat,automated,46,"@SHuang-Broad ,. You are correct. The phasing automated within DeepVariant is actually the haplotagging step that was manually done with whatshap before with `whatshap haplotag`. So, it's read haplotagging that is automated not variant phasing. If you want to phase the variants, then you need to run either whatshap or margin on the output of DeepVariant to obtain the phased variants.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/649
https://github.com/google/deepvariant/issues/649:214,testability,automat,automated,214,"@SHuang-Broad ,. You are correct. The phasing automated within DeepVariant is actually the haplotagging step that was manually done with whatshap before with `whatshap haplotag`. So, it's read haplotagging that is automated not variant phasing. If you want to phase the variants, then you need to run either whatshap or margin on the output of DeepVariant to obtain the phased variants.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/649
https://github.com/google/deepvariant/issues/649:190,deployability,automat,automated,190,"Here Itself two contradictory Answers, . @danielecook says No need of Phasing as it is ""Direct phasing"" in v1.4 No longger need of `whatshap`, while @kishwarshafin says only haplotagging is automated **""Not variant phasing""**. Which is is right by the way.?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/649
https://github.com/google/deepvariant/issues/649:190,testability,automat,automated,190,"Here Itself two contradictory Answers, . @danielecook says No need of Phasing as it is ""Direct phasing"" in v1.4 No longger need of `whatshap`, while @kishwarshafin says only haplotagging is automated **""Not variant phasing""**. Which is is right by the way.?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/649
https://github.com/google/deepvariant/issues/649:269,energy efficiency,current,currently,269,"Sorry for the confusion here. We're phasing the reads internally to help with DeepVariant accuracy. (""Direct"" is referring to that we're doing this now directly in DeepVariant instead of relying on external HP tags from other tools like WhatsHap). However, DeepVariant currently still doesn't generate phased variant calls. Therefore you won't see ""|"" in the VCF files. Hopefully this is more clear.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/649
https://github.com/google/deepvariant/issues/649:285,reliability,doe,doesn,285,"Sorry for the confusion here. We're phasing the reads internally to help with DeepVariant accuracy. (""Direct"" is referring to that we're doing this now directly in DeepVariant instead of relying on external HP tags from other tools like WhatsHap). However, DeepVariant currently still doesn't generate phased variant calls. Therefore you won't see ""|"" in the VCF files. Hopefully this is more clear.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/649
https://github.com/google/deepvariant/issues/649:68,usability,help,help,68,"Sorry for the confusion here. We're phasing the reads internally to help with DeepVariant accuracy. (""Direct"" is referring to that we're doing this now directly in DeepVariant instead of relying on external HP tags from other tools like WhatsHap). However, DeepVariant currently still doesn't generate phased variant calls. Therefore you won't see ""|"" in the VCF files. Hopefully this is more clear.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/649
https://github.com/google/deepvariant/issues/649:226,usability,tool,tools,226,"Sorry for the confusion here. We're phasing the reads internally to help with DeepVariant accuracy. (""Direct"" is referring to that we're doing this now directly in DeepVariant instead of relying on external HP tags from other tools like WhatsHap). However, DeepVariant currently still doesn't generate phased variant calls. Therefore you won't see ""|"" in the VCF files. Hopefully this is more clear.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/649
https://github.com/google/deepvariant/issues/649:393,usability,clear,clear,393,"Sorry for the confusion here. We're phasing the reads internally to help with DeepVariant accuracy. (""Direct"" is referring to that we're doing this now directly in DeepVariant instead of relying on external HP tags from other tools like WhatsHap). However, DeepVariant currently still doesn't generate phased variant calls. Therefore you won't see ""|"" in the VCF files. Hopefully this is more clear.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/649
https://github.com/google/deepvariant/issues/649:79,deployability,automat,automates,79,"@kiranpatil222 , . ""Phasing"" can apply to both reads and variants. DeepVariant automates `haplotagging` which is synonymous for `read phasing` and it does not attempt to phase the called variants or it doesn't do `variant phasing` currently. So you wouldn't get `|` bars in the variants as they are not phased as @pichuan said.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/649
https://github.com/google/deepvariant/issues/649:231,energy efficiency,current,currently,231,"@kiranpatil222 , . ""Phasing"" can apply to both reads and variants. DeepVariant automates `haplotagging` which is synonymous for `read phasing` and it does not attempt to phase the called variants or it doesn't do `variant phasing` currently. So you wouldn't get `|` bars in the variants as they are not phased as @pichuan said.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/649
https://github.com/google/deepvariant/issues/649:150,reliability,doe,does,150,"@kiranpatil222 , . ""Phasing"" can apply to both reads and variants. DeepVariant automates `haplotagging` which is synonymous for `read phasing` and it does not attempt to phase the called variants or it doesn't do `variant phasing` currently. So you wouldn't get `|` bars in the variants as they are not phased as @pichuan said.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/649
https://github.com/google/deepvariant/issues/649:202,reliability,doe,doesn,202,"@kiranpatil222 , . ""Phasing"" can apply to both reads and variants. DeepVariant automates `haplotagging` which is synonymous for `read phasing` and it does not attempt to phase the called variants or it doesn't do `variant phasing` currently. So you wouldn't get `|` bars in the variants as they are not phased as @pichuan said.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/649
https://github.com/google/deepvariant/issues/649:79,testability,automat,automates,79,"@kiranpatil222 , . ""Phasing"" can apply to both reads and variants. DeepVariant automates `haplotagging` which is synonymous for `read phasing` and it does not attempt to phase the called variants or it doesn't do `variant phasing` currently. So you wouldn't get `|` bars in the variants as they are not phased as @pichuan said.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/649
https://github.com/google/deepvariant/issues/649:143,availability,down,down,143,"Given these confusions, I feel like some explanations of what ""direct phasing"" means, other than in this ticket, would reduce users' questions down the road.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/649
https://github.com/google/deepvariant/issues/649:119,energy efficiency,reduc,reduce,119,"Given these confusions, I feel like some explanations of what ""direct phasing"" means, other than in this ticket, would reduce users' questions down the road.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/649
https://github.com/google/deepvariant/issues/649:126,usability,user,users,126,"Given these confusions, I feel like some explanations of what ""direct phasing"" means, other than in this ticket, would reduce users' questions down the road.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/649
https://github.com/google/deepvariant/issues/649:470,availability,operat,operation,470,"@SHuang-Broad, in the upcoming release, we will introduce a section in the DeepVariant documentation that explains ""direct phasing"" for long reads. . To summarize:. * Incorporating phasing reads enhances the accuracy of DeepVariant. With the 1.4 version, we introduced direct phasing, thereby eliminating the need to execute make_examples twice and call an external tool in the process. However, internal phasing does not permit the output of phased variants due to the operation being limited to 25,000 base regions at a time, with no synchronization between these regions. * From version 1.4 onwards, DeepVariant internally handles read phasing, eliminating the need for external phasing to attain the anticipated accuracy. * If desired, one may opt to run third-party phasing after executing DeepVariant.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/649
https://github.com/google/deepvariant/issues/649:31,deployability,releas,release,31,"@SHuang-Broad, in the upcoming release, we will introduce a section in the DeepVariant documentation that explains ""direct phasing"" for long reads. . To summarize:. * Incorporating phasing reads enhances the accuracy of DeepVariant. With the 1.4 version, we introduced direct phasing, thereby eliminating the need to execute make_examples twice and call an external tool in the process. However, internal phasing does not permit the output of phased variants due to the operation being limited to 25,000 base regions at a time, with no synchronization between these regions. * From version 1.4 onwards, DeepVariant internally handles read phasing, eliminating the need for external phasing to attain the anticipated accuracy. * If desired, one may opt to run third-party phasing after executing DeepVariant.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/649
https://github.com/google/deepvariant/issues/649:246,deployability,version,version,246,"@SHuang-Broad, in the upcoming release, we will introduce a section in the DeepVariant documentation that explains ""direct phasing"" for long reads. . To summarize:. * Incorporating phasing reads enhances the accuracy of DeepVariant. With the 1.4 version, we introduced direct phasing, thereby eliminating the need to execute make_examples twice and call an external tool in the process. However, internal phasing does not permit the output of phased variants due to the operation being limited to 25,000 base regions at a time, with no synchronization between these regions. * From version 1.4 onwards, DeepVariant internally handles read phasing, eliminating the need for external phasing to attain the anticipated accuracy. * If desired, one may opt to run third-party phasing after executing DeepVariant.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/649
https://github.com/google/deepvariant/issues/649:582,deployability,version,version,582,"@SHuang-Broad, in the upcoming release, we will introduce a section in the DeepVariant documentation that explains ""direct phasing"" for long reads. . To summarize:. * Incorporating phasing reads enhances the accuracy of DeepVariant. With the 1.4 version, we introduced direct phasing, thereby eliminating the need to execute make_examples twice and call an external tool in the process. However, internal phasing does not permit the output of phased variants due to the operation being limited to 25,000 base regions at a time, with no synchronization between these regions. * From version 1.4 onwards, DeepVariant internally handles read phasing, eliminating the need for external phasing to attain the anticipated accuracy. * If desired, one may opt to run third-party phasing after executing DeepVariant.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/649
https://github.com/google/deepvariant/issues/649:246,integrability,version,version,246,"@SHuang-Broad, in the upcoming release, we will introduce a section in the DeepVariant documentation that explains ""direct phasing"" for long reads. . To summarize:. * Incorporating phasing reads enhances the accuracy of DeepVariant. With the 1.4 version, we introduced direct phasing, thereby eliminating the need to execute make_examples twice and call an external tool in the process. However, internal phasing does not permit the output of phased variants due to the operation being limited to 25,000 base regions at a time, with no synchronization between these regions. * From version 1.4 onwards, DeepVariant internally handles read phasing, eliminating the need for external phasing to attain the anticipated accuracy. * If desired, one may opt to run third-party phasing after executing DeepVariant.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/649
https://github.com/google/deepvariant/issues/649:582,integrability,version,version,582,"@SHuang-Broad, in the upcoming release, we will introduce a section in the DeepVariant documentation that explains ""direct phasing"" for long reads. . To summarize:. * Incorporating phasing reads enhances the accuracy of DeepVariant. With the 1.4 version, we introduced direct phasing, thereby eliminating the need to execute make_examples twice and call an external tool in the process. However, internal phasing does not permit the output of phased variants due to the operation being limited to 25,000 base regions at a time, with no synchronization between these regions. * From version 1.4 onwards, DeepVariant internally handles read phasing, eliminating the need for external phasing to attain the anticipated accuracy. * If desired, one may opt to run third-party phasing after executing DeepVariant.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/649
https://github.com/google/deepvariant/issues/649:246,modifiability,version,version,246,"@SHuang-Broad, in the upcoming release, we will introduce a section in the DeepVariant documentation that explains ""direct phasing"" for long reads. . To summarize:. * Incorporating phasing reads enhances the accuracy of DeepVariant. With the 1.4 version, we introduced direct phasing, thereby eliminating the need to execute make_examples twice and call an external tool in the process. However, internal phasing does not permit the output of phased variants due to the operation being limited to 25,000 base regions at a time, with no synchronization between these regions. * From version 1.4 onwards, DeepVariant internally handles read phasing, eliminating the need for external phasing to attain the anticipated accuracy. * If desired, one may opt to run third-party phasing after executing DeepVariant.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/649
https://github.com/google/deepvariant/issues/649:582,modifiability,version,version,582,"@SHuang-Broad, in the upcoming release, we will introduce a section in the DeepVariant documentation that explains ""direct phasing"" for long reads. . To summarize:. * Incorporating phasing reads enhances the accuracy of DeepVariant. With the 1.4 version, we introduced direct phasing, thereby eliminating the need to execute make_examples twice and call an external tool in the process. However, internal phasing does not permit the output of phased variants due to the operation being limited to 25,000 base regions at a time, with no synchronization between these regions. * From version 1.4 onwards, DeepVariant internally handles read phasing, eliminating the need for external phasing to attain the anticipated accuracy. * If desired, one may opt to run third-party phasing after executing DeepVariant.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/649
https://github.com/google/deepvariant/issues/649:522,performance,time,time,522,"@SHuang-Broad, in the upcoming release, we will introduce a section in the DeepVariant documentation that explains ""direct phasing"" for long reads. . To summarize:. * Incorporating phasing reads enhances the accuracy of DeepVariant. With the 1.4 version, we introduced direct phasing, thereby eliminating the need to execute make_examples twice and call an external tool in the process. However, internal phasing does not permit the output of phased variants due to the operation being limited to 25,000 base regions at a time, with no synchronization between these regions. * From version 1.4 onwards, DeepVariant internally handles read phasing, eliminating the need for external phasing to attain the anticipated accuracy. * If desired, one may opt to run third-party phasing after executing DeepVariant.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/649
https://github.com/google/deepvariant/issues/649:536,performance,synch,synchronization,536,"@SHuang-Broad, in the upcoming release, we will introduce a section in the DeepVariant documentation that explains ""direct phasing"" for long reads. . To summarize:. * Incorporating phasing reads enhances the accuracy of DeepVariant. With the 1.4 version, we introduced direct phasing, thereby eliminating the need to execute make_examples twice and call an external tool in the process. However, internal phasing does not permit the output of phased variants due to the operation being limited to 25,000 base regions at a time, with no synchronization between these regions. * From version 1.4 onwards, DeepVariant internally handles read phasing, eliminating the need for external phasing to attain the anticipated accuracy. * If desired, one may opt to run third-party phasing after executing DeepVariant.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/649
https://github.com/google/deepvariant/issues/649:413,reliability,doe,does,413,"@SHuang-Broad, in the upcoming release, we will introduce a section in the DeepVariant documentation that explains ""direct phasing"" for long reads. . To summarize:. * Incorporating phasing reads enhances the accuracy of DeepVariant. With the 1.4 version, we introduced direct phasing, thereby eliminating the need to execute make_examples twice and call an external tool in the process. However, internal phasing does not permit the output of phased variants due to the operation being limited to 25,000 base regions at a time, with no synchronization between these regions. * From version 1.4 onwards, DeepVariant internally handles read phasing, eliminating the need for external phasing to attain the anticipated accuracy. * If desired, one may opt to run third-party phasing after executing DeepVariant.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/649
https://github.com/google/deepvariant/issues/649:87,usability,document,documentation,87,"@SHuang-Broad, in the upcoming release, we will introduce a section in the DeepVariant documentation that explains ""direct phasing"" for long reads. . To summarize:. * Incorporating phasing reads enhances the accuracy of DeepVariant. With the 1.4 version, we introduced direct phasing, thereby eliminating the need to execute make_examples twice and call an external tool in the process. However, internal phasing does not permit the output of phased variants due to the operation being limited to 25,000 base regions at a time, with no synchronization between these regions. * From version 1.4 onwards, DeepVariant internally handles read phasing, eliminating the need for external phasing to attain the anticipated accuracy. * If desired, one may opt to run third-party phasing after executing DeepVariant.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/649
https://github.com/google/deepvariant/issues/649:366,usability,tool,tool,366,"@SHuang-Broad, in the upcoming release, we will introduce a section in the DeepVariant documentation that explains ""direct phasing"" for long reads. . To summarize:. * Incorporating phasing reads enhances the accuracy of DeepVariant. With the 1.4 version, we introduced direct phasing, thereby eliminating the need to execute make_examples twice and call an external tool in the process. However, internal phasing does not permit the output of phased variants due to the operation being limited to 25,000 base regions at a time, with no synchronization between these regions. * From version 1.4 onwards, DeepVariant internally handles read phasing, eliminating the need for external phasing to attain the anticipated accuracy. * If desired, one may opt to run third-party phasing after executing DeepVariant.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/649
https://github.com/google/deepvariant/issues/649:51,usability,close,close,51,Thanks all for the discussion in this thread! I'll close this issue now.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/649
https://github.com/google/deepvariant/issues/650:100,deployability,releas,releases,100,"Hi @SHuang-Broad . This is a good question. There have been a number of changes to DeepVariant over releases. However, I don't think we've deeply revisited the timing advantages between CPU and GPU. I think that in our investigations, we see that in the GCP machines we use CPU instances still have some degree of cost advantage relative to GPU. I agree that it's likely we're not optimally using the GPU, and I don't think we've specifically optimized for higher utilization of it. Although that's something we could look more into over time, right now we're prioritizing other feature improvements. . For GPUs, how do you consider Parabricks as an option? I believe that with their most recent release, they will be able to run v1.5. For the optimization of GPU performance, I expect they are much better in using the GPU than our out-of-the-box code. By the way, $100/sample seems high to me relative to our current experience. I hope that might be a function of the older method being Call-Phase-Call. If you have new benchmarks for this I would be curious to know. Thanks,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:696,deployability,releas,release,696,"Hi @SHuang-Broad . This is a good question. There have been a number of changes to DeepVariant over releases. However, I don't think we've deeply revisited the timing advantages between CPU and GPU. I think that in our investigations, we see that in the GCP machines we use CPU instances still have some degree of cost advantage relative to GPU. I agree that it's likely we're not optimally using the GPU, and I don't think we've specifically optimized for higher utilization of it. Although that's something we could look more into over time, right now we're prioritizing other feature improvements. . For GPUs, how do you consider Parabricks as an option? I believe that with their most recent release, they will be able to run v1.5. For the optimization of GPU performance, I expect they are much better in using the GPU than our out-of-the-box code. By the way, $100/sample seems high to me relative to our current experience. I hope that might be a function of the older method being Call-Phase-Call. If you have new benchmarks for this I would be curious to know. Thanks,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:186,energy efficiency,CPU,CPU,186,"Hi @SHuang-Broad . This is a good question. There have been a number of changes to DeepVariant over releases. However, I don't think we've deeply revisited the timing advantages between CPU and GPU. I think that in our investigations, we see that in the GCP machines we use CPU instances still have some degree of cost advantage relative to GPU. I agree that it's likely we're not optimally using the GPU, and I don't think we've specifically optimized for higher utilization of it. Although that's something we could look more into over time, right now we're prioritizing other feature improvements. . For GPUs, how do you consider Parabricks as an option? I believe that with their most recent release, they will be able to run v1.5. For the optimization of GPU performance, I expect they are much better in using the GPU than our out-of-the-box code. By the way, $100/sample seems high to me relative to our current experience. I hope that might be a function of the older method being Call-Phase-Call. If you have new benchmarks for this I would be curious to know. Thanks,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:194,energy efficiency,GPU,GPU,194,"Hi @SHuang-Broad . This is a good question. There have been a number of changes to DeepVariant over releases. However, I don't think we've deeply revisited the timing advantages between CPU and GPU. I think that in our investigations, we see that in the GCP machines we use CPU instances still have some degree of cost advantage relative to GPU. I agree that it's likely we're not optimally using the GPU, and I don't think we've specifically optimized for higher utilization of it. Although that's something we could look more into over time, right now we're prioritizing other feature improvements. . For GPUs, how do you consider Parabricks as an option? I believe that with their most recent release, they will be able to run v1.5. For the optimization of GPU performance, I expect they are much better in using the GPU than our out-of-the-box code. By the way, $100/sample seems high to me relative to our current experience. I hope that might be a function of the older method being Call-Phase-Call. If you have new benchmarks for this I would be curious to know. Thanks,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:274,energy efficiency,CPU,CPU,274,"Hi @SHuang-Broad . This is a good question. There have been a number of changes to DeepVariant over releases. However, I don't think we've deeply revisited the timing advantages between CPU and GPU. I think that in our investigations, we see that in the GCP machines we use CPU instances still have some degree of cost advantage relative to GPU. I agree that it's likely we're not optimally using the GPU, and I don't think we've specifically optimized for higher utilization of it. Although that's something we could look more into over time, right now we're prioritizing other feature improvements. . For GPUs, how do you consider Parabricks as an option? I believe that with their most recent release, they will be able to run v1.5. For the optimization of GPU performance, I expect they are much better in using the GPU than our out-of-the-box code. By the way, $100/sample seems high to me relative to our current experience. I hope that might be a function of the older method being Call-Phase-Call. If you have new benchmarks for this I would be curious to know. Thanks,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:341,energy efficiency,GPU,GPU,341,"Hi @SHuang-Broad . This is a good question. There have been a number of changes to DeepVariant over releases. However, I don't think we've deeply revisited the timing advantages between CPU and GPU. I think that in our investigations, we see that in the GCP machines we use CPU instances still have some degree of cost advantage relative to GPU. I agree that it's likely we're not optimally using the GPU, and I don't think we've specifically optimized for higher utilization of it. Although that's something we could look more into over time, right now we're prioritizing other feature improvements. . For GPUs, how do you consider Parabricks as an option? I believe that with their most recent release, they will be able to run v1.5. For the optimization of GPU performance, I expect they are much better in using the GPU than our out-of-the-box code. By the way, $100/sample seems high to me relative to our current experience. I hope that might be a function of the older method being Call-Phase-Call. If you have new benchmarks for this I would be curious to know. Thanks,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:381,energy efficiency,optim,optimally,381,"Hi @SHuang-Broad . This is a good question. There have been a number of changes to DeepVariant over releases. However, I don't think we've deeply revisited the timing advantages between CPU and GPU. I think that in our investigations, we see that in the GCP machines we use CPU instances still have some degree of cost advantage relative to GPU. I agree that it's likely we're not optimally using the GPU, and I don't think we've specifically optimized for higher utilization of it. Although that's something we could look more into over time, right now we're prioritizing other feature improvements. . For GPUs, how do you consider Parabricks as an option? I believe that with their most recent release, they will be able to run v1.5. For the optimization of GPU performance, I expect they are much better in using the GPU than our out-of-the-box code. By the way, $100/sample seems high to me relative to our current experience. I hope that might be a function of the older method being Call-Phase-Call. If you have new benchmarks for this I would be curious to know. Thanks,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:401,energy efficiency,GPU,GPU,401,"Hi @SHuang-Broad . This is a good question. There have been a number of changes to DeepVariant over releases. However, I don't think we've deeply revisited the timing advantages between CPU and GPU. I think that in our investigations, we see that in the GCP machines we use CPU instances still have some degree of cost advantage relative to GPU. I agree that it's likely we're not optimally using the GPU, and I don't think we've specifically optimized for higher utilization of it. Although that's something we could look more into over time, right now we're prioritizing other feature improvements. . For GPUs, how do you consider Parabricks as an option? I believe that with their most recent release, they will be able to run v1.5. For the optimization of GPU performance, I expect they are much better in using the GPU than our out-of-the-box code. By the way, $100/sample seems high to me relative to our current experience. I hope that might be a function of the older method being Call-Phase-Call. If you have new benchmarks for this I would be curious to know. Thanks,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:443,energy efficiency,optim,optimized,443,"Hi @SHuang-Broad . This is a good question. There have been a number of changes to DeepVariant over releases. However, I don't think we've deeply revisited the timing advantages between CPU and GPU. I think that in our investigations, we see that in the GCP machines we use CPU instances still have some degree of cost advantage relative to GPU. I agree that it's likely we're not optimally using the GPU, and I don't think we've specifically optimized for higher utilization of it. Although that's something we could look more into over time, right now we're prioritizing other feature improvements. . For GPUs, how do you consider Parabricks as an option? I believe that with their most recent release, they will be able to run v1.5. For the optimization of GPU performance, I expect they are much better in using the GPU than our out-of-the-box code. By the way, $100/sample seems high to me relative to our current experience. I hope that might be a function of the older method being Call-Phase-Call. If you have new benchmarks for this I would be curious to know. Thanks,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:607,energy efficiency,GPU,GPUs,607,"Hi @SHuang-Broad . This is a good question. There have been a number of changes to DeepVariant over releases. However, I don't think we've deeply revisited the timing advantages between CPU and GPU. I think that in our investigations, we see that in the GCP machines we use CPU instances still have some degree of cost advantage relative to GPU. I agree that it's likely we're not optimally using the GPU, and I don't think we've specifically optimized for higher utilization of it. Although that's something we could look more into over time, right now we're prioritizing other feature improvements. . For GPUs, how do you consider Parabricks as an option? I believe that with their most recent release, they will be able to run v1.5. For the optimization of GPU performance, I expect they are much better in using the GPU than our out-of-the-box code. By the way, $100/sample seems high to me relative to our current experience. I hope that might be a function of the older method being Call-Phase-Call. If you have new benchmarks for this I would be curious to know. Thanks,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:744,energy efficiency,optim,optimization,744,"Hi @SHuang-Broad . This is a good question. There have been a number of changes to DeepVariant over releases. However, I don't think we've deeply revisited the timing advantages between CPU and GPU. I think that in our investigations, we see that in the GCP machines we use CPU instances still have some degree of cost advantage relative to GPU. I agree that it's likely we're not optimally using the GPU, and I don't think we've specifically optimized for higher utilization of it. Although that's something we could look more into over time, right now we're prioritizing other feature improvements. . For GPUs, how do you consider Parabricks as an option? I believe that with their most recent release, they will be able to run v1.5. For the optimization of GPU performance, I expect they are much better in using the GPU than our out-of-the-box code. By the way, $100/sample seems high to me relative to our current experience. I hope that might be a function of the older method being Call-Phase-Call. If you have new benchmarks for this I would be curious to know. Thanks,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:760,energy efficiency,GPU,GPU,760,"Hi @SHuang-Broad . This is a good question. There have been a number of changes to DeepVariant over releases. However, I don't think we've deeply revisited the timing advantages between CPU and GPU. I think that in our investigations, we see that in the GCP machines we use CPU instances still have some degree of cost advantage relative to GPU. I agree that it's likely we're not optimally using the GPU, and I don't think we've specifically optimized for higher utilization of it. Although that's something we could look more into over time, right now we're prioritizing other feature improvements. . For GPUs, how do you consider Parabricks as an option? I believe that with their most recent release, they will be able to run v1.5. For the optimization of GPU performance, I expect they are much better in using the GPU than our out-of-the-box code. By the way, $100/sample seems high to me relative to our current experience. I hope that might be a function of the older method being Call-Phase-Call. If you have new benchmarks for this I would be curious to know. Thanks,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:820,energy efficiency,GPU,GPU,820,"Hi @SHuang-Broad . This is a good question. There have been a number of changes to DeepVariant over releases. However, I don't think we've deeply revisited the timing advantages between CPU and GPU. I think that in our investigations, we see that in the GCP machines we use CPU instances still have some degree of cost advantage relative to GPU. I agree that it's likely we're not optimally using the GPU, and I don't think we've specifically optimized for higher utilization of it. Although that's something we could look more into over time, right now we're prioritizing other feature improvements. . For GPUs, how do you consider Parabricks as an option? I believe that with their most recent release, they will be able to run v1.5. For the optimization of GPU performance, I expect they are much better in using the GPU than our out-of-the-box code. By the way, $100/sample seems high to me relative to our current experience. I hope that might be a function of the older method being Call-Phase-Call. If you have new benchmarks for this I would be curious to know. Thanks,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:911,energy efficiency,current,current,911,"Hi @SHuang-Broad . This is a good question. There have been a number of changes to DeepVariant over releases. However, I don't think we've deeply revisited the timing advantages between CPU and GPU. I think that in our investigations, we see that in the GCP machines we use CPU instances still have some degree of cost advantage relative to GPU. I agree that it's likely we're not optimally using the GPU, and I don't think we've specifically optimized for higher utilization of it. Although that's something we could look more into over time, right now we're prioritizing other feature improvements. . For GPUs, how do you consider Parabricks as an option? I believe that with their most recent release, they will be able to run v1.5. For the optimization of GPU performance, I expect they are much better in using the GPU than our out-of-the-box code. By the way, $100/sample seems high to me relative to our current experience. I hope that might be a function of the older method being Call-Phase-Call. If you have new benchmarks for this I would be curious to know. Thanks,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:430,interoperability,specif,specifically,430,"Hi @SHuang-Broad . This is a good question. There have been a number of changes to DeepVariant over releases. However, I don't think we've deeply revisited the timing advantages between CPU and GPU. I think that in our investigations, we see that in the GCP machines we use CPU instances still have some degree of cost advantage relative to GPU. I agree that it's likely we're not optimally using the GPU, and I don't think we've specifically optimized for higher utilization of it. Although that's something we could look more into over time, right now we're prioritizing other feature improvements. . For GPUs, how do you consider Parabricks as an option? I believe that with their most recent release, they will be able to run v1.5. For the optimization of GPU performance, I expect they are much better in using the GPU than our out-of-the-box code. By the way, $100/sample seems high to me relative to our current experience. I hope that might be a function of the older method being Call-Phase-Call. If you have new benchmarks for this I would be curious to know. Thanks,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:186,performance,CPU,CPU,186,"Hi @SHuang-Broad . This is a good question. There have been a number of changes to DeepVariant over releases. However, I don't think we've deeply revisited the timing advantages between CPU and GPU. I think that in our investigations, we see that in the GCP machines we use CPU instances still have some degree of cost advantage relative to GPU. I agree that it's likely we're not optimally using the GPU, and I don't think we've specifically optimized for higher utilization of it. Although that's something we could look more into over time, right now we're prioritizing other feature improvements. . For GPUs, how do you consider Parabricks as an option? I believe that with their most recent release, they will be able to run v1.5. For the optimization of GPU performance, I expect they are much better in using the GPU than our out-of-the-box code. By the way, $100/sample seems high to me relative to our current experience. I hope that might be a function of the older method being Call-Phase-Call. If you have new benchmarks for this I would be curious to know. Thanks,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:194,performance,GPU,GPU,194,"Hi @SHuang-Broad . This is a good question. There have been a number of changes to DeepVariant over releases. However, I don't think we've deeply revisited the timing advantages between CPU and GPU. I think that in our investigations, we see that in the GCP machines we use CPU instances still have some degree of cost advantage relative to GPU. I agree that it's likely we're not optimally using the GPU, and I don't think we've specifically optimized for higher utilization of it. Although that's something we could look more into over time, right now we're prioritizing other feature improvements. . For GPUs, how do you consider Parabricks as an option? I believe that with their most recent release, they will be able to run v1.5. For the optimization of GPU performance, I expect they are much better in using the GPU than our out-of-the-box code. By the way, $100/sample seems high to me relative to our current experience. I hope that might be a function of the older method being Call-Phase-Call. If you have new benchmarks for this I would be curious to know. Thanks,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:274,performance,CPU,CPU,274,"Hi @SHuang-Broad . This is a good question. There have been a number of changes to DeepVariant over releases. However, I don't think we've deeply revisited the timing advantages between CPU and GPU. I think that in our investigations, we see that in the GCP machines we use CPU instances still have some degree of cost advantage relative to GPU. I agree that it's likely we're not optimally using the GPU, and I don't think we've specifically optimized for higher utilization of it. Although that's something we could look more into over time, right now we're prioritizing other feature improvements. . For GPUs, how do you consider Parabricks as an option? I believe that with their most recent release, they will be able to run v1.5. For the optimization of GPU performance, I expect they are much better in using the GPU than our out-of-the-box code. By the way, $100/sample seems high to me relative to our current experience. I hope that might be a function of the older method being Call-Phase-Call. If you have new benchmarks for this I would be curious to know. Thanks,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:341,performance,GPU,GPU,341,"Hi @SHuang-Broad . This is a good question. There have been a number of changes to DeepVariant over releases. However, I don't think we've deeply revisited the timing advantages between CPU and GPU. I think that in our investigations, we see that in the GCP machines we use CPU instances still have some degree of cost advantage relative to GPU. I agree that it's likely we're not optimally using the GPU, and I don't think we've specifically optimized for higher utilization of it. Although that's something we could look more into over time, right now we're prioritizing other feature improvements. . For GPUs, how do you consider Parabricks as an option? I believe that with their most recent release, they will be able to run v1.5. For the optimization of GPU performance, I expect they are much better in using the GPU than our out-of-the-box code. By the way, $100/sample seems high to me relative to our current experience. I hope that might be a function of the older method being Call-Phase-Call. If you have new benchmarks for this I would be curious to know. Thanks,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:401,performance,GPU,GPU,401,"Hi @SHuang-Broad . This is a good question. There have been a number of changes to DeepVariant over releases. However, I don't think we've deeply revisited the timing advantages between CPU and GPU. I think that in our investigations, we see that in the GCP machines we use CPU instances still have some degree of cost advantage relative to GPU. I agree that it's likely we're not optimally using the GPU, and I don't think we've specifically optimized for higher utilization of it. Although that's something we could look more into over time, right now we're prioritizing other feature improvements. . For GPUs, how do you consider Parabricks as an option? I believe that with their most recent release, they will be able to run v1.5. For the optimization of GPU performance, I expect they are much better in using the GPU than our out-of-the-box code. By the way, $100/sample seems high to me relative to our current experience. I hope that might be a function of the older method being Call-Phase-Call. If you have new benchmarks for this I would be curious to know. Thanks,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:443,performance,optimiz,optimized,443,"Hi @SHuang-Broad . This is a good question. There have been a number of changes to DeepVariant over releases. However, I don't think we've deeply revisited the timing advantages between CPU and GPU. I think that in our investigations, we see that in the GCP machines we use CPU instances still have some degree of cost advantage relative to GPU. I agree that it's likely we're not optimally using the GPU, and I don't think we've specifically optimized for higher utilization of it. Although that's something we could look more into over time, right now we're prioritizing other feature improvements. . For GPUs, how do you consider Parabricks as an option? I believe that with their most recent release, they will be able to run v1.5. For the optimization of GPU performance, I expect they are much better in using the GPU than our out-of-the-box code. By the way, $100/sample seems high to me relative to our current experience. I hope that might be a function of the older method being Call-Phase-Call. If you have new benchmarks for this I would be curious to know. Thanks,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:538,performance,time,time,538,"Hi @SHuang-Broad . This is a good question. There have been a number of changes to DeepVariant over releases. However, I don't think we've deeply revisited the timing advantages between CPU and GPU. I think that in our investigations, we see that in the GCP machines we use CPU instances still have some degree of cost advantage relative to GPU. I agree that it's likely we're not optimally using the GPU, and I don't think we've specifically optimized for higher utilization of it. Although that's something we could look more into over time, right now we're prioritizing other feature improvements. . For GPUs, how do you consider Parabricks as an option? I believe that with their most recent release, they will be able to run v1.5. For the optimization of GPU performance, I expect they are much better in using the GPU than our out-of-the-box code. By the way, $100/sample seems high to me relative to our current experience. I hope that might be a function of the older method being Call-Phase-Call. If you have new benchmarks for this I would be curious to know. Thanks,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:607,performance,GPU,GPUs,607,"Hi @SHuang-Broad . This is a good question. There have been a number of changes to DeepVariant over releases. However, I don't think we've deeply revisited the timing advantages between CPU and GPU. I think that in our investigations, we see that in the GCP machines we use CPU instances still have some degree of cost advantage relative to GPU. I agree that it's likely we're not optimally using the GPU, and I don't think we've specifically optimized for higher utilization of it. Although that's something we could look more into over time, right now we're prioritizing other feature improvements. . For GPUs, how do you consider Parabricks as an option? I believe that with their most recent release, they will be able to run v1.5. For the optimization of GPU performance, I expect they are much better in using the GPU than our out-of-the-box code. By the way, $100/sample seems high to me relative to our current experience. I hope that might be a function of the older method being Call-Phase-Call. If you have new benchmarks for this I would be curious to know. Thanks,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:744,performance,optimiz,optimization,744,"Hi @SHuang-Broad . This is a good question. There have been a number of changes to DeepVariant over releases. However, I don't think we've deeply revisited the timing advantages between CPU and GPU. I think that in our investigations, we see that in the GCP machines we use CPU instances still have some degree of cost advantage relative to GPU. I agree that it's likely we're not optimally using the GPU, and I don't think we've specifically optimized for higher utilization of it. Although that's something we could look more into over time, right now we're prioritizing other feature improvements. . For GPUs, how do you consider Parabricks as an option? I believe that with their most recent release, they will be able to run v1.5. For the optimization of GPU performance, I expect they are much better in using the GPU than our out-of-the-box code. By the way, $100/sample seems high to me relative to our current experience. I hope that might be a function of the older method being Call-Phase-Call. If you have new benchmarks for this I would be curious to know. Thanks,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:760,performance,GPU,GPU,760,"Hi @SHuang-Broad . This is a good question. There have been a number of changes to DeepVariant over releases. However, I don't think we've deeply revisited the timing advantages between CPU and GPU. I think that in our investigations, we see that in the GCP machines we use CPU instances still have some degree of cost advantage relative to GPU. I agree that it's likely we're not optimally using the GPU, and I don't think we've specifically optimized for higher utilization of it. Although that's something we could look more into over time, right now we're prioritizing other feature improvements. . For GPUs, how do you consider Parabricks as an option? I believe that with their most recent release, they will be able to run v1.5. For the optimization of GPU performance, I expect they are much better in using the GPU than our out-of-the-box code. By the way, $100/sample seems high to me relative to our current experience. I hope that might be a function of the older method being Call-Phase-Call. If you have new benchmarks for this I would be curious to know. Thanks,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:764,performance,perform,performance,764,"Hi @SHuang-Broad . This is a good question. There have been a number of changes to DeepVariant over releases. However, I don't think we've deeply revisited the timing advantages between CPU and GPU. I think that in our investigations, we see that in the GCP machines we use CPU instances still have some degree of cost advantage relative to GPU. I agree that it's likely we're not optimally using the GPU, and I don't think we've specifically optimized for higher utilization of it. Although that's something we could look more into over time, right now we're prioritizing other feature improvements. . For GPUs, how do you consider Parabricks as an option? I believe that with their most recent release, they will be able to run v1.5. For the optimization of GPU performance, I expect they are much better in using the GPU than our out-of-the-box code. By the way, $100/sample seems high to me relative to our current experience. I hope that might be a function of the older method being Call-Phase-Call. If you have new benchmarks for this I would be curious to know. Thanks,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:820,performance,GPU,GPU,820,"Hi @SHuang-Broad . This is a good question. There have been a number of changes to DeepVariant over releases. However, I don't think we've deeply revisited the timing advantages between CPU and GPU. I think that in our investigations, we see that in the GCP machines we use CPU instances still have some degree of cost advantage relative to GPU. I agree that it's likely we're not optimally using the GPU, and I don't think we've specifically optimized for higher utilization of it. Although that's something we could look more into over time, right now we're prioritizing other feature improvements. . For GPUs, how do you consider Parabricks as an option? I believe that with their most recent release, they will be able to run v1.5. For the optimization of GPU performance, I expect they are much better in using the GPU than our out-of-the-box code. By the way, $100/sample seems high to me relative to our current experience. I hope that might be a function of the older method being Call-Phase-Call. If you have new benchmarks for this I would be curious to know. Thanks,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:764,usability,perform,performance,764,"Hi @SHuang-Broad . This is a good question. There have been a number of changes to DeepVariant over releases. However, I don't think we've deeply revisited the timing advantages between CPU and GPU. I think that in our investigations, we see that in the GCP machines we use CPU instances still have some degree of cost advantage relative to GPU. I agree that it's likely we're not optimally using the GPU, and I don't think we've specifically optimized for higher utilization of it. Although that's something we could look more into over time, right now we're prioritizing other feature improvements. . For GPUs, how do you consider Parabricks as an option? I believe that with their most recent release, they will be able to run v1.5. For the optimization of GPU performance, I expect they are much better in using the GPU than our out-of-the-box code. By the way, $100/sample seems high to me relative to our current experience. I hope that might be a function of the older method being Call-Phase-Call. If you have new benchmarks for this I would be curious to know. Thanks,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:919,usability,experien,experience,919,"Hi @SHuang-Broad . This is a good question. There have been a number of changes to DeepVariant over releases. However, I don't think we've deeply revisited the timing advantages between CPU and GPU. I think that in our investigations, we see that in the GCP machines we use CPU instances still have some degree of cost advantage relative to GPU. I agree that it's likely we're not optimally using the GPU, and I don't think we've specifically optimized for higher utilization of it. Although that's something we could look more into over time, right now we're prioritizing other feature improvements. . For GPUs, how do you consider Parabricks as an option? I believe that with their most recent release, they will be able to run v1.5. For the optimization of GPU performance, I expect they are much better in using the GPU than our out-of-the-box code. By the way, $100/sample seems high to me relative to our current experience. I hope that might be a function of the older method being Call-Phase-Call. If you have new benchmarks for this I would be curious to know. Thanks,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:288,availability,operat,operations,288,"An interesting feature seems jump up when profiling of DV 1.5 in Docker, which looks to be heavy on I/O more than anything else:. ![image](https://github.com/google/deepvariant/assets/6555937/a912b74c-3daa-44f1-a0d4-ef7ecc91bcce). Then with sampling the I/O, the distribution of types of operations hint at some bottleneck opportunity:. ![image](https://github.com/google/deepvariant/assets/6555937/9b6e2296-789b-4eff-852b-f053214f291a). Maybe optimizing for I/O via balancing by having equal number of resources for `|GPU| == |CPU|` with significant on-board memory could do the trick, though not sure if all the code has been ported to Keras, which should help with the distribution strategy in that situation to make it all click together.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:503,deployability,resourc,resources,503,"An interesting feature seems jump up when profiling of DV 1.5 in Docker, which looks to be heavy on I/O more than anything else:. ![image](https://github.com/google/deepvariant/assets/6555937/a912b74c-3daa-44f1-a0d4-ef7ecc91bcce). Then with sampling the I/O, the distribution of types of operations hint at some bottleneck opportunity:. ![image](https://github.com/google/deepvariant/assets/6555937/9b6e2296-789b-4eff-852b-f053214f291a). Maybe optimizing for I/O via balancing by having equal number of resources for `|GPU| == |CPU|` with significant on-board memory could do the trick, though not sure if all the code has been ported to Keras, which should help with the distribution strategy in that situation to make it all click together.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:42,energy efficiency,profil,profiling,42,"An interesting feature seems jump up when profiling of DV 1.5 in Docker, which looks to be heavy on I/O more than anything else:. ![image](https://github.com/google/deepvariant/assets/6555937/a912b74c-3daa-44f1-a0d4-ef7ecc91bcce). Then with sampling the I/O, the distribution of types of operations hint at some bottleneck opportunity:. ![image](https://github.com/google/deepvariant/assets/6555937/9b6e2296-789b-4eff-852b-f053214f291a). Maybe optimizing for I/O via balancing by having equal number of resources for `|GPU| == |CPU|` with significant on-board memory could do the trick, though not sure if all the code has been ported to Keras, which should help with the distribution strategy in that situation to make it all click together.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:444,energy efficiency,optim,optimizing,444,"An interesting feature seems jump up when profiling of DV 1.5 in Docker, which looks to be heavy on I/O more than anything else:. ![image](https://github.com/google/deepvariant/assets/6555937/a912b74c-3daa-44f1-a0d4-ef7ecc91bcce). Then with sampling the I/O, the distribution of types of operations hint at some bottleneck opportunity:. ![image](https://github.com/google/deepvariant/assets/6555937/9b6e2296-789b-4eff-852b-f053214f291a). Maybe optimizing for I/O via balancing by having equal number of resources for `|GPU| == |CPU|` with significant on-board memory could do the trick, though not sure if all the code has been ported to Keras, which should help with the distribution strategy in that situation to make it all click together.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:503,energy efficiency,resourc,resources,503,"An interesting feature seems jump up when profiling of DV 1.5 in Docker, which looks to be heavy on I/O more than anything else:. ![image](https://github.com/google/deepvariant/assets/6555937/a912b74c-3daa-44f1-a0d4-ef7ecc91bcce). Then with sampling the I/O, the distribution of types of operations hint at some bottleneck opportunity:. ![image](https://github.com/google/deepvariant/assets/6555937/9b6e2296-789b-4eff-852b-f053214f291a). Maybe optimizing for I/O via balancing by having equal number of resources for `|GPU| == |CPU|` with significant on-board memory could do the trick, though not sure if all the code has been ported to Keras, which should help with the distribution strategy in that situation to make it all click together.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:519,energy efficiency,GPU,GPU,519,"An interesting feature seems jump up when profiling of DV 1.5 in Docker, which looks to be heavy on I/O more than anything else:. ![image](https://github.com/google/deepvariant/assets/6555937/a912b74c-3daa-44f1-a0d4-ef7ecc91bcce). Then with sampling the I/O, the distribution of types of operations hint at some bottleneck opportunity:. ![image](https://github.com/google/deepvariant/assets/6555937/9b6e2296-789b-4eff-852b-f053214f291a). Maybe optimizing for I/O via balancing by having equal number of resources for `|GPU| == |CPU|` with significant on-board memory could do the trick, though not sure if all the code has been ported to Keras, which should help with the distribution strategy in that situation to make it all click together.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:528,energy efficiency,CPU,CPU,528,"An interesting feature seems jump up when profiling of DV 1.5 in Docker, which looks to be heavy on I/O more than anything else:. ![image](https://github.com/google/deepvariant/assets/6555937/a912b74c-3daa-44f1-a0d4-ef7ecc91bcce). Then with sampling the I/O, the distribution of types of operations hint at some bottleneck opportunity:. ![image](https://github.com/google/deepvariant/assets/6555937/9b6e2296-789b-4eff-852b-f053214f291a). Maybe optimizing for I/O via balancing by having equal number of resources for `|GPU| == |CPU|` with significant on-board memory could do the trick, though not sure if all the code has been ported to Keras, which should help with the distribution strategy in that situation to make it all click together.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:263,interoperability,distribut,distribution,263,"An interesting feature seems jump up when profiling of DV 1.5 in Docker, which looks to be heavy on I/O more than anything else:. ![image](https://github.com/google/deepvariant/assets/6555937/a912b74c-3daa-44f1-a0d4-ef7ecc91bcce). Then with sampling the I/O, the distribution of types of operations hint at some bottleneck opportunity:. ![image](https://github.com/google/deepvariant/assets/6555937/9b6e2296-789b-4eff-852b-f053214f291a). Maybe optimizing for I/O via balancing by having equal number of resources for `|GPU| == |CPU|` with significant on-board memory could do the trick, though not sure if all the code has been ported to Keras, which should help with the distribution strategy in that situation to make it all click together.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:672,interoperability,distribut,distribution,672,"An interesting feature seems jump up when profiling of DV 1.5 in Docker, which looks to be heavy on I/O more than anything else:. ![image](https://github.com/google/deepvariant/assets/6555937/a912b74c-3daa-44f1-a0d4-ef7ecc91bcce). Then with sampling the I/O, the distribution of types of operations hint at some bottleneck opportunity:. ![image](https://github.com/google/deepvariant/assets/6555937/9b6e2296-789b-4eff-852b-f053214f291a). Maybe optimizing for I/O via balancing by having equal number of resources for `|GPU| == |CPU|` with significant on-board memory could do the trick, though not sure if all the code has been ported to Keras, which should help with the distribution strategy in that situation to make it all click together.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:42,performance,profil,profiling,42,"An interesting feature seems jump up when profiling of DV 1.5 in Docker, which looks to be heavy on I/O more than anything else:. ![image](https://github.com/google/deepvariant/assets/6555937/a912b74c-3daa-44f1-a0d4-ef7ecc91bcce). Then with sampling the I/O, the distribution of types of operations hint at some bottleneck opportunity:. ![image](https://github.com/google/deepvariant/assets/6555937/9b6e2296-789b-4eff-852b-f053214f291a). Maybe optimizing for I/O via balancing by having equal number of resources for `|GPU| == |CPU|` with significant on-board memory could do the trick, though not sure if all the code has been ported to Keras, which should help with the distribution strategy in that situation to make it all click together.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:100,performance,I/O,I/O,100,"An interesting feature seems jump up when profiling of DV 1.5 in Docker, which looks to be heavy on I/O more than anything else:. ![image](https://github.com/google/deepvariant/assets/6555937/a912b74c-3daa-44f1-a0d4-ef7ecc91bcce). Then with sampling the I/O, the distribution of types of operations hint at some bottleneck opportunity:. ![image](https://github.com/google/deepvariant/assets/6555937/9b6e2296-789b-4eff-852b-f053214f291a). Maybe optimizing for I/O via balancing by having equal number of resources for `|GPU| == |CPU|` with significant on-board memory could do the trick, though not sure if all the code has been ported to Keras, which should help with the distribution strategy in that situation to make it all click together.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:254,performance,I/O,I/O,254,"An interesting feature seems jump up when profiling of DV 1.5 in Docker, which looks to be heavy on I/O more than anything else:. ![image](https://github.com/google/deepvariant/assets/6555937/a912b74c-3daa-44f1-a0d4-ef7ecc91bcce). Then with sampling the I/O, the distribution of types of operations hint at some bottleneck opportunity:. ![image](https://github.com/google/deepvariant/assets/6555937/9b6e2296-789b-4eff-852b-f053214f291a). Maybe optimizing for I/O via balancing by having equal number of resources for `|GPU| == |CPU|` with significant on-board memory could do the trick, though not sure if all the code has been ported to Keras, which should help with the distribution strategy in that situation to make it all click together.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:312,performance,bottleneck,bottleneck,312,"An interesting feature seems jump up when profiling of DV 1.5 in Docker, which looks to be heavy on I/O more than anything else:. ![image](https://github.com/google/deepvariant/assets/6555937/a912b74c-3daa-44f1-a0d4-ef7ecc91bcce). Then with sampling the I/O, the distribution of types of operations hint at some bottleneck opportunity:. ![image](https://github.com/google/deepvariant/assets/6555937/9b6e2296-789b-4eff-852b-f053214f291a). Maybe optimizing for I/O via balancing by having equal number of resources for `|GPU| == |CPU|` with significant on-board memory could do the trick, though not sure if all the code has been ported to Keras, which should help with the distribution strategy in that situation to make it all click together.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:444,performance,optimiz,optimizing,444,"An interesting feature seems jump up when profiling of DV 1.5 in Docker, which looks to be heavy on I/O more than anything else:. ![image](https://github.com/google/deepvariant/assets/6555937/a912b74c-3daa-44f1-a0d4-ef7ecc91bcce). Then with sampling the I/O, the distribution of types of operations hint at some bottleneck opportunity:. ![image](https://github.com/google/deepvariant/assets/6555937/9b6e2296-789b-4eff-852b-f053214f291a). Maybe optimizing for I/O via balancing by having equal number of resources for `|GPU| == |CPU|` with significant on-board memory could do the trick, though not sure if all the code has been ported to Keras, which should help with the distribution strategy in that situation to make it all click together.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:459,performance,I/O,I/O,459,"An interesting feature seems jump up when profiling of DV 1.5 in Docker, which looks to be heavy on I/O more than anything else:. ![image](https://github.com/google/deepvariant/assets/6555937/a912b74c-3daa-44f1-a0d4-ef7ecc91bcce). Then with sampling the I/O, the distribution of types of operations hint at some bottleneck opportunity:. ![image](https://github.com/google/deepvariant/assets/6555937/9b6e2296-789b-4eff-852b-f053214f291a). Maybe optimizing for I/O via balancing by having equal number of resources for `|GPU| == |CPU|` with significant on-board memory could do the trick, though not sure if all the code has been ported to Keras, which should help with the distribution strategy in that situation to make it all click together.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:503,performance,resourc,resources,503,"An interesting feature seems jump up when profiling of DV 1.5 in Docker, which looks to be heavy on I/O more than anything else:. ![image](https://github.com/google/deepvariant/assets/6555937/a912b74c-3daa-44f1-a0d4-ef7ecc91bcce). Then with sampling the I/O, the distribution of types of operations hint at some bottleneck opportunity:. ![image](https://github.com/google/deepvariant/assets/6555937/9b6e2296-789b-4eff-852b-f053214f291a). Maybe optimizing for I/O via balancing by having equal number of resources for `|GPU| == |CPU|` with significant on-board memory could do the trick, though not sure if all the code has been ported to Keras, which should help with the distribution strategy in that situation to make it all click together.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:519,performance,GPU,GPU,519,"An interesting feature seems jump up when profiling of DV 1.5 in Docker, which looks to be heavy on I/O more than anything else:. ![image](https://github.com/google/deepvariant/assets/6555937/a912b74c-3daa-44f1-a0d4-ef7ecc91bcce). Then with sampling the I/O, the distribution of types of operations hint at some bottleneck opportunity:. ![image](https://github.com/google/deepvariant/assets/6555937/9b6e2296-789b-4eff-852b-f053214f291a). Maybe optimizing for I/O via balancing by having equal number of resources for `|GPU| == |CPU|` with significant on-board memory could do the trick, though not sure if all the code has been ported to Keras, which should help with the distribution strategy in that situation to make it all click together.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:528,performance,CPU,CPU,528,"An interesting feature seems jump up when profiling of DV 1.5 in Docker, which looks to be heavy on I/O more than anything else:. ![image](https://github.com/google/deepvariant/assets/6555937/a912b74c-3daa-44f1-a0d4-ef7ecc91bcce). Then with sampling the I/O, the distribution of types of operations hint at some bottleneck opportunity:. ![image](https://github.com/google/deepvariant/assets/6555937/9b6e2296-789b-4eff-852b-f053214f291a). Maybe optimizing for I/O via balancing by having equal number of resources for `|GPU| == |CPU|` with significant on-board memory could do the trick, though not sure if all the code has been ported to Keras, which should help with the distribution strategy in that situation to make it all click together.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:560,performance,memor,memory,560,"An interesting feature seems jump up when profiling of DV 1.5 in Docker, which looks to be heavy on I/O more than anything else:. ![image](https://github.com/google/deepvariant/assets/6555937/a912b74c-3daa-44f1-a0d4-ef7ecc91bcce). Then with sampling the I/O, the distribution of types of operations hint at some bottleneck opportunity:. ![image](https://github.com/google/deepvariant/assets/6555937/9b6e2296-789b-4eff-852b-f053214f291a). Maybe optimizing for I/O via balancing by having equal number of resources for `|GPU| == |CPU|` with significant on-board memory could do the trick, though not sure if all the code has been ported to Keras, which should help with the distribution strategy in that situation to make it all click together.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:503,safety,resourc,resources,503,"An interesting feature seems jump up when profiling of DV 1.5 in Docker, which looks to be heavy on I/O more than anything else:. ![image](https://github.com/google/deepvariant/assets/6555937/a912b74c-3daa-44f1-a0d4-ef7ecc91bcce). Then with sampling the I/O, the distribution of types of operations hint at some bottleneck opportunity:. ![image](https://github.com/google/deepvariant/assets/6555937/9b6e2296-789b-4eff-852b-f053214f291a). Maybe optimizing for I/O via balancing by having equal number of resources for `|GPU| == |CPU|` with significant on-board memory could do the trick, though not sure if all the code has been ported to Keras, which should help with the distribution strategy in that situation to make it all click together.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:539,security,sign,significant,539,"An interesting feature seems jump up when profiling of DV 1.5 in Docker, which looks to be heavy on I/O more than anything else:. ![image](https://github.com/google/deepvariant/assets/6555937/a912b74c-3daa-44f1-a0d4-ef7ecc91bcce). Then with sampling the I/O, the distribution of types of operations hint at some bottleneck opportunity:. ![image](https://github.com/google/deepvariant/assets/6555937/9b6e2296-789b-4eff-852b-f053214f291a). Maybe optimizing for I/O via balancing by having equal number of resources for `|GPU| == |CPU|` with significant on-board memory could do the trick, though not sure if all the code has been ported to Keras, which should help with the distribution strategy in that situation to make it all click together.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:503,testability,resourc,resources,503,"An interesting feature seems jump up when profiling of DV 1.5 in Docker, which looks to be heavy on I/O more than anything else:. ![image](https://github.com/google/deepvariant/assets/6555937/a912b74c-3daa-44f1-a0d4-ef7ecc91bcce). Then with sampling the I/O, the distribution of types of operations hint at some bottleneck opportunity:. ![image](https://github.com/google/deepvariant/assets/6555937/9b6e2296-789b-4eff-852b-f053214f291a). Maybe optimizing for I/O via balancing by having equal number of resources for `|GPU| == |CPU|` with significant on-board memory could do the trick, though not sure if all the code has been ported to Keras, which should help with the distribution strategy in that situation to make it all click together.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:299,usability,hint,hint,299,"An interesting feature seems jump up when profiling of DV 1.5 in Docker, which looks to be heavy on I/O more than anything else:. ![image](https://github.com/google/deepvariant/assets/6555937/a912b74c-3daa-44f1-a0d4-ef7ecc91bcce). Then with sampling the I/O, the distribution of types of operations hint at some bottleneck opportunity:. ![image](https://github.com/google/deepvariant/assets/6555937/9b6e2296-789b-4eff-852b-f053214f291a). Maybe optimizing for I/O via balancing by having equal number of resources for `|GPU| == |CPU|` with significant on-board memory could do the trick, though not sure if all the code has been ported to Keras, which should help with the distribution strategy in that situation to make it all click together.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:560,usability,memor,memory,560,"An interesting feature seems jump up when profiling of DV 1.5 in Docker, which looks to be heavy on I/O more than anything else:. ![image](https://github.com/google/deepvariant/assets/6555937/a912b74c-3daa-44f1-a0d4-ef7ecc91bcce). Then with sampling the I/O, the distribution of types of operations hint at some bottleneck opportunity:. ![image](https://github.com/google/deepvariant/assets/6555937/9b6e2296-789b-4eff-852b-f053214f291a). Maybe optimizing for I/O via balancing by having equal number of resources for `|GPU| == |CPU|` with significant on-board memory could do the trick, though not sure if all the code has been ported to Keras, which should help with the distribution strategy in that situation to make it all click together.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:658,usability,help,help,658,"An interesting feature seems jump up when profiling of DV 1.5 in Docker, which looks to be heavy on I/O more than anything else:. ![image](https://github.com/google/deepvariant/assets/6555937/a912b74c-3daa-44f1-a0d4-ef7ecc91bcce). Then with sampling the I/O, the distribution of types of operations hint at some bottleneck opportunity:. ![image](https://github.com/google/deepvariant/assets/6555937/9b6e2296-789b-4eff-852b-f053214f291a). Maybe optimizing for I/O via balancing by having equal number of resources for `|GPU| == |CPU|` with significant on-board memory could do the trick, though not sure if all the code has been ported to Keras, which should help with the distribution strategy in that situation to make it all click together.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:703,availability,down,down,703,"Thank you both for your inputs. Regarding the old, anecdotal observation that GPU isn't efficiently used, I didn't collect any GPU resources like this time. And here are the CPU/memory/disk usages for the same input data, under CPU vs under GPU modes, back then. Note these are collected from running the PEPPER pipeline (release 0.4.0). Note how using the GPU didn't help in terms wallclock time. ![pepper shard-0 high-cpu resources](https://github.com/google/deepvariant/assets/16310888/e7970a06-b38d-4972-8a8d-6a5cf0595ef1). ![pepper shard-0 NVDA-P100 resources](https://github.com/google/deepvariant/assets/16310888/78b9da04-d9c7-4c5b-a730-ae99b8ee4908). And here's the cost of the GPU mode, broken down by SKU (we use Google cloud). ![pepper NVDA-P100 cost](https://github.com/google/deepvariant/assets/16310888/64316bdd-3ceb-47cb-a130-dd6da34dc8fc). In terms of running DV in ""production"", we did optimize the CPU pipeline down to below $20/sample. We didn't optimize the GPU pipeline because of the wall-clock issue. Based on the samples I've analyzed with 1.5.0 so far, the cost indeed dropped. Parabricks isn't really an option for us, as we build pipelines using WDL and run them on Terra. With all these said, I'm definitely not saying that this is a blocker for us. It is more of a curiosity/optimization question. Thank you! Steve.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:929,availability,down,down,929,"Thank you both for your inputs. Regarding the old, anecdotal observation that GPU isn't efficiently used, I didn't collect any GPU resources like this time. And here are the CPU/memory/disk usages for the same input data, under CPU vs under GPU modes, back then. Note these are collected from running the PEPPER pipeline (release 0.4.0). Note how using the GPU didn't help in terms wallclock time. ![pepper shard-0 high-cpu resources](https://github.com/google/deepvariant/assets/16310888/e7970a06-b38d-4972-8a8d-6a5cf0595ef1). ![pepper shard-0 NVDA-P100 resources](https://github.com/google/deepvariant/assets/16310888/78b9da04-d9c7-4c5b-a730-ae99b8ee4908). And here's the cost of the GPU mode, broken down by SKU (we use Google cloud). ![pepper NVDA-P100 cost](https://github.com/google/deepvariant/assets/16310888/64316bdd-3ceb-47cb-a130-dd6da34dc8fc). In terms of running DV in ""production"", we did optimize the CPU pipeline down to below $20/sample. We didn't optimize the GPU pipeline because of the wall-clock issue. Based on the samples I've analyzed with 1.5.0 so far, the cost indeed dropped. Parabricks isn't really an option for us, as we build pipelines using WDL and run them on Terra. With all these said, I'm definitely not saying that this is a blocker for us. It is more of a curiosity/optimization question. Thank you! Steve.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:61,deployability,observ,observation,61,"Thank you both for your inputs. Regarding the old, anecdotal observation that GPU isn't efficiently used, I didn't collect any GPU resources like this time. And here are the CPU/memory/disk usages for the same input data, under CPU vs under GPU modes, back then. Note these are collected from running the PEPPER pipeline (release 0.4.0). Note how using the GPU didn't help in terms wallclock time. ![pepper shard-0 high-cpu resources](https://github.com/google/deepvariant/assets/16310888/e7970a06-b38d-4972-8a8d-6a5cf0595ef1). ![pepper shard-0 NVDA-P100 resources](https://github.com/google/deepvariant/assets/16310888/78b9da04-d9c7-4c5b-a730-ae99b8ee4908). And here's the cost of the GPU mode, broken down by SKU (we use Google cloud). ![pepper NVDA-P100 cost](https://github.com/google/deepvariant/assets/16310888/64316bdd-3ceb-47cb-a130-dd6da34dc8fc). In terms of running DV in ""production"", we did optimize the CPU pipeline down to below $20/sample. We didn't optimize the GPU pipeline because of the wall-clock issue. Based on the samples I've analyzed with 1.5.0 so far, the cost indeed dropped. Parabricks isn't really an option for us, as we build pipelines using WDL and run them on Terra. With all these said, I'm definitely not saying that this is a blocker for us. It is more of a curiosity/optimization question. Thank you! Steve.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:131,deployability,resourc,resources,131,"Thank you both for your inputs. Regarding the old, anecdotal observation that GPU isn't efficiently used, I didn't collect any GPU resources like this time. And here are the CPU/memory/disk usages for the same input data, under CPU vs under GPU modes, back then. Note these are collected from running the PEPPER pipeline (release 0.4.0). Note how using the GPU didn't help in terms wallclock time. ![pepper shard-0 high-cpu resources](https://github.com/google/deepvariant/assets/16310888/e7970a06-b38d-4972-8a8d-6a5cf0595ef1). ![pepper shard-0 NVDA-P100 resources](https://github.com/google/deepvariant/assets/16310888/78b9da04-d9c7-4c5b-a730-ae99b8ee4908). And here's the cost of the GPU mode, broken down by SKU (we use Google cloud). ![pepper NVDA-P100 cost](https://github.com/google/deepvariant/assets/16310888/64316bdd-3ceb-47cb-a130-dd6da34dc8fc). In terms of running DV in ""production"", we did optimize the CPU pipeline down to below $20/sample. We didn't optimize the GPU pipeline because of the wall-clock issue. Based on the samples I've analyzed with 1.5.0 so far, the cost indeed dropped. Parabricks isn't really an option for us, as we build pipelines using WDL and run them on Terra. With all these said, I'm definitely not saying that this is a blocker for us. It is more of a curiosity/optimization question. Thank you! Steve.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:312,deployability,pipelin,pipeline,312,"Thank you both for your inputs. Regarding the old, anecdotal observation that GPU isn't efficiently used, I didn't collect any GPU resources like this time. And here are the CPU/memory/disk usages for the same input data, under CPU vs under GPU modes, back then. Note these are collected from running the PEPPER pipeline (release 0.4.0). Note how using the GPU didn't help in terms wallclock time. ![pepper shard-0 high-cpu resources](https://github.com/google/deepvariant/assets/16310888/e7970a06-b38d-4972-8a8d-6a5cf0595ef1). ![pepper shard-0 NVDA-P100 resources](https://github.com/google/deepvariant/assets/16310888/78b9da04-d9c7-4c5b-a730-ae99b8ee4908). And here's the cost of the GPU mode, broken down by SKU (we use Google cloud). ![pepper NVDA-P100 cost](https://github.com/google/deepvariant/assets/16310888/64316bdd-3ceb-47cb-a130-dd6da34dc8fc). In terms of running DV in ""production"", we did optimize the CPU pipeline down to below $20/sample. We didn't optimize the GPU pipeline because of the wall-clock issue. Based on the samples I've analyzed with 1.5.0 so far, the cost indeed dropped. Parabricks isn't really an option for us, as we build pipelines using WDL and run them on Terra. With all these said, I'm definitely not saying that this is a blocker for us. It is more of a curiosity/optimization question. Thank you! Steve.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:322,deployability,releas,release,322,"Thank you both for your inputs. Regarding the old, anecdotal observation that GPU isn't efficiently used, I didn't collect any GPU resources like this time. And here are the CPU/memory/disk usages for the same input data, under CPU vs under GPU modes, back then. Note these are collected from running the PEPPER pipeline (release 0.4.0). Note how using the GPU didn't help in terms wallclock time. ![pepper shard-0 high-cpu resources](https://github.com/google/deepvariant/assets/16310888/e7970a06-b38d-4972-8a8d-6a5cf0595ef1). ![pepper shard-0 NVDA-P100 resources](https://github.com/google/deepvariant/assets/16310888/78b9da04-d9c7-4c5b-a730-ae99b8ee4908). And here's the cost of the GPU mode, broken down by SKU (we use Google cloud). ![pepper NVDA-P100 cost](https://github.com/google/deepvariant/assets/16310888/64316bdd-3ceb-47cb-a130-dd6da34dc8fc). In terms of running DV in ""production"", we did optimize the CPU pipeline down to below $20/sample. We didn't optimize the GPU pipeline because of the wall-clock issue. Based on the samples I've analyzed with 1.5.0 so far, the cost indeed dropped. Parabricks isn't really an option for us, as we build pipelines using WDL and run them on Terra. With all these said, I'm definitely not saying that this is a blocker for us. It is more of a curiosity/optimization question. Thank you! Steve.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:424,deployability,resourc,resources,424,"Thank you both for your inputs. Regarding the old, anecdotal observation that GPU isn't efficiently used, I didn't collect any GPU resources like this time. And here are the CPU/memory/disk usages for the same input data, under CPU vs under GPU modes, back then. Note these are collected from running the PEPPER pipeline (release 0.4.0). Note how using the GPU didn't help in terms wallclock time. ![pepper shard-0 high-cpu resources](https://github.com/google/deepvariant/assets/16310888/e7970a06-b38d-4972-8a8d-6a5cf0595ef1). ![pepper shard-0 NVDA-P100 resources](https://github.com/google/deepvariant/assets/16310888/78b9da04-d9c7-4c5b-a730-ae99b8ee4908). And here's the cost of the GPU mode, broken down by SKU (we use Google cloud). ![pepper NVDA-P100 cost](https://github.com/google/deepvariant/assets/16310888/64316bdd-3ceb-47cb-a130-dd6da34dc8fc). In terms of running DV in ""production"", we did optimize the CPU pipeline down to below $20/sample. We didn't optimize the GPU pipeline because of the wall-clock issue. Based on the samples I've analyzed with 1.5.0 so far, the cost indeed dropped. Parabricks isn't really an option for us, as we build pipelines using WDL and run them on Terra. With all these said, I'm definitely not saying that this is a blocker for us. It is more of a curiosity/optimization question. Thank you! Steve.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:555,deployability,resourc,resources,555,"Thank you both for your inputs. Regarding the old, anecdotal observation that GPU isn't efficiently used, I didn't collect any GPU resources like this time. And here are the CPU/memory/disk usages for the same input data, under CPU vs under GPU modes, back then. Note these are collected from running the PEPPER pipeline (release 0.4.0). Note how using the GPU didn't help in terms wallclock time. ![pepper shard-0 high-cpu resources](https://github.com/google/deepvariant/assets/16310888/e7970a06-b38d-4972-8a8d-6a5cf0595ef1). ![pepper shard-0 NVDA-P100 resources](https://github.com/google/deepvariant/assets/16310888/78b9da04-d9c7-4c5b-a730-ae99b8ee4908). And here's the cost of the GPU mode, broken down by SKU (we use Google cloud). ![pepper NVDA-P100 cost](https://github.com/google/deepvariant/assets/16310888/64316bdd-3ceb-47cb-a130-dd6da34dc8fc). In terms of running DV in ""production"", we did optimize the CPU pipeline down to below $20/sample. We didn't optimize the GPU pipeline because of the wall-clock issue. Based on the samples I've analyzed with 1.5.0 so far, the cost indeed dropped. Parabricks isn't really an option for us, as we build pipelines using WDL and run them on Terra. With all these said, I'm definitely not saying that this is a blocker for us. It is more of a curiosity/optimization question. Thank you! Steve.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:920,deployability,pipelin,pipeline,920,"Thank you both for your inputs. Regarding the old, anecdotal observation that GPU isn't efficiently used, I didn't collect any GPU resources like this time. And here are the CPU/memory/disk usages for the same input data, under CPU vs under GPU modes, back then. Note these are collected from running the PEPPER pipeline (release 0.4.0). Note how using the GPU didn't help in terms wallclock time. ![pepper shard-0 high-cpu resources](https://github.com/google/deepvariant/assets/16310888/e7970a06-b38d-4972-8a8d-6a5cf0595ef1). ![pepper shard-0 NVDA-P100 resources](https://github.com/google/deepvariant/assets/16310888/78b9da04-d9c7-4c5b-a730-ae99b8ee4908). And here's the cost of the GPU mode, broken down by SKU (we use Google cloud). ![pepper NVDA-P100 cost](https://github.com/google/deepvariant/assets/16310888/64316bdd-3ceb-47cb-a130-dd6da34dc8fc). In terms of running DV in ""production"", we did optimize the CPU pipeline down to below $20/sample. We didn't optimize the GPU pipeline because of the wall-clock issue. Based on the samples I've analyzed with 1.5.0 so far, the cost indeed dropped. Parabricks isn't really an option for us, as we build pipelines using WDL and run them on Terra. With all these said, I'm definitely not saying that this is a blocker for us. It is more of a curiosity/optimization question. Thank you! Steve.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:982,deployability,pipelin,pipeline,982,"Thank you both for your inputs. Regarding the old, anecdotal observation that GPU isn't efficiently used, I didn't collect any GPU resources like this time. And here are the CPU/memory/disk usages for the same input data, under CPU vs under GPU modes, back then. Note these are collected from running the PEPPER pipeline (release 0.4.0). Note how using the GPU didn't help in terms wallclock time. ![pepper shard-0 high-cpu resources](https://github.com/google/deepvariant/assets/16310888/e7970a06-b38d-4972-8a8d-6a5cf0595ef1). ![pepper shard-0 NVDA-P100 resources](https://github.com/google/deepvariant/assets/16310888/78b9da04-d9c7-4c5b-a730-ae99b8ee4908). And here's the cost of the GPU mode, broken down by SKU (we use Google cloud). ![pepper NVDA-P100 cost](https://github.com/google/deepvariant/assets/16310888/64316bdd-3ceb-47cb-a130-dd6da34dc8fc). In terms of running DV in ""production"", we did optimize the CPU pipeline down to below $20/sample. We didn't optimize the GPU pipeline because of the wall-clock issue. Based on the samples I've analyzed with 1.5.0 so far, the cost indeed dropped. Parabricks isn't really an option for us, as we build pipelines using WDL and run them on Terra. With all these said, I'm definitely not saying that this is a blocker for us. It is more of a curiosity/optimization question. Thank you! Steve.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:1151,deployability,build,build,1151,"Thank you both for your inputs. Regarding the old, anecdotal observation that GPU isn't efficiently used, I didn't collect any GPU resources like this time. And here are the CPU/memory/disk usages for the same input data, under CPU vs under GPU modes, back then. Note these are collected from running the PEPPER pipeline (release 0.4.0). Note how using the GPU didn't help in terms wallclock time. ![pepper shard-0 high-cpu resources](https://github.com/google/deepvariant/assets/16310888/e7970a06-b38d-4972-8a8d-6a5cf0595ef1). ![pepper shard-0 NVDA-P100 resources](https://github.com/google/deepvariant/assets/16310888/78b9da04-d9c7-4c5b-a730-ae99b8ee4908). And here's the cost of the GPU mode, broken down by SKU (we use Google cloud). ![pepper NVDA-P100 cost](https://github.com/google/deepvariant/assets/16310888/64316bdd-3ceb-47cb-a130-dd6da34dc8fc). In terms of running DV in ""production"", we did optimize the CPU pipeline down to below $20/sample. We didn't optimize the GPU pipeline because of the wall-clock issue. Based on the samples I've analyzed with 1.5.0 so far, the cost indeed dropped. Parabricks isn't really an option for us, as we build pipelines using WDL and run them on Terra. With all these said, I'm definitely not saying that this is a blocker for us. It is more of a curiosity/optimization question. Thank you! Steve.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:1157,deployability,pipelin,pipelines,1157,"Thank you both for your inputs. Regarding the old, anecdotal observation that GPU isn't efficiently used, I didn't collect any GPU resources like this time. And here are the CPU/memory/disk usages for the same input data, under CPU vs under GPU modes, back then. Note these are collected from running the PEPPER pipeline (release 0.4.0). Note how using the GPU didn't help in terms wallclock time. ![pepper shard-0 high-cpu resources](https://github.com/google/deepvariant/assets/16310888/e7970a06-b38d-4972-8a8d-6a5cf0595ef1). ![pepper shard-0 NVDA-P100 resources](https://github.com/google/deepvariant/assets/16310888/78b9da04-d9c7-4c5b-a730-ae99b8ee4908). And here's the cost of the GPU mode, broken down by SKU (we use Google cloud). ![pepper NVDA-P100 cost](https://github.com/google/deepvariant/assets/16310888/64316bdd-3ceb-47cb-a130-dd6da34dc8fc). In terms of running DV in ""production"", we did optimize the CPU pipeline down to below $20/sample. We didn't optimize the GPU pipeline because of the wall-clock issue. Based on the samples I've analyzed with 1.5.0 so far, the cost indeed dropped. Parabricks isn't really an option for us, as we build pipelines using WDL and run them on Terra. With all these said, I'm definitely not saying that this is a blocker for us. It is more of a curiosity/optimization question. Thank you! Steve.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:78,energy efficiency,GPU,GPU,78,"Thank you both for your inputs. Regarding the old, anecdotal observation that GPU isn't efficiently used, I didn't collect any GPU resources like this time. And here are the CPU/memory/disk usages for the same input data, under CPU vs under GPU modes, back then. Note these are collected from running the PEPPER pipeline (release 0.4.0). Note how using the GPU didn't help in terms wallclock time. ![pepper shard-0 high-cpu resources](https://github.com/google/deepvariant/assets/16310888/e7970a06-b38d-4972-8a8d-6a5cf0595ef1). ![pepper shard-0 NVDA-P100 resources](https://github.com/google/deepvariant/assets/16310888/78b9da04-d9c7-4c5b-a730-ae99b8ee4908). And here's the cost of the GPU mode, broken down by SKU (we use Google cloud). ![pepper NVDA-P100 cost](https://github.com/google/deepvariant/assets/16310888/64316bdd-3ceb-47cb-a130-dd6da34dc8fc). In terms of running DV in ""production"", we did optimize the CPU pipeline down to below $20/sample. We didn't optimize the GPU pipeline because of the wall-clock issue. Based on the samples I've analyzed with 1.5.0 so far, the cost indeed dropped. Parabricks isn't really an option for us, as we build pipelines using WDL and run them on Terra. With all these said, I'm definitely not saying that this is a blocker for us. It is more of a curiosity/optimization question. Thank you! Steve.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:127,energy efficiency,GPU,GPU,127,"Thank you both for your inputs. Regarding the old, anecdotal observation that GPU isn't efficiently used, I didn't collect any GPU resources like this time. And here are the CPU/memory/disk usages for the same input data, under CPU vs under GPU modes, back then. Note these are collected from running the PEPPER pipeline (release 0.4.0). Note how using the GPU didn't help in terms wallclock time. ![pepper shard-0 high-cpu resources](https://github.com/google/deepvariant/assets/16310888/e7970a06-b38d-4972-8a8d-6a5cf0595ef1). ![pepper shard-0 NVDA-P100 resources](https://github.com/google/deepvariant/assets/16310888/78b9da04-d9c7-4c5b-a730-ae99b8ee4908). And here's the cost of the GPU mode, broken down by SKU (we use Google cloud). ![pepper NVDA-P100 cost](https://github.com/google/deepvariant/assets/16310888/64316bdd-3ceb-47cb-a130-dd6da34dc8fc). In terms of running DV in ""production"", we did optimize the CPU pipeline down to below $20/sample. We didn't optimize the GPU pipeline because of the wall-clock issue. Based on the samples I've analyzed with 1.5.0 so far, the cost indeed dropped. Parabricks isn't really an option for us, as we build pipelines using WDL and run them on Terra. With all these said, I'm definitely not saying that this is a blocker for us. It is more of a curiosity/optimization question. Thank you! Steve.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:131,energy efficiency,resourc,resources,131,"Thank you both for your inputs. Regarding the old, anecdotal observation that GPU isn't efficiently used, I didn't collect any GPU resources like this time. And here are the CPU/memory/disk usages for the same input data, under CPU vs under GPU modes, back then. Note these are collected from running the PEPPER pipeline (release 0.4.0). Note how using the GPU didn't help in terms wallclock time. ![pepper shard-0 high-cpu resources](https://github.com/google/deepvariant/assets/16310888/e7970a06-b38d-4972-8a8d-6a5cf0595ef1). ![pepper shard-0 NVDA-P100 resources](https://github.com/google/deepvariant/assets/16310888/78b9da04-d9c7-4c5b-a730-ae99b8ee4908). And here's the cost of the GPU mode, broken down by SKU (we use Google cloud). ![pepper NVDA-P100 cost](https://github.com/google/deepvariant/assets/16310888/64316bdd-3ceb-47cb-a130-dd6da34dc8fc). In terms of running DV in ""production"", we did optimize the CPU pipeline down to below $20/sample. We didn't optimize the GPU pipeline because of the wall-clock issue. Based on the samples I've analyzed with 1.5.0 so far, the cost indeed dropped. Parabricks isn't really an option for us, as we build pipelines using WDL and run them on Terra. With all these said, I'm definitely not saying that this is a blocker for us. It is more of a curiosity/optimization question. Thank you! Steve.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:174,energy efficiency,CPU,CPU,174,"Thank you both for your inputs. Regarding the old, anecdotal observation that GPU isn't efficiently used, I didn't collect any GPU resources like this time. And here are the CPU/memory/disk usages for the same input data, under CPU vs under GPU modes, back then. Note these are collected from running the PEPPER pipeline (release 0.4.0). Note how using the GPU didn't help in terms wallclock time. ![pepper shard-0 high-cpu resources](https://github.com/google/deepvariant/assets/16310888/e7970a06-b38d-4972-8a8d-6a5cf0595ef1). ![pepper shard-0 NVDA-P100 resources](https://github.com/google/deepvariant/assets/16310888/78b9da04-d9c7-4c5b-a730-ae99b8ee4908). And here's the cost of the GPU mode, broken down by SKU (we use Google cloud). ![pepper NVDA-P100 cost](https://github.com/google/deepvariant/assets/16310888/64316bdd-3ceb-47cb-a130-dd6da34dc8fc). In terms of running DV in ""production"", we did optimize the CPU pipeline down to below $20/sample. We didn't optimize the GPU pipeline because of the wall-clock issue. Based on the samples I've analyzed with 1.5.0 so far, the cost indeed dropped. Parabricks isn't really an option for us, as we build pipelines using WDL and run them on Terra. With all these said, I'm definitely not saying that this is a blocker for us. It is more of a curiosity/optimization question. Thank you! Steve.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:228,energy efficiency,CPU,CPU,228,"Thank you both for your inputs. Regarding the old, anecdotal observation that GPU isn't efficiently used, I didn't collect any GPU resources like this time. And here are the CPU/memory/disk usages for the same input data, under CPU vs under GPU modes, back then. Note these are collected from running the PEPPER pipeline (release 0.4.0). Note how using the GPU didn't help in terms wallclock time. ![pepper shard-0 high-cpu resources](https://github.com/google/deepvariant/assets/16310888/e7970a06-b38d-4972-8a8d-6a5cf0595ef1). ![pepper shard-0 NVDA-P100 resources](https://github.com/google/deepvariant/assets/16310888/78b9da04-d9c7-4c5b-a730-ae99b8ee4908). And here's the cost of the GPU mode, broken down by SKU (we use Google cloud). ![pepper NVDA-P100 cost](https://github.com/google/deepvariant/assets/16310888/64316bdd-3ceb-47cb-a130-dd6da34dc8fc). In terms of running DV in ""production"", we did optimize the CPU pipeline down to below $20/sample. We didn't optimize the GPU pipeline because of the wall-clock issue. Based on the samples I've analyzed with 1.5.0 so far, the cost indeed dropped. Parabricks isn't really an option for us, as we build pipelines using WDL and run them on Terra. With all these said, I'm definitely not saying that this is a blocker for us. It is more of a curiosity/optimization question. Thank you! Steve.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:241,energy efficiency,GPU,GPU,241,"Thank you both for your inputs. Regarding the old, anecdotal observation that GPU isn't efficiently used, I didn't collect any GPU resources like this time. And here are the CPU/memory/disk usages for the same input data, under CPU vs under GPU modes, back then. Note these are collected from running the PEPPER pipeline (release 0.4.0). Note how using the GPU didn't help in terms wallclock time. ![pepper shard-0 high-cpu resources](https://github.com/google/deepvariant/assets/16310888/e7970a06-b38d-4972-8a8d-6a5cf0595ef1). ![pepper shard-0 NVDA-P100 resources](https://github.com/google/deepvariant/assets/16310888/78b9da04-d9c7-4c5b-a730-ae99b8ee4908). And here's the cost of the GPU mode, broken down by SKU (we use Google cloud). ![pepper NVDA-P100 cost](https://github.com/google/deepvariant/assets/16310888/64316bdd-3ceb-47cb-a130-dd6da34dc8fc). In terms of running DV in ""production"", we did optimize the CPU pipeline down to below $20/sample. We didn't optimize the GPU pipeline because of the wall-clock issue. Based on the samples I've analyzed with 1.5.0 so far, the cost indeed dropped. Parabricks isn't really an option for us, as we build pipelines using WDL and run them on Terra. With all these said, I'm definitely not saying that this is a blocker for us. It is more of a curiosity/optimization question. Thank you! Steve.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:357,energy efficiency,GPU,GPU,357,"Thank you both for your inputs. Regarding the old, anecdotal observation that GPU isn't efficiently used, I didn't collect any GPU resources like this time. And here are the CPU/memory/disk usages for the same input data, under CPU vs under GPU modes, back then. Note these are collected from running the PEPPER pipeline (release 0.4.0). Note how using the GPU didn't help in terms wallclock time. ![pepper shard-0 high-cpu resources](https://github.com/google/deepvariant/assets/16310888/e7970a06-b38d-4972-8a8d-6a5cf0595ef1). ![pepper shard-0 NVDA-P100 resources](https://github.com/google/deepvariant/assets/16310888/78b9da04-d9c7-4c5b-a730-ae99b8ee4908). And here's the cost of the GPU mode, broken down by SKU (we use Google cloud). ![pepper NVDA-P100 cost](https://github.com/google/deepvariant/assets/16310888/64316bdd-3ceb-47cb-a130-dd6da34dc8fc). In terms of running DV in ""production"", we did optimize the CPU pipeline down to below $20/sample. We didn't optimize the GPU pipeline because of the wall-clock issue. Based on the samples I've analyzed with 1.5.0 so far, the cost indeed dropped. Parabricks isn't really an option for us, as we build pipelines using WDL and run them on Terra. With all these said, I'm definitely not saying that this is a blocker for us. It is more of a curiosity/optimization question. Thank you! Steve.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:420,energy efficiency,cpu,cpu,420,"Thank you both for your inputs. Regarding the old, anecdotal observation that GPU isn't efficiently used, I didn't collect any GPU resources like this time. And here are the CPU/memory/disk usages for the same input data, under CPU vs under GPU modes, back then. Note these are collected from running the PEPPER pipeline (release 0.4.0). Note how using the GPU didn't help in terms wallclock time. ![pepper shard-0 high-cpu resources](https://github.com/google/deepvariant/assets/16310888/e7970a06-b38d-4972-8a8d-6a5cf0595ef1). ![pepper shard-0 NVDA-P100 resources](https://github.com/google/deepvariant/assets/16310888/78b9da04-d9c7-4c5b-a730-ae99b8ee4908). And here's the cost of the GPU mode, broken down by SKU (we use Google cloud). ![pepper NVDA-P100 cost](https://github.com/google/deepvariant/assets/16310888/64316bdd-3ceb-47cb-a130-dd6da34dc8fc). In terms of running DV in ""production"", we did optimize the CPU pipeline down to below $20/sample. We didn't optimize the GPU pipeline because of the wall-clock issue. Based on the samples I've analyzed with 1.5.0 so far, the cost indeed dropped. Parabricks isn't really an option for us, as we build pipelines using WDL and run them on Terra. With all these said, I'm definitely not saying that this is a blocker for us. It is more of a curiosity/optimization question. Thank you! Steve.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:424,energy efficiency,resourc,resources,424,"Thank you both for your inputs. Regarding the old, anecdotal observation that GPU isn't efficiently used, I didn't collect any GPU resources like this time. And here are the CPU/memory/disk usages for the same input data, under CPU vs under GPU modes, back then. Note these are collected from running the PEPPER pipeline (release 0.4.0). Note how using the GPU didn't help in terms wallclock time. ![pepper shard-0 high-cpu resources](https://github.com/google/deepvariant/assets/16310888/e7970a06-b38d-4972-8a8d-6a5cf0595ef1). ![pepper shard-0 NVDA-P100 resources](https://github.com/google/deepvariant/assets/16310888/78b9da04-d9c7-4c5b-a730-ae99b8ee4908). And here's the cost of the GPU mode, broken down by SKU (we use Google cloud). ![pepper NVDA-P100 cost](https://github.com/google/deepvariant/assets/16310888/64316bdd-3ceb-47cb-a130-dd6da34dc8fc). In terms of running DV in ""production"", we did optimize the CPU pipeline down to below $20/sample. We didn't optimize the GPU pipeline because of the wall-clock issue. Based on the samples I've analyzed with 1.5.0 so far, the cost indeed dropped. Parabricks isn't really an option for us, as we build pipelines using WDL and run them on Terra. With all these said, I'm definitely not saying that this is a blocker for us. It is more of a curiosity/optimization question. Thank you! Steve.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:555,energy efficiency,resourc,resources,555,"Thank you both for your inputs. Regarding the old, anecdotal observation that GPU isn't efficiently used, I didn't collect any GPU resources like this time. And here are the CPU/memory/disk usages for the same input data, under CPU vs under GPU modes, back then. Note these are collected from running the PEPPER pipeline (release 0.4.0). Note how using the GPU didn't help in terms wallclock time. ![pepper shard-0 high-cpu resources](https://github.com/google/deepvariant/assets/16310888/e7970a06-b38d-4972-8a8d-6a5cf0595ef1). ![pepper shard-0 NVDA-P100 resources](https://github.com/google/deepvariant/assets/16310888/78b9da04-d9c7-4c5b-a730-ae99b8ee4908). And here's the cost of the GPU mode, broken down by SKU (we use Google cloud). ![pepper NVDA-P100 cost](https://github.com/google/deepvariant/assets/16310888/64316bdd-3ceb-47cb-a130-dd6da34dc8fc). In terms of running DV in ""production"", we did optimize the CPU pipeline down to below $20/sample. We didn't optimize the GPU pipeline because of the wall-clock issue. Based on the samples I've analyzed with 1.5.0 so far, the cost indeed dropped. Parabricks isn't really an option for us, as we build pipelines using WDL and run them on Terra. With all these said, I'm definitely not saying that this is a blocker for us. It is more of a curiosity/optimization question. Thank you! Steve.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:686,energy efficiency,GPU,GPU,686,"Thank you both for your inputs. Regarding the old, anecdotal observation that GPU isn't efficiently used, I didn't collect any GPU resources like this time. And here are the CPU/memory/disk usages for the same input data, under CPU vs under GPU modes, back then. Note these are collected from running the PEPPER pipeline (release 0.4.0). Note how using the GPU didn't help in terms wallclock time. ![pepper shard-0 high-cpu resources](https://github.com/google/deepvariant/assets/16310888/e7970a06-b38d-4972-8a8d-6a5cf0595ef1). ![pepper shard-0 NVDA-P100 resources](https://github.com/google/deepvariant/assets/16310888/78b9da04-d9c7-4c5b-a730-ae99b8ee4908). And here's the cost of the GPU mode, broken down by SKU (we use Google cloud). ![pepper NVDA-P100 cost](https://github.com/google/deepvariant/assets/16310888/64316bdd-3ceb-47cb-a130-dd6da34dc8fc). In terms of running DV in ""production"", we did optimize the CPU pipeline down to below $20/sample. We didn't optimize the GPU pipeline because of the wall-clock issue. Based on the samples I've analyzed with 1.5.0 so far, the cost indeed dropped. Parabricks isn't really an option for us, as we build pipelines using WDL and run them on Terra. With all these said, I'm definitely not saying that this is a blocker for us. It is more of a curiosity/optimization question. Thank you! Steve.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:730,energy efficiency,cloud,cloud,730,"Thank you both for your inputs. Regarding the old, anecdotal observation that GPU isn't efficiently used, I didn't collect any GPU resources like this time. And here are the CPU/memory/disk usages for the same input data, under CPU vs under GPU modes, back then. Note these are collected from running the PEPPER pipeline (release 0.4.0). Note how using the GPU didn't help in terms wallclock time. ![pepper shard-0 high-cpu resources](https://github.com/google/deepvariant/assets/16310888/e7970a06-b38d-4972-8a8d-6a5cf0595ef1). ![pepper shard-0 NVDA-P100 resources](https://github.com/google/deepvariant/assets/16310888/78b9da04-d9c7-4c5b-a730-ae99b8ee4908). And here's the cost of the GPU mode, broken down by SKU (we use Google cloud). ![pepper NVDA-P100 cost](https://github.com/google/deepvariant/assets/16310888/64316bdd-3ceb-47cb-a130-dd6da34dc8fc). In terms of running DV in ""production"", we did optimize the CPU pipeline down to below $20/sample. We didn't optimize the GPU pipeline because of the wall-clock issue. Based on the samples I've analyzed with 1.5.0 so far, the cost indeed dropped. Parabricks isn't really an option for us, as we build pipelines using WDL and run them on Terra. With all these said, I'm definitely not saying that this is a blocker for us. It is more of a curiosity/optimization question. Thank you! Steve.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:903,energy efficiency,optim,optimize,903,"Thank you both for your inputs. Regarding the old, anecdotal observation that GPU isn't efficiently used, I didn't collect any GPU resources like this time. And here are the CPU/memory/disk usages for the same input data, under CPU vs under GPU modes, back then. Note these are collected from running the PEPPER pipeline (release 0.4.0). Note how using the GPU didn't help in terms wallclock time. ![pepper shard-0 high-cpu resources](https://github.com/google/deepvariant/assets/16310888/e7970a06-b38d-4972-8a8d-6a5cf0595ef1). ![pepper shard-0 NVDA-P100 resources](https://github.com/google/deepvariant/assets/16310888/78b9da04-d9c7-4c5b-a730-ae99b8ee4908). And here's the cost of the GPU mode, broken down by SKU (we use Google cloud). ![pepper NVDA-P100 cost](https://github.com/google/deepvariant/assets/16310888/64316bdd-3ceb-47cb-a130-dd6da34dc8fc). In terms of running DV in ""production"", we did optimize the CPU pipeline down to below $20/sample. We didn't optimize the GPU pipeline because of the wall-clock issue. Based on the samples I've analyzed with 1.5.0 so far, the cost indeed dropped. Parabricks isn't really an option for us, as we build pipelines using WDL and run them on Terra. With all these said, I'm definitely not saying that this is a blocker for us. It is more of a curiosity/optimization question. Thank you! Steve.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:916,energy efficiency,CPU,CPU,916,"Thank you both for your inputs. Regarding the old, anecdotal observation that GPU isn't efficiently used, I didn't collect any GPU resources like this time. And here are the CPU/memory/disk usages for the same input data, under CPU vs under GPU modes, back then. Note these are collected from running the PEPPER pipeline (release 0.4.0). Note how using the GPU didn't help in terms wallclock time. ![pepper shard-0 high-cpu resources](https://github.com/google/deepvariant/assets/16310888/e7970a06-b38d-4972-8a8d-6a5cf0595ef1). ![pepper shard-0 NVDA-P100 resources](https://github.com/google/deepvariant/assets/16310888/78b9da04-d9c7-4c5b-a730-ae99b8ee4908). And here's the cost of the GPU mode, broken down by SKU (we use Google cloud). ![pepper NVDA-P100 cost](https://github.com/google/deepvariant/assets/16310888/64316bdd-3ceb-47cb-a130-dd6da34dc8fc). In terms of running DV in ""production"", we did optimize the CPU pipeline down to below $20/sample. We didn't optimize the GPU pipeline because of the wall-clock issue. Based on the samples I've analyzed with 1.5.0 so far, the cost indeed dropped. Parabricks isn't really an option for us, as we build pipelines using WDL and run them on Terra. With all these said, I'm definitely not saying that this is a blocker for us. It is more of a curiosity/optimization question. Thank you! Steve.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:965,energy efficiency,optim,optimize,965,"Thank you both for your inputs. Regarding the old, anecdotal observation that GPU isn't efficiently used, I didn't collect any GPU resources like this time. And here are the CPU/memory/disk usages for the same input data, under CPU vs under GPU modes, back then. Note these are collected from running the PEPPER pipeline (release 0.4.0). Note how using the GPU didn't help in terms wallclock time. ![pepper shard-0 high-cpu resources](https://github.com/google/deepvariant/assets/16310888/e7970a06-b38d-4972-8a8d-6a5cf0595ef1). ![pepper shard-0 NVDA-P100 resources](https://github.com/google/deepvariant/assets/16310888/78b9da04-d9c7-4c5b-a730-ae99b8ee4908). And here's the cost of the GPU mode, broken down by SKU (we use Google cloud). ![pepper NVDA-P100 cost](https://github.com/google/deepvariant/assets/16310888/64316bdd-3ceb-47cb-a130-dd6da34dc8fc). In terms of running DV in ""production"", we did optimize the CPU pipeline down to below $20/sample. We didn't optimize the GPU pipeline because of the wall-clock issue. Based on the samples I've analyzed with 1.5.0 so far, the cost indeed dropped. Parabricks isn't really an option for us, as we build pipelines using WDL and run them on Terra. With all these said, I'm definitely not saying that this is a blocker for us. It is more of a curiosity/optimization question. Thank you! Steve.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:978,energy efficiency,GPU,GPU,978,"Thank you both for your inputs. Regarding the old, anecdotal observation that GPU isn't efficiently used, I didn't collect any GPU resources like this time. And here are the CPU/memory/disk usages for the same input data, under CPU vs under GPU modes, back then. Note these are collected from running the PEPPER pipeline (release 0.4.0). Note how using the GPU didn't help in terms wallclock time. ![pepper shard-0 high-cpu resources](https://github.com/google/deepvariant/assets/16310888/e7970a06-b38d-4972-8a8d-6a5cf0595ef1). ![pepper shard-0 NVDA-P100 resources](https://github.com/google/deepvariant/assets/16310888/78b9da04-d9c7-4c5b-a730-ae99b8ee4908). And here's the cost of the GPU mode, broken down by SKU (we use Google cloud). ![pepper NVDA-P100 cost](https://github.com/google/deepvariant/assets/16310888/64316bdd-3ceb-47cb-a130-dd6da34dc8fc). In terms of running DV in ""production"", we did optimize the CPU pipeline down to below $20/sample. We didn't optimize the GPU pipeline because of the wall-clock issue. Based on the samples I've analyzed with 1.5.0 so far, the cost indeed dropped. Parabricks isn't really an option for us, as we build pipelines using WDL and run them on Terra. With all these said, I'm definitely not saying that this is a blocker for us. It is more of a curiosity/optimization question. Thank you! Steve.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:1011,energy efficiency,clock,clock,1011,"Thank you both for your inputs. Regarding the old, anecdotal observation that GPU isn't efficiently used, I didn't collect any GPU resources like this time. And here are the CPU/memory/disk usages for the same input data, under CPU vs under GPU modes, back then. Note these are collected from running the PEPPER pipeline (release 0.4.0). Note how using the GPU didn't help in terms wallclock time. ![pepper shard-0 high-cpu resources](https://github.com/google/deepvariant/assets/16310888/e7970a06-b38d-4972-8a8d-6a5cf0595ef1). ![pepper shard-0 NVDA-P100 resources](https://github.com/google/deepvariant/assets/16310888/78b9da04-d9c7-4c5b-a730-ae99b8ee4908). And here's the cost of the GPU mode, broken down by SKU (we use Google cloud). ![pepper NVDA-P100 cost](https://github.com/google/deepvariant/assets/16310888/64316bdd-3ceb-47cb-a130-dd6da34dc8fc). In terms of running DV in ""production"", we did optimize the CPU pipeline down to below $20/sample. We didn't optimize the GPU pipeline because of the wall-clock issue. Based on the samples I've analyzed with 1.5.0 so far, the cost indeed dropped. Parabricks isn't really an option for us, as we build pipelines using WDL and run them on Terra. With all these said, I'm definitely not saying that this is a blocker for us. It is more of a curiosity/optimization question. Thank you! Steve.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:1304,energy efficiency,optim,optimization,1304,"Thank you both for your inputs. Regarding the old, anecdotal observation that GPU isn't efficiently used, I didn't collect any GPU resources like this time. And here are the CPU/memory/disk usages for the same input data, under CPU vs under GPU modes, back then. Note these are collected from running the PEPPER pipeline (release 0.4.0). Note how using the GPU didn't help in terms wallclock time. ![pepper shard-0 high-cpu resources](https://github.com/google/deepvariant/assets/16310888/e7970a06-b38d-4972-8a8d-6a5cf0595ef1). ![pepper shard-0 NVDA-P100 resources](https://github.com/google/deepvariant/assets/16310888/78b9da04-d9c7-4c5b-a730-ae99b8ee4908). And here's the cost of the GPU mode, broken down by SKU (we use Google cloud). ![pepper NVDA-P100 cost](https://github.com/google/deepvariant/assets/16310888/64316bdd-3ceb-47cb-a130-dd6da34dc8fc). In terms of running DV in ""production"", we did optimize the CPU pipeline down to below $20/sample. We didn't optimize the GPU pipeline because of the wall-clock issue. Based on the samples I've analyzed with 1.5.0 so far, the cost indeed dropped. Parabricks isn't really an option for us, as we build pipelines using WDL and run them on Terra. With all these said, I'm definitely not saying that this is a blocker for us. It is more of a curiosity/optimization question. Thank you! Steve.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:312,integrability,pipelin,pipeline,312,"Thank you both for your inputs. Regarding the old, anecdotal observation that GPU isn't efficiently used, I didn't collect any GPU resources like this time. And here are the CPU/memory/disk usages for the same input data, under CPU vs under GPU modes, back then. Note these are collected from running the PEPPER pipeline (release 0.4.0). Note how using the GPU didn't help in terms wallclock time. ![pepper shard-0 high-cpu resources](https://github.com/google/deepvariant/assets/16310888/e7970a06-b38d-4972-8a8d-6a5cf0595ef1). ![pepper shard-0 NVDA-P100 resources](https://github.com/google/deepvariant/assets/16310888/78b9da04-d9c7-4c5b-a730-ae99b8ee4908). And here's the cost of the GPU mode, broken down by SKU (we use Google cloud). ![pepper NVDA-P100 cost](https://github.com/google/deepvariant/assets/16310888/64316bdd-3ceb-47cb-a130-dd6da34dc8fc). In terms of running DV in ""production"", we did optimize the CPU pipeline down to below $20/sample. We didn't optimize the GPU pipeline because of the wall-clock issue. Based on the samples I've analyzed with 1.5.0 so far, the cost indeed dropped. Parabricks isn't really an option for us, as we build pipelines using WDL and run them on Terra. With all these said, I'm definitely not saying that this is a blocker for us. It is more of a curiosity/optimization question. Thank you! Steve.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:920,integrability,pipelin,pipeline,920,"Thank you both for your inputs. Regarding the old, anecdotal observation that GPU isn't efficiently used, I didn't collect any GPU resources like this time. And here are the CPU/memory/disk usages for the same input data, under CPU vs under GPU modes, back then. Note these are collected from running the PEPPER pipeline (release 0.4.0). Note how using the GPU didn't help in terms wallclock time. ![pepper shard-0 high-cpu resources](https://github.com/google/deepvariant/assets/16310888/e7970a06-b38d-4972-8a8d-6a5cf0595ef1). ![pepper shard-0 NVDA-P100 resources](https://github.com/google/deepvariant/assets/16310888/78b9da04-d9c7-4c5b-a730-ae99b8ee4908). And here's the cost of the GPU mode, broken down by SKU (we use Google cloud). ![pepper NVDA-P100 cost](https://github.com/google/deepvariant/assets/16310888/64316bdd-3ceb-47cb-a130-dd6da34dc8fc). In terms of running DV in ""production"", we did optimize the CPU pipeline down to below $20/sample. We didn't optimize the GPU pipeline because of the wall-clock issue. Based on the samples I've analyzed with 1.5.0 so far, the cost indeed dropped. Parabricks isn't really an option for us, as we build pipelines using WDL and run them on Terra. With all these said, I'm definitely not saying that this is a blocker for us. It is more of a curiosity/optimization question. Thank you! Steve.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:982,integrability,pipelin,pipeline,982,"Thank you both for your inputs. Regarding the old, anecdotal observation that GPU isn't efficiently used, I didn't collect any GPU resources like this time. And here are the CPU/memory/disk usages for the same input data, under CPU vs under GPU modes, back then. Note these are collected from running the PEPPER pipeline (release 0.4.0). Note how using the GPU didn't help in terms wallclock time. ![pepper shard-0 high-cpu resources](https://github.com/google/deepvariant/assets/16310888/e7970a06-b38d-4972-8a8d-6a5cf0595ef1). ![pepper shard-0 NVDA-P100 resources](https://github.com/google/deepvariant/assets/16310888/78b9da04-d9c7-4c5b-a730-ae99b8ee4908). And here's the cost of the GPU mode, broken down by SKU (we use Google cloud). ![pepper NVDA-P100 cost](https://github.com/google/deepvariant/assets/16310888/64316bdd-3ceb-47cb-a130-dd6da34dc8fc). In terms of running DV in ""production"", we did optimize the CPU pipeline down to below $20/sample. We didn't optimize the GPU pipeline because of the wall-clock issue. Based on the samples I've analyzed with 1.5.0 so far, the cost indeed dropped. Parabricks isn't really an option for us, as we build pipelines using WDL and run them on Terra. With all these said, I'm definitely not saying that this is a blocker for us. It is more of a curiosity/optimization question. Thank you! Steve.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:1157,integrability,pipelin,pipelines,1157,"Thank you both for your inputs. Regarding the old, anecdotal observation that GPU isn't efficiently used, I didn't collect any GPU resources like this time. And here are the CPU/memory/disk usages for the same input data, under CPU vs under GPU modes, back then. Note these are collected from running the PEPPER pipeline (release 0.4.0). Note how using the GPU didn't help in terms wallclock time. ![pepper shard-0 high-cpu resources](https://github.com/google/deepvariant/assets/16310888/e7970a06-b38d-4972-8a8d-6a5cf0595ef1). ![pepper shard-0 NVDA-P100 resources](https://github.com/google/deepvariant/assets/16310888/78b9da04-d9c7-4c5b-a730-ae99b8ee4908). And here's the cost of the GPU mode, broken down by SKU (we use Google cloud). ![pepper NVDA-P100 cost](https://github.com/google/deepvariant/assets/16310888/64316bdd-3ceb-47cb-a130-dd6da34dc8fc). In terms of running DV in ""production"", we did optimize the CPU pipeline down to below $20/sample. We didn't optimize the GPU pipeline because of the wall-clock issue. Based on the samples I've analyzed with 1.5.0 so far, the cost indeed dropped. Parabricks isn't really an option for us, as we build pipelines using WDL and run them on Terra. With all these said, I'm definitely not saying that this is a blocker for us. It is more of a curiosity/optimization question. Thank you! Steve.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:78,performance,GPU,GPU,78,"Thank you both for your inputs. Regarding the old, anecdotal observation that GPU isn't efficiently used, I didn't collect any GPU resources like this time. And here are the CPU/memory/disk usages for the same input data, under CPU vs under GPU modes, back then. Note these are collected from running the PEPPER pipeline (release 0.4.0). Note how using the GPU didn't help in terms wallclock time. ![pepper shard-0 high-cpu resources](https://github.com/google/deepvariant/assets/16310888/e7970a06-b38d-4972-8a8d-6a5cf0595ef1). ![pepper shard-0 NVDA-P100 resources](https://github.com/google/deepvariant/assets/16310888/78b9da04-d9c7-4c5b-a730-ae99b8ee4908). And here's the cost of the GPU mode, broken down by SKU (we use Google cloud). ![pepper NVDA-P100 cost](https://github.com/google/deepvariant/assets/16310888/64316bdd-3ceb-47cb-a130-dd6da34dc8fc). In terms of running DV in ""production"", we did optimize the CPU pipeline down to below $20/sample. We didn't optimize the GPU pipeline because of the wall-clock issue. Based on the samples I've analyzed with 1.5.0 so far, the cost indeed dropped. Parabricks isn't really an option for us, as we build pipelines using WDL and run them on Terra. With all these said, I'm definitely not saying that this is a blocker for us. It is more of a curiosity/optimization question. Thank you! Steve.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:127,performance,GPU,GPU,127,"Thank you both for your inputs. Regarding the old, anecdotal observation that GPU isn't efficiently used, I didn't collect any GPU resources like this time. And here are the CPU/memory/disk usages for the same input data, under CPU vs under GPU modes, back then. Note these are collected from running the PEPPER pipeline (release 0.4.0). Note how using the GPU didn't help in terms wallclock time. ![pepper shard-0 high-cpu resources](https://github.com/google/deepvariant/assets/16310888/e7970a06-b38d-4972-8a8d-6a5cf0595ef1). ![pepper shard-0 NVDA-P100 resources](https://github.com/google/deepvariant/assets/16310888/78b9da04-d9c7-4c5b-a730-ae99b8ee4908). And here's the cost of the GPU mode, broken down by SKU (we use Google cloud). ![pepper NVDA-P100 cost](https://github.com/google/deepvariant/assets/16310888/64316bdd-3ceb-47cb-a130-dd6da34dc8fc). In terms of running DV in ""production"", we did optimize the CPU pipeline down to below $20/sample. We didn't optimize the GPU pipeline because of the wall-clock issue. Based on the samples I've analyzed with 1.5.0 so far, the cost indeed dropped. Parabricks isn't really an option for us, as we build pipelines using WDL and run them on Terra. With all these said, I'm definitely not saying that this is a blocker for us. It is more of a curiosity/optimization question. Thank you! Steve.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:131,performance,resourc,resources,131,"Thank you both for your inputs. Regarding the old, anecdotal observation that GPU isn't efficiently used, I didn't collect any GPU resources like this time. And here are the CPU/memory/disk usages for the same input data, under CPU vs under GPU modes, back then. Note these are collected from running the PEPPER pipeline (release 0.4.0). Note how using the GPU didn't help in terms wallclock time. ![pepper shard-0 high-cpu resources](https://github.com/google/deepvariant/assets/16310888/e7970a06-b38d-4972-8a8d-6a5cf0595ef1). ![pepper shard-0 NVDA-P100 resources](https://github.com/google/deepvariant/assets/16310888/78b9da04-d9c7-4c5b-a730-ae99b8ee4908). And here's the cost of the GPU mode, broken down by SKU (we use Google cloud). ![pepper NVDA-P100 cost](https://github.com/google/deepvariant/assets/16310888/64316bdd-3ceb-47cb-a130-dd6da34dc8fc). In terms of running DV in ""production"", we did optimize the CPU pipeline down to below $20/sample. We didn't optimize the GPU pipeline because of the wall-clock issue. Based on the samples I've analyzed with 1.5.0 so far, the cost indeed dropped. Parabricks isn't really an option for us, as we build pipelines using WDL and run them on Terra. With all these said, I'm definitely not saying that this is a blocker for us. It is more of a curiosity/optimization question. Thank you! Steve.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:151,performance,time,time,151,"Thank you both for your inputs. Regarding the old, anecdotal observation that GPU isn't efficiently used, I didn't collect any GPU resources like this time. And here are the CPU/memory/disk usages for the same input data, under CPU vs under GPU modes, back then. Note these are collected from running the PEPPER pipeline (release 0.4.0). Note how using the GPU didn't help in terms wallclock time. ![pepper shard-0 high-cpu resources](https://github.com/google/deepvariant/assets/16310888/e7970a06-b38d-4972-8a8d-6a5cf0595ef1). ![pepper shard-0 NVDA-P100 resources](https://github.com/google/deepvariant/assets/16310888/78b9da04-d9c7-4c5b-a730-ae99b8ee4908). And here's the cost of the GPU mode, broken down by SKU (we use Google cloud). ![pepper NVDA-P100 cost](https://github.com/google/deepvariant/assets/16310888/64316bdd-3ceb-47cb-a130-dd6da34dc8fc). In terms of running DV in ""production"", we did optimize the CPU pipeline down to below $20/sample. We didn't optimize the GPU pipeline because of the wall-clock issue. Based on the samples I've analyzed with 1.5.0 so far, the cost indeed dropped. Parabricks isn't really an option for us, as we build pipelines using WDL and run them on Terra. With all these said, I'm definitely not saying that this is a blocker for us. It is more of a curiosity/optimization question. Thank you! Steve.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:174,performance,CPU,CPU,174,"Thank you both for your inputs. Regarding the old, anecdotal observation that GPU isn't efficiently used, I didn't collect any GPU resources like this time. And here are the CPU/memory/disk usages for the same input data, under CPU vs under GPU modes, back then. Note these are collected from running the PEPPER pipeline (release 0.4.0). Note how using the GPU didn't help in terms wallclock time. ![pepper shard-0 high-cpu resources](https://github.com/google/deepvariant/assets/16310888/e7970a06-b38d-4972-8a8d-6a5cf0595ef1). ![pepper shard-0 NVDA-P100 resources](https://github.com/google/deepvariant/assets/16310888/78b9da04-d9c7-4c5b-a730-ae99b8ee4908). And here's the cost of the GPU mode, broken down by SKU (we use Google cloud). ![pepper NVDA-P100 cost](https://github.com/google/deepvariant/assets/16310888/64316bdd-3ceb-47cb-a130-dd6da34dc8fc). In terms of running DV in ""production"", we did optimize the CPU pipeline down to below $20/sample. We didn't optimize the GPU pipeline because of the wall-clock issue. Based on the samples I've analyzed with 1.5.0 so far, the cost indeed dropped. Parabricks isn't really an option for us, as we build pipelines using WDL and run them on Terra. With all these said, I'm definitely not saying that this is a blocker for us. It is more of a curiosity/optimization question. Thank you! Steve.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:178,performance,memor,memory,178,"Thank you both for your inputs. Regarding the old, anecdotal observation that GPU isn't efficiently used, I didn't collect any GPU resources like this time. And here are the CPU/memory/disk usages for the same input data, under CPU vs under GPU modes, back then. Note these are collected from running the PEPPER pipeline (release 0.4.0). Note how using the GPU didn't help in terms wallclock time. ![pepper shard-0 high-cpu resources](https://github.com/google/deepvariant/assets/16310888/e7970a06-b38d-4972-8a8d-6a5cf0595ef1). ![pepper shard-0 NVDA-P100 resources](https://github.com/google/deepvariant/assets/16310888/78b9da04-d9c7-4c5b-a730-ae99b8ee4908). And here's the cost of the GPU mode, broken down by SKU (we use Google cloud). ![pepper NVDA-P100 cost](https://github.com/google/deepvariant/assets/16310888/64316bdd-3ceb-47cb-a130-dd6da34dc8fc). In terms of running DV in ""production"", we did optimize the CPU pipeline down to below $20/sample. We didn't optimize the GPU pipeline because of the wall-clock issue. Based on the samples I've analyzed with 1.5.0 so far, the cost indeed dropped. Parabricks isn't really an option for us, as we build pipelines using WDL and run them on Terra. With all these said, I'm definitely not saying that this is a blocker for us. It is more of a curiosity/optimization question. Thank you! Steve.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:185,performance,disk,disk,185,"Thank you both for your inputs. Regarding the old, anecdotal observation that GPU isn't efficiently used, I didn't collect any GPU resources like this time. And here are the CPU/memory/disk usages for the same input data, under CPU vs under GPU modes, back then. Note these are collected from running the PEPPER pipeline (release 0.4.0). Note how using the GPU didn't help in terms wallclock time. ![pepper shard-0 high-cpu resources](https://github.com/google/deepvariant/assets/16310888/e7970a06-b38d-4972-8a8d-6a5cf0595ef1). ![pepper shard-0 NVDA-P100 resources](https://github.com/google/deepvariant/assets/16310888/78b9da04-d9c7-4c5b-a730-ae99b8ee4908). And here's the cost of the GPU mode, broken down by SKU (we use Google cloud). ![pepper NVDA-P100 cost](https://github.com/google/deepvariant/assets/16310888/64316bdd-3ceb-47cb-a130-dd6da34dc8fc). In terms of running DV in ""production"", we did optimize the CPU pipeline down to below $20/sample. We didn't optimize the GPU pipeline because of the wall-clock issue. Based on the samples I've analyzed with 1.5.0 so far, the cost indeed dropped. Parabricks isn't really an option for us, as we build pipelines using WDL and run them on Terra. With all these said, I'm definitely not saying that this is a blocker for us. It is more of a curiosity/optimization question. Thank you! Steve.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:228,performance,CPU,CPU,228,"Thank you both for your inputs. Regarding the old, anecdotal observation that GPU isn't efficiently used, I didn't collect any GPU resources like this time. And here are the CPU/memory/disk usages for the same input data, under CPU vs under GPU modes, back then. Note these are collected from running the PEPPER pipeline (release 0.4.0). Note how using the GPU didn't help in terms wallclock time. ![pepper shard-0 high-cpu resources](https://github.com/google/deepvariant/assets/16310888/e7970a06-b38d-4972-8a8d-6a5cf0595ef1). ![pepper shard-0 NVDA-P100 resources](https://github.com/google/deepvariant/assets/16310888/78b9da04-d9c7-4c5b-a730-ae99b8ee4908). And here's the cost of the GPU mode, broken down by SKU (we use Google cloud). ![pepper NVDA-P100 cost](https://github.com/google/deepvariant/assets/16310888/64316bdd-3ceb-47cb-a130-dd6da34dc8fc). In terms of running DV in ""production"", we did optimize the CPU pipeline down to below $20/sample. We didn't optimize the GPU pipeline because of the wall-clock issue. Based on the samples I've analyzed with 1.5.0 so far, the cost indeed dropped. Parabricks isn't really an option for us, as we build pipelines using WDL and run them on Terra. With all these said, I'm definitely not saying that this is a blocker for us. It is more of a curiosity/optimization question. Thank you! Steve.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:241,performance,GPU,GPU,241,"Thank you both for your inputs. Regarding the old, anecdotal observation that GPU isn't efficiently used, I didn't collect any GPU resources like this time. And here are the CPU/memory/disk usages for the same input data, under CPU vs under GPU modes, back then. Note these are collected from running the PEPPER pipeline (release 0.4.0). Note how using the GPU didn't help in terms wallclock time. ![pepper shard-0 high-cpu resources](https://github.com/google/deepvariant/assets/16310888/e7970a06-b38d-4972-8a8d-6a5cf0595ef1). ![pepper shard-0 NVDA-P100 resources](https://github.com/google/deepvariant/assets/16310888/78b9da04-d9c7-4c5b-a730-ae99b8ee4908). And here's the cost of the GPU mode, broken down by SKU (we use Google cloud). ![pepper NVDA-P100 cost](https://github.com/google/deepvariant/assets/16310888/64316bdd-3ceb-47cb-a130-dd6da34dc8fc). In terms of running DV in ""production"", we did optimize the CPU pipeline down to below $20/sample. We didn't optimize the GPU pipeline because of the wall-clock issue. Based on the samples I've analyzed with 1.5.0 so far, the cost indeed dropped. Parabricks isn't really an option for us, as we build pipelines using WDL and run them on Terra. With all these said, I'm definitely not saying that this is a blocker for us. It is more of a curiosity/optimization question. Thank you! Steve.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:357,performance,GPU,GPU,357,"Thank you both for your inputs. Regarding the old, anecdotal observation that GPU isn't efficiently used, I didn't collect any GPU resources like this time. And here are the CPU/memory/disk usages for the same input data, under CPU vs under GPU modes, back then. Note these are collected from running the PEPPER pipeline (release 0.4.0). Note how using the GPU didn't help in terms wallclock time. ![pepper shard-0 high-cpu resources](https://github.com/google/deepvariant/assets/16310888/e7970a06-b38d-4972-8a8d-6a5cf0595ef1). ![pepper shard-0 NVDA-P100 resources](https://github.com/google/deepvariant/assets/16310888/78b9da04-d9c7-4c5b-a730-ae99b8ee4908). And here's the cost of the GPU mode, broken down by SKU (we use Google cloud). ![pepper NVDA-P100 cost](https://github.com/google/deepvariant/assets/16310888/64316bdd-3ceb-47cb-a130-dd6da34dc8fc). In terms of running DV in ""production"", we did optimize the CPU pipeline down to below $20/sample. We didn't optimize the GPU pipeline because of the wall-clock issue. Based on the samples I've analyzed with 1.5.0 so far, the cost indeed dropped. Parabricks isn't really an option for us, as we build pipelines using WDL and run them on Terra. With all these said, I'm definitely not saying that this is a blocker for us. It is more of a curiosity/optimization question. Thank you! Steve.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:392,performance,time,time,392,"Thank you both for your inputs. Regarding the old, anecdotal observation that GPU isn't efficiently used, I didn't collect any GPU resources like this time. And here are the CPU/memory/disk usages for the same input data, under CPU vs under GPU modes, back then. Note these are collected from running the PEPPER pipeline (release 0.4.0). Note how using the GPU didn't help in terms wallclock time. ![pepper shard-0 high-cpu resources](https://github.com/google/deepvariant/assets/16310888/e7970a06-b38d-4972-8a8d-6a5cf0595ef1). ![pepper shard-0 NVDA-P100 resources](https://github.com/google/deepvariant/assets/16310888/78b9da04-d9c7-4c5b-a730-ae99b8ee4908). And here's the cost of the GPU mode, broken down by SKU (we use Google cloud). ![pepper NVDA-P100 cost](https://github.com/google/deepvariant/assets/16310888/64316bdd-3ceb-47cb-a130-dd6da34dc8fc). In terms of running DV in ""production"", we did optimize the CPU pipeline down to below $20/sample. We didn't optimize the GPU pipeline because of the wall-clock issue. Based on the samples I've analyzed with 1.5.0 so far, the cost indeed dropped. Parabricks isn't really an option for us, as we build pipelines using WDL and run them on Terra. With all these said, I'm definitely not saying that this is a blocker for us. It is more of a curiosity/optimization question. Thank you! Steve.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:420,performance,cpu,cpu,420,"Thank you both for your inputs. Regarding the old, anecdotal observation that GPU isn't efficiently used, I didn't collect any GPU resources like this time. And here are the CPU/memory/disk usages for the same input data, under CPU vs under GPU modes, back then. Note these are collected from running the PEPPER pipeline (release 0.4.0). Note how using the GPU didn't help in terms wallclock time. ![pepper shard-0 high-cpu resources](https://github.com/google/deepvariant/assets/16310888/e7970a06-b38d-4972-8a8d-6a5cf0595ef1). ![pepper shard-0 NVDA-P100 resources](https://github.com/google/deepvariant/assets/16310888/78b9da04-d9c7-4c5b-a730-ae99b8ee4908). And here's the cost of the GPU mode, broken down by SKU (we use Google cloud). ![pepper NVDA-P100 cost](https://github.com/google/deepvariant/assets/16310888/64316bdd-3ceb-47cb-a130-dd6da34dc8fc). In terms of running DV in ""production"", we did optimize the CPU pipeline down to below $20/sample. We didn't optimize the GPU pipeline because of the wall-clock issue. Based on the samples I've analyzed with 1.5.0 so far, the cost indeed dropped. Parabricks isn't really an option for us, as we build pipelines using WDL and run them on Terra. With all these said, I'm definitely not saying that this is a blocker for us. It is more of a curiosity/optimization question. Thank you! Steve.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:424,performance,resourc,resources,424,"Thank you both for your inputs. Regarding the old, anecdotal observation that GPU isn't efficiently used, I didn't collect any GPU resources like this time. And here are the CPU/memory/disk usages for the same input data, under CPU vs under GPU modes, back then. Note these are collected from running the PEPPER pipeline (release 0.4.0). Note how using the GPU didn't help in terms wallclock time. ![pepper shard-0 high-cpu resources](https://github.com/google/deepvariant/assets/16310888/e7970a06-b38d-4972-8a8d-6a5cf0595ef1). ![pepper shard-0 NVDA-P100 resources](https://github.com/google/deepvariant/assets/16310888/78b9da04-d9c7-4c5b-a730-ae99b8ee4908). And here's the cost of the GPU mode, broken down by SKU (we use Google cloud). ![pepper NVDA-P100 cost](https://github.com/google/deepvariant/assets/16310888/64316bdd-3ceb-47cb-a130-dd6da34dc8fc). In terms of running DV in ""production"", we did optimize the CPU pipeline down to below $20/sample. We didn't optimize the GPU pipeline because of the wall-clock issue. Based on the samples I've analyzed with 1.5.0 so far, the cost indeed dropped. Parabricks isn't really an option for us, as we build pipelines using WDL and run them on Terra. With all these said, I'm definitely not saying that this is a blocker for us. It is more of a curiosity/optimization question. Thank you! Steve.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:555,performance,resourc,resources,555,"Thank you both for your inputs. Regarding the old, anecdotal observation that GPU isn't efficiently used, I didn't collect any GPU resources like this time. And here are the CPU/memory/disk usages for the same input data, under CPU vs under GPU modes, back then. Note these are collected from running the PEPPER pipeline (release 0.4.0). Note how using the GPU didn't help in terms wallclock time. ![pepper shard-0 high-cpu resources](https://github.com/google/deepvariant/assets/16310888/e7970a06-b38d-4972-8a8d-6a5cf0595ef1). ![pepper shard-0 NVDA-P100 resources](https://github.com/google/deepvariant/assets/16310888/78b9da04-d9c7-4c5b-a730-ae99b8ee4908). And here's the cost of the GPU mode, broken down by SKU (we use Google cloud). ![pepper NVDA-P100 cost](https://github.com/google/deepvariant/assets/16310888/64316bdd-3ceb-47cb-a130-dd6da34dc8fc). In terms of running DV in ""production"", we did optimize the CPU pipeline down to below $20/sample. We didn't optimize the GPU pipeline because of the wall-clock issue. Based on the samples I've analyzed with 1.5.0 so far, the cost indeed dropped. Parabricks isn't really an option for us, as we build pipelines using WDL and run them on Terra. With all these said, I'm definitely not saying that this is a blocker for us. It is more of a curiosity/optimization question. Thank you! Steve.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:686,performance,GPU,GPU,686,"Thank you both for your inputs. Regarding the old, anecdotal observation that GPU isn't efficiently used, I didn't collect any GPU resources like this time. And here are the CPU/memory/disk usages for the same input data, under CPU vs under GPU modes, back then. Note these are collected from running the PEPPER pipeline (release 0.4.0). Note how using the GPU didn't help in terms wallclock time. ![pepper shard-0 high-cpu resources](https://github.com/google/deepvariant/assets/16310888/e7970a06-b38d-4972-8a8d-6a5cf0595ef1). ![pepper shard-0 NVDA-P100 resources](https://github.com/google/deepvariant/assets/16310888/78b9da04-d9c7-4c5b-a730-ae99b8ee4908). And here's the cost of the GPU mode, broken down by SKU (we use Google cloud). ![pepper NVDA-P100 cost](https://github.com/google/deepvariant/assets/16310888/64316bdd-3ceb-47cb-a130-dd6da34dc8fc). In terms of running DV in ""production"", we did optimize the CPU pipeline down to below $20/sample. We didn't optimize the GPU pipeline because of the wall-clock issue. Based on the samples I've analyzed with 1.5.0 so far, the cost indeed dropped. Parabricks isn't really an option for us, as we build pipelines using WDL and run them on Terra. With all these said, I'm definitely not saying that this is a blocker for us. It is more of a curiosity/optimization question. Thank you! Steve.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:903,performance,optimiz,optimize,903,"Thank you both for your inputs. Regarding the old, anecdotal observation that GPU isn't efficiently used, I didn't collect any GPU resources like this time. And here are the CPU/memory/disk usages for the same input data, under CPU vs under GPU modes, back then. Note these are collected from running the PEPPER pipeline (release 0.4.0). Note how using the GPU didn't help in terms wallclock time. ![pepper shard-0 high-cpu resources](https://github.com/google/deepvariant/assets/16310888/e7970a06-b38d-4972-8a8d-6a5cf0595ef1). ![pepper shard-0 NVDA-P100 resources](https://github.com/google/deepvariant/assets/16310888/78b9da04-d9c7-4c5b-a730-ae99b8ee4908). And here's the cost of the GPU mode, broken down by SKU (we use Google cloud). ![pepper NVDA-P100 cost](https://github.com/google/deepvariant/assets/16310888/64316bdd-3ceb-47cb-a130-dd6da34dc8fc). In terms of running DV in ""production"", we did optimize the CPU pipeline down to below $20/sample. We didn't optimize the GPU pipeline because of the wall-clock issue. Based on the samples I've analyzed with 1.5.0 so far, the cost indeed dropped. Parabricks isn't really an option for us, as we build pipelines using WDL and run them on Terra. With all these said, I'm definitely not saying that this is a blocker for us. It is more of a curiosity/optimization question. Thank you! Steve.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:916,performance,CPU,CPU,916,"Thank you both for your inputs. Regarding the old, anecdotal observation that GPU isn't efficiently used, I didn't collect any GPU resources like this time. And here are the CPU/memory/disk usages for the same input data, under CPU vs under GPU modes, back then. Note these are collected from running the PEPPER pipeline (release 0.4.0). Note how using the GPU didn't help in terms wallclock time. ![pepper shard-0 high-cpu resources](https://github.com/google/deepvariant/assets/16310888/e7970a06-b38d-4972-8a8d-6a5cf0595ef1). ![pepper shard-0 NVDA-P100 resources](https://github.com/google/deepvariant/assets/16310888/78b9da04-d9c7-4c5b-a730-ae99b8ee4908). And here's the cost of the GPU mode, broken down by SKU (we use Google cloud). ![pepper NVDA-P100 cost](https://github.com/google/deepvariant/assets/16310888/64316bdd-3ceb-47cb-a130-dd6da34dc8fc). In terms of running DV in ""production"", we did optimize the CPU pipeline down to below $20/sample. We didn't optimize the GPU pipeline because of the wall-clock issue. Based on the samples I've analyzed with 1.5.0 so far, the cost indeed dropped. Parabricks isn't really an option for us, as we build pipelines using WDL and run them on Terra. With all these said, I'm definitely not saying that this is a blocker for us. It is more of a curiosity/optimization question. Thank you! Steve.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:965,performance,optimiz,optimize,965,"Thank you both for your inputs. Regarding the old, anecdotal observation that GPU isn't efficiently used, I didn't collect any GPU resources like this time. And here are the CPU/memory/disk usages for the same input data, under CPU vs under GPU modes, back then. Note these are collected from running the PEPPER pipeline (release 0.4.0). Note how using the GPU didn't help in terms wallclock time. ![pepper shard-0 high-cpu resources](https://github.com/google/deepvariant/assets/16310888/e7970a06-b38d-4972-8a8d-6a5cf0595ef1). ![pepper shard-0 NVDA-P100 resources](https://github.com/google/deepvariant/assets/16310888/78b9da04-d9c7-4c5b-a730-ae99b8ee4908). And here's the cost of the GPU mode, broken down by SKU (we use Google cloud). ![pepper NVDA-P100 cost](https://github.com/google/deepvariant/assets/16310888/64316bdd-3ceb-47cb-a130-dd6da34dc8fc). In terms of running DV in ""production"", we did optimize the CPU pipeline down to below $20/sample. We didn't optimize the GPU pipeline because of the wall-clock issue. Based on the samples I've analyzed with 1.5.0 so far, the cost indeed dropped. Parabricks isn't really an option for us, as we build pipelines using WDL and run them on Terra. With all these said, I'm definitely not saying that this is a blocker for us. It is more of a curiosity/optimization question. Thank you! Steve.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:978,performance,GPU,GPU,978,"Thank you both for your inputs. Regarding the old, anecdotal observation that GPU isn't efficiently used, I didn't collect any GPU resources like this time. And here are the CPU/memory/disk usages for the same input data, under CPU vs under GPU modes, back then. Note these are collected from running the PEPPER pipeline (release 0.4.0). Note how using the GPU didn't help in terms wallclock time. ![pepper shard-0 high-cpu resources](https://github.com/google/deepvariant/assets/16310888/e7970a06-b38d-4972-8a8d-6a5cf0595ef1). ![pepper shard-0 NVDA-P100 resources](https://github.com/google/deepvariant/assets/16310888/78b9da04-d9c7-4c5b-a730-ae99b8ee4908). And here's the cost of the GPU mode, broken down by SKU (we use Google cloud). ![pepper NVDA-P100 cost](https://github.com/google/deepvariant/assets/16310888/64316bdd-3ceb-47cb-a130-dd6da34dc8fc). In terms of running DV in ""production"", we did optimize the CPU pipeline down to below $20/sample. We didn't optimize the GPU pipeline because of the wall-clock issue. Based on the samples I've analyzed with 1.5.0 so far, the cost indeed dropped. Parabricks isn't really an option for us, as we build pipelines using WDL and run them on Terra. With all these said, I'm definitely not saying that this is a blocker for us. It is more of a curiosity/optimization question. Thank you! Steve.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:1304,performance,optimiz,optimization,1304,"Thank you both for your inputs. Regarding the old, anecdotal observation that GPU isn't efficiently used, I didn't collect any GPU resources like this time. And here are the CPU/memory/disk usages for the same input data, under CPU vs under GPU modes, back then. Note these are collected from running the PEPPER pipeline (release 0.4.0). Note how using the GPU didn't help in terms wallclock time. ![pepper shard-0 high-cpu resources](https://github.com/google/deepvariant/assets/16310888/e7970a06-b38d-4972-8a8d-6a5cf0595ef1). ![pepper shard-0 NVDA-P100 resources](https://github.com/google/deepvariant/assets/16310888/78b9da04-d9c7-4c5b-a730-ae99b8ee4908). And here's the cost of the GPU mode, broken down by SKU (we use Google cloud). ![pepper NVDA-P100 cost](https://github.com/google/deepvariant/assets/16310888/64316bdd-3ceb-47cb-a130-dd6da34dc8fc). In terms of running DV in ""production"", we did optimize the CPU pipeline down to below $20/sample. We didn't optimize the GPU pipeline because of the wall-clock issue. Based on the samples I've analyzed with 1.5.0 so far, the cost indeed dropped. Parabricks isn't really an option for us, as we build pipelines using WDL and run them on Terra. With all these said, I'm definitely not saying that this is a blocker for us. It is more of a curiosity/optimization question. Thank you! Steve.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:24,safety,input,inputs,24,"Thank you both for your inputs. Regarding the old, anecdotal observation that GPU isn't efficiently used, I didn't collect any GPU resources like this time. And here are the CPU/memory/disk usages for the same input data, under CPU vs under GPU modes, back then. Note these are collected from running the PEPPER pipeline (release 0.4.0). Note how using the GPU didn't help in terms wallclock time. ![pepper shard-0 high-cpu resources](https://github.com/google/deepvariant/assets/16310888/e7970a06-b38d-4972-8a8d-6a5cf0595ef1). ![pepper shard-0 NVDA-P100 resources](https://github.com/google/deepvariant/assets/16310888/78b9da04-d9c7-4c5b-a730-ae99b8ee4908). And here's the cost of the GPU mode, broken down by SKU (we use Google cloud). ![pepper NVDA-P100 cost](https://github.com/google/deepvariant/assets/16310888/64316bdd-3ceb-47cb-a130-dd6da34dc8fc). In terms of running DV in ""production"", we did optimize the CPU pipeline down to below $20/sample. We didn't optimize the GPU pipeline because of the wall-clock issue. Based on the samples I've analyzed with 1.5.0 so far, the cost indeed dropped. Parabricks isn't really an option for us, as we build pipelines using WDL and run them on Terra. With all these said, I'm definitely not saying that this is a blocker for us. It is more of a curiosity/optimization question. Thank you! Steve.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:131,safety,resourc,resources,131,"Thank you both for your inputs. Regarding the old, anecdotal observation that GPU isn't efficiently used, I didn't collect any GPU resources like this time. And here are the CPU/memory/disk usages for the same input data, under CPU vs under GPU modes, back then. Note these are collected from running the PEPPER pipeline (release 0.4.0). Note how using the GPU didn't help in terms wallclock time. ![pepper shard-0 high-cpu resources](https://github.com/google/deepvariant/assets/16310888/e7970a06-b38d-4972-8a8d-6a5cf0595ef1). ![pepper shard-0 NVDA-P100 resources](https://github.com/google/deepvariant/assets/16310888/78b9da04-d9c7-4c5b-a730-ae99b8ee4908). And here's the cost of the GPU mode, broken down by SKU (we use Google cloud). ![pepper NVDA-P100 cost](https://github.com/google/deepvariant/assets/16310888/64316bdd-3ceb-47cb-a130-dd6da34dc8fc). In terms of running DV in ""production"", we did optimize the CPU pipeline down to below $20/sample. We didn't optimize the GPU pipeline because of the wall-clock issue. Based on the samples I've analyzed with 1.5.0 so far, the cost indeed dropped. Parabricks isn't really an option for us, as we build pipelines using WDL and run them on Terra. With all these said, I'm definitely not saying that this is a blocker for us. It is more of a curiosity/optimization question. Thank you! Steve.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:210,safety,input,input,210,"Thank you both for your inputs. Regarding the old, anecdotal observation that GPU isn't efficiently used, I didn't collect any GPU resources like this time. And here are the CPU/memory/disk usages for the same input data, under CPU vs under GPU modes, back then. Note these are collected from running the PEPPER pipeline (release 0.4.0). Note how using the GPU didn't help in terms wallclock time. ![pepper shard-0 high-cpu resources](https://github.com/google/deepvariant/assets/16310888/e7970a06-b38d-4972-8a8d-6a5cf0595ef1). ![pepper shard-0 NVDA-P100 resources](https://github.com/google/deepvariant/assets/16310888/78b9da04-d9c7-4c5b-a730-ae99b8ee4908). And here's the cost of the GPU mode, broken down by SKU (we use Google cloud). ![pepper NVDA-P100 cost](https://github.com/google/deepvariant/assets/16310888/64316bdd-3ceb-47cb-a130-dd6da34dc8fc). In terms of running DV in ""production"", we did optimize the CPU pipeline down to below $20/sample. We didn't optimize the GPU pipeline because of the wall-clock issue. Based on the samples I've analyzed with 1.5.0 so far, the cost indeed dropped. Parabricks isn't really an option for us, as we build pipelines using WDL and run them on Terra. With all these said, I'm definitely not saying that this is a blocker for us. It is more of a curiosity/optimization question. Thank you! Steve.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:424,safety,resourc,resources,424,"Thank you both for your inputs. Regarding the old, anecdotal observation that GPU isn't efficiently used, I didn't collect any GPU resources like this time. And here are the CPU/memory/disk usages for the same input data, under CPU vs under GPU modes, back then. Note these are collected from running the PEPPER pipeline (release 0.4.0). Note how using the GPU didn't help in terms wallclock time. ![pepper shard-0 high-cpu resources](https://github.com/google/deepvariant/assets/16310888/e7970a06-b38d-4972-8a8d-6a5cf0595ef1). ![pepper shard-0 NVDA-P100 resources](https://github.com/google/deepvariant/assets/16310888/78b9da04-d9c7-4c5b-a730-ae99b8ee4908). And here's the cost of the GPU mode, broken down by SKU (we use Google cloud). ![pepper NVDA-P100 cost](https://github.com/google/deepvariant/assets/16310888/64316bdd-3ceb-47cb-a130-dd6da34dc8fc). In terms of running DV in ""production"", we did optimize the CPU pipeline down to below $20/sample. We didn't optimize the GPU pipeline because of the wall-clock issue. Based on the samples I've analyzed with 1.5.0 so far, the cost indeed dropped. Parabricks isn't really an option for us, as we build pipelines using WDL and run them on Terra. With all these said, I'm definitely not saying that this is a blocker for us. It is more of a curiosity/optimization question. Thank you! Steve.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:555,safety,resourc,resources,555,"Thank you both for your inputs. Regarding the old, anecdotal observation that GPU isn't efficiently used, I didn't collect any GPU resources like this time. And here are the CPU/memory/disk usages for the same input data, under CPU vs under GPU modes, back then. Note these are collected from running the PEPPER pipeline (release 0.4.0). Note how using the GPU didn't help in terms wallclock time. ![pepper shard-0 high-cpu resources](https://github.com/google/deepvariant/assets/16310888/e7970a06-b38d-4972-8a8d-6a5cf0595ef1). ![pepper shard-0 NVDA-P100 resources](https://github.com/google/deepvariant/assets/16310888/78b9da04-d9c7-4c5b-a730-ae99b8ee4908). And here's the cost of the GPU mode, broken down by SKU (we use Google cloud). ![pepper NVDA-P100 cost](https://github.com/google/deepvariant/assets/16310888/64316bdd-3ceb-47cb-a130-dd6da34dc8fc). In terms of running DV in ""production"", we did optimize the CPU pipeline down to below $20/sample. We didn't optimize the GPU pipeline because of the wall-clock issue. Based on the samples I've analyzed with 1.5.0 so far, the cost indeed dropped. Parabricks isn't really an option for us, as we build pipelines using WDL and run them on Terra. With all these said, I'm definitely not saying that this is a blocker for us. It is more of a curiosity/optimization question. Thank you! Steve.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:61,testability,observ,observation,61,"Thank you both for your inputs. Regarding the old, anecdotal observation that GPU isn't efficiently used, I didn't collect any GPU resources like this time. And here are the CPU/memory/disk usages for the same input data, under CPU vs under GPU modes, back then. Note these are collected from running the PEPPER pipeline (release 0.4.0). Note how using the GPU didn't help in terms wallclock time. ![pepper shard-0 high-cpu resources](https://github.com/google/deepvariant/assets/16310888/e7970a06-b38d-4972-8a8d-6a5cf0595ef1). ![pepper shard-0 NVDA-P100 resources](https://github.com/google/deepvariant/assets/16310888/78b9da04-d9c7-4c5b-a730-ae99b8ee4908). And here's the cost of the GPU mode, broken down by SKU (we use Google cloud). ![pepper NVDA-P100 cost](https://github.com/google/deepvariant/assets/16310888/64316bdd-3ceb-47cb-a130-dd6da34dc8fc). In terms of running DV in ""production"", we did optimize the CPU pipeline down to below $20/sample. We didn't optimize the GPU pipeline because of the wall-clock issue. Based on the samples I've analyzed with 1.5.0 so far, the cost indeed dropped. Parabricks isn't really an option for us, as we build pipelines using WDL and run them on Terra. With all these said, I'm definitely not saying that this is a blocker for us. It is more of a curiosity/optimization question. Thank you! Steve.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:131,testability,resourc,resources,131,"Thank you both for your inputs. Regarding the old, anecdotal observation that GPU isn't efficiently used, I didn't collect any GPU resources like this time. And here are the CPU/memory/disk usages for the same input data, under CPU vs under GPU modes, back then. Note these are collected from running the PEPPER pipeline (release 0.4.0). Note how using the GPU didn't help in terms wallclock time. ![pepper shard-0 high-cpu resources](https://github.com/google/deepvariant/assets/16310888/e7970a06-b38d-4972-8a8d-6a5cf0595ef1). ![pepper shard-0 NVDA-P100 resources](https://github.com/google/deepvariant/assets/16310888/78b9da04-d9c7-4c5b-a730-ae99b8ee4908). And here's the cost of the GPU mode, broken down by SKU (we use Google cloud). ![pepper NVDA-P100 cost](https://github.com/google/deepvariant/assets/16310888/64316bdd-3ceb-47cb-a130-dd6da34dc8fc). In terms of running DV in ""production"", we did optimize the CPU pipeline down to below $20/sample. We didn't optimize the GPU pipeline because of the wall-clock issue. Based on the samples I've analyzed with 1.5.0 so far, the cost indeed dropped. Parabricks isn't really an option for us, as we build pipelines using WDL and run them on Terra. With all these said, I'm definitely not saying that this is a blocker for us. It is more of a curiosity/optimization question. Thank you! Steve.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:424,testability,resourc,resources,424,"Thank you both for your inputs. Regarding the old, anecdotal observation that GPU isn't efficiently used, I didn't collect any GPU resources like this time. And here are the CPU/memory/disk usages for the same input data, under CPU vs under GPU modes, back then. Note these are collected from running the PEPPER pipeline (release 0.4.0). Note how using the GPU didn't help in terms wallclock time. ![pepper shard-0 high-cpu resources](https://github.com/google/deepvariant/assets/16310888/e7970a06-b38d-4972-8a8d-6a5cf0595ef1). ![pepper shard-0 NVDA-P100 resources](https://github.com/google/deepvariant/assets/16310888/78b9da04-d9c7-4c5b-a730-ae99b8ee4908). And here's the cost of the GPU mode, broken down by SKU (we use Google cloud). ![pepper NVDA-P100 cost](https://github.com/google/deepvariant/assets/16310888/64316bdd-3ceb-47cb-a130-dd6da34dc8fc). In terms of running DV in ""production"", we did optimize the CPU pipeline down to below $20/sample. We didn't optimize the GPU pipeline because of the wall-clock issue. Based on the samples I've analyzed with 1.5.0 so far, the cost indeed dropped. Parabricks isn't really an option for us, as we build pipelines using WDL and run them on Terra. With all these said, I'm definitely not saying that this is a blocker for us. It is more of a curiosity/optimization question. Thank you! Steve.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:555,testability,resourc,resources,555,"Thank you both for your inputs. Regarding the old, anecdotal observation that GPU isn't efficiently used, I didn't collect any GPU resources like this time. And here are the CPU/memory/disk usages for the same input data, under CPU vs under GPU modes, back then. Note these are collected from running the PEPPER pipeline (release 0.4.0). Note how using the GPU didn't help in terms wallclock time. ![pepper shard-0 high-cpu resources](https://github.com/google/deepvariant/assets/16310888/e7970a06-b38d-4972-8a8d-6a5cf0595ef1). ![pepper shard-0 NVDA-P100 resources](https://github.com/google/deepvariant/assets/16310888/78b9da04-d9c7-4c5b-a730-ae99b8ee4908). And here's the cost of the GPU mode, broken down by SKU (we use Google cloud). ![pepper NVDA-P100 cost](https://github.com/google/deepvariant/assets/16310888/64316bdd-3ceb-47cb-a130-dd6da34dc8fc). In terms of running DV in ""production"", we did optimize the CPU pipeline down to below $20/sample. We didn't optimize the GPU pipeline because of the wall-clock issue. Based on the samples I've analyzed with 1.5.0 so far, the cost indeed dropped. Parabricks isn't really an option for us, as we build pipelines using WDL and run them on Terra. With all these said, I'm definitely not saying that this is a blocker for us. It is more of a curiosity/optimization question. Thank you! Steve.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:24,usability,input,inputs,24,"Thank you both for your inputs. Regarding the old, anecdotal observation that GPU isn't efficiently used, I didn't collect any GPU resources like this time. And here are the CPU/memory/disk usages for the same input data, under CPU vs under GPU modes, back then. Note these are collected from running the PEPPER pipeline (release 0.4.0). Note how using the GPU didn't help in terms wallclock time. ![pepper shard-0 high-cpu resources](https://github.com/google/deepvariant/assets/16310888/e7970a06-b38d-4972-8a8d-6a5cf0595ef1). ![pepper shard-0 NVDA-P100 resources](https://github.com/google/deepvariant/assets/16310888/78b9da04-d9c7-4c5b-a730-ae99b8ee4908). And here's the cost of the GPU mode, broken down by SKU (we use Google cloud). ![pepper NVDA-P100 cost](https://github.com/google/deepvariant/assets/16310888/64316bdd-3ceb-47cb-a130-dd6da34dc8fc). In terms of running DV in ""production"", we did optimize the CPU pipeline down to below $20/sample. We didn't optimize the GPU pipeline because of the wall-clock issue. Based on the samples I've analyzed with 1.5.0 so far, the cost indeed dropped. Parabricks isn't really an option for us, as we build pipelines using WDL and run them on Terra. With all these said, I'm definitely not saying that this is a blocker for us. It is more of a curiosity/optimization question. Thank you! Steve.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:88,usability,efficien,efficiently,88,"Thank you both for your inputs. Regarding the old, anecdotal observation that GPU isn't efficiently used, I didn't collect any GPU resources like this time. And here are the CPU/memory/disk usages for the same input data, under CPU vs under GPU modes, back then. Note these are collected from running the PEPPER pipeline (release 0.4.0). Note how using the GPU didn't help in terms wallclock time. ![pepper shard-0 high-cpu resources](https://github.com/google/deepvariant/assets/16310888/e7970a06-b38d-4972-8a8d-6a5cf0595ef1). ![pepper shard-0 NVDA-P100 resources](https://github.com/google/deepvariant/assets/16310888/78b9da04-d9c7-4c5b-a730-ae99b8ee4908). And here's the cost of the GPU mode, broken down by SKU (we use Google cloud). ![pepper NVDA-P100 cost](https://github.com/google/deepvariant/assets/16310888/64316bdd-3ceb-47cb-a130-dd6da34dc8fc). In terms of running DV in ""production"", we did optimize the CPU pipeline down to below $20/sample. We didn't optimize the GPU pipeline because of the wall-clock issue. Based on the samples I've analyzed with 1.5.0 so far, the cost indeed dropped. Parabricks isn't really an option for us, as we build pipelines using WDL and run them on Terra. With all these said, I'm definitely not saying that this is a blocker for us. It is more of a curiosity/optimization question. Thank you! Steve.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:178,usability,memor,memory,178,"Thank you both for your inputs. Regarding the old, anecdotal observation that GPU isn't efficiently used, I didn't collect any GPU resources like this time. And here are the CPU/memory/disk usages for the same input data, under CPU vs under GPU modes, back then. Note these are collected from running the PEPPER pipeline (release 0.4.0). Note how using the GPU didn't help in terms wallclock time. ![pepper shard-0 high-cpu resources](https://github.com/google/deepvariant/assets/16310888/e7970a06-b38d-4972-8a8d-6a5cf0595ef1). ![pepper shard-0 NVDA-P100 resources](https://github.com/google/deepvariant/assets/16310888/78b9da04-d9c7-4c5b-a730-ae99b8ee4908). And here's the cost of the GPU mode, broken down by SKU (we use Google cloud). ![pepper NVDA-P100 cost](https://github.com/google/deepvariant/assets/16310888/64316bdd-3ceb-47cb-a130-dd6da34dc8fc). In terms of running DV in ""production"", we did optimize the CPU pipeline down to below $20/sample. We didn't optimize the GPU pipeline because of the wall-clock issue. Based on the samples I've analyzed with 1.5.0 so far, the cost indeed dropped. Parabricks isn't really an option for us, as we build pipelines using WDL and run them on Terra. With all these said, I'm definitely not saying that this is a blocker for us. It is more of a curiosity/optimization question. Thank you! Steve.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:210,usability,input,input,210,"Thank you both for your inputs. Regarding the old, anecdotal observation that GPU isn't efficiently used, I didn't collect any GPU resources like this time. And here are the CPU/memory/disk usages for the same input data, under CPU vs under GPU modes, back then. Note these are collected from running the PEPPER pipeline (release 0.4.0). Note how using the GPU didn't help in terms wallclock time. ![pepper shard-0 high-cpu resources](https://github.com/google/deepvariant/assets/16310888/e7970a06-b38d-4972-8a8d-6a5cf0595ef1). ![pepper shard-0 NVDA-P100 resources](https://github.com/google/deepvariant/assets/16310888/78b9da04-d9c7-4c5b-a730-ae99b8ee4908). And here's the cost of the GPU mode, broken down by SKU (we use Google cloud). ![pepper NVDA-P100 cost](https://github.com/google/deepvariant/assets/16310888/64316bdd-3ceb-47cb-a130-dd6da34dc8fc). In terms of running DV in ""production"", we did optimize the CPU pipeline down to below $20/sample. We didn't optimize the GPU pipeline because of the wall-clock issue. Based on the samples I've analyzed with 1.5.0 so far, the cost indeed dropped. Parabricks isn't really an option for us, as we build pipelines using WDL and run them on Terra. With all these said, I'm definitely not saying that this is a blocker for us. It is more of a curiosity/optimization question. Thank you! Steve.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:368,usability,help,help,368,"Thank you both for your inputs. Regarding the old, anecdotal observation that GPU isn't efficiently used, I didn't collect any GPU resources like this time. And here are the CPU/memory/disk usages for the same input data, under CPU vs under GPU modes, back then. Note these are collected from running the PEPPER pipeline (release 0.4.0). Note how using the GPU didn't help in terms wallclock time. ![pepper shard-0 high-cpu resources](https://github.com/google/deepvariant/assets/16310888/e7970a06-b38d-4972-8a8d-6a5cf0595ef1). ![pepper shard-0 NVDA-P100 resources](https://github.com/google/deepvariant/assets/16310888/78b9da04-d9c7-4c5b-a730-ae99b8ee4908). And here's the cost of the GPU mode, broken down by SKU (we use Google cloud). ![pepper NVDA-P100 cost](https://github.com/google/deepvariant/assets/16310888/64316bdd-3ceb-47cb-a130-dd6da34dc8fc). In terms of running DV in ""production"", we did optimize the CPU pipeline down to below $20/sample. We didn't optimize the GPU pipeline because of the wall-clock issue. Based on the samples I've analyzed with 1.5.0 so far, the cost indeed dropped. Parabricks isn't really an option for us, as we build pipelines using WDL and run them on Terra. With all these said, I'm definitely not saying that this is a blocker for us. It is more of a curiosity/optimization question. Thank you! Steve.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:187,deployability,log,logic,187,"Hi @SHuang-Broad,. Thank you both for the detailed statistics background information. For PEPPER - we're now not using the code from that approach and instead are using the make_examples logic in DeepVariant. However, you conclusion will still be correct. Within make_examples and even call_variants we're still not using the GPU to its fullest capability. Improving GPU utilization is an area we could make progress on. However, the skill profile of the team will allow us to do the work, but not as quickly as for groups that have more experience optimizing GPU performance. As a result, we don't feel that our team's leverage is as high for that work as opposed to other projects. We may get to it eventually, but it will probably remain lower priority. @pgrosu Thank you for the I/O profile. We have also flagged I/O as a bottleneck and area for improvement in DeepVariant in general. At this time, for speed improvements, we are looking into optimizations which reduce the amount of data being passed back and forth for various function calls. This seems to us to be the most engineer-time efficient way to speed up DeepVariant currently. I am hopeful that this will result in runtime improvements in the next 1 or 2 releases. This should benefit both CPU and GPU ways of running. Thank you,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:1222,deployability,releas,releases,1222,"Hi @SHuang-Broad,. Thank you both for the detailed statistics background information. For PEPPER - we're now not using the code from that approach and instead are using the make_examples logic in DeepVariant. However, you conclusion will still be correct. Within make_examples and even call_variants we're still not using the GPU to its fullest capability. Improving GPU utilization is an area we could make progress on. However, the skill profile of the team will allow us to do the work, but not as quickly as for groups that have more experience optimizing GPU performance. As a result, we don't feel that our team's leverage is as high for that work as opposed to other projects. We may get to it eventually, but it will probably remain lower priority. @pgrosu Thank you for the I/O profile. We have also flagged I/O as a bottleneck and area for improvement in DeepVariant in general. At this time, for speed improvements, we are looking into optimizations which reduce the amount of data being passed back and forth for various function calls. This seems to us to be the most engineer-time efficient way to speed up DeepVariant currently. I am hopeful that this will result in runtime improvements in the next 1 or 2 releases. This should benefit both CPU and GPU ways of running. Thank you,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:326,energy efficiency,GPU,GPU,326,"Hi @SHuang-Broad,. Thank you both for the detailed statistics background information. For PEPPER - we're now not using the code from that approach and instead are using the make_examples logic in DeepVariant. However, you conclusion will still be correct. Within make_examples and even call_variants we're still not using the GPU to its fullest capability. Improving GPU utilization is an area we could make progress on. However, the skill profile of the team will allow us to do the work, but not as quickly as for groups that have more experience optimizing GPU performance. As a result, we don't feel that our team's leverage is as high for that work as opposed to other projects. We may get to it eventually, but it will probably remain lower priority. @pgrosu Thank you for the I/O profile. We have also flagged I/O as a bottleneck and area for improvement in DeepVariant in general. At this time, for speed improvements, we are looking into optimizations which reduce the amount of data being passed back and forth for various function calls. This seems to us to be the most engineer-time efficient way to speed up DeepVariant currently. I am hopeful that this will result in runtime improvements in the next 1 or 2 releases. This should benefit both CPU and GPU ways of running. Thank you,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:367,energy efficiency,GPU,GPU,367,"Hi @SHuang-Broad,. Thank you both for the detailed statistics background information. For PEPPER - we're now not using the code from that approach and instead are using the make_examples logic in DeepVariant. However, you conclusion will still be correct. Within make_examples and even call_variants we're still not using the GPU to its fullest capability. Improving GPU utilization is an area we could make progress on. However, the skill profile of the team will allow us to do the work, but not as quickly as for groups that have more experience optimizing GPU performance. As a result, we don't feel that our team's leverage is as high for that work as opposed to other projects. We may get to it eventually, but it will probably remain lower priority. @pgrosu Thank you for the I/O profile. We have also flagged I/O as a bottleneck and area for improvement in DeepVariant in general. At this time, for speed improvements, we are looking into optimizations which reduce the amount of data being passed back and forth for various function calls. This seems to us to be the most engineer-time efficient way to speed up DeepVariant currently. I am hopeful that this will result in runtime improvements in the next 1 or 2 releases. This should benefit both CPU and GPU ways of running. Thank you,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:440,energy efficiency,profil,profile,440,"Hi @SHuang-Broad,. Thank you both for the detailed statistics background information. For PEPPER - we're now not using the code from that approach and instead are using the make_examples logic in DeepVariant. However, you conclusion will still be correct. Within make_examples and even call_variants we're still not using the GPU to its fullest capability. Improving GPU utilization is an area we could make progress on. However, the skill profile of the team will allow us to do the work, but not as quickly as for groups that have more experience optimizing GPU performance. As a result, we don't feel that our team's leverage is as high for that work as opposed to other projects. We may get to it eventually, but it will probably remain lower priority. @pgrosu Thank you for the I/O profile. We have also flagged I/O as a bottleneck and area for improvement in DeepVariant in general. At this time, for speed improvements, we are looking into optimizations which reduce the amount of data being passed back and forth for various function calls. This seems to us to be the most engineer-time efficient way to speed up DeepVariant currently. I am hopeful that this will result in runtime improvements in the next 1 or 2 releases. This should benefit both CPU and GPU ways of running. Thank you,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:549,energy efficiency,optim,optimizing,549,"Hi @SHuang-Broad,. Thank you both for the detailed statistics background information. For PEPPER - we're now not using the code from that approach and instead are using the make_examples logic in DeepVariant. However, you conclusion will still be correct. Within make_examples and even call_variants we're still not using the GPU to its fullest capability. Improving GPU utilization is an area we could make progress on. However, the skill profile of the team will allow us to do the work, but not as quickly as for groups that have more experience optimizing GPU performance. As a result, we don't feel that our team's leverage is as high for that work as opposed to other projects. We may get to it eventually, but it will probably remain lower priority. @pgrosu Thank you for the I/O profile. We have also flagged I/O as a bottleneck and area for improvement in DeepVariant in general. At this time, for speed improvements, we are looking into optimizations which reduce the amount of data being passed back and forth for various function calls. This seems to us to be the most engineer-time efficient way to speed up DeepVariant currently. I am hopeful that this will result in runtime improvements in the next 1 or 2 releases. This should benefit both CPU and GPU ways of running. Thank you,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:560,energy efficiency,GPU,GPU,560,"Hi @SHuang-Broad,. Thank you both for the detailed statistics background information. For PEPPER - we're now not using the code from that approach and instead are using the make_examples logic in DeepVariant. However, you conclusion will still be correct. Within make_examples and even call_variants we're still not using the GPU to its fullest capability. Improving GPU utilization is an area we could make progress on. However, the skill profile of the team will allow us to do the work, but not as quickly as for groups that have more experience optimizing GPU performance. As a result, we don't feel that our team's leverage is as high for that work as opposed to other projects. We may get to it eventually, but it will probably remain lower priority. @pgrosu Thank you for the I/O profile. We have also flagged I/O as a bottleneck and area for improvement in DeepVariant in general. At this time, for speed improvements, we are looking into optimizations which reduce the amount of data being passed back and forth for various function calls. This seems to us to be the most engineer-time efficient way to speed up DeepVariant currently. I am hopeful that this will result in runtime improvements in the next 1 or 2 releases. This should benefit both CPU and GPU ways of running. Thank you,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:787,energy efficiency,profil,profile,787,"Hi @SHuang-Broad,. Thank you both for the detailed statistics background information. For PEPPER - we're now not using the code from that approach and instead are using the make_examples logic in DeepVariant. However, you conclusion will still be correct. Within make_examples and even call_variants we're still not using the GPU to its fullest capability. Improving GPU utilization is an area we could make progress on. However, the skill profile of the team will allow us to do the work, but not as quickly as for groups that have more experience optimizing GPU performance. As a result, we don't feel that our team's leverage is as high for that work as opposed to other projects. We may get to it eventually, but it will probably remain lower priority. @pgrosu Thank you for the I/O profile. We have also flagged I/O as a bottleneck and area for improvement in DeepVariant in general. At this time, for speed improvements, we are looking into optimizations which reduce the amount of data being passed back and forth for various function calls. This seems to us to be the most engineer-time efficient way to speed up DeepVariant currently. I am hopeful that this will result in runtime improvements in the next 1 or 2 releases. This should benefit both CPU and GPU ways of running. Thank you,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:947,energy efficiency,optim,optimizations,947,"Hi @SHuang-Broad,. Thank you both for the detailed statistics background information. For PEPPER - we're now not using the code from that approach and instead are using the make_examples logic in DeepVariant. However, you conclusion will still be correct. Within make_examples and even call_variants we're still not using the GPU to its fullest capability. Improving GPU utilization is an area we could make progress on. However, the skill profile of the team will allow us to do the work, but not as quickly as for groups that have more experience optimizing GPU performance. As a result, we don't feel that our team's leverage is as high for that work as opposed to other projects. We may get to it eventually, but it will probably remain lower priority. @pgrosu Thank you for the I/O profile. We have also flagged I/O as a bottleneck and area for improvement in DeepVariant in general. At this time, for speed improvements, we are looking into optimizations which reduce the amount of data being passed back and forth for various function calls. This seems to us to be the most engineer-time efficient way to speed up DeepVariant currently. I am hopeful that this will result in runtime improvements in the next 1 or 2 releases. This should benefit both CPU and GPU ways of running. Thank you,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:967,energy efficiency,reduc,reduce,967,"Hi @SHuang-Broad,. Thank you both for the detailed statistics background information. For PEPPER - we're now not using the code from that approach and instead are using the make_examples logic in DeepVariant. However, you conclusion will still be correct. Within make_examples and even call_variants we're still not using the GPU to its fullest capability. Improving GPU utilization is an area we could make progress on. However, the skill profile of the team will allow us to do the work, but not as quickly as for groups that have more experience optimizing GPU performance. As a result, we don't feel that our team's leverage is as high for that work as opposed to other projects. We may get to it eventually, but it will probably remain lower priority. @pgrosu Thank you for the I/O profile. We have also flagged I/O as a bottleneck and area for improvement in DeepVariant in general. At this time, for speed improvements, we are looking into optimizations which reduce the amount of data being passed back and forth for various function calls. This seems to us to be the most engineer-time efficient way to speed up DeepVariant currently. I am hopeful that this will result in runtime improvements in the next 1 or 2 releases. This should benefit both CPU and GPU ways of running. Thank you,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:1133,energy efficiency,current,currently,1133,"Hi @SHuang-Broad,. Thank you both for the detailed statistics background information. For PEPPER - we're now not using the code from that approach and instead are using the make_examples logic in DeepVariant. However, you conclusion will still be correct. Within make_examples and even call_variants we're still not using the GPU to its fullest capability. Improving GPU utilization is an area we could make progress on. However, the skill profile of the team will allow us to do the work, but not as quickly as for groups that have more experience optimizing GPU performance. As a result, we don't feel that our team's leverage is as high for that work as opposed to other projects. We may get to it eventually, but it will probably remain lower priority. @pgrosu Thank you for the I/O profile. We have also flagged I/O as a bottleneck and area for improvement in DeepVariant in general. At this time, for speed improvements, we are looking into optimizations which reduce the amount of data being passed back and forth for various function calls. This seems to us to be the most engineer-time efficient way to speed up DeepVariant currently. I am hopeful that this will result in runtime improvements in the next 1 or 2 releases. This should benefit both CPU and GPU ways of running. Thank you,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:1257,energy efficiency,CPU,CPU,1257,"Hi @SHuang-Broad,. Thank you both for the detailed statistics background information. For PEPPER - we're now not using the code from that approach and instead are using the make_examples logic in DeepVariant. However, you conclusion will still be correct. Within make_examples and even call_variants we're still not using the GPU to its fullest capability. Improving GPU utilization is an area we could make progress on. However, the skill profile of the team will allow us to do the work, but not as quickly as for groups that have more experience optimizing GPU performance. As a result, we don't feel that our team's leverage is as high for that work as opposed to other projects. We may get to it eventually, but it will probably remain lower priority. @pgrosu Thank you for the I/O profile. We have also flagged I/O as a bottleneck and area for improvement in DeepVariant in general. At this time, for speed improvements, we are looking into optimizations which reduce the amount of data being passed back and forth for various function calls. This seems to us to be the most engineer-time efficient way to speed up DeepVariant currently. I am hopeful that this will result in runtime improvements in the next 1 or 2 releases. This should benefit both CPU and GPU ways of running. Thank you,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:1265,energy efficiency,GPU,GPU,1265,"Hi @SHuang-Broad,. Thank you both for the detailed statistics background information. For PEPPER - we're now not using the code from that approach and instead are using the make_examples logic in DeepVariant. However, you conclusion will still be correct. Within make_examples and even call_variants we're still not using the GPU to its fullest capability. Improving GPU utilization is an area we could make progress on. However, the skill profile of the team will allow us to do the work, but not as quickly as for groups that have more experience optimizing GPU performance. As a result, we don't feel that our team's leverage is as high for that work as opposed to other projects. We may get to it eventually, but it will probably remain lower priority. @pgrosu Thank you for the I/O profile. We have also flagged I/O as a bottleneck and area for improvement in DeepVariant in general. At this time, for speed improvements, we are looking into optimizations which reduce the amount of data being passed back and forth for various function calls. This seems to us to be the most engineer-time efficient way to speed up DeepVariant currently. I am hopeful that this will result in runtime improvements in the next 1 or 2 releases. This should benefit both CPU and GPU ways of running. Thank you,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:701,integrability,event,eventually,701,"Hi @SHuang-Broad,. Thank you both for the detailed statistics background information. For PEPPER - we're now not using the code from that approach and instead are using the make_examples logic in DeepVariant. However, you conclusion will still be correct. Within make_examples and even call_variants we're still not using the GPU to its fullest capability. Improving GPU utilization is an area we could make progress on. However, the skill profile of the team will allow us to do the work, but not as quickly as for groups that have more experience optimizing GPU performance. As a result, we don't feel that our team's leverage is as high for that work as opposed to other projects. We may get to it eventually, but it will probably remain lower priority. @pgrosu Thank you for the I/O profile. We have also flagged I/O as a bottleneck and area for improvement in DeepVariant in general. At this time, for speed improvements, we are looking into optimizations which reduce the amount of data being passed back and forth for various function calls. This seems to us to be the most engineer-time efficient way to speed up DeepVariant currently. I am hopeful that this will result in runtime improvements in the next 1 or 2 releases. This should benefit both CPU and GPU ways of running. Thank you,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:326,performance,GPU,GPU,326,"Hi @SHuang-Broad,. Thank you both for the detailed statistics background information. For PEPPER - we're now not using the code from that approach and instead are using the make_examples logic in DeepVariant. However, you conclusion will still be correct. Within make_examples and even call_variants we're still not using the GPU to its fullest capability. Improving GPU utilization is an area we could make progress on. However, the skill profile of the team will allow us to do the work, but not as quickly as for groups that have more experience optimizing GPU performance. As a result, we don't feel that our team's leverage is as high for that work as opposed to other projects. We may get to it eventually, but it will probably remain lower priority. @pgrosu Thank you for the I/O profile. We have also flagged I/O as a bottleneck and area for improvement in DeepVariant in general. At this time, for speed improvements, we are looking into optimizations which reduce the amount of data being passed back and forth for various function calls. This seems to us to be the most engineer-time efficient way to speed up DeepVariant currently. I am hopeful that this will result in runtime improvements in the next 1 or 2 releases. This should benefit both CPU and GPU ways of running. Thank you,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:367,performance,GPU,GPU,367,"Hi @SHuang-Broad,. Thank you both for the detailed statistics background information. For PEPPER - we're now not using the code from that approach and instead are using the make_examples logic in DeepVariant. However, you conclusion will still be correct. Within make_examples and even call_variants we're still not using the GPU to its fullest capability. Improving GPU utilization is an area we could make progress on. However, the skill profile of the team will allow us to do the work, but not as quickly as for groups that have more experience optimizing GPU performance. As a result, we don't feel that our team's leverage is as high for that work as opposed to other projects. We may get to it eventually, but it will probably remain lower priority. @pgrosu Thank you for the I/O profile. We have also flagged I/O as a bottleneck and area for improvement in DeepVariant in general. At this time, for speed improvements, we are looking into optimizations which reduce the amount of data being passed back and forth for various function calls. This seems to us to be the most engineer-time efficient way to speed up DeepVariant currently. I am hopeful that this will result in runtime improvements in the next 1 or 2 releases. This should benefit both CPU and GPU ways of running. Thank you,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:440,performance,profil,profile,440,"Hi @SHuang-Broad,. Thank you both for the detailed statistics background information. For PEPPER - we're now not using the code from that approach and instead are using the make_examples logic in DeepVariant. However, you conclusion will still be correct. Within make_examples and even call_variants we're still not using the GPU to its fullest capability. Improving GPU utilization is an area we could make progress on. However, the skill profile of the team will allow us to do the work, but not as quickly as for groups that have more experience optimizing GPU performance. As a result, we don't feel that our team's leverage is as high for that work as opposed to other projects. We may get to it eventually, but it will probably remain lower priority. @pgrosu Thank you for the I/O profile. We have also flagged I/O as a bottleneck and area for improvement in DeepVariant in general. At this time, for speed improvements, we are looking into optimizations which reduce the amount of data being passed back and forth for various function calls. This seems to us to be the most engineer-time efficient way to speed up DeepVariant currently. I am hopeful that this will result in runtime improvements in the next 1 or 2 releases. This should benefit both CPU and GPU ways of running. Thank you,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:549,performance,optimiz,optimizing,549,"Hi @SHuang-Broad,. Thank you both for the detailed statistics background information. For PEPPER - we're now not using the code from that approach and instead are using the make_examples logic in DeepVariant. However, you conclusion will still be correct. Within make_examples and even call_variants we're still not using the GPU to its fullest capability. Improving GPU utilization is an area we could make progress on. However, the skill profile of the team will allow us to do the work, but not as quickly as for groups that have more experience optimizing GPU performance. As a result, we don't feel that our team's leverage is as high for that work as opposed to other projects. We may get to it eventually, but it will probably remain lower priority. @pgrosu Thank you for the I/O profile. We have also flagged I/O as a bottleneck and area for improvement in DeepVariant in general. At this time, for speed improvements, we are looking into optimizations which reduce the amount of data being passed back and forth for various function calls. This seems to us to be the most engineer-time efficient way to speed up DeepVariant currently. I am hopeful that this will result in runtime improvements in the next 1 or 2 releases. This should benefit both CPU and GPU ways of running. Thank you,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:560,performance,GPU,GPU,560,"Hi @SHuang-Broad,. Thank you both for the detailed statistics background information. For PEPPER - we're now not using the code from that approach and instead are using the make_examples logic in DeepVariant. However, you conclusion will still be correct. Within make_examples and even call_variants we're still not using the GPU to its fullest capability. Improving GPU utilization is an area we could make progress on. However, the skill profile of the team will allow us to do the work, but not as quickly as for groups that have more experience optimizing GPU performance. As a result, we don't feel that our team's leverage is as high for that work as opposed to other projects. We may get to it eventually, but it will probably remain lower priority. @pgrosu Thank you for the I/O profile. We have also flagged I/O as a bottleneck and area for improvement in DeepVariant in general. At this time, for speed improvements, we are looking into optimizations which reduce the amount of data being passed back and forth for various function calls. This seems to us to be the most engineer-time efficient way to speed up DeepVariant currently. I am hopeful that this will result in runtime improvements in the next 1 or 2 releases. This should benefit both CPU and GPU ways of running. Thank you,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:564,performance,perform,performance,564,"Hi @SHuang-Broad,. Thank you both for the detailed statistics background information. For PEPPER - we're now not using the code from that approach and instead are using the make_examples logic in DeepVariant. However, you conclusion will still be correct. Within make_examples and even call_variants we're still not using the GPU to its fullest capability. Improving GPU utilization is an area we could make progress on. However, the skill profile of the team will allow us to do the work, but not as quickly as for groups that have more experience optimizing GPU performance. As a result, we don't feel that our team's leverage is as high for that work as opposed to other projects. We may get to it eventually, but it will probably remain lower priority. @pgrosu Thank you for the I/O profile. We have also flagged I/O as a bottleneck and area for improvement in DeepVariant in general. At this time, for speed improvements, we are looking into optimizations which reduce the amount of data being passed back and forth for various function calls. This seems to us to be the most engineer-time efficient way to speed up DeepVariant currently. I am hopeful that this will result in runtime improvements in the next 1 or 2 releases. This should benefit both CPU and GPU ways of running. Thank you,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:783,performance,I/O,I/O,783,"Hi @SHuang-Broad,. Thank you both for the detailed statistics background information. For PEPPER - we're now not using the code from that approach and instead are using the make_examples logic in DeepVariant. However, you conclusion will still be correct. Within make_examples and even call_variants we're still not using the GPU to its fullest capability. Improving GPU utilization is an area we could make progress on. However, the skill profile of the team will allow us to do the work, but not as quickly as for groups that have more experience optimizing GPU performance. As a result, we don't feel that our team's leverage is as high for that work as opposed to other projects. We may get to it eventually, but it will probably remain lower priority. @pgrosu Thank you for the I/O profile. We have also flagged I/O as a bottleneck and area for improvement in DeepVariant in general. At this time, for speed improvements, we are looking into optimizations which reduce the amount of data being passed back and forth for various function calls. This seems to us to be the most engineer-time efficient way to speed up DeepVariant currently. I am hopeful that this will result in runtime improvements in the next 1 or 2 releases. This should benefit both CPU and GPU ways of running. Thank you,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:787,performance,profil,profile,787,"Hi @SHuang-Broad,. Thank you both for the detailed statistics background information. For PEPPER - we're now not using the code from that approach and instead are using the make_examples logic in DeepVariant. However, you conclusion will still be correct. Within make_examples and even call_variants we're still not using the GPU to its fullest capability. Improving GPU utilization is an area we could make progress on. However, the skill profile of the team will allow us to do the work, but not as quickly as for groups that have more experience optimizing GPU performance. As a result, we don't feel that our team's leverage is as high for that work as opposed to other projects. We may get to it eventually, but it will probably remain lower priority. @pgrosu Thank you for the I/O profile. We have also flagged I/O as a bottleneck and area for improvement in DeepVariant in general. At this time, for speed improvements, we are looking into optimizations which reduce the amount of data being passed back and forth for various function calls. This seems to us to be the most engineer-time efficient way to speed up DeepVariant currently. I am hopeful that this will result in runtime improvements in the next 1 or 2 releases. This should benefit both CPU and GPU ways of running. Thank you,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:817,performance,I/O,I/O,817,"Hi @SHuang-Broad,. Thank you both for the detailed statistics background information. For PEPPER - we're now not using the code from that approach and instead are using the make_examples logic in DeepVariant. However, you conclusion will still be correct. Within make_examples and even call_variants we're still not using the GPU to its fullest capability. Improving GPU utilization is an area we could make progress on. However, the skill profile of the team will allow us to do the work, but not as quickly as for groups that have more experience optimizing GPU performance. As a result, we don't feel that our team's leverage is as high for that work as opposed to other projects. We may get to it eventually, but it will probably remain lower priority. @pgrosu Thank you for the I/O profile. We have also flagged I/O as a bottleneck and area for improvement in DeepVariant in general. At this time, for speed improvements, we are looking into optimizations which reduce the amount of data being passed back and forth for various function calls. This seems to us to be the most engineer-time efficient way to speed up DeepVariant currently. I am hopeful that this will result in runtime improvements in the next 1 or 2 releases. This should benefit both CPU and GPU ways of running. Thank you,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:826,performance,bottleneck,bottleneck,826,"Hi @SHuang-Broad,. Thank you both for the detailed statistics background information. For PEPPER - we're now not using the code from that approach and instead are using the make_examples logic in DeepVariant. However, you conclusion will still be correct. Within make_examples and even call_variants we're still not using the GPU to its fullest capability. Improving GPU utilization is an area we could make progress on. However, the skill profile of the team will allow us to do the work, but not as quickly as for groups that have more experience optimizing GPU performance. As a result, we don't feel that our team's leverage is as high for that work as opposed to other projects. We may get to it eventually, but it will probably remain lower priority. @pgrosu Thank you for the I/O profile. We have also flagged I/O as a bottleneck and area for improvement in DeepVariant in general. At this time, for speed improvements, we are looking into optimizations which reduce the amount of data being passed back and forth for various function calls. This seems to us to be the most engineer-time efficient way to speed up DeepVariant currently. I am hopeful that this will result in runtime improvements in the next 1 or 2 releases. This should benefit both CPU and GPU ways of running. Thank you,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:897,performance,time,time,897,"Hi @SHuang-Broad,. Thank you both for the detailed statistics background information. For PEPPER - we're now not using the code from that approach and instead are using the make_examples logic in DeepVariant. However, you conclusion will still be correct. Within make_examples and even call_variants we're still not using the GPU to its fullest capability. Improving GPU utilization is an area we could make progress on. However, the skill profile of the team will allow us to do the work, but not as quickly as for groups that have more experience optimizing GPU performance. As a result, we don't feel that our team's leverage is as high for that work as opposed to other projects. We may get to it eventually, but it will probably remain lower priority. @pgrosu Thank you for the I/O profile. We have also flagged I/O as a bottleneck and area for improvement in DeepVariant in general. At this time, for speed improvements, we are looking into optimizations which reduce the amount of data being passed back and forth for various function calls. This seems to us to be the most engineer-time efficient way to speed up DeepVariant currently. I am hopeful that this will result in runtime improvements in the next 1 or 2 releases. This should benefit both CPU and GPU ways of running. Thank you,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:947,performance,optimiz,optimizations,947,"Hi @SHuang-Broad,. Thank you both for the detailed statistics background information. For PEPPER - we're now not using the code from that approach and instead are using the make_examples logic in DeepVariant. However, you conclusion will still be correct. Within make_examples and even call_variants we're still not using the GPU to its fullest capability. Improving GPU utilization is an area we could make progress on. However, the skill profile of the team will allow us to do the work, but not as quickly as for groups that have more experience optimizing GPU performance. As a result, we don't feel that our team's leverage is as high for that work as opposed to other projects. We may get to it eventually, but it will probably remain lower priority. @pgrosu Thank you for the I/O profile. We have also flagged I/O as a bottleneck and area for improvement in DeepVariant in general. At this time, for speed improvements, we are looking into optimizations which reduce the amount of data being passed back and forth for various function calls. This seems to us to be the most engineer-time efficient way to speed up DeepVariant currently. I am hopeful that this will result in runtime improvements in the next 1 or 2 releases. This should benefit both CPU and GPU ways of running. Thank you,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:1090,performance,time,time,1090,"Hi @SHuang-Broad,. Thank you both for the detailed statistics background information. For PEPPER - we're now not using the code from that approach and instead are using the make_examples logic in DeepVariant. However, you conclusion will still be correct. Within make_examples and even call_variants we're still not using the GPU to its fullest capability. Improving GPU utilization is an area we could make progress on. However, the skill profile of the team will allow us to do the work, but not as quickly as for groups that have more experience optimizing GPU performance. As a result, we don't feel that our team's leverage is as high for that work as opposed to other projects. We may get to it eventually, but it will probably remain lower priority. @pgrosu Thank you for the I/O profile. We have also flagged I/O as a bottleneck and area for improvement in DeepVariant in general. At this time, for speed improvements, we are looking into optimizations which reduce the amount of data being passed back and forth for various function calls. This seems to us to be the most engineer-time efficient way to speed up DeepVariant currently. I am hopeful that this will result in runtime improvements in the next 1 or 2 releases. This should benefit both CPU and GPU ways of running. Thank you,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:1257,performance,CPU,CPU,1257,"Hi @SHuang-Broad,. Thank you both for the detailed statistics background information. For PEPPER - we're now not using the code from that approach and instead are using the make_examples logic in DeepVariant. However, you conclusion will still be correct. Within make_examples and even call_variants we're still not using the GPU to its fullest capability. Improving GPU utilization is an area we could make progress on. However, the skill profile of the team will allow us to do the work, but not as quickly as for groups that have more experience optimizing GPU performance. As a result, we don't feel that our team's leverage is as high for that work as opposed to other projects. We may get to it eventually, but it will probably remain lower priority. @pgrosu Thank you for the I/O profile. We have also flagged I/O as a bottleneck and area for improvement in DeepVariant in general. At this time, for speed improvements, we are looking into optimizations which reduce the amount of data being passed back and forth for various function calls. This seems to us to be the most engineer-time efficient way to speed up DeepVariant currently. I am hopeful that this will result in runtime improvements in the next 1 or 2 releases. This should benefit both CPU and GPU ways of running. Thank you,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:1265,performance,GPU,GPU,1265,"Hi @SHuang-Broad,. Thank you both for the detailed statistics background information. For PEPPER - we're now not using the code from that approach and instead are using the make_examples logic in DeepVariant. However, you conclusion will still be correct. Within make_examples and even call_variants we're still not using the GPU to its fullest capability. Improving GPU utilization is an area we could make progress on. However, the skill profile of the team will allow us to do the work, but not as quickly as for groups that have more experience optimizing GPU performance. As a result, we don't feel that our team's leverage is as high for that work as opposed to other projects. We may get to it eventually, but it will probably remain lower priority. @pgrosu Thank you for the I/O profile. We have also flagged I/O as a bottleneck and area for improvement in DeepVariant in general. At this time, for speed improvements, we are looking into optimizations which reduce the amount of data being passed back and forth for various function calls. This seems to us to be the most engineer-time efficient way to speed up DeepVariant currently. I am hopeful that this will result in runtime improvements in the next 1 or 2 releases. This should benefit both CPU and GPU ways of running. Thank you,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:187,safety,log,logic,187,"Hi @SHuang-Broad,. Thank you both for the detailed statistics background information. For PEPPER - we're now not using the code from that approach and instead are using the make_examples logic in DeepVariant. However, you conclusion will still be correct. Within make_examples and even call_variants we're still not using the GPU to its fullest capability. Improving GPU utilization is an area we could make progress on. However, the skill profile of the team will allow us to do the work, but not as quickly as for groups that have more experience optimizing GPU performance. As a result, we don't feel that our team's leverage is as high for that work as opposed to other projects. We may get to it eventually, but it will probably remain lower priority. @pgrosu Thank you for the I/O profile. We have also flagged I/O as a bottleneck and area for improvement in DeepVariant in general. At this time, for speed improvements, we are looking into optimizations which reduce the amount of data being passed back and forth for various function calls. This seems to us to be the most engineer-time efficient way to speed up DeepVariant currently. I am hopeful that this will result in runtime improvements in the next 1 or 2 releases. This should benefit both CPU and GPU ways of running. Thank you,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:187,security,log,logic,187,"Hi @SHuang-Broad,. Thank you both for the detailed statistics background information. For PEPPER - we're now not using the code from that approach and instead are using the make_examples logic in DeepVariant. However, you conclusion will still be correct. Within make_examples and even call_variants we're still not using the GPU to its fullest capability. Improving GPU utilization is an area we could make progress on. However, the skill profile of the team will allow us to do the work, but not as quickly as for groups that have more experience optimizing GPU performance. As a result, we don't feel that our team's leverage is as high for that work as opposed to other projects. We may get to it eventually, but it will probably remain lower priority. @pgrosu Thank you for the I/O profile. We have also flagged I/O as a bottleneck and area for improvement in DeepVariant in general. At this time, for speed improvements, we are looking into optimizations which reduce the amount of data being passed back and forth for various function calls. This seems to us to be the most engineer-time efficient way to speed up DeepVariant currently. I am hopeful that this will result in runtime improvements in the next 1 or 2 releases. This should benefit both CPU and GPU ways of running. Thank you,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:455,security,team,team,455,"Hi @SHuang-Broad,. Thank you both for the detailed statistics background information. For PEPPER - we're now not using the code from that approach and instead are using the make_examples logic in DeepVariant. However, you conclusion will still be correct. Within make_examples and even call_variants we're still not using the GPU to its fullest capability. Improving GPU utilization is an area we could make progress on. However, the skill profile of the team will allow us to do the work, but not as quickly as for groups that have more experience optimizing GPU performance. As a result, we don't feel that our team's leverage is as high for that work as opposed to other projects. We may get to it eventually, but it will probably remain lower priority. @pgrosu Thank you for the I/O profile. We have also flagged I/O as a bottleneck and area for improvement in DeepVariant in general. At this time, for speed improvements, we are looking into optimizations which reduce the amount of data being passed back and forth for various function calls. This seems to us to be the most engineer-time efficient way to speed up DeepVariant currently. I am hopeful that this will result in runtime improvements in the next 1 or 2 releases. This should benefit both CPU and GPU ways of running. Thank you,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:613,security,team,team,613,"Hi @SHuang-Broad,. Thank you both for the detailed statistics background information. For PEPPER - we're now not using the code from that approach and instead are using the make_examples logic in DeepVariant. However, you conclusion will still be correct. Within make_examples and even call_variants we're still not using the GPU to its fullest capability. Improving GPU utilization is an area we could make progress on. However, the skill profile of the team will allow us to do the work, but not as quickly as for groups that have more experience optimizing GPU performance. As a result, we don't feel that our team's leverage is as high for that work as opposed to other projects. We may get to it eventually, but it will probably remain lower priority. @pgrosu Thank you for the I/O profile. We have also flagged I/O as a bottleneck and area for improvement in DeepVariant in general. At this time, for speed improvements, we are looking into optimizations which reduce the amount of data being passed back and forth for various function calls. This seems to us to be the most engineer-time efficient way to speed up DeepVariant currently. I am hopeful that this will result in runtime improvements in the next 1 or 2 releases. This should benefit both CPU and GPU ways of running. Thank you,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:187,testability,log,logic,187,"Hi @SHuang-Broad,. Thank you both for the detailed statistics background information. For PEPPER - we're now not using the code from that approach and instead are using the make_examples logic in DeepVariant. However, you conclusion will still be correct. Within make_examples and even call_variants we're still not using the GPU to its fullest capability. Improving GPU utilization is an area we could make progress on. However, the skill profile of the team will allow us to do the work, but not as quickly as for groups that have more experience optimizing GPU performance. As a result, we don't feel that our team's leverage is as high for that work as opposed to other projects. We may get to it eventually, but it will probably remain lower priority. @pgrosu Thank you for the I/O profile. We have also flagged I/O as a bottleneck and area for improvement in DeepVariant in general. At this time, for speed improvements, we are looking into optimizations which reduce the amount of data being passed back and forth for various function calls. This seems to us to be the most engineer-time efficient way to speed up DeepVariant currently. I am hopeful that this will result in runtime improvements in the next 1 or 2 releases. This should benefit both CPU and GPU ways of running. Thank you,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:408,usability,progress,progress,408,"Hi @SHuang-Broad,. Thank you both for the detailed statistics background information. For PEPPER - we're now not using the code from that approach and instead are using the make_examples logic in DeepVariant. However, you conclusion will still be correct. Within make_examples and even call_variants we're still not using the GPU to its fullest capability. Improving GPU utilization is an area we could make progress on. However, the skill profile of the team will allow us to do the work, but not as quickly as for groups that have more experience optimizing GPU performance. As a result, we don't feel that our team's leverage is as high for that work as opposed to other projects. We may get to it eventually, but it will probably remain lower priority. @pgrosu Thank you for the I/O profile. We have also flagged I/O as a bottleneck and area for improvement in DeepVariant in general. At this time, for speed improvements, we are looking into optimizations which reduce the amount of data being passed back and forth for various function calls. This seems to us to be the most engineer-time efficient way to speed up DeepVariant currently. I am hopeful that this will result in runtime improvements in the next 1 or 2 releases. This should benefit both CPU and GPU ways of running. Thank you,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:538,usability,experien,experience,538,"Hi @SHuang-Broad,. Thank you both for the detailed statistics background information. For PEPPER - we're now not using the code from that approach and instead are using the make_examples logic in DeepVariant. However, you conclusion will still be correct. Within make_examples and even call_variants we're still not using the GPU to its fullest capability. Improving GPU utilization is an area we could make progress on. However, the skill profile of the team will allow us to do the work, but not as quickly as for groups that have more experience optimizing GPU performance. As a result, we don't feel that our team's leverage is as high for that work as opposed to other projects. We may get to it eventually, but it will probably remain lower priority. @pgrosu Thank you for the I/O profile. We have also flagged I/O as a bottleneck and area for improvement in DeepVariant in general. At this time, for speed improvements, we are looking into optimizations which reduce the amount of data being passed back and forth for various function calls. This seems to us to be the most engineer-time efficient way to speed up DeepVariant currently. I am hopeful that this will result in runtime improvements in the next 1 or 2 releases. This should benefit both CPU and GPU ways of running. Thank you,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:564,usability,perform,performance,564,"Hi @SHuang-Broad,. Thank you both for the detailed statistics background information. For PEPPER - we're now not using the code from that approach and instead are using the make_examples logic in DeepVariant. However, you conclusion will still be correct. Within make_examples and even call_variants we're still not using the GPU to its fullest capability. Improving GPU utilization is an area we could make progress on. However, the skill profile of the team will allow us to do the work, but not as quickly as for groups that have more experience optimizing GPU performance. As a result, we don't feel that our team's leverage is as high for that work as opposed to other projects. We may get to it eventually, but it will probably remain lower priority. @pgrosu Thank you for the I/O profile. We have also flagged I/O as a bottleneck and area for improvement in DeepVariant in general. At this time, for speed improvements, we are looking into optimizations which reduce the amount of data being passed back and forth for various function calls. This seems to us to be the most engineer-time efficient way to speed up DeepVariant currently. I am hopeful that this will result in runtime improvements in the next 1 or 2 releases. This should benefit both CPU and GPU ways of running. Thank you,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:1095,usability,efficien,efficient,1095,"Hi @SHuang-Broad,. Thank you both for the detailed statistics background information. For PEPPER - we're now not using the code from that approach and instead are using the make_examples logic in DeepVariant. However, you conclusion will still be correct. Within make_examples and even call_variants we're still not using the GPU to its fullest capability. Improving GPU utilization is an area we could make progress on. However, the skill profile of the team will allow us to do the work, but not as quickly as for groups that have more experience optimizing GPU performance. As a result, we don't feel that our team's leverage is as high for that work as opposed to other projects. We may get to it eventually, but it will probably remain lower priority. @pgrosu Thank you for the I/O profile. We have also flagged I/O as a bottleneck and area for improvement in DeepVariant in general. At this time, for speed improvements, we are looking into optimizations which reduce the amount of data being passed back and forth for various function calls. This seems to us to be the most engineer-time efficient way to speed up DeepVariant currently. I am hopeful that this will result in runtime improvements in the next 1 or 2 releases. This should benefit both CPU and GPU ways of running. Thank you,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:275,availability,avail,available,275,"Thanks, Andrew! I totally understand the priority setting. In the meantime, I've collected some DV-1.5.0 resource usage data on a particular sample (note the scattering schemes aren't the same between the two references). Overall it's making really good use of the resources available (**_maybe_** except the memory, but it's more likely I over-allocated). Hopefully, it can help your team with further tuning. [30X.human.GRCh38.deepvariant.regular-resources-usage.pdf](https://github.com/google/deepvariant/files/11501324/30X.human.GRCh38.deepvariant.regular-resources-usage.pdf). [30X.human.T2T.deepvariant.regular-resources-usage.pdf](https://github.com/google/deepvariant/files/11501325/30X.human.T2T.deepvariant.regular-resources-usage.pdf). Thanks,. Steve",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:105,deployability,resourc,resource,105,"Thanks, Andrew! I totally understand the priority setting. In the meantime, I've collected some DV-1.5.0 resource usage data on a particular sample (note the scattering schemes aren't the same between the two references). Overall it's making really good use of the resources available (**_maybe_** except the memory, but it's more likely I over-allocated). Hopefully, it can help your team with further tuning. [30X.human.GRCh38.deepvariant.regular-resources-usage.pdf](https://github.com/google/deepvariant/files/11501324/30X.human.GRCh38.deepvariant.regular-resources-usage.pdf). [30X.human.T2T.deepvariant.regular-resources-usage.pdf](https://github.com/google/deepvariant/files/11501325/30X.human.T2T.deepvariant.regular-resources-usage.pdf). Thanks,. Steve",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:265,deployability,resourc,resources,265,"Thanks, Andrew! I totally understand the priority setting. In the meantime, I've collected some DV-1.5.0 resource usage data on a particular sample (note the scattering schemes aren't the same between the two references). Overall it's making really good use of the resources available (**_maybe_** except the memory, but it's more likely I over-allocated). Hopefully, it can help your team with further tuning. [30X.human.GRCh38.deepvariant.regular-resources-usage.pdf](https://github.com/google/deepvariant/files/11501324/30X.human.GRCh38.deepvariant.regular-resources-usage.pdf). [30X.human.T2T.deepvariant.regular-resources-usage.pdf](https://github.com/google/deepvariant/files/11501325/30X.human.T2T.deepvariant.regular-resources-usage.pdf). Thanks,. Steve",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:449,deployability,resourc,resources-usage,449,"Thanks, Andrew! I totally understand the priority setting. In the meantime, I've collected some DV-1.5.0 resource usage data on a particular sample (note the scattering schemes aren't the same between the two references). Overall it's making really good use of the resources available (**_maybe_** except the memory, but it's more likely I over-allocated). Hopefully, it can help your team with further tuning. [30X.human.GRCh38.deepvariant.regular-resources-usage.pdf](https://github.com/google/deepvariant/files/11501324/30X.human.GRCh38.deepvariant.regular-resources-usage.pdf). [30X.human.T2T.deepvariant.regular-resources-usage.pdf](https://github.com/google/deepvariant/files/11501325/30X.human.T2T.deepvariant.regular-resources-usage.pdf). Thanks,. Steve",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:560,deployability,resourc,resources-usage,560,"Thanks, Andrew! I totally understand the priority setting. In the meantime, I've collected some DV-1.5.0 resource usage data on a particular sample (note the scattering schemes aren't the same between the two references). Overall it's making really good use of the resources available (**_maybe_** except the memory, but it's more likely I over-allocated). Hopefully, it can help your team with further tuning. [30X.human.GRCh38.deepvariant.regular-resources-usage.pdf](https://github.com/google/deepvariant/files/11501324/30X.human.GRCh38.deepvariant.regular-resources-usage.pdf). [30X.human.T2T.deepvariant.regular-resources-usage.pdf](https://github.com/google/deepvariant/files/11501325/30X.human.T2T.deepvariant.regular-resources-usage.pdf). Thanks,. Steve",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:617,deployability,resourc,resources-usage,617,"Thanks, Andrew! I totally understand the priority setting. In the meantime, I've collected some DV-1.5.0 resource usage data on a particular sample (note the scattering schemes aren't the same between the two references). Overall it's making really good use of the resources available (**_maybe_** except the memory, but it's more likely I over-allocated). Hopefully, it can help your team with further tuning. [30X.human.GRCh38.deepvariant.regular-resources-usage.pdf](https://github.com/google/deepvariant/files/11501324/30X.human.GRCh38.deepvariant.regular-resources-usage.pdf). [30X.human.T2T.deepvariant.regular-resources-usage.pdf](https://github.com/google/deepvariant/files/11501325/30X.human.T2T.deepvariant.regular-resources-usage.pdf). Thanks,. Steve",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:725,deployability,resourc,resources-usage,725,"Thanks, Andrew! I totally understand the priority setting. In the meantime, I've collected some DV-1.5.0 resource usage data on a particular sample (note the scattering schemes aren't the same between the two references). Overall it's making really good use of the resources available (**_maybe_** except the memory, but it's more likely I over-allocated). Hopefully, it can help your team with further tuning. [30X.human.GRCh38.deepvariant.regular-resources-usage.pdf](https://github.com/google/deepvariant/files/11501324/30X.human.GRCh38.deepvariant.regular-resources-usage.pdf). [30X.human.T2T.deepvariant.regular-resources-usage.pdf](https://github.com/google/deepvariant/files/11501325/30X.human.T2T.deepvariant.regular-resources-usage.pdf). Thanks,. Steve",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:105,energy efficiency,resource usag,resource usage,105,"Thanks, Andrew! I totally understand the priority setting. In the meantime, I've collected some DV-1.5.0 resource usage data on a particular sample (note the scattering schemes aren't the same between the two references). Overall it's making really good use of the resources available (**_maybe_** except the memory, but it's more likely I over-allocated). Hopefully, it can help your team with further tuning. [30X.human.GRCh38.deepvariant.regular-resources-usage.pdf](https://github.com/google/deepvariant/files/11501324/30X.human.GRCh38.deepvariant.regular-resources-usage.pdf). [30X.human.T2T.deepvariant.regular-resources-usage.pdf](https://github.com/google/deepvariant/files/11501325/30X.human.T2T.deepvariant.regular-resources-usage.pdf). Thanks,. Steve",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:265,energy efficiency,resourc,resources,265,"Thanks, Andrew! I totally understand the priority setting. In the meantime, I've collected some DV-1.5.0 resource usage data on a particular sample (note the scattering schemes aren't the same between the two references). Overall it's making really good use of the resources available (**_maybe_** except the memory, but it's more likely I over-allocated). Hopefully, it can help your team with further tuning. [30X.human.GRCh38.deepvariant.regular-resources-usage.pdf](https://github.com/google/deepvariant/files/11501324/30X.human.GRCh38.deepvariant.regular-resources-usage.pdf). [30X.human.T2T.deepvariant.regular-resources-usage.pdf](https://github.com/google/deepvariant/files/11501325/30X.human.T2T.deepvariant.regular-resources-usage.pdf). Thanks,. Steve",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:345,energy efficiency,alloc,allocated,345,"Thanks, Andrew! I totally understand the priority setting. In the meantime, I've collected some DV-1.5.0 resource usage data on a particular sample (note the scattering schemes aren't the same between the two references). Overall it's making really good use of the resources available (**_maybe_** except the memory, but it's more likely I over-allocated). Hopefully, it can help your team with further tuning. [30X.human.GRCh38.deepvariant.regular-resources-usage.pdf](https://github.com/google/deepvariant/files/11501324/30X.human.GRCh38.deepvariant.regular-resources-usage.pdf). [30X.human.T2T.deepvariant.regular-resources-usage.pdf](https://github.com/google/deepvariant/files/11501325/30X.human.T2T.deepvariant.regular-resources-usage.pdf). Thanks,. Steve",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:449,energy efficiency,resourc,resources-usage,449,"Thanks, Andrew! I totally understand the priority setting. In the meantime, I've collected some DV-1.5.0 resource usage data on a particular sample (note the scattering schemes aren't the same between the two references). Overall it's making really good use of the resources available (**_maybe_** except the memory, but it's more likely I over-allocated). Hopefully, it can help your team with further tuning. [30X.human.GRCh38.deepvariant.regular-resources-usage.pdf](https://github.com/google/deepvariant/files/11501324/30X.human.GRCh38.deepvariant.regular-resources-usage.pdf). [30X.human.T2T.deepvariant.regular-resources-usage.pdf](https://github.com/google/deepvariant/files/11501325/30X.human.T2T.deepvariant.regular-resources-usage.pdf). Thanks,. Steve",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:560,energy efficiency,resourc,resources-usage,560,"Thanks, Andrew! I totally understand the priority setting. In the meantime, I've collected some DV-1.5.0 resource usage data on a particular sample (note the scattering schemes aren't the same between the two references). Overall it's making really good use of the resources available (**_maybe_** except the memory, but it's more likely I over-allocated). Hopefully, it can help your team with further tuning. [30X.human.GRCh38.deepvariant.regular-resources-usage.pdf](https://github.com/google/deepvariant/files/11501324/30X.human.GRCh38.deepvariant.regular-resources-usage.pdf). [30X.human.T2T.deepvariant.regular-resources-usage.pdf](https://github.com/google/deepvariant/files/11501325/30X.human.T2T.deepvariant.regular-resources-usage.pdf). Thanks,. Steve",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:617,energy efficiency,resourc,resources-usage,617,"Thanks, Andrew! I totally understand the priority setting. In the meantime, I've collected some DV-1.5.0 resource usage data on a particular sample (note the scattering schemes aren't the same between the two references). Overall it's making really good use of the resources available (**_maybe_** except the memory, but it's more likely I over-allocated). Hopefully, it can help your team with further tuning. [30X.human.GRCh38.deepvariant.regular-resources-usage.pdf](https://github.com/google/deepvariant/files/11501324/30X.human.GRCh38.deepvariant.regular-resources-usage.pdf). [30X.human.T2T.deepvariant.regular-resources-usage.pdf](https://github.com/google/deepvariant/files/11501325/30X.human.T2T.deepvariant.regular-resources-usage.pdf). Thanks,. Steve",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:725,energy efficiency,resourc,resources-usage,725,"Thanks, Andrew! I totally understand the priority setting. In the meantime, I've collected some DV-1.5.0 resource usage data on a particular sample (note the scattering schemes aren't the same between the two references). Overall it's making really good use of the resources available (**_maybe_** except the memory, but it's more likely I over-allocated). Hopefully, it can help your team with further tuning. [30X.human.GRCh38.deepvariant.regular-resources-usage.pdf](https://github.com/google/deepvariant/files/11501324/30X.human.GRCh38.deepvariant.regular-resources-usage.pdf). [30X.human.T2T.deepvariant.regular-resources-usage.pdf](https://github.com/google/deepvariant/files/11501325/30X.human.T2T.deepvariant.regular-resources-usage.pdf). Thanks,. Steve",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:105,performance,resourc,resource,105,"Thanks, Andrew! I totally understand the priority setting. In the meantime, I've collected some DV-1.5.0 resource usage data on a particular sample (note the scattering schemes aren't the same between the two references). Overall it's making really good use of the resources available (**_maybe_** except the memory, but it's more likely I over-allocated). Hopefully, it can help your team with further tuning. [30X.human.GRCh38.deepvariant.regular-resources-usage.pdf](https://github.com/google/deepvariant/files/11501324/30X.human.GRCh38.deepvariant.regular-resources-usage.pdf). [30X.human.T2T.deepvariant.regular-resources-usage.pdf](https://github.com/google/deepvariant/files/11501325/30X.human.T2T.deepvariant.regular-resources-usage.pdf). Thanks,. Steve",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:265,performance,resourc,resources,265,"Thanks, Andrew! I totally understand the priority setting. In the meantime, I've collected some DV-1.5.0 resource usage data on a particular sample (note the scattering schemes aren't the same between the two references). Overall it's making really good use of the resources available (**_maybe_** except the memory, but it's more likely I over-allocated). Hopefully, it can help your team with further tuning. [30X.human.GRCh38.deepvariant.regular-resources-usage.pdf](https://github.com/google/deepvariant/files/11501324/30X.human.GRCh38.deepvariant.regular-resources-usage.pdf). [30X.human.T2T.deepvariant.regular-resources-usage.pdf](https://github.com/google/deepvariant/files/11501325/30X.human.T2T.deepvariant.regular-resources-usage.pdf). Thanks,. Steve",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:309,performance,memor,memory,309,"Thanks, Andrew! I totally understand the priority setting. In the meantime, I've collected some DV-1.5.0 resource usage data on a particular sample (note the scattering schemes aren't the same between the two references). Overall it's making really good use of the resources available (**_maybe_** except the memory, but it's more likely I over-allocated). Hopefully, it can help your team with further tuning. [30X.human.GRCh38.deepvariant.regular-resources-usage.pdf](https://github.com/google/deepvariant/files/11501324/30X.human.GRCh38.deepvariant.regular-resources-usage.pdf). [30X.human.T2T.deepvariant.regular-resources-usage.pdf](https://github.com/google/deepvariant/files/11501325/30X.human.T2T.deepvariant.regular-resources-usage.pdf). Thanks,. Steve",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:449,performance,resourc,resources-usage,449,"Thanks, Andrew! I totally understand the priority setting. In the meantime, I've collected some DV-1.5.0 resource usage data on a particular sample (note the scattering schemes aren't the same between the two references). Overall it's making really good use of the resources available (**_maybe_** except the memory, but it's more likely I over-allocated). Hopefully, it can help your team with further tuning. [30X.human.GRCh38.deepvariant.regular-resources-usage.pdf](https://github.com/google/deepvariant/files/11501324/30X.human.GRCh38.deepvariant.regular-resources-usage.pdf). [30X.human.T2T.deepvariant.regular-resources-usage.pdf](https://github.com/google/deepvariant/files/11501325/30X.human.T2T.deepvariant.regular-resources-usage.pdf). Thanks,. Steve",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:560,performance,resourc,resources-usage,560,"Thanks, Andrew! I totally understand the priority setting. In the meantime, I've collected some DV-1.5.0 resource usage data on a particular sample (note the scattering schemes aren't the same between the two references). Overall it's making really good use of the resources available (**_maybe_** except the memory, but it's more likely I over-allocated). Hopefully, it can help your team with further tuning. [30X.human.GRCh38.deepvariant.regular-resources-usage.pdf](https://github.com/google/deepvariant/files/11501324/30X.human.GRCh38.deepvariant.regular-resources-usage.pdf). [30X.human.T2T.deepvariant.regular-resources-usage.pdf](https://github.com/google/deepvariant/files/11501325/30X.human.T2T.deepvariant.regular-resources-usage.pdf). Thanks,. Steve",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:617,performance,resourc,resources-usage,617,"Thanks, Andrew! I totally understand the priority setting. In the meantime, I've collected some DV-1.5.0 resource usage data on a particular sample (note the scattering schemes aren't the same between the two references). Overall it's making really good use of the resources available (**_maybe_** except the memory, but it's more likely I over-allocated). Hopefully, it can help your team with further tuning. [30X.human.GRCh38.deepvariant.regular-resources-usage.pdf](https://github.com/google/deepvariant/files/11501324/30X.human.GRCh38.deepvariant.regular-resources-usage.pdf). [30X.human.T2T.deepvariant.regular-resources-usage.pdf](https://github.com/google/deepvariant/files/11501325/30X.human.T2T.deepvariant.regular-resources-usage.pdf). Thanks,. Steve",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:725,performance,resourc,resources-usage,725,"Thanks, Andrew! I totally understand the priority setting. In the meantime, I've collected some DV-1.5.0 resource usage data on a particular sample (note the scattering schemes aren't the same between the two references). Overall it's making really good use of the resources available (**_maybe_** except the memory, but it's more likely I over-allocated). Hopefully, it can help your team with further tuning. [30X.human.GRCh38.deepvariant.regular-resources-usage.pdf](https://github.com/google/deepvariant/files/11501324/30X.human.GRCh38.deepvariant.regular-resources-usage.pdf). [30X.human.T2T.deepvariant.regular-resources-usage.pdf](https://github.com/google/deepvariant/files/11501325/30X.human.T2T.deepvariant.regular-resources-usage.pdf). Thanks,. Steve",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:275,reliability,availab,available,275,"Thanks, Andrew! I totally understand the priority setting. In the meantime, I've collected some DV-1.5.0 resource usage data on a particular sample (note the scattering schemes aren't the same between the two references). Overall it's making really good use of the resources available (**_maybe_** except the memory, but it's more likely I over-allocated). Hopefully, it can help your team with further tuning. [30X.human.GRCh38.deepvariant.regular-resources-usage.pdf](https://github.com/google/deepvariant/files/11501324/30X.human.GRCh38.deepvariant.regular-resources-usage.pdf). [30X.human.T2T.deepvariant.regular-resources-usage.pdf](https://github.com/google/deepvariant/files/11501325/30X.human.T2T.deepvariant.regular-resources-usage.pdf). Thanks,. Steve",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:105,safety,resourc,resource,105,"Thanks, Andrew! I totally understand the priority setting. In the meantime, I've collected some DV-1.5.0 resource usage data on a particular sample (note the scattering schemes aren't the same between the two references). Overall it's making really good use of the resources available (**_maybe_** except the memory, but it's more likely I over-allocated). Hopefully, it can help your team with further tuning. [30X.human.GRCh38.deepvariant.regular-resources-usage.pdf](https://github.com/google/deepvariant/files/11501324/30X.human.GRCh38.deepvariant.regular-resources-usage.pdf). [30X.human.T2T.deepvariant.regular-resources-usage.pdf](https://github.com/google/deepvariant/files/11501325/30X.human.T2T.deepvariant.regular-resources-usage.pdf). Thanks,. Steve",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:265,safety,resourc,resources,265,"Thanks, Andrew! I totally understand the priority setting. In the meantime, I've collected some DV-1.5.0 resource usage data on a particular sample (note the scattering schemes aren't the same between the two references). Overall it's making really good use of the resources available (**_maybe_** except the memory, but it's more likely I over-allocated). Hopefully, it can help your team with further tuning. [30X.human.GRCh38.deepvariant.regular-resources-usage.pdf](https://github.com/google/deepvariant/files/11501324/30X.human.GRCh38.deepvariant.regular-resources-usage.pdf). [30X.human.T2T.deepvariant.regular-resources-usage.pdf](https://github.com/google/deepvariant/files/11501325/30X.human.T2T.deepvariant.regular-resources-usage.pdf). Thanks,. Steve",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:275,safety,avail,available,275,"Thanks, Andrew! I totally understand the priority setting. In the meantime, I've collected some DV-1.5.0 resource usage data on a particular sample (note the scattering schemes aren't the same between the two references). Overall it's making really good use of the resources available (**_maybe_** except the memory, but it's more likely I over-allocated). Hopefully, it can help your team with further tuning. [30X.human.GRCh38.deepvariant.regular-resources-usage.pdf](https://github.com/google/deepvariant/files/11501324/30X.human.GRCh38.deepvariant.regular-resources-usage.pdf). [30X.human.T2T.deepvariant.regular-resources-usage.pdf](https://github.com/google/deepvariant/files/11501325/30X.human.T2T.deepvariant.regular-resources-usage.pdf). Thanks,. Steve",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:298,safety,except,except,298,"Thanks, Andrew! I totally understand the priority setting. In the meantime, I've collected some DV-1.5.0 resource usage data on a particular sample (note the scattering schemes aren't the same between the two references). Overall it's making really good use of the resources available (**_maybe_** except the memory, but it's more likely I over-allocated). Hopefully, it can help your team with further tuning. [30X.human.GRCh38.deepvariant.regular-resources-usage.pdf](https://github.com/google/deepvariant/files/11501324/30X.human.GRCh38.deepvariant.regular-resources-usage.pdf). [30X.human.T2T.deepvariant.regular-resources-usage.pdf](https://github.com/google/deepvariant/files/11501325/30X.human.T2T.deepvariant.regular-resources-usage.pdf). Thanks,. Steve",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:449,safety,resourc,resources-usage,449,"Thanks, Andrew! I totally understand the priority setting. In the meantime, I've collected some DV-1.5.0 resource usage data on a particular sample (note the scattering schemes aren't the same between the two references). Overall it's making really good use of the resources available (**_maybe_** except the memory, but it's more likely I over-allocated). Hopefully, it can help your team with further tuning. [30X.human.GRCh38.deepvariant.regular-resources-usage.pdf](https://github.com/google/deepvariant/files/11501324/30X.human.GRCh38.deepvariant.regular-resources-usage.pdf). [30X.human.T2T.deepvariant.regular-resources-usage.pdf](https://github.com/google/deepvariant/files/11501325/30X.human.T2T.deepvariant.regular-resources-usage.pdf). Thanks,. Steve",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:560,safety,resourc,resources-usage,560,"Thanks, Andrew! I totally understand the priority setting. In the meantime, I've collected some DV-1.5.0 resource usage data on a particular sample (note the scattering schemes aren't the same between the two references). Overall it's making really good use of the resources available (**_maybe_** except the memory, but it's more likely I over-allocated). Hopefully, it can help your team with further tuning. [30X.human.GRCh38.deepvariant.regular-resources-usage.pdf](https://github.com/google/deepvariant/files/11501324/30X.human.GRCh38.deepvariant.regular-resources-usage.pdf). [30X.human.T2T.deepvariant.regular-resources-usage.pdf](https://github.com/google/deepvariant/files/11501325/30X.human.T2T.deepvariant.regular-resources-usage.pdf). Thanks,. Steve",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:617,safety,resourc,resources-usage,617,"Thanks, Andrew! I totally understand the priority setting. In the meantime, I've collected some DV-1.5.0 resource usage data on a particular sample (note the scattering schemes aren't the same between the two references). Overall it's making really good use of the resources available (**_maybe_** except the memory, but it's more likely I over-allocated). Hopefully, it can help your team with further tuning. [30X.human.GRCh38.deepvariant.regular-resources-usage.pdf](https://github.com/google/deepvariant/files/11501324/30X.human.GRCh38.deepvariant.regular-resources-usage.pdf). [30X.human.T2T.deepvariant.regular-resources-usage.pdf](https://github.com/google/deepvariant/files/11501325/30X.human.T2T.deepvariant.regular-resources-usage.pdf). Thanks,. Steve",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:725,safety,resourc,resources-usage,725,"Thanks, Andrew! I totally understand the priority setting. In the meantime, I've collected some DV-1.5.0 resource usage data on a particular sample (note the scattering schemes aren't the same between the two references). Overall it's making really good use of the resources available (**_maybe_** except the memory, but it's more likely I over-allocated). Hopefully, it can help your team with further tuning. [30X.human.GRCh38.deepvariant.regular-resources-usage.pdf](https://github.com/google/deepvariant/files/11501324/30X.human.GRCh38.deepvariant.regular-resources-usage.pdf). [30X.human.T2T.deepvariant.regular-resources-usage.pdf](https://github.com/google/deepvariant/files/11501325/30X.human.T2T.deepvariant.regular-resources-usage.pdf). Thanks,. Steve",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:275,security,availab,available,275,"Thanks, Andrew! I totally understand the priority setting. In the meantime, I've collected some DV-1.5.0 resource usage data on a particular sample (note the scattering schemes aren't the same between the two references). Overall it's making really good use of the resources available (**_maybe_** except the memory, but it's more likely I over-allocated). Hopefully, it can help your team with further tuning. [30X.human.GRCh38.deepvariant.regular-resources-usage.pdf](https://github.com/google/deepvariant/files/11501324/30X.human.GRCh38.deepvariant.regular-resources-usage.pdf). [30X.human.T2T.deepvariant.regular-resources-usage.pdf](https://github.com/google/deepvariant/files/11501325/30X.human.T2T.deepvariant.regular-resources-usage.pdf). Thanks,. Steve",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:385,security,team,team,385,"Thanks, Andrew! I totally understand the priority setting. In the meantime, I've collected some DV-1.5.0 resource usage data on a particular sample (note the scattering schemes aren't the same between the two references). Overall it's making really good use of the resources available (**_maybe_** except the memory, but it's more likely I over-allocated). Hopefully, it can help your team with further tuning. [30X.human.GRCh38.deepvariant.regular-resources-usage.pdf](https://github.com/google/deepvariant/files/11501324/30X.human.GRCh38.deepvariant.regular-resources-usage.pdf). [30X.human.T2T.deepvariant.regular-resources-usage.pdf](https://github.com/google/deepvariant/files/11501325/30X.human.T2T.deepvariant.regular-resources-usage.pdf). Thanks,. Steve",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:26,testability,understand,understand,26,"Thanks, Andrew! I totally understand the priority setting. In the meantime, I've collected some DV-1.5.0 resource usage data on a particular sample (note the scattering schemes aren't the same between the two references). Overall it's making really good use of the resources available (**_maybe_** except the memory, but it's more likely I over-allocated). Hopefully, it can help your team with further tuning. [30X.human.GRCh38.deepvariant.regular-resources-usage.pdf](https://github.com/google/deepvariant/files/11501324/30X.human.GRCh38.deepvariant.regular-resources-usage.pdf). [30X.human.T2T.deepvariant.regular-resources-usage.pdf](https://github.com/google/deepvariant/files/11501325/30X.human.T2T.deepvariant.regular-resources-usage.pdf). Thanks,. Steve",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:105,testability,resourc,resource,105,"Thanks, Andrew! I totally understand the priority setting. In the meantime, I've collected some DV-1.5.0 resource usage data on a particular sample (note the scattering schemes aren't the same between the two references). Overall it's making really good use of the resources available (**_maybe_** except the memory, but it's more likely I over-allocated). Hopefully, it can help your team with further tuning. [30X.human.GRCh38.deepvariant.regular-resources-usage.pdf](https://github.com/google/deepvariant/files/11501324/30X.human.GRCh38.deepvariant.regular-resources-usage.pdf). [30X.human.T2T.deepvariant.regular-resources-usage.pdf](https://github.com/google/deepvariant/files/11501325/30X.human.T2T.deepvariant.regular-resources-usage.pdf). Thanks,. Steve",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:265,testability,resourc,resources,265,"Thanks, Andrew! I totally understand the priority setting. In the meantime, I've collected some DV-1.5.0 resource usage data on a particular sample (note the scattering schemes aren't the same between the two references). Overall it's making really good use of the resources available (**_maybe_** except the memory, but it's more likely I over-allocated). Hopefully, it can help your team with further tuning. [30X.human.GRCh38.deepvariant.regular-resources-usage.pdf](https://github.com/google/deepvariant/files/11501324/30X.human.GRCh38.deepvariant.regular-resources-usage.pdf). [30X.human.T2T.deepvariant.regular-resources-usage.pdf](https://github.com/google/deepvariant/files/11501325/30X.human.T2T.deepvariant.regular-resources-usage.pdf). Thanks,. Steve",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:449,testability,resourc,resources-usage,449,"Thanks, Andrew! I totally understand the priority setting. In the meantime, I've collected some DV-1.5.0 resource usage data on a particular sample (note the scattering schemes aren't the same between the two references). Overall it's making really good use of the resources available (**_maybe_** except the memory, but it's more likely I over-allocated). Hopefully, it can help your team with further tuning. [30X.human.GRCh38.deepvariant.regular-resources-usage.pdf](https://github.com/google/deepvariant/files/11501324/30X.human.GRCh38.deepvariant.regular-resources-usage.pdf). [30X.human.T2T.deepvariant.regular-resources-usage.pdf](https://github.com/google/deepvariant/files/11501325/30X.human.T2T.deepvariant.regular-resources-usage.pdf). Thanks,. Steve",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:560,testability,resourc,resources-usage,560,"Thanks, Andrew! I totally understand the priority setting. In the meantime, I've collected some DV-1.5.0 resource usage data on a particular sample (note the scattering schemes aren't the same between the two references). Overall it's making really good use of the resources available (**_maybe_** except the memory, but it's more likely I over-allocated). Hopefully, it can help your team with further tuning. [30X.human.GRCh38.deepvariant.regular-resources-usage.pdf](https://github.com/google/deepvariant/files/11501324/30X.human.GRCh38.deepvariant.regular-resources-usage.pdf). [30X.human.T2T.deepvariant.regular-resources-usage.pdf](https://github.com/google/deepvariant/files/11501325/30X.human.T2T.deepvariant.regular-resources-usage.pdf). Thanks,. Steve",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:617,testability,resourc,resources-usage,617,"Thanks, Andrew! I totally understand the priority setting. In the meantime, I've collected some DV-1.5.0 resource usage data on a particular sample (note the scattering schemes aren't the same between the two references). Overall it's making really good use of the resources available (**_maybe_** except the memory, but it's more likely I over-allocated). Hopefully, it can help your team with further tuning. [30X.human.GRCh38.deepvariant.regular-resources-usage.pdf](https://github.com/google/deepvariant/files/11501324/30X.human.GRCh38.deepvariant.regular-resources-usage.pdf). [30X.human.T2T.deepvariant.regular-resources-usage.pdf](https://github.com/google/deepvariant/files/11501325/30X.human.T2T.deepvariant.regular-resources-usage.pdf). Thanks,. Steve",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:725,testability,resourc,resources-usage,725,"Thanks, Andrew! I totally understand the priority setting. In the meantime, I've collected some DV-1.5.0 resource usage data on a particular sample (note the scattering schemes aren't the same between the two references). Overall it's making really good use of the resources available (**_maybe_** except the memory, but it's more likely I over-allocated). Hopefully, it can help your team with further tuning. [30X.human.GRCh38.deepvariant.regular-resources-usage.pdf](https://github.com/google/deepvariant/files/11501324/30X.human.GRCh38.deepvariant.regular-resources-usage.pdf). [30X.human.T2T.deepvariant.regular-resources-usage.pdf](https://github.com/google/deepvariant/files/11501325/30X.human.T2T.deepvariant.regular-resources-usage.pdf). Thanks,. Steve",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:309,usability,memor,memory,309,"Thanks, Andrew! I totally understand the priority setting. In the meantime, I've collected some DV-1.5.0 resource usage data on a particular sample (note the scattering schemes aren't the same between the two references). Overall it's making really good use of the resources available (**_maybe_** except the memory, but it's more likely I over-allocated). Hopefully, it can help your team with further tuning. [30X.human.GRCh38.deepvariant.regular-resources-usage.pdf](https://github.com/google/deepvariant/files/11501324/30X.human.GRCh38.deepvariant.regular-resources-usage.pdf). [30X.human.T2T.deepvariant.regular-resources-usage.pdf](https://github.com/google/deepvariant/files/11501325/30X.human.T2T.deepvariant.regular-resources-usage.pdf). Thanks,. Steve",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:375,usability,help,help,375,"Thanks, Andrew! I totally understand the priority setting. In the meantime, I've collected some DV-1.5.0 resource usage data on a particular sample (note the scattering schemes aren't the same between the two references). Overall it's making really good use of the resources available (**_maybe_** except the memory, but it's more likely I over-allocated). Hopefully, it can help your team with further tuning. [30X.human.GRCh38.deepvariant.regular-resources-usage.pdf](https://github.com/google/deepvariant/files/11501324/30X.human.GRCh38.deepvariant.regular-resources-usage.pdf). [30X.human.T2T.deepvariant.regular-resources-usage.pdf](https://github.com/google/deepvariant/files/11501325/30X.human.T2T.deepvariant.regular-resources-usage.pdf). Thanks,. Steve",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:208,availability,operat,operated,208,"Hi Andrew and Steve,. That's a good idea, and probably shifting to pass-by-reference might minimize the overhead between functions or even through minimizing the number of functions. A shared memory approach operated upon by the different functions could be ideal with maximizing performance, where even utilizing [NVidia's Unified Virtual Memory (UVM)](https://developer.nvidia.com/blog/maximizing-unified-memory-performance-cuda/) could help, but requires some care. Currently the `call_variants` memory profile looks something like this, with minimal changes across the running process:. ```. types | # objects | total size. ================================================================ | =========== | ============. str | 223354 | 35.79 MB. dict | 88941 | 25.94 MB. code | 50107 | 8.54 MB. type | 6121 | 5.65 MB. tuple | 63884 | 3.62 MB. list | 30942 | 3.19 MB. set | 2864 | 1.51 MB. weakref | 14251 | 1002.02 KB. abc.ABCMeta | 784 | 826.05 KB. cell | 20911 | 816.84 KB. int | 25259 | 697.77 KB. builtin_function_or_method | 8801 | 618.82 KB. google.protobuf.pyext.cpp_message.GeneratedProtocolMessageType | 679 | 594.12 KB. frozenset | 1862 | 541.02 KB. function (__init__) | 3439 | 456.74 KB. ```. Based on the above, that made me curious as pass-by-reference could show benefits, but wanted to see what the functions with lots of calls might be. Those types of functions are quite heavy regarding stack setup and teardown -- with intermediate local memory allocation -- which could also be the culprit based on Steve's temporal data:. ```. Wed May 17 18:54:58 2023 call_variants_profile.txt. 12867185 function calls (11416794 primitive calls) in 61.905 seconds. Ordered by: internal time. List reduced from 3098 to 300 due to restriction <300>. ncalls tottime percall cumtime percall filename:lineno(function). 9 31.607 3.512 31.608 3.512 {built-in method tensorflow.python.client._pywrap_tf_session.TF_SessionRun_wrapper}. 2 14.137 7.069 14.137 7.069 {built-in method tensorflow.python.cli",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:1407,deployability,stack,stack,1407,"ory-performance-cuda/) could help, but requires some care. Currently the `call_variants` memory profile looks something like this, with minimal changes across the running process:. ```. types | # objects | total size. ================================================================ | =========== | ============. str | 223354 | 35.79 MB. dict | 88941 | 25.94 MB. code | 50107 | 8.54 MB. type | 6121 | 5.65 MB. tuple | 63884 | 3.62 MB. list | 30942 | 3.19 MB. set | 2864 | 1.51 MB. weakref | 14251 | 1002.02 KB. abc.ABCMeta | 784 | 826.05 KB. cell | 20911 | 816.84 KB. int | 25259 | 697.77 KB. builtin_function_or_method | 8801 | 618.82 KB. google.protobuf.pyext.cpp_message.GeneratedProtocolMessageType | 679 | 594.12 KB. frozenset | 1862 | 541.02 KB. function (__init__) | 3439 | 456.74 KB. ```. Based on the above, that made me curious as pass-by-reference could show benefits, but wanted to see what the functions with lots of calls might be. Those types of functions are quite heavy regarding stack setup and teardown -- with intermediate local memory allocation -- which could also be the culprit based on Steve's temporal data:. ```. Wed May 17 18:54:58 2023 call_variants_profile.txt. 12867185 function calls (11416794 primitive calls) in 61.905 seconds. Ordered by: internal time. List reduced from 3098 to 300 due to restriction <300>. ncalls tottime percall cumtime percall filename:lineno(function). 9 31.607 3.512 31.608 3.512 {built-in method tensorflow.python.client._pywrap_tf_session.TF_SessionRun_wrapper}. 2 14.137 7.069 14.137 7.069 {built-in method tensorflow.python.client._pywrap_tf_session.TF_CloseSession}. 434721 0.900 0.000 0.913 0.000 {built-in method builtins.hasattr}. 241294 0.868 0.000 3.111 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:5866(get_controller). 241296 0.462 0.000 0.462 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py:932(_mode). 413316 0.452 0.000 0.541 0.000 /usr/lib/python3.8/contextl",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:10142,deployability,integr,integrate,10142,"numpy.random.mtrand.RandomState' objects}. 5759 0.094 0.000 0.937 0.000 <frozen importlib._bootstrap_external>:1498(find_spec). 23926 0.089 0.000 0.109 0.000 /usr/lib/python3.8/enum.py:313(__call__). 3136 0.089 0.000 1.439 0.000 <frozen importlib._bootstrap_external>:1034(get_data). 473 0.083 0.000 0.120 0.000 <frozen importlib._bootstrap_external>:1317(_path_hooks). 1417/288 0.077 0.000 0.151 0.001 /usr/lib/python3.8/sre_parse.py:493(_parse). 4910 0.069 0.000 0.069 0.000 {method 'PythonNext' of 'third_party.nucleus.io.python.sam_reader.SamIterable' objects}. 3614/1 0.069 0.000 11.965 11.965 {built-in method builtins.exec}. 701 0.066 0.000 0.066 0.000 {built-in method posix.listdir}. 15152 0.063 0.000 0.268 0.000 /usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:69(_wrapreduction). 32755 0.062 0.000 0.135 0.000 <frozen importlib._bootstrap_external>:121(_path_join). 171/133 0.060 0.000 0.178 0.001 {built-in method _imp.exec_dynamic}. 1 0.059 0.059 0.059 0.059 {method 'calls_from_allele_counts' of 'deepvariant.python.variant_calling_multisample.VariantCaller' objects}. 23323 0.058 0.000 0.163 0.000 /usr/lib/python3.8/inspect.py:2489(__init__). 15563/15359 0.053 0.000 0.384 0.000 {built-in method numpy.core._multiarray_umath.implement_array_function}. 3601 0.053 0.000 0.344 0.000 /usr/lib/python3.8/inspect.py:1102(getfullargspec). 15152 0.051 0.000 0.319 0.000 /usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:2703(amax). 12155 0.049 0.000 0.060 0.000 {built-in method builtins.max}. 40200 0.049 0.000 0.049 0.000 {built-in method third_party.nucleus.util.python.utils.read_overlaps_region}. ```. In any case, the most optimized approach is some sort shared memory model with minimal overhead to keep the code nimble and easy to integrate or extend. The code itself is pleasantly easy to go through given the nice design, which makes it quite amendable to that. In the meantime, pass-by-reference probably could be an easier win. . Thank you,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:469,energy efficiency,Current,Currently,469,"Hi Andrew and Steve,. That's a good idea, and probably shifting to pass-by-reference might minimize the overhead between functions or even through minimizing the number of functions. A shared memory approach operated upon by the different functions could be ideal with maximizing performance, where even utilizing [NVidia's Unified Virtual Memory (UVM)](https://developer.nvidia.com/blog/maximizing-unified-memory-performance-cuda/) could help, but requires some care. Currently the `call_variants` memory profile looks something like this, with minimal changes across the running process:. ```. types | # objects | total size. ================================================================ | =========== | ============. str | 223354 | 35.79 MB. dict | 88941 | 25.94 MB. code | 50107 | 8.54 MB. type | 6121 | 5.65 MB. tuple | 63884 | 3.62 MB. list | 30942 | 3.19 MB. set | 2864 | 1.51 MB. weakref | 14251 | 1002.02 KB. abc.ABCMeta | 784 | 826.05 KB. cell | 20911 | 816.84 KB. int | 25259 | 697.77 KB. builtin_function_or_method | 8801 | 618.82 KB. google.protobuf.pyext.cpp_message.GeneratedProtocolMessageType | 679 | 594.12 KB. frozenset | 1862 | 541.02 KB. function (__init__) | 3439 | 456.74 KB. ```. Based on the above, that made me curious as pass-by-reference could show benefits, but wanted to see what the functions with lots of calls might be. Those types of functions are quite heavy regarding stack setup and teardown -- with intermediate local memory allocation -- which could also be the culprit based on Steve's temporal data:. ```. Wed May 17 18:54:58 2023 call_variants_profile.txt. 12867185 function calls (11416794 primitive calls) in 61.905 seconds. Ordered by: internal time. List reduced from 3098 to 300 due to restriction <300>. ncalls tottime percall cumtime percall filename:lineno(function). 9 31.607 3.512 31.608 3.512 {built-in method tensorflow.python.client._pywrap_tf_session.TF_SessionRun_wrapper}. 2 14.137 7.069 14.137 7.069 {built-in method tensorflow.python.cli",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:506,energy efficiency,profil,profile,506,"Hi Andrew and Steve,. That's a good idea, and probably shifting to pass-by-reference might minimize the overhead between functions or even through minimizing the number of functions. A shared memory approach operated upon by the different functions could be ideal with maximizing performance, where even utilizing [NVidia's Unified Virtual Memory (UVM)](https://developer.nvidia.com/blog/maximizing-unified-memory-performance-cuda/) could help, but requires some care. Currently the `call_variants` memory profile looks something like this, with minimal changes across the running process:. ```. types | # objects | total size. ================================================================ | =========== | ============. str | 223354 | 35.79 MB. dict | 88941 | 25.94 MB. code | 50107 | 8.54 MB. type | 6121 | 5.65 MB. tuple | 63884 | 3.62 MB. list | 30942 | 3.19 MB. set | 2864 | 1.51 MB. weakref | 14251 | 1002.02 KB. abc.ABCMeta | 784 | 826.05 KB. cell | 20911 | 816.84 KB. int | 25259 | 697.77 KB. builtin_function_or_method | 8801 | 618.82 KB. google.protobuf.pyext.cpp_message.GeneratedProtocolMessageType | 679 | 594.12 KB. frozenset | 1862 | 541.02 KB. function (__init__) | 3439 | 456.74 KB. ```. Based on the above, that made me curious as pass-by-reference could show benefits, but wanted to see what the functions with lots of calls might be. Those types of functions are quite heavy regarding stack setup and teardown -- with intermediate local memory allocation -- which could also be the culprit based on Steve's temporal data:. ```. Wed May 17 18:54:58 2023 call_variants_profile.txt. 12867185 function calls (11416794 primitive calls) in 61.905 seconds. Ordered by: internal time. List reduced from 3098 to 300 due to restriction <300>. ncalls tottime percall cumtime percall filename:lineno(function). 9 31.607 3.512 31.608 3.512 {built-in method tensorflow.python.client._pywrap_tf_session.TF_SessionRun_wrapper}. 2 14.137 7.069 14.137 7.069 {built-in method tensorflow.python.cli",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:1466,energy efficiency,alloc,allocation,1466,"rrently the `call_variants` memory profile looks something like this, with minimal changes across the running process:. ```. types | # objects | total size. ================================================================ | =========== | ============. str | 223354 | 35.79 MB. dict | 88941 | 25.94 MB. code | 50107 | 8.54 MB. type | 6121 | 5.65 MB. tuple | 63884 | 3.62 MB. list | 30942 | 3.19 MB. set | 2864 | 1.51 MB. weakref | 14251 | 1002.02 KB. abc.ABCMeta | 784 | 826.05 KB. cell | 20911 | 816.84 KB. int | 25259 | 697.77 KB. builtin_function_or_method | 8801 | 618.82 KB. google.protobuf.pyext.cpp_message.GeneratedProtocolMessageType | 679 | 594.12 KB. frozenset | 1862 | 541.02 KB. function (__init__) | 3439 | 456.74 KB. ```. Based on the above, that made me curious as pass-by-reference could show benefits, but wanted to see what the functions with lots of calls might be. Those types of functions are quite heavy regarding stack setup and teardown -- with intermediate local memory allocation -- which could also be the culprit based on Steve's temporal data:. ```. Wed May 17 18:54:58 2023 call_variants_profile.txt. 12867185 function calls (11416794 primitive calls) in 61.905 seconds. Ordered by: internal time. List reduced from 3098 to 300 due to restriction <300>. ncalls tottime percall cumtime percall filename:lineno(function). 9 31.607 3.512 31.608 3.512 {built-in method tensorflow.python.client._pywrap_tf_session.TF_SessionRun_wrapper}. 2 14.137 7.069 14.137 7.069 {built-in method tensorflow.python.client._pywrap_tf_session.TF_CloseSession}. 434721 0.900 0.000 0.913 0.000 {built-in method builtins.hasattr}. 241294 0.868 0.000 3.111 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:5866(get_controller). 241296 0.462 0.000 0.462 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py:932(_mode). 413316 0.452 0.000 0.541 0.000 /usr/lib/python3.8/contextlib.py:82(__init__). 9 0.409 0.045 0.409 0.045 {built-in metho",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:1704,energy efficiency,reduc,reduced,1704," ============. str | 223354 | 35.79 MB. dict | 88941 | 25.94 MB. code | 50107 | 8.54 MB. type | 6121 | 5.65 MB. tuple | 63884 | 3.62 MB. list | 30942 | 3.19 MB. set | 2864 | 1.51 MB. weakref | 14251 | 1002.02 KB. abc.ABCMeta | 784 | 826.05 KB. cell | 20911 | 816.84 KB. int | 25259 | 697.77 KB. builtin_function_or_method | 8801 | 618.82 KB. google.protobuf.pyext.cpp_message.GeneratedProtocolMessageType | 679 | 594.12 KB. frozenset | 1862 | 541.02 KB. function (__init__) | 3439 | 456.74 KB. ```. Based on the above, that made me curious as pass-by-reference could show benefits, but wanted to see what the functions with lots of calls might be. Those types of functions are quite heavy regarding stack setup and teardown -- with intermediate local memory allocation -- which could also be the culprit based on Steve's temporal data:. ```. Wed May 17 18:54:58 2023 call_variants_profile.txt. 12867185 function calls (11416794 primitive calls) in 61.905 seconds. Ordered by: internal time. List reduced from 3098 to 300 due to restriction <300>. ncalls tottime percall cumtime percall filename:lineno(function). 9 31.607 3.512 31.608 3.512 {built-in method tensorflow.python.client._pywrap_tf_session.TF_SessionRun_wrapper}. 2 14.137 7.069 14.137 7.069 {built-in method tensorflow.python.client._pywrap_tf_session.TF_CloseSession}. 434721 0.900 0.000 0.913 0.000 {built-in method builtins.hasattr}. 241294 0.868 0.000 3.111 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:5866(get_controller). 241296 0.462 0.000 0.462 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py:932(_mode). 413316 0.452 0.000 0.541 0.000 /usr/lib/python3.8/contextlib.py:82(__init__). 9 0.409 0.045 0.409 0.045 {built-in method tensorflow.python.client._pywrap_tf_session.ExtendSession}. 413315/164299 0.370 0.000 1.371 0.000 /usr/lib/python3.8/contextlib.py:117(__exit__). 825922/196 0.347 0.000 61.452 0.314 {built-in method builtins.next}. 113550 0.334 0.000 0",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:5979,energy efficiency,optim,optimizations,5979,"amework/ops.py:5351(_device_function_stack). 58954 0.117 0.000 0.200 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/dtypes.py:719(as_dtype). 6669 0.113 0.000 0.118 0.000 /usr/local/lib/python3.8/dist-packages/gast/gast.py:15(create_node). 155316 0.108 0.000 0.183 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:6314(get_default_graph). 6015 0.105 0.000 0.105 0.000 {built-in method tensorflow.python.util._tf_stack.extract_stack_for_op}. 6015 0.104 0.000 0.149 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:1879(_NodeDef). ```. What I'm seeing above is something quite interesting. The execution seems heavy on [eager execution rather than graph execution](https://blog.tensorflow.org/2018/08/code-with-eager-execution-run-with-graphs.html), which could be the culprit. The thing is that there are some quick ways now to make it fast with almost no major code-rewrite, though there are other natural optimizations that are relatively obvious given the analysis. For the `make_examples` analysis, the task is much simpler to fix by localizing I/O through just a shared memory model and/or pass-by-reference approach, as illustrated by the call distribution (with an eye on the serialization/deserialization of ProtoBuf if that becomes more heavily used):. ```. Wed May 17 17:51:33 2023 make_examples_profile.txt. 2790682 function calls (2732833 primitive calls) in 11.860 seconds. Ordered by: internal time. List reduced from 11503 to 100 due to restriction <100>. ncalls tottime percall cumtime percall filename:lineno(function). 171/168 1.927 0.011 1.940 0.012 {built-in method _imp.create_dynamic}. 3159 0.930 0.000 0.930 0.000 {method 'read' of '_io.BufferedReader' objects}. 19018 0.694 0.000 0.694 0.000 {built-in method posix.stat}. 2 0.586 0.293 0.586 0.293 {method 'counts' of 'deepvariant.python.allelecounter.AlleleCounter' objects}. 3136 0.475 0.000 0.475 0.000 {built-in method marshal.loads}. 3",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:6154,energy efficiency,model,model,6154," 0.113 0.000 0.118 0.000 /usr/local/lib/python3.8/dist-packages/gast/gast.py:15(create_node). 155316 0.108 0.000 0.183 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:6314(get_default_graph). 6015 0.105 0.000 0.105 0.000 {built-in method tensorflow.python.util._tf_stack.extract_stack_for_op}. 6015 0.104 0.000 0.149 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:1879(_NodeDef). ```. What I'm seeing above is something quite interesting. The execution seems heavy on [eager execution rather than graph execution](https://blog.tensorflow.org/2018/08/code-with-eager-execution-run-with-graphs.html), which could be the culprit. The thing is that there are some quick ways now to make it fast with almost no major code-rewrite, though there are other natural optimizations that are relatively obvious given the analysis. For the `make_examples` analysis, the task is much simpler to fix by localizing I/O through just a shared memory model and/or pass-by-reference approach, as illustrated by the call distribution (with an eye on the serialization/deserialization of ProtoBuf if that becomes more heavily used):. ```. Wed May 17 17:51:33 2023 make_examples_profile.txt. 2790682 function calls (2732833 primitive calls) in 11.860 seconds. Ordered by: internal time. List reduced from 11503 to 100 due to restriction <100>. ncalls tottime percall cumtime percall filename:lineno(function). 171/168 1.927 0.011 1.940 0.012 {built-in method _imp.create_dynamic}. 3159 0.930 0.000 0.930 0.000 {method 'read' of '_io.BufferedReader' objects}. 19018 0.694 0.000 0.694 0.000 {built-in method posix.stat}. 2 0.586 0.293 0.586 0.293 {method 'counts' of 'deepvariant.python.allelecounter.AlleleCounter' objects}. 3136 0.475 0.000 0.475 0.000 {built-in method marshal.loads}. 3136 0.422 0.000 0.422 0.000 {built-in method io.open_code}. 15152 0.183 0.000 0.183 0.000 {method 'reduce' of 'numpy.ufunc' objects}. 3136 0.164 0.000 2.265 0.001 <frozen ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:6491,energy efficiency,reduc,reduced,6491,".000 0.149 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:1879(_NodeDef). ```. What I'm seeing above is something quite interesting. The execution seems heavy on [eager execution rather than graph execution](https://blog.tensorflow.org/2018/08/code-with-eager-execution-run-with-graphs.html), which could be the culprit. The thing is that there are some quick ways now to make it fast with almost no major code-rewrite, though there are other natural optimizations that are relatively obvious given the analysis. For the `make_examples` analysis, the task is much simpler to fix by localizing I/O through just a shared memory model and/or pass-by-reference approach, as illustrated by the call distribution (with an eye on the serialization/deserialization of ProtoBuf if that becomes more heavily used):. ```. Wed May 17 17:51:33 2023 make_examples_profile.txt. 2790682 function calls (2732833 primitive calls) in 11.860 seconds. Ordered by: internal time. List reduced from 11503 to 100 due to restriction <100>. ncalls tottime percall cumtime percall filename:lineno(function). 171/168 1.927 0.011 1.940 0.012 {built-in method _imp.create_dynamic}. 3159 0.930 0.000 0.930 0.000 {method 'read' of '_io.BufferedReader' objects}. 19018 0.694 0.000 0.694 0.000 {built-in method posix.stat}. 2 0.586 0.293 0.586 0.293 {method 'counts' of 'deepvariant.python.allelecounter.AlleleCounter' objects}. 3136 0.475 0.000 0.475 0.000 {built-in method marshal.loads}. 3136 0.422 0.000 0.422 0.000 {built-in method io.open_code}. 15152 0.183 0.000 0.183 0.000 {method 'reduce' of 'numpy.ufunc' objects}. 3136 0.164 0.000 2.265 0.001 <frozen importlib._bootstrap_external>:914(get_code). 5151 0.146 0.000 0.436 0.000 /tmp/Bazel.runfiles_t7ewtxb0/runfiles/com_google_deepvariant/third_party/nucleus/util/genomics_math.py:196(normalize_log10_probs). 1 0.146 0.146 0.146 0.146 {method 'summary_counts' of 'deepvariant.python.allelecounter.AlleleCounter' objects}. 10001 0.142 0.000 0",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:6977,energy efficiency,load,loads,6977,"ptimizations that are relatively obvious given the analysis. For the `make_examples` analysis, the task is much simpler to fix by localizing I/O through just a shared memory model and/or pass-by-reference approach, as illustrated by the call distribution (with an eye on the serialization/deserialization of ProtoBuf if that becomes more heavily used):. ```. Wed May 17 17:51:33 2023 make_examples_profile.txt. 2790682 function calls (2732833 primitive calls) in 11.860 seconds. Ordered by: internal time. List reduced from 11503 to 100 due to restriction <100>. ncalls tottime percall cumtime percall filename:lineno(function). 171/168 1.927 0.011 1.940 0.012 {built-in method _imp.create_dynamic}. 3159 0.930 0.000 0.930 0.000 {method 'read' of '_io.BufferedReader' objects}. 19018 0.694 0.000 0.694 0.000 {built-in method posix.stat}. 2 0.586 0.293 0.586 0.293 {method 'counts' of 'deepvariant.python.allelecounter.AlleleCounter' objects}. 3136 0.475 0.000 0.475 0.000 {built-in method marshal.loads}. 3136 0.422 0.000 0.422 0.000 {built-in method io.open_code}. 15152 0.183 0.000 0.183 0.000 {method 'reduce' of 'numpy.ufunc' objects}. 3136 0.164 0.000 2.265 0.001 <frozen importlib._bootstrap_external>:914(get_code). 5151 0.146 0.000 0.436 0.000 /tmp/Bazel.runfiles_t7ewtxb0/runfiles/com_google_deepvariant/third_party/nucleus/util/genomics_math.py:196(normalize_log10_probs). 1 0.146 0.146 0.146 0.146 {method 'summary_counts' of 'deepvariant.python.allelecounter.AlleleCounter' objects}. 10001 0.142 0.000 0.428 0.000 /tmp/Bazel.runfiles_t7ewtxb0/runfiles/com_google_deepvariant/deepvariant/variant_caller.py:249(with_gq_and_likelihoods). 5151 0.134 0.000 0.610 0.000 /tmp/Bazel.runfiles_t7ewtxb0/runfiles/com_google_deepvariant/deepvariant/variant_caller.py:194(_calc_reference_confidence). 5268/5203 0.119 0.000 0.562 0.000 {built-in method builtins.__build_class__}. 1339 0.119 0.000 0.119 0.000 {method 'write' of 'third_party.nucleus.io.python.tfrecord_writer.TFRecordWriter' objects}. 1",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:7085,energy efficiency,reduc,reduce,7085,"uch simpler to fix by localizing I/O through just a shared memory model and/or pass-by-reference approach, as illustrated by the call distribution (with an eye on the serialization/deserialization of ProtoBuf if that becomes more heavily used):. ```. Wed May 17 17:51:33 2023 make_examples_profile.txt. 2790682 function calls (2732833 primitive calls) in 11.860 seconds. Ordered by: internal time. List reduced from 11503 to 100 due to restriction <100>. ncalls tottime percall cumtime percall filename:lineno(function). 171/168 1.927 0.011 1.940 0.012 {built-in method _imp.create_dynamic}. 3159 0.930 0.000 0.930 0.000 {method 'read' of '_io.BufferedReader' objects}. 19018 0.694 0.000 0.694 0.000 {built-in method posix.stat}. 2 0.586 0.293 0.586 0.293 {method 'counts' of 'deepvariant.python.allelecounter.AlleleCounter' objects}. 3136 0.475 0.000 0.475 0.000 {built-in method marshal.loads}. 3136 0.422 0.000 0.422 0.000 {built-in method io.open_code}. 15152 0.183 0.000 0.183 0.000 {method 'reduce' of 'numpy.ufunc' objects}. 3136 0.164 0.000 2.265 0.001 <frozen importlib._bootstrap_external>:914(get_code). 5151 0.146 0.000 0.436 0.000 /tmp/Bazel.runfiles_t7ewtxb0/runfiles/com_google_deepvariant/third_party/nucleus/util/genomics_math.py:196(normalize_log10_probs). 1 0.146 0.146 0.146 0.146 {method 'summary_counts' of 'deepvariant.python.allelecounter.AlleleCounter' objects}. 10001 0.142 0.000 0.428 0.000 /tmp/Bazel.runfiles_t7ewtxb0/runfiles/com_google_deepvariant/deepvariant/variant_caller.py:249(with_gq_and_likelihoods). 5151 0.134 0.000 0.610 0.000 /tmp/Bazel.runfiles_t7ewtxb0/runfiles/com_google_deepvariant/deepvariant/variant_caller.py:194(_calc_reference_confidence). 5268/5203 0.119 0.000 0.562 0.000 {built-in method builtins.__build_class__}. 1339 0.119 0.000 0.119 0.000 {method 'write' of 'third_party.nucleus.io.python.tfrecord_writer.TFRecordWriter' objects}. 1276 0.116 0.000 0.676 0.001 /tmp/Bazel.runfiles_t7ewtxb0/runfiles/com_google_deepvariant/deepvariant/variant",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:9130,energy efficiency,core,core,9130,"107 0.054 {built-in method tensorflow.python.client._pywrap_tf_session.TF_LoadLibrary}. 5581 0.095 0.000 0.328 0.000 /usr/lib/python3.8/inspect.py:2124(_signature_from_function). 4309 0.095 0.000 0.095 0.000 {method 'randint' of 'numpy.random.mtrand.RandomState' objects}. 5759 0.094 0.000 0.937 0.000 <frozen importlib._bootstrap_external>:1498(find_spec). 23926 0.089 0.000 0.109 0.000 /usr/lib/python3.8/enum.py:313(__call__). 3136 0.089 0.000 1.439 0.000 <frozen importlib._bootstrap_external>:1034(get_data). 473 0.083 0.000 0.120 0.000 <frozen importlib._bootstrap_external>:1317(_path_hooks). 1417/288 0.077 0.000 0.151 0.001 /usr/lib/python3.8/sre_parse.py:493(_parse). 4910 0.069 0.000 0.069 0.000 {method 'PythonNext' of 'third_party.nucleus.io.python.sam_reader.SamIterable' objects}. 3614/1 0.069 0.000 11.965 11.965 {built-in method builtins.exec}. 701 0.066 0.000 0.066 0.000 {built-in method posix.listdir}. 15152 0.063 0.000 0.268 0.000 /usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:69(_wrapreduction). 32755 0.062 0.000 0.135 0.000 <frozen importlib._bootstrap_external>:121(_path_join). 171/133 0.060 0.000 0.178 0.001 {built-in method _imp.exec_dynamic}. 1 0.059 0.059 0.059 0.059 {method 'calls_from_allele_counts' of 'deepvariant.python.variant_calling_multisample.VariantCaller' objects}. 23323 0.058 0.000 0.163 0.000 /usr/lib/python3.8/inspect.py:2489(__init__). 15563/15359 0.053 0.000 0.384 0.000 {built-in method numpy.core._multiarray_umath.implement_array_function}. 3601 0.053 0.000 0.344 0.000 /usr/lib/python3.8/inspect.py:1102(getfullargspec). 15152 0.051 0.000 0.319 0.000 /usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:2703(amax). 12155 0.049 0.000 0.060 0.000 {built-in method builtins.max}. 40200 0.049 0.000 0.049 0.000 {built-in method third_party.nucleus.util.python.utils.read_overlaps_region}. ```. In any case, the most optimized approach is some sort shared memory model with minimal overhead to keep the code nimble an",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:9597,energy efficiency,core,core,9597,"numpy.random.mtrand.RandomState' objects}. 5759 0.094 0.000 0.937 0.000 <frozen importlib._bootstrap_external>:1498(find_spec). 23926 0.089 0.000 0.109 0.000 /usr/lib/python3.8/enum.py:313(__call__). 3136 0.089 0.000 1.439 0.000 <frozen importlib._bootstrap_external>:1034(get_data). 473 0.083 0.000 0.120 0.000 <frozen importlib._bootstrap_external>:1317(_path_hooks). 1417/288 0.077 0.000 0.151 0.001 /usr/lib/python3.8/sre_parse.py:493(_parse). 4910 0.069 0.000 0.069 0.000 {method 'PythonNext' of 'third_party.nucleus.io.python.sam_reader.SamIterable' objects}. 3614/1 0.069 0.000 11.965 11.965 {built-in method builtins.exec}. 701 0.066 0.000 0.066 0.000 {built-in method posix.listdir}. 15152 0.063 0.000 0.268 0.000 /usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:69(_wrapreduction). 32755 0.062 0.000 0.135 0.000 <frozen importlib._bootstrap_external>:121(_path_join). 171/133 0.060 0.000 0.178 0.001 {built-in method _imp.exec_dynamic}. 1 0.059 0.059 0.059 0.059 {method 'calls_from_allele_counts' of 'deepvariant.python.variant_calling_multisample.VariantCaller' objects}. 23323 0.058 0.000 0.163 0.000 /usr/lib/python3.8/inspect.py:2489(__init__). 15563/15359 0.053 0.000 0.384 0.000 {built-in method numpy.core._multiarray_umath.implement_array_function}. 3601 0.053 0.000 0.344 0.000 /usr/lib/python3.8/inspect.py:1102(getfullargspec). 15152 0.051 0.000 0.319 0.000 /usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:2703(amax). 12155 0.049 0.000 0.060 0.000 {built-in method builtins.max}. 40200 0.049 0.000 0.049 0.000 {built-in method third_party.nucleus.util.python.utils.read_overlaps_region}. ```. In any case, the most optimized approach is some sort shared memory model with minimal overhead to keep the code nimble and easy to integrate or extend. The code itself is pleasantly easy to go through given the nice design, which makes it quite amendable to that. In the meantime, pass-by-reference probably could be an easier win. . Thank you,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:9803,energy efficiency,core,core,9803,"numpy.random.mtrand.RandomState' objects}. 5759 0.094 0.000 0.937 0.000 <frozen importlib._bootstrap_external>:1498(find_spec). 23926 0.089 0.000 0.109 0.000 /usr/lib/python3.8/enum.py:313(__call__). 3136 0.089 0.000 1.439 0.000 <frozen importlib._bootstrap_external>:1034(get_data). 473 0.083 0.000 0.120 0.000 <frozen importlib._bootstrap_external>:1317(_path_hooks). 1417/288 0.077 0.000 0.151 0.001 /usr/lib/python3.8/sre_parse.py:493(_parse). 4910 0.069 0.000 0.069 0.000 {method 'PythonNext' of 'third_party.nucleus.io.python.sam_reader.SamIterable' objects}. 3614/1 0.069 0.000 11.965 11.965 {built-in method builtins.exec}. 701 0.066 0.000 0.066 0.000 {built-in method posix.listdir}. 15152 0.063 0.000 0.268 0.000 /usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:69(_wrapreduction). 32755 0.062 0.000 0.135 0.000 <frozen importlib._bootstrap_external>:121(_path_join). 171/133 0.060 0.000 0.178 0.001 {built-in method _imp.exec_dynamic}. 1 0.059 0.059 0.059 0.059 {method 'calls_from_allele_counts' of 'deepvariant.python.variant_calling_multisample.VariantCaller' objects}. 23323 0.058 0.000 0.163 0.000 /usr/lib/python3.8/inspect.py:2489(__init__). 15563/15359 0.053 0.000 0.384 0.000 {built-in method numpy.core._multiarray_umath.implement_array_function}. 3601 0.053 0.000 0.344 0.000 /usr/lib/python3.8/inspect.py:1102(getfullargspec). 15152 0.051 0.000 0.319 0.000 /usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:2703(amax). 12155 0.049 0.000 0.060 0.000 {built-in method builtins.max}. 40200 0.049 0.000 0.049 0.000 {built-in method third_party.nucleus.util.python.utils.read_overlaps_region}. ```. In any case, the most optimized approach is some sort shared memory model with minimal overhead to keep the code nimble and easy to integrate or extend. The code itself is pleasantly easy to go through given the nice design, which makes it quite amendable to that. In the meantime, pass-by-reference probably could be an easier win. . Thank you,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:10032,energy efficiency,optim,optimized,10032,"numpy.random.mtrand.RandomState' objects}. 5759 0.094 0.000 0.937 0.000 <frozen importlib._bootstrap_external>:1498(find_spec). 23926 0.089 0.000 0.109 0.000 /usr/lib/python3.8/enum.py:313(__call__). 3136 0.089 0.000 1.439 0.000 <frozen importlib._bootstrap_external>:1034(get_data). 473 0.083 0.000 0.120 0.000 <frozen importlib._bootstrap_external>:1317(_path_hooks). 1417/288 0.077 0.000 0.151 0.001 /usr/lib/python3.8/sre_parse.py:493(_parse). 4910 0.069 0.000 0.069 0.000 {method 'PythonNext' of 'third_party.nucleus.io.python.sam_reader.SamIterable' objects}. 3614/1 0.069 0.000 11.965 11.965 {built-in method builtins.exec}. 701 0.066 0.000 0.066 0.000 {built-in method posix.listdir}. 15152 0.063 0.000 0.268 0.000 /usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:69(_wrapreduction). 32755 0.062 0.000 0.135 0.000 <frozen importlib._bootstrap_external>:121(_path_join). 171/133 0.060 0.000 0.178 0.001 {built-in method _imp.exec_dynamic}. 1 0.059 0.059 0.059 0.059 {method 'calls_from_allele_counts' of 'deepvariant.python.variant_calling_multisample.VariantCaller' objects}. 23323 0.058 0.000 0.163 0.000 /usr/lib/python3.8/inspect.py:2489(__init__). 15563/15359 0.053 0.000 0.384 0.000 {built-in method numpy.core._multiarray_umath.implement_array_function}. 3601 0.053 0.000 0.344 0.000 /usr/lib/python3.8/inspect.py:1102(getfullargspec). 15152 0.051 0.000 0.319 0.000 /usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:2703(amax). 12155 0.049 0.000 0.060 0.000 {built-in method builtins.max}. 40200 0.049 0.000 0.049 0.000 {built-in method third_party.nucleus.util.python.utils.read_overlaps_region}. ```. In any case, the most optimized approach is some sort shared memory model with minimal overhead to keep the code nimble and easy to integrate or extend. The code itself is pleasantly easy to go through given the nice design, which makes it quite amendable to that. In the meantime, pass-by-reference probably could be an easier win. . Thank you,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:10078,energy efficiency,model,model,10078,"numpy.random.mtrand.RandomState' objects}. 5759 0.094 0.000 0.937 0.000 <frozen importlib._bootstrap_external>:1498(find_spec). 23926 0.089 0.000 0.109 0.000 /usr/lib/python3.8/enum.py:313(__call__). 3136 0.089 0.000 1.439 0.000 <frozen importlib._bootstrap_external>:1034(get_data). 473 0.083 0.000 0.120 0.000 <frozen importlib._bootstrap_external>:1317(_path_hooks). 1417/288 0.077 0.000 0.151 0.001 /usr/lib/python3.8/sre_parse.py:493(_parse). 4910 0.069 0.000 0.069 0.000 {method 'PythonNext' of 'third_party.nucleus.io.python.sam_reader.SamIterable' objects}. 3614/1 0.069 0.000 11.965 11.965 {built-in method builtins.exec}. 701 0.066 0.000 0.066 0.000 {built-in method posix.listdir}. 15152 0.063 0.000 0.268 0.000 /usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:69(_wrapreduction). 32755 0.062 0.000 0.135 0.000 <frozen importlib._bootstrap_external>:121(_path_join). 171/133 0.060 0.000 0.178 0.001 {built-in method _imp.exec_dynamic}. 1 0.059 0.059 0.059 0.059 {method 'calls_from_allele_counts' of 'deepvariant.python.variant_calling_multisample.VariantCaller' objects}. 23323 0.058 0.000 0.163 0.000 /usr/lib/python3.8/inspect.py:2489(__init__). 15563/15359 0.053 0.000 0.384 0.000 {built-in method numpy.core._multiarray_umath.implement_array_function}. 3601 0.053 0.000 0.344 0.000 /usr/lib/python3.8/inspect.py:1102(getfullargspec). 15152 0.051 0.000 0.319 0.000 /usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:2703(amax). 12155 0.049 0.000 0.060 0.000 {built-in method builtins.max}. 40200 0.049 0.000 0.049 0.000 {built-in method third_party.nucleus.util.python.utils.read_overlaps_region}. ```. In any case, the most optimized approach is some sort shared memory model with minimal overhead to keep the code nimble and easy to integrate or extend. The code itself is pleasantly easy to go through given the nice design, which makes it quite amendable to that. In the meantime, pass-by-reference probably could be an easier win. . Thank you,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:6732,integrability,Buffer,BufferedReader,6732,"ps://blog.tensorflow.org/2018/08/code-with-eager-execution-run-with-graphs.html), which could be the culprit. The thing is that there are some quick ways now to make it fast with almost no major code-rewrite, though there are other natural optimizations that are relatively obvious given the analysis. For the `make_examples` analysis, the task is much simpler to fix by localizing I/O through just a shared memory model and/or pass-by-reference approach, as illustrated by the call distribution (with an eye on the serialization/deserialization of ProtoBuf if that becomes more heavily used):. ```. Wed May 17 17:51:33 2023 make_examples_profile.txt. 2790682 function calls (2732833 primitive calls) in 11.860 seconds. Ordered by: internal time. List reduced from 11503 to 100 due to restriction <100>. ncalls tottime percall cumtime percall filename:lineno(function). 171/168 1.927 0.011 1.940 0.012 {built-in method _imp.create_dynamic}. 3159 0.930 0.000 0.930 0.000 {method 'read' of '_io.BufferedReader' objects}. 19018 0.694 0.000 0.694 0.000 {built-in method posix.stat}. 2 0.586 0.293 0.586 0.293 {method 'counts' of 'deepvariant.python.allelecounter.AlleleCounter' objects}. 3136 0.475 0.000 0.475 0.000 {built-in method marshal.loads}. 3136 0.422 0.000 0.422 0.000 {built-in method io.open_code}. 15152 0.183 0.000 0.183 0.000 {method 'reduce' of 'numpy.ufunc' objects}. 3136 0.164 0.000 2.265 0.001 <frozen importlib._bootstrap_external>:914(get_code). 5151 0.146 0.000 0.436 0.000 /tmp/Bazel.runfiles_t7ewtxb0/runfiles/com_google_deepvariant/third_party/nucleus/util/genomics_math.py:196(normalize_log10_probs). 1 0.146 0.146 0.146 0.146 {method 'summary_counts' of 'deepvariant.python.allelecounter.AlleleCounter' objects}. 10001 0.142 0.000 0.428 0.000 /tmp/Bazel.runfiles_t7ewtxb0/runfiles/com_google_deepvariant/deepvariant/variant_caller.py:249(with_gq_and_likelihoods). 5151 0.134 0.000 0.610 0.000 /tmp/Bazel.runfiles_t7ewtxb0/runfiles/com_google_deepvariant/deepvariant/variant_ca",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:10142,integrability,integr,integrate,10142,"numpy.random.mtrand.RandomState' objects}. 5759 0.094 0.000 0.937 0.000 <frozen importlib._bootstrap_external>:1498(find_spec). 23926 0.089 0.000 0.109 0.000 /usr/lib/python3.8/enum.py:313(__call__). 3136 0.089 0.000 1.439 0.000 <frozen importlib._bootstrap_external>:1034(get_data). 473 0.083 0.000 0.120 0.000 <frozen importlib._bootstrap_external>:1317(_path_hooks). 1417/288 0.077 0.000 0.151 0.001 /usr/lib/python3.8/sre_parse.py:493(_parse). 4910 0.069 0.000 0.069 0.000 {method 'PythonNext' of 'third_party.nucleus.io.python.sam_reader.SamIterable' objects}. 3614/1 0.069 0.000 11.965 11.965 {built-in method builtins.exec}. 701 0.066 0.000 0.066 0.000 {built-in method posix.listdir}. 15152 0.063 0.000 0.268 0.000 /usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:69(_wrapreduction). 32755 0.062 0.000 0.135 0.000 <frozen importlib._bootstrap_external>:121(_path_join). 171/133 0.060 0.000 0.178 0.001 {built-in method _imp.exec_dynamic}. 1 0.059 0.059 0.059 0.059 {method 'calls_from_allele_counts' of 'deepvariant.python.variant_calling_multisample.VariantCaller' objects}. 23323 0.058 0.000 0.163 0.000 /usr/lib/python3.8/inspect.py:2489(__init__). 15563/15359 0.053 0.000 0.384 0.000 {built-in method numpy.core._multiarray_umath.implement_array_function}. 3601 0.053 0.000 0.344 0.000 /usr/lib/python3.8/inspect.py:1102(getfullargspec). 15152 0.051 0.000 0.319 0.000 /usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:2703(amax). 12155 0.049 0.000 0.060 0.000 {built-in method builtins.max}. 40200 0.049 0.000 0.049 0.000 {built-in method third_party.nucleus.util.python.utils.read_overlaps_region}. ```. In any case, the most optimized approach is some sort shared memory model with minimal overhead to keep the code nimble and easy to integrate or extend. The code itself is pleasantly easy to go through given the nice design, which makes it quite amendable to that. In the meantime, pass-by-reference probably could be an easier win. . Thank you,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:185,interoperability,share,shared,185,"Hi Andrew and Steve,. That's a good idea, and probably shifting to pass-by-reference might minimize the overhead between functions or even through minimizing the number of functions. A shared memory approach operated upon by the different functions could be ideal with maximizing performance, where even utilizing [NVidia's Unified Virtual Memory (UVM)](https://developer.nvidia.com/blog/maximizing-unified-memory-performance-cuda/) could help, but requires some care. Currently the `call_variants` memory profile looks something like this, with minimal changes across the running process:. ```. types | # objects | total size. ================================================================ | =========== | ============. str | 223354 | 35.79 MB. dict | 88941 | 25.94 MB. code | 50107 | 8.54 MB. type | 6121 | 5.65 MB. tuple | 63884 | 3.62 MB. list | 30942 | 3.19 MB. set | 2864 | 1.51 MB. weakref | 14251 | 1002.02 KB. abc.ABCMeta | 784 | 826.05 KB. cell | 20911 | 816.84 KB. int | 25259 | 697.77 KB. builtin_function_or_method | 8801 | 618.82 KB. google.protobuf.pyext.cpp_message.GeneratedProtocolMessageType | 679 | 594.12 KB. frozenset | 1862 | 541.02 KB. function (__init__) | 3439 | 456.74 KB. ```. Based on the above, that made me curious as pass-by-reference could show benefits, but wanted to see what the functions with lots of calls might be. Those types of functions are quite heavy regarding stack setup and teardown -- with intermediate local memory allocation -- which could also be the culprit based on Steve's temporal data:. ```. Wed May 17 18:54:58 2023 call_variants_profile.txt. 12867185 function calls (11416794 primitive calls) in 61.905 seconds. Ordered by: internal time. List reduced from 3098 to 300 due to restriction <300>. ncalls tottime percall cumtime percall filename:lineno(function). 9 31.607 3.512 31.608 3.512 {built-in method tensorflow.python.client._pywrap_tf_session.TF_SessionRun_wrapper}. 2 14.137 7.069 14.137 7.069 {built-in method tensorflow.python.cli",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:6140,interoperability,share,shared,6140,"s_dtype). 6669 0.113 0.000 0.118 0.000 /usr/local/lib/python3.8/dist-packages/gast/gast.py:15(create_node). 155316 0.108 0.000 0.183 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:6314(get_default_graph). 6015 0.105 0.000 0.105 0.000 {built-in method tensorflow.python.util._tf_stack.extract_stack_for_op}. 6015 0.104 0.000 0.149 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:1879(_NodeDef). ```. What I'm seeing above is something quite interesting. The execution seems heavy on [eager execution rather than graph execution](https://blog.tensorflow.org/2018/08/code-with-eager-execution-run-with-graphs.html), which could be the culprit. The thing is that there are some quick ways now to make it fast with almost no major code-rewrite, though there are other natural optimizations that are relatively obvious given the analysis. For the `make_examples` analysis, the task is much simpler to fix by localizing I/O through just a shared memory model and/or pass-by-reference approach, as illustrated by the call distribution (with an eye on the serialization/deserialization of ProtoBuf if that becomes more heavily used):. ```. Wed May 17 17:51:33 2023 make_examples_profile.txt. 2790682 function calls (2732833 primitive calls) in 11.860 seconds. Ordered by: internal time. List reduced from 11503 to 100 due to restriction <100>. ncalls tottime percall cumtime percall filename:lineno(function). 171/168 1.927 0.011 1.940 0.012 {built-in method _imp.create_dynamic}. 3159 0.930 0.000 0.930 0.000 {method 'read' of '_io.BufferedReader' objects}. 19018 0.694 0.000 0.694 0.000 {built-in method posix.stat}. 2 0.586 0.293 0.586 0.293 {method 'counts' of 'deepvariant.python.allelecounter.AlleleCounter' objects}. 3136 0.475 0.000 0.475 0.000 {built-in method marshal.loads}. 3136 0.422 0.000 0.422 0.000 {built-in method io.open_code}. 15152 0.183 0.000 0.183 0.000 {method 'reduce' of 'numpy.ufunc' objects}. 3136 0.164 0.000 2.265 ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:6222,interoperability,distribut,distribution,6222,"st.py:15(create_node). 155316 0.108 0.000 0.183 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:6314(get_default_graph). 6015 0.105 0.000 0.105 0.000 {built-in method tensorflow.python.util._tf_stack.extract_stack_for_op}. 6015 0.104 0.000 0.149 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:1879(_NodeDef). ```. What I'm seeing above is something quite interesting. The execution seems heavy on [eager execution rather than graph execution](https://blog.tensorflow.org/2018/08/code-with-eager-execution-run-with-graphs.html), which could be the culprit. The thing is that there are some quick ways now to make it fast with almost no major code-rewrite, though there are other natural optimizations that are relatively obvious given the analysis. For the `make_examples` analysis, the task is much simpler to fix by localizing I/O through just a shared memory model and/or pass-by-reference approach, as illustrated by the call distribution (with an eye on the serialization/deserialization of ProtoBuf if that becomes more heavily used):. ```. Wed May 17 17:51:33 2023 make_examples_profile.txt. 2790682 function calls (2732833 primitive calls) in 11.860 seconds. Ordered by: internal time. List reduced from 11503 to 100 due to restriction <100>. ncalls tottime percall cumtime percall filename:lineno(function). 171/168 1.927 0.011 1.940 0.012 {built-in method _imp.create_dynamic}. 3159 0.930 0.000 0.930 0.000 {method 'read' of '_io.BufferedReader' objects}. 19018 0.694 0.000 0.694 0.000 {built-in method posix.stat}. 2 0.586 0.293 0.586 0.293 {method 'counts' of 'deepvariant.python.allelecounter.AlleleCounter' objects}. 3136 0.475 0.000 0.475 0.000 {built-in method marshal.loads}. 3136 0.422 0.000 0.422 0.000 {built-in method io.open_code}. 15152 0.183 0.000 0.183 0.000 {method 'reduce' of 'numpy.ufunc' objects}. 3136 0.164 0.000 2.265 0.001 <frozen importlib._bootstrap_external>:914(get_code). 5151 0.146 0.000 0.436 0.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:6969,interoperability,marshal,marshal,6969,"tural optimizations that are relatively obvious given the analysis. For the `make_examples` analysis, the task is much simpler to fix by localizing I/O through just a shared memory model and/or pass-by-reference approach, as illustrated by the call distribution (with an eye on the serialization/deserialization of ProtoBuf if that becomes more heavily used):. ```. Wed May 17 17:51:33 2023 make_examples_profile.txt. 2790682 function calls (2732833 primitive calls) in 11.860 seconds. Ordered by: internal time. List reduced from 11503 to 100 due to restriction <100>. ncalls tottime percall cumtime percall filename:lineno(function). 171/168 1.927 0.011 1.940 0.012 {built-in method _imp.create_dynamic}. 3159 0.930 0.000 0.930 0.000 {method 'read' of '_io.BufferedReader' objects}. 19018 0.694 0.000 0.694 0.000 {built-in method posix.stat}. 2 0.586 0.293 0.586 0.293 {method 'counts' of 'deepvariant.python.allelecounter.AlleleCounter' objects}. 3136 0.475 0.000 0.475 0.000 {built-in method marshal.loads}. 3136 0.422 0.000 0.422 0.000 {built-in method io.open_code}. 15152 0.183 0.000 0.183 0.000 {method 'reduce' of 'numpy.ufunc' objects}. 3136 0.164 0.000 2.265 0.001 <frozen importlib._bootstrap_external>:914(get_code). 5151 0.146 0.000 0.436 0.000 /tmp/Bazel.runfiles_t7ewtxb0/runfiles/com_google_deepvariant/third_party/nucleus/util/genomics_math.py:196(normalize_log10_probs). 1 0.146 0.146 0.146 0.146 {method 'summary_counts' of 'deepvariant.python.allelecounter.AlleleCounter' objects}. 10001 0.142 0.000 0.428 0.000 /tmp/Bazel.runfiles_t7ewtxb0/runfiles/com_google_deepvariant/deepvariant/variant_caller.py:249(with_gq_and_likelihoods). 5151 0.134 0.000 0.610 0.000 /tmp/Bazel.runfiles_t7ewtxb0/runfiles/com_google_deepvariant/deepvariant/variant_caller.py:194(_calc_reference_confidence). 5268/5203 0.119 0.000 0.562 0.000 {built-in method builtins.__build_class__}. 1339 0.119 0.000 0.119 0.000 {method 'write' of 'third_party.nucleus.io.python.tfrecord_writer.TFRecordWriter' obje",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:10064,interoperability,share,shared,10064,"numpy.random.mtrand.RandomState' objects}. 5759 0.094 0.000 0.937 0.000 <frozen importlib._bootstrap_external>:1498(find_spec). 23926 0.089 0.000 0.109 0.000 /usr/lib/python3.8/enum.py:313(__call__). 3136 0.089 0.000 1.439 0.000 <frozen importlib._bootstrap_external>:1034(get_data). 473 0.083 0.000 0.120 0.000 <frozen importlib._bootstrap_external>:1317(_path_hooks). 1417/288 0.077 0.000 0.151 0.001 /usr/lib/python3.8/sre_parse.py:493(_parse). 4910 0.069 0.000 0.069 0.000 {method 'PythonNext' of 'third_party.nucleus.io.python.sam_reader.SamIterable' objects}. 3614/1 0.069 0.000 11.965 11.965 {built-in method builtins.exec}. 701 0.066 0.000 0.066 0.000 {built-in method posix.listdir}. 15152 0.063 0.000 0.268 0.000 /usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:69(_wrapreduction). 32755 0.062 0.000 0.135 0.000 <frozen importlib._bootstrap_external>:121(_path_join). 171/133 0.060 0.000 0.178 0.001 {built-in method _imp.exec_dynamic}. 1 0.059 0.059 0.059 0.059 {method 'calls_from_allele_counts' of 'deepvariant.python.variant_calling_multisample.VariantCaller' objects}. 23323 0.058 0.000 0.163 0.000 /usr/lib/python3.8/inspect.py:2489(__init__). 15563/15359 0.053 0.000 0.384 0.000 {built-in method numpy.core._multiarray_umath.implement_array_function}. 3601 0.053 0.000 0.344 0.000 /usr/lib/python3.8/inspect.py:1102(getfullargspec). 15152 0.051 0.000 0.319 0.000 /usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:2703(amax). 12155 0.049 0.000 0.060 0.000 {built-in method builtins.max}. 40200 0.049 0.000 0.049 0.000 {built-in method third_party.nucleus.util.python.utils.read_overlaps_region}. ```. In any case, the most optimized approach is some sort shared memory model with minimal overhead to keep the code nimble and easy to integrate or extend. The code itself is pleasantly easy to go through given the nice design, which makes it quite amendable to that. In the meantime, pass-by-reference probably could be an easier win. . Thank you,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:10142,interoperability,integr,integrate,10142,"numpy.random.mtrand.RandomState' objects}. 5759 0.094 0.000 0.937 0.000 <frozen importlib._bootstrap_external>:1498(find_spec). 23926 0.089 0.000 0.109 0.000 /usr/lib/python3.8/enum.py:313(__call__). 3136 0.089 0.000 1.439 0.000 <frozen importlib._bootstrap_external>:1034(get_data). 473 0.083 0.000 0.120 0.000 <frozen importlib._bootstrap_external>:1317(_path_hooks). 1417/288 0.077 0.000 0.151 0.001 /usr/lib/python3.8/sre_parse.py:493(_parse). 4910 0.069 0.000 0.069 0.000 {method 'PythonNext' of 'third_party.nucleus.io.python.sam_reader.SamIterable' objects}. 3614/1 0.069 0.000 11.965 11.965 {built-in method builtins.exec}. 701 0.066 0.000 0.066 0.000 {built-in method posix.listdir}. 15152 0.063 0.000 0.268 0.000 /usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:69(_wrapreduction). 32755 0.062 0.000 0.135 0.000 <frozen importlib._bootstrap_external>:121(_path_join). 171/133 0.060 0.000 0.178 0.001 {built-in method _imp.exec_dynamic}. 1 0.059 0.059 0.059 0.059 {method 'calls_from_allele_counts' of 'deepvariant.python.variant_calling_multisample.VariantCaller' objects}. 23323 0.058 0.000 0.163 0.000 /usr/lib/python3.8/inspect.py:2489(__init__). 15563/15359 0.053 0.000 0.384 0.000 {built-in method numpy.core._multiarray_umath.implement_array_function}. 3601 0.053 0.000 0.344 0.000 /usr/lib/python3.8/inspect.py:1102(getfullargspec). 15152 0.051 0.000 0.319 0.000 /usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:2703(amax). 12155 0.049 0.000 0.060 0.000 {built-in method builtins.max}. 40200 0.049 0.000 0.049 0.000 {built-in method third_party.nucleus.util.python.utils.read_overlaps_region}. ```. In any case, the most optimized approach is some sort shared memory model with minimal overhead to keep the code nimble and easy to integrate or extend. The code itself is pleasantly easy to go through given the nice design, which makes it quite amendable to that. In the meantime, pass-by-reference probably could be an easier win. . Thank you,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:1440,modifiability,interm,intermediate,1440,"ut requires some care. Currently the `call_variants` memory profile looks something like this, with minimal changes across the running process:. ```. types | # objects | total size. ================================================================ | =========== | ============. str | 223354 | 35.79 MB. dict | 88941 | 25.94 MB. code | 50107 | 8.54 MB. type | 6121 | 5.65 MB. tuple | 63884 | 3.62 MB. list | 30942 | 3.19 MB. set | 2864 | 1.51 MB. weakref | 14251 | 1002.02 KB. abc.ABCMeta | 784 | 826.05 KB. cell | 20911 | 816.84 KB. int | 25259 | 697.77 KB. builtin_function_or_method | 8801 | 618.82 KB. google.protobuf.pyext.cpp_message.GeneratedProtocolMessageType | 679 | 594.12 KB. frozenset | 1862 | 541.02 KB. function (__init__) | 3439 | 456.74 KB. ```. Based on the above, that made me curious as pass-by-reference could show benefits, but wanted to see what the functions with lots of calls might be. Those types of functions are quite heavy regarding stack setup and teardown -- with intermediate local memory allocation -- which could also be the culprit based on Steve's temporal data:. ```. Wed May 17 18:54:58 2023 call_variants_profile.txt. 12867185 function calls (11416794 primitive calls) in 61.905 seconds. Ordered by: internal time. List reduced from 3098 to 300 due to restriction <300>. ncalls tottime percall cumtime percall filename:lineno(function). 9 31.607 3.512 31.608 3.512 {built-in method tensorflow.python.client._pywrap_tf_session.TF_SessionRun_wrapper}. 2 14.137 7.069 14.137 7.069 {built-in method tensorflow.python.client._pywrap_tf_session.TF_CloseSession}. 434721 0.900 0.000 0.913 0.000 {built-in method builtins.hasattr}. 241294 0.868 0.000 3.111 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:5866(get_controller). 241296 0.462 0.000 0.462 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py:932(_mode). 413316 0.452 0.000 0.541 0.000 /usr/lib/python3.8/contextlib.py:82(__init__). 9 0.409 0.045 0.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:2169,modifiability,pac,packages,2169,"_init__) | 3439 | 456.74 KB. ```. Based on the above, that made me curious as pass-by-reference could show benefits, but wanted to see what the functions with lots of calls might be. Those types of functions are quite heavy regarding stack setup and teardown -- with intermediate local memory allocation -- which could also be the culprit based on Steve's temporal data:. ```. Wed May 17 18:54:58 2023 call_variants_profile.txt. 12867185 function calls (11416794 primitive calls) in 61.905 seconds. Ordered by: internal time. List reduced from 3098 to 300 due to restriction <300>. ncalls tottime percall cumtime percall filename:lineno(function). 9 31.607 3.512 31.608 3.512 {built-in method tensorflow.python.client._pywrap_tf_session.TF_SessionRun_wrapper}. 2 14.137 7.069 14.137 7.069 {built-in method tensorflow.python.client._pywrap_tf_session.TF_CloseSession}. 434721 0.900 0.000 0.913 0.000 {built-in method builtins.hasattr}. 241294 0.868 0.000 3.111 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:5866(get_controller). 241296 0.462 0.000 0.462 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py:932(_mode). 413316 0.452 0.000 0.541 0.000 /usr/lib/python3.8/contextlib.py:82(__init__). 9 0.409 0.045 0.409 0.045 {built-in method tensorflow.python.client._pywrap_tf_session.ExtendSession}. 413315/164299 0.370 0.000 1.371 0.000 /usr/lib/python3.8/contextlib.py:117(__exit__). 825922/196 0.347 0.000 61.452 0.314 {built-in method builtins.next}. 113550 0.334 0.000 0.646 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:5887(_get_outer_context_and_inner_device_stack). 111268 0.304 0.000 4.646 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:6030(executing_eagerly_outside_functions). 1379 0.300 0.000 0.300 0.000 {method 'copy' of 'dict' objects}. 257584 0.293 0.000 0.293 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py:950(executing_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:2296,modifiability,pac,packages,2296, to see what the functions with lots of calls might be. Those types of functions are quite heavy regarding stack setup and teardown -- with intermediate local memory allocation -- which could also be the culprit based on Steve's temporal data:. ```. Wed May 17 18:54:58 2023 call_variants_profile.txt. 12867185 function calls (11416794 primitive calls) in 61.905 seconds. Ordered by: internal time. List reduced from 3098 to 300 due to restriction <300>. ncalls tottime percall cumtime percall filename:lineno(function). 9 31.607 3.512 31.608 3.512 {built-in method tensorflow.python.client._pywrap_tf_session.TF_SessionRun_wrapper}. 2 14.137 7.069 14.137 7.069 {built-in method tensorflow.python.client._pywrap_tf_session.TF_CloseSession}. 434721 0.900 0.000 0.913 0.000 {built-in method builtins.hasattr}. 241294 0.868 0.000 3.111 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:5866(get_controller). 241296 0.462 0.000 0.462 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py:932(_mode). 413316 0.452 0.000 0.541 0.000 /usr/lib/python3.8/contextlib.py:82(__init__). 9 0.409 0.045 0.409 0.045 {built-in method tensorflow.python.client._pywrap_tf_session.ExtendSession}. 413315/164299 0.370 0.000 1.371 0.000 /usr/lib/python3.8/contextlib.py:117(__exit__). 825922/196 0.347 0.000 61.452 0.314 {built-in method builtins.next}. 113550 0.334 0.000 0.646 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:5887(_get_outer_context_and_inner_device_stack). 111268 0.304 0.000 4.646 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:6030(executing_eagerly_outside_functions). 1379 0.300 0.000 0.300 0.000 {method 'copy' of 'dict' objects}. 257584 0.293 0.000 0.293 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py:950(executing_eagerly). 413316/164111 0.253 0.000 2.725 0.000 /usr/lib/python3.8/contextlib.py:108(__enter__). 659/110 0.247 0.000 0.248 0.00,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:2517,modifiability,Exten,ExtendSession,2517,ve's temporal data:. ```. Wed May 17 18:54:58 2023 call_variants_profile.txt. 12867185 function calls (11416794 primitive calls) in 61.905 seconds. Ordered by: internal time. List reduced from 3098 to 300 due to restriction <300>. ncalls tottime percall cumtime percall filename:lineno(function). 9 31.607 3.512 31.608 3.512 {built-in method tensorflow.python.client._pywrap_tf_session.TF_SessionRun_wrapper}. 2 14.137 7.069 14.137 7.069 {built-in method tensorflow.python.client._pywrap_tf_session.TF_CloseSession}. 434721 0.900 0.000 0.913 0.000 {built-in method builtins.hasattr}. 241294 0.868 0.000 3.111 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:5866(get_controller). 241296 0.462 0.000 0.462 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py:932(_mode). 413316 0.452 0.000 0.541 0.000 /usr/lib/python3.8/contextlib.py:82(__init__). 9 0.409 0.045 0.409 0.045 {built-in method tensorflow.python.client._pywrap_tf_session.ExtendSession}. 413315/164299 0.370 0.000 1.371 0.000 /usr/lib/python3.8/contextlib.py:117(__exit__). 825922/196 0.347 0.000 61.452 0.314 {built-in method builtins.next}. 113550 0.334 0.000 0.646 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:5887(_get_outer_context_and_inner_device_stack). 111268 0.304 0.000 4.646 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:6030(executing_eagerly_outside_functions). 1379 0.300 0.000 0.300 0.000 {method 'copy' of 'dict' objects}. 257584 0.293 0.000 0.293 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py:950(executing_eagerly). 413316/164111 0.253 0.000 2.725 0.000 /usr/lib/python3.8/contextlib.py:108(__enter__). 659/110 0.247 0.000 0.248 0.002 /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/utils/version_utils.py:97(swap_class). 413316 0.239 0.000 0.781 0.000 /usr/lib/python3.8/contextlib.py:238(helper). 3367 0.228 0.000 0.228 0.000 {built-in meth,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:2749,modifiability,pac,packages,2749,. ncalls tottime percall cumtime percall filename:lineno(function). 9 31.607 3.512 31.608 3.512 {built-in method tensorflow.python.client._pywrap_tf_session.TF_SessionRun_wrapper}. 2 14.137 7.069 14.137 7.069 {built-in method tensorflow.python.client._pywrap_tf_session.TF_CloseSession}. 434721 0.900 0.000 0.913 0.000 {built-in method builtins.hasattr}. 241294 0.868 0.000 3.111 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:5866(get_controller). 241296 0.462 0.000 0.462 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py:932(_mode). 413316 0.452 0.000 0.541 0.000 /usr/lib/python3.8/contextlib.py:82(__init__). 9 0.409 0.045 0.409 0.045 {built-in method tensorflow.python.client._pywrap_tf_session.ExtendSession}. 413315/164299 0.370 0.000 1.371 0.000 /usr/lib/python3.8/contextlib.py:117(__exit__). 825922/196 0.347 0.000 61.452 0.314 {built-in method builtins.next}. 113550 0.334 0.000 0.646 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:5887(_get_outer_context_and_inner_device_stack). 111268 0.304 0.000 4.646 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:6030(executing_eagerly_outside_functions). 1379 0.300 0.000 0.300 0.000 {method 'copy' of 'dict' objects}. 257584 0.293 0.000 0.293 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py:950(executing_eagerly). 413316/164111 0.253 0.000 2.725 0.000 /usr/lib/python3.8/contextlib.py:108(__enter__). 659/110 0.247 0.000 0.248 0.002 /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/utils/version_utils.py:97(swap_class). 413316 0.239 0.000 0.781 0.000 /usr/lib/python3.8/contextlib.py:238(helper). 3367 0.228 0.000 0.228 0.000 {built-in method posix.stat}. 6015 0.223 0.000 0.223 0.000 {built-in method tensorflow.python.client._pywrap_tf_session.TF_FinishOperation}. 241296 0.210 0.000 0.240 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:2903,modifiability,pac,packages,2903,on.TF_SessionRun_wrapper}. 2 14.137 7.069 14.137 7.069 {built-in method tensorflow.python.client._pywrap_tf_session.TF_CloseSession}. 434721 0.900 0.000 0.913 0.000 {built-in method builtins.hasattr}. 241294 0.868 0.000 3.111 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:5866(get_controller). 241296 0.462 0.000 0.462 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py:932(_mode). 413316 0.452 0.000 0.541 0.000 /usr/lib/python3.8/contextlib.py:82(__init__). 9 0.409 0.045 0.409 0.045 {built-in method tensorflow.python.client._pywrap_tf_session.ExtendSession}. 413315/164299 0.370 0.000 1.371 0.000 /usr/lib/python3.8/contextlib.py:117(__exit__). 825922/196 0.347 0.000 61.452 0.314 {built-in method builtins.next}. 113550 0.334 0.000 0.646 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:5887(_get_outer_context_and_inner_device_stack). 111268 0.304 0.000 4.646 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:6030(executing_eagerly_outside_functions). 1379 0.300 0.000 0.300 0.000 {method 'copy' of 'dict' objects}. 257584 0.293 0.000 0.293 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py:950(executing_eagerly). 413316/164111 0.253 0.000 2.725 0.000 /usr/lib/python3.8/contextlib.py:108(__enter__). 659/110 0.247 0.000 0.248 0.002 /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/utils/version_utils.py:97(swap_class). 413316 0.239 0.000 0.781 0.000 /usr/lib/python3.8/contextlib.py:238(helper). 3367 0.228 0.000 0.228 0.000 {built-in method posix.stat}. 6015 0.223 0.000 0.223 0.000 {built-in method tensorflow.python.client._pywrap_tf_session.TF_FinishOperation}. 241296 0.210 0.000 0.240 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:5673(get_controller). 256036 0.201 0.000 0.519 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py:2149(executing_eagerly).,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:3115,modifiability,pac,packages,3115,8 0.000 3.111 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:5866(get_controller). 241296 0.462 0.000 0.462 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py:932(_mode). 413316 0.452 0.000 0.541 0.000 /usr/lib/python3.8/contextlib.py:82(__init__). 9 0.409 0.045 0.409 0.045 {built-in method tensorflow.python.client._pywrap_tf_session.ExtendSession}. 413315/164299 0.370 0.000 1.371 0.000 /usr/lib/python3.8/contextlib.py:117(__exit__). 825922/196 0.347 0.000 61.452 0.314 {built-in method builtins.next}. 113550 0.334 0.000 0.646 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:5887(_get_outer_context_and_inner_device_stack). 111268 0.304 0.000 4.646 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:6030(executing_eagerly_outside_functions). 1379 0.300 0.000 0.300 0.000 {method 'copy' of 'dict' objects}. 257584 0.293 0.000 0.293 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py:950(executing_eagerly). 413316/164111 0.253 0.000 2.725 0.000 /usr/lib/python3.8/contextlib.py:108(__enter__). 659/110 0.247 0.000 0.248 0.002 /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/utils/version_utils.py:97(swap_class). 413316 0.239 0.000 0.781 0.000 /usr/lib/python3.8/contextlib.py:238(helper). 3367 0.228 0.000 0.228 0.000 {built-in method posix.stat}. 6015 0.223 0.000 0.223 0.000 {built-in method tensorflow.python.client._pywrap_tf_session.TF_FinishOperation}. 241296 0.210 0.000 0.240 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:5673(get_controller). 256036 0.201 0.000 0.519 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py:2149(executing_eagerly). 31845 0.189 0.000 0.189 0.000 {built-in method posix.lstat}. 5154/4773 0.189 0.000 0.751 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/op_def_library.py:410(_ExtractInputsAndAttrs). 51,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:3332,modifiability,pac,packages,3332,.py:932(_mode). 413316 0.452 0.000 0.541 0.000 /usr/lib/python3.8/contextlib.py:82(__init__). 9 0.409 0.045 0.409 0.045 {built-in method tensorflow.python.client._pywrap_tf_session.ExtendSession}. 413315/164299 0.370 0.000 1.371 0.000 /usr/lib/python3.8/contextlib.py:117(__exit__). 825922/196 0.347 0.000 61.452 0.314 {built-in method builtins.next}. 113550 0.334 0.000 0.646 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:5887(_get_outer_context_and_inner_device_stack). 111268 0.304 0.000 4.646 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:6030(executing_eagerly_outside_functions). 1379 0.300 0.000 0.300 0.000 {method 'copy' of 'dict' objects}. 257584 0.293 0.000 0.293 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py:950(executing_eagerly). 413316/164111 0.253 0.000 2.725 0.000 /usr/lib/python3.8/contextlib.py:108(__enter__). 659/110 0.247 0.000 0.248 0.002 /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/utils/version_utils.py:97(swap_class). 413316 0.239 0.000 0.781 0.000 /usr/lib/python3.8/contextlib.py:238(helper). 3367 0.228 0.000 0.228 0.000 {built-in method posix.stat}. 6015 0.223 0.000 0.223 0.000 {built-in method tensorflow.python.client._pywrap_tf_session.TF_FinishOperation}. 241296 0.210 0.000 0.240 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:5673(get_controller). 256036 0.201 0.000 0.519 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py:2149(executing_eagerly). 31845 0.189 0.000 0.189 0.000 {built-in method posix.lstat}. 5154/4773 0.189 0.000 0.751 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/op_def_library.py:410(_ExtractInputsAndAttrs). 5154/4773 0.187 0.000 4.064 0.001 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/op_def_library.py:751(_apply_op_helper). 6015 0.167 0.000 0.935 0.000 /usr/local/lib/python3.8/dist-packages/tensorflo,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:3712,modifiability,pac,packages,3712,00 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:5887(_get_outer_context_and_inner_device_stack). 111268 0.304 0.000 4.646 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:6030(executing_eagerly_outside_functions). 1379 0.300 0.000 0.300 0.000 {method 'copy' of 'dict' objects}. 257584 0.293 0.000 0.293 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py:950(executing_eagerly). 413316/164111 0.253 0.000 2.725 0.000 /usr/lib/python3.8/contextlib.py:108(__enter__). 659/110 0.247 0.000 0.248 0.002 /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/utils/version_utils.py:97(swap_class). 413316 0.239 0.000 0.781 0.000 /usr/lib/python3.8/contextlib.py:238(helper). 3367 0.228 0.000 0.228 0.000 {built-in method posix.stat}. 6015 0.223 0.000 0.223 0.000 {built-in method tensorflow.python.client._pywrap_tf_session.TF_FinishOperation}. 241296 0.210 0.000 0.240 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:5673(get_controller). 256036 0.201 0.000 0.519 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py:2149(executing_eagerly). 31845 0.189 0.000 0.189 0.000 {built-in method posix.lstat}. 5154/4773 0.189 0.000 0.751 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/op_def_library.py:410(_ExtractInputsAndAttrs). 5154/4773 0.187 0.000 4.064 0.001 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/op_def_library.py:751(_apply_op_helper). 6015 0.167 0.000 0.935 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:1906(_create_c_op). 627432/627423 0.165 0.000 0.182 0.000 {built-in method builtins.getattr}. 709467/709457 0.165 0.000 0.232 0.000 {built-in method builtins.isinstance}. 564/94 0.149 0.000 0.149 0.002 /usr/local/lib/python3.8/dist-packages/keras/utils/version_utils.py:89(swap_class). 6024 0.148 0.000 0.161 0.000 /usr/local/lib/python3.8/dist-packages/,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:3839,modifiability,pac,packages,3839,11268 0.304 0.000 4.646 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:6030(executing_eagerly_outside_functions). 1379 0.300 0.000 0.300 0.000 {method 'copy' of 'dict' objects}. 257584 0.293 0.000 0.293 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py:950(executing_eagerly). 413316/164111 0.253 0.000 2.725 0.000 /usr/lib/python3.8/contextlib.py:108(__enter__). 659/110 0.247 0.000 0.248 0.002 /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/utils/version_utils.py:97(swap_class). 413316 0.239 0.000 0.781 0.000 /usr/lib/python3.8/contextlib.py:238(helper). 3367 0.228 0.000 0.228 0.000 {built-in method posix.stat}. 6015 0.223 0.000 0.223 0.000 {built-in method tensorflow.python.client._pywrap_tf_session.TF_FinishOperation}. 241296 0.210 0.000 0.240 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:5673(get_controller). 256036 0.201 0.000 0.519 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py:2149(executing_eagerly). 31845 0.189 0.000 0.189 0.000 {built-in method posix.lstat}. 5154/4773 0.189 0.000 0.751 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/op_def_library.py:410(_ExtractInputsAndAttrs). 5154/4773 0.187 0.000 4.064 0.001 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/op_def_library.py:751(_apply_op_helper). 6015 0.167 0.000 0.935 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:1906(_create_c_op). 627432/627423 0.165 0.000 0.182 0.000 {built-in method builtins.getattr}. 709467/709457 0.165 0.000 0.232 0.000 {built-in method builtins.isinstance}. 564/94 0.149 0.000 0.149 0.002 /usr/local/lib/python3.8/dist-packages/keras/utils/version_utils.py:89(swap_class). 6024 0.148 0.000 0.161 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/util/lock_util.py:105(_another_group_active). 120647 0.140 0.000 0.266 0.000 /usr/local/lib/python3.8/dist-pa,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:4033,modifiability,pac,packages,4033,'dict' objects}. 257584 0.293 0.000 0.293 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py:950(executing_eagerly). 413316/164111 0.253 0.000 2.725 0.000 /usr/lib/python3.8/contextlib.py:108(__enter__). 659/110 0.247 0.000 0.248 0.002 /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/utils/version_utils.py:97(swap_class). 413316 0.239 0.000 0.781 0.000 /usr/lib/python3.8/contextlib.py:238(helper). 3367 0.228 0.000 0.228 0.000 {built-in method posix.stat}. 6015 0.223 0.000 0.223 0.000 {built-in method tensorflow.python.client._pywrap_tf_session.TF_FinishOperation}. 241296 0.210 0.000 0.240 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:5673(get_controller). 256036 0.201 0.000 0.519 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py:2149(executing_eagerly). 31845 0.189 0.000 0.189 0.000 {built-in method posix.lstat}. 5154/4773 0.189 0.000 0.751 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/op_def_library.py:410(_ExtractInputsAndAttrs). 5154/4773 0.187 0.000 4.064 0.001 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/op_def_library.py:751(_apply_op_helper). 6015 0.167 0.000 0.935 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:1906(_create_c_op). 627432/627423 0.165 0.000 0.182 0.000 {built-in method builtins.getattr}. 709467/709457 0.165 0.000 0.232 0.000 {built-in method builtins.isinstance}. 564/94 0.149 0.000 0.149 0.002 /usr/local/lib/python3.8/dist-packages/keras/utils/version_utils.py:89(swap_class). 6024 0.148 0.000 0.161 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/util/lock_util.py:105(_another_group_active). 120647 0.140 0.000 0.266 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py:249(push). 259701 0.137 0.000 0.197 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:5351(_device_function_stack). 58954 ,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:4181,modifiability,pac,packages,4181,3316/164111 0.253 0.000 2.725 0.000 /usr/lib/python3.8/contextlib.py:108(__enter__). 659/110 0.247 0.000 0.248 0.002 /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/utils/version_utils.py:97(swap_class). 413316 0.239 0.000 0.781 0.000 /usr/lib/python3.8/contextlib.py:238(helper). 3367 0.228 0.000 0.228 0.000 {built-in method posix.stat}. 6015 0.223 0.000 0.223 0.000 {built-in method tensorflow.python.client._pywrap_tf_session.TF_FinishOperation}. 241296 0.210 0.000 0.240 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:5673(get_controller). 256036 0.201 0.000 0.519 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py:2149(executing_eagerly). 31845 0.189 0.000 0.189 0.000 {built-in method posix.lstat}. 5154/4773 0.189 0.000 0.751 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/op_def_library.py:410(_ExtractInputsAndAttrs). 5154/4773 0.187 0.000 4.064 0.001 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/op_def_library.py:751(_apply_op_helper). 6015 0.167 0.000 0.935 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:1906(_create_c_op). 627432/627423 0.165 0.000 0.182 0.000 {built-in method builtins.getattr}. 709467/709457 0.165 0.000 0.232 0.000 {built-in method builtins.isinstance}. 564/94 0.149 0.000 0.149 0.002 /usr/local/lib/python3.8/dist-packages/keras/utils/version_utils.py:89(swap_class). 6024 0.148 0.000 0.161 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/util/lock_util.py:105(_another_group_active). 120647 0.140 0.000 0.266 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py:249(push). 259701 0.137 0.000 0.197 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:5351(_device_function_stack). 58954 0.117 0.000 0.200 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/dtypes.py:719(as_dtype). 6669 0.113 0.000 0.118 0.000 /us,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:4318,modifiability,pac,packages,4318,n3.8/dist-packages/tensorflow/python/keras/utils/version_utils.py:97(swap_class). 413316 0.239 0.000 0.781 0.000 /usr/lib/python3.8/contextlib.py:238(helper). 3367 0.228 0.000 0.228 0.000 {built-in method posix.stat}. 6015 0.223 0.000 0.223 0.000 {built-in method tensorflow.python.client._pywrap_tf_session.TF_FinishOperation}. 241296 0.210 0.000 0.240 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:5673(get_controller). 256036 0.201 0.000 0.519 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py:2149(executing_eagerly). 31845 0.189 0.000 0.189 0.000 {built-in method posix.lstat}. 5154/4773 0.189 0.000 0.751 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/op_def_library.py:410(_ExtractInputsAndAttrs). 5154/4773 0.187 0.000 4.064 0.001 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/op_def_library.py:751(_apply_op_helper). 6015 0.167 0.000 0.935 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:1906(_create_c_op). 627432/627423 0.165 0.000 0.182 0.000 {built-in method builtins.getattr}. 709467/709457 0.165 0.000 0.232 0.000 {built-in method builtins.isinstance}. 564/94 0.149 0.000 0.149 0.002 /usr/local/lib/python3.8/dist-packages/keras/utils/version_utils.py:89(swap_class). 6024 0.148 0.000 0.161 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/util/lock_util.py:105(_another_group_active). 120647 0.140 0.000 0.266 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py:249(push). 259701 0.137 0.000 0.197 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:5351(_device_function_stack). 58954 0.117 0.000 0.200 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/dtypes.py:719(as_dtype). 6669 0.113 0.000 0.118 0.000 /usr/local/lib/python3.8/dist-packages/gast/gast.py:15(create_node). 155316 0.108 0.000 0.183 0.000 /usr/local/lib/python3.8/dist-packages/t,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:4594,modifiability,pac,packages,4594,ython.client._pywrap_tf_session.TF_FinishOperation}. 241296 0.210 0.000 0.240 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:5673(get_controller). 256036 0.201 0.000 0.519 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py:2149(executing_eagerly). 31845 0.189 0.000 0.189 0.000 {built-in method posix.lstat}. 5154/4773 0.189 0.000 0.751 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/op_def_library.py:410(_ExtractInputsAndAttrs). 5154/4773 0.187 0.000 4.064 0.001 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/op_def_library.py:751(_apply_op_helper). 6015 0.167 0.000 0.935 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:1906(_create_c_op). 627432/627423 0.165 0.000 0.182 0.000 {built-in method builtins.getattr}. 709467/709457 0.165 0.000 0.232 0.000 {built-in method builtins.isinstance}. 564/94 0.149 0.000 0.149 0.002 /usr/local/lib/python3.8/dist-packages/keras/utils/version_utils.py:89(swap_class). 6024 0.148 0.000 0.161 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/util/lock_util.py:105(_another_group_active). 120647 0.140 0.000 0.266 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py:249(push). 259701 0.137 0.000 0.197 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:5351(_device_function_stack). 58954 0.117 0.000 0.200 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/dtypes.py:719(as_dtype). 6669 0.113 0.000 0.118 0.000 /usr/local/lib/python3.8/dist-packages/gast/gast.py:15(create_node). 155316 0.108 0.000 0.183 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:6314(get_default_graph). 6015 0.105 0.000 0.105 0.000 {built-in method tensorflow.python.util._tf_stack.extract_stack_for_op}. 6015 0.104 0.000 0.149 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:1879(_NodeDe,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:4707,modifiability,pac,packages,4707,-packages/tensorflow/python/framework/ops.py:5673(get_controller). 256036 0.201 0.000 0.519 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py:2149(executing_eagerly). 31845 0.189 0.000 0.189 0.000 {built-in method posix.lstat}. 5154/4773 0.189 0.000 0.751 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/op_def_library.py:410(_ExtractInputsAndAttrs). 5154/4773 0.187 0.000 4.064 0.001 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/op_def_library.py:751(_apply_op_helper). 6015 0.167 0.000 0.935 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:1906(_create_c_op). 627432/627423 0.165 0.000 0.182 0.000 {built-in method builtins.getattr}. 709467/709457 0.165 0.000 0.232 0.000 {built-in method builtins.isinstance}. 564/94 0.149 0.000 0.149 0.002 /usr/local/lib/python3.8/dist-packages/keras/utils/version_utils.py:89(swap_class). 6024 0.148 0.000 0.161 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/util/lock_util.py:105(_another_group_active). 120647 0.140 0.000 0.266 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py:249(push). 259701 0.137 0.000 0.197 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:5351(_device_function_stack). 58954 0.117 0.000 0.200 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/dtypes.py:719(as_dtype). 6669 0.113 0.000 0.118 0.000 /usr/local/lib/python3.8/dist-packages/gast/gast.py:15(create_node). 155316 0.108 0.000 0.183 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:6314(get_default_graph). 6015 0.105 0.000 0.105 0.000 {built-in method tensorflow.python.util._tf_stack.extract_stack_for_op}. 6015 0.104 0.000 0.149 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:1879(_NodeDef). ```. What I'm seeing above is something quite interesting. The execution seems heavy on [eager execution rath,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:4841,modifiability,pac,packages,4841,"es/tensorflow/python/eager/context.py:2149(executing_eagerly). 31845 0.189 0.000 0.189 0.000 {built-in method posix.lstat}. 5154/4773 0.189 0.000 0.751 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/op_def_library.py:410(_ExtractInputsAndAttrs). 5154/4773 0.187 0.000 4.064 0.001 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/op_def_library.py:751(_apply_op_helper). 6015 0.167 0.000 0.935 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:1906(_create_c_op). 627432/627423 0.165 0.000 0.182 0.000 {built-in method builtins.getattr}. 709467/709457 0.165 0.000 0.232 0.000 {built-in method builtins.isinstance}. 564/94 0.149 0.000 0.149 0.002 /usr/local/lib/python3.8/dist-packages/keras/utils/version_utils.py:89(swap_class). 6024 0.148 0.000 0.161 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/util/lock_util.py:105(_another_group_active). 120647 0.140 0.000 0.266 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py:249(push). 259701 0.137 0.000 0.197 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:5351(_device_function_stack). 58954 0.117 0.000 0.200 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/dtypes.py:719(as_dtype). 6669 0.113 0.000 0.118 0.000 /usr/local/lib/python3.8/dist-packages/gast/gast.py:15(create_node). 155316 0.108 0.000 0.183 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:6314(get_default_graph). 6015 0.105 0.000 0.105 0.000 {built-in method tensorflow.python.util._tf_stack.extract_stack_for_op}. 6015 0.104 0.000 0.149 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:1879(_NodeDef). ```. What I'm seeing above is something quite interesting. The execution seems heavy on [eager execution rather than graph execution](https://blog.tensorflow.org/2018/08/code-with-eager-execution-run-with-graphs.html), which could be the culpr",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:4957,modifiability,pac,packages,4957,"lstat}. 5154/4773 0.189 0.000 0.751 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/op_def_library.py:410(_ExtractInputsAndAttrs). 5154/4773 0.187 0.000 4.064 0.001 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/op_def_library.py:751(_apply_op_helper). 6015 0.167 0.000 0.935 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:1906(_create_c_op). 627432/627423 0.165 0.000 0.182 0.000 {built-in method builtins.getattr}. 709467/709457 0.165 0.000 0.232 0.000 {built-in method builtins.isinstance}. 564/94 0.149 0.000 0.149 0.002 /usr/local/lib/python3.8/dist-packages/keras/utils/version_utils.py:89(swap_class). 6024 0.148 0.000 0.161 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/util/lock_util.py:105(_another_group_active). 120647 0.140 0.000 0.266 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py:249(push). 259701 0.137 0.000 0.197 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:5351(_device_function_stack). 58954 0.117 0.000 0.200 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/dtypes.py:719(as_dtype). 6669 0.113 0.000 0.118 0.000 /usr/local/lib/python3.8/dist-packages/gast/gast.py:15(create_node). 155316 0.108 0.000 0.183 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:6314(get_default_graph). 6015 0.105 0.000 0.105 0.000 {built-in method tensorflow.python.util._tf_stack.extract_stack_for_op}. 6015 0.104 0.000 0.149 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:1879(_NodeDef). ```. What I'm seeing above is something quite interesting. The execution seems heavy on [eager execution rather than graph execution](https://blog.tensorflow.org/2018/08/code-with-eager-execution-run-with-graphs.html), which could be the culprit. The thing is that there are some quick ways now to make it fast with almost no major code-rewrite, though there ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:5091,modifiability,pac,packages,5091,"tractInputsAndAttrs). 5154/4773 0.187 0.000 4.064 0.001 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/op_def_library.py:751(_apply_op_helper). 6015 0.167 0.000 0.935 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:1906(_create_c_op). 627432/627423 0.165 0.000 0.182 0.000 {built-in method builtins.getattr}. 709467/709457 0.165 0.000 0.232 0.000 {built-in method builtins.isinstance}. 564/94 0.149 0.000 0.149 0.002 /usr/local/lib/python3.8/dist-packages/keras/utils/version_utils.py:89(swap_class). 6024 0.148 0.000 0.161 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/util/lock_util.py:105(_another_group_active). 120647 0.140 0.000 0.266 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py:249(push). 259701 0.137 0.000 0.197 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:5351(_device_function_stack). 58954 0.117 0.000 0.200 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/dtypes.py:719(as_dtype). 6669 0.113 0.000 0.118 0.000 /usr/local/lib/python3.8/dist-packages/gast/gast.py:15(create_node). 155316 0.108 0.000 0.183 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:6314(get_default_graph). 6015 0.105 0.000 0.105 0.000 {built-in method tensorflow.python.util._tf_stack.extract_stack_for_op}. 6015 0.104 0.000 0.149 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:1879(_NodeDef). ```. What I'm seeing above is something quite interesting. The execution seems heavy on [eager execution rather than graph execution](https://blog.tensorflow.org/2018/08/code-with-eager-execution-run-with-graphs.html), which could be the culprit. The thing is that there are some quick ways now to make it fast with almost no major code-rewrite, though there are other natural optimizations that are relatively obvious given the analysis. For the `make_examples` analysis, the task is much sim",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:5212,modifiability,pac,packages,5212,"k/op_def_library.py:751(_apply_op_helper). 6015 0.167 0.000 0.935 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:1906(_create_c_op). 627432/627423 0.165 0.000 0.182 0.000 {built-in method builtins.getattr}. 709467/709457 0.165 0.000 0.232 0.000 {built-in method builtins.isinstance}. 564/94 0.149 0.000 0.149 0.002 /usr/local/lib/python3.8/dist-packages/keras/utils/version_utils.py:89(swap_class). 6024 0.148 0.000 0.161 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/util/lock_util.py:105(_another_group_active). 120647 0.140 0.000 0.266 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py:249(push). 259701 0.137 0.000 0.197 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:5351(_device_function_stack). 58954 0.117 0.000 0.200 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/dtypes.py:719(as_dtype). 6669 0.113 0.000 0.118 0.000 /usr/local/lib/python3.8/dist-packages/gast/gast.py:15(create_node). 155316 0.108 0.000 0.183 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:6314(get_default_graph). 6015 0.105 0.000 0.105 0.000 {built-in method tensorflow.python.util._tf_stack.extract_stack_for_op}. 6015 0.104 0.000 0.149 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:1879(_NodeDef). ```. What I'm seeing above is something quite interesting. The execution seems heavy on [eager execution rather than graph execution](https://blog.tensorflow.org/2018/08/code-with-eager-execution-run-with-graphs.html), which could be the culprit. The thing is that there are some quick ways now to make it fast with almost no major code-rewrite, though there are other natural optimizations that are relatively obvious given the analysis. For the `make_examples` analysis, the task is much simpler to fix by localizing I/O through just a shared memory model and/or pass-by-reference approach, as illustrated by the",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:5312,modifiability,pac,packages,5312,"t-packages/tensorflow/python/framework/ops.py:1906(_create_c_op). 627432/627423 0.165 0.000 0.182 0.000 {built-in method builtins.getattr}. 709467/709457 0.165 0.000 0.232 0.000 {built-in method builtins.isinstance}. 564/94 0.149 0.000 0.149 0.002 /usr/local/lib/python3.8/dist-packages/keras/utils/version_utils.py:89(swap_class). 6024 0.148 0.000 0.161 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/util/lock_util.py:105(_another_group_active). 120647 0.140 0.000 0.266 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py:249(push). 259701 0.137 0.000 0.197 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:5351(_device_function_stack). 58954 0.117 0.000 0.200 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/dtypes.py:719(as_dtype). 6669 0.113 0.000 0.118 0.000 /usr/local/lib/python3.8/dist-packages/gast/gast.py:15(create_node). 155316 0.108 0.000 0.183 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:6314(get_default_graph). 6015 0.105 0.000 0.105 0.000 {built-in method tensorflow.python.util._tf_stack.extract_stack_for_op}. 6015 0.104 0.000 0.149 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:1879(_NodeDef). ```. What I'm seeing above is something quite interesting. The execution seems heavy on [eager execution rather than graph execution](https://blog.tensorflow.org/2018/08/code-with-eager-execution-run-with-graphs.html), which could be the culprit. The thing is that there are some quick ways now to make it fast with almost no major code-rewrite, though there are other natural optimizations that are relatively obvious given the analysis. For the `make_examples` analysis, the task is much simpler to fix by localizing I/O through just a shared memory model and/or pass-by-reference approach, as illustrated by the call distribution (with an eye on the serialization/deserialization of ProtoBuf if that becomes mor",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:5542,modifiability,pac,packages,5542,"0.000 0.149 0.002 /usr/local/lib/python3.8/dist-packages/keras/utils/version_utils.py:89(swap_class). 6024 0.148 0.000 0.161 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/util/lock_util.py:105(_another_group_active). 120647 0.140 0.000 0.266 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py:249(push). 259701 0.137 0.000 0.197 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:5351(_device_function_stack). 58954 0.117 0.000 0.200 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/dtypes.py:719(as_dtype). 6669 0.113 0.000 0.118 0.000 /usr/local/lib/python3.8/dist-packages/gast/gast.py:15(create_node). 155316 0.108 0.000 0.183 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:6314(get_default_graph). 6015 0.105 0.000 0.105 0.000 {built-in method tensorflow.python.util._tf_stack.extract_stack_for_op}. 6015 0.104 0.000 0.149 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:1879(_NodeDef). ```. What I'm seeing above is something quite interesting. The execution seems heavy on [eager execution rather than graph execution](https://blog.tensorflow.org/2018/08/code-with-eager-execution-run-with-graphs.html), which could be the culprit. The thing is that there are some quick ways now to make it fast with almost no major code-rewrite, though there are other natural optimizations that are relatively obvious given the analysis. For the `make_examples` analysis, the task is much simpler to fix by localizing I/O through just a shared memory model and/or pass-by-reference approach, as illustrated by the call distribution (with an eye on the serialization/deserialization of ProtoBuf if that becomes more heavily used):. ```. Wed May 17 17:51:33 2023 make_examples_profile.txt. 2790682 function calls (2732833 primitive calls) in 11.860 seconds. Ordered by: internal time. List reduced from 11503 to 100 due to restriction <100>. nca",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:9115,modifiability,pac,packages,9115,".107 0.054 0.107 0.054 {built-in method tensorflow.python.client._pywrap_tf_session.TF_LoadLibrary}. 5581 0.095 0.000 0.328 0.000 /usr/lib/python3.8/inspect.py:2124(_signature_from_function). 4309 0.095 0.000 0.095 0.000 {method 'randint' of 'numpy.random.mtrand.RandomState' objects}. 5759 0.094 0.000 0.937 0.000 <frozen importlib._bootstrap_external>:1498(find_spec). 23926 0.089 0.000 0.109 0.000 /usr/lib/python3.8/enum.py:313(__call__). 3136 0.089 0.000 1.439 0.000 <frozen importlib._bootstrap_external>:1034(get_data). 473 0.083 0.000 0.120 0.000 <frozen importlib._bootstrap_external>:1317(_path_hooks). 1417/288 0.077 0.000 0.151 0.001 /usr/lib/python3.8/sre_parse.py:493(_parse). 4910 0.069 0.000 0.069 0.000 {method 'PythonNext' of 'third_party.nucleus.io.python.sam_reader.SamIterable' objects}. 3614/1 0.069 0.000 11.965 11.965 {built-in method builtins.exec}. 701 0.066 0.000 0.066 0.000 {built-in method posix.listdir}. 15152 0.063 0.000 0.268 0.000 /usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:69(_wrapreduction). 32755 0.062 0.000 0.135 0.000 <frozen importlib._bootstrap_external>:121(_path_join). 171/133 0.060 0.000 0.178 0.001 {built-in method _imp.exec_dynamic}. 1 0.059 0.059 0.059 0.059 {method 'calls_from_allele_counts' of 'deepvariant.python.variant_calling_multisample.VariantCaller' objects}. 23323 0.058 0.000 0.163 0.000 /usr/lib/python3.8/inspect.py:2489(__init__). 15563/15359 0.053 0.000 0.384 0.000 {built-in method numpy.core._multiarray_umath.implement_array_function}. 3601 0.053 0.000 0.344 0.000 /usr/lib/python3.8/inspect.py:1102(getfullargspec). 15152 0.051 0.000 0.319 0.000 /usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:2703(amax). 12155 0.049 0.000 0.060 0.000 {built-in method builtins.max}. 40200 0.049 0.000 0.049 0.000 {built-in method third_party.nucleus.util.python.utils.read_overlaps_region}. ```. In any case, the most optimized approach is some sort shared memory model with minimal overhead to keep the c",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:9788,modifiability,pac,packages,9788,"numpy.random.mtrand.RandomState' objects}. 5759 0.094 0.000 0.937 0.000 <frozen importlib._bootstrap_external>:1498(find_spec). 23926 0.089 0.000 0.109 0.000 /usr/lib/python3.8/enum.py:313(__call__). 3136 0.089 0.000 1.439 0.000 <frozen importlib._bootstrap_external>:1034(get_data). 473 0.083 0.000 0.120 0.000 <frozen importlib._bootstrap_external>:1317(_path_hooks). 1417/288 0.077 0.000 0.151 0.001 /usr/lib/python3.8/sre_parse.py:493(_parse). 4910 0.069 0.000 0.069 0.000 {method 'PythonNext' of 'third_party.nucleus.io.python.sam_reader.SamIterable' objects}. 3614/1 0.069 0.000 11.965 11.965 {built-in method builtins.exec}. 701 0.066 0.000 0.066 0.000 {built-in method posix.listdir}. 15152 0.063 0.000 0.268 0.000 /usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:69(_wrapreduction). 32755 0.062 0.000 0.135 0.000 <frozen importlib._bootstrap_external>:121(_path_join). 171/133 0.060 0.000 0.178 0.001 {built-in method _imp.exec_dynamic}. 1 0.059 0.059 0.059 0.059 {method 'calls_from_allele_counts' of 'deepvariant.python.variant_calling_multisample.VariantCaller' objects}. 23323 0.058 0.000 0.163 0.000 /usr/lib/python3.8/inspect.py:2489(__init__). 15563/15359 0.053 0.000 0.384 0.000 {built-in method numpy.core._multiarray_umath.implement_array_function}. 3601 0.053 0.000 0.344 0.000 /usr/lib/python3.8/inspect.py:1102(getfullargspec). 15152 0.051 0.000 0.319 0.000 /usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:2703(amax). 12155 0.049 0.000 0.060 0.000 {built-in method builtins.max}. 40200 0.049 0.000 0.049 0.000 {built-in method third_party.nucleus.util.python.utils.read_overlaps_region}. ```. In any case, the most optimized approach is some sort shared memory model with minimal overhead to keep the code nimble and easy to integrate or extend. The code itself is pleasantly easy to go through given the nice design, which makes it quite amendable to that. In the meantime, pass-by-reference probably could be an easier win. . Thank you,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:10142,modifiability,integr,integrate,10142,"numpy.random.mtrand.RandomState' objects}. 5759 0.094 0.000 0.937 0.000 <frozen importlib._bootstrap_external>:1498(find_spec). 23926 0.089 0.000 0.109 0.000 /usr/lib/python3.8/enum.py:313(__call__). 3136 0.089 0.000 1.439 0.000 <frozen importlib._bootstrap_external>:1034(get_data). 473 0.083 0.000 0.120 0.000 <frozen importlib._bootstrap_external>:1317(_path_hooks). 1417/288 0.077 0.000 0.151 0.001 /usr/lib/python3.8/sre_parse.py:493(_parse). 4910 0.069 0.000 0.069 0.000 {method 'PythonNext' of 'third_party.nucleus.io.python.sam_reader.SamIterable' objects}. 3614/1 0.069 0.000 11.965 11.965 {built-in method builtins.exec}. 701 0.066 0.000 0.066 0.000 {built-in method posix.listdir}. 15152 0.063 0.000 0.268 0.000 /usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:69(_wrapreduction). 32755 0.062 0.000 0.135 0.000 <frozen importlib._bootstrap_external>:121(_path_join). 171/133 0.060 0.000 0.178 0.001 {built-in method _imp.exec_dynamic}. 1 0.059 0.059 0.059 0.059 {method 'calls_from_allele_counts' of 'deepvariant.python.variant_calling_multisample.VariantCaller' objects}. 23323 0.058 0.000 0.163 0.000 /usr/lib/python3.8/inspect.py:2489(__init__). 15563/15359 0.053 0.000 0.384 0.000 {built-in method numpy.core._multiarray_umath.implement_array_function}. 3601 0.053 0.000 0.344 0.000 /usr/lib/python3.8/inspect.py:1102(getfullargspec). 15152 0.051 0.000 0.319 0.000 /usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:2703(amax). 12155 0.049 0.000 0.060 0.000 {built-in method builtins.max}. 40200 0.049 0.000 0.049 0.000 {built-in method third_party.nucleus.util.python.utils.read_overlaps_region}. ```. In any case, the most optimized approach is some sort shared memory model with minimal overhead to keep the code nimble and easy to integrate or extend. The code itself is pleasantly easy to go through given the nice design, which makes it quite amendable to that. In the meantime, pass-by-reference probably could be an easier win. . Thank you,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:10155,modifiability,exten,extend,10155,"numpy.random.mtrand.RandomState' objects}. 5759 0.094 0.000 0.937 0.000 <frozen importlib._bootstrap_external>:1498(find_spec). 23926 0.089 0.000 0.109 0.000 /usr/lib/python3.8/enum.py:313(__call__). 3136 0.089 0.000 1.439 0.000 <frozen importlib._bootstrap_external>:1034(get_data). 473 0.083 0.000 0.120 0.000 <frozen importlib._bootstrap_external>:1317(_path_hooks). 1417/288 0.077 0.000 0.151 0.001 /usr/lib/python3.8/sre_parse.py:493(_parse). 4910 0.069 0.000 0.069 0.000 {method 'PythonNext' of 'third_party.nucleus.io.python.sam_reader.SamIterable' objects}. 3614/1 0.069 0.000 11.965 11.965 {built-in method builtins.exec}. 701 0.066 0.000 0.066 0.000 {built-in method posix.listdir}. 15152 0.063 0.000 0.268 0.000 /usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:69(_wrapreduction). 32755 0.062 0.000 0.135 0.000 <frozen importlib._bootstrap_external>:121(_path_join). 171/133 0.060 0.000 0.178 0.001 {built-in method _imp.exec_dynamic}. 1 0.059 0.059 0.059 0.059 {method 'calls_from_allele_counts' of 'deepvariant.python.variant_calling_multisample.VariantCaller' objects}. 23323 0.058 0.000 0.163 0.000 /usr/lib/python3.8/inspect.py:2489(__init__). 15563/15359 0.053 0.000 0.384 0.000 {built-in method numpy.core._multiarray_umath.implement_array_function}. 3601 0.053 0.000 0.344 0.000 /usr/lib/python3.8/inspect.py:1102(getfullargspec). 15152 0.051 0.000 0.319 0.000 /usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:2703(amax). 12155 0.049 0.000 0.060 0.000 {built-in method builtins.max}. 40200 0.049 0.000 0.049 0.000 {built-in method third_party.nucleus.util.python.utils.read_overlaps_region}. ```. In any case, the most optimized approach is some sort shared memory model with minimal overhead to keep the code nimble and easy to integrate or extend. The code itself is pleasantly easy to go through given the nice design, which makes it quite amendable to that. In the meantime, pass-by-reference probably could be an easier win. . Thank you,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:104,performance,overhead,overhead,104,"Hi Andrew and Steve,. That's a good idea, and probably shifting to pass-by-reference might minimize the overhead between functions or even through minimizing the number of functions. A shared memory approach operated upon by the different functions could be ideal with maximizing performance, where even utilizing [NVidia's Unified Virtual Memory (UVM)](https://developer.nvidia.com/blog/maximizing-unified-memory-performance-cuda/) could help, but requires some care. Currently the `call_variants` memory profile looks something like this, with minimal changes across the running process:. ```. types | # objects | total size. ================================================================ | =========== | ============. str | 223354 | 35.79 MB. dict | 88941 | 25.94 MB. code | 50107 | 8.54 MB. type | 6121 | 5.65 MB. tuple | 63884 | 3.62 MB. list | 30942 | 3.19 MB. set | 2864 | 1.51 MB. weakref | 14251 | 1002.02 KB. abc.ABCMeta | 784 | 826.05 KB. cell | 20911 | 816.84 KB. int | 25259 | 697.77 KB. builtin_function_or_method | 8801 | 618.82 KB. google.protobuf.pyext.cpp_message.GeneratedProtocolMessageType | 679 | 594.12 KB. frozenset | 1862 | 541.02 KB. function (__init__) | 3439 | 456.74 KB. ```. Based on the above, that made me curious as pass-by-reference could show benefits, but wanted to see what the functions with lots of calls might be. Those types of functions are quite heavy regarding stack setup and teardown -- with intermediate local memory allocation -- which could also be the culprit based on Steve's temporal data:. ```. Wed May 17 18:54:58 2023 call_variants_profile.txt. 12867185 function calls (11416794 primitive calls) in 61.905 seconds. Ordered by: internal time. List reduced from 3098 to 300 due to restriction <300>. ncalls tottime percall cumtime percall filename:lineno(function). 9 31.607 3.512 31.608 3.512 {built-in method tensorflow.python.client._pywrap_tf_session.TF_SessionRun_wrapper}. 2 14.137 7.069 14.137 7.069 {built-in method tensorflow.python.cli",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:192,performance,memor,memory,192,"Hi Andrew and Steve,. That's a good idea, and probably shifting to pass-by-reference might minimize the overhead between functions or even through minimizing the number of functions. A shared memory approach operated upon by the different functions could be ideal with maximizing performance, where even utilizing [NVidia's Unified Virtual Memory (UVM)](https://developer.nvidia.com/blog/maximizing-unified-memory-performance-cuda/) could help, but requires some care. Currently the `call_variants` memory profile looks something like this, with minimal changes across the running process:. ```. types | # objects | total size. ================================================================ | =========== | ============. str | 223354 | 35.79 MB. dict | 88941 | 25.94 MB. code | 50107 | 8.54 MB. type | 6121 | 5.65 MB. tuple | 63884 | 3.62 MB. list | 30942 | 3.19 MB. set | 2864 | 1.51 MB. weakref | 14251 | 1002.02 KB. abc.ABCMeta | 784 | 826.05 KB. cell | 20911 | 816.84 KB. int | 25259 | 697.77 KB. builtin_function_or_method | 8801 | 618.82 KB. google.protobuf.pyext.cpp_message.GeneratedProtocolMessageType | 679 | 594.12 KB. frozenset | 1862 | 541.02 KB. function (__init__) | 3439 | 456.74 KB. ```. Based on the above, that made me curious as pass-by-reference could show benefits, but wanted to see what the functions with lots of calls might be. Those types of functions are quite heavy regarding stack setup and teardown -- with intermediate local memory allocation -- which could also be the culprit based on Steve's temporal data:. ```. Wed May 17 18:54:58 2023 call_variants_profile.txt. 12867185 function calls (11416794 primitive calls) in 61.905 seconds. Ordered by: internal time. List reduced from 3098 to 300 due to restriction <300>. ncalls tottime percall cumtime percall filename:lineno(function). 9 31.607 3.512 31.608 3.512 {built-in method tensorflow.python.client._pywrap_tf_session.TF_SessionRun_wrapper}. 2 14.137 7.069 14.137 7.069 {built-in method tensorflow.python.cli",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:280,performance,perform,performance,280,"Hi Andrew and Steve,. That's a good idea, and probably shifting to pass-by-reference might minimize the overhead between functions or even through minimizing the number of functions. A shared memory approach operated upon by the different functions could be ideal with maximizing performance, where even utilizing [NVidia's Unified Virtual Memory (UVM)](https://developer.nvidia.com/blog/maximizing-unified-memory-performance-cuda/) could help, but requires some care. Currently the `call_variants` memory profile looks something like this, with minimal changes across the running process:. ```. types | # objects | total size. ================================================================ | =========== | ============. str | 223354 | 35.79 MB. dict | 88941 | 25.94 MB. code | 50107 | 8.54 MB. type | 6121 | 5.65 MB. tuple | 63884 | 3.62 MB. list | 30942 | 3.19 MB. set | 2864 | 1.51 MB. weakref | 14251 | 1002.02 KB. abc.ABCMeta | 784 | 826.05 KB. cell | 20911 | 816.84 KB. int | 25259 | 697.77 KB. builtin_function_or_method | 8801 | 618.82 KB. google.protobuf.pyext.cpp_message.GeneratedProtocolMessageType | 679 | 594.12 KB. frozenset | 1862 | 541.02 KB. function (__init__) | 3439 | 456.74 KB. ```. Based on the above, that made me curious as pass-by-reference could show benefits, but wanted to see what the functions with lots of calls might be. Those types of functions are quite heavy regarding stack setup and teardown -- with intermediate local memory allocation -- which could also be the culprit based on Steve's temporal data:. ```. Wed May 17 18:54:58 2023 call_variants_profile.txt. 12867185 function calls (11416794 primitive calls) in 61.905 seconds. Ordered by: internal time. List reduced from 3098 to 300 due to restriction <300>. ncalls tottime percall cumtime percall filename:lineno(function). 9 31.607 3.512 31.608 3.512 {built-in method tensorflow.python.client._pywrap_tf_session.TF_SessionRun_wrapper}. 2 14.137 7.069 14.137 7.069 {built-in method tensorflow.python.cli",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:340,performance,Memor,Memory,340,"Hi Andrew and Steve,. That's a good idea, and probably shifting to pass-by-reference might minimize the overhead between functions or even through minimizing the number of functions. A shared memory approach operated upon by the different functions could be ideal with maximizing performance, where even utilizing [NVidia's Unified Virtual Memory (UVM)](https://developer.nvidia.com/blog/maximizing-unified-memory-performance-cuda/) could help, but requires some care. Currently the `call_variants` memory profile looks something like this, with minimal changes across the running process:. ```. types | # objects | total size. ================================================================ | =========== | ============. str | 223354 | 35.79 MB. dict | 88941 | 25.94 MB. code | 50107 | 8.54 MB. type | 6121 | 5.65 MB. tuple | 63884 | 3.62 MB. list | 30942 | 3.19 MB. set | 2864 | 1.51 MB. weakref | 14251 | 1002.02 KB. abc.ABCMeta | 784 | 826.05 KB. cell | 20911 | 816.84 KB. int | 25259 | 697.77 KB. builtin_function_or_method | 8801 | 618.82 KB. google.protobuf.pyext.cpp_message.GeneratedProtocolMessageType | 679 | 594.12 KB. frozenset | 1862 | 541.02 KB. function (__init__) | 3439 | 456.74 KB. ```. Based on the above, that made me curious as pass-by-reference could show benefits, but wanted to see what the functions with lots of calls might be. Those types of functions are quite heavy regarding stack setup and teardown -- with intermediate local memory allocation -- which could also be the culprit based on Steve's temporal data:. ```. Wed May 17 18:54:58 2023 call_variants_profile.txt. 12867185 function calls (11416794 primitive calls) in 61.905 seconds. Ordered by: internal time. List reduced from 3098 to 300 due to restriction <300>. ncalls tottime percall cumtime percall filename:lineno(function). 9 31.607 3.512 31.608 3.512 {built-in method tensorflow.python.client._pywrap_tf_session.TF_SessionRun_wrapper}. 2 14.137 7.069 14.137 7.069 {built-in method tensorflow.python.cli",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:407,performance,memor,memory-performance-cuda,407,"Hi Andrew and Steve,. That's a good idea, and probably shifting to pass-by-reference might minimize the overhead between functions or even through minimizing the number of functions. A shared memory approach operated upon by the different functions could be ideal with maximizing performance, where even utilizing [NVidia's Unified Virtual Memory (UVM)](https://developer.nvidia.com/blog/maximizing-unified-memory-performance-cuda/) could help, but requires some care. Currently the `call_variants` memory profile looks something like this, with minimal changes across the running process:. ```. types | # objects | total size. ================================================================ | =========== | ============. str | 223354 | 35.79 MB. dict | 88941 | 25.94 MB. code | 50107 | 8.54 MB. type | 6121 | 5.65 MB. tuple | 63884 | 3.62 MB. list | 30942 | 3.19 MB. set | 2864 | 1.51 MB. weakref | 14251 | 1002.02 KB. abc.ABCMeta | 784 | 826.05 KB. cell | 20911 | 816.84 KB. int | 25259 | 697.77 KB. builtin_function_or_method | 8801 | 618.82 KB. google.protobuf.pyext.cpp_message.GeneratedProtocolMessageType | 679 | 594.12 KB. frozenset | 1862 | 541.02 KB. function (__init__) | 3439 | 456.74 KB. ```. Based on the above, that made me curious as pass-by-reference could show benefits, but wanted to see what the functions with lots of calls might be. Those types of functions are quite heavy regarding stack setup and teardown -- with intermediate local memory allocation -- which could also be the culprit based on Steve's temporal data:. ```. Wed May 17 18:54:58 2023 call_variants_profile.txt. 12867185 function calls (11416794 primitive calls) in 61.905 seconds. Ordered by: internal time. List reduced from 3098 to 300 due to restriction <300>. ncalls tottime percall cumtime percall filename:lineno(function). 9 31.607 3.512 31.608 3.512 {built-in method tensorflow.python.client._pywrap_tf_session.TF_SessionRun_wrapper}. 2 14.137 7.069 14.137 7.069 {built-in method tensorflow.python.cli",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:499,performance,memor,memory,499,"Hi Andrew and Steve,. That's a good idea, and probably shifting to pass-by-reference might minimize the overhead between functions or even through minimizing the number of functions. A shared memory approach operated upon by the different functions could be ideal with maximizing performance, where even utilizing [NVidia's Unified Virtual Memory (UVM)](https://developer.nvidia.com/blog/maximizing-unified-memory-performance-cuda/) could help, but requires some care. Currently the `call_variants` memory profile looks something like this, with minimal changes across the running process:. ```. types | # objects | total size. ================================================================ | =========== | ============. str | 223354 | 35.79 MB. dict | 88941 | 25.94 MB. code | 50107 | 8.54 MB. type | 6121 | 5.65 MB. tuple | 63884 | 3.62 MB. list | 30942 | 3.19 MB. set | 2864 | 1.51 MB. weakref | 14251 | 1002.02 KB. abc.ABCMeta | 784 | 826.05 KB. cell | 20911 | 816.84 KB. int | 25259 | 697.77 KB. builtin_function_or_method | 8801 | 618.82 KB. google.protobuf.pyext.cpp_message.GeneratedProtocolMessageType | 679 | 594.12 KB. frozenset | 1862 | 541.02 KB. function (__init__) | 3439 | 456.74 KB. ```. Based on the above, that made me curious as pass-by-reference could show benefits, but wanted to see what the functions with lots of calls might be. Those types of functions are quite heavy regarding stack setup and teardown -- with intermediate local memory allocation -- which could also be the culprit based on Steve's temporal data:. ```. Wed May 17 18:54:58 2023 call_variants_profile.txt. 12867185 function calls (11416794 primitive calls) in 61.905 seconds. Ordered by: internal time. List reduced from 3098 to 300 due to restriction <300>. ncalls tottime percall cumtime percall filename:lineno(function). 9 31.607 3.512 31.608 3.512 {built-in method tensorflow.python.client._pywrap_tf_session.TF_SessionRun_wrapper}. 2 14.137 7.069 14.137 7.069 {built-in method tensorflow.python.cli",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:506,performance,profil,profile,506,"Hi Andrew and Steve,. That's a good idea, and probably shifting to pass-by-reference might minimize the overhead between functions or even through minimizing the number of functions. A shared memory approach operated upon by the different functions could be ideal with maximizing performance, where even utilizing [NVidia's Unified Virtual Memory (UVM)](https://developer.nvidia.com/blog/maximizing-unified-memory-performance-cuda/) could help, but requires some care. Currently the `call_variants` memory profile looks something like this, with minimal changes across the running process:. ```. types | # objects | total size. ================================================================ | =========== | ============. str | 223354 | 35.79 MB. dict | 88941 | 25.94 MB. code | 50107 | 8.54 MB. type | 6121 | 5.65 MB. tuple | 63884 | 3.62 MB. list | 30942 | 3.19 MB. set | 2864 | 1.51 MB. weakref | 14251 | 1002.02 KB. abc.ABCMeta | 784 | 826.05 KB. cell | 20911 | 816.84 KB. int | 25259 | 697.77 KB. builtin_function_or_method | 8801 | 618.82 KB. google.protobuf.pyext.cpp_message.GeneratedProtocolMessageType | 679 | 594.12 KB. frozenset | 1862 | 541.02 KB. function (__init__) | 3439 | 456.74 KB. ```. Based on the above, that made me curious as pass-by-reference could show benefits, but wanted to see what the functions with lots of calls might be. Those types of functions are quite heavy regarding stack setup and teardown -- with intermediate local memory allocation -- which could also be the culprit based on Steve's temporal data:. ```. Wed May 17 18:54:58 2023 call_variants_profile.txt. 12867185 function calls (11416794 primitive calls) in 61.905 seconds. Ordered by: internal time. List reduced from 3098 to 300 due to restriction <300>. ncalls tottime percall cumtime percall filename:lineno(function). 9 31.607 3.512 31.608 3.512 {built-in method tensorflow.python.client._pywrap_tf_session.TF_SessionRun_wrapper}. 2 14.137 7.069 14.137 7.069 {built-in method tensorflow.python.cli",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:1459,performance,memor,memory,1459," care. Currently the `call_variants` memory profile looks something like this, with minimal changes across the running process:. ```. types | # objects | total size. ================================================================ | =========== | ============. str | 223354 | 35.79 MB. dict | 88941 | 25.94 MB. code | 50107 | 8.54 MB. type | 6121 | 5.65 MB. tuple | 63884 | 3.62 MB. list | 30942 | 3.19 MB. set | 2864 | 1.51 MB. weakref | 14251 | 1002.02 KB. abc.ABCMeta | 784 | 826.05 KB. cell | 20911 | 816.84 KB. int | 25259 | 697.77 KB. builtin_function_or_method | 8801 | 618.82 KB. google.protobuf.pyext.cpp_message.GeneratedProtocolMessageType | 679 | 594.12 KB. frozenset | 1862 | 541.02 KB. function (__init__) | 3439 | 456.74 KB. ```. Based on the above, that made me curious as pass-by-reference could show benefits, but wanted to see what the functions with lots of calls might be. Those types of functions are quite heavy regarding stack setup and teardown -- with intermediate local memory allocation -- which could also be the culprit based on Steve's temporal data:. ```. Wed May 17 18:54:58 2023 call_variants_profile.txt. 12867185 function calls (11416794 primitive calls) in 61.905 seconds. Ordered by: internal time. List reduced from 3098 to 300 due to restriction <300>. ncalls tottime percall cumtime percall filename:lineno(function). 9 31.607 3.512 31.608 3.512 {built-in method tensorflow.python.client._pywrap_tf_session.TF_SessionRun_wrapper}. 2 14.137 7.069 14.137 7.069 {built-in method tensorflow.python.client._pywrap_tf_session.TF_CloseSession}. 434721 0.900 0.000 0.913 0.000 {built-in method builtins.hasattr}. 241294 0.868 0.000 3.111 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:5866(get_controller). 241296 0.462 0.000 0.462 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py:932(_mode). 413316 0.452 0.000 0.541 0.000 /usr/lib/python3.8/contextlib.py:82(__init__). 9 0.409 0.045 0.409 0.045 {built",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:1693,performance,time,time,1693,"=========== | ============. str | 223354 | 35.79 MB. dict | 88941 | 25.94 MB. code | 50107 | 8.54 MB. type | 6121 | 5.65 MB. tuple | 63884 | 3.62 MB. list | 30942 | 3.19 MB. set | 2864 | 1.51 MB. weakref | 14251 | 1002.02 KB. abc.ABCMeta | 784 | 826.05 KB. cell | 20911 | 816.84 KB. int | 25259 | 697.77 KB. builtin_function_or_method | 8801 | 618.82 KB. google.protobuf.pyext.cpp_message.GeneratedProtocolMessageType | 679 | 594.12 KB. frozenset | 1862 | 541.02 KB. function (__init__) | 3439 | 456.74 KB. ```. Based on the above, that made me curious as pass-by-reference could show benefits, but wanted to see what the functions with lots of calls might be. Those types of functions are quite heavy regarding stack setup and teardown -- with intermediate local memory allocation -- which could also be the culprit based on Steve's temporal data:. ```. Wed May 17 18:54:58 2023 call_variants_profile.txt. 12867185 function calls (11416794 primitive calls) in 61.905 seconds. Ordered by: internal time. List reduced from 3098 to 300 due to restriction <300>. ncalls tottime percall cumtime percall filename:lineno(function). 9 31.607 3.512 31.608 3.512 {built-in method tensorflow.python.client._pywrap_tf_session.TF_SessionRun_wrapper}. 2 14.137 7.069 14.137 7.069 {built-in method tensorflow.python.client._pywrap_tf_session.TF_CloseSession}. 434721 0.900 0.000 0.913 0.000 {built-in method builtins.hasattr}. 241294 0.868 0.000 3.111 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:5866(get_controller). 241296 0.462 0.000 0.462 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py:932(_mode). 413316 0.452 0.000 0.541 0.000 /usr/lib/python3.8/contextlib.py:82(__init__). 9 0.409 0.045 0.409 0.045 {built-in method tensorflow.python.client._pywrap_tf_session.ExtendSession}. 413315/164299 0.370 0.000 1.371 0.000 /usr/lib/python3.8/contextlib.py:117(__exit__). 825922/196 0.347 0.000 61.452 0.314 {built-in method builtins.next}. 113550 ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:5979,performance,optimiz,optimizations,5979,"amework/ops.py:5351(_device_function_stack). 58954 0.117 0.000 0.200 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/dtypes.py:719(as_dtype). 6669 0.113 0.000 0.118 0.000 /usr/local/lib/python3.8/dist-packages/gast/gast.py:15(create_node). 155316 0.108 0.000 0.183 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:6314(get_default_graph). 6015 0.105 0.000 0.105 0.000 {built-in method tensorflow.python.util._tf_stack.extract_stack_for_op}. 6015 0.104 0.000 0.149 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:1879(_NodeDef). ```. What I'm seeing above is something quite interesting. The execution seems heavy on [eager execution rather than graph execution](https://blog.tensorflow.org/2018/08/code-with-eager-execution-run-with-graphs.html), which could be the culprit. The thing is that there are some quick ways now to make it fast with almost no major code-rewrite, though there are other natural optimizations that are relatively obvious given the analysis. For the `make_examples` analysis, the task is much simpler to fix by localizing I/O through just a shared memory model and/or pass-by-reference approach, as illustrated by the call distribution (with an eye on the serialization/deserialization of ProtoBuf if that becomes more heavily used):. ```. Wed May 17 17:51:33 2023 make_examples_profile.txt. 2790682 function calls (2732833 primitive calls) in 11.860 seconds. Ordered by: internal time. List reduced from 11503 to 100 due to restriction <100>. ncalls tottime percall cumtime percall filename:lineno(function). 171/168 1.927 0.011 1.940 0.012 {built-in method _imp.create_dynamic}. 3159 0.930 0.000 0.930 0.000 {method 'read' of '_io.BufferedReader' objects}. 19018 0.694 0.000 0.694 0.000 {built-in method posix.stat}. 2 0.586 0.293 0.586 0.293 {method 'counts' of 'deepvariant.python.allelecounter.AlleleCounter' objects}. 3136 0.475 0.000 0.475 0.000 {built-in method marshal.loads}. 3",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:6121,performance,I/O,I/O,6121,"work/dtypes.py:719(as_dtype). 6669 0.113 0.000 0.118 0.000 /usr/local/lib/python3.8/dist-packages/gast/gast.py:15(create_node). 155316 0.108 0.000 0.183 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:6314(get_default_graph). 6015 0.105 0.000 0.105 0.000 {built-in method tensorflow.python.util._tf_stack.extract_stack_for_op}. 6015 0.104 0.000 0.149 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:1879(_NodeDef). ```. What I'm seeing above is something quite interesting. The execution seems heavy on [eager execution rather than graph execution](https://blog.tensorflow.org/2018/08/code-with-eager-execution-run-with-graphs.html), which could be the culprit. The thing is that there are some quick ways now to make it fast with almost no major code-rewrite, though there are other natural optimizations that are relatively obvious given the analysis. For the `make_examples` analysis, the task is much simpler to fix by localizing I/O through just a shared memory model and/or pass-by-reference approach, as illustrated by the call distribution (with an eye on the serialization/deserialization of ProtoBuf if that becomes more heavily used):. ```. Wed May 17 17:51:33 2023 make_examples_profile.txt. 2790682 function calls (2732833 primitive calls) in 11.860 seconds. Ordered by: internal time. List reduced from 11503 to 100 due to restriction <100>. ncalls tottime percall cumtime percall filename:lineno(function). 171/168 1.927 0.011 1.940 0.012 {built-in method _imp.create_dynamic}. 3159 0.930 0.000 0.930 0.000 {method 'read' of '_io.BufferedReader' objects}. 19018 0.694 0.000 0.694 0.000 {built-in method posix.stat}. 2 0.586 0.293 0.586 0.293 {method 'counts' of 'deepvariant.python.allelecounter.AlleleCounter' objects}. 3136 0.475 0.000 0.475 0.000 {built-in method marshal.loads}. 3136 0.422 0.000 0.422 0.000 {built-in method io.open_code}. 15152 0.183 0.000 0.183 0.000 {method 'reduce' of 'numpy.ufunc' objects}. 313",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:6147,performance,memor,memory,6147,"). 6669 0.113 0.000 0.118 0.000 /usr/local/lib/python3.8/dist-packages/gast/gast.py:15(create_node). 155316 0.108 0.000 0.183 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:6314(get_default_graph). 6015 0.105 0.000 0.105 0.000 {built-in method tensorflow.python.util._tf_stack.extract_stack_for_op}. 6015 0.104 0.000 0.149 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:1879(_NodeDef). ```. What I'm seeing above is something quite interesting. The execution seems heavy on [eager execution rather than graph execution](https://blog.tensorflow.org/2018/08/code-with-eager-execution-run-with-graphs.html), which could be the culprit. The thing is that there are some quick ways now to make it fast with almost no major code-rewrite, though there are other natural optimizations that are relatively obvious given the analysis. For the `make_examples` analysis, the task is much simpler to fix by localizing I/O through just a shared memory model and/or pass-by-reference approach, as illustrated by the call distribution (with an eye on the serialization/deserialization of ProtoBuf if that becomes more heavily used):. ```. Wed May 17 17:51:33 2023 make_examples_profile.txt. 2790682 function calls (2732833 primitive calls) in 11.860 seconds. Ordered by: internal time. List reduced from 11503 to 100 due to restriction <100>. ncalls tottime percall cumtime percall filename:lineno(function). 171/168 1.927 0.011 1.940 0.012 {built-in method _imp.create_dynamic}. 3159 0.930 0.000 0.930 0.000 {method 'read' of '_io.BufferedReader' objects}. 19018 0.694 0.000 0.694 0.000 {built-in method posix.stat}. 2 0.586 0.293 0.586 0.293 {method 'counts' of 'deepvariant.python.allelecounter.AlleleCounter' objects}. 3136 0.475 0.000 0.475 0.000 {built-in method marshal.loads}. 3136 0.422 0.000 0.422 0.000 {built-in method io.open_code}. 15152 0.183 0.000 0.183 0.000 {method 'reduce' of 'numpy.ufunc' objects}. 3136 0.164 0.000 2.265 0.001 <",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:6480,performance,time,time,6480," 6015 0.104 0.000 0.149 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:1879(_NodeDef). ```. What I'm seeing above is something quite interesting. The execution seems heavy on [eager execution rather than graph execution](https://blog.tensorflow.org/2018/08/code-with-eager-execution-run-with-graphs.html), which could be the culprit. The thing is that there are some quick ways now to make it fast with almost no major code-rewrite, though there are other natural optimizations that are relatively obvious given the analysis. For the `make_examples` analysis, the task is much simpler to fix by localizing I/O through just a shared memory model and/or pass-by-reference approach, as illustrated by the call distribution (with an eye on the serialization/deserialization of ProtoBuf if that becomes more heavily used):. ```. Wed May 17 17:51:33 2023 make_examples_profile.txt. 2790682 function calls (2732833 primitive calls) in 11.860 seconds. Ordered by: internal time. List reduced from 11503 to 100 due to restriction <100>. ncalls tottime percall cumtime percall filename:lineno(function). 171/168 1.927 0.011 1.940 0.012 {built-in method _imp.create_dynamic}. 3159 0.930 0.000 0.930 0.000 {method 'read' of '_io.BufferedReader' objects}. 19018 0.694 0.000 0.694 0.000 {built-in method posix.stat}. 2 0.586 0.293 0.586 0.293 {method 'counts' of 'deepvariant.python.allelecounter.AlleleCounter' objects}. 3136 0.475 0.000 0.475 0.000 {built-in method marshal.loads}. 3136 0.422 0.000 0.422 0.000 {built-in method io.open_code}. 15152 0.183 0.000 0.183 0.000 {method 'reduce' of 'numpy.ufunc' objects}. 3136 0.164 0.000 2.265 0.001 <frozen importlib._bootstrap_external>:914(get_code). 5151 0.146 0.000 0.436 0.000 /tmp/Bazel.runfiles_t7ewtxb0/runfiles/com_google_deepvariant/third_party/nucleus/util/genomics_math.py:196(normalize_log10_probs). 1 0.146 0.146 0.146 0.146 {method 'summary_counts' of 'deepvariant.python.allelecounter.AlleleCounter' objects}. 10001 ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:6977,performance,load,loads,6977,"ptimizations that are relatively obvious given the analysis. For the `make_examples` analysis, the task is much simpler to fix by localizing I/O through just a shared memory model and/or pass-by-reference approach, as illustrated by the call distribution (with an eye on the serialization/deserialization of ProtoBuf if that becomes more heavily used):. ```. Wed May 17 17:51:33 2023 make_examples_profile.txt. 2790682 function calls (2732833 primitive calls) in 11.860 seconds. Ordered by: internal time. List reduced from 11503 to 100 due to restriction <100>. ncalls tottime percall cumtime percall filename:lineno(function). 171/168 1.927 0.011 1.940 0.012 {built-in method _imp.create_dynamic}. 3159 0.930 0.000 0.930 0.000 {method 'read' of '_io.BufferedReader' objects}. 19018 0.694 0.000 0.694 0.000 {built-in method posix.stat}. 2 0.586 0.293 0.586 0.293 {method 'counts' of 'deepvariant.python.allelecounter.AlleleCounter' objects}. 3136 0.475 0.000 0.475 0.000 {built-in method marshal.loads}. 3136 0.422 0.000 0.422 0.000 {built-in method io.open_code}. 15152 0.183 0.000 0.183 0.000 {method 'reduce' of 'numpy.ufunc' objects}. 3136 0.164 0.000 2.265 0.001 <frozen importlib._bootstrap_external>:914(get_code). 5151 0.146 0.000 0.436 0.000 /tmp/Bazel.runfiles_t7ewtxb0/runfiles/com_google_deepvariant/third_party/nucleus/util/genomics_math.py:196(normalize_log10_probs). 1 0.146 0.146 0.146 0.146 {method 'summary_counts' of 'deepvariant.python.allelecounter.AlleleCounter' objects}. 10001 0.142 0.000 0.428 0.000 /tmp/Bazel.runfiles_t7ewtxb0/runfiles/com_google_deepvariant/deepvariant/variant_caller.py:249(with_gq_and_likelihoods). 5151 0.134 0.000 0.610 0.000 /tmp/Bazel.runfiles_t7ewtxb0/runfiles/com_google_deepvariant/deepvariant/variant_caller.py:194(_calc_reference_confidence). 5268/5203 0.119 0.000 0.562 0.000 {built-in method builtins.__build_class__}. 1339 0.119 0.000 0.119 0.000 {method 'write' of 'third_party.nucleus.io.python.tfrecord_writer.TFRecordWriter' objects}. 1",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:10032,performance,optimiz,optimized,10032,"numpy.random.mtrand.RandomState' objects}. 5759 0.094 0.000 0.937 0.000 <frozen importlib._bootstrap_external>:1498(find_spec). 23926 0.089 0.000 0.109 0.000 /usr/lib/python3.8/enum.py:313(__call__). 3136 0.089 0.000 1.439 0.000 <frozen importlib._bootstrap_external>:1034(get_data). 473 0.083 0.000 0.120 0.000 <frozen importlib._bootstrap_external>:1317(_path_hooks). 1417/288 0.077 0.000 0.151 0.001 /usr/lib/python3.8/sre_parse.py:493(_parse). 4910 0.069 0.000 0.069 0.000 {method 'PythonNext' of 'third_party.nucleus.io.python.sam_reader.SamIterable' objects}. 3614/1 0.069 0.000 11.965 11.965 {built-in method builtins.exec}. 701 0.066 0.000 0.066 0.000 {built-in method posix.listdir}. 15152 0.063 0.000 0.268 0.000 /usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:69(_wrapreduction). 32755 0.062 0.000 0.135 0.000 <frozen importlib._bootstrap_external>:121(_path_join). 171/133 0.060 0.000 0.178 0.001 {built-in method _imp.exec_dynamic}. 1 0.059 0.059 0.059 0.059 {method 'calls_from_allele_counts' of 'deepvariant.python.variant_calling_multisample.VariantCaller' objects}. 23323 0.058 0.000 0.163 0.000 /usr/lib/python3.8/inspect.py:2489(__init__). 15563/15359 0.053 0.000 0.384 0.000 {built-in method numpy.core._multiarray_umath.implement_array_function}. 3601 0.053 0.000 0.344 0.000 /usr/lib/python3.8/inspect.py:1102(getfullargspec). 15152 0.051 0.000 0.319 0.000 /usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:2703(amax). 12155 0.049 0.000 0.060 0.000 {built-in method builtins.max}. 40200 0.049 0.000 0.049 0.000 {built-in method third_party.nucleus.util.python.utils.read_overlaps_region}. ```. In any case, the most optimized approach is some sort shared memory model with minimal overhead to keep the code nimble and easy to integrate or extend. The code itself is pleasantly easy to go through given the nice design, which makes it quite amendable to that. In the meantime, pass-by-reference probably could be an easier win. . Thank you,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:10071,performance,memor,memory,10071,"numpy.random.mtrand.RandomState' objects}. 5759 0.094 0.000 0.937 0.000 <frozen importlib._bootstrap_external>:1498(find_spec). 23926 0.089 0.000 0.109 0.000 /usr/lib/python3.8/enum.py:313(__call__). 3136 0.089 0.000 1.439 0.000 <frozen importlib._bootstrap_external>:1034(get_data). 473 0.083 0.000 0.120 0.000 <frozen importlib._bootstrap_external>:1317(_path_hooks). 1417/288 0.077 0.000 0.151 0.001 /usr/lib/python3.8/sre_parse.py:493(_parse). 4910 0.069 0.000 0.069 0.000 {method 'PythonNext' of 'third_party.nucleus.io.python.sam_reader.SamIterable' objects}. 3614/1 0.069 0.000 11.965 11.965 {built-in method builtins.exec}. 701 0.066 0.000 0.066 0.000 {built-in method posix.listdir}. 15152 0.063 0.000 0.268 0.000 /usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:69(_wrapreduction). 32755 0.062 0.000 0.135 0.000 <frozen importlib._bootstrap_external>:121(_path_join). 171/133 0.060 0.000 0.178 0.001 {built-in method _imp.exec_dynamic}. 1 0.059 0.059 0.059 0.059 {method 'calls_from_allele_counts' of 'deepvariant.python.variant_calling_multisample.VariantCaller' objects}. 23323 0.058 0.000 0.163 0.000 /usr/lib/python3.8/inspect.py:2489(__init__). 15563/15359 0.053 0.000 0.384 0.000 {built-in method numpy.core._multiarray_umath.implement_array_function}. 3601 0.053 0.000 0.344 0.000 /usr/lib/python3.8/inspect.py:1102(getfullargspec). 15152 0.051 0.000 0.319 0.000 /usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:2703(amax). 12155 0.049 0.000 0.060 0.000 {built-in method builtins.max}. 40200 0.049 0.000 0.049 0.000 {built-in method third_party.nucleus.util.python.utils.read_overlaps_region}. ```. In any case, the most optimized approach is some sort shared memory model with minimal overhead to keep the code nimble and easy to integrate or extend. The code itself is pleasantly easy to go through given the nice design, which makes it quite amendable to that. In the meantime, pass-by-reference probably could be an easier win. . Thank you,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:10097,performance,overhead,overhead,10097,"numpy.random.mtrand.RandomState' objects}. 5759 0.094 0.000 0.937 0.000 <frozen importlib._bootstrap_external>:1498(find_spec). 23926 0.089 0.000 0.109 0.000 /usr/lib/python3.8/enum.py:313(__call__). 3136 0.089 0.000 1.439 0.000 <frozen importlib._bootstrap_external>:1034(get_data). 473 0.083 0.000 0.120 0.000 <frozen importlib._bootstrap_external>:1317(_path_hooks). 1417/288 0.077 0.000 0.151 0.001 /usr/lib/python3.8/sre_parse.py:493(_parse). 4910 0.069 0.000 0.069 0.000 {method 'PythonNext' of 'third_party.nucleus.io.python.sam_reader.SamIterable' objects}. 3614/1 0.069 0.000 11.965 11.965 {built-in method builtins.exec}. 701 0.066 0.000 0.066 0.000 {built-in method posix.listdir}. 15152 0.063 0.000 0.268 0.000 /usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:69(_wrapreduction). 32755 0.062 0.000 0.135 0.000 <frozen importlib._bootstrap_external>:121(_path_join). 171/133 0.060 0.000 0.178 0.001 {built-in method _imp.exec_dynamic}. 1 0.059 0.059 0.059 0.059 {method 'calls_from_allele_counts' of 'deepvariant.python.variant_calling_multisample.VariantCaller' objects}. 23323 0.058 0.000 0.163 0.000 /usr/lib/python3.8/inspect.py:2489(__init__). 15563/15359 0.053 0.000 0.384 0.000 {built-in method numpy.core._multiarray_umath.implement_array_function}. 3601 0.053 0.000 0.344 0.000 /usr/lib/python3.8/inspect.py:1102(getfullargspec). 15152 0.051 0.000 0.319 0.000 /usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:2703(amax). 12155 0.049 0.000 0.060 0.000 {built-in method builtins.max}. 40200 0.049 0.000 0.049 0.000 {built-in method third_party.nucleus.util.python.utils.read_overlaps_region}. ```. In any case, the most optimized approach is some sort shared memory model with minimal overhead to keep the code nimble and easy to integrate or extend. The code itself is pleasantly easy to go through given the nice design, which makes it quite amendable to that. In the meantime, pass-by-reference probably could be an easier win. . Thank you,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:10142,reliability,integr,integrate,10142,"numpy.random.mtrand.RandomState' objects}. 5759 0.094 0.000 0.937 0.000 <frozen importlib._bootstrap_external>:1498(find_spec). 23926 0.089 0.000 0.109 0.000 /usr/lib/python3.8/enum.py:313(__call__). 3136 0.089 0.000 1.439 0.000 <frozen importlib._bootstrap_external>:1034(get_data). 473 0.083 0.000 0.120 0.000 <frozen importlib._bootstrap_external>:1317(_path_hooks). 1417/288 0.077 0.000 0.151 0.001 /usr/lib/python3.8/sre_parse.py:493(_parse). 4910 0.069 0.000 0.069 0.000 {method 'PythonNext' of 'third_party.nucleus.io.python.sam_reader.SamIterable' objects}. 3614/1 0.069 0.000 11.965 11.965 {built-in method builtins.exec}. 701 0.066 0.000 0.066 0.000 {built-in method posix.listdir}. 15152 0.063 0.000 0.268 0.000 /usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:69(_wrapreduction). 32755 0.062 0.000 0.135 0.000 <frozen importlib._bootstrap_external>:121(_path_join). 171/133 0.060 0.000 0.178 0.001 {built-in method _imp.exec_dynamic}. 1 0.059 0.059 0.059 0.059 {method 'calls_from_allele_counts' of 'deepvariant.python.variant_calling_multisample.VariantCaller' objects}. 23323 0.058 0.000 0.163 0.000 /usr/lib/python3.8/inspect.py:2489(__init__). 15563/15359 0.053 0.000 0.384 0.000 {built-in method numpy.core._multiarray_umath.implement_array_function}. 3601 0.053 0.000 0.344 0.000 /usr/lib/python3.8/inspect.py:1102(getfullargspec). 15152 0.051 0.000 0.319 0.000 /usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:2703(amax). 12155 0.049 0.000 0.060 0.000 {built-in method builtins.max}. 40200 0.049 0.000 0.049 0.000 {built-in method third_party.nucleus.util.python.utils.read_overlaps_region}. ```. In any case, the most optimized approach is some sort shared memory model with minimal overhead to keep the code nimble and easy to integrate or extend. The code itself is pleasantly easy to go through given the nice design, which makes it quite amendable to that. In the meantime, pass-by-reference probably could be an easier win. . Thank you,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:6154,security,model,model,6154," 0.113 0.000 0.118 0.000 /usr/local/lib/python3.8/dist-packages/gast/gast.py:15(create_node). 155316 0.108 0.000 0.183 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:6314(get_default_graph). 6015 0.105 0.000 0.105 0.000 {built-in method tensorflow.python.util._tf_stack.extract_stack_for_op}. 6015 0.104 0.000 0.149 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:1879(_NodeDef). ```. What I'm seeing above is something quite interesting. The execution seems heavy on [eager execution rather than graph execution](https://blog.tensorflow.org/2018/08/code-with-eager-execution-run-with-graphs.html), which could be the culprit. The thing is that there are some quick ways now to make it fast with almost no major code-rewrite, though there are other natural optimizations that are relatively obvious given the analysis. For the `make_examples` analysis, the task is much simpler to fix by localizing I/O through just a shared memory model and/or pass-by-reference approach, as illustrated by the call distribution (with an eye on the serialization/deserialization of ProtoBuf if that becomes more heavily used):. ```. Wed May 17 17:51:33 2023 make_examples_profile.txt. 2790682 function calls (2732833 primitive calls) in 11.860 seconds. Ordered by: internal time. List reduced from 11503 to 100 due to restriction <100>. ncalls tottime percall cumtime percall filename:lineno(function). 171/168 1.927 0.011 1.940 0.012 {built-in method _imp.create_dynamic}. 3159 0.930 0.000 0.930 0.000 {method 'read' of '_io.BufferedReader' objects}. 19018 0.694 0.000 0.694 0.000 {built-in method posix.stat}. 2 0.586 0.293 0.586 0.293 {method 'counts' of 'deepvariant.python.allelecounter.AlleleCounter' objects}. 3136 0.475 0.000 0.475 0.000 {built-in method marshal.loads}. 3136 0.422 0.000 0.422 0.000 {built-in method io.open_code}. 15152 0.183 0.000 0.183 0.000 {method 'reduce' of 'numpy.ufunc' objects}. 3136 0.164 0.000 2.265 0.001 <frozen ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:10078,security,model,model,10078,"numpy.random.mtrand.RandomState' objects}. 5759 0.094 0.000 0.937 0.000 <frozen importlib._bootstrap_external>:1498(find_spec). 23926 0.089 0.000 0.109 0.000 /usr/lib/python3.8/enum.py:313(__call__). 3136 0.089 0.000 1.439 0.000 <frozen importlib._bootstrap_external>:1034(get_data). 473 0.083 0.000 0.120 0.000 <frozen importlib._bootstrap_external>:1317(_path_hooks). 1417/288 0.077 0.000 0.151 0.001 /usr/lib/python3.8/sre_parse.py:493(_parse). 4910 0.069 0.000 0.069 0.000 {method 'PythonNext' of 'third_party.nucleus.io.python.sam_reader.SamIterable' objects}. 3614/1 0.069 0.000 11.965 11.965 {built-in method builtins.exec}. 701 0.066 0.000 0.066 0.000 {built-in method posix.listdir}. 15152 0.063 0.000 0.268 0.000 /usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:69(_wrapreduction). 32755 0.062 0.000 0.135 0.000 <frozen importlib._bootstrap_external>:121(_path_join). 171/133 0.060 0.000 0.178 0.001 {built-in method _imp.exec_dynamic}. 1 0.059 0.059 0.059 0.059 {method 'calls_from_allele_counts' of 'deepvariant.python.variant_calling_multisample.VariantCaller' objects}. 23323 0.058 0.000 0.163 0.000 /usr/lib/python3.8/inspect.py:2489(__init__). 15563/15359 0.053 0.000 0.384 0.000 {built-in method numpy.core._multiarray_umath.implement_array_function}. 3601 0.053 0.000 0.344 0.000 /usr/lib/python3.8/inspect.py:1102(getfullargspec). 15152 0.051 0.000 0.319 0.000 /usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:2703(amax). 12155 0.049 0.000 0.060 0.000 {built-in method builtins.max}. 40200 0.049 0.000 0.049 0.000 {built-in method third_party.nucleus.util.python.utils.read_overlaps_region}. ```. In any case, the most optimized approach is some sort shared memory model with minimal overhead to keep the code nimble and easy to integrate or extend. The code itself is pleasantly easy to go through given the nice design, which makes it quite amendable to that. In the meantime, pass-by-reference probably could be an easier win. . Thank you,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:10142,security,integr,integrate,10142,"numpy.random.mtrand.RandomState' objects}. 5759 0.094 0.000 0.937 0.000 <frozen importlib._bootstrap_external>:1498(find_spec). 23926 0.089 0.000 0.109 0.000 /usr/lib/python3.8/enum.py:313(__call__). 3136 0.089 0.000 1.439 0.000 <frozen importlib._bootstrap_external>:1034(get_data). 473 0.083 0.000 0.120 0.000 <frozen importlib._bootstrap_external>:1317(_path_hooks). 1417/288 0.077 0.000 0.151 0.001 /usr/lib/python3.8/sre_parse.py:493(_parse). 4910 0.069 0.000 0.069 0.000 {method 'PythonNext' of 'third_party.nucleus.io.python.sam_reader.SamIterable' objects}. 3614/1 0.069 0.000 11.965 11.965 {built-in method builtins.exec}. 701 0.066 0.000 0.066 0.000 {built-in method posix.listdir}. 15152 0.063 0.000 0.268 0.000 /usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:69(_wrapreduction). 32755 0.062 0.000 0.135 0.000 <frozen importlib._bootstrap_external>:121(_path_join). 171/133 0.060 0.000 0.178 0.001 {built-in method _imp.exec_dynamic}. 1 0.059 0.059 0.059 0.059 {method 'calls_from_allele_counts' of 'deepvariant.python.variant_calling_multisample.VariantCaller' objects}. 23323 0.058 0.000 0.163 0.000 /usr/lib/python3.8/inspect.py:2489(__init__). 15563/15359 0.053 0.000 0.384 0.000 {built-in method numpy.core._multiarray_umath.implement_array_function}. 3601 0.053 0.000 0.344 0.000 /usr/lib/python3.8/inspect.py:1102(getfullargspec). 15152 0.051 0.000 0.319 0.000 /usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:2703(amax). 12155 0.049 0.000 0.060 0.000 {built-in method builtins.max}. 40200 0.049 0.000 0.049 0.000 {built-in method third_party.nucleus.util.python.utils.read_overlaps_region}. ```. In any case, the most optimized approach is some sort shared memory model with minimal overhead to keep the code nimble and easy to integrate or extend. The code itself is pleasantly easy to go through given the nice design, which makes it quite amendable to that. In the meantime, pass-by-reference probably could be an easier win. . Thank you,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:2329,testability,context,context,2329,ots of calls might be. Those types of functions are quite heavy regarding stack setup and teardown -- with intermediate local memory allocation -- which could also be the culprit based on Steve's temporal data:. ```. Wed May 17 18:54:58 2023 call_variants_profile.txt. 12867185 function calls (11416794 primitive calls) in 61.905 seconds. Ordered by: internal time. List reduced from 3098 to 300 due to restriction <300>. ncalls tottime percall cumtime percall filename:lineno(function). 9 31.607 3.512 31.608 3.512 {built-in method tensorflow.python.client._pywrap_tf_session.TF_SessionRun_wrapper}. 2 14.137 7.069 14.137 7.069 {built-in method tensorflow.python.client._pywrap_tf_session.TF_CloseSession}. 434721 0.900 0.000 0.913 0.000 {built-in method builtins.hasattr}. 241294 0.868 0.000 3.111 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:5866(get_controller). 241296 0.462 0.000 0.462 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py:932(_mode). 413316 0.452 0.000 0.541 0.000 /usr/lib/python3.8/contextlib.py:82(__init__). 9 0.409 0.045 0.409 0.045 {built-in method tensorflow.python.client._pywrap_tf_session.ExtendSession}. 413315/164299 0.370 0.000 1.371 0.000 /usr/lib/python3.8/contextlib.py:117(__exit__). 825922/196 0.347 0.000 61.452 0.314 {built-in method builtins.next}. 113550 0.334 0.000 0.646 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:5887(_get_outer_context_and_inner_device_stack). 111268 0.304 0.000 4.646 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:6030(executing_eagerly_outside_functions). 1379 0.300 0.000 0.300 0.000 {method 'copy' of 'dict' objects}. 257584 0.293 0.000 0.293 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py:950(executing_eagerly). 413316/164111 0.253 0.000 2.725 0.000 /usr/lib/python3.8/contextlib.py:108(__enter__). 659/110 0.247 0.000 0.248 0.002 /usr/local/lib/python3.8/dist-p,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:2402,testability,context,contextlib,2402,stack setup and teardown -- with intermediate local memory allocation -- which could also be the culprit based on Steve's temporal data:. ```. Wed May 17 18:54:58 2023 call_variants_profile.txt. 12867185 function calls (11416794 primitive calls) in 61.905 seconds. Ordered by: internal time. List reduced from 3098 to 300 due to restriction <300>. ncalls tottime percall cumtime percall filename:lineno(function). 9 31.607 3.512 31.608 3.512 {built-in method tensorflow.python.client._pywrap_tf_session.TF_SessionRun_wrapper}. 2 14.137 7.069 14.137 7.069 {built-in method tensorflow.python.client._pywrap_tf_session.TF_CloseSession}. 434721 0.900 0.000 0.913 0.000 {built-in method builtins.hasattr}. 241294 0.868 0.000 3.111 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:5866(get_controller). 241296 0.462 0.000 0.462 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py:932(_mode). 413316 0.452 0.000 0.541 0.000 /usr/lib/python3.8/contextlib.py:82(__init__). 9 0.409 0.045 0.409 0.045 {built-in method tensorflow.python.client._pywrap_tf_session.ExtendSession}. 413315/164299 0.370 0.000 1.371 0.000 /usr/lib/python3.8/contextlib.py:117(__exit__). 825922/196 0.347 0.000 61.452 0.314 {built-in method builtins.next}. 113550 0.334 0.000 0.646 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:5887(_get_outer_context_and_inner_device_stack). 111268 0.304 0.000 4.646 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:6030(executing_eagerly_outside_functions). 1379 0.300 0.000 0.300 0.000 {method 'copy' of 'dict' objects}. 257584 0.293 0.000 0.293 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py:950(executing_eagerly). 413316/164111 0.253 0.000 2.725 0.000 /usr/lib/python3.8/contextlib.py:108(__enter__). 659/110 0.247 0.000 0.248 0.002 /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/utils/version_utils.py:97(swap_class). 413,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:2590,testability,context,contextlib,2590,e.txt. 12867185 function calls (11416794 primitive calls) in 61.905 seconds. Ordered by: internal time. List reduced from 3098 to 300 due to restriction <300>. ncalls tottime percall cumtime percall filename:lineno(function). 9 31.607 3.512 31.608 3.512 {built-in method tensorflow.python.client._pywrap_tf_session.TF_SessionRun_wrapper}. 2 14.137 7.069 14.137 7.069 {built-in method tensorflow.python.client._pywrap_tf_session.TF_CloseSession}. 434721 0.900 0.000 0.913 0.000 {built-in method builtins.hasattr}. 241294 0.868 0.000 3.111 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:5866(get_controller). 241296 0.462 0.000 0.462 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py:932(_mode). 413316 0.452 0.000 0.541 0.000 /usr/lib/python3.8/contextlib.py:82(__init__). 9 0.409 0.045 0.409 0.045 {built-in method tensorflow.python.client._pywrap_tf_session.ExtendSession}. 413315/164299 0.370 0.000 1.371 0.000 /usr/lib/python3.8/contextlib.py:117(__exit__). 825922/196 0.347 0.000 61.452 0.314 {built-in method builtins.next}. 113550 0.334 0.000 0.646 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:5887(_get_outer_context_and_inner_device_stack). 111268 0.304 0.000 4.646 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:6030(executing_eagerly_outside_functions). 1379 0.300 0.000 0.300 0.000 {method 'copy' of 'dict' objects}. 257584 0.293 0.000 0.293 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py:950(executing_eagerly). 413316/164111 0.253 0.000 2.725 0.000 /usr/lib/python3.8/contextlib.py:108(__enter__). 659/110 0.247 0.000 0.248 0.002 /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/utils/version_utils.py:97(swap_class). 413316 0.239 0.000 0.781 0.000 /usr/lib/python3.8/contextlib.py:238(helper). 3367 0.228 0.000 0.228 0.000 {built-in method posix.stat}. 6015 0.223 0.000 0.223 0.000 {built-in method tensorflo,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:3148,testability,context,context,3148,b/python3.8/dist-packages/tensorflow/python/framework/ops.py:5866(get_controller). 241296 0.462 0.000 0.462 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py:932(_mode). 413316 0.452 0.000 0.541 0.000 /usr/lib/python3.8/contextlib.py:82(__init__). 9 0.409 0.045 0.409 0.045 {built-in method tensorflow.python.client._pywrap_tf_session.ExtendSession}. 413315/164299 0.370 0.000 1.371 0.000 /usr/lib/python3.8/contextlib.py:117(__exit__). 825922/196 0.347 0.000 61.452 0.314 {built-in method builtins.next}. 113550 0.334 0.000 0.646 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:5887(_get_outer_context_and_inner_device_stack). 111268 0.304 0.000 4.646 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:6030(executing_eagerly_outside_functions). 1379 0.300 0.000 0.300 0.000 {method 'copy' of 'dict' objects}. 257584 0.293 0.000 0.293 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py:950(executing_eagerly). 413316/164111 0.253 0.000 2.725 0.000 /usr/lib/python3.8/contextlib.py:108(__enter__). 659/110 0.247 0.000 0.248 0.002 /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/utils/version_utils.py:97(swap_class). 413316 0.239 0.000 0.781 0.000 /usr/lib/python3.8/contextlib.py:238(helper). 3367 0.228 0.000 0.228 0.000 {built-in method posix.stat}. 6015 0.223 0.000 0.223 0.000 {built-in method tensorflow.python.client._pywrap_tf_session.TF_FinishOperation}. 241296 0.210 0.000 0.240 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:5673(get_controller). 256036 0.201 0.000 0.519 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py:2149(executing_eagerly). 31845 0.189 0.000 0.189 0.000 {built-in method posix.lstat}. 5154/4773 0.189 0.000 0.751 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/op_def_library.py:410(_ExtractInputsAndAttrs). 5154/4773 0.187 0.000 4.064 0.001 /,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:3240,testability,context,contextlib,3240,62 0.000 0.462 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py:932(_mode). 413316 0.452 0.000 0.541 0.000 /usr/lib/python3.8/contextlib.py:82(__init__). 9 0.409 0.045 0.409 0.045 {built-in method tensorflow.python.client._pywrap_tf_session.ExtendSession}. 413315/164299 0.370 0.000 1.371 0.000 /usr/lib/python3.8/contextlib.py:117(__exit__). 825922/196 0.347 0.000 61.452 0.314 {built-in method builtins.next}. 113550 0.334 0.000 0.646 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:5887(_get_outer_context_and_inner_device_stack). 111268 0.304 0.000 4.646 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:6030(executing_eagerly_outside_functions). 1379 0.300 0.000 0.300 0.000 {method 'copy' of 'dict' objects}. 257584 0.293 0.000 0.293 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py:950(executing_eagerly). 413316/164111 0.253 0.000 2.725 0.000 /usr/lib/python3.8/contextlib.py:108(__enter__). 659/110 0.247 0.000 0.248 0.002 /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/utils/version_utils.py:97(swap_class). 413316 0.239 0.000 0.781 0.000 /usr/lib/python3.8/contextlib.py:238(helper). 3367 0.228 0.000 0.228 0.000 {built-in method posix.stat}. 6015 0.223 0.000 0.223 0.000 {built-in method tensorflow.python.client._pywrap_tf_session.TF_FinishOperation}. 241296 0.210 0.000 0.240 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:5673(get_controller). 256036 0.201 0.000 0.519 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py:2149(executing_eagerly). 31845 0.189 0.000 0.189 0.000 {built-in method posix.lstat}. 5154/4773 0.189 0.000 0.751 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/op_def_library.py:410(_ExtractInputsAndAttrs). 5154/4773 0.187 0.000 4.064 0.001 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/op_def_library.py:751(_appl,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:3454,testability,context,contextlib,3454,ilt-in method tensorflow.python.client._pywrap_tf_session.ExtendSession}. 413315/164299 0.370 0.000 1.371 0.000 /usr/lib/python3.8/contextlib.py:117(__exit__). 825922/196 0.347 0.000 61.452 0.314 {built-in method builtins.next}. 113550 0.334 0.000 0.646 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:5887(_get_outer_context_and_inner_device_stack). 111268 0.304 0.000 4.646 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:6030(executing_eagerly_outside_functions). 1379 0.300 0.000 0.300 0.000 {method 'copy' of 'dict' objects}. 257584 0.293 0.000 0.293 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py:950(executing_eagerly). 413316/164111 0.253 0.000 2.725 0.000 /usr/lib/python3.8/contextlib.py:108(__enter__). 659/110 0.247 0.000 0.248 0.002 /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/utils/version_utils.py:97(swap_class). 413316 0.239 0.000 0.781 0.000 /usr/lib/python3.8/contextlib.py:238(helper). 3367 0.228 0.000 0.228 0.000 {built-in method posix.stat}. 6015 0.223 0.000 0.223 0.000 {built-in method tensorflow.python.client._pywrap_tf_session.TF_FinishOperation}. 241296 0.210 0.000 0.240 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:5673(get_controller). 256036 0.201 0.000 0.519 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py:2149(executing_eagerly). 31845 0.189 0.000 0.189 0.000 {built-in method posix.lstat}. 5154/4773 0.189 0.000 0.751 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/op_def_library.py:410(_ExtractInputsAndAttrs). 5154/4773 0.187 0.000 4.064 0.001 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/op_def_library.py:751(_apply_op_helper). 6015 0.167 0.000 0.935 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:1906(_create_c_op). 627432/627423 0.165 0.000 0.182 0.000 {built-in method builtins.getattr}. 709,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:3872,testability,context,context,3872,r/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:6030(executing_eagerly_outside_functions). 1379 0.300 0.000 0.300 0.000 {method 'copy' of 'dict' objects}. 257584 0.293 0.000 0.293 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py:950(executing_eagerly). 413316/164111 0.253 0.000 2.725 0.000 /usr/lib/python3.8/contextlib.py:108(__enter__). 659/110 0.247 0.000 0.248 0.002 /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/utils/version_utils.py:97(swap_class). 413316 0.239 0.000 0.781 0.000 /usr/lib/python3.8/contextlib.py:238(helper). 3367 0.228 0.000 0.228 0.000 {built-in method posix.stat}. 6015 0.223 0.000 0.223 0.000 {built-in method tensorflow.python.client._pywrap_tf_session.TF_FinishOperation}. 241296 0.210 0.000 0.240 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:5673(get_controller). 256036 0.201 0.000 0.519 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py:2149(executing_eagerly). 31845 0.189 0.000 0.189 0.000 {built-in method posix.lstat}. 5154/4773 0.189 0.000 0.751 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/op_def_library.py:410(_ExtractInputsAndAttrs). 5154/4773 0.187 0.000 4.064 0.001 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/op_def_library.py:751(_apply_op_helper). 6015 0.167 0.000 0.935 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:1906(_create_c_op). 627432/627423 0.165 0.000 0.182 0.000 {built-in method builtins.getattr}. 709467/709457 0.165 0.000 0.232 0.000 {built-in method builtins.isinstance}. 564/94 0.149 0.000 0.149 0.002 /usr/local/lib/python3.8/dist-packages/keras/utils/version_utils.py:89(swap_class). 6024 0.148 0.000 0.161 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/util/lock_util.py:105(_another_group_active). 120647 0.140 0.000 0.266 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/co,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:4874,testability,context,context,4874,"t.py:2149(executing_eagerly). 31845 0.189 0.000 0.189 0.000 {built-in method posix.lstat}. 5154/4773 0.189 0.000 0.751 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/op_def_library.py:410(_ExtractInputsAndAttrs). 5154/4773 0.187 0.000 4.064 0.001 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/op_def_library.py:751(_apply_op_helper). 6015 0.167 0.000 0.935 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:1906(_create_c_op). 627432/627423 0.165 0.000 0.182 0.000 {built-in method builtins.getattr}. 709467/709457 0.165 0.000 0.232 0.000 {built-in method builtins.isinstance}. 564/94 0.149 0.000 0.149 0.002 /usr/local/lib/python3.8/dist-packages/keras/utils/version_utils.py:89(swap_class). 6024 0.148 0.000 0.161 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/util/lock_util.py:105(_another_group_active). 120647 0.140 0.000 0.266 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py:249(push). 259701 0.137 0.000 0.197 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:5351(_device_function_stack). 58954 0.117 0.000 0.200 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/dtypes.py:719(as_dtype). 6669 0.113 0.000 0.118 0.000 /usr/local/lib/python3.8/dist-packages/gast/gast.py:15(create_node). 155316 0.108 0.000 0.183 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:6314(get_default_graph). 6015 0.105 0.000 0.105 0.000 {built-in method tensorflow.python.util._tf_stack.extract_stack_for_op}. 6015 0.104 0.000 0.149 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:1879(_NodeDef). ```. What I'm seeing above is something quite interesting. The execution seems heavy on [eager execution rather than graph execution](https://blog.tensorflow.org/2018/08/code-with-eager-execution-run-with-graphs.html), which could be the culprit. The thing is that there are s",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:6092,testability,simpl,simpler,6092,"ges/tensorflow/python/framework/dtypes.py:719(as_dtype). 6669 0.113 0.000 0.118 0.000 /usr/local/lib/python3.8/dist-packages/gast/gast.py:15(create_node). 155316 0.108 0.000 0.183 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:6314(get_default_graph). 6015 0.105 0.000 0.105 0.000 {built-in method tensorflow.python.util._tf_stack.extract_stack_for_op}. 6015 0.104 0.000 0.149 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:1879(_NodeDef). ```. What I'm seeing above is something quite interesting. The execution seems heavy on [eager execution rather than graph execution](https://blog.tensorflow.org/2018/08/code-with-eager-execution-run-with-graphs.html), which could be the culprit. The thing is that there are some quick ways now to make it fast with almost no major code-rewrite, though there are other natural optimizations that are relatively obvious given the analysis. For the `make_examples` analysis, the task is much simpler to fix by localizing I/O through just a shared memory model and/or pass-by-reference approach, as illustrated by the call distribution (with an eye on the serialization/deserialization of ProtoBuf if that becomes more heavily used):. ```. Wed May 17 17:51:33 2023 make_examples_profile.txt. 2790682 function calls (2732833 primitive calls) in 11.860 seconds. Ordered by: internal time. List reduced from 11503 to 100 due to restriction <100>. ncalls tottime percall cumtime percall filename:lineno(function). 171/168 1.927 0.011 1.940 0.012 {built-in method _imp.create_dynamic}. 3159 0.930 0.000 0.930 0.000 {method 'read' of '_io.BufferedReader' objects}. 19018 0.694 0.000 0.694 0.000 {built-in method posix.stat}. 2 0.586 0.293 0.586 0.293 {method 'counts' of 'deepvariant.python.allelecounter.AlleleCounter' objects}. 3136 0.475 0.000 0.475 0.000 {built-in method marshal.loads}. 3136 0.422 0.000 0.422 0.000 {built-in method io.open_code}. 15152 0.183 0.000 0.183 0.000 {method 'reduce' of ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:10142,testability,integr,integrate,10142,"numpy.random.mtrand.RandomState' objects}. 5759 0.094 0.000 0.937 0.000 <frozen importlib._bootstrap_external>:1498(find_spec). 23926 0.089 0.000 0.109 0.000 /usr/lib/python3.8/enum.py:313(__call__). 3136 0.089 0.000 1.439 0.000 <frozen importlib._bootstrap_external>:1034(get_data). 473 0.083 0.000 0.120 0.000 <frozen importlib._bootstrap_external>:1317(_path_hooks). 1417/288 0.077 0.000 0.151 0.001 /usr/lib/python3.8/sre_parse.py:493(_parse). 4910 0.069 0.000 0.069 0.000 {method 'PythonNext' of 'third_party.nucleus.io.python.sam_reader.SamIterable' objects}. 3614/1 0.069 0.000 11.965 11.965 {built-in method builtins.exec}. 701 0.066 0.000 0.066 0.000 {built-in method posix.listdir}. 15152 0.063 0.000 0.268 0.000 /usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:69(_wrapreduction). 32755 0.062 0.000 0.135 0.000 <frozen importlib._bootstrap_external>:121(_path_join). 171/133 0.060 0.000 0.178 0.001 {built-in method _imp.exec_dynamic}. 1 0.059 0.059 0.059 0.059 {method 'calls_from_allele_counts' of 'deepvariant.python.variant_calling_multisample.VariantCaller' objects}. 23323 0.058 0.000 0.163 0.000 /usr/lib/python3.8/inspect.py:2489(__init__). 15563/15359 0.053 0.000 0.384 0.000 {built-in method numpy.core._multiarray_umath.implement_array_function}. 3601 0.053 0.000 0.344 0.000 /usr/lib/python3.8/inspect.py:1102(getfullargspec). 15152 0.051 0.000 0.319 0.000 /usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:2703(amax). 12155 0.049 0.000 0.060 0.000 {built-in method builtins.max}. 40200 0.049 0.000 0.049 0.000 {built-in method third_party.nucleus.util.python.utils.read_overlaps_region}. ```. In any case, the most optimized approach is some sort shared memory model with minimal overhead to keep the code nimble and easy to integrate or extend. The code itself is pleasantly easy to go through given the nice design, which makes it quite amendable to that. In the meantime, pass-by-reference probably could be an easier win. . Thank you,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:91,usability,minim,minimize,91,"Hi Andrew and Steve,. That's a good idea, and probably shifting to pass-by-reference might minimize the overhead between functions or even through minimizing the number of functions. A shared memory approach operated upon by the different functions could be ideal with maximizing performance, where even utilizing [NVidia's Unified Virtual Memory (UVM)](https://developer.nvidia.com/blog/maximizing-unified-memory-performance-cuda/) could help, but requires some care. Currently the `call_variants` memory profile looks something like this, with minimal changes across the running process:. ```. types | # objects | total size. ================================================================ | =========== | ============. str | 223354 | 35.79 MB. dict | 88941 | 25.94 MB. code | 50107 | 8.54 MB. type | 6121 | 5.65 MB. tuple | 63884 | 3.62 MB. list | 30942 | 3.19 MB. set | 2864 | 1.51 MB. weakref | 14251 | 1002.02 KB. abc.ABCMeta | 784 | 826.05 KB. cell | 20911 | 816.84 KB. int | 25259 | 697.77 KB. builtin_function_or_method | 8801 | 618.82 KB. google.protobuf.pyext.cpp_message.GeneratedProtocolMessageType | 679 | 594.12 KB. frozenset | 1862 | 541.02 KB. function (__init__) | 3439 | 456.74 KB. ```. Based on the above, that made me curious as pass-by-reference could show benefits, but wanted to see what the functions with lots of calls might be. Those types of functions are quite heavy regarding stack setup and teardown -- with intermediate local memory allocation -- which could also be the culprit based on Steve's temporal data:. ```. Wed May 17 18:54:58 2023 call_variants_profile.txt. 12867185 function calls (11416794 primitive calls) in 61.905 seconds. Ordered by: internal time. List reduced from 3098 to 300 due to restriction <300>. ncalls tottime percall cumtime percall filename:lineno(function). 9 31.607 3.512 31.608 3.512 {built-in method tensorflow.python.client._pywrap_tf_session.TF_SessionRun_wrapper}. 2 14.137 7.069 14.137 7.069 {built-in method tensorflow.python.cli",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:147,usability,minim,minimizing,147,"Hi Andrew and Steve,. That's a good idea, and probably shifting to pass-by-reference might minimize the overhead between functions or even through minimizing the number of functions. A shared memory approach operated upon by the different functions could be ideal with maximizing performance, where even utilizing [NVidia's Unified Virtual Memory (UVM)](https://developer.nvidia.com/blog/maximizing-unified-memory-performance-cuda/) could help, but requires some care. Currently the `call_variants` memory profile looks something like this, with minimal changes across the running process:. ```. types | # objects | total size. ================================================================ | =========== | ============. str | 223354 | 35.79 MB. dict | 88941 | 25.94 MB. code | 50107 | 8.54 MB. type | 6121 | 5.65 MB. tuple | 63884 | 3.62 MB. list | 30942 | 3.19 MB. set | 2864 | 1.51 MB. weakref | 14251 | 1002.02 KB. abc.ABCMeta | 784 | 826.05 KB. cell | 20911 | 816.84 KB. int | 25259 | 697.77 KB. builtin_function_or_method | 8801 | 618.82 KB. google.protobuf.pyext.cpp_message.GeneratedProtocolMessageType | 679 | 594.12 KB. frozenset | 1862 | 541.02 KB. function (__init__) | 3439 | 456.74 KB. ```. Based on the above, that made me curious as pass-by-reference could show benefits, but wanted to see what the functions with lots of calls might be. Those types of functions are quite heavy regarding stack setup and teardown -- with intermediate local memory allocation -- which could also be the culprit based on Steve's temporal data:. ```. Wed May 17 18:54:58 2023 call_variants_profile.txt. 12867185 function calls (11416794 primitive calls) in 61.905 seconds. Ordered by: internal time. List reduced from 3098 to 300 due to restriction <300>. ncalls tottime percall cumtime percall filename:lineno(function). 9 31.607 3.512 31.608 3.512 {built-in method tensorflow.python.client._pywrap_tf_session.TF_SessionRun_wrapper}. 2 14.137 7.069 14.137 7.069 {built-in method tensorflow.python.cli",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:192,usability,memor,memory,192,"Hi Andrew and Steve,. That's a good idea, and probably shifting to pass-by-reference might minimize the overhead between functions or even through minimizing the number of functions. A shared memory approach operated upon by the different functions could be ideal with maximizing performance, where even utilizing [NVidia's Unified Virtual Memory (UVM)](https://developer.nvidia.com/blog/maximizing-unified-memory-performance-cuda/) could help, but requires some care. Currently the `call_variants` memory profile looks something like this, with minimal changes across the running process:. ```. types | # objects | total size. ================================================================ | =========== | ============. str | 223354 | 35.79 MB. dict | 88941 | 25.94 MB. code | 50107 | 8.54 MB. type | 6121 | 5.65 MB. tuple | 63884 | 3.62 MB. list | 30942 | 3.19 MB. set | 2864 | 1.51 MB. weakref | 14251 | 1002.02 KB. abc.ABCMeta | 784 | 826.05 KB. cell | 20911 | 816.84 KB. int | 25259 | 697.77 KB. builtin_function_or_method | 8801 | 618.82 KB. google.protobuf.pyext.cpp_message.GeneratedProtocolMessageType | 679 | 594.12 KB. frozenset | 1862 | 541.02 KB. function (__init__) | 3439 | 456.74 KB. ```. Based on the above, that made me curious as pass-by-reference could show benefits, but wanted to see what the functions with lots of calls might be. Those types of functions are quite heavy regarding stack setup and teardown -- with intermediate local memory allocation -- which could also be the culprit based on Steve's temporal data:. ```. Wed May 17 18:54:58 2023 call_variants_profile.txt. 12867185 function calls (11416794 primitive calls) in 61.905 seconds. Ordered by: internal time. List reduced from 3098 to 300 due to restriction <300>. ncalls tottime percall cumtime percall filename:lineno(function). 9 31.607 3.512 31.608 3.512 {built-in method tensorflow.python.client._pywrap_tf_session.TF_SessionRun_wrapper}. 2 14.137 7.069 14.137 7.069 {built-in method tensorflow.python.cli",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:280,usability,perform,performance,280,"Hi Andrew and Steve,. That's a good idea, and probably shifting to pass-by-reference might minimize the overhead between functions or even through minimizing the number of functions. A shared memory approach operated upon by the different functions could be ideal with maximizing performance, where even utilizing [NVidia's Unified Virtual Memory (UVM)](https://developer.nvidia.com/blog/maximizing-unified-memory-performance-cuda/) could help, but requires some care. Currently the `call_variants` memory profile looks something like this, with minimal changes across the running process:. ```. types | # objects | total size. ================================================================ | =========== | ============. str | 223354 | 35.79 MB. dict | 88941 | 25.94 MB. code | 50107 | 8.54 MB. type | 6121 | 5.65 MB. tuple | 63884 | 3.62 MB. list | 30942 | 3.19 MB. set | 2864 | 1.51 MB. weakref | 14251 | 1002.02 KB. abc.ABCMeta | 784 | 826.05 KB. cell | 20911 | 816.84 KB. int | 25259 | 697.77 KB. builtin_function_or_method | 8801 | 618.82 KB. google.protobuf.pyext.cpp_message.GeneratedProtocolMessageType | 679 | 594.12 KB. frozenset | 1862 | 541.02 KB. function (__init__) | 3439 | 456.74 KB. ```. Based on the above, that made me curious as pass-by-reference could show benefits, but wanted to see what the functions with lots of calls might be. Those types of functions are quite heavy regarding stack setup and teardown -- with intermediate local memory allocation -- which could also be the culprit based on Steve's temporal data:. ```. Wed May 17 18:54:58 2023 call_variants_profile.txt. 12867185 function calls (11416794 primitive calls) in 61.905 seconds. Ordered by: internal time. List reduced from 3098 to 300 due to restriction <300>. ncalls tottime percall cumtime percall filename:lineno(function). 9 31.607 3.512 31.608 3.512 {built-in method tensorflow.python.client._pywrap_tf_session.TF_SessionRun_wrapper}. 2 14.137 7.069 14.137 7.069 {built-in method tensorflow.python.cli",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:340,usability,Memor,Memory,340,"Hi Andrew and Steve,. That's a good idea, and probably shifting to pass-by-reference might minimize the overhead between functions or even through minimizing the number of functions. A shared memory approach operated upon by the different functions could be ideal with maximizing performance, where even utilizing [NVidia's Unified Virtual Memory (UVM)](https://developer.nvidia.com/blog/maximizing-unified-memory-performance-cuda/) could help, but requires some care. Currently the `call_variants` memory profile looks something like this, with minimal changes across the running process:. ```. types | # objects | total size. ================================================================ | =========== | ============. str | 223354 | 35.79 MB. dict | 88941 | 25.94 MB. code | 50107 | 8.54 MB. type | 6121 | 5.65 MB. tuple | 63884 | 3.62 MB. list | 30942 | 3.19 MB. set | 2864 | 1.51 MB. weakref | 14251 | 1002.02 KB. abc.ABCMeta | 784 | 826.05 KB. cell | 20911 | 816.84 KB. int | 25259 | 697.77 KB. builtin_function_or_method | 8801 | 618.82 KB. google.protobuf.pyext.cpp_message.GeneratedProtocolMessageType | 679 | 594.12 KB. frozenset | 1862 | 541.02 KB. function (__init__) | 3439 | 456.74 KB. ```. Based on the above, that made me curious as pass-by-reference could show benefits, but wanted to see what the functions with lots of calls might be. Those types of functions are quite heavy regarding stack setup and teardown -- with intermediate local memory allocation -- which could also be the culprit based on Steve's temporal data:. ```. Wed May 17 18:54:58 2023 call_variants_profile.txt. 12867185 function calls (11416794 primitive calls) in 61.905 seconds. Ordered by: internal time. List reduced from 3098 to 300 due to restriction <300>. ncalls tottime percall cumtime percall filename:lineno(function). 9 31.607 3.512 31.608 3.512 {built-in method tensorflow.python.client._pywrap_tf_session.TF_SessionRun_wrapper}. 2 14.137 7.069 14.137 7.069 {built-in method tensorflow.python.cli",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:407,usability,memor,memory-performance-cuda,407,"Hi Andrew and Steve,. That's a good idea, and probably shifting to pass-by-reference might minimize the overhead between functions or even through minimizing the number of functions. A shared memory approach operated upon by the different functions could be ideal with maximizing performance, where even utilizing [NVidia's Unified Virtual Memory (UVM)](https://developer.nvidia.com/blog/maximizing-unified-memory-performance-cuda/) could help, but requires some care. Currently the `call_variants` memory profile looks something like this, with minimal changes across the running process:. ```. types | # objects | total size. ================================================================ | =========== | ============. str | 223354 | 35.79 MB. dict | 88941 | 25.94 MB. code | 50107 | 8.54 MB. type | 6121 | 5.65 MB. tuple | 63884 | 3.62 MB. list | 30942 | 3.19 MB. set | 2864 | 1.51 MB. weakref | 14251 | 1002.02 KB. abc.ABCMeta | 784 | 826.05 KB. cell | 20911 | 816.84 KB. int | 25259 | 697.77 KB. builtin_function_or_method | 8801 | 618.82 KB. google.protobuf.pyext.cpp_message.GeneratedProtocolMessageType | 679 | 594.12 KB. frozenset | 1862 | 541.02 KB. function (__init__) | 3439 | 456.74 KB. ```. Based on the above, that made me curious as pass-by-reference could show benefits, but wanted to see what the functions with lots of calls might be. Those types of functions are quite heavy regarding stack setup and teardown -- with intermediate local memory allocation -- which could also be the culprit based on Steve's temporal data:. ```. Wed May 17 18:54:58 2023 call_variants_profile.txt. 12867185 function calls (11416794 primitive calls) in 61.905 seconds. Ordered by: internal time. List reduced from 3098 to 300 due to restriction <300>. ncalls tottime percall cumtime percall filename:lineno(function). 9 31.607 3.512 31.608 3.512 {built-in method tensorflow.python.client._pywrap_tf_session.TF_SessionRun_wrapper}. 2 14.137 7.069 14.137 7.069 {built-in method tensorflow.python.cli",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:439,usability,help,help,439,"Hi Andrew and Steve,. That's a good idea, and probably shifting to pass-by-reference might minimize the overhead between functions or even through minimizing the number of functions. A shared memory approach operated upon by the different functions could be ideal with maximizing performance, where even utilizing [NVidia's Unified Virtual Memory (UVM)](https://developer.nvidia.com/blog/maximizing-unified-memory-performance-cuda/) could help, but requires some care. Currently the `call_variants` memory profile looks something like this, with minimal changes across the running process:. ```. types | # objects | total size. ================================================================ | =========== | ============. str | 223354 | 35.79 MB. dict | 88941 | 25.94 MB. code | 50107 | 8.54 MB. type | 6121 | 5.65 MB. tuple | 63884 | 3.62 MB. list | 30942 | 3.19 MB. set | 2864 | 1.51 MB. weakref | 14251 | 1002.02 KB. abc.ABCMeta | 784 | 826.05 KB. cell | 20911 | 816.84 KB. int | 25259 | 697.77 KB. builtin_function_or_method | 8801 | 618.82 KB. google.protobuf.pyext.cpp_message.GeneratedProtocolMessageType | 679 | 594.12 KB. frozenset | 1862 | 541.02 KB. function (__init__) | 3439 | 456.74 KB. ```. Based on the above, that made me curious as pass-by-reference could show benefits, but wanted to see what the functions with lots of calls might be. Those types of functions are quite heavy regarding stack setup and teardown -- with intermediate local memory allocation -- which could also be the culprit based on Steve's temporal data:. ```. Wed May 17 18:54:58 2023 call_variants_profile.txt. 12867185 function calls (11416794 primitive calls) in 61.905 seconds. Ordered by: internal time. List reduced from 3098 to 300 due to restriction <300>. ncalls tottime percall cumtime percall filename:lineno(function). 9 31.607 3.512 31.608 3.512 {built-in method tensorflow.python.client._pywrap_tf_session.TF_SessionRun_wrapper}. 2 14.137 7.069 14.137 7.069 {built-in method tensorflow.python.cli",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:499,usability,memor,memory,499,"Hi Andrew and Steve,. That's a good idea, and probably shifting to pass-by-reference might minimize the overhead between functions or even through minimizing the number of functions. A shared memory approach operated upon by the different functions could be ideal with maximizing performance, where even utilizing [NVidia's Unified Virtual Memory (UVM)](https://developer.nvidia.com/blog/maximizing-unified-memory-performance-cuda/) could help, but requires some care. Currently the `call_variants` memory profile looks something like this, with minimal changes across the running process:. ```. types | # objects | total size. ================================================================ | =========== | ============. str | 223354 | 35.79 MB. dict | 88941 | 25.94 MB. code | 50107 | 8.54 MB. type | 6121 | 5.65 MB. tuple | 63884 | 3.62 MB. list | 30942 | 3.19 MB. set | 2864 | 1.51 MB. weakref | 14251 | 1002.02 KB. abc.ABCMeta | 784 | 826.05 KB. cell | 20911 | 816.84 KB. int | 25259 | 697.77 KB. builtin_function_or_method | 8801 | 618.82 KB. google.protobuf.pyext.cpp_message.GeneratedProtocolMessageType | 679 | 594.12 KB. frozenset | 1862 | 541.02 KB. function (__init__) | 3439 | 456.74 KB. ```. Based on the above, that made me curious as pass-by-reference could show benefits, but wanted to see what the functions with lots of calls might be. Those types of functions are quite heavy regarding stack setup and teardown -- with intermediate local memory allocation -- which could also be the culprit based on Steve's temporal data:. ```. Wed May 17 18:54:58 2023 call_variants_profile.txt. 12867185 function calls (11416794 primitive calls) in 61.905 seconds. Ordered by: internal time. List reduced from 3098 to 300 due to restriction <300>. ncalls tottime percall cumtime percall filename:lineno(function). 9 31.607 3.512 31.608 3.512 {built-in method tensorflow.python.client._pywrap_tf_session.TF_SessionRun_wrapper}. 2 14.137 7.069 14.137 7.069 {built-in method tensorflow.python.cli",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:546,usability,minim,minimal,546,"Hi Andrew and Steve,. That's a good idea, and probably shifting to pass-by-reference might minimize the overhead between functions or even through minimizing the number of functions. A shared memory approach operated upon by the different functions could be ideal with maximizing performance, where even utilizing [NVidia's Unified Virtual Memory (UVM)](https://developer.nvidia.com/blog/maximizing-unified-memory-performance-cuda/) could help, but requires some care. Currently the `call_variants` memory profile looks something like this, with minimal changes across the running process:. ```. types | # objects | total size. ================================================================ | =========== | ============. str | 223354 | 35.79 MB. dict | 88941 | 25.94 MB. code | 50107 | 8.54 MB. type | 6121 | 5.65 MB. tuple | 63884 | 3.62 MB. list | 30942 | 3.19 MB. set | 2864 | 1.51 MB. weakref | 14251 | 1002.02 KB. abc.ABCMeta | 784 | 826.05 KB. cell | 20911 | 816.84 KB. int | 25259 | 697.77 KB. builtin_function_or_method | 8801 | 618.82 KB. google.protobuf.pyext.cpp_message.GeneratedProtocolMessageType | 679 | 594.12 KB. frozenset | 1862 | 541.02 KB. function (__init__) | 3439 | 456.74 KB. ```. Based on the above, that made me curious as pass-by-reference could show benefits, but wanted to see what the functions with lots of calls might be. Those types of functions are quite heavy regarding stack setup and teardown -- with intermediate local memory allocation -- which could also be the culprit based on Steve's temporal data:. ```. Wed May 17 18:54:58 2023 call_variants_profile.txt. 12867185 function calls (11416794 primitive calls) in 61.905 seconds. Ordered by: internal time. List reduced from 3098 to 300 due to restriction <300>. ncalls tottime percall cumtime percall filename:lineno(function). 9 31.607 3.512 31.608 3.512 {built-in method tensorflow.python.client._pywrap_tf_session.TF_SessionRun_wrapper}. 2 14.137 7.069 14.137 7.069 {built-in method tensorflow.python.cli",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:1459,usability,memor,memory,1459," care. Currently the `call_variants` memory profile looks something like this, with minimal changes across the running process:. ```. types | # objects | total size. ================================================================ | =========== | ============. str | 223354 | 35.79 MB. dict | 88941 | 25.94 MB. code | 50107 | 8.54 MB. type | 6121 | 5.65 MB. tuple | 63884 | 3.62 MB. list | 30942 | 3.19 MB. set | 2864 | 1.51 MB. weakref | 14251 | 1002.02 KB. abc.ABCMeta | 784 | 826.05 KB. cell | 20911 | 816.84 KB. int | 25259 | 697.77 KB. builtin_function_or_method | 8801 | 618.82 KB. google.protobuf.pyext.cpp_message.GeneratedProtocolMessageType | 679 | 594.12 KB. frozenset | 1862 | 541.02 KB. function (__init__) | 3439 | 456.74 KB. ```. Based on the above, that made me curious as pass-by-reference could show benefits, but wanted to see what the functions with lots of calls might be. Those types of functions are quite heavy regarding stack setup and teardown -- with intermediate local memory allocation -- which could also be the culprit based on Steve's temporal data:. ```. Wed May 17 18:54:58 2023 call_variants_profile.txt. 12867185 function calls (11416794 primitive calls) in 61.905 seconds. Ordered by: internal time. List reduced from 3098 to 300 due to restriction <300>. ncalls tottime percall cumtime percall filename:lineno(function). 9 31.607 3.512 31.608 3.512 {built-in method tensorflow.python.client._pywrap_tf_session.TF_SessionRun_wrapper}. 2 14.137 7.069 14.137 7.069 {built-in method tensorflow.python.client._pywrap_tf_session.TF_CloseSession}. 434721 0.900 0.000 0.913 0.000 {built-in method builtins.hasattr}. 241294 0.868 0.000 3.111 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:5866(get_controller). 241296 0.462 0.000 0.462 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py:932(_mode). 413316 0.452 0.000 0.541 0.000 /usr/lib/python3.8/contextlib.py:82(__init__). 9 0.409 0.045 0.409 0.045 {built",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:3472,usability,help,helper,3472,nsorflow.python.client._pywrap_tf_session.ExtendSession}. 413315/164299 0.370 0.000 1.371 0.000 /usr/lib/python3.8/contextlib.py:117(__exit__). 825922/196 0.347 0.000 61.452 0.314 {built-in method builtins.next}. 113550 0.334 0.000 0.646 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:5887(_get_outer_context_and_inner_device_stack). 111268 0.304 0.000 4.646 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:6030(executing_eagerly_outside_functions). 1379 0.300 0.000 0.300 0.000 {method 'copy' of 'dict' objects}. 257584 0.293 0.000 0.293 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py:950(executing_eagerly). 413316/164111 0.253 0.000 2.725 0.000 /usr/lib/python3.8/contextlib.py:108(__enter__). 659/110 0.247 0.000 0.248 0.002 /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/utils/version_utils.py:97(swap_class). 413316 0.239 0.000 0.781 0.000 /usr/lib/python3.8/contextlib.py:238(helper). 3367 0.228 0.000 0.228 0.000 {built-in method posix.stat}. 6015 0.223 0.000 0.223 0.000 {built-in method tensorflow.python.client._pywrap_tf_session.TF_FinishOperation}. 241296 0.210 0.000 0.240 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:5673(get_controller). 256036 0.201 0.000 0.519 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py:2149(executing_eagerly). 31845 0.189 0.000 0.189 0.000 {built-in method posix.lstat}. 5154/4773 0.189 0.000 0.751 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/op_def_library.py:410(_ExtractInputsAndAttrs). 5154/4773 0.187 0.000 4.064 0.001 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/op_def_library.py:751(_apply_op_helper). 6015 0.167 0.000 0.935 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:1906(_create_c_op). 627432/627423 0.165 0.000 0.182 0.000 {built-in method builtins.getattr}. 709467/709457 0.165,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:6092,usability,simpl,simpler,6092,"ges/tensorflow/python/framework/dtypes.py:719(as_dtype). 6669 0.113 0.000 0.118 0.000 /usr/local/lib/python3.8/dist-packages/gast/gast.py:15(create_node). 155316 0.108 0.000 0.183 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:6314(get_default_graph). 6015 0.105 0.000 0.105 0.000 {built-in method tensorflow.python.util._tf_stack.extract_stack_for_op}. 6015 0.104 0.000 0.149 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:1879(_NodeDef). ```. What I'm seeing above is something quite interesting. The execution seems heavy on [eager execution rather than graph execution](https://blog.tensorflow.org/2018/08/code-with-eager-execution-run-with-graphs.html), which could be the culprit. The thing is that there are some quick ways now to make it fast with almost no major code-rewrite, though there are other natural optimizations that are relatively obvious given the analysis. For the `make_examples` analysis, the task is much simpler to fix by localizing I/O through just a shared memory model and/or pass-by-reference approach, as illustrated by the call distribution (with an eye on the serialization/deserialization of ProtoBuf if that becomes more heavily used):. ```. Wed May 17 17:51:33 2023 make_examples_profile.txt. 2790682 function calls (2732833 primitive calls) in 11.860 seconds. Ordered by: internal time. List reduced from 11503 to 100 due to restriction <100>. ncalls tottime percall cumtime percall filename:lineno(function). 171/168 1.927 0.011 1.940 0.012 {built-in method _imp.create_dynamic}. 3159 0.930 0.000 0.930 0.000 {method 'read' of '_io.BufferedReader' objects}. 19018 0.694 0.000 0.694 0.000 {built-in method posix.stat}. 2 0.586 0.293 0.586 0.293 {method 'counts' of 'deepvariant.python.allelecounter.AlleleCounter' objects}. 3136 0.475 0.000 0.475 0.000 {built-in method marshal.loads}. 3136 0.422 0.000 0.422 0.000 {built-in method io.open_code}. 15152 0.183 0.000 0.183 0.000 {method 'reduce' of ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:6147,usability,memor,memory,6147,"). 6669 0.113 0.000 0.118 0.000 /usr/local/lib/python3.8/dist-packages/gast/gast.py:15(create_node). 155316 0.108 0.000 0.183 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:6314(get_default_graph). 6015 0.105 0.000 0.105 0.000 {built-in method tensorflow.python.util._tf_stack.extract_stack_for_op}. 6015 0.104 0.000 0.149 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:1879(_NodeDef). ```. What I'm seeing above is something quite interesting. The execution seems heavy on [eager execution rather than graph execution](https://blog.tensorflow.org/2018/08/code-with-eager-execution-run-with-graphs.html), which could be the culprit. The thing is that there are some quick ways now to make it fast with almost no major code-rewrite, though there are other natural optimizations that are relatively obvious given the analysis. For the `make_examples` analysis, the task is much simpler to fix by localizing I/O through just a shared memory model and/or pass-by-reference approach, as illustrated by the call distribution (with an eye on the serialization/deserialization of ProtoBuf if that becomes more heavily used):. ```. Wed May 17 17:51:33 2023 make_examples_profile.txt. 2790682 function calls (2732833 primitive calls) in 11.860 seconds. Ordered by: internal time. List reduced from 11503 to 100 due to restriction <100>. ncalls tottime percall cumtime percall filename:lineno(function). 171/168 1.927 0.011 1.940 0.012 {built-in method _imp.create_dynamic}. 3159 0.930 0.000 0.930 0.000 {method 'read' of '_io.BufferedReader' objects}. 19018 0.694 0.000 0.694 0.000 {built-in method posix.stat}. 2 0.586 0.293 0.586 0.293 {method 'counts' of 'deepvariant.python.allelecounter.AlleleCounter' objects}. 3136 0.475 0.000 0.475 0.000 {built-in method marshal.loads}. 3136 0.422 0.000 0.422 0.000 {built-in method io.open_code}. 15152 0.183 0.000 0.183 0.000 {method 'reduce' of 'numpy.ufunc' objects}. 3136 0.164 0.000 2.265 0.001 <",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:10071,usability,memor,memory,10071,"numpy.random.mtrand.RandomState' objects}. 5759 0.094 0.000 0.937 0.000 <frozen importlib._bootstrap_external>:1498(find_spec). 23926 0.089 0.000 0.109 0.000 /usr/lib/python3.8/enum.py:313(__call__). 3136 0.089 0.000 1.439 0.000 <frozen importlib._bootstrap_external>:1034(get_data). 473 0.083 0.000 0.120 0.000 <frozen importlib._bootstrap_external>:1317(_path_hooks). 1417/288 0.077 0.000 0.151 0.001 /usr/lib/python3.8/sre_parse.py:493(_parse). 4910 0.069 0.000 0.069 0.000 {method 'PythonNext' of 'third_party.nucleus.io.python.sam_reader.SamIterable' objects}. 3614/1 0.069 0.000 11.965 11.965 {built-in method builtins.exec}. 701 0.066 0.000 0.066 0.000 {built-in method posix.listdir}. 15152 0.063 0.000 0.268 0.000 /usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:69(_wrapreduction). 32755 0.062 0.000 0.135 0.000 <frozen importlib._bootstrap_external>:121(_path_join). 171/133 0.060 0.000 0.178 0.001 {built-in method _imp.exec_dynamic}. 1 0.059 0.059 0.059 0.059 {method 'calls_from_allele_counts' of 'deepvariant.python.variant_calling_multisample.VariantCaller' objects}. 23323 0.058 0.000 0.163 0.000 /usr/lib/python3.8/inspect.py:2489(__init__). 15563/15359 0.053 0.000 0.384 0.000 {built-in method numpy.core._multiarray_umath.implement_array_function}. 3601 0.053 0.000 0.344 0.000 /usr/lib/python3.8/inspect.py:1102(getfullargspec). 15152 0.051 0.000 0.319 0.000 /usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:2703(amax). 12155 0.049 0.000 0.060 0.000 {built-in method builtins.max}. 40200 0.049 0.000 0.049 0.000 {built-in method third_party.nucleus.util.python.utils.read_overlaps_region}. ```. In any case, the most optimized approach is some sort shared memory model with minimal overhead to keep the code nimble and easy to integrate or extend. The code itself is pleasantly easy to go through given the nice design, which makes it quite amendable to that. In the meantime, pass-by-reference probably could be an easier win. . Thank you,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:10089,usability,minim,minimal,10089,"numpy.random.mtrand.RandomState' objects}. 5759 0.094 0.000 0.937 0.000 <frozen importlib._bootstrap_external>:1498(find_spec). 23926 0.089 0.000 0.109 0.000 /usr/lib/python3.8/enum.py:313(__call__). 3136 0.089 0.000 1.439 0.000 <frozen importlib._bootstrap_external>:1034(get_data). 473 0.083 0.000 0.120 0.000 <frozen importlib._bootstrap_external>:1317(_path_hooks). 1417/288 0.077 0.000 0.151 0.001 /usr/lib/python3.8/sre_parse.py:493(_parse). 4910 0.069 0.000 0.069 0.000 {method 'PythonNext' of 'third_party.nucleus.io.python.sam_reader.SamIterable' objects}. 3614/1 0.069 0.000 11.965 11.965 {built-in method builtins.exec}. 701 0.066 0.000 0.066 0.000 {built-in method posix.listdir}. 15152 0.063 0.000 0.268 0.000 /usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:69(_wrapreduction). 32755 0.062 0.000 0.135 0.000 <frozen importlib._bootstrap_external>:121(_path_join). 171/133 0.060 0.000 0.178 0.001 {built-in method _imp.exec_dynamic}. 1 0.059 0.059 0.059 0.059 {method 'calls_from_allele_counts' of 'deepvariant.python.variant_calling_multisample.VariantCaller' objects}. 23323 0.058 0.000 0.163 0.000 /usr/lib/python3.8/inspect.py:2489(__init__). 15563/15359 0.053 0.000 0.384 0.000 {built-in method numpy.core._multiarray_umath.implement_array_function}. 3601 0.053 0.000 0.344 0.000 /usr/lib/python3.8/inspect.py:1102(getfullargspec). 15152 0.051 0.000 0.319 0.000 /usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:2703(amax). 12155 0.049 0.000 0.060 0.000 {built-in method builtins.max}. 40200 0.049 0.000 0.049 0.000 {built-in method third_party.nucleus.util.python.utils.read_overlaps_region}. ```. In any case, the most optimized approach is some sort shared memory model with minimal overhead to keep the code nimble and easy to integrate or extend. The code itself is pleasantly easy to go through given the nice design, which makes it quite amendable to that. In the meantime, pass-by-reference probably could be an easier win. . Thank you,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:19,usability,feedback,feedback,19,Thanks for all the feedback and discussion above! I'll close this issue now.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:55,usability,close,close,55,Thanks for all the feedback and discussion above! I'll close this issue now.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/651:454,availability,error,error,454,"@ChristinaMulch . With respect to the mechanics of DeepVariant - You can merge a BAM file with multiple read groups or from several files with the same read group. When you do so, if there is only one Sample among these files (specifically, all read groups have the same SM: value), DeepVariant will call all of them together, and will use that sample name for the VCF header field. If the read groups have more than one Sample, DeepVariant will give an error. If you really need to run on that file even so, there is a flag you can provide to apply a sample name to the BAM. Does this answer your question?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/651
https://github.com/google/deepvariant/issues/651:227,interoperability,specif,specifically,227,"@ChristinaMulch . With respect to the mechanics of DeepVariant - You can merge a BAM file with multiple read groups or from several files with the same read group. When you do so, if there is only one Sample among these files (specifically, all read groups have the same SM: value), DeepVariant will call all of them together, and will use that sample name for the VCF header field. If the read groups have more than one Sample, DeepVariant will give an error. If you really need to run on that file even so, there is a flag you can provide to apply a sample name to the BAM. Does this answer your question?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/651
https://github.com/google/deepvariant/issues/651:454,performance,error,error,454,"@ChristinaMulch . With respect to the mechanics of DeepVariant - You can merge a BAM file with multiple read groups or from several files with the same read group. When you do so, if there is only one Sample among these files (specifically, all read groups have the same SM: value), DeepVariant will call all of them together, and will use that sample name for the VCF header field. If the read groups have more than one Sample, DeepVariant will give an error. If you really need to run on that file even so, there is a flag you can provide to apply a sample name to the BAM. Does this answer your question?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/651
https://github.com/google/deepvariant/issues/651:576,reliability,Doe,Does,576,"@ChristinaMulch . With respect to the mechanics of DeepVariant - You can merge a BAM file with multiple read groups or from several files with the same read group. When you do so, if there is only one Sample among these files (specifically, all read groups have the same SM: value), DeepVariant will call all of them together, and will use that sample name for the VCF header field. If the read groups have more than one Sample, DeepVariant will give an error. If you really need to run on that file even so, there is a flag you can provide to apply a sample name to the BAM. Does this answer your question?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/651
https://github.com/google/deepvariant/issues/651:454,safety,error,error,454,"@ChristinaMulch . With respect to the mechanics of DeepVariant - You can merge a BAM file with multiple read groups or from several files with the same read group. When you do so, if there is only one Sample among these files (specifically, all read groups have the same SM: value), DeepVariant will call all of them together, and will use that sample name for the VCF header field. If the read groups have more than one Sample, DeepVariant will give an error. If you really need to run on that file even so, there is a flag you can provide to apply a sample name to the BAM. Does this answer your question?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/651
https://github.com/google/deepvariant/issues/651:454,usability,error,error,454,"@ChristinaMulch . With respect to the mechanics of DeepVariant - You can merge a BAM file with multiple read groups or from several files with the same read group. When you do so, if there is only one Sample among these files (specifically, all read groups have the same SM: value), DeepVariant will call all of them together, and will use that sample name for the VCF header field. If the read groups have more than one Sample, DeepVariant will give an error. If you really need to run on that file even so, there is a flag you can provide to apply a sample name to the BAM. Does this answer your question?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/651
https://github.com/google/deepvariant/issues/651:496,safety,input,input,496,"> Hi @ChristinaMulch , if the answer above didn't answer your question, please feel free to reopen. Hi @pichuan . Thanks for the great tools. I have a set of pupolation resequencing data including 151 individuals for SNP calling. The reads were mapped to the genome by bowtie2 and marked duplications by GATK. I have used GATK HaplotypeCaller to call SNPs with 151 samples together. I would like to compare the results with deepvariant. However, deepvariant seemed to accept only one bam file as input. Will it work when I merge the 151 bams as input? best ~",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/651
https://github.com/google/deepvariant/issues/651:545,safety,input,input,545,"> Hi @ChristinaMulch , if the answer above didn't answer your question, please feel free to reopen. Hi @pichuan . Thanks for the great tools. I have a set of pupolation resequencing data including 151 individuals for SNP calling. The reads were mapped to the genome by bowtie2 and marked duplications by GATK. I have used GATK HaplotypeCaller to call SNPs with 151 samples together. I would like to compare the results with deepvariant. However, deepvariant seemed to accept only one bam file as input. Will it work when I merge the 151 bams as input? best ~",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/651
https://github.com/google/deepvariant/issues/651:135,usability,tool,tools,135,"> Hi @ChristinaMulch , if the answer above didn't answer your question, please feel free to reopen. Hi @pichuan . Thanks for the great tools. I have a set of pupolation resequencing data including 151 individuals for SNP calling. The reads were mapped to the genome by bowtie2 and marked duplications by GATK. I have used GATK HaplotypeCaller to call SNPs with 151 samples together. I would like to compare the results with deepvariant. However, deepvariant seemed to accept only one bam file as input. Will it work when I merge the 151 bams as input? best ~",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/651
https://github.com/google/deepvariant/issues/651:496,usability,input,input,496,"> Hi @ChristinaMulch , if the answer above didn't answer your question, please feel free to reopen. Hi @pichuan . Thanks for the great tools. I have a set of pupolation resequencing data including 151 individuals for SNP calling. The reads were mapped to the genome by bowtie2 and marked duplications by GATK. I have used GATK HaplotypeCaller to call SNPs with 151 samples together. I would like to compare the results with deepvariant. However, deepvariant seemed to accept only one bam file as input. Will it work when I merge the 151 bams as input? best ~",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/651
https://github.com/google/deepvariant/issues/651:545,usability,input,input,545,"> Hi @ChristinaMulch , if the answer above didn't answer your question, please feel free to reopen. Hi @pichuan . Thanks for the great tools. I have a set of pupolation resequencing data including 151 individuals for SNP calling. The reads were mapped to the genome by bowtie2 and marked duplications by GATK. I have used GATK HaplotypeCaller to call SNPs with 151 samples together. I would like to compare the results with deepvariant. However, deepvariant seemed to accept only one bam file as input. Will it work when I merge the 151 bams as input? best ~",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/651
https://github.com/google/deepvariant/issues/651:335,availability,avail,available,335,"Probably the quickest way would be to process each sample individually and then combine them together, by first creating [gVCF outputs](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-gvcf-support.md) and then merging them using [GLnexus](https://github.com/dnanexus-rnd/GLnexus). A nice tutorial on how to do this is available at the following link:. https://github.com/google/deepvariant/blob/r1.5/docs/trio-merge-case-study.md.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/651
https://github.com/google/deepvariant/issues/651:335,reliability,availab,available,335,"Probably the quickest way would be to process each sample individually and then combine them together, by first creating [gVCF outputs](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-gvcf-support.md) and then merging them using [GLnexus](https://github.com/dnanexus-rnd/GLnexus). A nice tutorial on how to do this is available at the following link:. https://github.com/google/deepvariant/blob/r1.5/docs/trio-merge-case-study.md.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/651
https://github.com/google/deepvariant/issues/651:335,safety,avail,available,335,"Probably the quickest way would be to process each sample individually and then combine them together, by first creating [gVCF outputs](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-gvcf-support.md) and then merging them using [GLnexus](https://github.com/dnanexus-rnd/GLnexus). A nice tutorial on how to do this is available at the following link:. https://github.com/google/deepvariant/blob/r1.5/docs/trio-merge-case-study.md.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/651
https://github.com/google/deepvariant/issues/651:335,security,availab,available,335,"Probably the quickest way would be to process each sample individually and then combine them together, by first creating [gVCF outputs](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-gvcf-support.md) and then merging them using [GLnexus](https://github.com/dnanexus-rnd/GLnexus). A nice tutorial on how to do this is available at the following link:. https://github.com/google/deepvariant/blob/r1.5/docs/trio-merge-case-study.md.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/651
https://github.com/google/deepvariant/issues/651:206,usability,support,support,206,"Probably the quickest way would be to process each sample individually and then combine them together, by first creating [gVCF outputs](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-gvcf-support.md) and then merging them using [GLnexus](https://github.com/dnanexus-rnd/GLnexus). A nice tutorial on how to do this is available at the following link:. https://github.com/google/deepvariant/blob/r1.5/docs/trio-merge-case-study.md.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/651
https://github.com/google/deepvariant/issues/651:337,availability,avail,available,337,"> Probably the quickest way would be to process each sample individually and then combine them together, by first creating [gVCF outputs](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-gvcf-support.md) and then merging them using [GLnexus](https://github.com/dnanexus-rnd/GLnexus). A nice tutorial on how to do this is available at the following link:. > . > https://github.com/google/deepvariant/blob/r1.5/docs/trio-merge-case-study.md. Thanks for your reply! It seems too time and resource consuming to separately run for each sample. If I got the resource, I would have a try. Best ~",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/651
https://github.com/google/deepvariant/issues/651:501,deployability,resourc,resource,501,"> Probably the quickest way would be to process each sample individually and then combine them together, by first creating [gVCF outputs](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-gvcf-support.md) and then merging them using [GLnexus](https://github.com/dnanexus-rnd/GLnexus). A nice tutorial on how to do this is available at the following link:. > . > https://github.com/google/deepvariant/blob/r1.5/docs/trio-merge-case-study.md. Thanks for your reply! It seems too time and resource consuming to separately run for each sample. If I got the resource, I would have a try. Best ~",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/651
https://github.com/google/deepvariant/issues/651:568,deployability,resourc,resource,568,"> Probably the quickest way would be to process each sample individually and then combine them together, by first creating [gVCF outputs](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-gvcf-support.md) and then merging them using [GLnexus](https://github.com/dnanexus-rnd/GLnexus). A nice tutorial on how to do this is available at the following link:. > . > https://github.com/google/deepvariant/blob/r1.5/docs/trio-merge-case-study.md. Thanks for your reply! It seems too time and resource consuming to separately run for each sample. If I got the resource, I would have a try. Best ~",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/651
https://github.com/google/deepvariant/issues/651:501,energy efficiency,resourc,resource,501,"> Probably the quickest way would be to process each sample individually and then combine them together, by first creating [gVCF outputs](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-gvcf-support.md) and then merging them using [GLnexus](https://github.com/dnanexus-rnd/GLnexus). A nice tutorial on how to do this is available at the following link:. > . > https://github.com/google/deepvariant/blob/r1.5/docs/trio-merge-case-study.md. Thanks for your reply! It seems too time and resource consuming to separately run for each sample. If I got the resource, I would have a try. Best ~",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/651
https://github.com/google/deepvariant/issues/651:568,energy efficiency,resourc,resource,568,"> Probably the quickest way would be to process each sample individually and then combine them together, by first creating [gVCF outputs](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-gvcf-support.md) and then merging them using [GLnexus](https://github.com/dnanexus-rnd/GLnexus). A nice tutorial on how to do this is available at the following link:. > . > https://github.com/google/deepvariant/blob/r1.5/docs/trio-merge-case-study.md. Thanks for your reply! It seems too time and resource consuming to separately run for each sample. If I got the resource, I would have a try. Best ~",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/651
https://github.com/google/deepvariant/issues/651:492,performance,time,time,492,"> Probably the quickest way would be to process each sample individually and then combine them together, by first creating [gVCF outputs](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-gvcf-support.md) and then merging them using [GLnexus](https://github.com/dnanexus-rnd/GLnexus). A nice tutorial on how to do this is available at the following link:. > . > https://github.com/google/deepvariant/blob/r1.5/docs/trio-merge-case-study.md. Thanks for your reply! It seems too time and resource consuming to separately run for each sample. If I got the resource, I would have a try. Best ~",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/651
https://github.com/google/deepvariant/issues/651:501,performance,resourc,resource,501,"> Probably the quickest way would be to process each sample individually and then combine them together, by first creating [gVCF outputs](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-gvcf-support.md) and then merging them using [GLnexus](https://github.com/dnanexus-rnd/GLnexus). A nice tutorial on how to do this is available at the following link:. > . > https://github.com/google/deepvariant/blob/r1.5/docs/trio-merge-case-study.md. Thanks for your reply! It seems too time and resource consuming to separately run for each sample. If I got the resource, I would have a try. Best ~",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/651
https://github.com/google/deepvariant/issues/651:568,performance,resourc,resource,568,"> Probably the quickest way would be to process each sample individually and then combine them together, by first creating [gVCF outputs](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-gvcf-support.md) and then merging them using [GLnexus](https://github.com/dnanexus-rnd/GLnexus). A nice tutorial on how to do this is available at the following link:. > . > https://github.com/google/deepvariant/blob/r1.5/docs/trio-merge-case-study.md. Thanks for your reply! It seems too time and resource consuming to separately run for each sample. If I got the resource, I would have a try. Best ~",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/651
https://github.com/google/deepvariant/issues/651:337,reliability,availab,available,337,"> Probably the quickest way would be to process each sample individually and then combine them together, by first creating [gVCF outputs](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-gvcf-support.md) and then merging them using [GLnexus](https://github.com/dnanexus-rnd/GLnexus). A nice tutorial on how to do this is available at the following link:. > . > https://github.com/google/deepvariant/blob/r1.5/docs/trio-merge-case-study.md. Thanks for your reply! It seems too time and resource consuming to separately run for each sample. If I got the resource, I would have a try. Best ~",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/651
https://github.com/google/deepvariant/issues/651:337,safety,avail,available,337,"> Probably the quickest way would be to process each sample individually and then combine them together, by first creating [gVCF outputs](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-gvcf-support.md) and then merging them using [GLnexus](https://github.com/dnanexus-rnd/GLnexus). A nice tutorial on how to do this is available at the following link:. > . > https://github.com/google/deepvariant/blob/r1.5/docs/trio-merge-case-study.md. Thanks for your reply! It seems too time and resource consuming to separately run for each sample. If I got the resource, I would have a try. Best ~",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/651
https://github.com/google/deepvariant/issues/651:501,safety,resourc,resource,501,"> Probably the quickest way would be to process each sample individually and then combine them together, by first creating [gVCF outputs](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-gvcf-support.md) and then merging them using [GLnexus](https://github.com/dnanexus-rnd/GLnexus). A nice tutorial on how to do this is available at the following link:. > . > https://github.com/google/deepvariant/blob/r1.5/docs/trio-merge-case-study.md. Thanks for your reply! It seems too time and resource consuming to separately run for each sample. If I got the resource, I would have a try. Best ~",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/651
https://github.com/google/deepvariant/issues/651:568,safety,resourc,resource,568,"> Probably the quickest way would be to process each sample individually and then combine them together, by first creating [gVCF outputs](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-gvcf-support.md) and then merging them using [GLnexus](https://github.com/dnanexus-rnd/GLnexus). A nice tutorial on how to do this is available at the following link:. > . > https://github.com/google/deepvariant/blob/r1.5/docs/trio-merge-case-study.md. Thanks for your reply! It seems too time and resource consuming to separately run for each sample. If I got the resource, I would have a try. Best ~",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/651
https://github.com/google/deepvariant/issues/651:337,security,availab,available,337,"> Probably the quickest way would be to process each sample individually and then combine them together, by first creating [gVCF outputs](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-gvcf-support.md) and then merging them using [GLnexus](https://github.com/dnanexus-rnd/GLnexus). A nice tutorial on how to do this is available at the following link:. > . > https://github.com/google/deepvariant/blob/r1.5/docs/trio-merge-case-study.md. Thanks for your reply! It seems too time and resource consuming to separately run for each sample. If I got the resource, I would have a try. Best ~",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/651
https://github.com/google/deepvariant/issues/651:501,testability,resourc,resource,501,"> Probably the quickest way would be to process each sample individually and then combine them together, by first creating [gVCF outputs](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-gvcf-support.md) and then merging them using [GLnexus](https://github.com/dnanexus-rnd/GLnexus). A nice tutorial on how to do this is available at the following link:. > . > https://github.com/google/deepvariant/blob/r1.5/docs/trio-merge-case-study.md. Thanks for your reply! It seems too time and resource consuming to separately run for each sample. If I got the resource, I would have a try. Best ~",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/651
https://github.com/google/deepvariant/issues/651:568,testability,resourc,resource,568,"> Probably the quickest way would be to process each sample individually and then combine them together, by first creating [gVCF outputs](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-gvcf-support.md) and then merging them using [GLnexus](https://github.com/dnanexus-rnd/GLnexus). A nice tutorial on how to do this is available at the following link:. > . > https://github.com/google/deepvariant/blob/r1.5/docs/trio-merge-case-study.md. Thanks for your reply! It seems too time and resource consuming to separately run for each sample. If I got the resource, I would have a try. Best ~",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/651
https://github.com/google/deepvariant/issues/651:208,usability,support,support,208,"> Probably the quickest way would be to process each sample individually and then combine them together, by first creating [gVCF outputs](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-gvcf-support.md) and then merging them using [GLnexus](https://github.com/dnanexus-rnd/GLnexus). A nice tutorial on how to do this is available at the following link:. > . > https://github.com/google/deepvariant/blob/r1.5/docs/trio-merge-case-study.md. Thanks for your reply! It seems too time and resource consuming to separately run for each sample. If I got the resource, I would have a try. Best ~",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/651
https://github.com/google/deepvariant/issues/651:47,deployability,resourc,resources,47,Google Cloud might be an option for additional resources: https://cloud.google.com/life-sciences/docs/tutorials/deepvariant,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/651
https://github.com/google/deepvariant/issues/651:7,energy efficiency,Cloud,Cloud,7,Google Cloud might be an option for additional resources: https://cloud.google.com/life-sciences/docs/tutorials/deepvariant,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/651
https://github.com/google/deepvariant/issues/651:47,energy efficiency,resourc,resources,47,Google Cloud might be an option for additional resources: https://cloud.google.com/life-sciences/docs/tutorials/deepvariant,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/651
https://github.com/google/deepvariant/issues/651:66,energy efficiency,cloud,cloud,66,Google Cloud might be an option for additional resources: https://cloud.google.com/life-sciences/docs/tutorials/deepvariant,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/651
https://github.com/google/deepvariant/issues/651:47,performance,resourc,resources,47,Google Cloud might be an option for additional resources: https://cloud.google.com/life-sciences/docs/tutorials/deepvariant,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/651
https://github.com/google/deepvariant/issues/651:47,safety,resourc,resources,47,Google Cloud might be an option for additional resources: https://cloud.google.com/life-sciences/docs/tutorials/deepvariant,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/651
https://github.com/google/deepvariant/issues/651:47,testability,resourc,resources,47,Google Cloud might be an option for additional resources: https://cloud.google.com/life-sciences/docs/tutorials/deepvariant,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/651
https://github.com/google/deepvariant/issues/651:226,availability,operat,operation,226,"Hi @tinyfallen . @pgrosu is correct, the most efficient way is to run DeepVariant individually and then merge with GLnexus. One point to make, GLnexus runs quite quickly relative to the variant calling step. The joint calling operation won't add much cost relative to DeepVariant (scaling benchmarks for GLnexus can be found in Figure 7C of [the DeepVariant-GLnexus paper](https://academic.oup.com/bioinformatics/article/36/24/5582/6064144). So the main resource use can be estimated from the single sample runtime multiplied by sample number. If you are curious for a comparison, it might make sense to run a single sample and compare the calls for that sample with the extracted set of calls from the GATK joint calls.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/651
https://github.com/google/deepvariant/issues/651:454,deployability,resourc,resource,454,"Hi @tinyfallen . @pgrosu is correct, the most efficient way is to run DeepVariant individually and then merge with GLnexus. One point to make, GLnexus runs quite quickly relative to the variant calling step. The joint calling operation won't add much cost relative to DeepVariant (scaling benchmarks for GLnexus can be found in Figure 7C of [the DeepVariant-GLnexus paper](https://academic.oup.com/bioinformatics/article/36/24/5582/6064144). So the main resource use can be estimated from the single sample runtime multiplied by sample number. If you are curious for a comparison, it might make sense to run a single sample and compare the calls for that sample with the extracted set of calls from the GATK joint calls.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/651
https://github.com/google/deepvariant/issues/651:454,energy efficiency,resourc,resource,454,"Hi @tinyfallen . @pgrosu is correct, the most efficient way is to run DeepVariant individually and then merge with GLnexus. One point to make, GLnexus runs quite quickly relative to the variant calling step. The joint calling operation won't add much cost relative to DeepVariant (scaling benchmarks for GLnexus can be found in Figure 7C of [the DeepVariant-GLnexus paper](https://academic.oup.com/bioinformatics/article/36/24/5582/6064144). So the main resource use can be estimated from the single sample runtime multiplied by sample number. If you are curious for a comparison, it might make sense to run a single sample and compare the calls for that sample with the extracted set of calls from the GATK joint calls.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/651
https://github.com/google/deepvariant/issues/651:474,energy efficiency,estimat,estimated,474,"Hi @tinyfallen . @pgrosu is correct, the most efficient way is to run DeepVariant individually and then merge with GLnexus. One point to make, GLnexus runs quite quickly relative to the variant calling step. The joint calling operation won't add much cost relative to DeepVariant (scaling benchmarks for GLnexus can be found in Figure 7C of [the DeepVariant-GLnexus paper](https://academic.oup.com/bioinformatics/article/36/24/5582/6064144). So the main resource use can be estimated from the single sample runtime multiplied by sample number. If you are curious for a comparison, it might make sense to run a single sample and compare the calls for that sample with the extracted set of calls from the GATK joint calls.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/651
https://github.com/google/deepvariant/issues/651:281,modifiability,scal,scaling,281,"Hi @tinyfallen . @pgrosu is correct, the most efficient way is to run DeepVariant individually and then merge with GLnexus. One point to make, GLnexus runs quite quickly relative to the variant calling step. The joint calling operation won't add much cost relative to DeepVariant (scaling benchmarks for GLnexus can be found in Figure 7C of [the DeepVariant-GLnexus paper](https://academic.oup.com/bioinformatics/article/36/24/5582/6064144). So the main resource use can be estimated from the single sample runtime multiplied by sample number. If you are curious for a comparison, it might make sense to run a single sample and compare the calls for that sample with the extracted set of calls from the GATK joint calls.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/651
https://github.com/google/deepvariant/issues/651:454,performance,resourc,resource,454,"Hi @tinyfallen . @pgrosu is correct, the most efficient way is to run DeepVariant individually and then merge with GLnexus. One point to make, GLnexus runs quite quickly relative to the variant calling step. The joint calling operation won't add much cost relative to DeepVariant (scaling benchmarks for GLnexus can be found in Figure 7C of [the DeepVariant-GLnexus paper](https://academic.oup.com/bioinformatics/article/36/24/5582/6064144). So the main resource use can be estimated from the single sample runtime multiplied by sample number. If you are curious for a comparison, it might make sense to run a single sample and compare the calls for that sample with the extracted set of calls from the GATK joint calls.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/651
https://github.com/google/deepvariant/issues/651:454,safety,resourc,resource,454,"Hi @tinyfallen . @pgrosu is correct, the most efficient way is to run DeepVariant individually and then merge with GLnexus. One point to make, GLnexus runs quite quickly relative to the variant calling step. The joint calling operation won't add much cost relative to DeepVariant (scaling benchmarks for GLnexus can be found in Figure 7C of [the DeepVariant-GLnexus paper](https://academic.oup.com/bioinformatics/article/36/24/5582/6064144). So the main resource use can be estimated from the single sample runtime multiplied by sample number. If you are curious for a comparison, it might make sense to run a single sample and compare the calls for that sample with the extracted set of calls from the GATK joint calls.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/651
https://github.com/google/deepvariant/issues/651:454,testability,resourc,resource,454,"Hi @tinyfallen . @pgrosu is correct, the most efficient way is to run DeepVariant individually and then merge with GLnexus. One point to make, GLnexus runs quite quickly relative to the variant calling step. The joint calling operation won't add much cost relative to DeepVariant (scaling benchmarks for GLnexus can be found in Figure 7C of [the DeepVariant-GLnexus paper](https://academic.oup.com/bioinformatics/article/36/24/5582/6064144). So the main resource use can be estimated from the single sample runtime multiplied by sample number. If you are curious for a comparison, it might make sense to run a single sample and compare the calls for that sample with the extracted set of calls from the GATK joint calls.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/651
https://github.com/google/deepvariant/issues/651:46,usability,efficien,efficient,46,"Hi @tinyfallen . @pgrosu is correct, the most efficient way is to run DeepVariant individually and then merge with GLnexus. One point to make, GLnexus runs quite quickly relative to the variant calling step. The joint calling operation won't add much cost relative to DeepVariant (scaling benchmarks for GLnexus can be found in Figure 7C of [the DeepVariant-GLnexus paper](https://academic.oup.com/bioinformatics/article/36/24/5582/6064144). So the main resource use can be estimated from the single sample runtime multiplied by sample number. If you are curious for a comparison, it might make sense to run a single sample and compare the calls for that sample with the extracted set of calls from the GATK joint calls.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/651
https://github.com/google/deepvariant/issues/651:233,availability,operat,operation,233,"> Hi @tinyfallen. > . > @pgrosu is correct, the most efficient way is to run DeepVariant individually and then merge with GLnexus. One point to make, GLnexus runs quite quickly relative to the variant calling step. The joint calling operation won't add much cost relative to DeepVariant (scaling benchmarks for GLnexus can be found in Figure 7C of [the DeepVariant-GLnexus paper](https://academic.oup.com/bioinformatics/article/36/24/5582/6064144). > . > So the main resource use can be estimated from the single sample runtime multiplied by sample number. > . > If you are curious for a comparison, it might make sense to run a single sample and compare the calls for that sample with the extracted set of calls from the GATK joint calls. Many thanks for your reply! I would have a try. best ~",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/651
https://github.com/google/deepvariant/issues/651:467,deployability,resourc,resource,467,"> Hi @tinyfallen. > . > @pgrosu is correct, the most efficient way is to run DeepVariant individually and then merge with GLnexus. One point to make, GLnexus runs quite quickly relative to the variant calling step. The joint calling operation won't add much cost relative to DeepVariant (scaling benchmarks for GLnexus can be found in Figure 7C of [the DeepVariant-GLnexus paper](https://academic.oup.com/bioinformatics/article/36/24/5582/6064144). > . > So the main resource use can be estimated from the single sample runtime multiplied by sample number. > . > If you are curious for a comparison, it might make sense to run a single sample and compare the calls for that sample with the extracted set of calls from the GATK joint calls. Many thanks for your reply! I would have a try. best ~",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/651
https://github.com/google/deepvariant/issues/651:467,energy efficiency,resourc,resource,467,"> Hi @tinyfallen. > . > @pgrosu is correct, the most efficient way is to run DeepVariant individually and then merge with GLnexus. One point to make, GLnexus runs quite quickly relative to the variant calling step. The joint calling operation won't add much cost relative to DeepVariant (scaling benchmarks for GLnexus can be found in Figure 7C of [the DeepVariant-GLnexus paper](https://academic.oup.com/bioinformatics/article/36/24/5582/6064144). > . > So the main resource use can be estimated from the single sample runtime multiplied by sample number. > . > If you are curious for a comparison, it might make sense to run a single sample and compare the calls for that sample with the extracted set of calls from the GATK joint calls. Many thanks for your reply! I would have a try. best ~",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/651
https://github.com/google/deepvariant/issues/651:487,energy efficiency,estimat,estimated,487,"> Hi @tinyfallen. > . > @pgrosu is correct, the most efficient way is to run DeepVariant individually and then merge with GLnexus. One point to make, GLnexus runs quite quickly relative to the variant calling step. The joint calling operation won't add much cost relative to DeepVariant (scaling benchmarks for GLnexus can be found in Figure 7C of [the DeepVariant-GLnexus paper](https://academic.oup.com/bioinformatics/article/36/24/5582/6064144). > . > So the main resource use can be estimated from the single sample runtime multiplied by sample number. > . > If you are curious for a comparison, it might make sense to run a single sample and compare the calls for that sample with the extracted set of calls from the GATK joint calls. Many thanks for your reply! I would have a try. best ~",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/651
https://github.com/google/deepvariant/issues/651:288,modifiability,scal,scaling,288,"> Hi @tinyfallen. > . > @pgrosu is correct, the most efficient way is to run DeepVariant individually and then merge with GLnexus. One point to make, GLnexus runs quite quickly relative to the variant calling step. The joint calling operation won't add much cost relative to DeepVariant (scaling benchmarks for GLnexus can be found in Figure 7C of [the DeepVariant-GLnexus paper](https://academic.oup.com/bioinformatics/article/36/24/5582/6064144). > . > So the main resource use can be estimated from the single sample runtime multiplied by sample number. > . > If you are curious for a comparison, it might make sense to run a single sample and compare the calls for that sample with the extracted set of calls from the GATK joint calls. Many thanks for your reply! I would have a try. best ~",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/651
https://github.com/google/deepvariant/issues/651:467,performance,resourc,resource,467,"> Hi @tinyfallen. > . > @pgrosu is correct, the most efficient way is to run DeepVariant individually and then merge with GLnexus. One point to make, GLnexus runs quite quickly relative to the variant calling step. The joint calling operation won't add much cost relative to DeepVariant (scaling benchmarks for GLnexus can be found in Figure 7C of [the DeepVariant-GLnexus paper](https://academic.oup.com/bioinformatics/article/36/24/5582/6064144). > . > So the main resource use can be estimated from the single sample runtime multiplied by sample number. > . > If you are curious for a comparison, it might make sense to run a single sample and compare the calls for that sample with the extracted set of calls from the GATK joint calls. Many thanks for your reply! I would have a try. best ~",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/651
https://github.com/google/deepvariant/issues/651:467,safety,resourc,resource,467,"> Hi @tinyfallen. > . > @pgrosu is correct, the most efficient way is to run DeepVariant individually and then merge with GLnexus. One point to make, GLnexus runs quite quickly relative to the variant calling step. The joint calling operation won't add much cost relative to DeepVariant (scaling benchmarks for GLnexus can be found in Figure 7C of [the DeepVariant-GLnexus paper](https://academic.oup.com/bioinformatics/article/36/24/5582/6064144). > . > So the main resource use can be estimated from the single sample runtime multiplied by sample number. > . > If you are curious for a comparison, it might make sense to run a single sample and compare the calls for that sample with the extracted set of calls from the GATK joint calls. Many thanks for your reply! I would have a try. best ~",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/651
https://github.com/google/deepvariant/issues/651:467,testability,resourc,resource,467,"> Hi @tinyfallen. > . > @pgrosu is correct, the most efficient way is to run DeepVariant individually and then merge with GLnexus. One point to make, GLnexus runs quite quickly relative to the variant calling step. The joint calling operation won't add much cost relative to DeepVariant (scaling benchmarks for GLnexus can be found in Figure 7C of [the DeepVariant-GLnexus paper](https://academic.oup.com/bioinformatics/article/36/24/5582/6064144). > . > So the main resource use can be estimated from the single sample runtime multiplied by sample number. > . > If you are curious for a comparison, it might make sense to run a single sample and compare the calls for that sample with the extracted set of calls from the GATK joint calls. Many thanks for your reply! I would have a try. best ~",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/651
https://github.com/google/deepvariant/issues/651:53,usability,efficien,efficient,53,"> Hi @tinyfallen. > . > @pgrosu is correct, the most efficient way is to run DeepVariant individually and then merge with GLnexus. One point to make, GLnexus runs quite quickly relative to the variant calling step. The joint calling operation won't add much cost relative to DeepVariant (scaling benchmarks for GLnexus can be found in Figure 7C of [the DeepVariant-GLnexus paper](https://academic.oup.com/bioinformatics/article/36/24/5582/6064144). > . > So the main resource use can be estimated from the single sample runtime multiplied by sample number. > . > If you are curious for a comparison, it might make sense to run a single sample and compare the calls for that sample with the extracted set of calls from the GATK joint calls. Many thanks for your reply! I would have a try. best ~",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/651
https://github.com/google/deepvariant/issues/652:168,deployability,observ,observed,168,"@crazysummerW my intuition is that it would not make a big difference, but we have not empirically tested this. My guess is that It could make a difference in variants observed in regions with lower coverage. Here is a similar issue (although, in the context of DNA-seq): https://github.com/google/deepvariant/issues/384",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/652
https://github.com/google/deepvariant/issues/652:99,safety,test,tested,99,"@crazysummerW my intuition is that it would not make a big difference, but we have not empirically tested this. My guess is that It could make a difference in variants observed in regions with lower coverage. Here is a similar issue (although, in the context of DNA-seq): https://github.com/google/deepvariant/issues/384",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/652
https://github.com/google/deepvariant/issues/652:99,testability,test,tested,99,"@crazysummerW my intuition is that it would not make a big difference, but we have not empirically tested this. My guess is that It could make a difference in variants observed in regions with lower coverage. Here is a similar issue (although, in the context of DNA-seq): https://github.com/google/deepvariant/issues/384",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/652
https://github.com/google/deepvariant/issues/652:168,testability,observ,observed,168,"@crazysummerW my intuition is that it would not make a big difference, but we have not empirically tested this. My guess is that It could make a difference in variants observed in regions with lower coverage. Here is a similar issue (although, in the context of DNA-seq): https://github.com/google/deepvariant/issues/384",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/652
https://github.com/google/deepvariant/issues/652:199,testability,coverag,coverage,199,"@crazysummerW my intuition is that it would not make a big difference, but we have not empirically tested this. My guess is that It could make a difference in variants observed in regions with lower coverage. Here is a similar issue (although, in the context of DNA-seq): https://github.com/google/deepvariant/issues/384",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/652
https://github.com/google/deepvariant/issues/652:251,testability,context,context,251,"@crazysummerW my intuition is that it would not make a big difference, but we have not empirically tested this. My guess is that It could make a difference in variants observed in regions with lower coverage. Here is a similar issue (although, in the context of DNA-seq): https://github.com/google/deepvariant/issues/384",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/652
https://github.com/google/deepvariant/issues/652:17,usability,intuit,intuition,17,"@crazysummerW my intuition is that it would not make a big difference, but we have not empirically tested this. My guess is that It could make a difference in variants observed in regions with lower coverage. Here is a similar issue (although, in the context of DNA-seq): https://github.com/google/deepvariant/issues/384",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/652
https://github.com/google/deepvariant/issues/652:64,usability,close,close,64,"Hi @crazysummerW , hopefully we've answered your question. I'll close this issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/652
https://github.com/google/deepvariant/issues/653:20,deployability,log,log,20,"Hi @Zjianglin , the log above is a bit difficult for me to read. Can you confirm whether you have the fai file in your path `/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai`? If not, can you index your FASTA file? You can use `samtools faidx hs37d5.fa` to create the index file.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:20,safety,log,log,20,"Hi @Zjianglin , the log above is a bit difficult for me to read. Can you confirm whether you have the fai file in your path `/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai`? If not, can you index your FASTA file? You can use `samtools faidx hs37d5.fa` to create the index file.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:20,security,log,log,20,"Hi @Zjianglin , the log above is a bit difficult for me to read. Can you confirm whether you have the fai file in your path `/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai`? If not, can you index your FASTA file? You can use `samtools faidx hs37d5.fa` to create the index file.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:20,testability,log,log,20,"Hi @Zjianglin , the log above is a bit difficult for me to read. Can you confirm whether you have the fai file in your path `/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai`? If not, can you index your FASTA file? You can use `samtools faidx hs37d5.fa` to create the index file.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:73,usability,confirm,confirm,73,"Hi @Zjianglin , the log above is a bit difficult for me to read. Can you confirm whether you have the fai file in your path `/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai`? If not, can you index your FASTA file? You can use `samtools faidx hs37d5.fa` to create the index file.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:138,usability,tool,toolsDB,138,"Hi @Zjianglin , the log above is a bit difficult for me to read. Can you confirm whether you have the fai file in your path `/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai`? If not, can you index your FASTA file? You can use `samtools faidx hs37d5.fa` to create the index file.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:1030,modifiability,pac,pac,1030,"Hi @pichuan , I confirm that `reference` and `reference index` does exist. See `ls` output below:. *reference index does exist*. ```shell. $ ls /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. -rw-rw-r-- 1 zhoujianglin zhoujianglin 3189750467 Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. -rw-rw-r-- 1 zhoujianglin zhoujianglin 6274909010 Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123. -rw-rw-r-- 1 zhoujianglin zhoujianglin 106669 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb. -rw-rw-r-- 1 zhoujianglin zhoujianglin 6924 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann. -rw-rw-r-- 1 zhoujianglin zhoujianglin 10196727247 Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64. -rw-rw-r-- 1 zhoujianglin zhoujianglin 2813 May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai. -rw-rw-r-- 1 zhoujianglin zhoujianglin 784363628 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:63,reliability,doe,does,63,"Hi @pichuan , I confirm that `reference` and `reference index` does exist. See `ls` output below:. *reference index does exist*. ```shell. $ ls /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. -rw-rw-r-- 1 zhoujianglin zhoujianglin 3189750467 Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. -rw-rw-r-- 1 zhoujianglin zhoujianglin 6274909010 Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123. -rw-rw-r-- 1 zhoujianglin zhoujianglin 106669 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb. -rw-rw-r-- 1 zhoujianglin zhoujianglin 6924 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann. -rw-rw-r-- 1 zhoujianglin zhoujianglin 10196727247 Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64. -rw-rw-r-- 1 zhoujianglin zhoujianglin 2813 May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai. -rw-rw-r-- 1 zhoujianglin zhoujianglin 784363628 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:116,reliability,doe,does,116,"Hi @pichuan , I confirm that `reference` and `reference index` does exist. See `ls` output below:. *reference index does exist*. ```shell. $ ls /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. -rw-rw-r-- 1 zhoujianglin zhoujianglin 3189750467 Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. -rw-rw-r-- 1 zhoujianglin zhoujianglin 6274909010 Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123. -rw-rw-r-- 1 zhoujianglin zhoujianglin 106669 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb. -rw-rw-r-- 1 zhoujianglin zhoujianglin 6924 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann. -rw-rw-r-- 1 zhoujianglin zhoujianglin 10196727247 Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64. -rw-rw-r-- 1 zhoujianglin zhoujianglin 2813 May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai. -rw-rw-r-- 1 zhoujianglin zhoujianglin 784363628 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:16,usability,confirm,confirm,16,"Hi @pichuan , I confirm that `reference` and `reference index` does exist. See `ls` output below:. *reference index does exist*. ```shell. $ ls /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. -rw-rw-r-- 1 zhoujianglin zhoujianglin 3189750467 Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. -rw-rw-r-- 1 zhoujianglin zhoujianglin 6274909010 Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123. -rw-rw-r-- 1 zhoujianglin zhoujianglin 106669 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb. -rw-rw-r-- 1 zhoujianglin zhoujianglin 6924 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann. -rw-rw-r-- 1 zhoujianglin zhoujianglin 10196727247 Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64. -rw-rw-r-- 1 zhoujianglin zhoujianglin 2813 May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai. -rw-rw-r-- 1 zhoujianglin zhoujianglin 784363628 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:157,usability,tool,toolsDB,157,"Hi @pichuan , I confirm that `reference` and `reference index` does exist. See `ls` output below:. *reference index does exist*. ```shell. $ ls /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. -rw-rw-r-- 1 zhoujianglin zhoujianglin 3189750467 Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. -rw-rw-r-- 1 zhoujianglin zhoujianglin 6274909010 Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123. -rw-rw-r-- 1 zhoujianglin zhoujianglin 106669 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb. -rw-rw-r-- 1 zhoujianglin zhoujianglin 6924 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann. -rw-rw-r-- 1 zhoujianglin zhoujianglin 10196727247 Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64. -rw-rw-r-- 1 zhoujianglin zhoujianglin 2813 May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai. -rw-rw-r-- 1 zhoujianglin zhoujianglin 784363628 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:275,usability,tool,toolsDB,275,"Hi @pichuan , I confirm that `reference` and `reference index` does exist. See `ls` output below:. *reference index does exist*. ```shell. $ ls /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. -rw-rw-r-- 1 zhoujianglin zhoujianglin 3189750467 Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. -rw-rw-r-- 1 zhoujianglin zhoujianglin 6274909010 Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123. -rw-rw-r-- 1 zhoujianglin zhoujianglin 106669 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb. -rw-rw-r-- 1 zhoujianglin zhoujianglin 6924 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann. -rw-rw-r-- 1 zhoujianglin zhoujianglin 10196727247 Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64. -rw-rw-r-- 1 zhoujianglin zhoujianglin 2813 May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai. -rw-rw-r-- 1 zhoujianglin zhoujianglin 784363628 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:392,usability,tool,toolsDB,392,"Hi @pichuan , I confirm that `reference` and `reference index` does exist. See `ls` output below:. *reference index does exist*. ```shell. $ ls /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. -rw-rw-r-- 1 zhoujianglin zhoujianglin 3189750467 Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. -rw-rw-r-- 1 zhoujianglin zhoujianglin 6274909010 Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123. -rw-rw-r-- 1 zhoujianglin zhoujianglin 106669 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb. -rw-rw-r-- 1 zhoujianglin zhoujianglin 6924 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann. -rw-rw-r-- 1 zhoujianglin zhoujianglin 10196727247 Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64. -rw-rw-r-- 1 zhoujianglin zhoujianglin 2813 May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai. -rw-rw-r-- 1 zhoujianglin zhoujianglin 784363628 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:510,usability,tool,toolsDB,510,"Hi @pichuan , I confirm that `reference` and `reference index` does exist. See `ls` output below:. *reference index does exist*. ```shell. $ ls /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. -rw-rw-r-- 1 zhoujianglin zhoujianglin 3189750467 Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. -rw-rw-r-- 1 zhoujianglin zhoujianglin 6274909010 Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123. -rw-rw-r-- 1 zhoujianglin zhoujianglin 106669 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb. -rw-rw-r-- 1 zhoujianglin zhoujianglin 6924 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann. -rw-rw-r-- 1 zhoujianglin zhoujianglin 10196727247 Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64. -rw-rw-r-- 1 zhoujianglin zhoujianglin 2813 May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai. -rw-rw-r-- 1 zhoujianglin zhoujianglin 784363628 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:625,usability,tool,toolsDB,625,"Hi @pichuan , I confirm that `reference` and `reference index` does exist. See `ls` output below:. *reference index does exist*. ```shell. $ ls /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. -rw-rw-r-- 1 zhoujianglin zhoujianglin 3189750467 Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. -rw-rw-r-- 1 zhoujianglin zhoujianglin 6274909010 Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123. -rw-rw-r-- 1 zhoujianglin zhoujianglin 106669 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb. -rw-rw-r-- 1 zhoujianglin zhoujianglin 6924 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann. -rw-rw-r-- 1 zhoujianglin zhoujianglin 10196727247 Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64. -rw-rw-r-- 1 zhoujianglin zhoujianglin 2813 May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai. -rw-rw-r-- 1 zhoujianglin zhoujianglin 784363628 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:747,usability,tool,toolsDB,747,"Hi @pichuan , I confirm that `reference` and `reference index` does exist. See `ls` output below:. *reference index does exist*. ```shell. $ ls /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. -rw-rw-r-- 1 zhoujianglin zhoujianglin 3189750467 Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. -rw-rw-r-- 1 zhoujianglin zhoujianglin 6274909010 Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123. -rw-rw-r-- 1 zhoujianglin zhoujianglin 106669 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb. -rw-rw-r-- 1 zhoujianglin zhoujianglin 6924 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann. -rw-rw-r-- 1 zhoujianglin zhoujianglin 10196727247 Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64. -rw-rw-r-- 1 zhoujianglin zhoujianglin 2813 May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai. -rw-rw-r-- 1 zhoujianglin zhoujianglin 784363628 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:870,usability,tool,toolsDB,870,"Hi @pichuan , I confirm that `reference` and `reference index` does exist. See `ls` output below:. *reference index does exist*. ```shell. $ ls /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. -rw-rw-r-- 1 zhoujianglin zhoujianglin 3189750467 Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. -rw-rw-r-- 1 zhoujianglin zhoujianglin 6274909010 Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123. -rw-rw-r-- 1 zhoujianglin zhoujianglin 106669 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb. -rw-rw-r-- 1 zhoujianglin zhoujianglin 6924 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann. -rw-rw-r-- 1 zhoujianglin zhoujianglin 10196727247 Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64. -rw-rw-r-- 1 zhoujianglin zhoujianglin 2813 May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai. -rw-rw-r-- 1 zhoujianglin zhoujianglin 784363628 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:990,usability,tool,toolsDB,990,"Hi @pichuan , I confirm that `reference` and `reference index` does exist. See `ls` output below:. *reference index does exist*. ```shell. $ ls /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. -rw-rw-r-- 1 zhoujianglin zhoujianglin 3189750467 Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. -rw-rw-r-- 1 zhoujianglin zhoujianglin 6274909010 Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123. -rw-rw-r-- 1 zhoujianglin zhoujianglin 106669 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb. -rw-rw-r-- 1 zhoujianglin zhoujianglin 6924 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann. -rw-rw-r-- 1 zhoujianglin zhoujianglin 10196727247 Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64. -rw-rw-r-- 1 zhoujianglin zhoujianglin 2813 May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai. -rw-rw-r-- 1 zhoujianglin zhoujianglin 784363628 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:433,availability,echo,echo,433,"@Zjianglin Can you check whether your singularity run can see the index file? You said you have:. `singularity run /lustre/Data/toolsDB//deepvariant.sif /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=$ref_idx --reads=$dedupbam --output_vcf=$vcfout --output_gvcf=$gvcfout --num_shards=32 >$logx 2>&1`. Can you try something like:. `singularity run /lustre/Data/toolsDB/deepvariant.sif ls $ref_idx` ? By the way, can you `echo $ref_idx` to see what the value is? It should be something like `--ref=/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa`. Given that your name is `$ref_idx`, hopefully it's not the .fai file.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:302,deployability,log,logx,302,"@Zjianglin Can you check whether your singularity run can see the index file? You said you have:. `singularity run /lustre/Data/toolsDB//deepvariant.sif /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=$ref_idx --reads=$dedupbam --output_vcf=$vcfout --output_gvcf=$gvcfout --num_shards=32 >$logx 2>&1`. Can you try something like:. `singularity run /lustre/Data/toolsDB/deepvariant.sif ls $ref_idx` ? By the way, can you `echo $ref_idx` to see what the value is? It should be something like `--ref=/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa`. Given that your name is `$ref_idx`, hopefully it's not the .fai file.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:302,safety,log,logx,302,"@Zjianglin Can you check whether your singularity run can see the index file? You said you have:. `singularity run /lustre/Data/toolsDB//deepvariant.sif /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=$ref_idx --reads=$dedupbam --output_vcf=$vcfout --output_gvcf=$gvcfout --num_shards=32 >$logx 2>&1`. Can you try something like:. `singularity run /lustre/Data/toolsDB/deepvariant.sif ls $ref_idx` ? By the way, can you `echo $ref_idx` to see what the value is? It should be something like `--ref=/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa`. Given that your name is `$ref_idx`, hopefully it's not the .fai file.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:302,security,log,logx,302,"@Zjianglin Can you check whether your singularity run can see the index file? You said you have:. `singularity run /lustre/Data/toolsDB//deepvariant.sif /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=$ref_idx --reads=$dedupbam --output_vcf=$vcfout --output_gvcf=$gvcfout --num_shards=32 >$logx 2>&1`. Can you try something like:. `singularity run /lustre/Data/toolsDB/deepvariant.sif ls $ref_idx` ? By the way, can you `echo $ref_idx` to see what the value is? It should be something like `--ref=/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa`. Given that your name is `$ref_idx`, hopefully it's not the .fai file.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:302,testability,log,logx,302,"@Zjianglin Can you check whether your singularity run can see the index file? You said you have:. `singularity run /lustre/Data/toolsDB//deepvariant.sif /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=$ref_idx --reads=$dedupbam --output_vcf=$vcfout --output_gvcf=$gvcfout --num_shards=32 >$logx 2>&1`. Can you try something like:. `singularity run /lustre/Data/toolsDB/deepvariant.sif ls $ref_idx` ? By the way, can you `echo $ref_idx` to see what the value is? It should be something like `--ref=/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa`. Given that your name is `$ref_idx`, hopefully it's not the .fai file.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:128,usability,tool,toolsDB,128,"@Zjianglin Can you check whether your singularity run can see the index file? You said you have:. `singularity run /lustre/Data/toolsDB//deepvariant.sif /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=$ref_idx --reads=$dedupbam --output_vcf=$vcfout --output_gvcf=$gvcfout --num_shards=32 >$logx 2>&1`. Can you try something like:. `singularity run /lustre/Data/toolsDB/deepvariant.sif ls $ref_idx` ? By the way, can you `echo $ref_idx` to see what the value is? It should be something like `--ref=/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa`. Given that your name is `$ref_idx`, hopefully it's not the .fai file.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:373,usability,tool,toolsDB,373,"@Zjianglin Can you check whether your singularity run can see the index file? You said you have:. `singularity run /lustre/Data/toolsDB//deepvariant.sif /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=$ref_idx --reads=$dedupbam --output_vcf=$vcfout --output_gvcf=$gvcfout --num_shards=32 >$logx 2>&1`. Can you try something like:. `singularity run /lustre/Data/toolsDB/deepvariant.sif ls $ref_idx` ? By the way, can you `echo $ref_idx` to see what the value is? It should be something like `--ref=/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa`. Given that your name is `$ref_idx`, hopefully it's not the .fai file.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:522,usability,tool,toolsDB,522,"@Zjianglin Can you check whether your singularity run can see the index file? You said you have:. `singularity run /lustre/Data/toolsDB//deepvariant.sif /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=$ref_idx --reads=$dedupbam --output_vcf=$vcfout --output_gvcf=$gvcfout --num_shards=32 >$logx 2>&1`. Can you try something like:. `singularity run /lustre/Data/toolsDB/deepvariant.sif ls $ref_idx` ? By the way, can you `echo $ref_idx` to see what the value is? It should be something like `--ref=/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa`. Given that your name is `$ref_idx`, hopefully it's not the .fai file.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:1869,availability,echo,echo,1869,"/hs37d5.fa.amb. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann. -rw-rw-r-- 1 zhoujianglin zhoujianglin 9725M Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai. -rw-rw-r-- 1 zhoujianglin zhoujianglin 749M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac. ```. However, If I run the follow bash script, it can not find `ref_idx` through `ls` command, whereas shell condition expression ` [ -f $ref_idx ]` return `true`. here is the script:. ```shell. #!/bin/bash. dvsif=""/lustre/Data/toolsDB//deepvariant.sif"". ref_idx=""/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa"". wkdir=""/lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP"". bamdir=""${wkdir}/mappinged_bams"". logdir=""${wkdir}/logs"". vcfoutdir=""${wkdir}/DeepVariant_outputs"". source activate ~/.conda/envs/zjlEnv. echo -e ""ref_idx is $ref_idx\n"". if [ -f ""$ref_idx"" ];. then. echo -e ""ref_idx $ref_idx exists!\n"". which ls. echo -e ""ls -al --block=M ${ref_idx}*\n"". ls -al --block=M ""${ref_idx}*"". echo -e ""ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*\n"". ls -al --block=M ""/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*"". echo -e ""/bin/ls -al --block=M ${ref_idx}*\n"". /bin/ls -al --block=M ""${ref_idx}*"". else. echo -e ""Warning!! ref_idx [$ref_idx] not exist!\n"". fi. ls -al ""${ref_idx}*"". singularity run $dvsif ls $ref_idx. ```. Here is the running output:. ```shell. $ bash scripts/02_run_deepvariant.sh . ref_idx is /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. ref_idx /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa exists! /usr/bin/ls. ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. ls: cannot access /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*: No such file or directory. ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:1931,availability,echo,echo,1931,"26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann. -rw-rw-r-- 1 zhoujianglin zhoujianglin 9725M Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai. -rw-rw-r-- 1 zhoujianglin zhoujianglin 749M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac. ```. However, If I run the follow bash script, it can not find `ref_idx` through `ls` command, whereas shell condition expression ` [ -f $ref_idx ]` return `true`. here is the script:. ```shell. #!/bin/bash. dvsif=""/lustre/Data/toolsDB//deepvariant.sif"". ref_idx=""/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa"". wkdir=""/lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP"". bamdir=""${wkdir}/mappinged_bams"". logdir=""${wkdir}/logs"". vcfoutdir=""${wkdir}/DeepVariant_outputs"". source activate ~/.conda/envs/zjlEnv. echo -e ""ref_idx is $ref_idx\n"". if [ -f ""$ref_idx"" ];. then. echo -e ""ref_idx $ref_idx exists!\n"". which ls. echo -e ""ls -al --block=M ${ref_idx}*\n"". ls -al --block=M ""${ref_idx}*"". echo -e ""ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*\n"". ls -al --block=M ""/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*"". echo -e ""/bin/ls -al --block=M ${ref_idx}*\n"". /bin/ls -al --block=M ""${ref_idx}*"". else. echo -e ""Warning!! ref_idx [$ref_idx] not exist!\n"". fi. ls -al ""${ref_idx}*"". singularity run $dvsif ls $ref_idx. ```. Here is the running output:. ```shell. $ bash scripts/02_run_deepvariant.sh . ref_idx is /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. ref_idx /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa exists! /usr/bin/ls. ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. ls: cannot access /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*: No such file or directory. ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. ls: cannot access /lustre/Data/toolsDB/Host",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:1979,availability,echo,echo,1979,"7d5/hs37d5.fa.ann. -rw-rw-r-- 1 zhoujianglin zhoujianglin 9725M Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai. -rw-rw-r-- 1 zhoujianglin zhoujianglin 749M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac. ```. However, If I run the follow bash script, it can not find `ref_idx` through `ls` command, whereas shell condition expression ` [ -f $ref_idx ]` return `true`. here is the script:. ```shell. #!/bin/bash. dvsif=""/lustre/Data/toolsDB//deepvariant.sif"". ref_idx=""/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa"". wkdir=""/lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP"". bamdir=""${wkdir}/mappinged_bams"". logdir=""${wkdir}/logs"". vcfoutdir=""${wkdir}/DeepVariant_outputs"". source activate ~/.conda/envs/zjlEnv. echo -e ""ref_idx is $ref_idx\n"". if [ -f ""$ref_idx"" ];. then. echo -e ""ref_idx $ref_idx exists!\n"". which ls. echo -e ""ls -al --block=M ${ref_idx}*\n"". ls -al --block=M ""${ref_idx}*"". echo -e ""ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*\n"". ls -al --block=M ""/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*"". echo -e ""/bin/ls -al --block=M ${ref_idx}*\n"". /bin/ls -al --block=M ""${ref_idx}*"". else. echo -e ""Warning!! ref_idx [$ref_idx] not exist!\n"". fi. ls -al ""${ref_idx}*"". singularity run $dvsif ls $ref_idx. ```. Here is the running output:. ```shell. $ bash scripts/02_run_deepvariant.sh . ref_idx is /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. ref_idx /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa exists! /usr/bin/ls. ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. ls: cannot access /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*: No such file or directory. ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. ls: cannot access /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*: No such file or di",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:2053,availability,echo,echo,2053,"42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai. -rw-rw-r-- 1 zhoujianglin zhoujianglin 749M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac. ```. However, If I run the follow bash script, it can not find `ref_idx` through `ls` command, whereas shell condition expression ` [ -f $ref_idx ]` return `true`. here is the script:. ```shell. #!/bin/bash. dvsif=""/lustre/Data/toolsDB//deepvariant.sif"". ref_idx=""/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa"". wkdir=""/lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP"". bamdir=""${wkdir}/mappinged_bams"". logdir=""${wkdir}/logs"". vcfoutdir=""${wkdir}/DeepVariant_outputs"". source activate ~/.conda/envs/zjlEnv. echo -e ""ref_idx is $ref_idx\n"". if [ -f ""$ref_idx"" ];. then. echo -e ""ref_idx $ref_idx exists!\n"". which ls. echo -e ""ls -al --block=M ${ref_idx}*\n"". ls -al --block=M ""${ref_idx}*"". echo -e ""ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*\n"". ls -al --block=M ""/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*"". echo -e ""/bin/ls -al --block=M ${ref_idx}*\n"". /bin/ls -al --block=M ""${ref_idx}*"". else. echo -e ""Warning!! ref_idx [$ref_idx] not exist!\n"". fi. ls -al ""${ref_idx}*"". singularity run $dvsif ls $ref_idx. ```. Here is the running output:. ```shell. $ bash scripts/02_run_deepvariant.sh . ref_idx is /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. ref_idx /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa exists! /usr/bin/ls. ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. ls: cannot access /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*: No such file or directory. ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. ls: cannot access /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*: No such file or directory. /bin/ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:2211,availability,echo,echo,2211,"n_hs37d5/hs37d5.fa.fai. -rw-rw-r-- 1 zhoujianglin zhoujianglin 749M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac. ```. However, If I run the follow bash script, it can not find `ref_idx` through `ls` command, whereas shell condition expression ` [ -f $ref_idx ]` return `true`. here is the script:. ```shell. #!/bin/bash. dvsif=""/lustre/Data/toolsDB//deepvariant.sif"". ref_idx=""/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa"". wkdir=""/lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP"". bamdir=""${wkdir}/mappinged_bams"". logdir=""${wkdir}/logs"". vcfoutdir=""${wkdir}/DeepVariant_outputs"". source activate ~/.conda/envs/zjlEnv. echo -e ""ref_idx is $ref_idx\n"". if [ -f ""$ref_idx"" ];. then. echo -e ""ref_idx $ref_idx exists!\n"". which ls. echo -e ""ls -al --block=M ${ref_idx}*\n"". ls -al --block=M ""${ref_idx}*"". echo -e ""ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*\n"". ls -al --block=M ""/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*"". echo -e ""/bin/ls -al --block=M ${ref_idx}*\n"". /bin/ls -al --block=M ""${ref_idx}*"". else. echo -e ""Warning!! ref_idx [$ref_idx] not exist!\n"". fi. ls -al ""${ref_idx}*"". singularity run $dvsif ls $ref_idx. ```. Here is the running output:. ```shell. $ bash scripts/02_run_deepvariant.sh . ref_idx is /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. ref_idx /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa exists! /usr/bin/ls. ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. ls: cannot access /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*: No such file or directory. ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. ls: cannot access /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*: No such file or directory. /bin/ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. /bin/ls: cannot access /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*: No such file or directory. ls: cannot access /lustre/Data/toolsDB/Ho",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:2301,availability,echo,echo,2301,"ata/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac. ```. However, If I run the follow bash script, it can not find `ref_idx` through `ls` command, whereas shell condition expression ` [ -f $ref_idx ]` return `true`. here is the script:. ```shell. #!/bin/bash. dvsif=""/lustre/Data/toolsDB//deepvariant.sif"". ref_idx=""/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa"". wkdir=""/lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP"". bamdir=""${wkdir}/mappinged_bams"". logdir=""${wkdir}/logs"". vcfoutdir=""${wkdir}/DeepVariant_outputs"". source activate ~/.conda/envs/zjlEnv. echo -e ""ref_idx is $ref_idx\n"". if [ -f ""$ref_idx"" ];. then. echo -e ""ref_idx $ref_idx exists!\n"". which ls. echo -e ""ls -al --block=M ${ref_idx}*\n"". ls -al --block=M ""${ref_idx}*"". echo -e ""ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*\n"". ls -al --block=M ""/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*"". echo -e ""/bin/ls -al --block=M ${ref_idx}*\n"". /bin/ls -al --block=M ""${ref_idx}*"". else. echo -e ""Warning!! ref_idx [$ref_idx] not exist!\n"". fi. ls -al ""${ref_idx}*"". singularity run $dvsif ls $ref_idx. ```. Here is the running output:. ```shell. $ bash scripts/02_run_deepvariant.sh . ref_idx is /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. ref_idx /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa exists! /usr/bin/ls. ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. ls: cannot access /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*: No such file or directory. ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. ls: cannot access /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*: No such file or directory. /bin/ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. /bin/ls: cannot access /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*: No such file or directory. ls: cannot access /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*: No such file or directory. INFO: Converting SIF file to te",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:1765,deployability,log,logdir,1765,"0123. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann. -rw-rw-r-- 1 zhoujianglin zhoujianglin 9725M Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai. -rw-rw-r-- 1 zhoujianglin zhoujianglin 749M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac. ```. However, If I run the follow bash script, it can not find `ref_idx` through `ls` command, whereas shell condition expression ` [ -f $ref_idx ]` return `true`. here is the script:. ```shell. #!/bin/bash. dvsif=""/lustre/Data/toolsDB//deepvariant.sif"". ref_idx=""/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa"". wkdir=""/lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP"". bamdir=""${wkdir}/mappinged_bams"". logdir=""${wkdir}/logs"". vcfoutdir=""${wkdir}/DeepVariant_outputs"". source activate ~/.conda/envs/zjlEnv. echo -e ""ref_idx is $ref_idx\n"". if [ -f ""$ref_idx"" ];. then. echo -e ""ref_idx $ref_idx exists!\n"". which ls. echo -e ""ls -al --block=M ${ref_idx}*\n"". ls -al --block=M ""${ref_idx}*"". echo -e ""ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*\n"". ls -al --block=M ""/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*"". echo -e ""/bin/ls -al --block=M ${ref_idx}*\n"". /bin/ls -al --block=M ""${ref_idx}*"". else. echo -e ""Warning!! ref_idx [$ref_idx] not exist!\n"". fi. ls -al ""${ref_idx}*"". singularity run $dvsif ls $ref_idx. ```. Here is the running output:. ```shell. $ bash scripts/02_run_deepvariant.sh . ref_idx is /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. ref_idx /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa exists! /usr/bin/ls. ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. ls: cannot access /lustre/Data/toolsDB/HostRefs/Hu",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:1782,deployability,log,logs,1782," 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann. -rw-rw-r-- 1 zhoujianglin zhoujianglin 9725M Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai. -rw-rw-r-- 1 zhoujianglin zhoujianglin 749M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac. ```. However, If I run the follow bash script, it can not find `ref_idx` through `ls` command, whereas shell condition expression ` [ -f $ref_idx ]` return `true`. here is the script:. ```shell. #!/bin/bash. dvsif=""/lustre/Data/toolsDB//deepvariant.sif"". ref_idx=""/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa"". wkdir=""/lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP"". bamdir=""${wkdir}/mappinged_bams"". logdir=""${wkdir}/logs"". vcfoutdir=""${wkdir}/DeepVariant_outputs"". source activate ~/.conda/envs/zjlEnv. echo -e ""ref_idx is $ref_idx\n"". if [ -f ""$ref_idx"" ];. then. echo -e ""ref_idx $ref_idx exists!\n"". which ls. echo -e ""ls -al --block=M ${ref_idx}*\n"". ls -al --block=M ""${ref_idx}*"". echo -e ""ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*\n"". ls -al --block=M ""/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*"". echo -e ""/bin/ls -al --block=M ${ref_idx}*\n"". /bin/ls -al --block=M ""${ref_idx}*"". else. echo -e ""Warning!! ref_idx [$ref_idx] not exist!\n"". fi. ls -al ""${ref_idx}*"". singularity run $dvsif ls $ref_idx. ```. Here is the running output:. ```shell. $ bash scripts/02_run_deepvariant.sh . ref_idx is /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. ref_idx /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa exists! /usr/bin/ls. ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. ls: cannot access /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:1347,modifiability,pac,pac,1347," file and `bwa-mem2` index and `fasta index fai` files do exist.):. ```shell. (base) [zhoujianglin@master 2304GQS_FSZ_SNP]$ ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. -rw-rw-r-- 1 zhoujianglin zhoujianglin 3042M Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. -rw-rw-r-- 1 zhoujianglin zhoujianglin 5985M Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann. -rw-rw-r-- 1 zhoujianglin zhoujianglin 9725M Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai. -rw-rw-r-- 1 zhoujianglin zhoujianglin 749M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac. ```. However, If I run the follow bash script, it can not find `ref_idx` through `ls` command, whereas shell condition expression ` [ -f $ref_idx ]` return `true`. here is the script:. ```shell. #!/bin/bash. dvsif=""/lustre/Data/toolsDB//deepvariant.sif"". ref_idx=""/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa"". wkdir=""/lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP"". bamdir=""${wkdir}/mappinged_bams"". logdir=""${wkdir}/logs"". vcfoutdir=""${wkdir}/DeepVariant_outputs"". source activate ~/.conda/envs/zjlEnv. echo -e ""ref_idx is $ref_idx\n"". if [ -f ""$ref_idx"" ];. then. echo -e ""ref_idx $ref_idx exists!\n"". which ls. echo -e ""ls -al --block=M ${ref_idx}*\n"". ls -al --block=M ""${ref_idx}*"". echo -e ""ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*\n"". ls -al --block=M ""/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*"". echo -e ""/bin/ls -al --block=M ${ref_idx}*\n"". /bin/ls -al --block=M ""${ref_idx}*"". else. echo -e ""Warning!! ref_idx [$ref_idx] not exist!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:1765,safety,log,logdir,1765,"0123. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann. -rw-rw-r-- 1 zhoujianglin zhoujianglin 9725M Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai. -rw-rw-r-- 1 zhoujianglin zhoujianglin 749M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac. ```. However, If I run the follow bash script, it can not find `ref_idx` through `ls` command, whereas shell condition expression ` [ -f $ref_idx ]` return `true`. here is the script:. ```shell. #!/bin/bash. dvsif=""/lustre/Data/toolsDB//deepvariant.sif"". ref_idx=""/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa"". wkdir=""/lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP"". bamdir=""${wkdir}/mappinged_bams"". logdir=""${wkdir}/logs"". vcfoutdir=""${wkdir}/DeepVariant_outputs"". source activate ~/.conda/envs/zjlEnv. echo -e ""ref_idx is $ref_idx\n"". if [ -f ""$ref_idx"" ];. then. echo -e ""ref_idx $ref_idx exists!\n"". which ls. echo -e ""ls -al --block=M ${ref_idx}*\n"". ls -al --block=M ""${ref_idx}*"". echo -e ""ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*\n"". ls -al --block=M ""/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*"". echo -e ""/bin/ls -al --block=M ${ref_idx}*\n"". /bin/ls -al --block=M ""${ref_idx}*"". else. echo -e ""Warning!! ref_idx [$ref_idx] not exist!\n"". fi. ls -al ""${ref_idx}*"". singularity run $dvsif ls $ref_idx. ```. Here is the running output:. ```shell. $ bash scripts/02_run_deepvariant.sh . ref_idx is /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. ref_idx /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa exists! /usr/bin/ls. ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. ls: cannot access /lustre/Data/toolsDB/HostRefs/Hu",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:1782,safety,log,logs,1782," 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann. -rw-rw-r-- 1 zhoujianglin zhoujianglin 9725M Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai. -rw-rw-r-- 1 zhoujianglin zhoujianglin 749M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac. ```. However, If I run the follow bash script, it can not find `ref_idx` through `ls` command, whereas shell condition expression ` [ -f $ref_idx ]` return `true`. here is the script:. ```shell. #!/bin/bash. dvsif=""/lustre/Data/toolsDB//deepvariant.sif"". ref_idx=""/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa"". wkdir=""/lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP"". bamdir=""${wkdir}/mappinged_bams"". logdir=""${wkdir}/logs"". vcfoutdir=""${wkdir}/DeepVariant_outputs"". source activate ~/.conda/envs/zjlEnv. echo -e ""ref_idx is $ref_idx\n"". if [ -f ""$ref_idx"" ];. then. echo -e ""ref_idx $ref_idx exists!\n"". which ls. echo -e ""ls -al --block=M ${ref_idx}*\n"". ls -al --block=M ""${ref_idx}*"". echo -e ""ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*\n"". ls -al --block=M ""/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*"". echo -e ""/bin/ls -al --block=M ${ref_idx}*\n"". /bin/ls -al --block=M ""${ref_idx}*"". else. echo -e ""Warning!! ref_idx [$ref_idx] not exist!\n"". fi. ls -al ""${ref_idx}*"". singularity run $dvsif ls $ref_idx. ```. Here is the running output:. ```shell. $ bash scripts/02_run_deepvariant.sh . ref_idx is /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. ref_idx /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa exists! /usr/bin/ls. ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. ls: cannot access /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:1765,security,log,logdir,1765,"0123. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann. -rw-rw-r-- 1 zhoujianglin zhoujianglin 9725M Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai. -rw-rw-r-- 1 zhoujianglin zhoujianglin 749M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac. ```. However, If I run the follow bash script, it can not find `ref_idx` through `ls` command, whereas shell condition expression ` [ -f $ref_idx ]` return `true`. here is the script:. ```shell. #!/bin/bash. dvsif=""/lustre/Data/toolsDB//deepvariant.sif"". ref_idx=""/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa"". wkdir=""/lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP"". bamdir=""${wkdir}/mappinged_bams"". logdir=""${wkdir}/logs"". vcfoutdir=""${wkdir}/DeepVariant_outputs"". source activate ~/.conda/envs/zjlEnv. echo -e ""ref_idx is $ref_idx\n"". if [ -f ""$ref_idx"" ];. then. echo -e ""ref_idx $ref_idx exists!\n"". which ls. echo -e ""ls -al --block=M ${ref_idx}*\n"". ls -al --block=M ""${ref_idx}*"". echo -e ""ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*\n"". ls -al --block=M ""/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*"". echo -e ""/bin/ls -al --block=M ${ref_idx}*\n"". /bin/ls -al --block=M ""${ref_idx}*"". else. echo -e ""Warning!! ref_idx [$ref_idx] not exist!\n"". fi. ls -al ""${ref_idx}*"". singularity run $dvsif ls $ref_idx. ```. Here is the running output:. ```shell. $ bash scripts/02_run_deepvariant.sh . ref_idx is /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. ref_idx /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa exists! /usr/bin/ls. ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. ls: cannot access /lustre/Data/toolsDB/HostRefs/Hu",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:1782,security,log,logs,1782," 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann. -rw-rw-r-- 1 zhoujianglin zhoujianglin 9725M Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai. -rw-rw-r-- 1 zhoujianglin zhoujianglin 749M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac. ```. However, If I run the follow bash script, it can not find `ref_idx` through `ls` command, whereas shell condition expression ` [ -f $ref_idx ]` return `true`. here is the script:. ```shell. #!/bin/bash. dvsif=""/lustre/Data/toolsDB//deepvariant.sif"". ref_idx=""/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa"". wkdir=""/lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP"". bamdir=""${wkdir}/mappinged_bams"". logdir=""${wkdir}/logs"". vcfoutdir=""${wkdir}/DeepVariant_outputs"". source activate ~/.conda/envs/zjlEnv. echo -e ""ref_idx is $ref_idx\n"". if [ -f ""$ref_idx"" ];. then. echo -e ""ref_idx $ref_idx exists!\n"". which ls. echo -e ""ls -al --block=M ${ref_idx}*\n"". ls -al --block=M ""${ref_idx}*"". echo -e ""ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*\n"". ls -al --block=M ""/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*"". echo -e ""/bin/ls -al --block=M ${ref_idx}*\n"". /bin/ls -al --block=M ""${ref_idx}*"". else. echo -e ""Warning!! ref_idx [$ref_idx] not exist!\n"". fi. ls -al ""${ref_idx}*"". singularity run $dvsif ls $ref_idx. ```. Here is the running output:. ```shell. $ bash scripts/02_run_deepvariant.sh . ref_idx is /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. ref_idx /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa exists! /usr/bin/ls. ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. ls: cannot access /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:2729,security,access,access,2729,"$ref_idx ]` return `true`. here is the script:. ```shell. #!/bin/bash. dvsif=""/lustre/Data/toolsDB//deepvariant.sif"". ref_idx=""/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa"". wkdir=""/lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP"". bamdir=""${wkdir}/mappinged_bams"". logdir=""${wkdir}/logs"". vcfoutdir=""${wkdir}/DeepVariant_outputs"". source activate ~/.conda/envs/zjlEnv. echo -e ""ref_idx is $ref_idx\n"". if [ -f ""$ref_idx"" ];. then. echo -e ""ref_idx $ref_idx exists!\n"". which ls. echo -e ""ls -al --block=M ${ref_idx}*\n"". ls -al --block=M ""${ref_idx}*"". echo -e ""ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*\n"". ls -al --block=M ""/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*"". echo -e ""/bin/ls -al --block=M ${ref_idx}*\n"". /bin/ls -al --block=M ""${ref_idx}*"". else. echo -e ""Warning!! ref_idx [$ref_idx] not exist!\n"". fi. ls -al ""${ref_idx}*"". singularity run $dvsif ls $ref_idx. ```. Here is the running output:. ```shell. $ bash scripts/02_run_deepvariant.sh . ref_idx is /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. ref_idx /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa exists! /usr/bin/ls. ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. ls: cannot access /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*: No such file or directory. ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. ls: cannot access /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*: No such file or directory. /bin/ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. /bin/ls: cannot access /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*: No such file or directory. ls: cannot access /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*: No such file or directory. INFO: Converting SIF file to temporary sandbox... /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa': No such file or directory. INFO: Cleaning up image... ```. Could you please help me?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:2901,security,access,access,2901,"$ref_idx ]` return `true`. here is the script:. ```shell. #!/bin/bash. dvsif=""/lustre/Data/toolsDB//deepvariant.sif"". ref_idx=""/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa"". wkdir=""/lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP"". bamdir=""${wkdir}/mappinged_bams"". logdir=""${wkdir}/logs"". vcfoutdir=""${wkdir}/DeepVariant_outputs"". source activate ~/.conda/envs/zjlEnv. echo -e ""ref_idx is $ref_idx\n"". if [ -f ""$ref_idx"" ];. then. echo -e ""ref_idx $ref_idx exists!\n"". which ls. echo -e ""ls -al --block=M ${ref_idx}*\n"". ls -al --block=M ""${ref_idx}*"". echo -e ""ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*\n"". ls -al --block=M ""/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*"". echo -e ""/bin/ls -al --block=M ${ref_idx}*\n"". /bin/ls -al --block=M ""${ref_idx}*"". else. echo -e ""Warning!! ref_idx [$ref_idx] not exist!\n"". fi. ls -al ""${ref_idx}*"". singularity run $dvsif ls $ref_idx. ```. Here is the running output:. ```shell. $ bash scripts/02_run_deepvariant.sh . ref_idx is /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. ref_idx /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa exists! /usr/bin/ls. ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. ls: cannot access /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*: No such file or directory. ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. ls: cannot access /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*: No such file or directory. /bin/ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. /bin/ls: cannot access /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*: No such file or directory. ls: cannot access /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*: No such file or directory. INFO: Converting SIF file to temporary sandbox... /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa': No such file or directory. INFO: Cleaning up image... ```. Could you please help me?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:3083,security,access,access,3083,"$ref_idx ]` return `true`. here is the script:. ```shell. #!/bin/bash. dvsif=""/lustre/Data/toolsDB//deepvariant.sif"". ref_idx=""/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa"". wkdir=""/lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP"". bamdir=""${wkdir}/mappinged_bams"". logdir=""${wkdir}/logs"". vcfoutdir=""${wkdir}/DeepVariant_outputs"". source activate ~/.conda/envs/zjlEnv. echo -e ""ref_idx is $ref_idx\n"". if [ -f ""$ref_idx"" ];. then. echo -e ""ref_idx $ref_idx exists!\n"". which ls. echo -e ""ls -al --block=M ${ref_idx}*\n"". ls -al --block=M ""${ref_idx}*"". echo -e ""ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*\n"". ls -al --block=M ""/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*"". echo -e ""/bin/ls -al --block=M ${ref_idx}*\n"". /bin/ls -al --block=M ""${ref_idx}*"". else. echo -e ""Warning!! ref_idx [$ref_idx] not exist!\n"". fi. ls -al ""${ref_idx}*"". singularity run $dvsif ls $ref_idx. ```. Here is the running output:. ```shell. $ bash scripts/02_run_deepvariant.sh . ref_idx is /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. ref_idx /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa exists! /usr/bin/ls. ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. ls: cannot access /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*: No such file or directory. ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. ls: cannot access /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*: No such file or directory. /bin/ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. /bin/ls: cannot access /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*: No such file or directory. ls: cannot access /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*: No such file or directory. INFO: Converting SIF file to temporary sandbox... /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa': No such file or directory. INFO: Cleaning up image... ```. Could you please help me?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:3183,security,access,access,3183,"$ref_idx ]` return `true`. here is the script:. ```shell. #!/bin/bash. dvsif=""/lustre/Data/toolsDB//deepvariant.sif"". ref_idx=""/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa"". wkdir=""/lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP"". bamdir=""${wkdir}/mappinged_bams"". logdir=""${wkdir}/logs"". vcfoutdir=""${wkdir}/DeepVariant_outputs"". source activate ~/.conda/envs/zjlEnv. echo -e ""ref_idx is $ref_idx\n"". if [ -f ""$ref_idx"" ];. then. echo -e ""ref_idx $ref_idx exists!\n"". which ls. echo -e ""ls -al --block=M ${ref_idx}*\n"". ls -al --block=M ""${ref_idx}*"". echo -e ""ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*\n"". ls -al --block=M ""/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*"". echo -e ""/bin/ls -al --block=M ${ref_idx}*\n"". /bin/ls -al --block=M ""${ref_idx}*"". else. echo -e ""Warning!! ref_idx [$ref_idx] not exist!\n"". fi. ls -al ""${ref_idx}*"". singularity run $dvsif ls $ref_idx. ```. Here is the running output:. ```shell. $ bash scripts/02_run_deepvariant.sh . ref_idx is /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. ref_idx /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa exists! /usr/bin/ls. ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. ls: cannot access /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*: No such file or directory. ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. ls: cannot access /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*: No such file or directory. /bin/ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. /bin/ls: cannot access /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*: No such file or directory. ls: cannot access /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*: No such file or directory. INFO: Converting SIF file to temporary sandbox... /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa': No such file or directory. INFO: Cleaning up image... ```. Could you please help me?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:3311,security,sandbox,sandbox,3311,"$ref_idx ]` return `true`. here is the script:. ```shell. #!/bin/bash. dvsif=""/lustre/Data/toolsDB//deepvariant.sif"". ref_idx=""/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa"". wkdir=""/lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP"". bamdir=""${wkdir}/mappinged_bams"". logdir=""${wkdir}/logs"". vcfoutdir=""${wkdir}/DeepVariant_outputs"". source activate ~/.conda/envs/zjlEnv. echo -e ""ref_idx is $ref_idx\n"". if [ -f ""$ref_idx"" ];. then. echo -e ""ref_idx $ref_idx exists!\n"". which ls. echo -e ""ls -al --block=M ${ref_idx}*\n"". ls -al --block=M ""${ref_idx}*"". echo -e ""ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*\n"". ls -al --block=M ""/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*"". echo -e ""/bin/ls -al --block=M ${ref_idx}*\n"". /bin/ls -al --block=M ""${ref_idx}*"". else. echo -e ""Warning!! ref_idx [$ref_idx] not exist!\n"". fi. ls -al ""${ref_idx}*"". singularity run $dvsif ls $ref_idx. ```. Here is the running output:. ```shell. $ bash scripts/02_run_deepvariant.sh . ref_idx is /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. ref_idx /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa exists! /usr/bin/ls. ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. ls: cannot access /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*: No such file or directory. ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. ls: cannot access /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*: No such file or directory. /bin/ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. /bin/ls: cannot access /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*: No such file or directory. ls: cannot access /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*: No such file or directory. INFO: Converting SIF file to temporary sandbox... /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa': No such file or directory. INFO: Cleaning up image... ```. Could you please help me?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:3342,security,access,access,3342,"$ref_idx ]` return `true`. here is the script:. ```shell. #!/bin/bash. dvsif=""/lustre/Data/toolsDB//deepvariant.sif"". ref_idx=""/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa"". wkdir=""/lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP"". bamdir=""${wkdir}/mappinged_bams"". logdir=""${wkdir}/logs"". vcfoutdir=""${wkdir}/DeepVariant_outputs"". source activate ~/.conda/envs/zjlEnv. echo -e ""ref_idx is $ref_idx\n"". if [ -f ""$ref_idx"" ];. then. echo -e ""ref_idx $ref_idx exists!\n"". which ls. echo -e ""ls -al --block=M ${ref_idx}*\n"". ls -al --block=M ""${ref_idx}*"". echo -e ""ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*\n"". ls -al --block=M ""/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*"". echo -e ""/bin/ls -al --block=M ${ref_idx}*\n"". /bin/ls -al --block=M ""${ref_idx}*"". else. echo -e ""Warning!! ref_idx [$ref_idx] not exist!\n"". fi. ls -al ""${ref_idx}*"". singularity run $dvsif ls $ref_idx. ```. Here is the running output:. ```shell. $ bash scripts/02_run_deepvariant.sh . ref_idx is /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. ref_idx /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa exists! /usr/bin/ls. ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. ls: cannot access /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*: No such file or directory. ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. ls: cannot access /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*: No such file or directory. /bin/ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. /bin/ls: cannot access /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*: No such file or directory. ls: cannot access /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*: No such file or directory. INFO: Converting SIF file to temporary sandbox... /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa': No such file or directory. INFO: Cleaning up image... ```. Could you please help me?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:1765,testability,log,logdir,1765,"0123. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann. -rw-rw-r-- 1 zhoujianglin zhoujianglin 9725M Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai. -rw-rw-r-- 1 zhoujianglin zhoujianglin 749M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac. ```. However, If I run the follow bash script, it can not find `ref_idx` through `ls` command, whereas shell condition expression ` [ -f $ref_idx ]` return `true`. here is the script:. ```shell. #!/bin/bash. dvsif=""/lustre/Data/toolsDB//deepvariant.sif"". ref_idx=""/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa"". wkdir=""/lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP"". bamdir=""${wkdir}/mappinged_bams"". logdir=""${wkdir}/logs"". vcfoutdir=""${wkdir}/DeepVariant_outputs"". source activate ~/.conda/envs/zjlEnv. echo -e ""ref_idx is $ref_idx\n"". if [ -f ""$ref_idx"" ];. then. echo -e ""ref_idx $ref_idx exists!\n"". which ls. echo -e ""ls -al --block=M ${ref_idx}*\n"". ls -al --block=M ""${ref_idx}*"". echo -e ""ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*\n"". ls -al --block=M ""/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*"". echo -e ""/bin/ls -al --block=M ${ref_idx}*\n"". /bin/ls -al --block=M ""${ref_idx}*"". else. echo -e ""Warning!! ref_idx [$ref_idx] not exist!\n"". fi. ls -al ""${ref_idx}*"". singularity run $dvsif ls $ref_idx. ```. Here is the running output:. ```shell. $ bash scripts/02_run_deepvariant.sh . ref_idx is /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. ref_idx /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa exists! /usr/bin/ls. ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. ls: cannot access /lustre/Data/toolsDB/HostRefs/Hu",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:1782,testability,log,logs,1782," 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann. -rw-rw-r-- 1 zhoujianglin zhoujianglin 9725M Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai. -rw-rw-r-- 1 zhoujianglin zhoujianglin 749M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac. ```. However, If I run the follow bash script, it can not find `ref_idx` through `ls` command, whereas shell condition expression ` [ -f $ref_idx ]` return `true`. here is the script:. ```shell. #!/bin/bash. dvsif=""/lustre/Data/toolsDB//deepvariant.sif"". ref_idx=""/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa"". wkdir=""/lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP"". bamdir=""${wkdir}/mappinged_bams"". logdir=""${wkdir}/logs"". vcfoutdir=""${wkdir}/DeepVariant_outputs"". source activate ~/.conda/envs/zjlEnv. echo -e ""ref_idx is $ref_idx\n"". if [ -f ""$ref_idx"" ];. then. echo -e ""ref_idx $ref_idx exists!\n"". which ls. echo -e ""ls -al --block=M ${ref_idx}*\n"". ls -al --block=M ""${ref_idx}*"". echo -e ""ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*\n"". ls -al --block=M ""/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*"". echo -e ""/bin/ls -al --block=M ${ref_idx}*\n"". /bin/ls -al --block=M ""${ref_idx}*"". else. echo -e ""Warning!! ref_idx [$ref_idx] not exist!\n"". fi. ls -al ""${ref_idx}*"". singularity run $dvsif ls $ref_idx. ```. Here is the running output:. ```shell. $ bash scripts/02_run_deepvariant.sh . ref_idx is /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. ref_idx /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa exists! /usr/bin/ls. ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. ls: cannot access /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:140,usability,tool,toolsDB,140,"Hi @pichuan , thanks for your reply. I checked them as your suggestion and I found a *strange* phenomenon. the reference file `/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa` can be found through `ls` command in `bash Terminal`, but can not be found through `bash script`. Its's confusing. Here is the `Terminal`output (all the reference fasta file and `bwa-mem2` index and `fasta index fai` files do exist.):. ```shell. (base) [zhoujianglin@master 2304GQS_FSZ_SNP]$ ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. -rw-rw-r-- 1 zhoujianglin zhoujianglin 3042M Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. -rw-rw-r-- 1 zhoujianglin zhoujianglin 5985M Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann. -rw-rw-r-- 1 zhoujianglin zhoujianglin 9725M Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai. -rw-rw-r-- 1 zhoujianglin zhoujianglin 749M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac. ```. However, If I run the follow bash script, it can not find `ref_idx` through `ls` command, whereas shell condition expression ` [ -f $ref_idx ]` return `true`. here is the script:. ```shell. #!/bin/bash. dvsif=""/lustre/Data/toolsDB//deepvariant.sif"". ref_idx=""/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa"". wkdir=""/lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP"". bamdir=""${wkdir}/mappinged_bams"". logdir=""${wkdir}/logs"". vcfoutdir=""${wkdir}/DeepVariant_outputs"". source activate ~/.conda/envs/zjlEnv. echo -e ""ref_idx is $ref_idx\n"". if [ -f ""$ref_idx"" ];. then. echo -e ""ref_idx $ref_idx exists!\n"". which ls. echo -e ""ls -al --blo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:207,usability,command,command,207,"Hi @pichuan , thanks for your reply. I checked them as your suggestion and I found a *strange* phenomenon. the reference file `/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa` can be found through `ls` command in `bash Terminal`, but can not be found through `bash script`. Its's confusing. Here is the `Terminal`output (all the reference fasta file and `bwa-mem2` index and `fasta index fai` files do exist.):. ```shell. (base) [zhoujianglin@master 2304GQS_FSZ_SNP]$ ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. -rw-rw-r-- 1 zhoujianglin zhoujianglin 3042M Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. -rw-rw-r-- 1 zhoujianglin zhoujianglin 5985M Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann. -rw-rw-r-- 1 zhoujianglin zhoujianglin 9725M Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai. -rw-rw-r-- 1 zhoujianglin zhoujianglin 749M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac. ```. However, If I run the follow bash script, it can not find `ref_idx` through `ls` command, whereas shell condition expression ` [ -f $ref_idx ]` return `true`. here is the script:. ```shell. #!/bin/bash. dvsif=""/lustre/Data/toolsDB//deepvariant.sif"". ref_idx=""/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa"". wkdir=""/lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP"". bamdir=""${wkdir}/mappinged_bams"". logdir=""${wkdir}/logs"". vcfoutdir=""${wkdir}/DeepVariant_outputs"". source activate ~/.conda/envs/zjlEnv. echo -e ""ref_idx is $ref_idx\n"". if [ -f ""$ref_idx"" ];. then. echo -e ""ref_idx $ref_idx exists!\n"". which ls. echo -e ""ls -al --blo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:503,usability,tool,toolsDB,503,"Hi @pichuan , thanks for your reply. I checked them as your suggestion and I found a *strange* phenomenon. the reference file `/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa` can be found through `ls` command in `bash Terminal`, but can not be found through `bash script`. Its's confusing. Here is the `Terminal`output (all the reference fasta file and `bwa-mem2` index and `fasta index fai` files do exist.):. ```shell. (base) [zhoujianglin@master 2304GQS_FSZ_SNP]$ ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. -rw-rw-r-- 1 zhoujianglin zhoujianglin 3042M Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. -rw-rw-r-- 1 zhoujianglin zhoujianglin 5985M Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann. -rw-rw-r-- 1 zhoujianglin zhoujianglin 9725M Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai. -rw-rw-r-- 1 zhoujianglin zhoujianglin 749M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac. ```. However, If I run the follow bash script, it can not find `ref_idx` through `ls` command, whereas shell condition expression ` [ -f $ref_idx ]` return `true`. here is the script:. ```shell. #!/bin/bash. dvsif=""/lustre/Data/toolsDB//deepvariant.sif"". ref_idx=""/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa"". wkdir=""/lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP"". bamdir=""${wkdir}/mappinged_bams"". logdir=""${wkdir}/logs"". vcfoutdir=""${wkdir}/DeepVariant_outputs"". source activate ~/.conda/envs/zjlEnv. echo -e ""ref_idx is $ref_idx\n"". if [ -f ""$ref_idx"" ];. then. echo -e ""ref_idx $ref_idx exists!\n"". which ls. echo -e ""ls -al --blo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:616,usability,tool,toolsDB,616,"Hi @pichuan , thanks for your reply. I checked them as your suggestion and I found a *strange* phenomenon. the reference file `/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa` can be found through `ls` command in `bash Terminal`, but can not be found through `bash script`. Its's confusing. Here is the `Terminal`output (all the reference fasta file and `bwa-mem2` index and `fasta index fai` files do exist.):. ```shell. (base) [zhoujianglin@master 2304GQS_FSZ_SNP]$ ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. -rw-rw-r-- 1 zhoujianglin zhoujianglin 3042M Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. -rw-rw-r-- 1 zhoujianglin zhoujianglin 5985M Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann. -rw-rw-r-- 1 zhoujianglin zhoujianglin 9725M Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai. -rw-rw-r-- 1 zhoujianglin zhoujianglin 749M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac. ```. However, If I run the follow bash script, it can not find `ref_idx` through `ls` command, whereas shell condition expression ` [ -f $ref_idx ]` return `true`. here is the script:. ```shell. #!/bin/bash. dvsif=""/lustre/Data/toolsDB//deepvariant.sif"". ref_idx=""/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa"". wkdir=""/lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP"". bamdir=""${wkdir}/mappinged_bams"". logdir=""${wkdir}/logs"". vcfoutdir=""${wkdir}/DeepVariant_outputs"". source activate ~/.conda/envs/zjlEnv. echo -e ""ref_idx is $ref_idx\n"". if [ -f ""$ref_idx"" ];. then. echo -e ""ref_idx $ref_idx exists!\n"". which ls. echo -e ""ls -al --blo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:728,usability,tool,toolsDB,728,"Hi @pichuan , thanks for your reply. I checked them as your suggestion and I found a *strange* phenomenon. the reference file `/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa` can be found through `ls` command in `bash Terminal`, but can not be found through `bash script`. Its's confusing. Here is the `Terminal`output (all the reference fasta file and `bwa-mem2` index and `fasta index fai` files do exist.):. ```shell. (base) [zhoujianglin@master 2304GQS_FSZ_SNP]$ ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. -rw-rw-r-- 1 zhoujianglin zhoujianglin 3042M Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. -rw-rw-r-- 1 zhoujianglin zhoujianglin 5985M Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann. -rw-rw-r-- 1 zhoujianglin zhoujianglin 9725M Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai. -rw-rw-r-- 1 zhoujianglin zhoujianglin 749M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac. ```. However, If I run the follow bash script, it can not find `ref_idx` through `ls` command, whereas shell condition expression ` [ -f $ref_idx ]` return `true`. here is the script:. ```shell. #!/bin/bash. dvsif=""/lustre/Data/toolsDB//deepvariant.sif"". ref_idx=""/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa"". wkdir=""/lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP"". bamdir=""${wkdir}/mappinged_bams"". logdir=""${wkdir}/logs"". vcfoutdir=""${wkdir}/DeepVariant_outputs"". source activate ~/.conda/envs/zjlEnv. echo -e ""ref_idx is $ref_idx\n"". if [ -f ""$ref_idx"" ];. then. echo -e ""ref_idx $ref_idx exists!\n"". which ls. echo -e ""ls -al --blo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:842,usability,tool,toolsDB,842,"Hi @pichuan , thanks for your reply. I checked them as your suggestion and I found a *strange* phenomenon. the reference file `/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa` can be found through `ls` command in `bash Terminal`, but can not be found through `bash script`. Its's confusing. Here is the `Terminal`output (all the reference fasta file and `bwa-mem2` index and `fasta index fai` files do exist.):. ```shell. (base) [zhoujianglin@master 2304GQS_FSZ_SNP]$ ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. -rw-rw-r-- 1 zhoujianglin zhoujianglin 3042M Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. -rw-rw-r-- 1 zhoujianglin zhoujianglin 5985M Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann. -rw-rw-r-- 1 zhoujianglin zhoujianglin 9725M Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai. -rw-rw-r-- 1 zhoujianglin zhoujianglin 749M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac. ```. However, If I run the follow bash script, it can not find `ref_idx` through `ls` command, whereas shell condition expression ` [ -f $ref_idx ]` return `true`. here is the script:. ```shell. #!/bin/bash. dvsif=""/lustre/Data/toolsDB//deepvariant.sif"". ref_idx=""/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa"". wkdir=""/lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP"". bamdir=""${wkdir}/mappinged_bams"". logdir=""${wkdir}/logs"". vcfoutdir=""${wkdir}/DeepVariant_outputs"". source activate ~/.conda/envs/zjlEnv. echo -e ""ref_idx is $ref_idx\n"". if [ -f ""$ref_idx"" ];. then. echo -e ""ref_idx $ref_idx exists!\n"". which ls. echo -e ""ls -al --blo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:955,usability,tool,toolsDB,955,"Hi @pichuan , thanks for your reply. I checked them as your suggestion and I found a *strange* phenomenon. the reference file `/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa` can be found through `ls` command in `bash Terminal`, but can not be found through `bash script`. Its's confusing. Here is the `Terminal`output (all the reference fasta file and `bwa-mem2` index and `fasta index fai` files do exist.):. ```shell. (base) [zhoujianglin@master 2304GQS_FSZ_SNP]$ ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. -rw-rw-r-- 1 zhoujianglin zhoujianglin 3042M Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. -rw-rw-r-- 1 zhoujianglin zhoujianglin 5985M Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann. -rw-rw-r-- 1 zhoujianglin zhoujianglin 9725M Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai. -rw-rw-r-- 1 zhoujianglin zhoujianglin 749M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac. ```. However, If I run the follow bash script, it can not find `ref_idx` through `ls` command, whereas shell condition expression ` [ -f $ref_idx ]` return `true`. here is the script:. ```shell. #!/bin/bash. dvsif=""/lustre/Data/toolsDB//deepvariant.sif"". ref_idx=""/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa"". wkdir=""/lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP"". bamdir=""${wkdir}/mappinged_bams"". logdir=""${wkdir}/logs"". vcfoutdir=""${wkdir}/DeepVariant_outputs"". source activate ~/.conda/envs/zjlEnv. echo -e ""ref_idx is $ref_idx\n"". if [ -f ""$ref_idx"" ];. then. echo -e ""ref_idx $ref_idx exists!\n"". which ls. echo -e ""ls -al --blo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:1071,usability,tool,toolsDB,1071,"I found a *strange* phenomenon. the reference file `/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa` can be found through `ls` command in `bash Terminal`, but can not be found through `bash script`. Its's confusing. Here is the `Terminal`output (all the reference fasta file and `bwa-mem2` index and `fasta index fai` files do exist.):. ```shell. (base) [zhoujianglin@master 2304GQS_FSZ_SNP]$ ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. -rw-rw-r-- 1 zhoujianglin zhoujianglin 3042M Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. -rw-rw-r-- 1 zhoujianglin zhoujianglin 5985M Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann. -rw-rw-r-- 1 zhoujianglin zhoujianglin 9725M Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai. -rw-rw-r-- 1 zhoujianglin zhoujianglin 749M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac. ```. However, If I run the follow bash script, it can not find `ref_idx` through `ls` command, whereas shell condition expression ` [ -f $ref_idx ]` return `true`. here is the script:. ```shell. #!/bin/bash. dvsif=""/lustre/Data/toolsDB//deepvariant.sif"". ref_idx=""/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa"". wkdir=""/lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP"". bamdir=""${wkdir}/mappinged_bams"". logdir=""${wkdir}/logs"". vcfoutdir=""${wkdir}/DeepVariant_outputs"". source activate ~/.conda/envs/zjlEnv. echo -e ""ref_idx is $ref_idx\n"". if [ -f ""$ref_idx"" ];. then. echo -e ""ref_idx $ref_idx exists!\n"". which ls. echo -e ""ls -al --block=M ${ref_idx}*\n"". ls -al --block=M ""${ref_idx}*"". echo -e ""ls -al --bloc",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:1192,usability,tool,toolsDB,1192,"rough `ls` command in `bash Terminal`, but can not be found through `bash script`. Its's confusing. Here is the `Terminal`output (all the reference fasta file and `bwa-mem2` index and `fasta index fai` files do exist.):. ```shell. (base) [zhoujianglin@master 2304GQS_FSZ_SNP]$ ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. -rw-rw-r-- 1 zhoujianglin zhoujianglin 3042M Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. -rw-rw-r-- 1 zhoujianglin zhoujianglin 5985M Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann. -rw-rw-r-- 1 zhoujianglin zhoujianglin 9725M Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai. -rw-rw-r-- 1 zhoujianglin zhoujianglin 749M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac. ```. However, If I run the follow bash script, it can not find `ref_idx` through `ls` command, whereas shell condition expression ` [ -f $ref_idx ]` return `true`. here is the script:. ```shell. #!/bin/bash. dvsif=""/lustre/Data/toolsDB//deepvariant.sif"". ref_idx=""/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa"". wkdir=""/lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP"". bamdir=""${wkdir}/mappinged_bams"". logdir=""${wkdir}/logs"". vcfoutdir=""${wkdir}/DeepVariant_outputs"". source activate ~/.conda/envs/zjlEnv. echo -e ""ref_idx is $ref_idx\n"". if [ -f ""$ref_idx"" ];. then. echo -e ""ref_idx $ref_idx exists!\n"". which ls. echo -e ""ls -al --block=M ${ref_idx}*\n"". ls -al --block=M ""${ref_idx}*"". echo -e ""ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*\n"". ls -al --block=M ""/lustre/Data/toolsDB/HostRefs/Human_hs37d",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:1307,usability,tool,toolsDB,1307,"rminal`output (all the reference fasta file and `bwa-mem2` index and `fasta index fai` files do exist.):. ```shell. (base) [zhoujianglin@master 2304GQS_FSZ_SNP]$ ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. -rw-rw-r-- 1 zhoujianglin zhoujianglin 3042M Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. -rw-rw-r-- 1 zhoujianglin zhoujianglin 5985M Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann. -rw-rw-r-- 1 zhoujianglin zhoujianglin 9725M Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai. -rw-rw-r-- 1 zhoujianglin zhoujianglin 749M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac. ```. However, If I run the follow bash script, it can not find `ref_idx` through `ls` command, whereas shell condition expression ` [ -f $ref_idx ]` return `true`. here is the script:. ```shell. #!/bin/bash. dvsif=""/lustre/Data/toolsDB//deepvariant.sif"". ref_idx=""/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa"". wkdir=""/lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP"". bamdir=""${wkdir}/mappinged_bams"". logdir=""${wkdir}/logs"". vcfoutdir=""${wkdir}/DeepVariant_outputs"". source activate ~/.conda/envs/zjlEnv. echo -e ""ref_idx is $ref_idx\n"". if [ -f ""$ref_idx"" ];. then. echo -e ""ref_idx $ref_idx exists!\n"". which ls. echo -e ""ls -al --block=M ${ref_idx}*\n"". ls -al --block=M ""${ref_idx}*"". echo -e ""ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*\n"". ls -al --block=M ""/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*"". echo -e ""/bin/ls -al --block=M ${ref_idx}*\n"". /bin/ls -al --block=M ""${ref_idx}*"". else. echo -e ""W",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:1438,usability,command,command,1438,"nglin@master 2304GQS_FSZ_SNP]$ ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. -rw-rw-r-- 1 zhoujianglin zhoujianglin 3042M Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. -rw-rw-r-- 1 zhoujianglin zhoujianglin 5985M Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann. -rw-rw-r-- 1 zhoujianglin zhoujianglin 9725M Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai. -rw-rw-r-- 1 zhoujianglin zhoujianglin 749M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac. ```. However, If I run the follow bash script, it can not find `ref_idx` through `ls` command, whereas shell condition expression ` [ -f $ref_idx ]` return `true`. here is the script:. ```shell. #!/bin/bash. dvsif=""/lustre/Data/toolsDB//deepvariant.sif"". ref_idx=""/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa"". wkdir=""/lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP"". bamdir=""${wkdir}/mappinged_bams"". logdir=""${wkdir}/logs"". vcfoutdir=""${wkdir}/DeepVariant_outputs"". source activate ~/.conda/envs/zjlEnv. echo -e ""ref_idx is $ref_idx\n"". if [ -f ""$ref_idx"" ];. then. echo -e ""ref_idx $ref_idx exists!\n"". which ls. echo -e ""ls -al --block=M ${ref_idx}*\n"". ls -al --block=M ""${ref_idx}*"". echo -e ""ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*\n"". ls -al --block=M ""/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*"". echo -e ""/bin/ls -al --block=M ${ref_idx}*\n"". /bin/ls -al --block=M ""${ref_idx}*"". else. echo -e ""Warning!! ref_idx [$ref_idx] not exist!\n"". fi. ls -al ""${ref_idx}*"". singularity run $dvsif ls $ref_idx. ```. Here is the running o",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:1580,usability,tool,toolsDB,1580,"3042M Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. -rw-rw-r-- 1 zhoujianglin zhoujianglin 5985M Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann. -rw-rw-r-- 1 zhoujianglin zhoujianglin 9725M Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai. -rw-rw-r-- 1 zhoujianglin zhoujianglin 749M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac. ```. However, If I run the follow bash script, it can not find `ref_idx` through `ls` command, whereas shell condition expression ` [ -f $ref_idx ]` return `true`. here is the script:. ```shell. #!/bin/bash. dvsif=""/lustre/Data/toolsDB//deepvariant.sif"". ref_idx=""/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa"". wkdir=""/lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP"". bamdir=""${wkdir}/mappinged_bams"". logdir=""${wkdir}/logs"". vcfoutdir=""${wkdir}/DeepVariant_outputs"". source activate ~/.conda/envs/zjlEnv. echo -e ""ref_idx is $ref_idx\n"". if [ -f ""$ref_idx"" ];. then. echo -e ""ref_idx $ref_idx exists!\n"". which ls. echo -e ""ls -al --block=M ${ref_idx}*\n"". ls -al --block=M ""${ref_idx}*"". echo -e ""ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*\n"". ls -al --block=M ""/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*"". echo -e ""/bin/ls -al --block=M ${ref_idx}*\n"". /bin/ls -al --block=M ""${ref_idx}*"". else. echo -e ""Warning!! ref_idx [$ref_idx] not exist!\n"". fi. ls -al ""${ref_idx}*"". singularity run $dvsif ls $ref_idx. ```. Here is the running output:. ```shell. $ bash scripts/02_run_deepvariant.sh . ref_idx is /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. ref_idx /lustre/Data",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:1629,usability,tool,toolsDB,1629,"Human_hs37d5/hs37d5.fa. -rw-rw-r-- 1 zhoujianglin zhoujianglin 5985M Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann. -rw-rw-r-- 1 zhoujianglin zhoujianglin 9725M Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai. -rw-rw-r-- 1 zhoujianglin zhoujianglin 749M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac. ```. However, If I run the follow bash script, it can not find `ref_idx` through `ls` command, whereas shell condition expression ` [ -f $ref_idx ]` return `true`. here is the script:. ```shell. #!/bin/bash. dvsif=""/lustre/Data/toolsDB//deepvariant.sif"". ref_idx=""/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa"". wkdir=""/lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP"". bamdir=""${wkdir}/mappinged_bams"". logdir=""${wkdir}/logs"". vcfoutdir=""${wkdir}/DeepVariant_outputs"". source activate ~/.conda/envs/zjlEnv. echo -e ""ref_idx is $ref_idx\n"". if [ -f ""$ref_idx"" ];. then. echo -e ""ref_idx $ref_idx exists!\n"". which ls. echo -e ""ls -al --block=M ${ref_idx}*\n"". ls -al --block=M ""${ref_idx}*"". echo -e ""ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*\n"". ls -al --block=M ""/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*"". echo -e ""/bin/ls -al --block=M ${ref_idx}*\n"". /bin/ls -al --block=M ""${ref_idx}*"". else. echo -e ""Warning!! ref_idx [$ref_idx] not exist!\n"". fi. ls -al ""${ref_idx}*"". singularity run $dvsif ls $ref_idx. ```. Here is the running output:. ```shell. $ bash scripts/02_run_deepvariant.sh . ref_idx is /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. ref_idx /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa exists! ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:2092,usability,tool,toolsDB,2092,"37d5/hs37d5.fa.bwt.2bit.64. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai. -rw-rw-r-- 1 zhoujianglin zhoujianglin 749M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac. ```. However, If I run the follow bash script, it can not find `ref_idx` through `ls` command, whereas shell condition expression ` [ -f $ref_idx ]` return `true`. here is the script:. ```shell. #!/bin/bash. dvsif=""/lustre/Data/toolsDB//deepvariant.sif"". ref_idx=""/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa"". wkdir=""/lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP"". bamdir=""${wkdir}/mappinged_bams"". logdir=""${wkdir}/logs"". vcfoutdir=""${wkdir}/DeepVariant_outputs"". source activate ~/.conda/envs/zjlEnv. echo -e ""ref_idx is $ref_idx\n"". if [ -f ""$ref_idx"" ];. then. echo -e ""ref_idx $ref_idx exists!\n"". which ls. echo -e ""ls -al --block=M ${ref_idx}*\n"". ls -al --block=M ""${ref_idx}*"". echo -e ""ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*\n"". ls -al --block=M ""/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*"". echo -e ""/bin/ls -al --block=M ${ref_idx}*\n"". /bin/ls -al --block=M ""${ref_idx}*"". else. echo -e ""Warning!! ref_idx [$ref_idx] not exist!\n"". fi. ls -al ""${ref_idx}*"". singularity run $dvsif ls $ref_idx. ```. Here is the running output:. ```shell. $ bash scripts/02_run_deepvariant.sh . ref_idx is /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. ref_idx /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa exists! /usr/bin/ls. ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. ls: cannot access /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*: No such file or directory. ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. ls: cannot access /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*: No such file or directory. /bin/ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. /bin/ls: cannot access /lustr",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:2168,usability,tool,toolsDB,2168," 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai. -rw-rw-r-- 1 zhoujianglin zhoujianglin 749M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac. ```. However, If I run the follow bash script, it can not find `ref_idx` through `ls` command, whereas shell condition expression ` [ -f $ref_idx ]` return `true`. here is the script:. ```shell. #!/bin/bash. dvsif=""/lustre/Data/toolsDB//deepvariant.sif"". ref_idx=""/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa"". wkdir=""/lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP"". bamdir=""${wkdir}/mappinged_bams"". logdir=""${wkdir}/logs"". vcfoutdir=""${wkdir}/DeepVariant_outputs"". source activate ~/.conda/envs/zjlEnv. echo -e ""ref_idx is $ref_idx\n"". if [ -f ""$ref_idx"" ];. then. echo -e ""ref_idx $ref_idx exists!\n"". which ls. echo -e ""ls -al --block=M ${ref_idx}*\n"". ls -al --block=M ""${ref_idx}*"". echo -e ""ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*\n"". ls -al --block=M ""/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*"". echo -e ""/bin/ls -al --block=M ${ref_idx}*\n"". /bin/ls -al --block=M ""${ref_idx}*"". else. echo -e ""Warning!! ref_idx [$ref_idx] not exist!\n"". fi. ls -al ""${ref_idx}*"". singularity run $dvsif ls $ref_idx. ```. Here is the running output:. ```shell. $ bash scripts/02_run_deepvariant.sh . ref_idx is /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. ref_idx /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa exists! /usr/bin/ls. ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. ls: cannot access /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*: No such file or directory. ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. ls: cannot access /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*: No such file or directory. /bin/ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. /bin/ls: cannot access /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*: No such file or directory. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:2523,usability,tool,toolsDB,2523,"$ref_idx ]` return `true`. here is the script:. ```shell. #!/bin/bash. dvsif=""/lustre/Data/toolsDB//deepvariant.sif"". ref_idx=""/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa"". wkdir=""/lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP"". bamdir=""${wkdir}/mappinged_bams"". logdir=""${wkdir}/logs"". vcfoutdir=""${wkdir}/DeepVariant_outputs"". source activate ~/.conda/envs/zjlEnv. echo -e ""ref_idx is $ref_idx\n"". if [ -f ""$ref_idx"" ];. then. echo -e ""ref_idx $ref_idx exists!\n"". which ls. echo -e ""ls -al --block=M ${ref_idx}*\n"". ls -al --block=M ""${ref_idx}*"". echo -e ""ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*\n"". ls -al --block=M ""/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*"". echo -e ""/bin/ls -al --block=M ${ref_idx}*\n"". /bin/ls -al --block=M ""${ref_idx}*"". else. echo -e ""Warning!! ref_idx [$ref_idx] not exist!\n"". fi. ls -al ""${ref_idx}*"". singularity run $dvsif ls $ref_idx. ```. Here is the running output:. ```shell. $ bash scripts/02_run_deepvariant.sh . ref_idx is /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. ref_idx /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa exists! /usr/bin/ls. ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. ls: cannot access /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*: No such file or directory. ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. ls: cannot access /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*: No such file or directory. /bin/ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. /bin/ls: cannot access /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*: No such file or directory. ls: cannot access /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*: No such file or directory. INFO: Converting SIF file to temporary sandbox... /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa': No such file or directory. INFO: Cleaning up image... ```. Could you please help me?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:2585,usability,tool,toolsDB,2585,"$ref_idx ]` return `true`. here is the script:. ```shell. #!/bin/bash. dvsif=""/lustre/Data/toolsDB//deepvariant.sif"". ref_idx=""/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa"". wkdir=""/lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP"". bamdir=""${wkdir}/mappinged_bams"". logdir=""${wkdir}/logs"". vcfoutdir=""${wkdir}/DeepVariant_outputs"". source activate ~/.conda/envs/zjlEnv. echo -e ""ref_idx is $ref_idx\n"". if [ -f ""$ref_idx"" ];. then. echo -e ""ref_idx $ref_idx exists!\n"". which ls. echo -e ""ls -al --block=M ${ref_idx}*\n"". ls -al --block=M ""${ref_idx}*"". echo -e ""ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*\n"". ls -al --block=M ""/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*"". echo -e ""/bin/ls -al --block=M ${ref_idx}*\n"". /bin/ls -al --block=M ""${ref_idx}*"". else. echo -e ""Warning!! ref_idx [$ref_idx] not exist!\n"". fi. ls -al ""${ref_idx}*"". singularity run $dvsif ls $ref_idx. ```. Here is the running output:. ```shell. $ bash scripts/02_run_deepvariant.sh . ref_idx is /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. ref_idx /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa exists! /usr/bin/ls. ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. ls: cannot access /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*: No such file or directory. ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. ls: cannot access /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*: No such file or directory. /bin/ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. /bin/ls: cannot access /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*: No such file or directory. ls: cannot access /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*: No such file or directory. INFO: Converting SIF file to temporary sandbox... /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa': No such file or directory. INFO: Cleaning up image... ```. Could you please help me?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:2676,usability,tool,toolsDB,2676,"$ref_idx ]` return `true`. here is the script:. ```shell. #!/bin/bash. dvsif=""/lustre/Data/toolsDB//deepvariant.sif"". ref_idx=""/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa"". wkdir=""/lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP"". bamdir=""${wkdir}/mappinged_bams"". logdir=""${wkdir}/logs"". vcfoutdir=""${wkdir}/DeepVariant_outputs"". source activate ~/.conda/envs/zjlEnv. echo -e ""ref_idx is $ref_idx\n"". if [ -f ""$ref_idx"" ];. then. echo -e ""ref_idx $ref_idx exists!\n"". which ls. echo -e ""ls -al --block=M ${ref_idx}*\n"". ls -al --block=M ""${ref_idx}*"". echo -e ""ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*\n"". ls -al --block=M ""/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*"". echo -e ""/bin/ls -al --block=M ${ref_idx}*\n"". /bin/ls -al --block=M ""${ref_idx}*"". else. echo -e ""Warning!! ref_idx [$ref_idx] not exist!\n"". fi. ls -al ""${ref_idx}*"". singularity run $dvsif ls $ref_idx. ```. Here is the running output:. ```shell. $ bash scripts/02_run_deepvariant.sh . ref_idx is /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. ref_idx /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa exists! /usr/bin/ls. ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. ls: cannot access /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*: No such file or directory. ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. ls: cannot access /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*: No such file or directory. /bin/ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. /bin/ls: cannot access /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*: No such file or directory. ls: cannot access /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*: No such file or directory. INFO: Converting SIF file to temporary sandbox... /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa': No such file or directory. INFO: Cleaning up image... ```. Could you please help me?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:2749,usability,tool,toolsDB,2749,"$ref_idx ]` return `true`. here is the script:. ```shell. #!/bin/bash. dvsif=""/lustre/Data/toolsDB//deepvariant.sif"". ref_idx=""/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa"". wkdir=""/lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP"". bamdir=""${wkdir}/mappinged_bams"". logdir=""${wkdir}/logs"". vcfoutdir=""${wkdir}/DeepVariant_outputs"". source activate ~/.conda/envs/zjlEnv. echo -e ""ref_idx is $ref_idx\n"". if [ -f ""$ref_idx"" ];. then. echo -e ""ref_idx $ref_idx exists!\n"". which ls. echo -e ""ls -al --block=M ${ref_idx}*\n"". ls -al --block=M ""${ref_idx}*"". echo -e ""ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*\n"". ls -al --block=M ""/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*"". echo -e ""/bin/ls -al --block=M ${ref_idx}*\n"". /bin/ls -al --block=M ""${ref_idx}*"". else. echo -e ""Warning!! ref_idx [$ref_idx] not exist!\n"". fi. ls -al ""${ref_idx}*"". singularity run $dvsif ls $ref_idx. ```. Here is the running output:. ```shell. $ bash scripts/02_run_deepvariant.sh . ref_idx is /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. ref_idx /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa exists! /usr/bin/ls. ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. ls: cannot access /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*: No such file or directory. ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. ls: cannot access /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*: No such file or directory. /bin/ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. /bin/ls: cannot access /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*: No such file or directory. ls: cannot access /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*: No such file or directory. INFO: Converting SIF file to temporary sandbox... /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa': No such file or directory. INFO: Cleaning up image... ```. Could you please help me?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:2848,usability,tool,toolsDB,2848,"$ref_idx ]` return `true`. here is the script:. ```shell. #!/bin/bash. dvsif=""/lustre/Data/toolsDB//deepvariant.sif"". ref_idx=""/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa"". wkdir=""/lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP"". bamdir=""${wkdir}/mappinged_bams"". logdir=""${wkdir}/logs"". vcfoutdir=""${wkdir}/DeepVariant_outputs"". source activate ~/.conda/envs/zjlEnv. echo -e ""ref_idx is $ref_idx\n"". if [ -f ""$ref_idx"" ];. then. echo -e ""ref_idx $ref_idx exists!\n"". which ls. echo -e ""ls -al --block=M ${ref_idx}*\n"". ls -al --block=M ""${ref_idx}*"". echo -e ""ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*\n"". ls -al --block=M ""/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*"". echo -e ""/bin/ls -al --block=M ${ref_idx}*\n"". /bin/ls -al --block=M ""${ref_idx}*"". else. echo -e ""Warning!! ref_idx [$ref_idx] not exist!\n"". fi. ls -al ""${ref_idx}*"". singularity run $dvsif ls $ref_idx. ```. Here is the running output:. ```shell. $ bash scripts/02_run_deepvariant.sh . ref_idx is /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. ref_idx /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa exists! /usr/bin/ls. ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. ls: cannot access /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*: No such file or directory. ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. ls: cannot access /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*: No such file or directory. /bin/ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. /bin/ls: cannot access /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*: No such file or directory. ls: cannot access /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*: No such file or directory. INFO: Converting SIF file to temporary sandbox... /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa': No such file or directory. INFO: Cleaning up image... ```. Could you please help me?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:2921,usability,tool,toolsDB,2921,"$ref_idx ]` return `true`. here is the script:. ```shell. #!/bin/bash. dvsif=""/lustre/Data/toolsDB//deepvariant.sif"". ref_idx=""/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa"". wkdir=""/lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP"". bamdir=""${wkdir}/mappinged_bams"". logdir=""${wkdir}/logs"". vcfoutdir=""${wkdir}/DeepVariant_outputs"". source activate ~/.conda/envs/zjlEnv. echo -e ""ref_idx is $ref_idx\n"". if [ -f ""$ref_idx"" ];. then. echo -e ""ref_idx $ref_idx exists!\n"". which ls. echo -e ""ls -al --block=M ${ref_idx}*\n"". ls -al --block=M ""${ref_idx}*"". echo -e ""ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*\n"". ls -al --block=M ""/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*"". echo -e ""/bin/ls -al --block=M ${ref_idx}*\n"". /bin/ls -al --block=M ""${ref_idx}*"". else. echo -e ""Warning!! ref_idx [$ref_idx] not exist!\n"". fi. ls -al ""${ref_idx}*"". singularity run $dvsif ls $ref_idx. ```. Here is the running output:. ```shell. $ bash scripts/02_run_deepvariant.sh . ref_idx is /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. ref_idx /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa exists! /usr/bin/ls. ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. ls: cannot access /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*: No such file or directory. ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. ls: cannot access /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*: No such file or directory. /bin/ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. /bin/ls: cannot access /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*: No such file or directory. ls: cannot access /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*: No such file or directory. INFO: Converting SIF file to temporary sandbox... /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa': No such file or directory. INFO: Cleaning up image... ```. Could you please help me?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:3025,usability,tool,toolsDB,3025,"$ref_idx ]` return `true`. here is the script:. ```shell. #!/bin/bash. dvsif=""/lustre/Data/toolsDB//deepvariant.sif"". ref_idx=""/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa"". wkdir=""/lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP"". bamdir=""${wkdir}/mappinged_bams"". logdir=""${wkdir}/logs"". vcfoutdir=""${wkdir}/DeepVariant_outputs"". source activate ~/.conda/envs/zjlEnv. echo -e ""ref_idx is $ref_idx\n"". if [ -f ""$ref_idx"" ];. then. echo -e ""ref_idx $ref_idx exists!\n"". which ls. echo -e ""ls -al --block=M ${ref_idx}*\n"". ls -al --block=M ""${ref_idx}*"". echo -e ""ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*\n"". ls -al --block=M ""/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*"". echo -e ""/bin/ls -al --block=M ${ref_idx}*\n"". /bin/ls -al --block=M ""${ref_idx}*"". else. echo -e ""Warning!! ref_idx [$ref_idx] not exist!\n"". fi. ls -al ""${ref_idx}*"". singularity run $dvsif ls $ref_idx. ```. Here is the running output:. ```shell. $ bash scripts/02_run_deepvariant.sh . ref_idx is /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. ref_idx /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa exists! /usr/bin/ls. ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. ls: cannot access /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*: No such file or directory. ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. ls: cannot access /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*: No such file or directory. /bin/ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. /bin/ls: cannot access /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*: No such file or directory. ls: cannot access /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*: No such file or directory. INFO: Converting SIF file to temporary sandbox... /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa': No such file or directory. INFO: Cleaning up image... ```. Could you please help me?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:3103,usability,tool,toolsDB,3103,"$ref_idx ]` return `true`. here is the script:. ```shell. #!/bin/bash. dvsif=""/lustre/Data/toolsDB//deepvariant.sif"". ref_idx=""/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa"". wkdir=""/lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP"". bamdir=""${wkdir}/mappinged_bams"". logdir=""${wkdir}/logs"". vcfoutdir=""${wkdir}/DeepVariant_outputs"". source activate ~/.conda/envs/zjlEnv. echo -e ""ref_idx is $ref_idx\n"". if [ -f ""$ref_idx"" ];. then. echo -e ""ref_idx $ref_idx exists!\n"". which ls. echo -e ""ls -al --block=M ${ref_idx}*\n"". ls -al --block=M ""${ref_idx}*"". echo -e ""ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*\n"". ls -al --block=M ""/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*"". echo -e ""/bin/ls -al --block=M ${ref_idx}*\n"". /bin/ls -al --block=M ""${ref_idx}*"". else. echo -e ""Warning!! ref_idx [$ref_idx] not exist!\n"". fi. ls -al ""${ref_idx}*"". singularity run $dvsif ls $ref_idx. ```. Here is the running output:. ```shell. $ bash scripts/02_run_deepvariant.sh . ref_idx is /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. ref_idx /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa exists! /usr/bin/ls. ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. ls: cannot access /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*: No such file or directory. ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. ls: cannot access /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*: No such file or directory. /bin/ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. /bin/ls: cannot access /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*: No such file or directory. ls: cannot access /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*: No such file or directory. INFO: Converting SIF file to temporary sandbox... /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa': No such file or directory. INFO: Cleaning up image... ```. Could you please help me?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:3203,usability,tool,toolsDB,3203,"$ref_idx ]` return `true`. here is the script:. ```shell. #!/bin/bash. dvsif=""/lustre/Data/toolsDB//deepvariant.sif"". ref_idx=""/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa"". wkdir=""/lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP"". bamdir=""${wkdir}/mappinged_bams"". logdir=""${wkdir}/logs"". vcfoutdir=""${wkdir}/DeepVariant_outputs"". source activate ~/.conda/envs/zjlEnv. echo -e ""ref_idx is $ref_idx\n"". if [ -f ""$ref_idx"" ];. then. echo -e ""ref_idx $ref_idx exists!\n"". which ls. echo -e ""ls -al --block=M ${ref_idx}*\n"". ls -al --block=M ""${ref_idx}*"". echo -e ""ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*\n"". ls -al --block=M ""/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*"". echo -e ""/bin/ls -al --block=M ${ref_idx}*\n"". /bin/ls -al --block=M ""${ref_idx}*"". else. echo -e ""Warning!! ref_idx [$ref_idx] not exist!\n"". fi. ls -al ""${ref_idx}*"". singularity run $dvsif ls $ref_idx. ```. Here is the running output:. ```shell. $ bash scripts/02_run_deepvariant.sh . ref_idx is /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. ref_idx /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa exists! /usr/bin/ls. ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. ls: cannot access /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*: No such file or directory. ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. ls: cannot access /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*: No such file or directory. /bin/ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. /bin/ls: cannot access /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*: No such file or directory. ls: cannot access /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*: No such file or directory. INFO: Converting SIF file to temporary sandbox... /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa': No such file or directory. INFO: Cleaning up image... ```. Could you please help me?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:3363,usability,tool,toolsDB,3363,"$ref_idx ]` return `true`. here is the script:. ```shell. #!/bin/bash. dvsif=""/lustre/Data/toolsDB//deepvariant.sif"". ref_idx=""/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa"". wkdir=""/lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP"". bamdir=""${wkdir}/mappinged_bams"". logdir=""${wkdir}/logs"". vcfoutdir=""${wkdir}/DeepVariant_outputs"". source activate ~/.conda/envs/zjlEnv. echo -e ""ref_idx is $ref_idx\n"". if [ -f ""$ref_idx"" ];. then. echo -e ""ref_idx $ref_idx exists!\n"". which ls. echo -e ""ls -al --block=M ${ref_idx}*\n"". ls -al --block=M ""${ref_idx}*"". echo -e ""ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*\n"". ls -al --block=M ""/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*"". echo -e ""/bin/ls -al --block=M ${ref_idx}*\n"". /bin/ls -al --block=M ""${ref_idx}*"". else. echo -e ""Warning!! ref_idx [$ref_idx] not exist!\n"". fi. ls -al ""${ref_idx}*"". singularity run $dvsif ls $ref_idx. ```. Here is the running output:. ```shell. $ bash scripts/02_run_deepvariant.sh . ref_idx is /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. ref_idx /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa exists! /usr/bin/ls. ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. ls: cannot access /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*: No such file or directory. ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. ls: cannot access /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*: No such file or directory. /bin/ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. /bin/ls: cannot access /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*: No such file or directory. ls: cannot access /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*: No such file or directory. INFO: Converting SIF file to temporary sandbox... /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa': No such file or directory. INFO: Cleaning up image... ```. Could you please help me?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:3481,usability,help,help,3481,"$ref_idx ]` return `true`. here is the script:. ```shell. #!/bin/bash. dvsif=""/lustre/Data/toolsDB//deepvariant.sif"". ref_idx=""/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa"". wkdir=""/lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP"". bamdir=""${wkdir}/mappinged_bams"". logdir=""${wkdir}/logs"". vcfoutdir=""${wkdir}/DeepVariant_outputs"". source activate ~/.conda/envs/zjlEnv. echo -e ""ref_idx is $ref_idx\n"". if [ -f ""$ref_idx"" ];. then. echo -e ""ref_idx $ref_idx exists!\n"". which ls. echo -e ""ls -al --block=M ${ref_idx}*\n"". ls -al --block=M ""${ref_idx}*"". echo -e ""ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*\n"". ls -al --block=M ""/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*"". echo -e ""/bin/ls -al --block=M ${ref_idx}*\n"". /bin/ls -al --block=M ""${ref_idx}*"". else. echo -e ""Warning!! ref_idx [$ref_idx] not exist!\n"". fi. ls -al ""${ref_idx}*"". singularity run $dvsif ls $ref_idx. ```. Here is the running output:. ```shell. $ bash scripts/02_run_deepvariant.sh . ref_idx is /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. ref_idx /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa exists! /usr/bin/ls. ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. ls: cannot access /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*: No such file or directory. ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. ls: cannot access /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*: No such file or directory. /bin/ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. /bin/ls: cannot access /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*: No such file or directory. ls: cannot access /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*: No such file or directory. INFO: Converting SIF file to temporary sandbox... /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa': No such file or directory. INFO: Cleaning up image... ```. Could you please help me?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:133,deployability,version,version,133,"Hi @Zjianglin , I took a quick look of the script and I'm not sure I fully understand what you're testing here. I tried a simplified version on my side. (The following steps has nothing to do with DeepVariant anymore. I'm mostly just testing `ls` and `singularity` right now). First I got these files in my /tmp. ```. DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". wget -P /tmp ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P /tmp ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. ```. Then I made my deepvariant.sif. ```. singularity build deepvariant.sif docker://google/deepvariant:1.5.0. ```. First, I check that I have the files. ```. REF=/tmp/ucsc.hg19.chr20.unittest.fasta. ls -al ${REF}*. ```. This worked. (Note, you were doing something like `ls -al ""${ref_idx}*""`. Don't add the double quotes around the *. That didn't work for me. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. deepvariant.sif \. ls -al ${REF}*. ```. This also worked fine for me. I can see the files.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:584,deployability,build,build,584,"Hi @Zjianglin , I took a quick look of the script and I'm not sure I fully understand what you're testing here. I tried a simplified version on my side. (The following steps has nothing to do with DeepVariant anymore. I'm mostly just testing `ls` and `singularity` right now). First I got these files in my /tmp. ```. DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". wget -P /tmp ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P /tmp ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. ```. Then I made my deepvariant.sif. ```. singularity build deepvariant.sif docker://google/deepvariant:1.5.0. ```. First, I check that I have the files. ```. REF=/tmp/ucsc.hg19.chr20.unittest.fasta. ls -al ${REF}*. ```. This worked. (Note, you were doing something like `ls -al ""${ref_idx}*""`. Don't add the double quotes around the *. That didn't work for me. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. deepvariant.sif \. ls -al ${REF}*. ```. This also worked fine for me. I can see the files.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:133,integrability,version,version,133,"Hi @Zjianglin , I took a quick look of the script and I'm not sure I fully understand what you're testing here. I tried a simplified version on my side. (The following steps has nothing to do with DeepVariant anymore. I'm mostly just testing `ls` and `singularity` right now). First I got these files in my /tmp. ```. DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". wget -P /tmp ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P /tmp ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. ```. Then I made my deepvariant.sif. ```. singularity build deepvariant.sif docker://google/deepvariant:1.5.0. ```. First, I check that I have the files. ```. REF=/tmp/ucsc.hg19.chr20.unittest.fasta. ls -al ${REF}*. ```. This worked. (Note, you were doing something like `ls -al ""${ref_idx}*""`. Don't add the double quotes around the *. That didn't work for me. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. deepvariant.sif \. ls -al ${REF}*. ```. This also worked fine for me. I can see the files.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:133,modifiability,version,version,133,"Hi @Zjianglin , I took a quick look of the script and I'm not sure I fully understand what you're testing here. I tried a simplified version on my side. (The following steps has nothing to do with DeepVariant anymore. I'm mostly just testing `ls` and `singularity` right now). First I got these files in my /tmp. ```. DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". wget -P /tmp ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P /tmp ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. ```. Then I made my deepvariant.sif. ```. singularity build deepvariant.sif docker://google/deepvariant:1.5.0. ```. First, I check that I have the files. ```. REF=/tmp/ucsc.hg19.chr20.unittest.fasta. ls -al ${REF}*. ```. This worked. (Note, you were doing something like `ls -al ""${ref_idx}*""`. Don't add the double quotes around the *. That didn't work for me. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. deepvariant.sif \. ls -al ${REF}*. ```. This also worked fine for me. I can see the files.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:98,safety,test,testing,98,"Hi @Zjianglin , I took a quick look of the script and I'm not sure I fully understand what you're testing here. I tried a simplified version on my side. (The following steps has nothing to do with DeepVariant anymore. I'm mostly just testing `ls` and `singularity` right now). First I got these files in my /tmp. ```. DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". wget -P /tmp ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P /tmp ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. ```. Then I made my deepvariant.sif. ```. singularity build deepvariant.sif docker://google/deepvariant:1.5.0. ```. First, I check that I have the files. ```. REF=/tmp/ucsc.hg19.chr20.unittest.fasta. ls -al ${REF}*. ```. This worked. (Note, you were doing something like `ls -al ""${ref_idx}*""`. Don't add the double quotes around the *. That didn't work for me. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. deepvariant.sif \. ls -al ${REF}*. ```. This also worked fine for me. I can see the files.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:234,safety,test,testing,234,"Hi @Zjianglin , I took a quick look of the script and I'm not sure I fully understand what you're testing here. I tried a simplified version on my side. (The following steps has nothing to do with DeepVariant anymore. I'm mostly just testing `ls` and `singularity` right now). First I got these files in my /tmp. ```. DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". wget -P /tmp ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P /tmp ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. ```. Then I made my deepvariant.sif. ```. singularity build deepvariant.sif docker://google/deepvariant:1.5.0. ```. First, I check that I have the files. ```. REF=/tmp/ucsc.hg19.chr20.unittest.fasta. ls -al ${REF}*. ```. This worked. (Note, you were doing something like `ls -al ""${ref_idx}*""`. Don't add the double quotes around the *. That didn't work for me. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. deepvariant.sif \. ls -al ${REF}*. ```. This also worked fine for me. I can see the files.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:387,safety,test,testdata,387,"Hi @Zjianglin , I took a quick look of the script and I'm not sure I fully understand what you're testing here. I tried a simplified version on my side. (The following steps has nothing to do with DeepVariant anymore. I'm mostly just testing `ls` and `singularity` right now). First I got these files in my /tmp. ```. DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". wget -P /tmp ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P /tmp ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. ```. Then I made my deepvariant.sif. ```. singularity build deepvariant.sif docker://google/deepvariant:1.5.0. ```. First, I check that I have the files. ```. REF=/tmp/ucsc.hg19.chr20.unittest.fasta. ls -al ${REF}*. ```. This worked. (Note, you were doing something like `ls -al ""${ref_idx}*""`. Don't add the double quotes around the *. That didn't work for me. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. deepvariant.sif \. ls -al ${REF}*. ```. This also worked fine for me. I can see the files.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:75,testability,understand,understand,75,"Hi @Zjianglin , I took a quick look of the script and I'm not sure I fully understand what you're testing here. I tried a simplified version on my side. (The following steps has nothing to do with DeepVariant anymore. I'm mostly just testing `ls` and `singularity` right now). First I got these files in my /tmp. ```. DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". wget -P /tmp ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P /tmp ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. ```. Then I made my deepvariant.sif. ```. singularity build deepvariant.sif docker://google/deepvariant:1.5.0. ```. First, I check that I have the files. ```. REF=/tmp/ucsc.hg19.chr20.unittest.fasta. ls -al ${REF}*. ```. This worked. (Note, you were doing something like `ls -al ""${ref_idx}*""`. Don't add the double quotes around the *. That didn't work for me. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. deepvariant.sif \. ls -al ${REF}*. ```. This also worked fine for me. I can see the files.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:98,testability,test,testing,98,"Hi @Zjianglin , I took a quick look of the script and I'm not sure I fully understand what you're testing here. I tried a simplified version on my side. (The following steps has nothing to do with DeepVariant anymore. I'm mostly just testing `ls` and `singularity` right now). First I got these files in my /tmp. ```. DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". wget -P /tmp ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P /tmp ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. ```. Then I made my deepvariant.sif. ```. singularity build deepvariant.sif docker://google/deepvariant:1.5.0. ```. First, I check that I have the files. ```. REF=/tmp/ucsc.hg19.chr20.unittest.fasta. ls -al ${REF}*. ```. This worked. (Note, you were doing something like `ls -al ""${ref_idx}*""`. Don't add the double quotes around the *. That didn't work for me. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. deepvariant.sif \. ls -al ${REF}*. ```. This also worked fine for me. I can see the files.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:122,testability,simpl,simplified,122,"Hi @Zjianglin , I took a quick look of the script and I'm not sure I fully understand what you're testing here. I tried a simplified version on my side. (The following steps has nothing to do with DeepVariant anymore. I'm mostly just testing `ls` and `singularity` right now). First I got these files in my /tmp. ```. DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". wget -P /tmp ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P /tmp ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. ```. Then I made my deepvariant.sif. ```. singularity build deepvariant.sif docker://google/deepvariant:1.5.0. ```. First, I check that I have the files. ```. REF=/tmp/ucsc.hg19.chr20.unittest.fasta. ls -al ${REF}*. ```. This worked. (Note, you were doing something like `ls -al ""${ref_idx}*""`. Don't add the double quotes around the *. That didn't work for me. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. deepvariant.sif \. ls -al ${REF}*. ```. This also worked fine for me. I can see the files.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:234,testability,test,testing,234,"Hi @Zjianglin , I took a quick look of the script and I'm not sure I fully understand what you're testing here. I tried a simplified version on my side. (The following steps has nothing to do with DeepVariant anymore. I'm mostly just testing `ls` and `singularity` right now). First I got these files in my /tmp. ```. DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". wget -P /tmp ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P /tmp ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. ```. Then I made my deepvariant.sif. ```. singularity build deepvariant.sif docker://google/deepvariant:1.5.0. ```. First, I check that I have the files. ```. REF=/tmp/ucsc.hg19.chr20.unittest.fasta. ls -al ${REF}*. ```. This worked. (Note, you were doing something like `ls -al ""${ref_idx}*""`. Don't add the double quotes around the *. That didn't work for me. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. deepvariant.sif \. ls -al ${REF}*. ```. This also worked fine for me. I can see the files.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:387,testability,test,testdata,387,"Hi @Zjianglin , I took a quick look of the script and I'm not sure I fully understand what you're testing here. I tried a simplified version on my side. (The following steps has nothing to do with DeepVariant anymore. I'm mostly just testing `ls` and `singularity` right now). First I got these files in my /tmp. ```. DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". wget -P /tmp ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P /tmp ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. ```. Then I made my deepvariant.sif. ```. singularity build deepvariant.sif docker://google/deepvariant:1.5.0. ```. First, I check that I have the files. ```. REF=/tmp/ucsc.hg19.chr20.unittest.fasta. ls -al ${REF}*. ```. This worked. (Note, you were doing something like `ls -al ""${ref_idx}*""`. Don't add the double quotes around the *. That didn't work for me. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. deepvariant.sif \. ls -al ${REF}*. ```. This also worked fine for me. I can see the files.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:446,testability,unit,unittest,446,"Hi @Zjianglin , I took a quick look of the script and I'm not sure I fully understand what you're testing here. I tried a simplified version on my side. (The following steps has nothing to do with DeepVariant anymore. I'm mostly just testing `ls` and `singularity` right now). First I got these files in my /tmp. ```. DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". wget -P /tmp ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P /tmp ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. ```. Then I made my deepvariant.sif. ```. singularity build deepvariant.sif docker://google/deepvariant:1.5.0. ```. First, I check that I have the files. ```. REF=/tmp/ucsc.hg19.chr20.unittest.fasta. ls -al ${REF}*. ```. This worked. (Note, you were doing something like `ls -al ""${ref_idx}*""`. Don't add the double quotes around the *. That didn't work for me. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. deepvariant.sif \. ls -al ${REF}*. ```. This also worked fine for me. I can see the files.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:510,testability,unit,unittest,510,"Hi @Zjianglin , I took a quick look of the script and I'm not sure I fully understand what you're testing here. I tried a simplified version on my side. (The following steps has nothing to do with DeepVariant anymore. I'm mostly just testing `ls` and `singularity` right now). First I got these files in my /tmp. ```. DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". wget -P /tmp ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P /tmp ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. ```. Then I made my deepvariant.sif. ```. singularity build deepvariant.sif docker://google/deepvariant:1.5.0. ```. First, I check that I have the files. ```. REF=/tmp/ucsc.hg19.chr20.unittest.fasta. ls -al ${REF}*. ```. This worked. (Note, you were doing something like `ls -al ""${ref_idx}*""`. Don't add the double quotes around the *. That didn't work for me. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. deepvariant.sif \. ls -al ${REF}*. ```. This also worked fine for me. I can see the files.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:714,testability,unit,unittest,714,"Hi @Zjianglin , I took a quick look of the script and I'm not sure I fully understand what you're testing here. I tried a simplified version on my side. (The following steps has nothing to do with DeepVariant anymore. I'm mostly just testing `ls` and `singularity` right now). First I got these files in my /tmp. ```. DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". wget -P /tmp ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P /tmp ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. ```. Then I made my deepvariant.sif. ```. singularity build deepvariant.sif docker://google/deepvariant:1.5.0. ```. First, I check that I have the files. ```. REF=/tmp/ucsc.hg19.chr20.unittest.fasta. ls -al ${REF}*. ```. This worked. (Note, you were doing something like `ls -al ""${ref_idx}*""`. Don't add the double quotes around the *. That didn't work for me. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. deepvariant.sif \. ls -al ${REF}*. ```. This also worked fine for me. I can see the files.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:122,usability,simpl,simplified,122,"Hi @Zjianglin , I took a quick look of the script and I'm not sure I fully understand what you're testing here. I tried a simplified version on my side. (The following steps has nothing to do with DeepVariant anymore. I'm mostly just testing `ls` and `singularity` right now). First I got these files in my /tmp. ```. DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". wget -P /tmp ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P /tmp ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. ```. Then I made my deepvariant.sif. ```. singularity build deepvariant.sif docker://google/deepvariant:1.5.0. ```. First, I check that I have the files. ```. REF=/tmp/ucsc.hg19.chr20.unittest.fasta. ls -al ${REF}*. ```. This worked. (Note, you were doing something like `ls -al ""${ref_idx}*""`. Don't add the double quotes around the *. That didn't work for me. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. deepvariant.sif \. ls -al ${REF}*. ```. This also worked fine for me. I can see the files.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:716,availability,echo,echo,716,"Hi @pichuan , Thanks for your reply. As your suggestion, after removing the double quote around the *, the `ls` run normally in `bash` script. However, the `ls` in `deepvariant` image of `singularity` still failed. Notably, If I add a `which ls;` before a `ls -al $ref_idx*`, it runs normally so that I can see the detailed information for the files.. Here is the script:. ```bash. #!/bin/bash. nthreads=32. dvsif=""/lustre/Data/toolsDB/deepvariant.sif"". ref_idx=""/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa"". wkdir=""/lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP"". bamdir=""${wkdir}/mappinged_bams"". logdir=""${wkdir}/logs"". vcfoutdir=""${wkdir}/DeepVariant_outputs"". source activate ~/.conda/envs/zjlEnv. echo -e ""ref_idx is $ref_idx\n"". if [ -f ""$ref_idx"" ];. then. echo -e ""ref_idx $ref_idx exists!\n"". which ls. echo -e ""ls -al --block=M ${ref_idx}*\n"". ls -al --block=M ${ref_idx}*. echo -e ""ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*\n"". ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. else. echo -e ""Warning!! ref_idx [$ref_idx] not exist!\n"". fi. echo -e ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ $dvsif ls -al ${ref_idx}*"". singularity run -B /usr/lib/locale/:/usr/lib/locale/ $dvsif ls -al ${ref_idx}*. echo -e ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ $dvsif which ls; ls -al ${ref_idx}*"". singularity run -B /usr/lib/locale/:/usr/lib/locale/ $dvsif which ls; ls -al ${ref_idx}*. ```. Here is the running output:. ```shell. /usr/bin/ls. ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. -rw-rw-r-- 1 zhoujianglin zhoujianglin 3042M Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. -rw-rw-r-- 1 zhoujianglin zhoujianglin 5985M Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:778,availability,echo,echo,778,"Hi @pichuan , Thanks for your reply. As your suggestion, after removing the double quote around the *, the `ls` run normally in `bash` script. However, the `ls` in `deepvariant` image of `singularity` still failed. Notably, If I add a `which ls;` before a `ls -al $ref_idx*`, it runs normally so that I can see the detailed information for the files.. Here is the script:. ```bash. #!/bin/bash. nthreads=32. dvsif=""/lustre/Data/toolsDB/deepvariant.sif"". ref_idx=""/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa"". wkdir=""/lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP"". bamdir=""${wkdir}/mappinged_bams"". logdir=""${wkdir}/logs"". vcfoutdir=""${wkdir}/DeepVariant_outputs"". source activate ~/.conda/envs/zjlEnv. echo -e ""ref_idx is $ref_idx\n"". if [ -f ""$ref_idx"" ];. then. echo -e ""ref_idx $ref_idx exists!\n"". which ls. echo -e ""ls -al --block=M ${ref_idx}*\n"". ls -al --block=M ${ref_idx}*. echo -e ""ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*\n"". ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. else. echo -e ""Warning!! ref_idx [$ref_idx] not exist!\n"". fi. echo -e ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ $dvsif ls -al ${ref_idx}*"". singularity run -B /usr/lib/locale/:/usr/lib/locale/ $dvsif ls -al ${ref_idx}*. echo -e ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ $dvsif which ls; ls -al ${ref_idx}*"". singularity run -B /usr/lib/locale/:/usr/lib/locale/ $dvsif which ls; ls -al ${ref_idx}*. ```. Here is the running output:. ```shell. /usr/bin/ls. ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. -rw-rw-r-- 1 zhoujianglin zhoujianglin 3042M Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. -rw-rw-r-- 1 zhoujianglin zhoujianglin 5985M Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:826,availability,echo,echo,826,"Hi @pichuan , Thanks for your reply. As your suggestion, after removing the double quote around the *, the `ls` run normally in `bash` script. However, the `ls` in `deepvariant` image of `singularity` still failed. Notably, If I add a `which ls;` before a `ls -al $ref_idx*`, it runs normally so that I can see the detailed information for the files.. Here is the script:. ```bash. #!/bin/bash. nthreads=32. dvsif=""/lustre/Data/toolsDB/deepvariant.sif"". ref_idx=""/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa"". wkdir=""/lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP"". bamdir=""${wkdir}/mappinged_bams"". logdir=""${wkdir}/logs"". vcfoutdir=""${wkdir}/DeepVariant_outputs"". source activate ~/.conda/envs/zjlEnv. echo -e ""ref_idx is $ref_idx\n"". if [ -f ""$ref_idx"" ];. then. echo -e ""ref_idx $ref_idx exists!\n"". which ls. echo -e ""ls -al --block=M ${ref_idx}*\n"". ls -al --block=M ${ref_idx}*. echo -e ""ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*\n"". ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. else. echo -e ""Warning!! ref_idx [$ref_idx] not exist!\n"". fi. echo -e ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ $dvsif ls -al ${ref_idx}*"". singularity run -B /usr/lib/locale/:/usr/lib/locale/ $dvsif ls -al ${ref_idx}*. echo -e ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ $dvsif which ls; ls -al ${ref_idx}*"". singularity run -B /usr/lib/locale/:/usr/lib/locale/ $dvsif which ls; ls -al ${ref_idx}*. ```. Here is the running output:. ```shell. /usr/bin/ls. ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. -rw-rw-r-- 1 zhoujianglin zhoujianglin 3042M Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. -rw-rw-r-- 1 zhoujianglin zhoujianglin 5985M Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:898,availability,echo,echo,898,"Hi @pichuan , Thanks for your reply. As your suggestion, after removing the double quote around the *, the `ls` run normally in `bash` script. However, the `ls` in `deepvariant` image of `singularity` still failed. Notably, If I add a `which ls;` before a `ls -al $ref_idx*`, it runs normally so that I can see the detailed information for the files.. Here is the script:. ```bash. #!/bin/bash. nthreads=32. dvsif=""/lustre/Data/toolsDB/deepvariant.sif"". ref_idx=""/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa"". wkdir=""/lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP"". bamdir=""${wkdir}/mappinged_bams"". logdir=""${wkdir}/logs"". vcfoutdir=""${wkdir}/DeepVariant_outputs"". source activate ~/.conda/envs/zjlEnv. echo -e ""ref_idx is $ref_idx\n"". if [ -f ""$ref_idx"" ];. then. echo -e ""ref_idx $ref_idx exists!\n"". which ls. echo -e ""ls -al --block=M ${ref_idx}*\n"". ls -al --block=M ${ref_idx}*. echo -e ""ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*\n"". ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. else. echo -e ""Warning!! ref_idx [$ref_idx] not exist!\n"". fi. echo -e ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ $dvsif ls -al ${ref_idx}*"". singularity run -B /usr/lib/locale/:/usr/lib/locale/ $dvsif ls -al ${ref_idx}*. echo -e ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ $dvsif which ls; ls -al ${ref_idx}*"". singularity run -B /usr/lib/locale/:/usr/lib/locale/ $dvsif which ls; ls -al ${ref_idx}*. ```. Here is the running output:. ```shell. /usr/bin/ls. ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. -rw-rw-r-- 1 zhoujianglin zhoujianglin 3042M Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. -rw-rw-r-- 1 zhoujianglin zhoujianglin 5985M Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:1060,availability,echo,echo,1060," removing the double quote around the *, the `ls` run normally in `bash` script. However, the `ls` in `deepvariant` image of `singularity` still failed. Notably, If I add a `which ls;` before a `ls -al $ref_idx*`, it runs normally so that I can see the detailed information for the files.. Here is the script:. ```bash. #!/bin/bash. nthreads=32. dvsif=""/lustre/Data/toolsDB/deepvariant.sif"". ref_idx=""/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa"". wkdir=""/lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP"". bamdir=""${wkdir}/mappinged_bams"". logdir=""${wkdir}/logs"". vcfoutdir=""${wkdir}/DeepVariant_outputs"". source activate ~/.conda/envs/zjlEnv. echo -e ""ref_idx is $ref_idx\n"". if [ -f ""$ref_idx"" ];. then. echo -e ""ref_idx $ref_idx exists!\n"". which ls. echo -e ""ls -al --block=M ${ref_idx}*\n"". ls -al --block=M ${ref_idx}*. echo -e ""ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*\n"". ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. else. echo -e ""Warning!! ref_idx [$ref_idx] not exist!\n"". fi. echo -e ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ $dvsif ls -al ${ref_idx}*"". singularity run -B /usr/lib/locale/:/usr/lib/locale/ $dvsif ls -al ${ref_idx}*. echo -e ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ $dvsif which ls; ls -al ${ref_idx}*"". singularity run -B /usr/lib/locale/:/usr/lib/locale/ $dvsif which ls; ls -al ${ref_idx}*. ```. Here is the running output:. ```shell. /usr/bin/ls. ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. -rw-rw-r-- 1 zhoujianglin zhoujianglin 3042M Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. -rw-rw-r-- 1 zhoujianglin zhoujianglin 5985M Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann. -",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:1117,availability,echo,echo,1117,"mally in `bash` script. However, the `ls` in `deepvariant` image of `singularity` still failed. Notably, If I add a `which ls;` before a `ls -al $ref_idx*`, it runs normally so that I can see the detailed information for the files.. Here is the script:. ```bash. #!/bin/bash. nthreads=32. dvsif=""/lustre/Data/toolsDB/deepvariant.sif"". ref_idx=""/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa"". wkdir=""/lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP"". bamdir=""${wkdir}/mappinged_bams"". logdir=""${wkdir}/logs"". vcfoutdir=""${wkdir}/DeepVariant_outputs"". source activate ~/.conda/envs/zjlEnv. echo -e ""ref_idx is $ref_idx\n"". if [ -f ""$ref_idx"" ];. then. echo -e ""ref_idx $ref_idx exists!\n"". which ls. echo -e ""ls -al --block=M ${ref_idx}*\n"". ls -al --block=M ${ref_idx}*. echo -e ""ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*\n"". ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. else. echo -e ""Warning!! ref_idx [$ref_idx] not exist!\n"". fi. echo -e ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ $dvsif ls -al ${ref_idx}*"". singularity run -B /usr/lib/locale/:/usr/lib/locale/ $dvsif ls -al ${ref_idx}*. echo -e ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ $dvsif which ls; ls -al ${ref_idx}*"". singularity run -B /usr/lib/locale/:/usr/lib/locale/ $dvsif which ls; ls -al ${ref_idx}*. ```. Here is the running output:. ```shell. /usr/bin/ls. ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. -rw-rw-r-- 1 zhoujianglin zhoujianglin 3042M Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. -rw-rw-r-- 1 zhoujianglin zhoujianglin 5985M Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann. -rw-rw-r-- 1 zhoujianglin zhoujianglin 9725M Apr 26 15:42 ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:1287,availability,echo,echo,1287,"lly so that I can see the detailed information for the files.. Here is the script:. ```bash. #!/bin/bash. nthreads=32. dvsif=""/lustre/Data/toolsDB/deepvariant.sif"". ref_idx=""/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa"". wkdir=""/lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP"". bamdir=""${wkdir}/mappinged_bams"". logdir=""${wkdir}/logs"". vcfoutdir=""${wkdir}/DeepVariant_outputs"". source activate ~/.conda/envs/zjlEnv. echo -e ""ref_idx is $ref_idx\n"". if [ -f ""$ref_idx"" ];. then. echo -e ""ref_idx $ref_idx exists!\n"". which ls. echo -e ""ls -al --block=M ${ref_idx}*\n"". ls -al --block=M ${ref_idx}*. echo -e ""ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*\n"". ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. else. echo -e ""Warning!! ref_idx [$ref_idx] not exist!\n"". fi. echo -e ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ $dvsif ls -al ${ref_idx}*"". singularity run -B /usr/lib/locale/:/usr/lib/locale/ $dvsif ls -al ${ref_idx}*. echo -e ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ $dvsif which ls; ls -al ${ref_idx}*"". singularity run -B /usr/lib/locale/:/usr/lib/locale/ $dvsif which ls; ls -al ${ref_idx}*. ```. Here is the running output:. ```shell. /usr/bin/ls. ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. -rw-rw-r-- 1 zhoujianglin zhoujianglin 3042M Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. -rw-rw-r-- 1 zhoujianglin zhoujianglin 5985M Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann. -rw-rw-r-- 1 zhoujianglin zhoujianglin 9725M Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:207,deployability,fail,failed,207,"Hi @pichuan , Thanks for your reply. As your suggestion, after removing the double quote around the *, the `ls` run normally in `bash` script. However, the `ls` in `deepvariant` image of `singularity` still failed. Notably, If I add a `which ls;` before a `ls -al $ref_idx*`, it runs normally so that I can see the detailed information for the files.. Here is the script:. ```bash. #!/bin/bash. nthreads=32. dvsif=""/lustre/Data/toolsDB/deepvariant.sif"". ref_idx=""/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa"". wkdir=""/lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP"". bamdir=""${wkdir}/mappinged_bams"". logdir=""${wkdir}/logs"". vcfoutdir=""${wkdir}/DeepVariant_outputs"". source activate ~/.conda/envs/zjlEnv. echo -e ""ref_idx is $ref_idx\n"". if [ -f ""$ref_idx"" ];. then. echo -e ""ref_idx $ref_idx exists!\n"". which ls. echo -e ""ls -al --block=M ${ref_idx}*\n"". ls -al --block=M ${ref_idx}*. echo -e ""ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*\n"". ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. else. echo -e ""Warning!! ref_idx [$ref_idx] not exist!\n"". fi. echo -e ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ $dvsif ls -al ${ref_idx}*"". singularity run -B /usr/lib/locale/:/usr/lib/locale/ $dvsif ls -al ${ref_idx}*. echo -e ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ $dvsif which ls; ls -al ${ref_idx}*"". singularity run -B /usr/lib/locale/:/usr/lib/locale/ $dvsif which ls; ls -al ${ref_idx}*. ```. Here is the running output:. ```shell. /usr/bin/ls. ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. -rw-rw-r-- 1 zhoujianglin zhoujianglin 3042M Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. -rw-rw-r-- 1 zhoujianglin zhoujianglin 5985M Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:612,deployability,log,logdir,612,"Hi @pichuan , Thanks for your reply. As your suggestion, after removing the double quote around the *, the `ls` run normally in `bash` script. However, the `ls` in `deepvariant` image of `singularity` still failed. Notably, If I add a `which ls;` before a `ls -al $ref_idx*`, it runs normally so that I can see the detailed information for the files.. Here is the script:. ```bash. #!/bin/bash. nthreads=32. dvsif=""/lustre/Data/toolsDB/deepvariant.sif"". ref_idx=""/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa"". wkdir=""/lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP"". bamdir=""${wkdir}/mappinged_bams"". logdir=""${wkdir}/logs"". vcfoutdir=""${wkdir}/DeepVariant_outputs"". source activate ~/.conda/envs/zjlEnv. echo -e ""ref_idx is $ref_idx\n"". if [ -f ""$ref_idx"" ];. then. echo -e ""ref_idx $ref_idx exists!\n"". which ls. echo -e ""ls -al --block=M ${ref_idx}*\n"". ls -al --block=M ${ref_idx}*. echo -e ""ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*\n"". ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. else. echo -e ""Warning!! ref_idx [$ref_idx] not exist!\n"". fi. echo -e ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ $dvsif ls -al ${ref_idx}*"". singularity run -B /usr/lib/locale/:/usr/lib/locale/ $dvsif ls -al ${ref_idx}*. echo -e ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ $dvsif which ls; ls -al ${ref_idx}*"". singularity run -B /usr/lib/locale/:/usr/lib/locale/ $dvsif which ls; ls -al ${ref_idx}*. ```. Here is the running output:. ```shell. /usr/bin/ls. ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. -rw-rw-r-- 1 zhoujianglin zhoujianglin 3042M Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. -rw-rw-r-- 1 zhoujianglin zhoujianglin 5985M Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:629,deployability,log,logs,629,"Hi @pichuan , Thanks for your reply. As your suggestion, after removing the double quote around the *, the `ls` run normally in `bash` script. However, the `ls` in `deepvariant` image of `singularity` still failed. Notably, If I add a `which ls;` before a `ls -al $ref_idx*`, it runs normally so that I can see the detailed information for the files.. Here is the script:. ```bash. #!/bin/bash. nthreads=32. dvsif=""/lustre/Data/toolsDB/deepvariant.sif"". ref_idx=""/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa"". wkdir=""/lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP"". bamdir=""${wkdir}/mappinged_bams"". logdir=""${wkdir}/logs"". vcfoutdir=""${wkdir}/DeepVariant_outputs"". source activate ~/.conda/envs/zjlEnv. echo -e ""ref_idx is $ref_idx\n"". if [ -f ""$ref_idx"" ];. then. echo -e ""ref_idx $ref_idx exists!\n"". which ls. echo -e ""ls -al --block=M ${ref_idx}*\n"". ls -al --block=M ${ref_idx}*. echo -e ""ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*\n"". ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. else. echo -e ""Warning!! ref_idx [$ref_idx] not exist!\n"". fi. echo -e ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ $dvsif ls -al ${ref_idx}*"". singularity run -B /usr/lib/locale/:/usr/lib/locale/ $dvsif ls -al ${ref_idx}*. echo -e ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ $dvsif which ls; ls -al ${ref_idx}*"". singularity run -B /usr/lib/locale/:/usr/lib/locale/ $dvsif which ls; ls -al ${ref_idx}*. ```. Here is the running output:. ```shell. /usr/bin/ls. ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. -rw-rw-r-- 1 zhoujianglin zhoujianglin 3042M Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. -rw-rw-r-- 1 zhoujianglin zhoujianglin 5985M Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:5812,deployability,fail,faild,5812,s: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac': No such file or directory. INFO: Cleaning up image... singularity run -B /usr/lib/locale/:/usr/lib/locale/ /lustre/Data/toolsDB/deepvariant.sif which ls; ls -al /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. INFO: Converting SIF file to temporary sandbox... /usr/bin/ls. INFO: Cleaning up image... -rw-rw-r-- 1 zhoujianglin zhoujianglin 3189750467 Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. -rw-rw-r-- 1 zhoujianglin zhoujianglin 6274909010 Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123. -rw-rw-r-- 1 zhoujianglin zhoujianglin 106669 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb. -rw-rw-r-- 1 zhoujianglin zhoujianglin 6924 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann. -rw-rw-r-- 1 zhoujianglin zhoujianglin 10196727247 Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64. -rw-rw-r-- 1 zhoujianglin zhoujianglin 2813 May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai. -rw-rw-r-- 1 zhoujianglin zhoujianglin 784363628 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac. ```. The real case command `singularity run -B /usr/lib/locale/:/usr/lib/locale/ /lustre/Data/toolsDB/deepvariant.sif /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa --reads=/path/to/2-13A_bwa2Hs37d5_sorted_dedup.bam --output_vcf=DeepVariant_outputs/2-13A.vcf.gz --output_gvcf=DeepVariant_outputs/2-13A.g.vcf.gz --num_shards=32` still faild. Did I do anything wrong?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:2408,modifiability,pac,pac,2408,/lib/locale/:/usr/lib/locale/ $dvsif which ls; ls -al ${ref_idx}*. ```. Here is the running output:. ```shell. /usr/bin/ls. ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. -rw-rw-r-- 1 zhoujianglin zhoujianglin 3042M Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. -rw-rw-r-- 1 zhoujianglin zhoujianglin 5985M Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann. -rw-rw-r-- 1 zhoujianglin zhoujianglin 9725M Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai. -rw-rw-r-- 1 zhoujianglin zhoujianglin 749M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac. ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. -rw-rw-r-- 1 zhoujianglin zhoujianglin 3042M Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. -rw-rw-r-- 1 zhoujianglin zhoujianglin 5985M Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann. -rw-rw-r-- 1 zhoujianglin zhoujianglin 9725M Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai. -rw-rw-r-- 1 zhoujianglin zhoujianglin 749M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac. singularity run -B /usr/lib/locale/:/usr/lib/locale/ /lustre/Data/toolsDB/deepvariant.sif ls -al /lustre/Data/toolsDB/,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:3287,modifiability,pac,pac,3287,.fa.fai. -rw-rw-r-- 1 zhoujianglin zhoujianglin 749M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac. ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. -rw-rw-r-- 1 zhoujianglin zhoujianglin 3042M Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. -rw-rw-r-- 1 zhoujianglin zhoujianglin 5985M Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann. -rw-rw-r-- 1 zhoujianglin zhoujianglin 9725M Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai. -rw-rw-r-- 1 zhoujianglin zhoujianglin 749M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac. singularity run -B /usr/lib/locale/:/usr/lib/locale/ /lustre/Data/toolsDB/deepvariant.sif ls -al /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. INFO: Converting SIF file to temporary sandbox... /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac': No such file or dir,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:4264,modifiability,pac,pac,4264,man_hs37d5/hs37d5.fa.pac. singularity run -B /usr/lib/locale/:/usr/lib/locale/ /lustre/Data/toolsDB/deepvariant.sif ls -al /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. INFO: Converting SIF file to temporary sandbox... /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac': No such file or directory. INFO: Cleaning up image... singularity run -B /usr/lib/locale/:/usr/lib/locale/ /lustre/Data/toolsDB/deepvariant.sif which ls; ls -al /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. INFO: Converting SIF file to temporary sandbox... /usr/bin/ls. INFO: Cleaning up image... -rw-rw-r-- 1 zhoujianglin zhoujianglin 3189750467 Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. -rw-rw-r-- 1 zhoujianglin zhoujianglin 6274909010 Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123. -rw-rw-r-- 1 zhoujianglin zhoujianglin 106669 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb. -rw-rw-r-- 1 zhoujianglin zhoujianglin 6924 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann. -rw-rw-r-- 1 zhoujianglin zhoujianglin 10196727247 Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64. -rw-rw-r-- 1 zhoujianglin zhoujianglin 2813 May 19 17:15 /lustre/Data/toolsDB/HostRefs/Hu,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:5407,modifiability,pac,pac,5407,s: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac': No such file or directory. INFO: Cleaning up image... singularity run -B /usr/lib/locale/:/usr/lib/locale/ /lustre/Data/toolsDB/deepvariant.sif which ls; ls -al /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. INFO: Converting SIF file to temporary sandbox... /usr/bin/ls. INFO: Cleaning up image... -rw-rw-r-- 1 zhoujianglin zhoujianglin 3189750467 Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. -rw-rw-r-- 1 zhoujianglin zhoujianglin 6274909010 Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123. -rw-rw-r-- 1 zhoujianglin zhoujianglin 106669 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb. -rw-rw-r-- 1 zhoujianglin zhoujianglin 6924 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann. -rw-rw-r-- 1 zhoujianglin zhoujianglin 10196727247 Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64. -rw-rw-r-- 1 zhoujianglin zhoujianglin 2813 May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai. -rw-rw-r-- 1 zhoujianglin zhoujianglin 784363628 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac. ```. The real case command `singularity run -B /usr/lib/locale/:/usr/lib/locale/ /lustre/Data/toolsDB/deepvariant.sif /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa --reads=/path/to/2-13A_bwa2Hs37d5_sorted_dedup.bam --output_vcf=DeepVariant_outputs/2-13A.vcf.gz --output_gvcf=DeepVariant_outputs/2-13A.g.vcf.gz --num_shards=32` still faild. Did I do anything wrong?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:207,reliability,fail,failed,207,"Hi @pichuan , Thanks for your reply. As your suggestion, after removing the double quote around the *, the `ls` run normally in `bash` script. However, the `ls` in `deepvariant` image of `singularity` still failed. Notably, If I add a `which ls;` before a `ls -al $ref_idx*`, it runs normally so that I can see the detailed information for the files.. Here is the script:. ```bash. #!/bin/bash. nthreads=32. dvsif=""/lustre/Data/toolsDB/deepvariant.sif"". ref_idx=""/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa"". wkdir=""/lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP"". bamdir=""${wkdir}/mappinged_bams"". logdir=""${wkdir}/logs"". vcfoutdir=""${wkdir}/DeepVariant_outputs"". source activate ~/.conda/envs/zjlEnv. echo -e ""ref_idx is $ref_idx\n"". if [ -f ""$ref_idx"" ];. then. echo -e ""ref_idx $ref_idx exists!\n"". which ls. echo -e ""ls -al --block=M ${ref_idx}*\n"". ls -al --block=M ${ref_idx}*. echo -e ""ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*\n"". ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. else. echo -e ""Warning!! ref_idx [$ref_idx] not exist!\n"". fi. echo -e ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ $dvsif ls -al ${ref_idx}*"". singularity run -B /usr/lib/locale/:/usr/lib/locale/ $dvsif ls -al ${ref_idx}*. echo -e ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ $dvsif which ls; ls -al ${ref_idx}*"". singularity run -B /usr/lib/locale/:/usr/lib/locale/ $dvsif which ls; ls -al ${ref_idx}*. ```. Here is the running output:. ```shell. /usr/bin/ls. ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. -rw-rw-r-- 1 zhoujianglin zhoujianglin 3042M Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. -rw-rw-r-- 1 zhoujianglin zhoujianglin 5985M Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:5812,reliability,fail,faild,5812,s: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac': No such file or directory. INFO: Cleaning up image... singularity run -B /usr/lib/locale/:/usr/lib/locale/ /lustre/Data/toolsDB/deepvariant.sif which ls; ls -al /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. INFO: Converting SIF file to temporary sandbox... /usr/bin/ls. INFO: Cleaning up image... -rw-rw-r-- 1 zhoujianglin zhoujianglin 3189750467 Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. -rw-rw-r-- 1 zhoujianglin zhoujianglin 6274909010 Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123. -rw-rw-r-- 1 zhoujianglin zhoujianglin 106669 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb. -rw-rw-r-- 1 zhoujianglin zhoujianglin 6924 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann. -rw-rw-r-- 1 zhoujianglin zhoujianglin 10196727247 Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64. -rw-rw-r-- 1 zhoujianglin zhoujianglin 2813 May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai. -rw-rw-r-- 1 zhoujianglin zhoujianglin 784363628 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac. ```. The real case command `singularity run -B /usr/lib/locale/:/usr/lib/locale/ /lustre/Data/toolsDB/deepvariant.sif /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa --reads=/path/to/2-13A_bwa2Hs37d5_sorted_dedup.bam --output_vcf=DeepVariant_outputs/2-13A.vcf.gz --output_gvcf=DeepVariant_outputs/2-13A.g.vcf.gz --num_shards=32` still faild. Did I do anything wrong?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:612,safety,log,logdir,612,"Hi @pichuan , Thanks for your reply. As your suggestion, after removing the double quote around the *, the `ls` run normally in `bash` script. However, the `ls` in `deepvariant` image of `singularity` still failed. Notably, If I add a `which ls;` before a `ls -al $ref_idx*`, it runs normally so that I can see the detailed information for the files.. Here is the script:. ```bash. #!/bin/bash. nthreads=32. dvsif=""/lustre/Data/toolsDB/deepvariant.sif"". ref_idx=""/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa"". wkdir=""/lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP"". bamdir=""${wkdir}/mappinged_bams"". logdir=""${wkdir}/logs"". vcfoutdir=""${wkdir}/DeepVariant_outputs"". source activate ~/.conda/envs/zjlEnv. echo -e ""ref_idx is $ref_idx\n"". if [ -f ""$ref_idx"" ];. then. echo -e ""ref_idx $ref_idx exists!\n"". which ls. echo -e ""ls -al --block=M ${ref_idx}*\n"". ls -al --block=M ${ref_idx}*. echo -e ""ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*\n"". ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. else. echo -e ""Warning!! ref_idx [$ref_idx] not exist!\n"". fi. echo -e ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ $dvsif ls -al ${ref_idx}*"". singularity run -B /usr/lib/locale/:/usr/lib/locale/ $dvsif ls -al ${ref_idx}*. echo -e ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ $dvsif which ls; ls -al ${ref_idx}*"". singularity run -B /usr/lib/locale/:/usr/lib/locale/ $dvsif which ls; ls -al ${ref_idx}*. ```. Here is the running output:. ```shell. /usr/bin/ls. ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. -rw-rw-r-- 1 zhoujianglin zhoujianglin 3042M Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. -rw-rw-r-- 1 zhoujianglin zhoujianglin 5985M Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:629,safety,log,logs,629,"Hi @pichuan , Thanks for your reply. As your suggestion, after removing the double quote around the *, the `ls` run normally in `bash` script. However, the `ls` in `deepvariant` image of `singularity` still failed. Notably, If I add a `which ls;` before a `ls -al $ref_idx*`, it runs normally so that I can see the detailed information for the files.. Here is the script:. ```bash. #!/bin/bash. nthreads=32. dvsif=""/lustre/Data/toolsDB/deepvariant.sif"". ref_idx=""/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa"". wkdir=""/lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP"". bamdir=""${wkdir}/mappinged_bams"". logdir=""${wkdir}/logs"". vcfoutdir=""${wkdir}/DeepVariant_outputs"". source activate ~/.conda/envs/zjlEnv. echo -e ""ref_idx is $ref_idx\n"". if [ -f ""$ref_idx"" ];. then. echo -e ""ref_idx $ref_idx exists!\n"". which ls. echo -e ""ls -al --block=M ${ref_idx}*\n"". ls -al --block=M ${ref_idx}*. echo -e ""ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*\n"". ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. else. echo -e ""Warning!! ref_idx [$ref_idx] not exist!\n"". fi. echo -e ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ $dvsif ls -al ${ref_idx}*"". singularity run -B /usr/lib/locale/:/usr/lib/locale/ $dvsif ls -al ${ref_idx}*. echo -e ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ $dvsif which ls; ls -al ${ref_idx}*"". singularity run -B /usr/lib/locale/:/usr/lib/locale/ $dvsif which ls; ls -al ${ref_idx}*. ```. Here is the running output:. ```shell. /usr/bin/ls. ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. -rw-rw-r-- 1 zhoujianglin zhoujianglin 3042M Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. -rw-rw-r-- 1 zhoujianglin zhoujianglin 5985M Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:612,security,log,logdir,612,"Hi @pichuan , Thanks for your reply. As your suggestion, after removing the double quote around the *, the `ls` run normally in `bash` script. However, the `ls` in `deepvariant` image of `singularity` still failed. Notably, If I add a `which ls;` before a `ls -al $ref_idx*`, it runs normally so that I can see the detailed information for the files.. Here is the script:. ```bash. #!/bin/bash. nthreads=32. dvsif=""/lustre/Data/toolsDB/deepvariant.sif"". ref_idx=""/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa"". wkdir=""/lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP"". bamdir=""${wkdir}/mappinged_bams"". logdir=""${wkdir}/logs"". vcfoutdir=""${wkdir}/DeepVariant_outputs"". source activate ~/.conda/envs/zjlEnv. echo -e ""ref_idx is $ref_idx\n"". if [ -f ""$ref_idx"" ];. then. echo -e ""ref_idx $ref_idx exists!\n"". which ls. echo -e ""ls -al --block=M ${ref_idx}*\n"". ls -al --block=M ${ref_idx}*. echo -e ""ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*\n"". ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. else. echo -e ""Warning!! ref_idx [$ref_idx] not exist!\n"". fi. echo -e ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ $dvsif ls -al ${ref_idx}*"". singularity run -B /usr/lib/locale/:/usr/lib/locale/ $dvsif ls -al ${ref_idx}*. echo -e ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ $dvsif which ls; ls -al ${ref_idx}*"". singularity run -B /usr/lib/locale/:/usr/lib/locale/ $dvsif which ls; ls -al ${ref_idx}*. ```. Here is the running output:. ```shell. /usr/bin/ls. ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. -rw-rw-r-- 1 zhoujianglin zhoujianglin 3042M Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. -rw-rw-r-- 1 zhoujianglin zhoujianglin 5985M Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:629,security,log,logs,629,"Hi @pichuan , Thanks for your reply. As your suggestion, after removing the double quote around the *, the `ls` run normally in `bash` script. However, the `ls` in `deepvariant` image of `singularity` still failed. Notably, If I add a `which ls;` before a `ls -al $ref_idx*`, it runs normally so that I can see the detailed information for the files.. Here is the script:. ```bash. #!/bin/bash. nthreads=32. dvsif=""/lustre/Data/toolsDB/deepvariant.sif"". ref_idx=""/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa"". wkdir=""/lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP"". bamdir=""${wkdir}/mappinged_bams"". logdir=""${wkdir}/logs"". vcfoutdir=""${wkdir}/DeepVariant_outputs"". source activate ~/.conda/envs/zjlEnv. echo -e ""ref_idx is $ref_idx\n"". if [ -f ""$ref_idx"" ];. then. echo -e ""ref_idx $ref_idx exists!\n"". which ls. echo -e ""ls -al --block=M ${ref_idx}*\n"". ls -al --block=M ${ref_idx}*. echo -e ""ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*\n"". ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. else. echo -e ""Warning!! ref_idx [$ref_idx] not exist!\n"". fi. echo -e ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ $dvsif ls -al ${ref_idx}*"". singularity run -B /usr/lib/locale/:/usr/lib/locale/ $dvsif ls -al ${ref_idx}*. echo -e ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ $dvsif which ls; ls -al ${ref_idx}*"". singularity run -B /usr/lib/locale/:/usr/lib/locale/ $dvsif which ls; ls -al ${ref_idx}*. ```. Here is the running output:. ```shell. /usr/bin/ls. ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. -rw-rw-r-- 1 zhoujianglin zhoujianglin 3042M Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. -rw-rw-r-- 1 zhoujianglin zhoujianglin 5985M Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:3483,security,sandbox,sandbox,3483,w-rw-r-- 1 zhoujianglin zhoujianglin 3042M Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. -rw-rw-r-- 1 zhoujianglin zhoujianglin 5985M Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann. -rw-rw-r-- 1 zhoujianglin zhoujianglin 9725M Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai. -rw-rw-r-- 1 zhoujianglin zhoujianglin 749M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac. singularity run -B /usr/lib/locale/:/usr/lib/locale/ /lustre/Data/toolsDB/deepvariant.sif ls -al /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. INFO: Converting SIF file to temporary sandbox... /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac': No such file or directory. INFO: Cleaning up image... singularity run -B /usr/lib/locale/:/usr/lib/locale/ /lustre/Data/toolsDB/deepvariant.sif which ls; ls -al /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. I,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:3514,security,access,access,3514,anglin 3042M Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. -rw-rw-r-- 1 zhoujianglin zhoujianglin 5985M Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann. -rw-rw-r-- 1 zhoujianglin zhoujianglin 9725M Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai. -rw-rw-r-- 1 zhoujianglin zhoujianglin 749M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac. singularity run -B /usr/lib/locale/:/usr/lib/locale/ /lustre/Data/toolsDB/deepvariant.sif ls -al /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. INFO: Converting SIF file to temporary sandbox... /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac': No such file or directory. INFO: Cleaning up image... singularity run -B /usr/lib/locale/:/usr/lib/locale/ /lustre/Data/toolsDB/deepvariant.sif which ls; ls -al /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. INFO: Converting SIF file to te,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:3624,security,access,access,3624,jianglin 5985M Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann. -rw-rw-r-- 1 zhoujianglin zhoujianglin 9725M Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai. -rw-rw-r-- 1 zhoujianglin zhoujianglin 749M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac. singularity run -B /usr/lib/locale/:/usr/lib/locale/ /lustre/Data/toolsDB/deepvariant.sif ls -al /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. INFO: Converting SIF file to temporary sandbox... /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac': No such file or directory. INFO: Cleaning up image... singularity run -B /usr/lib/locale/:/usr/lib/locale/ /lustre/Data/toolsDB/deepvariant.sif which ls; ls -al /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. INFO: Converting SIF file to temporary sandbox... /usr/bin/ls. INFO: Cleaning up image... -rw-rw-r-- 1 zhoujianglin zhoujianglin 3189750467 A,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:3739,security,access,access,3739,oujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann. -rw-rw-r-- 1 zhoujianglin zhoujianglin 9725M Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai. -rw-rw-r-- 1 zhoujianglin zhoujianglin 749M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac. singularity run -B /usr/lib/locale/:/usr/lib/locale/ /lustre/Data/toolsDB/deepvariant.sif ls -al /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. INFO: Converting SIF file to temporary sandbox... /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac': No such file or directory. INFO: Cleaning up image... singularity run -B /usr/lib/locale/:/usr/lib/locale/ /lustre/Data/toolsDB/deepvariant.sif which ls; ls -al /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. INFO: Converting SIF file to temporary sandbox... /usr/bin/ls. INFO: Cleaning up image... -rw-rw-r-- 1 zhoujianglin zhoujianglin 3189750467 Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. -rw-rw-r-- 1 zhoujianglin zhoujianglin 6274909010,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:3853,security,access,access,3853,ujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann. -rw-rw-r-- 1 zhoujianglin zhoujianglin 9725M Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai. -rw-rw-r-- 1 zhoujianglin zhoujianglin 749M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac. singularity run -B /usr/lib/locale/:/usr/lib/locale/ /lustre/Data/toolsDB/deepvariant.sif ls -al /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. INFO: Converting SIF file to temporary sandbox... /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac': No such file or directory. INFO: Cleaning up image... singularity run -B /usr/lib/locale/:/usr/lib/locale/ /lustre/Data/toolsDB/deepvariant.sif which ls; ls -al /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. INFO: Converting SIF file to temporary sandbox... /usr/bin/ls. INFO: Cleaning up image... -rw-rw-r-- 1 zhoujianglin zhoujianglin 3189750467 Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. -rw-rw-r-- 1 zhoujianglin zhoujianglin 6274909010 Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123. -rw-rw-r-- 1 zhoujianglin zhoujianglin 10,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:3967,security,access,access,3967,jianglin 9725M Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai. -rw-rw-r-- 1 zhoujianglin zhoujianglin 749M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac. singularity run -B /usr/lib/locale/:/usr/lib/locale/ /lustre/Data/toolsDB/deepvariant.sif ls -al /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. INFO: Converting SIF file to temporary sandbox... /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac': No such file or directory. INFO: Cleaning up image... singularity run -B /usr/lib/locale/:/usr/lib/locale/ /lustre/Data/toolsDB/deepvariant.sif which ls; ls -al /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. INFO: Converting SIF file to temporary sandbox... /usr/bin/ls. INFO: Cleaning up image... -rw-rw-r-- 1 zhoujianglin zhoujianglin 3189750467 Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. -rw-rw-r-- 1 zhoujianglin zhoujianglin 6274909010 Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123. -rw-rw-r-- 1 zhoujianglin zhoujianglin 106669 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb. -rw-rw-r-- 1 zhoujianglin zhoujianglin,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:4089,security,access,access,4089,oujianglin 1M May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai. -rw-rw-r-- 1 zhoujianglin zhoujianglin 749M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac. singularity run -B /usr/lib/locale/:/usr/lib/locale/ /lustre/Data/toolsDB/deepvariant.sif ls -al /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. INFO: Converting SIF file to temporary sandbox... /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac': No such file or directory. INFO: Cleaning up image... singularity run -B /usr/lib/locale/:/usr/lib/locale/ /lustre/Data/toolsDB/deepvariant.sif which ls; ls -al /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. INFO: Converting SIF file to temporary sandbox... /usr/bin/ls. INFO: Cleaning up image... -rw-rw-r-- 1 zhoujianglin zhoujianglin 3189750467 Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. -rw-rw-r-- 1 zhoujianglin zhoujianglin 6274909010 Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123. -rw-rw-r-- 1 zhoujianglin zhoujianglin 106669 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb. -rw-rw-r-- 1 zhoujianglin zhoujianglin 6924 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann. -rw-rw-r-- 1 zhoujianglin zhoujianglin 101967,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:4203,security,access,access,4203,ujianglin 749M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac. singularity run -B /usr/lib/locale/:/usr/lib/locale/ /lustre/Data/toolsDB/deepvariant.sif ls -al /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. INFO: Converting SIF file to temporary sandbox... /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac': No such file or directory. INFO: Cleaning up image... singularity run -B /usr/lib/locale/:/usr/lib/locale/ /lustre/Data/toolsDB/deepvariant.sif which ls; ls -al /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. INFO: Converting SIF file to temporary sandbox... /usr/bin/ls. INFO: Cleaning up image... -rw-rw-r-- 1 zhoujianglin zhoujianglin 3189750467 Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. -rw-rw-r-- 1 zhoujianglin zhoujianglin 6274909010 Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123. -rw-rw-r-- 1 zhoujianglin zhoujianglin 106669 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb. -rw-rw-r-- 1 zhoujianglin zhoujianglin 6924 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann. -rw-rw-r-- 1 zhoujianglin zhoujianglin 10196727247 Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64. -rw-rw-r-- 1 zhoujianglin zho,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:4525,security,sandbox,sandbox,4525,/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac': No such file or directory. INFO: Cleaning up image... singularity run -B /usr/lib/locale/:/usr/lib/locale/ /lustre/Data/toolsDB/deepvariant.sif which ls; ls -al /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. INFO: Converting SIF file to temporary sandbox... /usr/bin/ls. INFO: Cleaning up image... -rw-rw-r-- 1 zhoujianglin zhoujianglin 3189750467 Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. -rw-rw-r-- 1 zhoujianglin zhoujianglin 6274909010 Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123. -rw-rw-r-- 1 zhoujianglin zhoujianglin 106669 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb. -rw-rw-r-- 1 zhoujianglin zhoujianglin 6924 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann. -rw-rw-r-- 1 zhoujianglin zhoujianglin 10196727247 Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64. -rw-rw-r-- 1 zhoujianglin zhoujianglin 2813 May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai. -rw-rw-r-- 1 zhoujianglin zhoujianglin 784363628 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac. ```. The real case command `singularity run -B /usr/lib/locale/:/usr/lib/locale/ /lustre/Data/toolsDB/deepvariant.sif,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:612,testability,log,logdir,612,"Hi @pichuan , Thanks for your reply. As your suggestion, after removing the double quote around the *, the `ls` run normally in `bash` script. However, the `ls` in `deepvariant` image of `singularity` still failed. Notably, If I add a `which ls;` before a `ls -al $ref_idx*`, it runs normally so that I can see the detailed information for the files.. Here is the script:. ```bash. #!/bin/bash. nthreads=32. dvsif=""/lustre/Data/toolsDB/deepvariant.sif"". ref_idx=""/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa"". wkdir=""/lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP"". bamdir=""${wkdir}/mappinged_bams"". logdir=""${wkdir}/logs"". vcfoutdir=""${wkdir}/DeepVariant_outputs"". source activate ~/.conda/envs/zjlEnv. echo -e ""ref_idx is $ref_idx\n"". if [ -f ""$ref_idx"" ];. then. echo -e ""ref_idx $ref_idx exists!\n"". which ls. echo -e ""ls -al --block=M ${ref_idx}*\n"". ls -al --block=M ${ref_idx}*. echo -e ""ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*\n"". ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. else. echo -e ""Warning!! ref_idx [$ref_idx] not exist!\n"". fi. echo -e ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ $dvsif ls -al ${ref_idx}*"". singularity run -B /usr/lib/locale/:/usr/lib/locale/ $dvsif ls -al ${ref_idx}*. echo -e ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ $dvsif which ls; ls -al ${ref_idx}*"". singularity run -B /usr/lib/locale/:/usr/lib/locale/ $dvsif which ls; ls -al ${ref_idx}*. ```. Here is the running output:. ```shell. /usr/bin/ls. ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. -rw-rw-r-- 1 zhoujianglin zhoujianglin 3042M Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. -rw-rw-r-- 1 zhoujianglin zhoujianglin 5985M Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:629,testability,log,logs,629,"Hi @pichuan , Thanks for your reply. As your suggestion, after removing the double quote around the *, the `ls` run normally in `bash` script. However, the `ls` in `deepvariant` image of `singularity` still failed. Notably, If I add a `which ls;` before a `ls -al $ref_idx*`, it runs normally so that I can see the detailed information for the files.. Here is the script:. ```bash. #!/bin/bash. nthreads=32. dvsif=""/lustre/Data/toolsDB/deepvariant.sif"". ref_idx=""/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa"". wkdir=""/lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP"". bamdir=""${wkdir}/mappinged_bams"". logdir=""${wkdir}/logs"". vcfoutdir=""${wkdir}/DeepVariant_outputs"". source activate ~/.conda/envs/zjlEnv. echo -e ""ref_idx is $ref_idx\n"". if [ -f ""$ref_idx"" ];. then. echo -e ""ref_idx $ref_idx exists!\n"". which ls. echo -e ""ls -al --block=M ${ref_idx}*\n"". ls -al --block=M ${ref_idx}*. echo -e ""ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*\n"". ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. else. echo -e ""Warning!! ref_idx [$ref_idx] not exist!\n"". fi. echo -e ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ $dvsif ls -al ${ref_idx}*"". singularity run -B /usr/lib/locale/:/usr/lib/locale/ $dvsif ls -al ${ref_idx}*. echo -e ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ $dvsif which ls; ls -al ${ref_idx}*"". singularity run -B /usr/lib/locale/:/usr/lib/locale/ $dvsif which ls; ls -al ${ref_idx}*. ```. Here is the running output:. ```shell. /usr/bin/ls. ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. -rw-rw-r-- 1 zhoujianglin zhoujianglin 3042M Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. -rw-rw-r-- 1 zhoujianglin zhoujianglin 5985M Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:428,usability,tool,toolsDB,428,"Hi @pichuan , Thanks for your reply. As your suggestion, after removing the double quote around the *, the `ls` run normally in `bash` script. However, the `ls` in `deepvariant` image of `singularity` still failed. Notably, If I add a `which ls;` before a `ls -al $ref_idx*`, it runs normally so that I can see the detailed information for the files.. Here is the script:. ```bash. #!/bin/bash. nthreads=32. dvsif=""/lustre/Data/toolsDB/deepvariant.sif"". ref_idx=""/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa"". wkdir=""/lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP"". bamdir=""${wkdir}/mappinged_bams"". logdir=""${wkdir}/logs"". vcfoutdir=""${wkdir}/DeepVariant_outputs"". source activate ~/.conda/envs/zjlEnv. echo -e ""ref_idx is $ref_idx\n"". if [ -f ""$ref_idx"" ];. then. echo -e ""ref_idx $ref_idx exists!\n"". which ls. echo -e ""ls -al --block=M ${ref_idx}*\n"". ls -al --block=M ${ref_idx}*. echo -e ""ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*\n"". ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. else. echo -e ""Warning!! ref_idx [$ref_idx] not exist!\n"". fi. echo -e ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ $dvsif ls -al ${ref_idx}*"". singularity run -B /usr/lib/locale/:/usr/lib/locale/ $dvsif ls -al ${ref_idx}*. echo -e ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ $dvsif which ls; ls -al ${ref_idx}*"". singularity run -B /usr/lib/locale/:/usr/lib/locale/ $dvsif which ls; ls -al ${ref_idx}*. ```. Here is the running output:. ```shell. /usr/bin/ls. ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. -rw-rw-r-- 1 zhoujianglin zhoujianglin 3042M Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. -rw-rw-r-- 1 zhoujianglin zhoujianglin 5985M Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:476,usability,tool,toolsDB,476,"Hi @pichuan , Thanks for your reply. As your suggestion, after removing the double quote around the *, the `ls` run normally in `bash` script. However, the `ls` in `deepvariant` image of `singularity` still failed. Notably, If I add a `which ls;` before a `ls -al $ref_idx*`, it runs normally so that I can see the detailed information for the files.. Here is the script:. ```bash. #!/bin/bash. nthreads=32. dvsif=""/lustre/Data/toolsDB/deepvariant.sif"". ref_idx=""/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa"". wkdir=""/lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP"". bamdir=""${wkdir}/mappinged_bams"". logdir=""${wkdir}/logs"". vcfoutdir=""${wkdir}/DeepVariant_outputs"". source activate ~/.conda/envs/zjlEnv. echo -e ""ref_idx is $ref_idx\n"". if [ -f ""$ref_idx"" ];. then. echo -e ""ref_idx $ref_idx exists!\n"". which ls. echo -e ""ls -al --block=M ${ref_idx}*\n"". ls -al --block=M ${ref_idx}*. echo -e ""ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*\n"". ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. else. echo -e ""Warning!! ref_idx [$ref_idx] not exist!\n"". fi. echo -e ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ $dvsif ls -al ${ref_idx}*"". singularity run -B /usr/lib/locale/:/usr/lib/locale/ $dvsif ls -al ${ref_idx}*. echo -e ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ $dvsif which ls; ls -al ${ref_idx}*"". singularity run -B /usr/lib/locale/:/usr/lib/locale/ $dvsif which ls; ls -al ${ref_idx}*. ```. Here is the running output:. ```shell. /usr/bin/ls. ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. -rw-rw-r-- 1 zhoujianglin zhoujianglin 3042M Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. -rw-rw-r-- 1 zhoujianglin zhoujianglin 5985M Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:937,usability,tool,toolsDB,937,"Hi @pichuan , Thanks for your reply. As your suggestion, after removing the double quote around the *, the `ls` run normally in `bash` script. However, the `ls` in `deepvariant` image of `singularity` still failed. Notably, If I add a `which ls;` before a `ls -al $ref_idx*`, it runs normally so that I can see the detailed information for the files.. Here is the script:. ```bash. #!/bin/bash. nthreads=32. dvsif=""/lustre/Data/toolsDB/deepvariant.sif"". ref_idx=""/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa"". wkdir=""/lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP"". bamdir=""${wkdir}/mappinged_bams"". logdir=""${wkdir}/logs"". vcfoutdir=""${wkdir}/DeepVariant_outputs"". source activate ~/.conda/envs/zjlEnv. echo -e ""ref_idx is $ref_idx\n"". if [ -f ""$ref_idx"" ];. then. echo -e ""ref_idx $ref_idx exists!\n"". which ls. echo -e ""ls -al --block=M ${ref_idx}*\n"". ls -al --block=M ${ref_idx}*. echo -e ""ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*\n"". ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. else. echo -e ""Warning!! ref_idx [$ref_idx] not exist!\n"". fi. echo -e ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ $dvsif ls -al ${ref_idx}*"". singularity run -B /usr/lib/locale/:/usr/lib/locale/ $dvsif ls -al ${ref_idx}*. echo -e ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ $dvsif which ls; ls -al ${ref_idx}*"". singularity run -B /usr/lib/locale/:/usr/lib/locale/ $dvsif which ls; ls -al ${ref_idx}*. ```. Here is the running output:. ```shell. /usr/bin/ls. ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. -rw-rw-r-- 1 zhoujianglin zhoujianglin 3042M Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. -rw-rw-r-- 1 zhoujianglin zhoujianglin 5985M Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:1012,usability,tool,toolsDB,1012,"anks for your reply. As your suggestion, after removing the double quote around the *, the `ls` run normally in `bash` script. However, the `ls` in `deepvariant` image of `singularity` still failed. Notably, If I add a `which ls;` before a `ls -al $ref_idx*`, it runs normally so that I can see the detailed information for the files.. Here is the script:. ```bash. #!/bin/bash. nthreads=32. dvsif=""/lustre/Data/toolsDB/deepvariant.sif"". ref_idx=""/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa"". wkdir=""/lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP"". bamdir=""${wkdir}/mappinged_bams"". logdir=""${wkdir}/logs"". vcfoutdir=""${wkdir}/DeepVariant_outputs"". source activate ~/.conda/envs/zjlEnv. echo -e ""ref_idx is $ref_idx\n"". if [ -f ""$ref_idx"" ];. then. echo -e ""ref_idx $ref_idx exists!\n"". which ls. echo -e ""ls -al --block=M ${ref_idx}*\n"". ls -al --block=M ${ref_idx}*. echo -e ""ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*\n"". ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. else. echo -e ""Warning!! ref_idx [$ref_idx] not exist!\n"". fi. echo -e ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ $dvsif ls -al ${ref_idx}*"". singularity run -B /usr/lib/locale/:/usr/lib/locale/ $dvsif ls -al ${ref_idx}*. echo -e ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ $dvsif which ls; ls -al ${ref_idx}*"". singularity run -B /usr/lib/locale/:/usr/lib/locale/ $dvsif which ls; ls -al ${ref_idx}*. ```. Here is the running output:. ```shell. /usr/bin/ls. ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. -rw-rw-r-- 1 zhoujianglin zhoujianglin 3042M Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. -rw-rw-r-- 1 zhoujianglin zhoujianglin 5985M Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:1564,usability,tool,toolsDB,1564,"FSZ_SNP"". bamdir=""${wkdir}/mappinged_bams"". logdir=""${wkdir}/logs"". vcfoutdir=""${wkdir}/DeepVariant_outputs"". source activate ~/.conda/envs/zjlEnv. echo -e ""ref_idx is $ref_idx\n"". if [ -f ""$ref_idx"" ];. then. echo -e ""ref_idx $ref_idx exists!\n"". which ls. echo -e ""ls -al --block=M ${ref_idx}*\n"". ls -al --block=M ${ref_idx}*. echo -e ""ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*\n"". ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. else. echo -e ""Warning!! ref_idx [$ref_idx] not exist!\n"". fi. echo -e ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ $dvsif ls -al ${ref_idx}*"". singularity run -B /usr/lib/locale/:/usr/lib/locale/ $dvsif ls -al ${ref_idx}*. echo -e ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ $dvsif which ls; ls -al ${ref_idx}*"". singularity run -B /usr/lib/locale/:/usr/lib/locale/ $dvsif which ls; ls -al ${ref_idx}*. ```. Here is the running output:. ```shell. /usr/bin/ls. ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. -rw-rw-r-- 1 zhoujianglin zhoujianglin 3042M Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. -rw-rw-r-- 1 zhoujianglin zhoujianglin 5985M Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann. -rw-rw-r-- 1 zhoujianglin zhoujianglin 9725M Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai. -rw-rw-r-- 1 zhoujianglin zhoujianglin 749M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac. ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. -rw-rw-r-- 1 zhoujianglin zhoujianglin 3042M Apr 26 14:53 /lustre/Data/toolsDB/Host",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:1677,usability,tool,toolsDB,1677,"rce activate ~/.conda/envs/zjlEnv. echo -e ""ref_idx is $ref_idx\n"". if [ -f ""$ref_idx"" ];. then. echo -e ""ref_idx $ref_idx exists!\n"". which ls. echo -e ""ls -al --block=M ${ref_idx}*\n"". ls -al --block=M ${ref_idx}*. echo -e ""ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*\n"". ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. else. echo -e ""Warning!! ref_idx [$ref_idx] not exist!\n"". fi. echo -e ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ $dvsif ls -al ${ref_idx}*"". singularity run -B /usr/lib/locale/:/usr/lib/locale/ $dvsif ls -al ${ref_idx}*. echo -e ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ $dvsif which ls; ls -al ${ref_idx}*"". singularity run -B /usr/lib/locale/:/usr/lib/locale/ $dvsif which ls; ls -al ${ref_idx}*. ```. Here is the running output:. ```shell. /usr/bin/ls. ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. -rw-rw-r-- 1 zhoujianglin zhoujianglin 3042M Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. -rw-rw-r-- 1 zhoujianglin zhoujianglin 5985M Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann. -rw-rw-r-- 1 zhoujianglin zhoujianglin 9725M Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai. -rw-rw-r-- 1 zhoujianglin zhoujianglin 749M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac. ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. -rw-rw-r-- 1 zhoujianglin zhoujianglin 3042M Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. -rw-rw-r-- 1 zhoujianglin zhoujianglin 5985M Apr 26 15:23 /lustre/Data/toolsDB/HostR",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:1789,usability,tool,toolsDB,1789,"x $ref_idx exists!\n"". which ls. echo -e ""ls -al --block=M ${ref_idx}*\n"". ls -al --block=M ${ref_idx}*. echo -e ""ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*\n"". ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. else. echo -e ""Warning!! ref_idx [$ref_idx] not exist!\n"". fi. echo -e ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ $dvsif ls -al ${ref_idx}*"". singularity run -B /usr/lib/locale/:/usr/lib/locale/ $dvsif ls -al ${ref_idx}*. echo -e ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ $dvsif which ls; ls -al ${ref_idx}*"". singularity run -B /usr/lib/locale/:/usr/lib/locale/ $dvsif which ls; ls -al ${ref_idx}*. ```. Here is the running output:. ```shell. /usr/bin/ls. ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. -rw-rw-r-- 1 zhoujianglin zhoujianglin 3042M Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. -rw-rw-r-- 1 zhoujianglin zhoujianglin 5985M Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann. -rw-rw-r-- 1 zhoujianglin zhoujianglin 9725M Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai. -rw-rw-r-- 1 zhoujianglin zhoujianglin 749M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac. ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. -rw-rw-r-- 1 zhoujianglin zhoujianglin 3042M Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. -rw-rw-r-- 1 zhoujianglin zhoujianglin 5985M Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/Hos",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:1903,usability,tool,toolsDB,1903,"ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*\n"". ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. else. echo -e ""Warning!! ref_idx [$ref_idx] not exist!\n"". fi. echo -e ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ $dvsif ls -al ${ref_idx}*"". singularity run -B /usr/lib/locale/:/usr/lib/locale/ $dvsif ls -al ${ref_idx}*. echo -e ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ $dvsif which ls; ls -al ${ref_idx}*"". singularity run -B /usr/lib/locale/:/usr/lib/locale/ $dvsif which ls; ls -al ${ref_idx}*. ```. Here is the running output:. ```shell. /usr/bin/ls. ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. -rw-rw-r-- 1 zhoujianglin zhoujianglin 3042M Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. -rw-rw-r-- 1 zhoujianglin zhoujianglin 5985M Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann. -rw-rw-r-- 1 zhoujianglin zhoujianglin 9725M Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai. -rw-rw-r-- 1 zhoujianglin zhoujianglin 749M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac. ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. -rw-rw-r-- 1 zhoujianglin zhoujianglin 3042M Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. -rw-rw-r-- 1 zhoujianglin zhoujianglin 5985M Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/Host",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:2016,usability,tool,toolsDB,2016,"HostRefs/Human_hs37d5/hs37d5.fa*. else. echo -e ""Warning!! ref_idx [$ref_idx] not exist!\n"". fi. echo -e ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ $dvsif ls -al ${ref_idx}*"". singularity run -B /usr/lib/locale/:/usr/lib/locale/ $dvsif ls -al ${ref_idx}*. echo -e ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ $dvsif which ls; ls -al ${ref_idx}*"". singularity run -B /usr/lib/locale/:/usr/lib/locale/ $dvsif which ls; ls -al ${ref_idx}*. ```. Here is the running output:. ```shell. /usr/bin/ls. ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. -rw-rw-r-- 1 zhoujianglin zhoujianglin 3042M Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. -rw-rw-r-- 1 zhoujianglin zhoujianglin 5985M Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann. -rw-rw-r-- 1 zhoujianglin zhoujianglin 9725M Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai. -rw-rw-r-- 1 zhoujianglin zhoujianglin 749M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac. ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. -rw-rw-r-- 1 zhoujianglin zhoujianglin 3042M Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. -rw-rw-r-- 1 zhoujianglin zhoujianglin 5985M Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann. -rw-rw-r-- 1 zhoujianglin zhoujianglin 9725M Apr 26 15:42 /lustre/Data/toolsDB/H",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:2132,usability,tool,toolsDB,2132,"y run -B /usr/lib/locale/:/usr/lib/locale/ $dvsif ls -al ${ref_idx}*"". singularity run -B /usr/lib/locale/:/usr/lib/locale/ $dvsif ls -al ${ref_idx}*. echo -e ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ $dvsif which ls; ls -al ${ref_idx}*"". singularity run -B /usr/lib/locale/:/usr/lib/locale/ $dvsif which ls; ls -al ${ref_idx}*. ```. Here is the running output:. ```shell. /usr/bin/ls. ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. -rw-rw-r-- 1 zhoujianglin zhoujianglin 3042M Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. -rw-rw-r-- 1 zhoujianglin zhoujianglin 5985M Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann. -rw-rw-r-- 1 zhoujianglin zhoujianglin 9725M Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai. -rw-rw-r-- 1 zhoujianglin zhoujianglin 749M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac. ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. -rw-rw-r-- 1 zhoujianglin zhoujianglin 3042M Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. -rw-rw-r-- 1 zhoujianglin zhoujianglin 5985M Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann. -rw-rw-r-- 1 zhoujianglin zhoujianglin 9725M Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M May 19 17:15 /lustre/Data/tool",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:2253,usability,tool,toolsDB,2253,"e/ $dvsif ls -al ${ref_idx}*. echo -e ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ $dvsif which ls; ls -al ${ref_idx}*"". singularity run -B /usr/lib/locale/:/usr/lib/locale/ $dvsif which ls; ls -al ${ref_idx}*. ```. Here is the running output:. ```shell. /usr/bin/ls. ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. -rw-rw-r-- 1 zhoujianglin zhoujianglin 3042M Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. -rw-rw-r-- 1 zhoujianglin zhoujianglin 5985M Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann. -rw-rw-r-- 1 zhoujianglin zhoujianglin 9725M Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai. -rw-rw-r-- 1 zhoujianglin zhoujianglin 749M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac. ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. -rw-rw-r-- 1 zhoujianglin zhoujianglin 3042M Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. -rw-rw-r-- 1 zhoujianglin zhoujianglin 5985M Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann. -rw-rw-r-- 1 zhoujianglin zhoujianglin 9725M Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai. -rw-rw-r-- 1 zhoujianglin zhoujianglin 749M Apr 26 15:21 /lustre/Data/toolsDB/Ho",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:2368,usability,tool,toolsDB,2368," ${ref_idx}*"". singularity run -B /usr/lib/locale/:/usr/lib/locale/ $dvsif which ls; ls -al ${ref_idx}*. ```. Here is the running output:. ```shell. /usr/bin/ls. ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. -rw-rw-r-- 1 zhoujianglin zhoujianglin 3042M Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. -rw-rw-r-- 1 zhoujianglin zhoujianglin 5985M Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann. -rw-rw-r-- 1 zhoujianglin zhoujianglin 9725M Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai. -rw-rw-r-- 1 zhoujianglin zhoujianglin 749M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac. ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. -rw-rw-r-- 1 zhoujianglin zhoujianglin 3042M Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. -rw-rw-r-- 1 zhoujianglin zhoujianglin 5985M Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann. -rw-rw-r-- 1 zhoujianglin zhoujianglin 9725M Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai. -rw-rw-r-- 1 zhoujianglin zhoujianglin 749M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac. singularity run -B /usr/lib/locale/:/usr/lib/locale/ /lustre/Data/toolsDB/deepva",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:2443,usability,tool,toolsDB,2443,which ls; ls -al ${ref_idx}*. ```. Here is the running output:. ```shell. /usr/bin/ls. ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. -rw-rw-r-- 1 zhoujianglin zhoujianglin 3042M Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. -rw-rw-r-- 1 zhoujianglin zhoujianglin 5985M Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann. -rw-rw-r-- 1 zhoujianglin zhoujianglin 9725M Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai. -rw-rw-r-- 1 zhoujianglin zhoujianglin 749M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac. ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. -rw-rw-r-- 1 zhoujianglin zhoujianglin 3042M Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. -rw-rw-r-- 1 zhoujianglin zhoujianglin 5985M Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann. -rw-rw-r-- 1 zhoujianglin zhoujianglin 9725M Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai. -rw-rw-r-- 1 zhoujianglin zhoujianglin 749M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac. singularity run -B /usr/lib/locale/:/usr/lib/locale/ /lustre/Data/toolsDB/deepvariant.sif ls -al /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. INF,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:2556,usability,tool,toolsDB,2556,ata/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. -rw-rw-r-- 1 zhoujianglin zhoujianglin 3042M Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. -rw-rw-r-- 1 zhoujianglin zhoujianglin 5985M Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann. -rw-rw-r-- 1 zhoujianglin zhoujianglin 9725M Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai. -rw-rw-r-- 1 zhoujianglin zhoujianglin 749M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac. ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. -rw-rw-r-- 1 zhoujianglin zhoujianglin 3042M Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. -rw-rw-r-- 1 zhoujianglin zhoujianglin 5985M Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann. -rw-rw-r-- 1 zhoujianglin zhoujianglin 9725M Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai. -rw-rw-r-- 1 zhoujianglin zhoujianglin 749M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac. singularity run -B /usr/lib/locale/:/usr/lib/locale/ /lustre/Data/toolsDB/deepvariant.sif ls -al /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. INFO: Converting SIF file to temporary sandbox... /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:2668,usability,tool,toolsDB,2668,Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. -rw-rw-r-- 1 zhoujianglin zhoujianglin 5985M Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann. -rw-rw-r-- 1 zhoujianglin zhoujianglin 9725M Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai. -rw-rw-r-- 1 zhoujianglin zhoujianglin 749M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac. ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. -rw-rw-r-- 1 zhoujianglin zhoujianglin 3042M Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. -rw-rw-r-- 1 zhoujianglin zhoujianglin 5985M Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann. -rw-rw-r-- 1 zhoujianglin zhoujianglin 9725M Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai. -rw-rw-r-- 1 zhoujianglin zhoujianglin 749M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac. singularity run -B /usr/lib/locale/:/usr/lib/locale/ /lustre/Data/toolsDB/deepvariant.sif ls -al /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. INFO: Converting SIF file to temporary sandbox... /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:2782,usability,tool,toolsDB,2782,ta/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann. -rw-rw-r-- 1 zhoujianglin zhoujianglin 9725M Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai. -rw-rw-r-- 1 zhoujianglin zhoujianglin 749M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac. ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. -rw-rw-r-- 1 zhoujianglin zhoujianglin 3042M Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. -rw-rw-r-- 1 zhoujianglin zhoujianglin 5985M Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann. -rw-rw-r-- 1 zhoujianglin zhoujianglin 9725M Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai. -rw-rw-r-- 1 zhoujianglin zhoujianglin 749M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac. singularity run -B /usr/lib/locale/:/usr/lib/locale/ /lustre/Data/toolsDB/deepvariant.sif ls -al /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. INFO: Converting SIF file to temporary sandbox... /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs3,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:2895,usability,tool,toolsDB,2895,ata/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann. -rw-rw-r-- 1 zhoujianglin zhoujianglin 9725M Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai. -rw-rw-r-- 1 zhoujianglin zhoujianglin 749M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac. ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. -rw-rw-r-- 1 zhoujianglin zhoujianglin 3042M Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. -rw-rw-r-- 1 zhoujianglin zhoujianglin 5985M Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann. -rw-rw-r-- 1 zhoujianglin zhoujianglin 9725M Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai. -rw-rw-r-- 1 zhoujianglin zhoujianglin 749M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac. singularity run -B /usr/lib/locale/:/usr/lib/locale/ /lustre/Data/toolsDB/deepvariant.sif ls -al /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. INFO: Converting SIF file to temporary sandbox... /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:3011,usability,tool,toolsDB,3011,/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann. -rw-rw-r-- 1 zhoujianglin zhoujianglin 9725M Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai. -rw-rw-r-- 1 zhoujianglin zhoujianglin 749M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac. ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. -rw-rw-r-- 1 zhoujianglin zhoujianglin 3042M Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. -rw-rw-r-- 1 zhoujianglin zhoujianglin 5985M Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann. -rw-rw-r-- 1 zhoujianglin zhoujianglin 9725M Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai. -rw-rw-r-- 1 zhoujianglin zhoujianglin 749M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac. singularity run -B /usr/lib/locale/:/usr/lib/locale/ /lustre/Data/toolsDB/deepvariant.sif ls -al /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. INFO: Converting SIF file to temporary sandbox... /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:3132,usability,tool,toolsDB,3132,sDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai. -rw-rw-r-- 1 zhoujianglin zhoujianglin 749M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac. ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. -rw-rw-r-- 1 zhoujianglin zhoujianglin 3042M Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. -rw-rw-r-- 1 zhoujianglin zhoujianglin 5985M Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann. -rw-rw-r-- 1 zhoujianglin zhoujianglin 9725M Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai. -rw-rw-r-- 1 zhoujianglin zhoujianglin 749M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac. singularity run -B /usr/lib/locale/:/usr/lib/locale/ /lustre/Data/toolsDB/deepvariant.sif ls -al /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. INFO: Converting SIF file to temporary sandbox... /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs3,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:3247,usability,tool,toolsDB,3247,a/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai. -rw-rw-r-- 1 zhoujianglin zhoujianglin 749M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac. ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. -rw-rw-r-- 1 zhoujianglin zhoujianglin 3042M Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. -rw-rw-r-- 1 zhoujianglin zhoujianglin 5985M Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann. -rw-rw-r-- 1 zhoujianglin zhoujianglin 9725M Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai. -rw-rw-r-- 1 zhoujianglin zhoujianglin 749M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac. singularity run -B /usr/lib/locale/:/usr/lib/locale/ /lustre/Data/toolsDB/deepvariant.sif ls -al /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. INFO: Converting SIF file to temporary sandbox... /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:3358,usability,tool,toolsDB,3358,/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac. ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. -rw-rw-r-- 1 zhoujianglin zhoujianglin 3042M Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. -rw-rw-r-- 1 zhoujianglin zhoujianglin 5985M Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann. -rw-rw-r-- 1 zhoujianglin zhoujianglin 9725M Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai. -rw-rw-r-- 1 zhoujianglin zhoujianglin 749M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac. singularity run -B /usr/lib/locale/:/usr/lib/locale/ /lustre/Data/toolsDB/deepvariant.sif ls -al /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. INFO: Converting SIF file to temporary sandbox... /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac': No such file or directory. INFO: Cleaning up image... singularity run -B /usr/lib/locale/:/u,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:3402,usability,tool,toolsDB,3402,a.pac. ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. -rw-rw-r-- 1 zhoujianglin zhoujianglin 3042M Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. -rw-rw-r-- 1 zhoujianglin zhoujianglin 5985M Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann. -rw-rw-r-- 1 zhoujianglin zhoujianglin 9725M Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai. -rw-rw-r-- 1 zhoujianglin zhoujianglin 749M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac. singularity run -B /usr/lib/locale/:/usr/lib/locale/ /lustre/Data/toolsDB/deepvariant.sif ls -al /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. INFO: Converting SIF file to temporary sandbox... /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac': No such file or directory. INFO: Cleaning up image... singularity run -B /usr/lib/locale/:/usr/lib/locale/ /lustre/Data/toolsDB/deepvari,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:3535,usability,tool,toolsDB,3535,:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. -rw-rw-r-- 1 zhoujianglin zhoujianglin 5985M Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann. -rw-rw-r-- 1 zhoujianglin zhoujianglin 9725M Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai. -rw-rw-r-- 1 zhoujianglin zhoujianglin 749M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac. singularity run -B /usr/lib/locale/:/usr/lib/locale/ /lustre/Data/toolsDB/deepvariant.sif ls -al /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. INFO: Converting SIF file to temporary sandbox... /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac': No such file or directory. INFO: Cleaning up image... singularity run -B /usr/lib/locale/:/usr/lib/locale/ /lustre/Data/toolsDB/deepvariant.sif which ls; ls -al /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. INFO: Converting SIF file to temporary sandbox... /us,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:3645,usability,tool,toolsDB,3645,15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann. -rw-rw-r-- 1 zhoujianglin zhoujianglin 9725M Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai. -rw-rw-r-- 1 zhoujianglin zhoujianglin 749M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac. singularity run -B /usr/lib/locale/:/usr/lib/locale/ /lustre/Data/toolsDB/deepvariant.sif ls -al /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. INFO: Converting SIF file to temporary sandbox... /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac': No such file or directory. INFO: Cleaning up image... singularity run -B /usr/lib/locale/:/usr/lib/locale/ /lustre/Data/toolsDB/deepvariant.sif which ls; ls -al /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. INFO: Converting SIF file to temporary sandbox... /usr/bin/ls. INFO: Cleaning up image... -rw-rw-r-- 1 zhoujianglin zhoujianglin 3189750467 Apr 26 14:53 /lustre/Da,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:3760,usability,tool,toolsDB,3760,5:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann. -rw-rw-r-- 1 zhoujianglin zhoujianglin 9725M Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai. -rw-rw-r-- 1 zhoujianglin zhoujianglin 749M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac. singularity run -B /usr/lib/locale/:/usr/lib/locale/ /lustre/Data/toolsDB/deepvariant.sif ls -al /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. INFO: Converting SIF file to temporary sandbox... /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac': No such file or directory. INFO: Cleaning up image... singularity run -B /usr/lib/locale/:/usr/lib/locale/ /lustre/Data/toolsDB/deepvariant.sif which ls; ls -al /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. INFO: Converting SIF file to temporary sandbox... /usr/bin/ls. INFO: Cleaning up image... -rw-rw-r-- 1 zhoujianglin zhoujianglin 3189750467 Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. -rw-rw-r-- 1 zhoujianglin zhoujianglin 6274909010 Apr 26 15:23 /lustre/,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:3874,usability,tool,toolsDB,3874,:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann. -rw-rw-r-- 1 zhoujianglin zhoujianglin 9725M Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai. -rw-rw-r-- 1 zhoujianglin zhoujianglin 749M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac. singularity run -B /usr/lib/locale/:/usr/lib/locale/ /lustre/Data/toolsDB/deepvariant.sif ls -al /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. INFO: Converting SIF file to temporary sandbox... /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac': No such file or directory. INFO: Cleaning up image... singularity run -B /usr/lib/locale/:/usr/lib/locale/ /lustre/Data/toolsDB/deepvariant.sif which ls; ls -al /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. INFO: Converting SIF file to temporary sandbox... /usr/bin/ls. INFO: Cleaning up image... -rw-rw-r-- 1 zhoujianglin zhoujianglin 3189750467 Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. -rw-rw-r-- 1 zhoujianglin zhoujianglin 6274909010 Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123. -rw-rw-r-- 1 zhoujianglin zhoujianglin 106669 Apr 26 15:21 /lus,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:3988,usability,tool,toolsDB,3988,15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64. -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai. -rw-rw-r-- 1 zhoujianglin zhoujianglin 749M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac. singularity run -B /usr/lib/locale/:/usr/lib/locale/ /lustre/Data/toolsDB/deepvariant.sif ls -al /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. INFO: Converting SIF file to temporary sandbox... /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac': No such file or directory. INFO: Cleaning up image... singularity run -B /usr/lib/locale/:/usr/lib/locale/ /lustre/Data/toolsDB/deepvariant.sif which ls; ls -al /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. INFO: Converting SIF file to temporary sandbox... /usr/bin/ls. INFO: Cleaning up image... -rw-rw-r-- 1 zhoujianglin zhoujianglin 3189750467 Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. -rw-rw-r-- 1 zhoujianglin zhoujianglin 6274909010 Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123. -rw-rw-r-- 1 zhoujianglin zhoujianglin 106669 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb. -rw-rw-r-- 1 zhoujianglin zhoujianglin 6924 Apr 26 15:21 /lu,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:4110,usability,tool,toolsDB,4110,7:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai. -rw-rw-r-- 1 zhoujianglin zhoujianglin 749M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac. singularity run -B /usr/lib/locale/:/usr/lib/locale/ /lustre/Data/toolsDB/deepvariant.sif ls -al /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. INFO: Converting SIF file to temporary sandbox... /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac': No such file or directory. INFO: Cleaning up image... singularity run -B /usr/lib/locale/:/usr/lib/locale/ /lustre/Data/toolsDB/deepvariant.sif which ls; ls -al /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. INFO: Converting SIF file to temporary sandbox... /usr/bin/ls. INFO: Cleaning up image... -rw-rw-r-- 1 zhoujianglin zhoujianglin 3189750467 Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. -rw-rw-r-- 1 zhoujianglin zhoujianglin 6274909010 Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123. -rw-rw-r-- 1 zhoujianglin zhoujianglin 106669 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb. -rw-rw-r-- 1 zhoujianglin zhoujianglin 6924 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann. -rw-rw-r-- 1 zhoujianglin zhoujianglin 10196727247 Apr 26 15:42 /lu,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:4224,usability,tool,toolsDB,4224,15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac. singularity run -B /usr/lib/locale/:/usr/lib/locale/ /lustre/Data/toolsDB/deepvariant.sif ls -al /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. INFO: Converting SIF file to temporary sandbox... /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac': No such file or directory. INFO: Cleaning up image... singularity run -B /usr/lib/locale/:/usr/lib/locale/ /lustre/Data/toolsDB/deepvariant.sif which ls; ls -al /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. INFO: Converting SIF file to temporary sandbox... /usr/bin/ls. INFO: Cleaning up image... -rw-rw-r-- 1 zhoujianglin zhoujianglin 3189750467 Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. -rw-rw-r-- 1 zhoujianglin zhoujianglin 6274909010 Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123. -rw-rw-r-- 1 zhoujianglin zhoujianglin 106669 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb. -rw-rw-r-- 1 zhoujianglin zhoujianglin 6924 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann. -rw-rw-r-- 1 zhoujianglin zhoujianglin 10196727247 Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64. -rw-rw-r-- 1 zhoujianglin zhoujianglin 2813 May 19 ,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:4390,usability,tool,toolsDB,4390,re/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. INFO: Converting SIF file to temporary sandbox... /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac': No such file or directory. INFO: Cleaning up image... singularity run -B /usr/lib/locale/:/usr/lib/locale/ /lustre/Data/toolsDB/deepvariant.sif which ls; ls -al /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. INFO: Converting SIF file to temporary sandbox... /usr/bin/ls. INFO: Cleaning up image... -rw-rw-r-- 1 zhoujianglin zhoujianglin 3189750467 Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. -rw-rw-r-- 1 zhoujianglin zhoujianglin 6274909010 Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123. -rw-rw-r-- 1 zhoujianglin zhoujianglin 106669 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb. -rw-rw-r-- 1 zhoujianglin zhoujianglin 6924 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann. -rw-rw-r-- 1 zhoujianglin zhoujianglin 10196727247 Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64. -rw-rw-r-- 1 zhoujianglin zhoujianglin 2813 May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai. -rw-rw-r-- 1 zhoujianglin zhoujianglin 784363628 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:4444,usability,tool,toolsDB,4444,: Converting SIF file to temporary sandbox... /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac': No such file or directory. INFO: Cleaning up image... singularity run -B /usr/lib/locale/:/usr/lib/locale/ /lustre/Data/toolsDB/deepvariant.sif which ls; ls -al /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. INFO: Converting SIF file to temporary sandbox... /usr/bin/ls. INFO: Cleaning up image... -rw-rw-r-- 1 zhoujianglin zhoujianglin 3189750467 Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. -rw-rw-r-- 1 zhoujianglin zhoujianglin 6274909010 Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123. -rw-rw-r-- 1 zhoujianglin zhoujianglin 106669 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb. -rw-rw-r-- 1 zhoujianglin zhoujianglin 6924 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann. -rw-rw-r-- 1 zhoujianglin zhoujianglin 10196727247 Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64. -rw-rw-r-- 1 zhoujianglin zhoujianglin 2813 May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai. -rw-rw-r-- 1 zhoujianglin zhoujianglin 784363628 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac. ```. The real case command `singular,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:4652,usability,tool,toolsDB,4652,tRefs/Human_hs37d5/hs37d5.fa.0123': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac': No such file or directory. INFO: Cleaning up image... singularity run -B /usr/lib/locale/:/usr/lib/locale/ /lustre/Data/toolsDB/deepvariant.sif which ls; ls -al /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. INFO: Converting SIF file to temporary sandbox... /usr/bin/ls. INFO: Cleaning up image... -rw-rw-r-- 1 zhoujianglin zhoujianglin 3189750467 Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. -rw-rw-r-- 1 zhoujianglin zhoujianglin 6274909010 Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123. -rw-rw-r-- 1 zhoujianglin zhoujianglin 106669 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb. -rw-rw-r-- 1 zhoujianglin zhoujianglin 6924 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann. -rw-rw-r-- 1 zhoujianglin zhoujianglin 10196727247 Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64. -rw-rw-r-- 1 zhoujianglin zhoujianglin 2813 May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai. -rw-rw-r-- 1 zhoujianglin zhoujianglin 784363628 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac. ```. The real case command `singularity run -B /usr/lib/locale/:/usr/lib/locale/ /lustre/Data/toolsDB/deepvariant.sif /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa --reads=/path,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:4769,usability,tool,toolsDB,4769,efs/Human_hs37d5/hs37d5.fa.amb': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac': No such file or directory. INFO: Cleaning up image... singularity run -B /usr/lib/locale/:/usr/lib/locale/ /lustre/Data/toolsDB/deepvariant.sif which ls; ls -al /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. INFO: Converting SIF file to temporary sandbox... /usr/bin/ls. INFO: Cleaning up image... -rw-rw-r-- 1 zhoujianglin zhoujianglin 3189750467 Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. -rw-rw-r-- 1 zhoujianglin zhoujianglin 6274909010 Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123. -rw-rw-r-- 1 zhoujianglin zhoujianglin 106669 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb. -rw-rw-r-- 1 zhoujianglin zhoujianglin 6924 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann. -rw-rw-r-- 1 zhoujianglin zhoujianglin 10196727247 Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64. -rw-rw-r-- 1 zhoujianglin zhoujianglin 2813 May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai. -rw-rw-r-- 1 zhoujianglin zhoujianglin 784363628 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac. ```. The real case command `singularity run -B /usr/lib/locale/:/usr/lib/locale/ /lustre/Data/toolsDB/deepvariant.sif /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa --reads=/path/to/2-13A_bwa2Hs37d5_sorted_dedup.bam --output_vcf=DeepVariant_outputs/2-13A.vcf.gz --output_gvcf=DeepVariant_outputs,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:4887,usability,tool,toolsDB,4887,s: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac': No such file or directory. INFO: Cleaning up image... singularity run -B /usr/lib/locale/:/usr/lib/locale/ /lustre/Data/toolsDB/deepvariant.sif which ls; ls -al /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. INFO: Converting SIF file to temporary sandbox... /usr/bin/ls. INFO: Cleaning up image... -rw-rw-r-- 1 zhoujianglin zhoujianglin 3189750467 Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. -rw-rw-r-- 1 zhoujianglin zhoujianglin 6274909010 Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123. -rw-rw-r-- 1 zhoujianglin zhoujianglin 106669 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb. -rw-rw-r-- 1 zhoujianglin zhoujianglin 6924 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann. -rw-rw-r-- 1 zhoujianglin zhoujianglin 10196727247 Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64. -rw-rw-r-- 1 zhoujianglin zhoujianglin 2813 May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai. -rw-rw-r-- 1 zhoujianglin zhoujianglin 784363628 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac. ```. The real case command `singularity run -B /usr/lib/locale/:/usr/lib/locale/ /lustre/Data/toolsDB/deepvariant.sif /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa --reads=/path/to/2-13A_bwa2Hs37d5_sorted_dedup.bam --output_vcf=DeepVariant_outputs/2-13A.vcf.gz --output_gvcf=DeepVariant_outputs/2-13A.g.vcf.gz --num_shards=32` still faild. Did I do anything wrong?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:5002,usability,tool,toolsDB,5002,s: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac': No such file or directory. INFO: Cleaning up image... singularity run -B /usr/lib/locale/:/usr/lib/locale/ /lustre/Data/toolsDB/deepvariant.sif which ls; ls -al /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. INFO: Converting SIF file to temporary sandbox... /usr/bin/ls. INFO: Cleaning up image... -rw-rw-r-- 1 zhoujianglin zhoujianglin 3189750467 Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. -rw-rw-r-- 1 zhoujianglin zhoujianglin 6274909010 Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123. -rw-rw-r-- 1 zhoujianglin zhoujianglin 106669 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb. -rw-rw-r-- 1 zhoujianglin zhoujianglin 6924 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann. -rw-rw-r-- 1 zhoujianglin zhoujianglin 10196727247 Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64. -rw-rw-r-- 1 zhoujianglin zhoujianglin 2813 May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai. -rw-rw-r-- 1 zhoujianglin zhoujianglin 784363628 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac. ```. The real case command `singularity run -B /usr/lib/locale/:/usr/lib/locale/ /lustre/Data/toolsDB/deepvariant.sif /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa --reads=/path/to/2-13A_bwa2Hs37d5_sorted_dedup.bam --output_vcf=DeepVariant_outputs/2-13A.vcf.gz --output_gvcf=DeepVariant_outputs/2-13A.g.vcf.gz --num_shards=32` still faild. Did I do anything wrong?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:5124,usability,tool,toolsDB,5124,s: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac': No such file or directory. INFO: Cleaning up image... singularity run -B /usr/lib/locale/:/usr/lib/locale/ /lustre/Data/toolsDB/deepvariant.sif which ls; ls -al /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. INFO: Converting SIF file to temporary sandbox... /usr/bin/ls. INFO: Cleaning up image... -rw-rw-r-- 1 zhoujianglin zhoujianglin 3189750467 Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. -rw-rw-r-- 1 zhoujianglin zhoujianglin 6274909010 Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123. -rw-rw-r-- 1 zhoujianglin zhoujianglin 106669 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb. -rw-rw-r-- 1 zhoujianglin zhoujianglin 6924 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann. -rw-rw-r-- 1 zhoujianglin zhoujianglin 10196727247 Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64. -rw-rw-r-- 1 zhoujianglin zhoujianglin 2813 May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai. -rw-rw-r-- 1 zhoujianglin zhoujianglin 784363628 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac. ```. The real case command `singularity run -B /usr/lib/locale/:/usr/lib/locale/ /lustre/Data/toolsDB/deepvariant.sif /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa --reads=/path/to/2-13A_bwa2Hs37d5_sorted_dedup.bam --output_vcf=DeepVariant_outputs/2-13A.vcf.gz --output_gvcf=DeepVariant_outputs/2-13A.g.vcf.gz --num_shards=32` still faild. Did I do anything wrong?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:5247,usability,tool,toolsDB,5247,s: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac': No such file or directory. INFO: Cleaning up image... singularity run -B /usr/lib/locale/:/usr/lib/locale/ /lustre/Data/toolsDB/deepvariant.sif which ls; ls -al /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. INFO: Converting SIF file to temporary sandbox... /usr/bin/ls. INFO: Cleaning up image... -rw-rw-r-- 1 zhoujianglin zhoujianglin 3189750467 Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. -rw-rw-r-- 1 zhoujianglin zhoujianglin 6274909010 Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123. -rw-rw-r-- 1 zhoujianglin zhoujianglin 106669 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb. -rw-rw-r-- 1 zhoujianglin zhoujianglin 6924 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann. -rw-rw-r-- 1 zhoujianglin zhoujianglin 10196727247 Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64. -rw-rw-r-- 1 zhoujianglin zhoujianglin 2813 May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai. -rw-rw-r-- 1 zhoujianglin zhoujianglin 784363628 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac. ```. The real case command `singularity run -B /usr/lib/locale/:/usr/lib/locale/ /lustre/Data/toolsDB/deepvariant.sif /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa --reads=/path/to/2-13A_bwa2Hs37d5_sorted_dedup.bam --output_vcf=DeepVariant_outputs/2-13A.vcf.gz --output_gvcf=DeepVariant_outputs/2-13A.g.vcf.gz --num_shards=32` still faild. Did I do anything wrong?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:5367,usability,tool,toolsDB,5367,s: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac': No such file or directory. INFO: Cleaning up image... singularity run -B /usr/lib/locale/:/usr/lib/locale/ /lustre/Data/toolsDB/deepvariant.sif which ls; ls -al /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. INFO: Converting SIF file to temporary sandbox... /usr/bin/ls. INFO: Cleaning up image... -rw-rw-r-- 1 zhoujianglin zhoujianglin 3189750467 Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. -rw-rw-r-- 1 zhoujianglin zhoujianglin 6274909010 Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123. -rw-rw-r-- 1 zhoujianglin zhoujianglin 106669 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb. -rw-rw-r-- 1 zhoujianglin zhoujianglin 6924 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann. -rw-rw-r-- 1 zhoujianglin zhoujianglin 10196727247 Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64. -rw-rw-r-- 1 zhoujianglin zhoujianglin 2813 May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai. -rw-rw-r-- 1 zhoujianglin zhoujianglin 784363628 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac. ```. The real case command `singularity run -B /usr/lib/locale/:/usr/lib/locale/ /lustre/Data/toolsDB/deepvariant.sif /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa --reads=/path/to/2-13A_bwa2Hs37d5_sorted_dedup.bam --output_vcf=DeepVariant_outputs/2-13A.vcf.gz --output_gvcf=DeepVariant_outputs/2-13A.g.vcf.gz --num_shards=32` still faild. Did I do anything wrong?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:5431,usability,command,command,5431,s: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac': No such file or directory. INFO: Cleaning up image... singularity run -B /usr/lib/locale/:/usr/lib/locale/ /lustre/Data/toolsDB/deepvariant.sif which ls; ls -al /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. INFO: Converting SIF file to temporary sandbox... /usr/bin/ls. INFO: Cleaning up image... -rw-rw-r-- 1 zhoujianglin zhoujianglin 3189750467 Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. -rw-rw-r-- 1 zhoujianglin zhoujianglin 6274909010 Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123. -rw-rw-r-- 1 zhoujianglin zhoujianglin 106669 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb. -rw-rw-r-- 1 zhoujianglin zhoujianglin 6924 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann. -rw-rw-r-- 1 zhoujianglin zhoujianglin 10196727247 Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64. -rw-rw-r-- 1 zhoujianglin zhoujianglin 2813 May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai. -rw-rw-r-- 1 zhoujianglin zhoujianglin 784363628 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac. ```. The real case command `singularity run -B /usr/lib/locale/:/usr/lib/locale/ /lustre/Data/toolsDB/deepvariant.sif /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa --reads=/path/to/2-13A_bwa2Hs37d5_sorted_dedup.bam --output_vcf=DeepVariant_outputs/2-13A.vcf.gz --output_gvcf=DeepVariant_outputs/2-13A.g.vcf.gz --num_shards=32` still faild. Did I do anything wrong?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:5506,usability,tool,toolsDB,5506,s: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac': No such file or directory. INFO: Cleaning up image... singularity run -B /usr/lib/locale/:/usr/lib/locale/ /lustre/Data/toolsDB/deepvariant.sif which ls; ls -al /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. INFO: Converting SIF file to temporary sandbox... /usr/bin/ls. INFO: Cleaning up image... -rw-rw-r-- 1 zhoujianglin zhoujianglin 3189750467 Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. -rw-rw-r-- 1 zhoujianglin zhoujianglin 6274909010 Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123. -rw-rw-r-- 1 zhoujianglin zhoujianglin 106669 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb. -rw-rw-r-- 1 zhoujianglin zhoujianglin 6924 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann. -rw-rw-r-- 1 zhoujianglin zhoujianglin 10196727247 Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64. -rw-rw-r-- 1 zhoujianglin zhoujianglin 2813 May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai. -rw-rw-r-- 1 zhoujianglin zhoujianglin 784363628 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac. ```. The real case command `singularity run -B /usr/lib/locale/:/usr/lib/locale/ /lustre/Data/toolsDB/deepvariant.sif /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa --reads=/path/to/2-13A_bwa2Hs37d5_sorted_dedup.bam --output_vcf=DeepVariant_outputs/2-13A.vcf.gz --output_gvcf=DeepVariant_outputs/2-13A.g.vcf.gz --num_shards=32` still faild. Did I do anything wrong?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:5603,usability,tool,toolsDB,5603,s: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai': No such file or directory. /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac': No such file or directory. INFO: Cleaning up image... singularity run -B /usr/lib/locale/:/usr/lib/locale/ /lustre/Data/toolsDB/deepvariant.sif which ls; ls -al /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. INFO: Converting SIF file to temporary sandbox... /usr/bin/ls. INFO: Cleaning up image... -rw-rw-r-- 1 zhoujianglin zhoujianglin 3189750467 Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. -rw-rw-r-- 1 zhoujianglin zhoujianglin 6274909010 Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123. -rw-rw-r-- 1 zhoujianglin zhoujianglin 106669 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb. -rw-rw-r-- 1 zhoujianglin zhoujianglin 6924 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann. -rw-rw-r-- 1 zhoujianglin zhoujianglin 10196727247 Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64. -rw-rw-r-- 1 zhoujianglin zhoujianglin 2813 May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai. -rw-rw-r-- 1 zhoujianglin zhoujianglin 784363628 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac. ```. The real case command `singularity run -B /usr/lib/locale/:/usr/lib/locale/ /lustre/Data/toolsDB/deepvariant.sif /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa --reads=/path/to/2-13A_bwa2Hs37d5_sorted_dedup.bam --output_vcf=DeepVariant_outputs/2-13A.vcf.gz --output_gvcf=DeepVariant_outputs/2-13A.g.vcf.gz --num_shards=32` still faild. Did I do anything wrong?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:198,deployability,contain,container,198,"@Zjianglin So the last thing you are missing is that you are not binding your `lustre` folder. Basically the `/lustre/Data/toolsDB/HostRefs/Human_hs37d5/` folder is not seen within your Singularity container at runtime, so you will need to bind that folder like this:. ```. singularity run -B /lustre/Data/toolsDB/HostRefs/Human_hs37d5/:/lustre/Data/toolsDB/HostRefs/Human_hs37d5/ /lustre/Data/toolsDB/deepvariant.sif ls -al /lustre/Data/toolsDB/HostRefs/Human_hs37d5/*. ```. For more information on binding paths, below is a link that expands on how it works:. https://docs.sylabs.io/guides/3.0/user-guide/bind_paths_and_mounts.html",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:65,interoperability,bind,binding,65,"@Zjianglin So the last thing you are missing is that you are not binding your `lustre` folder. Basically the `/lustre/Data/toolsDB/HostRefs/Human_hs37d5/` folder is not seen within your Singularity container at runtime, so you will need to bind that folder like this:. ```. singularity run -B /lustre/Data/toolsDB/HostRefs/Human_hs37d5/:/lustre/Data/toolsDB/HostRefs/Human_hs37d5/ /lustre/Data/toolsDB/deepvariant.sif ls -al /lustre/Data/toolsDB/HostRefs/Human_hs37d5/*. ```. For more information on binding paths, below is a link that expands on how it works:. https://docs.sylabs.io/guides/3.0/user-guide/bind_paths_and_mounts.html",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:240,interoperability,bind,bind,240,"@Zjianglin So the last thing you are missing is that you are not binding your `lustre` folder. Basically the `/lustre/Data/toolsDB/HostRefs/Human_hs37d5/` folder is not seen within your Singularity container at runtime, so you will need to bind that folder like this:. ```. singularity run -B /lustre/Data/toolsDB/HostRefs/Human_hs37d5/:/lustre/Data/toolsDB/HostRefs/Human_hs37d5/ /lustre/Data/toolsDB/deepvariant.sif ls -al /lustre/Data/toolsDB/HostRefs/Human_hs37d5/*. ```. For more information on binding paths, below is a link that expands on how it works:. https://docs.sylabs.io/guides/3.0/user-guide/bind_paths_and_mounts.html",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:500,interoperability,bind,binding,500,"@Zjianglin So the last thing you are missing is that you are not binding your `lustre` folder. Basically the `/lustre/Data/toolsDB/HostRefs/Human_hs37d5/` folder is not seen within your Singularity container at runtime, so you will need to bind that folder like this:. ```. singularity run -B /lustre/Data/toolsDB/HostRefs/Human_hs37d5/:/lustre/Data/toolsDB/HostRefs/Human_hs37d5/ /lustre/Data/toolsDB/deepvariant.sif ls -al /lustre/Data/toolsDB/HostRefs/Human_hs37d5/*. ```. For more information on binding paths, below is a link that expands on how it works:. https://docs.sylabs.io/guides/3.0/user-guide/bind_paths_and_mounts.html",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:65,modifiability,bind,binding,65,"@Zjianglin So the last thing you are missing is that you are not binding your `lustre` folder. Basically the `/lustre/Data/toolsDB/HostRefs/Human_hs37d5/` folder is not seen within your Singularity container at runtime, so you will need to bind that folder like this:. ```. singularity run -B /lustre/Data/toolsDB/HostRefs/Human_hs37d5/:/lustre/Data/toolsDB/HostRefs/Human_hs37d5/ /lustre/Data/toolsDB/deepvariant.sif ls -al /lustre/Data/toolsDB/HostRefs/Human_hs37d5/*. ```. For more information on binding paths, below is a link that expands on how it works:. https://docs.sylabs.io/guides/3.0/user-guide/bind_paths_and_mounts.html",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:240,modifiability,bind,bind,240,"@Zjianglin So the last thing you are missing is that you are not binding your `lustre` folder. Basically the `/lustre/Data/toolsDB/HostRefs/Human_hs37d5/` folder is not seen within your Singularity container at runtime, so you will need to bind that folder like this:. ```. singularity run -B /lustre/Data/toolsDB/HostRefs/Human_hs37d5/:/lustre/Data/toolsDB/HostRefs/Human_hs37d5/ /lustre/Data/toolsDB/deepvariant.sif ls -al /lustre/Data/toolsDB/HostRefs/Human_hs37d5/*. ```. For more information on binding paths, below is a link that expands on how it works:. https://docs.sylabs.io/guides/3.0/user-guide/bind_paths_and_mounts.html",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:500,modifiability,bind,binding,500,"@Zjianglin So the last thing you are missing is that you are not binding your `lustre` folder. Basically the `/lustre/Data/toolsDB/HostRefs/Human_hs37d5/` folder is not seen within your Singularity container at runtime, so you will need to bind that folder like this:. ```. singularity run -B /lustre/Data/toolsDB/HostRefs/Human_hs37d5/:/lustre/Data/toolsDB/HostRefs/Human_hs37d5/ /lustre/Data/toolsDB/deepvariant.sif ls -al /lustre/Data/toolsDB/HostRefs/Human_hs37d5/*. ```. For more information on binding paths, below is a link that expands on how it works:. https://docs.sylabs.io/guides/3.0/user-guide/bind_paths_and_mounts.html",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:123,usability,tool,toolsDB,123,"@Zjianglin So the last thing you are missing is that you are not binding your `lustre` folder. Basically the `/lustre/Data/toolsDB/HostRefs/Human_hs37d5/` folder is not seen within your Singularity container at runtime, so you will need to bind that folder like this:. ```. singularity run -B /lustre/Data/toolsDB/HostRefs/Human_hs37d5/:/lustre/Data/toolsDB/HostRefs/Human_hs37d5/ /lustre/Data/toolsDB/deepvariant.sif ls -al /lustre/Data/toolsDB/HostRefs/Human_hs37d5/*. ```. For more information on binding paths, below is a link that expands on how it works:. https://docs.sylabs.io/guides/3.0/user-guide/bind_paths_and_mounts.html",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:306,usability,tool,toolsDB,306,"@Zjianglin So the last thing you are missing is that you are not binding your `lustre` folder. Basically the `/lustre/Data/toolsDB/HostRefs/Human_hs37d5/` folder is not seen within your Singularity container at runtime, so you will need to bind that folder like this:. ```. singularity run -B /lustre/Data/toolsDB/HostRefs/Human_hs37d5/:/lustre/Data/toolsDB/HostRefs/Human_hs37d5/ /lustre/Data/toolsDB/deepvariant.sif ls -al /lustre/Data/toolsDB/HostRefs/Human_hs37d5/*. ```. For more information on binding paths, below is a link that expands on how it works:. https://docs.sylabs.io/guides/3.0/user-guide/bind_paths_and_mounts.html",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:350,usability,tool,toolsDB,350,"@Zjianglin So the last thing you are missing is that you are not binding your `lustre` folder. Basically the `/lustre/Data/toolsDB/HostRefs/Human_hs37d5/` folder is not seen within your Singularity container at runtime, so you will need to bind that folder like this:. ```. singularity run -B /lustre/Data/toolsDB/HostRefs/Human_hs37d5/:/lustre/Data/toolsDB/HostRefs/Human_hs37d5/ /lustre/Data/toolsDB/deepvariant.sif ls -al /lustre/Data/toolsDB/HostRefs/Human_hs37d5/*. ```. For more information on binding paths, below is a link that expands on how it works:. https://docs.sylabs.io/guides/3.0/user-guide/bind_paths_and_mounts.html",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:394,usability,tool,toolsDB,394,"@Zjianglin So the last thing you are missing is that you are not binding your `lustre` folder. Basically the `/lustre/Data/toolsDB/HostRefs/Human_hs37d5/` folder is not seen within your Singularity container at runtime, so you will need to bind that folder like this:. ```. singularity run -B /lustre/Data/toolsDB/HostRefs/Human_hs37d5/:/lustre/Data/toolsDB/HostRefs/Human_hs37d5/ /lustre/Data/toolsDB/deepvariant.sif ls -al /lustre/Data/toolsDB/HostRefs/Human_hs37d5/*. ```. For more information on binding paths, below is a link that expands on how it works:. https://docs.sylabs.io/guides/3.0/user-guide/bind_paths_and_mounts.html",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:438,usability,tool,toolsDB,438,"@Zjianglin So the last thing you are missing is that you are not binding your `lustre` folder. Basically the `/lustre/Data/toolsDB/HostRefs/Human_hs37d5/` folder is not seen within your Singularity container at runtime, so you will need to bind that folder like this:. ```. singularity run -B /lustre/Data/toolsDB/HostRefs/Human_hs37d5/:/lustre/Data/toolsDB/HostRefs/Human_hs37d5/ /lustre/Data/toolsDB/deepvariant.sif ls -al /lustre/Data/toolsDB/HostRefs/Human_hs37d5/*. ```. For more information on binding paths, below is a link that expands on how it works:. https://docs.sylabs.io/guides/3.0/user-guide/bind_paths_and_mounts.html",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:585,usability,guid,guides,585,"@Zjianglin So the last thing you are missing is that you are not binding your `lustre` folder. Basically the `/lustre/Data/toolsDB/HostRefs/Human_hs37d5/` folder is not seen within your Singularity container at runtime, so you will need to bind that folder like this:. ```. singularity run -B /lustre/Data/toolsDB/HostRefs/Human_hs37d5/:/lustre/Data/toolsDB/HostRefs/Human_hs37d5/ /lustre/Data/toolsDB/deepvariant.sif ls -al /lustre/Data/toolsDB/HostRefs/Human_hs37d5/*. ```. For more information on binding paths, below is a link that expands on how it works:. https://docs.sylabs.io/guides/3.0/user-guide/bind_paths_and_mounts.html",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:596,usability,user,user-guide,596,"@Zjianglin So the last thing you are missing is that you are not binding your `lustre` folder. Basically the `/lustre/Data/toolsDB/HostRefs/Human_hs37d5/` folder is not seen within your Singularity container at runtime, so you will need to bind that folder like this:. ```. singularity run -B /lustre/Data/toolsDB/HostRefs/Human_hs37d5/:/lustre/Data/toolsDB/HostRefs/Human_hs37d5/ /lustre/Data/toolsDB/deepvariant.sif ls -al /lustre/Data/toolsDB/HostRefs/Human_hs37d5/*. ```. For more information on binding paths, below is a link that expands on how it works:. https://docs.sylabs.io/guides/3.0/user-guide/bind_paths_and_mounts.html",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:33,interoperability,bind,binding,33,"Thanks @pgrosu @pichuan , After `binding` my `lustre` folder using `-B` option, now I can normally run the `deepvariant` using `singularity run` for my samples. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:33,modifiability,bind,binding,33,"Thanks @pgrosu @pichuan , After `binding` my `lustre` folder using `-B` option, now I can normally run the `deepvariant` using `singularity run` for my samples. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:67,availability,error,error,67,"I recently experienced a similar issue in which I kept getting the error `ValueError: NOT_FOUND: could not load fasta and/or fai for fasta` when I'd try to run DeepVariant via Docker. The command I was using looked like:. ```. docker run \. -v ""/data/analysis_data/ngs/r64558e_20231017_193725/bam"":""/input"" \. -v ""/data/analysis_data/ngs/r64558e_20231017_193725/result_DeepVariant"":""/output"" \. google/deepvariant:""1.6.0"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=/data/bioinfo.shared/index/hg38_gatk/hg38.fa \. --reads=/input/m64558e_231018_115114.hifi_reads.sorted.add-rg.rm-tags.bam \. --output_vcf=/output/FD800429.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=1. ```. Not having used Docker before, I wasn't aware that I needed to mount all directories I needed to access so that Docker could see them. This issue and [this article](https://towardsdatascience.com/how-to-mount-a-directory-inside-a-docker-container-4cee379c298b) helped me figure out where I went wrong (i.e. using the absolute path in the --ref parameter without having mounted it). The adjusted command (that worked) looked like:. ```. docker run \. -v ""/data/bioinfo.shared/index/hg38_gatk/"":""/index"" \. -v ""/data/analysis_data/ngs/r64558e_20231017_193725/bam"":""/input"" \. -v ""/data/analysis_data/ngs/r64558e_20231017_193725/result_DeepVariant"":""/output"" \. google/deepvariant:""1.6.0"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=/index/hg38.fa \. --reads=/input/m64558e_231018_115114.hifi_reads.sorted.add-rg.rm-tags.bam \. --output_vcf=/output/FD800429.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=1. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:980,deployability,contain,container-,980,"I recently experienced a similar issue in which I kept getting the error `ValueError: NOT_FOUND: could not load fasta and/or fai for fasta` when I'd try to run DeepVariant via Docker. The command I was using looked like:. ```. docker run \. -v ""/data/analysis_data/ngs/r64558e_20231017_193725/bam"":""/input"" \. -v ""/data/analysis_data/ngs/r64558e_20231017_193725/result_DeepVariant"":""/output"" \. google/deepvariant:""1.6.0"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=/data/bioinfo.shared/index/hg38_gatk/hg38.fa \. --reads=/input/m64558e_231018_115114.hifi_reads.sorted.add-rg.rm-tags.bam \. --output_vcf=/output/FD800429.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=1. ```. Not having used Docker before, I wasn't aware that I needed to mount all directories I needed to access so that Docker could see them. This issue and [this article](https://towardsdatascience.com/how-to-mount-a-directory-inside-a-docker-container-4cee379c298b) helped me figure out where I went wrong (i.e. using the absolute path in the --ref parameter without having mounted it). The adjusted command (that worked) looked like:. ```. docker run \. -v ""/data/bioinfo.shared/index/hg38_gatk/"":""/index"" \. -v ""/data/analysis_data/ngs/r64558e_20231017_193725/bam"":""/input"" \. -v ""/data/analysis_data/ngs/r64558e_20231017_193725/result_DeepVariant"":""/output"" \. google/deepvariant:""1.6.0"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=/index/hg38.fa \. --reads=/input/m64558e_231018_115114.hifi_reads.sorted.add-rg.rm-tags.bam \. --output_vcf=/output/FD800429.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=1. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:107,energy efficiency,load,load,107,"I recently experienced a similar issue in which I kept getting the error `ValueError: NOT_FOUND: could not load fasta and/or fai for fasta` when I'd try to run DeepVariant via Docker. The command I was using looked like:. ```. docker run \. -v ""/data/analysis_data/ngs/r64558e_20231017_193725/bam"":""/input"" \. -v ""/data/analysis_data/ngs/r64558e_20231017_193725/result_DeepVariant"":""/output"" \. google/deepvariant:""1.6.0"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=/data/bioinfo.shared/index/hg38_gatk/hg38.fa \. --reads=/input/m64558e_231018_115114.hifi_reads.sorted.add-rg.rm-tags.bam \. --output_vcf=/output/FD800429.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=1. ```. Not having used Docker before, I wasn't aware that I needed to mount all directories I needed to access so that Docker could see them. This issue and [this article](https://towardsdatascience.com/how-to-mount-a-directory-inside-a-docker-container-4cee379c298b) helped me figure out where I went wrong (i.e. using the absolute path in the --ref parameter without having mounted it). The adjusted command (that worked) looked like:. ```. docker run \. -v ""/data/bioinfo.shared/index/hg38_gatk/"":""/index"" \. -v ""/data/analysis_data/ngs/r64558e_20231017_193725/bam"":""/input"" \. -v ""/data/analysis_data/ngs/r64558e_20231017_193725/result_DeepVariant"":""/output"" \. google/deepvariant:""1.6.0"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=/index/hg38.fa \. --reads=/input/m64558e_231018_115114.hifi_reads.sorted.add-rg.rm-tags.bam \. --output_vcf=/output/FD800429.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=1. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:508,interoperability,share,shared,508,"I recently experienced a similar issue in which I kept getting the error `ValueError: NOT_FOUND: could not load fasta and/or fai for fasta` when I'd try to run DeepVariant via Docker. The command I was using looked like:. ```. docker run \. -v ""/data/analysis_data/ngs/r64558e_20231017_193725/bam"":""/input"" \. -v ""/data/analysis_data/ngs/r64558e_20231017_193725/result_DeepVariant"":""/output"" \. google/deepvariant:""1.6.0"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=/data/bioinfo.shared/index/hg38_gatk/hg38.fa \. --reads=/input/m64558e_231018_115114.hifi_reads.sorted.add-rg.rm-tags.bam \. --output_vcf=/output/FD800429.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=1. ```. Not having used Docker before, I wasn't aware that I needed to mount all directories I needed to access so that Docker could see them. This issue and [this article](https://towardsdatascience.com/how-to-mount-a-directory-inside-a-docker-container-4cee379c298b) helped me figure out where I went wrong (i.e. using the absolute path in the --ref parameter without having mounted it). The adjusted command (that worked) looked like:. ```. docker run \. -v ""/data/bioinfo.shared/index/hg38_gatk/"":""/index"" \. -v ""/data/analysis_data/ngs/r64558e_20231017_193725/bam"":""/input"" \. -v ""/data/analysis_data/ngs/r64558e_20231017_193725/result_DeepVariant"":""/output"" \. google/deepvariant:""1.6.0"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=/index/hg38.fa \. --reads=/input/m64558e_231018_115114.hifi_reads.sorted.add-rg.rm-tags.bam \. --output_vcf=/output/FD800429.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=1. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:1211,interoperability,share,shared,1211,"I recently experienced a similar issue in which I kept getting the error `ValueError: NOT_FOUND: could not load fasta and/or fai for fasta` when I'd try to run DeepVariant via Docker. The command I was using looked like:. ```. docker run \. -v ""/data/analysis_data/ngs/r64558e_20231017_193725/bam"":""/input"" \. -v ""/data/analysis_data/ngs/r64558e_20231017_193725/result_DeepVariant"":""/output"" \. google/deepvariant:""1.6.0"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=/data/bioinfo.shared/index/hg38_gatk/hg38.fa \. --reads=/input/m64558e_231018_115114.hifi_reads.sorted.add-rg.rm-tags.bam \. --output_vcf=/output/FD800429.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=1. ```. Not having used Docker before, I wasn't aware that I needed to mount all directories I needed to access so that Docker could see them. This issue and [this article](https://towardsdatascience.com/how-to-mount-a-directory-inside-a-docker-container-4cee379c298b) helped me figure out where I went wrong (i.e. using the absolute path in the --ref parameter without having mounted it). The adjusted command (that worked) looked like:. ```. docker run \. -v ""/data/bioinfo.shared/index/hg38_gatk/"":""/index"" \. -v ""/data/analysis_data/ngs/r64558e_20231017_193725/bam"":""/input"" \. -v ""/data/analysis_data/ngs/r64558e_20231017_193725/result_DeepVariant"":""/output"" \. google/deepvariant:""1.6.0"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=/index/hg38.fa \. --reads=/input/m64558e_231018_115114.hifi_reads.sorted.add-rg.rm-tags.bam \. --output_vcf=/output/FD800429.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=1. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:478,modifiability,PAC,PACBIO,478,"I recently experienced a similar issue in which I kept getting the error `ValueError: NOT_FOUND: could not load fasta and/or fai for fasta` when I'd try to run DeepVariant via Docker. The command I was using looked like:. ```. docker run \. -v ""/data/analysis_data/ngs/r64558e_20231017_193725/bam"":""/input"" \. -v ""/data/analysis_data/ngs/r64558e_20231017_193725/result_DeepVariant"":""/output"" \. google/deepvariant:""1.6.0"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=/data/bioinfo.shared/index/hg38_gatk/hg38.fa \. --reads=/input/m64558e_231018_115114.hifi_reads.sorted.add-rg.rm-tags.bam \. --output_vcf=/output/FD800429.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=1. ```. Not having used Docker before, I wasn't aware that I needed to mount all directories I needed to access so that Docker could see them. This issue and [this article](https://towardsdatascience.com/how-to-mount-a-directory-inside-a-docker-container-4cee379c298b) helped me figure out where I went wrong (i.e. using the absolute path in the --ref parameter without having mounted it). The adjusted command (that worked) looked like:. ```. docker run \. -v ""/data/bioinfo.shared/index/hg38_gatk/"":""/index"" \. -v ""/data/analysis_data/ngs/r64558e_20231017_193725/bam"":""/input"" \. -v ""/data/analysis_data/ngs/r64558e_20231017_193725/result_DeepVariant"":""/output"" \. google/deepvariant:""1.6.0"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=/index/hg38.fa \. --reads=/input/m64558e_231018_115114.hifi_reads.sorted.add-rg.rm-tags.bam \. --output_vcf=/output/FD800429.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=1. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:1087,modifiability,paramet,parameter,1087,"I recently experienced a similar issue in which I kept getting the error `ValueError: NOT_FOUND: could not load fasta and/or fai for fasta` when I'd try to run DeepVariant via Docker. The command I was using looked like:. ```. docker run \. -v ""/data/analysis_data/ngs/r64558e_20231017_193725/bam"":""/input"" \. -v ""/data/analysis_data/ngs/r64558e_20231017_193725/result_DeepVariant"":""/output"" \. google/deepvariant:""1.6.0"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=/data/bioinfo.shared/index/hg38_gatk/hg38.fa \. --reads=/input/m64558e_231018_115114.hifi_reads.sorted.add-rg.rm-tags.bam \. --output_vcf=/output/FD800429.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=1. ```. Not having used Docker before, I wasn't aware that I needed to mount all directories I needed to access so that Docker could see them. This issue and [this article](https://towardsdatascience.com/how-to-mount-a-directory-inside-a-docker-container-4cee379c298b) helped me figure out where I went wrong (i.e. using the absolute path in the --ref parameter without having mounted it). The adjusted command (that worked) looked like:. ```. docker run \. -v ""/data/bioinfo.shared/index/hg38_gatk/"":""/index"" \. -v ""/data/analysis_data/ngs/r64558e_20231017_193725/bam"":""/input"" \. -v ""/data/analysis_data/ngs/r64558e_20231017_193725/result_DeepVariant"":""/output"" \. google/deepvariant:""1.6.0"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=/index/hg38.fa \. --reads=/input/m64558e_231018_115114.hifi_reads.sorted.add-rg.rm-tags.bam \. --output_vcf=/output/FD800429.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=1. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:1485,modifiability,PAC,PACBIO,1485,"I recently experienced a similar issue in which I kept getting the error `ValueError: NOT_FOUND: could not load fasta and/or fai for fasta` when I'd try to run DeepVariant via Docker. The command I was using looked like:. ```. docker run \. -v ""/data/analysis_data/ngs/r64558e_20231017_193725/bam"":""/input"" \. -v ""/data/analysis_data/ngs/r64558e_20231017_193725/result_DeepVariant"":""/output"" \. google/deepvariant:""1.6.0"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=/data/bioinfo.shared/index/hg38_gatk/hg38.fa \. --reads=/input/m64558e_231018_115114.hifi_reads.sorted.add-rg.rm-tags.bam \. --output_vcf=/output/FD800429.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=1. ```. Not having used Docker before, I wasn't aware that I needed to mount all directories I needed to access so that Docker could see them. This issue and [this article](https://towardsdatascience.com/how-to-mount-a-directory-inside-a-docker-container-4cee379c298b) helped me figure out where I went wrong (i.e. using the absolute path in the --ref parameter without having mounted it). The adjusted command (that worked) looked like:. ```. docker run \. -v ""/data/bioinfo.shared/index/hg38_gatk/"":""/index"" \. -v ""/data/analysis_data/ngs/r64558e_20231017_193725/bam"":""/input"" \. -v ""/data/analysis_data/ngs/r64558e_20231017_193725/result_DeepVariant"":""/output"" \. google/deepvariant:""1.6.0"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=/index/hg38.fa \. --reads=/input/m64558e_231018_115114.hifi_reads.sorted.add-rg.rm-tags.bam \. --output_vcf=/output/FD800429.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=1. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:67,performance,error,error,67,"I recently experienced a similar issue in which I kept getting the error `ValueError: NOT_FOUND: could not load fasta and/or fai for fasta` when I'd try to run DeepVariant via Docker. The command I was using looked like:. ```. docker run \. -v ""/data/analysis_data/ngs/r64558e_20231017_193725/bam"":""/input"" \. -v ""/data/analysis_data/ngs/r64558e_20231017_193725/result_DeepVariant"":""/output"" \. google/deepvariant:""1.6.0"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=/data/bioinfo.shared/index/hg38_gatk/hg38.fa \. --reads=/input/m64558e_231018_115114.hifi_reads.sorted.add-rg.rm-tags.bam \. --output_vcf=/output/FD800429.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=1. ```. Not having used Docker before, I wasn't aware that I needed to mount all directories I needed to access so that Docker could see them. This issue and [this article](https://towardsdatascience.com/how-to-mount-a-directory-inside-a-docker-container-4cee379c298b) helped me figure out where I went wrong (i.e. using the absolute path in the --ref parameter without having mounted it). The adjusted command (that worked) looked like:. ```. docker run \. -v ""/data/bioinfo.shared/index/hg38_gatk/"":""/index"" \. -v ""/data/analysis_data/ngs/r64558e_20231017_193725/bam"":""/input"" \. -v ""/data/analysis_data/ngs/r64558e_20231017_193725/result_DeepVariant"":""/output"" \. google/deepvariant:""1.6.0"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=/index/hg38.fa \. --reads=/input/m64558e_231018_115114.hifi_reads.sorted.add-rg.rm-tags.bam \. --output_vcf=/output/FD800429.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=1. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:107,performance,load,load,107,"I recently experienced a similar issue in which I kept getting the error `ValueError: NOT_FOUND: could not load fasta and/or fai for fasta` when I'd try to run DeepVariant via Docker. The command I was using looked like:. ```. docker run \. -v ""/data/analysis_data/ngs/r64558e_20231017_193725/bam"":""/input"" \. -v ""/data/analysis_data/ngs/r64558e_20231017_193725/result_DeepVariant"":""/output"" \. google/deepvariant:""1.6.0"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=/data/bioinfo.shared/index/hg38_gatk/hg38.fa \. --reads=/input/m64558e_231018_115114.hifi_reads.sorted.add-rg.rm-tags.bam \. --output_vcf=/output/FD800429.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=1. ```. Not having used Docker before, I wasn't aware that I needed to mount all directories I needed to access so that Docker could see them. This issue and [this article](https://towardsdatascience.com/how-to-mount-a-directory-inside-a-docker-container-4cee379c298b) helped me figure out where I went wrong (i.e. using the absolute path in the --ref parameter without having mounted it). The adjusted command (that worked) looked like:. ```. docker run \. -v ""/data/bioinfo.shared/index/hg38_gatk/"":""/index"" \. -v ""/data/analysis_data/ngs/r64558e_20231017_193725/bam"":""/input"" \. -v ""/data/analysis_data/ngs/r64558e_20231017_193725/result_DeepVariant"":""/output"" \. google/deepvariant:""1.6.0"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=/index/hg38.fa \. --reads=/input/m64558e_231018_115114.hifi_reads.sorted.add-rg.rm-tags.bam \. --output_vcf=/output/FD800429.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=1. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:67,safety,error,error,67,"I recently experienced a similar issue in which I kept getting the error `ValueError: NOT_FOUND: could not load fasta and/or fai for fasta` when I'd try to run DeepVariant via Docker. The command I was using looked like:. ```. docker run \. -v ""/data/analysis_data/ngs/r64558e_20231017_193725/bam"":""/input"" \. -v ""/data/analysis_data/ngs/r64558e_20231017_193725/result_DeepVariant"":""/output"" \. google/deepvariant:""1.6.0"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=/data/bioinfo.shared/index/hg38_gatk/hg38.fa \. --reads=/input/m64558e_231018_115114.hifi_reads.sorted.add-rg.rm-tags.bam \. --output_vcf=/output/FD800429.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=1. ```. Not having used Docker before, I wasn't aware that I needed to mount all directories I needed to access so that Docker could see them. This issue and [this article](https://towardsdatascience.com/how-to-mount-a-directory-inside-a-docker-container-4cee379c298b) helped me figure out where I went wrong (i.e. using the absolute path in the --ref parameter without having mounted it). The adjusted command (that worked) looked like:. ```. docker run \. -v ""/data/bioinfo.shared/index/hg38_gatk/"":""/index"" \. -v ""/data/analysis_data/ngs/r64558e_20231017_193725/bam"":""/input"" \. -v ""/data/analysis_data/ngs/r64558e_20231017_193725/result_DeepVariant"":""/output"" \. google/deepvariant:""1.6.0"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=/index/hg38.fa \. --reads=/input/m64558e_231018_115114.hifi_reads.sorted.add-rg.rm-tags.bam \. --output_vcf=/output/FD800429.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=1. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:300,safety,input,input,300,"I recently experienced a similar issue in which I kept getting the error `ValueError: NOT_FOUND: could not load fasta and/or fai for fasta` when I'd try to run DeepVariant via Docker. The command I was using looked like:. ```. docker run \. -v ""/data/analysis_data/ngs/r64558e_20231017_193725/bam"":""/input"" \. -v ""/data/analysis_data/ngs/r64558e_20231017_193725/result_DeepVariant"":""/output"" \. google/deepvariant:""1.6.0"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=/data/bioinfo.shared/index/hg38_gatk/hg38.fa \. --reads=/input/m64558e_231018_115114.hifi_reads.sorted.add-rg.rm-tags.bam \. --output_vcf=/output/FD800429.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=1. ```. Not having used Docker before, I wasn't aware that I needed to mount all directories I needed to access so that Docker could see them. This issue and [this article](https://towardsdatascience.com/how-to-mount-a-directory-inside-a-docker-container-4cee379c298b) helped me figure out where I went wrong (i.e. using the absolute path in the --ref parameter without having mounted it). The adjusted command (that worked) looked like:. ```. docker run \. -v ""/data/bioinfo.shared/index/hg38_gatk/"":""/index"" \. -v ""/data/analysis_data/ngs/r64558e_20231017_193725/bam"":""/input"" \. -v ""/data/analysis_data/ngs/r64558e_20231017_193725/result_DeepVariant"":""/output"" \. google/deepvariant:""1.6.0"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=/index/hg38.fa \. --reads=/input/m64558e_231018_115114.hifi_reads.sorted.add-rg.rm-tags.bam \. --output_vcf=/output/FD800429.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=1. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:551,safety,input,input,551,"I recently experienced a similar issue in which I kept getting the error `ValueError: NOT_FOUND: could not load fasta and/or fai for fasta` when I'd try to run DeepVariant via Docker. The command I was using looked like:. ```. docker run \. -v ""/data/analysis_data/ngs/r64558e_20231017_193725/bam"":""/input"" \. -v ""/data/analysis_data/ngs/r64558e_20231017_193725/result_DeepVariant"":""/output"" \. google/deepvariant:""1.6.0"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=/data/bioinfo.shared/index/hg38_gatk/hg38.fa \. --reads=/input/m64558e_231018_115114.hifi_reads.sorted.add-rg.rm-tags.bam \. --output_vcf=/output/FD800429.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=1. ```. Not having used Docker before, I wasn't aware that I needed to mount all directories I needed to access so that Docker could see them. This issue and [this article](https://towardsdatascience.com/how-to-mount-a-directory-inside-a-docker-container-4cee379c298b) helped me figure out where I went wrong (i.e. using the absolute path in the --ref parameter without having mounted it). The adjusted command (that worked) looked like:. ```. docker run \. -v ""/data/bioinfo.shared/index/hg38_gatk/"":""/index"" \. -v ""/data/analysis_data/ngs/r64558e_20231017_193725/bam"":""/input"" \. -v ""/data/analysis_data/ngs/r64558e_20231017_193725/result_DeepVariant"":""/output"" \. google/deepvariant:""1.6.0"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=/index/hg38.fa \. --reads=/input/m64558e_231018_115114.hifi_reads.sorted.add-rg.rm-tags.bam \. --output_vcf=/output/FD800429.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=1. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:1307,safety,input,input,1307,"I recently experienced a similar issue in which I kept getting the error `ValueError: NOT_FOUND: could not load fasta and/or fai for fasta` when I'd try to run DeepVariant via Docker. The command I was using looked like:. ```. docker run \. -v ""/data/analysis_data/ngs/r64558e_20231017_193725/bam"":""/input"" \. -v ""/data/analysis_data/ngs/r64558e_20231017_193725/result_DeepVariant"":""/output"" \. google/deepvariant:""1.6.0"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=/data/bioinfo.shared/index/hg38_gatk/hg38.fa \. --reads=/input/m64558e_231018_115114.hifi_reads.sorted.add-rg.rm-tags.bam \. --output_vcf=/output/FD800429.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=1. ```. Not having used Docker before, I wasn't aware that I needed to mount all directories I needed to access so that Docker could see them. This issue and [this article](https://towardsdatascience.com/how-to-mount-a-directory-inside-a-docker-container-4cee379c298b) helped me figure out where I went wrong (i.e. using the absolute path in the --ref parameter without having mounted it). The adjusted command (that worked) looked like:. ```. docker run \. -v ""/data/bioinfo.shared/index/hg38_gatk/"":""/index"" \. -v ""/data/analysis_data/ngs/r64558e_20231017_193725/bam"":""/input"" \. -v ""/data/analysis_data/ngs/r64558e_20231017_193725/result_DeepVariant"":""/output"" \. google/deepvariant:""1.6.0"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=/index/hg38.fa \. --reads=/input/m64558e_231018_115114.hifi_reads.sorted.add-rg.rm-tags.bam \. --output_vcf=/output/FD800429.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=1. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:1528,safety,input,input,1528,"I recently experienced a similar issue in which I kept getting the error `ValueError: NOT_FOUND: could not load fasta and/or fai for fasta` when I'd try to run DeepVariant via Docker. The command I was using looked like:. ```. docker run \. -v ""/data/analysis_data/ngs/r64558e_20231017_193725/bam"":""/input"" \. -v ""/data/analysis_data/ngs/r64558e_20231017_193725/result_DeepVariant"":""/output"" \. google/deepvariant:""1.6.0"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=/data/bioinfo.shared/index/hg38_gatk/hg38.fa \. --reads=/input/m64558e_231018_115114.hifi_reads.sorted.add-rg.rm-tags.bam \. --output_vcf=/output/FD800429.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=1. ```. Not having used Docker before, I wasn't aware that I needed to mount all directories I needed to access so that Docker could see them. This issue and [this article](https://towardsdatascience.com/how-to-mount-a-directory-inside-a-docker-container-4cee379c298b) helped me figure out where I went wrong (i.e. using the absolute path in the --ref parameter without having mounted it). The adjusted command (that worked) looked like:. ```. docker run \. -v ""/data/bioinfo.shared/index/hg38_gatk/"":""/index"" \. -v ""/data/analysis_data/ngs/r64558e_20231017_193725/bam"":""/input"" \. -v ""/data/analysis_data/ngs/r64558e_20231017_193725/result_DeepVariant"":""/output"" \. google/deepvariant:""1.6.0"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=/index/hg38.fa \. --reads=/input/m64558e_231018_115114.hifi_reads.sorted.add-rg.rm-tags.bam \. --output_vcf=/output/FD800429.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=1. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:840,security,access,access,840,"I recently experienced a similar issue in which I kept getting the error `ValueError: NOT_FOUND: could not load fasta and/or fai for fasta` when I'd try to run DeepVariant via Docker. The command I was using looked like:. ```. docker run \. -v ""/data/analysis_data/ngs/r64558e_20231017_193725/bam"":""/input"" \. -v ""/data/analysis_data/ngs/r64558e_20231017_193725/result_DeepVariant"":""/output"" \. google/deepvariant:""1.6.0"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=/data/bioinfo.shared/index/hg38_gatk/hg38.fa \. --reads=/input/m64558e_231018_115114.hifi_reads.sorted.add-rg.rm-tags.bam \. --output_vcf=/output/FD800429.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=1. ```. Not having used Docker before, I wasn't aware that I needed to mount all directories I needed to access so that Docker could see them. This issue and [this article](https://towardsdatascience.com/how-to-mount-a-directory-inside-a-docker-container-4cee379c298b) helped me figure out where I went wrong (i.e. using the absolute path in the --ref parameter without having mounted it). The adjusted command (that worked) looked like:. ```. docker run \. -v ""/data/bioinfo.shared/index/hg38_gatk/"":""/index"" \. -v ""/data/analysis_data/ngs/r64558e_20231017_193725/bam"":""/input"" \. -v ""/data/analysis_data/ngs/r64558e_20231017_193725/result_DeepVariant"":""/output"" \. google/deepvariant:""1.6.0"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=/index/hg38.fa \. --reads=/input/m64558e_231018_115114.hifi_reads.sorted.add-rg.rm-tags.bam \. --output_vcf=/output/FD800429.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=1. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:11,usability,experien,experienced,11,"I recently experienced a similar issue in which I kept getting the error `ValueError: NOT_FOUND: could not load fasta and/or fai for fasta` when I'd try to run DeepVariant via Docker. The command I was using looked like:. ```. docker run \. -v ""/data/analysis_data/ngs/r64558e_20231017_193725/bam"":""/input"" \. -v ""/data/analysis_data/ngs/r64558e_20231017_193725/result_DeepVariant"":""/output"" \. google/deepvariant:""1.6.0"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=/data/bioinfo.shared/index/hg38_gatk/hg38.fa \. --reads=/input/m64558e_231018_115114.hifi_reads.sorted.add-rg.rm-tags.bam \. --output_vcf=/output/FD800429.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=1. ```. Not having used Docker before, I wasn't aware that I needed to mount all directories I needed to access so that Docker could see them. This issue and [this article](https://towardsdatascience.com/how-to-mount-a-directory-inside-a-docker-container-4cee379c298b) helped me figure out where I went wrong (i.e. using the absolute path in the --ref parameter without having mounted it). The adjusted command (that worked) looked like:. ```. docker run \. -v ""/data/bioinfo.shared/index/hg38_gatk/"":""/index"" \. -v ""/data/analysis_data/ngs/r64558e_20231017_193725/bam"":""/input"" \. -v ""/data/analysis_data/ngs/r64558e_20231017_193725/result_DeepVariant"":""/output"" \. google/deepvariant:""1.6.0"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=/index/hg38.fa \. --reads=/input/m64558e_231018_115114.hifi_reads.sorted.add-rg.rm-tags.bam \. --output_vcf=/output/FD800429.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=1. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:67,usability,error,error,67,"I recently experienced a similar issue in which I kept getting the error `ValueError: NOT_FOUND: could not load fasta and/or fai for fasta` when I'd try to run DeepVariant via Docker. The command I was using looked like:. ```. docker run \. -v ""/data/analysis_data/ngs/r64558e_20231017_193725/bam"":""/input"" \. -v ""/data/analysis_data/ngs/r64558e_20231017_193725/result_DeepVariant"":""/output"" \. google/deepvariant:""1.6.0"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=/data/bioinfo.shared/index/hg38_gatk/hg38.fa \. --reads=/input/m64558e_231018_115114.hifi_reads.sorted.add-rg.rm-tags.bam \. --output_vcf=/output/FD800429.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=1. ```. Not having used Docker before, I wasn't aware that I needed to mount all directories I needed to access so that Docker could see them. This issue and [this article](https://towardsdatascience.com/how-to-mount-a-directory-inside-a-docker-container-4cee379c298b) helped me figure out where I went wrong (i.e. using the absolute path in the --ref parameter without having mounted it). The adjusted command (that worked) looked like:. ```. docker run \. -v ""/data/bioinfo.shared/index/hg38_gatk/"":""/index"" \. -v ""/data/analysis_data/ngs/r64558e_20231017_193725/bam"":""/input"" \. -v ""/data/analysis_data/ngs/r64558e_20231017_193725/result_DeepVariant"":""/output"" \. google/deepvariant:""1.6.0"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=/index/hg38.fa \. --reads=/input/m64558e_231018_115114.hifi_reads.sorted.add-rg.rm-tags.bam \. --output_vcf=/output/FD800429.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=1. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:188,usability,command,command,188,"I recently experienced a similar issue in which I kept getting the error `ValueError: NOT_FOUND: could not load fasta and/or fai for fasta` when I'd try to run DeepVariant via Docker. The command I was using looked like:. ```. docker run \. -v ""/data/analysis_data/ngs/r64558e_20231017_193725/bam"":""/input"" \. -v ""/data/analysis_data/ngs/r64558e_20231017_193725/result_DeepVariant"":""/output"" \. google/deepvariant:""1.6.0"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=/data/bioinfo.shared/index/hg38_gatk/hg38.fa \. --reads=/input/m64558e_231018_115114.hifi_reads.sorted.add-rg.rm-tags.bam \. --output_vcf=/output/FD800429.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=1. ```. Not having used Docker before, I wasn't aware that I needed to mount all directories I needed to access so that Docker could see them. This issue and [this article](https://towardsdatascience.com/how-to-mount-a-directory-inside-a-docker-container-4cee379c298b) helped me figure out where I went wrong (i.e. using the absolute path in the --ref parameter without having mounted it). The adjusted command (that worked) looked like:. ```. docker run \. -v ""/data/bioinfo.shared/index/hg38_gatk/"":""/index"" \. -v ""/data/analysis_data/ngs/r64558e_20231017_193725/bam"":""/input"" \. -v ""/data/analysis_data/ngs/r64558e_20231017_193725/result_DeepVariant"":""/output"" \. google/deepvariant:""1.6.0"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=/index/hg38.fa \. --reads=/input/m64558e_231018_115114.hifi_reads.sorted.add-rg.rm-tags.bam \. --output_vcf=/output/FD800429.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=1. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:300,usability,input,input,300,"I recently experienced a similar issue in which I kept getting the error `ValueError: NOT_FOUND: could not load fasta and/or fai for fasta` when I'd try to run DeepVariant via Docker. The command I was using looked like:. ```. docker run \. -v ""/data/analysis_data/ngs/r64558e_20231017_193725/bam"":""/input"" \. -v ""/data/analysis_data/ngs/r64558e_20231017_193725/result_DeepVariant"":""/output"" \. google/deepvariant:""1.6.0"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=/data/bioinfo.shared/index/hg38_gatk/hg38.fa \. --reads=/input/m64558e_231018_115114.hifi_reads.sorted.add-rg.rm-tags.bam \. --output_vcf=/output/FD800429.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=1. ```. Not having used Docker before, I wasn't aware that I needed to mount all directories I needed to access so that Docker could see them. This issue and [this article](https://towardsdatascience.com/how-to-mount-a-directory-inside-a-docker-container-4cee379c298b) helped me figure out where I went wrong (i.e. using the absolute path in the --ref parameter without having mounted it). The adjusted command (that worked) looked like:. ```. docker run \. -v ""/data/bioinfo.shared/index/hg38_gatk/"":""/index"" \. -v ""/data/analysis_data/ngs/r64558e_20231017_193725/bam"":""/input"" \. -v ""/data/analysis_data/ngs/r64558e_20231017_193725/result_DeepVariant"":""/output"" \. google/deepvariant:""1.6.0"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=/index/hg38.fa \. --reads=/input/m64558e_231018_115114.hifi_reads.sorted.add-rg.rm-tags.bam \. --output_vcf=/output/FD800429.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=1. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:551,usability,input,input,551,"I recently experienced a similar issue in which I kept getting the error `ValueError: NOT_FOUND: could not load fasta and/or fai for fasta` when I'd try to run DeepVariant via Docker. The command I was using looked like:. ```. docker run \. -v ""/data/analysis_data/ngs/r64558e_20231017_193725/bam"":""/input"" \. -v ""/data/analysis_data/ngs/r64558e_20231017_193725/result_DeepVariant"":""/output"" \. google/deepvariant:""1.6.0"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=/data/bioinfo.shared/index/hg38_gatk/hg38.fa \. --reads=/input/m64558e_231018_115114.hifi_reads.sorted.add-rg.rm-tags.bam \. --output_vcf=/output/FD800429.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=1. ```. Not having used Docker before, I wasn't aware that I needed to mount all directories I needed to access so that Docker could see them. This issue and [this article](https://towardsdatascience.com/how-to-mount-a-directory-inside-a-docker-container-4cee379c298b) helped me figure out where I went wrong (i.e. using the absolute path in the --ref parameter without having mounted it). The adjusted command (that worked) looked like:. ```. docker run \. -v ""/data/bioinfo.shared/index/hg38_gatk/"":""/index"" \. -v ""/data/analysis_data/ngs/r64558e_20231017_193725/bam"":""/input"" \. -v ""/data/analysis_data/ngs/r64558e_20231017_193725/result_DeepVariant"":""/output"" \. google/deepvariant:""1.6.0"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=/index/hg38.fa \. --reads=/input/m64558e_231018_115114.hifi_reads.sorted.add-rg.rm-tags.bam \. --output_vcf=/output/FD800429.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=1. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:1004,usability,help,helped,1004,"I recently experienced a similar issue in which I kept getting the error `ValueError: NOT_FOUND: could not load fasta and/or fai for fasta` when I'd try to run DeepVariant via Docker. The command I was using looked like:. ```. docker run \. -v ""/data/analysis_data/ngs/r64558e_20231017_193725/bam"":""/input"" \. -v ""/data/analysis_data/ngs/r64558e_20231017_193725/result_DeepVariant"":""/output"" \. google/deepvariant:""1.6.0"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=/data/bioinfo.shared/index/hg38_gatk/hg38.fa \. --reads=/input/m64558e_231018_115114.hifi_reads.sorted.add-rg.rm-tags.bam \. --output_vcf=/output/FD800429.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=1. ```. Not having used Docker before, I wasn't aware that I needed to mount all directories I needed to access so that Docker could see them. This issue and [this article](https://towardsdatascience.com/how-to-mount-a-directory-inside-a-docker-container-4cee379c298b) helped me figure out where I went wrong (i.e. using the absolute path in the --ref parameter without having mounted it). The adjusted command (that worked) looked like:. ```. docker run \. -v ""/data/bioinfo.shared/index/hg38_gatk/"":""/index"" \. -v ""/data/analysis_data/ngs/r64558e_20231017_193725/bam"":""/input"" \. -v ""/data/analysis_data/ngs/r64558e_20231017_193725/result_DeepVariant"":""/output"" \. google/deepvariant:""1.6.0"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=/index/hg38.fa \. --reads=/input/m64558e_231018_115114.hifi_reads.sorted.add-rg.rm-tags.bam \. --output_vcf=/output/FD800429.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=1. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:1138,usability,command,command,1138,"I recently experienced a similar issue in which I kept getting the error `ValueError: NOT_FOUND: could not load fasta and/or fai for fasta` when I'd try to run DeepVariant via Docker. The command I was using looked like:. ```. docker run \. -v ""/data/analysis_data/ngs/r64558e_20231017_193725/bam"":""/input"" \. -v ""/data/analysis_data/ngs/r64558e_20231017_193725/result_DeepVariant"":""/output"" \. google/deepvariant:""1.6.0"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=/data/bioinfo.shared/index/hg38_gatk/hg38.fa \. --reads=/input/m64558e_231018_115114.hifi_reads.sorted.add-rg.rm-tags.bam \. --output_vcf=/output/FD800429.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=1. ```. Not having used Docker before, I wasn't aware that I needed to mount all directories I needed to access so that Docker could see them. This issue and [this article](https://towardsdatascience.com/how-to-mount-a-directory-inside-a-docker-container-4cee379c298b) helped me figure out where I went wrong (i.e. using the absolute path in the --ref parameter without having mounted it). The adjusted command (that worked) looked like:. ```. docker run \. -v ""/data/bioinfo.shared/index/hg38_gatk/"":""/index"" \. -v ""/data/analysis_data/ngs/r64558e_20231017_193725/bam"":""/input"" \. -v ""/data/analysis_data/ngs/r64558e_20231017_193725/result_DeepVariant"":""/output"" \. google/deepvariant:""1.6.0"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=/index/hg38.fa \. --reads=/input/m64558e_231018_115114.hifi_reads.sorted.add-rg.rm-tags.bam \. --output_vcf=/output/FD800429.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=1. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:1307,usability,input,input,1307,"I recently experienced a similar issue in which I kept getting the error `ValueError: NOT_FOUND: could not load fasta and/or fai for fasta` when I'd try to run DeepVariant via Docker. The command I was using looked like:. ```. docker run \. -v ""/data/analysis_data/ngs/r64558e_20231017_193725/bam"":""/input"" \. -v ""/data/analysis_data/ngs/r64558e_20231017_193725/result_DeepVariant"":""/output"" \. google/deepvariant:""1.6.0"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=/data/bioinfo.shared/index/hg38_gatk/hg38.fa \. --reads=/input/m64558e_231018_115114.hifi_reads.sorted.add-rg.rm-tags.bam \. --output_vcf=/output/FD800429.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=1. ```. Not having used Docker before, I wasn't aware that I needed to mount all directories I needed to access so that Docker could see them. This issue and [this article](https://towardsdatascience.com/how-to-mount-a-directory-inside-a-docker-container-4cee379c298b) helped me figure out where I went wrong (i.e. using the absolute path in the --ref parameter without having mounted it). The adjusted command (that worked) looked like:. ```. docker run \. -v ""/data/bioinfo.shared/index/hg38_gatk/"":""/index"" \. -v ""/data/analysis_data/ngs/r64558e_20231017_193725/bam"":""/input"" \. -v ""/data/analysis_data/ngs/r64558e_20231017_193725/result_DeepVariant"":""/output"" \. google/deepvariant:""1.6.0"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=/index/hg38.fa \. --reads=/input/m64558e_231018_115114.hifi_reads.sorted.add-rg.rm-tags.bam \. --output_vcf=/output/FD800429.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=1. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:1528,usability,input,input,1528,"I recently experienced a similar issue in which I kept getting the error `ValueError: NOT_FOUND: could not load fasta and/or fai for fasta` when I'd try to run DeepVariant via Docker. The command I was using looked like:. ```. docker run \. -v ""/data/analysis_data/ngs/r64558e_20231017_193725/bam"":""/input"" \. -v ""/data/analysis_data/ngs/r64558e_20231017_193725/result_DeepVariant"":""/output"" \. google/deepvariant:""1.6.0"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=/data/bioinfo.shared/index/hg38_gatk/hg38.fa \. --reads=/input/m64558e_231018_115114.hifi_reads.sorted.add-rg.rm-tags.bam \. --output_vcf=/output/FD800429.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=1. ```. Not having used Docker before, I wasn't aware that I needed to mount all directories I needed to access so that Docker could see them. This issue and [this article](https://towardsdatascience.com/how-to-mount-a-directory-inside-a-docker-container-4cee379c298b) helped me figure out where I went wrong (i.e. using the absolute path in the --ref parameter without having mounted it). The adjusted command (that worked) looked like:. ```. docker run \. -v ""/data/bioinfo.shared/index/hg38_gatk/"":""/index"" \. -v ""/data/analysis_data/ngs/r64558e_20231017_193725/bam"":""/input"" \. -v ""/data/analysis_data/ngs/r64558e_20231017_193725/result_DeepVariant"":""/output"" \. google/deepvariant:""1.6.0"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=/index/hg38.fa \. --reads=/input/m64558e_231018_115114.hifi_reads.sorted.add-rg.rm-tags.bam \. --output_vcf=/output/FD800429.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=1. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/655:1402,availability,down,downsample,1402,"o this variant call, there are a few things that I see: . First, DeepVariant has a very low confidence in this call. The GQ of 4 corresponds to a 40% chance of being incorrect. Interestingly, the second most likely call at this position according to DeepVariant is 0/0 (HomRef). This is in spite of a 0.27 variant allele frequency (something that most callers would probably consider as either 0/0 HomRef or 0/1 HET). . This is an indication that DeepVariant may think that one of the haplotypes (either the Ref one or the Alt one) are unreliable (e.g. that they are reads which map from a different part of the genome), but doesn't know which to consider. Some other lines of evidence DeepVariant might use for that is the higher coverage (1200 is a coverage that would often be seen in duplicated parts of the genome) and the unusual allele frequency ratio (70% Ref, 30% Alt). Another piece of evidence we can't see but DeepVariant may use is the Insert Size channel if this is Illumina data. . A few questions -. 1) What is the sequencing technology used here, and which type of instrument. Is this PacBio or Illumina here? . 2) Is this some form of panel sequencing targeting the region? . One suggestion to try (especially if this is panel short read sequencing) - downsample the region to ~80-100 coverage and see if the call changes. Especially look for the GQ confidence to go up as 4 is very low. If you want to avoid such situations more, you might want to put a higher GQ threshold for downstream filtering, and for a specific case like this look for cases where the PL values show higher probability for HOMREF and ALT than HET. A GQ threshold of 10 would be a 90% probability the call is correct, a GQ threshold of 20 would be a 99% threshold. . Just from the evidence presented here, this site is going to be difficult to call, as just on the VAF this look more like a HET (though the other factors mentioned here about copy number variants could be the factor in allowing a REF call. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/655
https://github.com/google/deepvariant/issues/655:1629,availability,down,downstream,1629,"o this variant call, there are a few things that I see: . First, DeepVariant has a very low confidence in this call. The GQ of 4 corresponds to a 40% chance of being incorrect. Interestingly, the second most likely call at this position according to DeepVariant is 0/0 (HomRef). This is in spite of a 0.27 variant allele frequency (something that most callers would probably consider as either 0/0 HomRef or 0/1 HET). . This is an indication that DeepVariant may think that one of the haplotypes (either the Ref one or the Alt one) are unreliable (e.g. that they are reads which map from a different part of the genome), but doesn't know which to consider. Some other lines of evidence DeepVariant might use for that is the higher coverage (1200 is a coverage that would often be seen in duplicated parts of the genome) and the unusual allele frequency ratio (70% Ref, 30% Alt). Another piece of evidence we can't see but DeepVariant may use is the Insert Size channel if this is Illumina data. . A few questions -. 1) What is the sequencing technology used here, and which type of instrument. Is this PacBio or Illumina here? . 2) Is this some form of panel sequencing targeting the region? . One suggestion to try (especially if this is panel short read sequencing) - downsample the region to ~80-100 coverage and see if the call changes. Especially look for the GQ confidence to go up as 4 is very low. If you want to avoid such situations more, you might want to put a higher GQ threshold for downstream filtering, and for a specific case like this look for cases where the PL values show higher probability for HOMREF and ALT than HET. A GQ threshold of 10 would be a 90% probability the call is correct, a GQ threshold of 20 would be a 99% threshold. . Just from the evidence presented here, this site is going to be difficult to call, as just on the VAF this look more like a HET (though the other factors mentioned here about copy number variants could be the factor in allowing a REF call. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/655
https://github.com/google/deepvariant/issues/655:453,energy efficiency,frequenc,frequency,453,"Hi @crazysummerW . Thank you for the pileup and for the trace. The trace makes it clear there isn't a variant here. . With respect to this variant call, there are a few things that I see: . First, DeepVariant has a very low confidence in this call. The GQ of 4 corresponds to a 40% chance of being incorrect. Interestingly, the second most likely call at this position according to DeepVariant is 0/0 (HomRef). This is in spite of a 0.27 variant allele frequency (something that most callers would probably consider as either 0/0 HomRef or 0/1 HET). . This is an indication that DeepVariant may think that one of the haplotypes (either the Ref one or the Alt one) are unreliable (e.g. that they are reads which map from a different part of the genome), but doesn't know which to consider. Some other lines of evidence DeepVariant might use for that is the higher coverage (1200 is a coverage that would often be seen in duplicated parts of the genome) and the unusual allele frequency ratio (70% Ref, 30% Alt). Another piece of evidence we can't see but DeepVariant may use is the Insert Size channel if this is Illumina data. . A few questions -. 1) What is the sequencing technology used here, and which type of instrument. Is this PacBio or Illumina here? . 2) Is this some form of panel sequencing targeting the region? . One suggestion to try (especially if this is panel short read sequencing) - downsample the region to ~80-100 coverage and see if the call changes. Especially look for the GQ confidence to go up as 4 is very low. If you want to avoid such situations more, you might want to put a higher GQ threshold for downstream filtering, and for a specific case like this look for cases where the PL values show higher probability for HOMREF and ALT than HET. A GQ threshold of 10 would be a 90% probability the call is correct, a GQ threshold of 20 would be a 99% threshold. . Just from the evidence presented here, this site is going to be difficult to call, as just on the VAF this lo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/655
https://github.com/google/deepvariant/issues/655:975,energy efficiency,frequenc,frequency,975,"Hi @crazysummerW . Thank you for the pileup and for the trace. The trace makes it clear there isn't a variant here. . With respect to this variant call, there are a few things that I see: . First, DeepVariant has a very low confidence in this call. The GQ of 4 corresponds to a 40% chance of being incorrect. Interestingly, the second most likely call at this position according to DeepVariant is 0/0 (HomRef). This is in spite of a 0.27 variant allele frequency (something that most callers would probably consider as either 0/0 HomRef or 0/1 HET). . This is an indication that DeepVariant may think that one of the haplotypes (either the Ref one or the Alt one) are unreliable (e.g. that they are reads which map from a different part of the genome), but doesn't know which to consider. Some other lines of evidence DeepVariant might use for that is the higher coverage (1200 is a coverage that would often be seen in duplicated parts of the genome) and the unusual allele frequency ratio (70% Ref, 30% Alt). Another piece of evidence we can't see but DeepVariant may use is the Insert Size channel if this is Illumina data. . A few questions -. 1) What is the sequencing technology used here, and which type of instrument. Is this PacBio or Illumina here? . 2) Is this some form of panel sequencing targeting the region? . One suggestion to try (especially if this is panel short read sequencing) - downsample the region to ~80-100 coverage and see if the call changes. Especially look for the GQ confidence to go up as 4 is very low. If you want to avoid such situations more, you might want to put a higher GQ threshold for downstream filtering, and for a specific case like this look for cases where the PL values show higher probability for HOMREF and ALT than HET. A GQ threshold of 10 would be a 90% probability the call is correct, a GQ threshold of 20 would be a 99% threshold. . Just from the evidence presented here, this site is going to be difficult to call, as just on the VAF this lo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/655
https://github.com/google/deepvariant/issues/655:1640,integrability,filter,filtering,1640,"o this variant call, there are a few things that I see: . First, DeepVariant has a very low confidence in this call. The GQ of 4 corresponds to a 40% chance of being incorrect. Interestingly, the second most likely call at this position according to DeepVariant is 0/0 (HomRef). This is in spite of a 0.27 variant allele frequency (something that most callers would probably consider as either 0/0 HomRef or 0/1 HET). . This is an indication that DeepVariant may think that one of the haplotypes (either the Ref one or the Alt one) are unreliable (e.g. that they are reads which map from a different part of the genome), but doesn't know which to consider. Some other lines of evidence DeepVariant might use for that is the higher coverage (1200 is a coverage that would often be seen in duplicated parts of the genome) and the unusual allele frequency ratio (70% Ref, 30% Alt). Another piece of evidence we can't see but DeepVariant may use is the Insert Size channel if this is Illumina data. . A few questions -. 1) What is the sequencing technology used here, and which type of instrument. Is this PacBio or Illumina here? . 2) Is this some form of panel sequencing targeting the region? . One suggestion to try (especially if this is panel short read sequencing) - downsample the region to ~80-100 coverage and see if the call changes. Especially look for the GQ confidence to go up as 4 is very low. If you want to avoid such situations more, you might want to put a higher GQ threshold for downstream filtering, and for a specific case like this look for cases where the PL values show higher probability for HOMREF and ALT than HET. A GQ threshold of 10 would be a 90% probability the call is correct, a GQ threshold of 20 would be a 99% threshold. . Just from the evidence presented here, this site is going to be difficult to call, as just on the VAF this look more like a HET (though the other factors mentioned here about copy number variants could be the factor in allowing a REF call. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/655
https://github.com/google/deepvariant/issues/655:1661,interoperability,specif,specific,1661,"o this variant call, there are a few things that I see: . First, DeepVariant has a very low confidence in this call. The GQ of 4 corresponds to a 40% chance of being incorrect. Interestingly, the second most likely call at this position according to DeepVariant is 0/0 (HomRef). This is in spite of a 0.27 variant allele frequency (something that most callers would probably consider as either 0/0 HomRef or 0/1 HET). . This is an indication that DeepVariant may think that one of the haplotypes (either the Ref one or the Alt one) are unreliable (e.g. that they are reads which map from a different part of the genome), but doesn't know which to consider. Some other lines of evidence DeepVariant might use for that is the higher coverage (1200 is a coverage that would often be seen in duplicated parts of the genome) and the unusual allele frequency ratio (70% Ref, 30% Alt). Another piece of evidence we can't see but DeepVariant may use is the Insert Size channel if this is Illumina data. . A few questions -. 1) What is the sequencing technology used here, and which type of instrument. Is this PacBio or Illumina here? . 2) Is this some form of panel sequencing targeting the region? . One suggestion to try (especially if this is panel short read sequencing) - downsample the region to ~80-100 coverage and see if the call changes. Especially look for the GQ confidence to go up as 4 is very low. If you want to avoid such situations more, you might want to put a higher GQ threshold for downstream filtering, and for a specific case like this look for cases where the PL values show higher probability for HOMREF and ALT than HET. A GQ threshold of 10 would be a 90% probability the call is correct, a GQ threshold of 20 would be a 99% threshold. . Just from the evidence presented here, this site is going to be difficult to call, as just on the VAF this look more like a HET (though the other factors mentioned here about copy number variants could be the factor in allowing a REF call. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/655
https://github.com/google/deepvariant/issues/655:1234,modifiability,Pac,PacBio,1234,"o this variant call, there are a few things that I see: . First, DeepVariant has a very low confidence in this call. The GQ of 4 corresponds to a 40% chance of being incorrect. Interestingly, the second most likely call at this position according to DeepVariant is 0/0 (HomRef). This is in spite of a 0.27 variant allele frequency (something that most callers would probably consider as either 0/0 HomRef or 0/1 HET). . This is an indication that DeepVariant may think that one of the haplotypes (either the Ref one or the Alt one) are unreliable (e.g. that they are reads which map from a different part of the genome), but doesn't know which to consider. Some other lines of evidence DeepVariant might use for that is the higher coverage (1200 is a coverage that would often be seen in duplicated parts of the genome) and the unusual allele frequency ratio (70% Ref, 30% Alt). Another piece of evidence we can't see but DeepVariant may use is the Insert Size channel if this is Illumina data. . A few questions -. 1) What is the sequencing technology used here, and which type of instrument. Is this PacBio or Illumina here? . 2) Is this some form of panel sequencing targeting the region? . One suggestion to try (especially if this is panel short read sequencing) - downsample the region to ~80-100 coverage and see if the call changes. Especially look for the GQ confidence to go up as 4 is very low. If you want to avoid such situations more, you might want to put a higher GQ threshold for downstream filtering, and for a specific case like this look for cases where the PL values show higher probability for HOMREF and ALT than HET. A GQ threshold of 10 would be a 90% probability the call is correct, a GQ threshold of 20 would be a 99% threshold. . Just from the evidence presented here, this site is going to be difficult to call, as just on the VAF this look more like a HET (though the other factors mentioned here about copy number variants could be the factor in allowing a REF call. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/655
https://github.com/google/deepvariant/issues/655:757,reliability,doe,doesn,757,"Hi @crazysummerW . Thank you for the pileup and for the trace. The trace makes it clear there isn't a variant here. . With respect to this variant call, there are a few things that I see: . First, DeepVariant has a very low confidence in this call. The GQ of 4 corresponds to a 40% chance of being incorrect. Interestingly, the second most likely call at this position according to DeepVariant is 0/0 (HomRef). This is in spite of a 0.27 variant allele frequency (something that most callers would probably consider as either 0/0 HomRef or 0/1 HET). . This is an indication that DeepVariant may think that one of the haplotypes (either the Ref one or the Alt one) are unreliable (e.g. that they are reads which map from a different part of the genome), but doesn't know which to consider. Some other lines of evidence DeepVariant might use for that is the higher coverage (1200 is a coverage that would often be seen in duplicated parts of the genome) and the unusual allele frequency ratio (70% Ref, 30% Alt). Another piece of evidence we can't see but DeepVariant may use is the Insert Size channel if this is Illumina data. . A few questions -. 1) What is the sequencing technology used here, and which type of instrument. Is this PacBio or Illumina here? . 2) Is this some form of panel sequencing targeting the region? . One suggestion to try (especially if this is panel short read sequencing) - downsample the region to ~80-100 coverage and see if the call changes. Especially look for the GQ confidence to go up as 4 is very low. If you want to avoid such situations more, you might want to put a higher GQ threshold for downstream filtering, and for a specific case like this look for cases where the PL values show higher probability for HOMREF and ALT than HET. A GQ threshold of 10 would be a 90% probability the call is correct, a GQ threshold of 20 would be a 99% threshold. . Just from the evidence presented here, this site is going to be difficult to call, as just on the VAF this lo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/655
https://github.com/google/deepvariant/issues/655:1553,safety,avoid,avoid,1553,"o this variant call, there are a few things that I see: . First, DeepVariant has a very low confidence in this call. The GQ of 4 corresponds to a 40% chance of being incorrect. Interestingly, the second most likely call at this position according to DeepVariant is 0/0 (HomRef). This is in spite of a 0.27 variant allele frequency (something that most callers would probably consider as either 0/0 HomRef or 0/1 HET). . This is an indication that DeepVariant may think that one of the haplotypes (either the Ref one or the Alt one) are unreliable (e.g. that they are reads which map from a different part of the genome), but doesn't know which to consider. Some other lines of evidence DeepVariant might use for that is the higher coverage (1200 is a coverage that would often be seen in duplicated parts of the genome) and the unusual allele frequency ratio (70% Ref, 30% Alt). Another piece of evidence we can't see but DeepVariant may use is the Insert Size channel if this is Illumina data. . A few questions -. 1) What is the sequencing technology used here, and which type of instrument. Is this PacBio or Illumina here? . 2) Is this some form of panel sequencing targeting the region? . One suggestion to try (especially if this is panel short read sequencing) - downsample the region to ~80-100 coverage and see if the call changes. Especially look for the GQ confidence to go up as 4 is very low. If you want to avoid such situations more, you might want to put a higher GQ threshold for downstream filtering, and for a specific case like this look for cases where the PL values show higher probability for HOMREF and ALT than HET. A GQ threshold of 10 would be a 90% probability the call is correct, a GQ threshold of 20 would be a 99% threshold. . Just from the evidence presented here, this site is going to be difficult to call, as just on the VAF this look more like a HET (though the other factors mentioned here about copy number variants could be the factor in allowing a REF call. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/655
https://github.com/google/deepvariant/issues/655:56,testability,trace,trace,56,"Hi @crazysummerW . Thank you for the pileup and for the trace. The trace makes it clear there isn't a variant here. . With respect to this variant call, there are a few things that I see: . First, DeepVariant has a very low confidence in this call. The GQ of 4 corresponds to a 40% chance of being incorrect. Interestingly, the second most likely call at this position according to DeepVariant is 0/0 (HomRef). This is in spite of a 0.27 variant allele frequency (something that most callers would probably consider as either 0/0 HomRef or 0/1 HET). . This is an indication that DeepVariant may think that one of the haplotypes (either the Ref one or the Alt one) are unreliable (e.g. that they are reads which map from a different part of the genome), but doesn't know which to consider. Some other lines of evidence DeepVariant might use for that is the higher coverage (1200 is a coverage that would often be seen in duplicated parts of the genome) and the unusual allele frequency ratio (70% Ref, 30% Alt). Another piece of evidence we can't see but DeepVariant may use is the Insert Size channel if this is Illumina data. . A few questions -. 1) What is the sequencing technology used here, and which type of instrument. Is this PacBio or Illumina here? . 2) Is this some form of panel sequencing targeting the region? . One suggestion to try (especially if this is panel short read sequencing) - downsample the region to ~80-100 coverage and see if the call changes. Especially look for the GQ confidence to go up as 4 is very low. If you want to avoid such situations more, you might want to put a higher GQ threshold for downstream filtering, and for a specific case like this look for cases where the PL values show higher probability for HOMREF and ALT than HET. A GQ threshold of 10 would be a 90% probability the call is correct, a GQ threshold of 20 would be a 99% threshold. . Just from the evidence presented here, this site is going to be difficult to call, as just on the VAF this lo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/655
https://github.com/google/deepvariant/issues/655:67,testability,trace,trace,67,"Hi @crazysummerW . Thank you for the pileup and for the trace. The trace makes it clear there isn't a variant here. . With respect to this variant call, there are a few things that I see: . First, DeepVariant has a very low confidence in this call. The GQ of 4 corresponds to a 40% chance of being incorrect. Interestingly, the second most likely call at this position according to DeepVariant is 0/0 (HomRef). This is in spite of a 0.27 variant allele frequency (something that most callers would probably consider as either 0/0 HomRef or 0/1 HET). . This is an indication that DeepVariant may think that one of the haplotypes (either the Ref one or the Alt one) are unreliable (e.g. that they are reads which map from a different part of the genome), but doesn't know which to consider. Some other lines of evidence DeepVariant might use for that is the higher coverage (1200 is a coverage that would often be seen in duplicated parts of the genome) and the unusual allele frequency ratio (70% Ref, 30% Alt). Another piece of evidence we can't see but DeepVariant may use is the Insert Size channel if this is Illumina data. . A few questions -. 1) What is the sequencing technology used here, and which type of instrument. Is this PacBio or Illumina here? . 2) Is this some form of panel sequencing targeting the region? . One suggestion to try (especially if this is panel short read sequencing) - downsample the region to ~80-100 coverage and see if the call changes. Especially look for the GQ confidence to go up as 4 is very low. If you want to avoid such situations more, you might want to put a higher GQ threshold for downstream filtering, and for a specific case like this look for cases where the PL values show higher probability for HOMREF and ALT than HET. A GQ threshold of 10 would be a 90% probability the call is correct, a GQ threshold of 20 would be a 99% threshold. . Just from the evidence presented here, this site is going to be difficult to call, as just on the VAF this lo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/655
https://github.com/google/deepvariant/issues/655:863,testability,coverag,coverage,863,"Hi @crazysummerW . Thank you for the pileup and for the trace. The trace makes it clear there isn't a variant here. . With respect to this variant call, there are a few things that I see: . First, DeepVariant has a very low confidence in this call. The GQ of 4 corresponds to a 40% chance of being incorrect. Interestingly, the second most likely call at this position according to DeepVariant is 0/0 (HomRef). This is in spite of a 0.27 variant allele frequency (something that most callers would probably consider as either 0/0 HomRef or 0/1 HET). . This is an indication that DeepVariant may think that one of the haplotypes (either the Ref one or the Alt one) are unreliable (e.g. that they are reads which map from a different part of the genome), but doesn't know which to consider. Some other lines of evidence DeepVariant might use for that is the higher coverage (1200 is a coverage that would often be seen in duplicated parts of the genome) and the unusual allele frequency ratio (70% Ref, 30% Alt). Another piece of evidence we can't see but DeepVariant may use is the Insert Size channel if this is Illumina data. . A few questions -. 1) What is the sequencing technology used here, and which type of instrument. Is this PacBio or Illumina here? . 2) Is this some form of panel sequencing targeting the region? . One suggestion to try (especially if this is panel short read sequencing) - downsample the region to ~80-100 coverage and see if the call changes. Especially look for the GQ confidence to go up as 4 is very low. If you want to avoid such situations more, you might want to put a higher GQ threshold for downstream filtering, and for a specific case like this look for cases where the PL values show higher probability for HOMREF and ALT than HET. A GQ threshold of 10 would be a 90% probability the call is correct, a GQ threshold of 20 would be a 99% threshold. . Just from the evidence presented here, this site is going to be difficult to call, as just on the VAF this lo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/655
https://github.com/google/deepvariant/issues/655:883,testability,coverag,coverage,883,"Hi @crazysummerW . Thank you for the pileup and for the trace. The trace makes it clear there isn't a variant here. . With respect to this variant call, there are a few things that I see: . First, DeepVariant has a very low confidence in this call. The GQ of 4 corresponds to a 40% chance of being incorrect. Interestingly, the second most likely call at this position according to DeepVariant is 0/0 (HomRef). This is in spite of a 0.27 variant allele frequency (something that most callers would probably consider as either 0/0 HomRef or 0/1 HET). . This is an indication that DeepVariant may think that one of the haplotypes (either the Ref one or the Alt one) are unreliable (e.g. that they are reads which map from a different part of the genome), but doesn't know which to consider. Some other lines of evidence DeepVariant might use for that is the higher coverage (1200 is a coverage that would often be seen in duplicated parts of the genome) and the unusual allele frequency ratio (70% Ref, 30% Alt). Another piece of evidence we can't see but DeepVariant may use is the Insert Size channel if this is Illumina data. . A few questions -. 1) What is the sequencing technology used here, and which type of instrument. Is this PacBio or Illumina here? . 2) Is this some form of panel sequencing targeting the region? . One suggestion to try (especially if this is panel short read sequencing) - downsample the region to ~80-100 coverage and see if the call changes. Especially look for the GQ confidence to go up as 4 is very low. If you want to avoid such situations more, you might want to put a higher GQ threshold for downstream filtering, and for a specific case like this look for cases where the PL values show higher probability for HOMREF and ALT than HET. A GQ threshold of 10 would be a 90% probability the call is correct, a GQ threshold of 20 would be a 99% threshold. . Just from the evidence presented here, this site is going to be difficult to call, as just on the VAF this lo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/655
https://github.com/google/deepvariant/issues/655:1214,testability,instrument,instrument,1214,"o this variant call, there are a few things that I see: . First, DeepVariant has a very low confidence in this call. The GQ of 4 corresponds to a 40% chance of being incorrect. Interestingly, the second most likely call at this position according to DeepVariant is 0/0 (HomRef). This is in spite of a 0.27 variant allele frequency (something that most callers would probably consider as either 0/0 HomRef or 0/1 HET). . This is an indication that DeepVariant may think that one of the haplotypes (either the Ref one or the Alt one) are unreliable (e.g. that they are reads which map from a different part of the genome), but doesn't know which to consider. Some other lines of evidence DeepVariant might use for that is the higher coverage (1200 is a coverage that would often be seen in duplicated parts of the genome) and the unusual allele frequency ratio (70% Ref, 30% Alt). Another piece of evidence we can't see but DeepVariant may use is the Insert Size channel if this is Illumina data. . A few questions -. 1) What is the sequencing technology used here, and which type of instrument. Is this PacBio or Illumina here? . 2) Is this some form of panel sequencing targeting the region? . One suggestion to try (especially if this is panel short read sequencing) - downsample the region to ~80-100 coverage and see if the call changes. Especially look for the GQ confidence to go up as 4 is very low. If you want to avoid such situations more, you might want to put a higher GQ threshold for downstream filtering, and for a specific case like this look for cases where the PL values show higher probability for HOMREF and ALT than HET. A GQ threshold of 10 would be a 90% probability the call is correct, a GQ threshold of 20 would be a 99% threshold. . Just from the evidence presented here, this site is going to be difficult to call, as just on the VAF this look more like a HET (though the other factors mentioned here about copy number variants could be the factor in allowing a REF call. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/655
https://github.com/google/deepvariant/issues/655:1435,testability,coverag,coverage,1435,"o this variant call, there are a few things that I see: . First, DeepVariant has a very low confidence in this call. The GQ of 4 corresponds to a 40% chance of being incorrect. Interestingly, the second most likely call at this position according to DeepVariant is 0/0 (HomRef). This is in spite of a 0.27 variant allele frequency (something that most callers would probably consider as either 0/0 HomRef or 0/1 HET). . This is an indication that DeepVariant may think that one of the haplotypes (either the Ref one or the Alt one) are unreliable (e.g. that they are reads which map from a different part of the genome), but doesn't know which to consider. Some other lines of evidence DeepVariant might use for that is the higher coverage (1200 is a coverage that would often be seen in duplicated parts of the genome) and the unusual allele frequency ratio (70% Ref, 30% Alt). Another piece of evidence we can't see but DeepVariant may use is the Insert Size channel if this is Illumina data. . A few questions -. 1) What is the sequencing technology used here, and which type of instrument. Is this PacBio or Illumina here? . 2) Is this some form of panel sequencing targeting the region? . One suggestion to try (especially if this is panel short read sequencing) - downsample the region to ~80-100 coverage and see if the call changes. Especially look for the GQ confidence to go up as 4 is very low. If you want to avoid such situations more, you might want to put a higher GQ threshold for downstream filtering, and for a specific case like this look for cases where the PL values show higher probability for HOMREF and ALT than HET. A GQ threshold of 10 would be a 90% probability the call is correct, a GQ threshold of 20 would be a 99% threshold. . Just from the evidence presented here, this site is going to be difficult to call, as just on the VAF this look more like a HET (though the other factors mentioned here about copy number variants could be the factor in allowing a REF call. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/655
https://github.com/google/deepvariant/issues/655:82,usability,clear,clear,82,"Hi @crazysummerW . Thank you for the pileup and for the trace. The trace makes it clear there isn't a variant here. . With respect to this variant call, there are a few things that I see: . First, DeepVariant has a very low confidence in this call. The GQ of 4 corresponds to a 40% chance of being incorrect. Interestingly, the second most likely call at this position according to DeepVariant is 0/0 (HomRef). This is in spite of a 0.27 variant allele frequency (something that most callers would probably consider as either 0/0 HomRef or 0/1 HET). . This is an indication that DeepVariant may think that one of the haplotypes (either the Ref one or the Alt one) are unreliable (e.g. that they are reads which map from a different part of the genome), but doesn't know which to consider. Some other lines of evidence DeepVariant might use for that is the higher coverage (1200 is a coverage that would often be seen in duplicated parts of the genome) and the unusual allele frequency ratio (70% Ref, 30% Alt). Another piece of evidence we can't see but DeepVariant may use is the Insert Size channel if this is Illumina data. . A few questions -. 1) What is the sequencing technology used here, and which type of instrument. Is this PacBio or Illumina here? . 2) Is this some form of panel sequencing targeting the region? . One suggestion to try (especially if this is panel short read sequencing) - downsample the region to ~80-100 coverage and see if the call changes. Especially look for the GQ confidence to go up as 4 is very low. If you want to avoid such situations more, you might want to put a higher GQ threshold for downstream filtering, and for a specific case like this look for cases where the PL values show higher probability for HOMREF and ALT than HET. A GQ threshold of 10 would be a 90% probability the call is correct, a GQ threshold of 20 would be a 99% threshold. . Just from the evidence presented here, this site is going to be difficult to call, as just on the VAF this lo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/655
https://github.com/google/deepvariant/issues/655:563,usability,indicat,indication,563,"Hi @crazysummerW . Thank you for the pileup and for the trace. The trace makes it clear there isn't a variant here. . With respect to this variant call, there are a few things that I see: . First, DeepVariant has a very low confidence in this call. The GQ of 4 corresponds to a 40% chance of being incorrect. Interestingly, the second most likely call at this position according to DeepVariant is 0/0 (HomRef). This is in spite of a 0.27 variant allele frequency (something that most callers would probably consider as either 0/0 HomRef or 0/1 HET). . This is an indication that DeepVariant may think that one of the haplotypes (either the Ref one or the Alt one) are unreliable (e.g. that they are reads which map from a different part of the genome), but doesn't know which to consider. Some other lines of evidence DeepVariant might use for that is the higher coverage (1200 is a coverage that would often be seen in duplicated parts of the genome) and the unusual allele frequency ratio (70% Ref, 30% Alt). Another piece of evidence we can't see but DeepVariant may use is the Insert Size channel if this is Illumina data. . A few questions -. 1) What is the sequencing technology used here, and which type of instrument. Is this PacBio or Illumina here? . 2) Is this some form of panel sequencing targeting the region? . One suggestion to try (especially if this is panel short read sequencing) - downsample the region to ~80-100 coverage and see if the call changes. Especially look for the GQ confidence to go up as 4 is very low. If you want to avoid such situations more, you might want to put a higher GQ threshold for downstream filtering, and for a specific case like this look for cases where the PL values show higher probability for HOMREF and ALT than HET. A GQ threshold of 10 would be a 90% probability the call is correct, a GQ threshold of 20 would be a 99% threshold. . Just from the evidence presented here, this site is going to be difficult to call, as just on the VAF this lo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/655
https://github.com/google/deepvariant/issues/655:103,integrability,filter,filtering,103,"hi，@AndrewCarroll . Thank you for your suggestion. This is a new platform panel data, we will consider filtering GQ to avoid this problem.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/655
https://github.com/google/deepvariant/issues/655:65,interoperability,platform,platform,65,"hi，@AndrewCarroll . Thank you for your suggestion. This is a new platform panel data, we will consider filtering GQ to avoid this problem.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/655
https://github.com/google/deepvariant/issues/655:119,safety,avoid,avoid,119,"hi，@AndrewCarroll . Thank you for your suggestion. This is a new platform panel data, we will consider filtering GQ to avoid this problem.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/655
https://github.com/google/deepvariant/issues/656:152,integrability,messag,message,152,https://github.com/google/nucleus/blob/v0.6.0/nucleus/protos/range.proto#L18. ```. // A 0-based half-open genomic coordinate range for search requests. message Range {. ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/656
https://github.com/google/deepvariant/issues/656:114,interoperability,coordinat,coordinate,114,https://github.com/google/nucleus/blob/v0.6.0/nucleus/protos/range.proto#L18. ```. // A 0-based half-open genomic coordinate range for search requests. message Range {. ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/656
https://github.com/google/deepvariant/issues/656:152,interoperability,messag,message,152,https://github.com/google/nucleus/blob/v0.6.0/nucleus/protos/range.proto#L18. ```. // A 0-based half-open genomic coordinate range for search requests. message Range {. ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/656
https://github.com/google/deepvariant/issues/656:37,interoperability,coordinat,coordinate,37,"Thank you @pgrosu. Yes, it's 0-based coordinate.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/656
https://github.com/google/deepvariant/issues/657:91,interoperability,platform,platform,91,"Out of curiosity, if you are using Docker Desktop for Apple Silicon, have you tried the `--platform` flag using the regular docker image for DeepVariant 1.5 based on linux/amd64? The flag used with `docker run` would be like this:. ```. docker run --platform linux/amd64 google/deepvariant:1.5.0. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:250,interoperability,platform,platform,250,"Out of curiosity, if you are using Docker Desktop for Apple Silicon, have you tried the `--platform` flag using the regular docker image for DeepVariant 1.5 based on linux/amd64? The flag used with `docker run` would be like this:. ```. docker run --platform linux/amd64 google/deepvariant:1.5.0. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:46,availability,error,error,46,"@pgrosu Thank you for your reply. This is the error (which started the whole troubleshooting):. ```. The TensorFlow library was compiled to use AVX instructions, but these aren't available on your machine. qemu: uncaught target signal 6 (Aborted) - core dumped. /opt/deepvariant/bin/run_deepvariant: line 2: 8 Aborted python3 -u /opt/deepvariant/bin/run_deepvariant.py ""$@"". ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:179,availability,avail,available,179,"@pgrosu Thank you for your reply. This is the error (which started the whole troubleshooting):. ```. The TensorFlow library was compiled to use AVX instructions, but these aren't available on your machine. qemu: uncaught target signal 6 (Aborted) - core dumped. /opt/deepvariant/bin/run_deepvariant: line 2: 8 Aborted python3 -u /opt/deepvariant/bin/run_deepvariant.py ""$@"". ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:249,energy efficiency,core,core,249,"@pgrosu Thank you for your reply. This is the error (which started the whole troubleshooting):. ```. The TensorFlow library was compiled to use AVX instructions, but these aren't available on your machine. qemu: uncaught target signal 6 (Aborted) - core dumped. /opt/deepvariant/bin/run_deepvariant: line 2: 8 Aborted python3 -u /opt/deepvariant/bin/run_deepvariant.py ""$@"". ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:46,performance,error,error,46,"@pgrosu Thank you for your reply. This is the error (which started the whole troubleshooting):. ```. The TensorFlow library was compiled to use AVX instructions, but these aren't available on your machine. qemu: uncaught target signal 6 (Aborted) - core dumped. /opt/deepvariant/bin/run_deepvariant: line 2: 8 Aborted python3 -u /opt/deepvariant/bin/run_deepvariant.py ""$@"". ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:179,reliability,availab,available,179,"@pgrosu Thank you for your reply. This is the error (which started the whole troubleshooting):. ```. The TensorFlow library was compiled to use AVX instructions, but these aren't available on your machine. qemu: uncaught target signal 6 (Aborted) - core dumped. /opt/deepvariant/bin/run_deepvariant: line 2: 8 Aborted python3 -u /opt/deepvariant/bin/run_deepvariant.py ""$@"". ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:46,safety,error,error,46,"@pgrosu Thank you for your reply. This is the error (which started the whole troubleshooting):. ```. The TensorFlow library was compiled to use AVX instructions, but these aren't available on your machine. qemu: uncaught target signal 6 (Aborted) - core dumped. /opt/deepvariant/bin/run_deepvariant: line 2: 8 Aborted python3 -u /opt/deepvariant/bin/run_deepvariant.py ""$@"". ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:179,safety,avail,available,179,"@pgrosu Thank you for your reply. This is the error (which started the whole troubleshooting):. ```. The TensorFlow library was compiled to use AVX instructions, but these aren't available on your machine. qemu: uncaught target signal 6 (Aborted) - core dumped. /opt/deepvariant/bin/run_deepvariant: line 2: 8 Aborted python3 -u /opt/deepvariant/bin/run_deepvariant.py ""$@"". ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:179,security,availab,available,179,"@pgrosu Thank you for your reply. This is the error (which started the whole troubleshooting):. ```. The TensorFlow library was compiled to use AVX instructions, but these aren't available on your machine. qemu: uncaught target signal 6 (Aborted) - core dumped. /opt/deepvariant/bin/run_deepvariant: line 2: 8 Aborted python3 -u /opt/deepvariant/bin/run_deepvariant.py ""$@"". ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:228,security,sign,signal,228,"@pgrosu Thank you for your reply. This is the error (which started the whole troubleshooting):. ```. The TensorFlow library was compiled to use AVX instructions, but these aren't available on your machine. qemu: uncaught target signal 6 (Aborted) - core dumped. /opt/deepvariant/bin/run_deepvariant: line 2: 8 Aborted python3 -u /opt/deepvariant/bin/run_deepvariant.py ""$@"". ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:46,usability,error,error,46,"@pgrosu Thank you for your reply. This is the error (which started the whole troubleshooting):. ```. The TensorFlow library was compiled to use AVX instructions, but these aren't available on your machine. qemu: uncaught target signal 6 (Aborted) - core dumped. /opt/deepvariant/bin/run_deepvariant: line 2: 8 Aborted python3 -u /opt/deepvariant/bin/run_deepvariant.py ""$@"". ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:511,availability,avail,available,511,"@heznanda Try the following to see if it fixes that requirement:. 1) Install `qemu` and `colima` like this -- colima will temporarily change the Docker runtime context to colima (see below):. ```. brew install qemu. brew install colima. ```. 2) Enable colima to be your runtime for Docker via `colima start` like this with this configuration:. ```. colima start --arch x86_64 --cpu 2 --memory 2 --disk 12 --cpu-type Broadwell-v4. ```. The above settings are in gigabytes, so adjust accordingly to what you have available on your machine. 3) Now try running the docker container for DeepVariant again. . 4) After you are done, then run `colima stop` to change Docker back to its default configurations.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:69,deployability,Instal,Install,69,"@heznanda Try the following to see if it fixes that requirement:. 1) Install `qemu` and `colima` like this -- colima will temporarily change the Docker runtime context to colima (see below):. ```. brew install qemu. brew install colima. ```. 2) Enable colima to be your runtime for Docker via `colima start` like this with this configuration:. ```. colima start --arch x86_64 --cpu 2 --memory 2 --disk 12 --cpu-type Broadwell-v4. ```. The above settings are in gigabytes, so adjust accordingly to what you have available on your machine. 3) Now try running the docker container for DeepVariant again. . 4) After you are done, then run `colima stop` to change Docker back to its default configurations.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:202,deployability,instal,install,202,"@heznanda Try the following to see if it fixes that requirement:. 1) Install `qemu` and `colima` like this -- colima will temporarily change the Docker runtime context to colima (see below):. ```. brew install qemu. brew install colima. ```. 2) Enable colima to be your runtime for Docker via `colima start` like this with this configuration:. ```. colima start --arch x86_64 --cpu 2 --memory 2 --disk 12 --cpu-type Broadwell-v4. ```. The above settings are in gigabytes, so adjust accordingly to what you have available on your machine. 3) Now try running the docker container for DeepVariant again. . 4) After you are done, then run `colima stop` to change Docker back to its default configurations.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:221,deployability,instal,install,221,"@heznanda Try the following to see if it fixes that requirement:. 1) Install `qemu` and `colima` like this -- colima will temporarily change the Docker runtime context to colima (see below):. ```. brew install qemu. brew install colima. ```. 2) Enable colima to be your runtime for Docker via `colima start` like this with this configuration:. ```. colima start --arch x86_64 --cpu 2 --memory 2 --disk 12 --cpu-type Broadwell-v4. ```. The above settings are in gigabytes, so adjust accordingly to what you have available on your machine. 3) Now try running the docker container for DeepVariant again. . 4) After you are done, then run `colima stop` to change Docker back to its default configurations.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:328,deployability,configurat,configuration,328,"@heznanda Try the following to see if it fixes that requirement:. 1) Install `qemu` and `colima` like this -- colima will temporarily change the Docker runtime context to colima (see below):. ```. brew install qemu. brew install colima. ```. 2) Enable colima to be your runtime for Docker via `colima start` like this with this configuration:. ```. colima start --arch x86_64 --cpu 2 --memory 2 --disk 12 --cpu-type Broadwell-v4. ```. The above settings are in gigabytes, so adjust accordingly to what you have available on your machine. 3) Now try running the docker container for DeepVariant again. . 4) After you are done, then run `colima stop` to change Docker back to its default configurations.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:568,deployability,contain,container,568,"@heznanda Try the following to see if it fixes that requirement:. 1) Install `qemu` and `colima` like this -- colima will temporarily change the Docker runtime context to colima (see below):. ```. brew install qemu. brew install colima. ```. 2) Enable colima to be your runtime for Docker via `colima start` like this with this configuration:. ```. colima start --arch x86_64 --cpu 2 --memory 2 --disk 12 --cpu-type Broadwell-v4. ```. The above settings are in gigabytes, so adjust accordingly to what you have available on your machine. 3) Now try running the docker container for DeepVariant again. . 4) After you are done, then run `colima stop` to change Docker back to its default configurations.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:686,deployability,configurat,configurations,686,"@heznanda Try the following to see if it fixes that requirement:. 1) Install `qemu` and `colima` like this -- colima will temporarily change the Docker runtime context to colima (see below):. ```. brew install qemu. brew install colima. ```. 2) Enable colima to be your runtime for Docker via `colima start` like this with this configuration:. ```. colima start --arch x86_64 --cpu 2 --memory 2 --disk 12 --cpu-type Broadwell-v4. ```. The above settings are in gigabytes, so adjust accordingly to what you have available on your machine. 3) Now try running the docker container for DeepVariant again. . 4) After you are done, then run `colima stop` to change Docker back to its default configurations.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:378,energy efficiency,cpu,cpu,378,"@heznanda Try the following to see if it fixes that requirement:. 1) Install `qemu` and `colima` like this -- colima will temporarily change the Docker runtime context to colima (see below):. ```. brew install qemu. brew install colima. ```. 2) Enable colima to be your runtime for Docker via `colima start` like this with this configuration:. ```. colima start --arch x86_64 --cpu 2 --memory 2 --disk 12 --cpu-type Broadwell-v4. ```. The above settings are in gigabytes, so adjust accordingly to what you have available on your machine. 3) Now try running the docker container for DeepVariant again. . 4) After you are done, then run `colima stop` to change Docker back to its default configurations.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:407,energy efficiency,cpu,cpu-type,407,"@heznanda Try the following to see if it fixes that requirement:. 1) Install `qemu` and `colima` like this -- colima will temporarily change the Docker runtime context to colima (see below):. ```. brew install qemu. brew install colima. ```. 2) Enable colima to be your runtime for Docker via `colima start` like this with this configuration:. ```. colima start --arch x86_64 --cpu 2 --memory 2 --disk 12 --cpu-type Broadwell-v4. ```. The above settings are in gigabytes, so adjust accordingly to what you have available on your machine. 3) Now try running the docker container for DeepVariant again. . 4) After you are done, then run `colima stop` to change Docker back to its default configurations.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:328,integrability,configur,configuration,328,"@heznanda Try the following to see if it fixes that requirement:. 1) Install `qemu` and `colima` like this -- colima will temporarily change the Docker runtime context to colima (see below):. ```. brew install qemu. brew install colima. ```. 2) Enable colima to be your runtime for Docker via `colima start` like this with this configuration:. ```. colima start --arch x86_64 --cpu 2 --memory 2 --disk 12 --cpu-type Broadwell-v4. ```. The above settings are in gigabytes, so adjust accordingly to what you have available on your machine. 3) Now try running the docker container for DeepVariant again. . 4) After you are done, then run `colima stop` to change Docker back to its default configurations.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:686,integrability,configur,configurations,686,"@heznanda Try the following to see if it fixes that requirement:. 1) Install `qemu` and `colima` like this -- colima will temporarily change the Docker runtime context to colima (see below):. ```. brew install qemu. brew install colima. ```. 2) Enable colima to be your runtime for Docker via `colima start` like this with this configuration:. ```. colima start --arch x86_64 --cpu 2 --memory 2 --disk 12 --cpu-type Broadwell-v4. ```. The above settings are in gigabytes, so adjust accordingly to what you have available on your machine. 3) Now try running the docker container for DeepVariant again. . 4) After you are done, then run `colima stop` to change Docker back to its default configurations.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:328,modifiability,configur,configuration,328,"@heznanda Try the following to see if it fixes that requirement:. 1) Install `qemu` and `colima` like this -- colima will temporarily change the Docker runtime context to colima (see below):. ```. brew install qemu. brew install colima. ```. 2) Enable colima to be your runtime for Docker via `colima start` like this with this configuration:. ```. colima start --arch x86_64 --cpu 2 --memory 2 --disk 12 --cpu-type Broadwell-v4. ```. The above settings are in gigabytes, so adjust accordingly to what you have available on your machine. 3) Now try running the docker container for DeepVariant again. . 4) After you are done, then run `colima stop` to change Docker back to its default configurations.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:686,modifiability,configur,configurations,686,"@heznanda Try the following to see if it fixes that requirement:. 1) Install `qemu` and `colima` like this -- colima will temporarily change the Docker runtime context to colima (see below):. ```. brew install qemu. brew install colima. ```. 2) Enable colima to be your runtime for Docker via `colima start` like this with this configuration:. ```. colima start --arch x86_64 --cpu 2 --memory 2 --disk 12 --cpu-type Broadwell-v4. ```. The above settings are in gigabytes, so adjust accordingly to what you have available on your machine. 3) Now try running the docker container for DeepVariant again. . 4) After you are done, then run `colima stop` to change Docker back to its default configurations.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:378,performance,cpu,cpu,378,"@heznanda Try the following to see if it fixes that requirement:. 1) Install `qemu` and `colima` like this -- colima will temporarily change the Docker runtime context to colima (see below):. ```. brew install qemu. brew install colima. ```. 2) Enable colima to be your runtime for Docker via `colima start` like this with this configuration:. ```. colima start --arch x86_64 --cpu 2 --memory 2 --disk 12 --cpu-type Broadwell-v4. ```. The above settings are in gigabytes, so adjust accordingly to what you have available on your machine. 3) Now try running the docker container for DeepVariant again. . 4) After you are done, then run `colima stop` to change Docker back to its default configurations.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:386,performance,memor,memory,386,"@heznanda Try the following to see if it fixes that requirement:. 1) Install `qemu` and `colima` like this -- colima will temporarily change the Docker runtime context to colima (see below):. ```. brew install qemu. brew install colima. ```. 2) Enable colima to be your runtime for Docker via `colima start` like this with this configuration:. ```. colima start --arch x86_64 --cpu 2 --memory 2 --disk 12 --cpu-type Broadwell-v4. ```. The above settings are in gigabytes, so adjust accordingly to what you have available on your machine. 3) Now try running the docker container for DeepVariant again. . 4) After you are done, then run `colima stop` to change Docker back to its default configurations.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:397,performance,disk,disk,397,"@heznanda Try the following to see if it fixes that requirement:. 1) Install `qemu` and `colima` like this -- colima will temporarily change the Docker runtime context to colima (see below):. ```. brew install qemu. brew install colima. ```. 2) Enable colima to be your runtime for Docker via `colima start` like this with this configuration:. ```. colima start --arch x86_64 --cpu 2 --memory 2 --disk 12 --cpu-type Broadwell-v4. ```. The above settings are in gigabytes, so adjust accordingly to what you have available on your machine. 3) Now try running the docker container for DeepVariant again. . 4) After you are done, then run `colima stop` to change Docker back to its default configurations.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:407,performance,cpu,cpu-type,407,"@heznanda Try the following to see if it fixes that requirement:. 1) Install `qemu` and `colima` like this -- colima will temporarily change the Docker runtime context to colima (see below):. ```. brew install qemu. brew install colima. ```. 2) Enable colima to be your runtime for Docker via `colima start` like this with this configuration:. ```. colima start --arch x86_64 --cpu 2 --memory 2 --disk 12 --cpu-type Broadwell-v4. ```. The above settings are in gigabytes, so adjust accordingly to what you have available on your machine. 3) Now try running the docker container for DeepVariant again. . 4) After you are done, then run `colima stop` to change Docker back to its default configurations.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:511,reliability,availab,available,511,"@heznanda Try the following to see if it fixes that requirement:. 1) Install `qemu` and `colima` like this -- colima will temporarily change the Docker runtime context to colima (see below):. ```. brew install qemu. brew install colima. ```. 2) Enable colima to be your runtime for Docker via `colima start` like this with this configuration:. ```. colima start --arch x86_64 --cpu 2 --memory 2 --disk 12 --cpu-type Broadwell-v4. ```. The above settings are in gigabytes, so adjust accordingly to what you have available on your machine. 3) Now try running the docker container for DeepVariant again. . 4) After you are done, then run `colima stop` to change Docker back to its default configurations.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:511,safety,avail,available,511,"@heznanda Try the following to see if it fixes that requirement:. 1) Install `qemu` and `colima` like this -- colima will temporarily change the Docker runtime context to colima (see below):. ```. brew install qemu. brew install colima. ```. 2) Enable colima to be your runtime for Docker via `colima start` like this with this configuration:. ```. colima start --arch x86_64 --cpu 2 --memory 2 --disk 12 --cpu-type Broadwell-v4. ```. The above settings are in gigabytes, so adjust accordingly to what you have available on your machine. 3) Now try running the docker container for DeepVariant again. . 4) After you are done, then run `colima stop` to change Docker back to its default configurations.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:328,security,configur,configuration,328,"@heznanda Try the following to see if it fixes that requirement:. 1) Install `qemu` and `colima` like this -- colima will temporarily change the Docker runtime context to colima (see below):. ```. brew install qemu. brew install colima. ```. 2) Enable colima to be your runtime for Docker via `colima start` like this with this configuration:. ```. colima start --arch x86_64 --cpu 2 --memory 2 --disk 12 --cpu-type Broadwell-v4. ```. The above settings are in gigabytes, so adjust accordingly to what you have available on your machine. 3) Now try running the docker container for DeepVariant again. . 4) After you are done, then run `colima stop` to change Docker back to its default configurations.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:511,security,availab,available,511,"@heznanda Try the following to see if it fixes that requirement:. 1) Install `qemu` and `colima` like this -- colima will temporarily change the Docker runtime context to colima (see below):. ```. brew install qemu. brew install colima. ```. 2) Enable colima to be your runtime for Docker via `colima start` like this with this configuration:. ```. colima start --arch x86_64 --cpu 2 --memory 2 --disk 12 --cpu-type Broadwell-v4. ```. The above settings are in gigabytes, so adjust accordingly to what you have available on your machine. 3) Now try running the docker container for DeepVariant again. . 4) After you are done, then run `colima stop` to change Docker back to its default configurations.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:686,security,configur,configurations,686,"@heznanda Try the following to see if it fixes that requirement:. 1) Install `qemu` and `colima` like this -- colima will temporarily change the Docker runtime context to colima (see below):. ```. brew install qemu. brew install colima. ```. 2) Enable colima to be your runtime for Docker via `colima start` like this with this configuration:. ```. colima start --arch x86_64 --cpu 2 --memory 2 --disk 12 --cpu-type Broadwell-v4. ```. The above settings are in gigabytes, so adjust accordingly to what you have available on your machine. 3) Now try running the docker container for DeepVariant again. . 4) After you are done, then run `colima stop` to change Docker back to its default configurations.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:160,testability,context,context,160,"@heznanda Try the following to see if it fixes that requirement:. 1) Install `qemu` and `colima` like this -- colima will temporarily change the Docker runtime context to colima (see below):. ```. brew install qemu. brew install colima. ```. 2) Enable colima to be your runtime for Docker via `colima start` like this with this configuration:. ```. colima start --arch x86_64 --cpu 2 --memory 2 --disk 12 --cpu-type Broadwell-v4. ```. The above settings are in gigabytes, so adjust accordingly to what you have available on your machine. 3) Now try running the docker container for DeepVariant again. . 4) After you are done, then run `colima stop` to change Docker back to its default configurations.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:386,usability,memor,memory,386,"@heznanda Try the following to see if it fixes that requirement:. 1) Install `qemu` and `colima` like this -- colima will temporarily change the Docker runtime context to colima (see below):. ```. brew install qemu. brew install colima. ```. 2) Enable colima to be your runtime for Docker via `colima start` like this with this configuration:. ```. colima start --arch x86_64 --cpu 2 --memory 2 --disk 12 --cpu-type Broadwell-v4. ```. The above settings are in gigabytes, so adjust accordingly to what you have available on your machine. 3) Now try running the docker container for DeepVariant again. . 4) After you are done, then run `colima stop` to change Docker back to its default configurations.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:643,usability,stop,stop,643,"@heznanda Try the following to see if it fixes that requirement:. 1) Install `qemu` and `colima` like this -- colima will temporarily change the Docker runtime context to colima (see below):. ```. brew install qemu. brew install colima. ```. 2) Enable colima to be your runtime for Docker via `colima start` like this with this configuration:. ```. colima start --arch x86_64 --cpu 2 --memory 2 --disk 12 --cpu-type Broadwell-v4. ```. The above settings are in gigabytes, so adjust accordingly to what you have available on your machine. 3) Now try running the docker container for DeepVariant again. . 4) After you are done, then run `colima stop` to change Docker back to its default configurations.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:818,availability,Down,Downloaded,818,"@pgrosu Thank you for the reply! I wouldn't have thought to install qemu and colima before. I tried your instructions and added `brew install docker` as well. Successfully started colima. ```. > colima start --arch x86_64 --cpu 4 --memory 8 --disk 20 --cpu-type Broadwell-v4. INFO[0000] starting colima . INFO[0000] runtime: docker . INFO[0000] preparing network ... context=vm. INFO[0000] starting ... context=vm. INFO[0073] provisioning ... context=docker. INFO[0074] starting ... context=docker. INFO[0092] done . ```. The docker run `docker run --platform linux/amd64 google/deepvariant:1.5.0` seems to be incomplete. The first installation, it gave this message `Unable to find image 'google/deepvariant:1.5.0' locally. 1.5.0: Pulling from google/deepvariant` and after the pull complete message and the `Status: Downloaded newer image for google/deepvariant:1.5.0`, it was stucked: . ```. 2023-06-04 16:15:06.784293: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. ```. and finally gave `ERRO[1898] error waiting for container: `. I stopped colima and rerun `docker run --platform linux/amd64 google/deepvariant:1.5.0`. it has been stuck like this for over 30 minutes with the same message:. ```. 2023-06-04 16:36:19.453896: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. ERRO[3173] error waiting for container:. ```. Do you have any suggestions?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:1123,availability,operat,operations,1123,"@pgrosu Thank you for the reply! I wouldn't have thought to install qemu and colima before. I tried your instructions and added `brew install docker` as well. Successfully started colima. ```. > colima start --arch x86_64 --cpu 4 --memory 8 --disk 20 --cpu-type Broadwell-v4. INFO[0000] starting colima . INFO[0000] runtime: docker . INFO[0000] preparing network ... context=vm. INFO[0000] starting ... context=vm. INFO[0073] provisioning ... context=docker. INFO[0074] starting ... context=docker. INFO[0092] done . ```. The docker run `docker run --platform linux/amd64 google/deepvariant:1.5.0` seems to be incomplete. The first installation, it gave this message `Unable to find image 'google/deepvariant:1.5.0' locally. 1.5.0: Pulling from google/deepvariant` and after the pull complete message and the `Status: Downloaded newer image for google/deepvariant:1.5.0`, it was stucked: . ```. 2023-06-04 16:15:06.784293: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. ```. and finally gave `ERRO[1898] error waiting for container: `. I stopped colima and rerun `docker run --platform linux/amd64 google/deepvariant:1.5.0`. it has been stuck like this for over 30 minutes with the same message:. ```. 2023-06-04 16:36:19.453896: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. ERRO[3173] error waiting for container:. ```. Do you have any suggestions?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:1169,availability,operat,operations,1169,"@pgrosu Thank you for the reply! I wouldn't have thought to install qemu and colima before. I tried your instructions and added `brew install docker` as well. Successfully started colima. ```. > colima start --arch x86_64 --cpu 4 --memory 8 --disk 20 --cpu-type Broadwell-v4. INFO[0000] starting colima . INFO[0000] runtime: docker . INFO[0000] preparing network ... context=vm. INFO[0000] starting ... context=vm. INFO[0073] provisioning ... context=docker. INFO[0074] starting ... context=docker. INFO[0092] done . ```. The docker run `docker run --platform linux/amd64 google/deepvariant:1.5.0` seems to be incomplete. The first installation, it gave this message `Unable to find image 'google/deepvariant:1.5.0' locally. 1.5.0: Pulling from google/deepvariant` and after the pull complete message and the `Status: Downloaded newer image for google/deepvariant:1.5.0`, it was stucked: . ```. 2023-06-04 16:15:06.784293: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. ```. and finally gave `ERRO[1898] error waiting for container: `. I stopped colima and rerun `docker run --platform linux/amd64 google/deepvariant:1.5.0`. it has been stuck like this for over 30 minutes with the same message:. ```. 2023-06-04 16:36:19.453896: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. ERRO[3173] error waiting for container:. ```. Do you have any suggestions?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:1271,availability,error,error,1271,"@pgrosu Thank you for the reply! I wouldn't have thought to install qemu and colima before. I tried your instructions and added `brew install docker` as well. Successfully started colima. ```. > colima start --arch x86_64 --cpu 4 --memory 8 --disk 20 --cpu-type Broadwell-v4. INFO[0000] starting colima . INFO[0000] runtime: docker . INFO[0000] preparing network ... context=vm. INFO[0000] starting ... context=vm. INFO[0073] provisioning ... context=docker. INFO[0074] starting ... context=docker. INFO[0092] done . ```. The docker run `docker run --platform linux/amd64 google/deepvariant:1.5.0` seems to be incomplete. The first installation, it gave this message `Unable to find image 'google/deepvariant:1.5.0' locally. 1.5.0: Pulling from google/deepvariant` and after the pull complete message and the `Status: Downloaded newer image for google/deepvariant:1.5.0`, it was stucked: . ```. 2023-06-04 16:15:06.784293: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. ```. and finally gave `ERRO[1898] error waiting for container: `. I stopped colima and rerun `docker run --platform linux/amd64 google/deepvariant:1.5.0`. it has been stuck like this for over 30 minutes with the same message:. ```. 2023-06-04 16:36:19.453896: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. ERRO[3173] error waiting for container:. ```. Do you have any suggestions?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:1697,availability,operat,operations,1697,"@pgrosu Thank you for the reply! I wouldn't have thought to install qemu and colima before. I tried your instructions and added `brew install docker` as well. Successfully started colima. ```. > colima start --arch x86_64 --cpu 4 --memory 8 --disk 20 --cpu-type Broadwell-v4. INFO[0000] starting colima . INFO[0000] runtime: docker . INFO[0000] preparing network ... context=vm. INFO[0000] starting ... context=vm. INFO[0073] provisioning ... context=docker. INFO[0074] starting ... context=docker. INFO[0092] done . ```. The docker run `docker run --platform linux/amd64 google/deepvariant:1.5.0` seems to be incomplete. The first installation, it gave this message `Unable to find image 'google/deepvariant:1.5.0' locally. 1.5.0: Pulling from google/deepvariant` and after the pull complete message and the `Status: Downloaded newer image for google/deepvariant:1.5.0`, it was stucked: . ```. 2023-06-04 16:15:06.784293: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. ```. and finally gave `ERRO[1898] error waiting for container: `. I stopped colima and rerun `docker run --platform linux/amd64 google/deepvariant:1.5.0`. it has been stuck like this for over 30 minutes with the same message:. ```. 2023-06-04 16:36:19.453896: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. ERRO[3173] error waiting for container:. ```. Do you have any suggestions?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:1743,availability,operat,operations,1743,"@pgrosu Thank you for the reply! I wouldn't have thought to install qemu and colima before. I tried your instructions and added `brew install docker` as well. Successfully started colima. ```. > colima start --arch x86_64 --cpu 4 --memory 8 --disk 20 --cpu-type Broadwell-v4. INFO[0000] starting colima . INFO[0000] runtime: docker . INFO[0000] preparing network ... context=vm. INFO[0000] starting ... context=vm. INFO[0073] provisioning ... context=docker. INFO[0074] starting ... context=docker. INFO[0092] done . ```. The docker run `docker run --platform linux/amd64 google/deepvariant:1.5.0` seems to be incomplete. The first installation, it gave this message `Unable to find image 'google/deepvariant:1.5.0' locally. 1.5.0: Pulling from google/deepvariant` and after the pull complete message and the `Status: Downloaded newer image for google/deepvariant:1.5.0`, it was stucked: . ```. 2023-06-04 16:15:06.784293: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. ```. and finally gave `ERRO[1898] error waiting for container: `. I stopped colima and rerun `docker run --platform linux/amd64 google/deepvariant:1.5.0`. it has been stuck like this for over 30 minutes with the same message:. ```. 2023-06-04 16:36:19.453896: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. ERRO[3173] error waiting for container:. ```. Do you have any suggestions?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:1822,availability,error,error,1822,"@pgrosu Thank you for the reply! I wouldn't have thought to install qemu and colima before. I tried your instructions and added `brew install docker` as well. Successfully started colima. ```. > colima start --arch x86_64 --cpu 4 --memory 8 --disk 20 --cpu-type Broadwell-v4. INFO[0000] starting colima . INFO[0000] runtime: docker . INFO[0000] preparing network ... context=vm. INFO[0000] starting ... context=vm. INFO[0073] provisioning ... context=docker. INFO[0074] starting ... context=docker. INFO[0092] done . ```. The docker run `docker run --platform linux/amd64 google/deepvariant:1.5.0` seems to be incomplete. The first installation, it gave this message `Unable to find image 'google/deepvariant:1.5.0' locally. 1.5.0: Pulling from google/deepvariant` and after the pull complete message and the `Status: Downloaded newer image for google/deepvariant:1.5.0`, it was stucked: . ```. 2023-06-04 16:15:06.784293: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. ```. and finally gave `ERRO[1898] error waiting for container: `. I stopped colima and rerun `docker run --platform linux/amd64 google/deepvariant:1.5.0`. it has been stuck like this for over 30 minutes with the same message:. ```. 2023-06-04 16:36:19.453896: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. ERRO[3173] error waiting for container:. ```. Do you have any suggestions?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:60,deployability,instal,install,60,"@pgrosu Thank you for the reply! I wouldn't have thought to install qemu and colima before. I tried your instructions and added `brew install docker` as well. Successfully started colima. ```. > colima start --arch x86_64 --cpu 4 --memory 8 --disk 20 --cpu-type Broadwell-v4. INFO[0000] starting colima . INFO[0000] runtime: docker . INFO[0000] preparing network ... context=vm. INFO[0000] starting ... context=vm. INFO[0073] provisioning ... context=docker. INFO[0074] starting ... context=docker. INFO[0092] done . ```. The docker run `docker run --platform linux/amd64 google/deepvariant:1.5.0` seems to be incomplete. The first installation, it gave this message `Unable to find image 'google/deepvariant:1.5.0' locally. 1.5.0: Pulling from google/deepvariant` and after the pull complete message and the `Status: Downloaded newer image for google/deepvariant:1.5.0`, it was stucked: . ```. 2023-06-04 16:15:06.784293: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. ```. and finally gave `ERRO[1898] error waiting for container: `. I stopped colima and rerun `docker run --platform linux/amd64 google/deepvariant:1.5.0`. it has been stuck like this for over 30 minutes with the same message:. ```. 2023-06-04 16:36:19.453896: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. ERRO[3173] error waiting for container:. ```. Do you have any suggestions?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:134,deployability,instal,install,134,"@pgrosu Thank you for the reply! I wouldn't have thought to install qemu and colima before. I tried your instructions and added `brew install docker` as well. Successfully started colima. ```. > colima start --arch x86_64 --cpu 4 --memory 8 --disk 20 --cpu-type Broadwell-v4. INFO[0000] starting colima . INFO[0000] runtime: docker . INFO[0000] preparing network ... context=vm. INFO[0000] starting ... context=vm. INFO[0073] provisioning ... context=docker. INFO[0074] starting ... context=docker. INFO[0092] done . ```. The docker run `docker run --platform linux/amd64 google/deepvariant:1.5.0` seems to be incomplete. The first installation, it gave this message `Unable to find image 'google/deepvariant:1.5.0' locally. 1.5.0: Pulling from google/deepvariant` and after the pull complete message and the `Status: Downloaded newer image for google/deepvariant:1.5.0`, it was stucked: . ```. 2023-06-04 16:15:06.784293: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. ```. and finally gave `ERRO[1898] error waiting for container: `. I stopped colima and rerun `docker run --platform linux/amd64 google/deepvariant:1.5.0`. it has been stuck like this for over 30 minutes with the same message:. ```. 2023-06-04 16:36:19.453896: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. ERRO[3173] error waiting for container:. ```. Do you have any suggestions?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:426,deployability,provis,provisioning,426,"@pgrosu Thank you for the reply! I wouldn't have thought to install qemu and colima before. I tried your instructions and added `brew install docker` as well. Successfully started colima. ```. > colima start --arch x86_64 --cpu 4 --memory 8 --disk 20 --cpu-type Broadwell-v4. INFO[0000] starting colima . INFO[0000] runtime: docker . INFO[0000] preparing network ... context=vm. INFO[0000] starting ... context=vm. INFO[0073] provisioning ... context=docker. INFO[0074] starting ... context=docker. INFO[0092] done . ```. The docker run `docker run --platform linux/amd64 google/deepvariant:1.5.0` seems to be incomplete. The first installation, it gave this message `Unable to find image 'google/deepvariant:1.5.0' locally. 1.5.0: Pulling from google/deepvariant` and after the pull complete message and the `Status: Downloaded newer image for google/deepvariant:1.5.0`, it was stucked: . ```. 2023-06-04 16:15:06.784293: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. ```. and finally gave `ERRO[1898] error waiting for container: `. I stopped colima and rerun `docker run --platform linux/amd64 google/deepvariant:1.5.0`. it has been stuck like this for over 30 minutes with the same message:. ```. 2023-06-04 16:36:19.453896: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. ERRO[3173] error waiting for container:. ```. Do you have any suggestions?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:632,deployability,instal,installation,632,"@pgrosu Thank you for the reply! I wouldn't have thought to install qemu and colima before. I tried your instructions and added `brew install docker` as well. Successfully started colima. ```. > colima start --arch x86_64 --cpu 4 --memory 8 --disk 20 --cpu-type Broadwell-v4. INFO[0000] starting colima . INFO[0000] runtime: docker . INFO[0000] preparing network ... context=vm. INFO[0000] starting ... context=vm. INFO[0073] provisioning ... context=docker. INFO[0074] starting ... context=docker. INFO[0092] done . ```. The docker run `docker run --platform linux/amd64 google/deepvariant:1.5.0` seems to be incomplete. The first installation, it gave this message `Unable to find image 'google/deepvariant:1.5.0' locally. 1.5.0: Pulling from google/deepvariant` and after the pull complete message and the `Status: Downloaded newer image for google/deepvariant:1.5.0`, it was stucked: . ```. 2023-06-04 16:15:06.784293: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. ```. and finally gave `ERRO[1898] error waiting for container: `. I stopped colima and rerun `docker run --platform linux/amd64 google/deepvariant:1.5.0`. it has been stuck like this for over 30 minutes with the same message:. ```. 2023-06-04 16:36:19.453896: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. ERRO[3173] error waiting for container:. ```. Do you have any suggestions?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:1289,deployability,contain,container,1289,"@pgrosu Thank you for the reply! I wouldn't have thought to install qemu and colima before. I tried your instructions and added `brew install docker` as well. Successfully started colima. ```. > colima start --arch x86_64 --cpu 4 --memory 8 --disk 20 --cpu-type Broadwell-v4. INFO[0000] starting colima . INFO[0000] runtime: docker . INFO[0000] preparing network ... context=vm. INFO[0000] starting ... context=vm. INFO[0073] provisioning ... context=docker. INFO[0074] starting ... context=docker. INFO[0092] done . ```. The docker run `docker run --platform linux/amd64 google/deepvariant:1.5.0` seems to be incomplete. The first installation, it gave this message `Unable to find image 'google/deepvariant:1.5.0' locally. 1.5.0: Pulling from google/deepvariant` and after the pull complete message and the `Status: Downloaded newer image for google/deepvariant:1.5.0`, it was stucked: . ```. 2023-06-04 16:15:06.784293: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. ```. and finally gave `ERRO[1898] error waiting for container: `. I stopped colima and rerun `docker run --platform linux/amd64 google/deepvariant:1.5.0`. it has been stuck like this for over 30 minutes with the same message:. ```. 2023-06-04 16:36:19.453896: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. ERRO[3173] error waiting for container:. ```. Do you have any suggestions?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:1840,deployability,contain,container,1840,"@pgrosu Thank you for the reply! I wouldn't have thought to install qemu and colima before. I tried your instructions and added `brew install docker` as well. Successfully started colima. ```. > colima start --arch x86_64 --cpu 4 --memory 8 --disk 20 --cpu-type Broadwell-v4. INFO[0000] starting colima . INFO[0000] runtime: docker . INFO[0000] preparing network ... context=vm. INFO[0000] starting ... context=vm. INFO[0073] provisioning ... context=docker. INFO[0074] starting ... context=docker. INFO[0092] done . ```. The docker run `docker run --platform linux/amd64 google/deepvariant:1.5.0` seems to be incomplete. The first installation, it gave this message `Unable to find image 'google/deepvariant:1.5.0' locally. 1.5.0: Pulling from google/deepvariant` and after the pull complete message and the `Status: Downloaded newer image for google/deepvariant:1.5.0`, it was stucked: . ```. 2023-06-04 16:15:06.784293: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. ```. and finally gave `ERRO[1898] error waiting for container: `. I stopped colima and rerun `docker run --platform linux/amd64 google/deepvariant:1.5.0`. it has been stuck like this for over 30 minutes with the same message:. ```. 2023-06-04 16:36:19.453896: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. ERRO[3173] error waiting for container:. ```. Do you have any suggestions?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:224,energy efficiency,cpu,cpu,224,"@pgrosu Thank you for the reply! I wouldn't have thought to install qemu and colima before. I tried your instructions and added `brew install docker` as well. Successfully started colima. ```. > colima start --arch x86_64 --cpu 4 --memory 8 --disk 20 --cpu-type Broadwell-v4. INFO[0000] starting colima . INFO[0000] runtime: docker . INFO[0000] preparing network ... context=vm. INFO[0000] starting ... context=vm. INFO[0073] provisioning ... context=docker. INFO[0074] starting ... context=docker. INFO[0092] done . ```. The docker run `docker run --platform linux/amd64 google/deepvariant:1.5.0` seems to be incomplete. The first installation, it gave this message `Unable to find image 'google/deepvariant:1.5.0' locally. 1.5.0: Pulling from google/deepvariant` and after the pull complete message and the `Status: Downloaded newer image for google/deepvariant:1.5.0`, it was stucked: . ```. 2023-06-04 16:15:06.784293: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. ```. and finally gave `ERRO[1898] error waiting for container: `. I stopped colima and rerun `docker run --platform linux/amd64 google/deepvariant:1.5.0`. it has been stuck like this for over 30 minutes with the same message:. ```. 2023-06-04 16:36:19.453896: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. ERRO[3173] error waiting for container:. ```. Do you have any suggestions?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:253,energy efficiency,cpu,cpu-type,253,"@pgrosu Thank you for the reply! I wouldn't have thought to install qemu and colima before. I tried your instructions and added `brew install docker` as well. Successfully started colima. ```. > colima start --arch x86_64 --cpu 4 --memory 8 --disk 20 --cpu-type Broadwell-v4. INFO[0000] starting colima . INFO[0000] runtime: docker . INFO[0000] preparing network ... context=vm. INFO[0000] starting ... context=vm. INFO[0073] provisioning ... context=docker. INFO[0074] starting ... context=docker. INFO[0092] done . ```. The docker run `docker run --platform linux/amd64 google/deepvariant:1.5.0` seems to be incomplete. The first installation, it gave this message `Unable to find image 'google/deepvariant:1.5.0' locally. 1.5.0: Pulling from google/deepvariant` and after the pull complete message and the `Status: Downloaded newer image for google/deepvariant:1.5.0`, it was stucked: . ```. 2023-06-04 16:15:06.784293: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. ```. and finally gave `ERRO[1898] error waiting for container: `. I stopped colima and rerun `docker run --platform linux/amd64 google/deepvariant:1.5.0`. it has been stuck like this for over 30 minutes with the same message:. ```. 2023-06-04 16:36:19.453896: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. ERRO[3173] error waiting for container:. ```. Do you have any suggestions?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:936,energy efficiency,core,core,936,"@pgrosu Thank you for the reply! I wouldn't have thought to install qemu and colima before. I tried your instructions and added `brew install docker` as well. Successfully started colima. ```. > colima start --arch x86_64 --cpu 4 --memory 8 --disk 20 --cpu-type Broadwell-v4. INFO[0000] starting colima . INFO[0000] runtime: docker . INFO[0000] preparing network ... context=vm. INFO[0000] starting ... context=vm. INFO[0073] provisioning ... context=docker. INFO[0074] starting ... context=docker. INFO[0092] done . ```. The docker run `docker run --platform linux/amd64 google/deepvariant:1.5.0` seems to be incomplete. The first installation, it gave this message `Unable to find image 'google/deepvariant:1.5.0' locally. 1.5.0: Pulling from google/deepvariant` and after the pull complete message and the `Status: Downloaded newer image for google/deepvariant:1.5.0`, it was stucked: . ```. 2023-06-04 16:15:06.784293: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. ```. and finally gave `ERRO[1898] error waiting for container: `. I stopped colima and rerun `docker run --platform linux/amd64 google/deepvariant:1.5.0`. it has been stuck like this for over 30 minutes with the same message:. ```. 2023-06-04 16:36:19.453896: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. ERRO[3173] error waiting for container:. ```. Do you have any suggestions?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:1002,energy efficiency,optim,optimized,1002,"@pgrosu Thank you for the reply! I wouldn't have thought to install qemu and colima before. I tried your instructions and added `brew install docker` as well. Successfully started colima. ```. > colima start --arch x86_64 --cpu 4 --memory 8 --disk 20 --cpu-type Broadwell-v4. INFO[0000] starting colima . INFO[0000] runtime: docker . INFO[0000] preparing network ... context=vm. INFO[0000] starting ... context=vm. INFO[0073] provisioning ... context=docker. INFO[0074] starting ... context=docker. INFO[0092] done . ```. The docker run `docker run --platform linux/amd64 google/deepvariant:1.5.0` seems to be incomplete. The first installation, it gave this message `Unable to find image 'google/deepvariant:1.5.0' locally. 1.5.0: Pulling from google/deepvariant` and after the pull complete message and the `Status: Downloaded newer image for google/deepvariant:1.5.0`, it was stucked: . ```. 2023-06-04 16:15:06.784293: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. ```. and finally gave `ERRO[1898] error waiting for container: `. I stopped colima and rerun `docker run --platform linux/amd64 google/deepvariant:1.5.0`. it has been stuck like this for over 30 minutes with the same message:. ```. 2023-06-04 16:36:19.453896: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. ERRO[3173] error waiting for container:. ```. Do you have any suggestions?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:1082,energy efficiency,CPU,CPU,1082,"@pgrosu Thank you for the reply! I wouldn't have thought to install qemu and colima before. I tried your instructions and added `brew install docker` as well. Successfully started colima. ```. > colima start --arch x86_64 --cpu 4 --memory 8 --disk 20 --cpu-type Broadwell-v4. INFO[0000] starting colima . INFO[0000] runtime: docker . INFO[0000] preparing network ... context=vm. INFO[0000] starting ... context=vm. INFO[0073] provisioning ... context=docker. INFO[0074] starting ... context=docker. INFO[0092] done . ```. The docker run `docker run --platform linux/amd64 google/deepvariant:1.5.0` seems to be incomplete. The first installation, it gave this message `Unable to find image 'google/deepvariant:1.5.0' locally. 1.5.0: Pulling from google/deepvariant` and after the pull complete message and the `Status: Downloaded newer image for google/deepvariant:1.5.0`, it was stucked: . ```. 2023-06-04 16:15:06.784293: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. ```. and finally gave `ERRO[1898] error waiting for container: `. I stopped colima and rerun `docker run --platform linux/amd64 google/deepvariant:1.5.0`. it has been stuck like this for over 30 minutes with the same message:. ```. 2023-06-04 16:36:19.453896: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. ERRO[3173] error waiting for container:. ```. Do you have any suggestions?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:1510,energy efficiency,core,core,1510,"@pgrosu Thank you for the reply! I wouldn't have thought to install qemu and colima before. I tried your instructions and added `brew install docker` as well. Successfully started colima. ```. > colima start --arch x86_64 --cpu 4 --memory 8 --disk 20 --cpu-type Broadwell-v4. INFO[0000] starting colima . INFO[0000] runtime: docker . INFO[0000] preparing network ... context=vm. INFO[0000] starting ... context=vm. INFO[0073] provisioning ... context=docker. INFO[0074] starting ... context=docker. INFO[0092] done . ```. The docker run `docker run --platform linux/amd64 google/deepvariant:1.5.0` seems to be incomplete. The first installation, it gave this message `Unable to find image 'google/deepvariant:1.5.0' locally. 1.5.0: Pulling from google/deepvariant` and after the pull complete message and the `Status: Downloaded newer image for google/deepvariant:1.5.0`, it was stucked: . ```. 2023-06-04 16:15:06.784293: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. ```. and finally gave `ERRO[1898] error waiting for container: `. I stopped colima and rerun `docker run --platform linux/amd64 google/deepvariant:1.5.0`. it has been stuck like this for over 30 minutes with the same message:. ```. 2023-06-04 16:36:19.453896: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. ERRO[3173] error waiting for container:. ```. Do you have any suggestions?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:1576,energy efficiency,optim,optimized,1576,"@pgrosu Thank you for the reply! I wouldn't have thought to install qemu and colima before. I tried your instructions and added `brew install docker` as well. Successfully started colima. ```. > colima start --arch x86_64 --cpu 4 --memory 8 --disk 20 --cpu-type Broadwell-v4. INFO[0000] starting colima . INFO[0000] runtime: docker . INFO[0000] preparing network ... context=vm. INFO[0000] starting ... context=vm. INFO[0073] provisioning ... context=docker. INFO[0074] starting ... context=docker. INFO[0092] done . ```. The docker run `docker run --platform linux/amd64 google/deepvariant:1.5.0` seems to be incomplete. The first installation, it gave this message `Unable to find image 'google/deepvariant:1.5.0' locally. 1.5.0: Pulling from google/deepvariant` and after the pull complete message and the `Status: Downloaded newer image for google/deepvariant:1.5.0`, it was stucked: . ```. 2023-06-04 16:15:06.784293: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. ```. and finally gave `ERRO[1898] error waiting for container: `. I stopped colima and rerun `docker run --platform linux/amd64 google/deepvariant:1.5.0`. it has been stuck like this for over 30 minutes with the same message:. ```. 2023-06-04 16:36:19.453896: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. ERRO[3173] error waiting for container:. ```. Do you have any suggestions?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:1656,energy efficiency,CPU,CPU,1656,"@pgrosu Thank you for the reply! I wouldn't have thought to install qemu and colima before. I tried your instructions and added `brew install docker` as well. Successfully started colima. ```. > colima start --arch x86_64 --cpu 4 --memory 8 --disk 20 --cpu-type Broadwell-v4. INFO[0000] starting colima . INFO[0000] runtime: docker . INFO[0000] preparing network ... context=vm. INFO[0000] starting ... context=vm. INFO[0073] provisioning ... context=docker. INFO[0074] starting ... context=docker. INFO[0092] done . ```. The docker run `docker run --platform linux/amd64 google/deepvariant:1.5.0` seems to be incomplete. The first installation, it gave this message `Unable to find image 'google/deepvariant:1.5.0' locally. 1.5.0: Pulling from google/deepvariant` and after the pull complete message and the `Status: Downloaded newer image for google/deepvariant:1.5.0`, it was stucked: . ```. 2023-06-04 16:15:06.784293: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. ```. and finally gave `ERRO[1898] error waiting for container: `. I stopped colima and rerun `docker run --platform linux/amd64 google/deepvariant:1.5.0`. it has been stuck like this for over 30 minutes with the same message:. ```. 2023-06-04 16:36:19.453896: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. ERRO[3173] error waiting for container:. ```. Do you have any suggestions?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:659,integrability,messag,message,659,"@pgrosu Thank you for the reply! I wouldn't have thought to install qemu and colima before. I tried your instructions and added `brew install docker` as well. Successfully started colima. ```. > colima start --arch x86_64 --cpu 4 --memory 8 --disk 20 --cpu-type Broadwell-v4. INFO[0000] starting colima . INFO[0000] runtime: docker . INFO[0000] preparing network ... context=vm. INFO[0000] starting ... context=vm. INFO[0073] provisioning ... context=docker. INFO[0074] starting ... context=docker. INFO[0092] done . ```. The docker run `docker run --platform linux/amd64 google/deepvariant:1.5.0` seems to be incomplete. The first installation, it gave this message `Unable to find image 'google/deepvariant:1.5.0' locally. 1.5.0: Pulling from google/deepvariant` and after the pull complete message and the `Status: Downloaded newer image for google/deepvariant:1.5.0`, it was stucked: . ```. 2023-06-04 16:15:06.784293: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. ```. and finally gave `ERRO[1898] error waiting for container: `. I stopped colima and rerun `docker run --platform linux/amd64 google/deepvariant:1.5.0`. it has been stuck like this for over 30 minutes with the same message:. ```. 2023-06-04 16:36:19.453896: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. ERRO[3173] error waiting for container:. ```. Do you have any suggestions?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:793,integrability,messag,message,793,"@pgrosu Thank you for the reply! I wouldn't have thought to install qemu and colima before. I tried your instructions and added `brew install docker` as well. Successfully started colima. ```. > colima start --arch x86_64 --cpu 4 --memory 8 --disk 20 --cpu-type Broadwell-v4. INFO[0000] starting colima . INFO[0000] runtime: docker . INFO[0000] preparing network ... context=vm. INFO[0000] starting ... context=vm. INFO[0073] provisioning ... context=docker. INFO[0074] starting ... context=docker. INFO[0092] done . ```. The docker run `docker run --platform linux/amd64 google/deepvariant:1.5.0` seems to be incomplete. The first installation, it gave this message `Unable to find image 'google/deepvariant:1.5.0' locally. 1.5.0: Pulling from google/deepvariant` and after the pull complete message and the `Status: Downloaded newer image for google/deepvariant:1.5.0`, it was stucked: . ```. 2023-06-04 16:15:06.784293: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. ```. and finally gave `ERRO[1898] error waiting for container: `. I stopped colima and rerun `docker run --platform linux/amd64 google/deepvariant:1.5.0`. it has been stuck like this for over 30 minutes with the same message:. ```. 2023-06-04 16:36:19.453896: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. ERRO[3173] error waiting for container:. ```. Do you have any suggestions?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:1454,integrability,messag,message,1454,"@pgrosu Thank you for the reply! I wouldn't have thought to install qemu and colima before. I tried your instructions and added `brew install docker` as well. Successfully started colima. ```. > colima start --arch x86_64 --cpu 4 --memory 8 --disk 20 --cpu-type Broadwell-v4. INFO[0000] starting colima . INFO[0000] runtime: docker . INFO[0000] preparing network ... context=vm. INFO[0000] starting ... context=vm. INFO[0073] provisioning ... context=docker. INFO[0074] starting ... context=docker. INFO[0092] done . ```. The docker run `docker run --platform linux/amd64 google/deepvariant:1.5.0` seems to be incomplete. The first installation, it gave this message `Unable to find image 'google/deepvariant:1.5.0' locally. 1.5.0: Pulling from google/deepvariant` and after the pull complete message and the `Status: Downloaded newer image for google/deepvariant:1.5.0`, it was stucked: . ```. 2023-06-04 16:15:06.784293: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. ```. and finally gave `ERRO[1898] error waiting for container: `. I stopped colima and rerun `docker run --platform linux/amd64 google/deepvariant:1.5.0`. it has been stuck like this for over 30 minutes with the same message:. ```. 2023-06-04 16:36:19.453896: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. ERRO[3173] error waiting for container:. ```. Do you have any suggestions?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:551,interoperability,platform,platform,551,"@pgrosu Thank you for the reply! I wouldn't have thought to install qemu and colima before. I tried your instructions and added `brew install docker` as well. Successfully started colima. ```. > colima start --arch x86_64 --cpu 4 --memory 8 --disk 20 --cpu-type Broadwell-v4. INFO[0000] starting colima . INFO[0000] runtime: docker . INFO[0000] preparing network ... context=vm. INFO[0000] starting ... context=vm. INFO[0073] provisioning ... context=docker. INFO[0074] starting ... context=docker. INFO[0092] done . ```. The docker run `docker run --platform linux/amd64 google/deepvariant:1.5.0` seems to be incomplete. The first installation, it gave this message `Unable to find image 'google/deepvariant:1.5.0' locally. 1.5.0: Pulling from google/deepvariant` and after the pull complete message and the `Status: Downloaded newer image for google/deepvariant:1.5.0`, it was stucked: . ```. 2023-06-04 16:15:06.784293: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. ```. and finally gave `ERRO[1898] error waiting for container: `. I stopped colima and rerun `docker run --platform linux/amd64 google/deepvariant:1.5.0`. it has been stuck like this for over 30 minutes with the same message:. ```. 2023-06-04 16:36:19.453896: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. ERRO[3173] error waiting for container:. ```. Do you have any suggestions?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:659,interoperability,messag,message,659,"@pgrosu Thank you for the reply! I wouldn't have thought to install qemu and colima before. I tried your instructions and added `brew install docker` as well. Successfully started colima. ```. > colima start --arch x86_64 --cpu 4 --memory 8 --disk 20 --cpu-type Broadwell-v4. INFO[0000] starting colima . INFO[0000] runtime: docker . INFO[0000] preparing network ... context=vm. INFO[0000] starting ... context=vm. INFO[0073] provisioning ... context=docker. INFO[0074] starting ... context=docker. INFO[0092] done . ```. The docker run `docker run --platform linux/amd64 google/deepvariant:1.5.0` seems to be incomplete. The first installation, it gave this message `Unable to find image 'google/deepvariant:1.5.0' locally. 1.5.0: Pulling from google/deepvariant` and after the pull complete message and the `Status: Downloaded newer image for google/deepvariant:1.5.0`, it was stucked: . ```. 2023-06-04 16:15:06.784293: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. ```. and finally gave `ERRO[1898] error waiting for container: `. I stopped colima and rerun `docker run --platform linux/amd64 google/deepvariant:1.5.0`. it has been stuck like this for over 30 minutes with the same message:. ```. 2023-06-04 16:36:19.453896: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. ERRO[3173] error waiting for container:. ```. Do you have any suggestions?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:793,interoperability,messag,message,793,"@pgrosu Thank you for the reply! I wouldn't have thought to install qemu and colima before. I tried your instructions and added `brew install docker` as well. Successfully started colima. ```. > colima start --arch x86_64 --cpu 4 --memory 8 --disk 20 --cpu-type Broadwell-v4. INFO[0000] starting colima . INFO[0000] runtime: docker . INFO[0000] preparing network ... context=vm. INFO[0000] starting ... context=vm. INFO[0073] provisioning ... context=docker. INFO[0074] starting ... context=docker. INFO[0092] done . ```. The docker run `docker run --platform linux/amd64 google/deepvariant:1.5.0` seems to be incomplete. The first installation, it gave this message `Unable to find image 'google/deepvariant:1.5.0' locally. 1.5.0: Pulling from google/deepvariant` and after the pull complete message and the `Status: Downloaded newer image for google/deepvariant:1.5.0`, it was stucked: . ```. 2023-06-04 16:15:06.784293: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. ```. and finally gave `ERRO[1898] error waiting for container: `. I stopped colima and rerun `docker run --platform linux/amd64 google/deepvariant:1.5.0`. it has been stuck like this for over 30 minutes with the same message:. ```. 2023-06-04 16:36:19.453896: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. ERRO[3173] error waiting for container:. ```. Do you have any suggestions?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:941,interoperability,platform,platform,941,"@pgrosu Thank you for the reply! I wouldn't have thought to install qemu and colima before. I tried your instructions and added `brew install docker` as well. Successfully started colima. ```. > colima start --arch x86_64 --cpu 4 --memory 8 --disk 20 --cpu-type Broadwell-v4. INFO[0000] starting colima . INFO[0000] runtime: docker . INFO[0000] preparing network ... context=vm. INFO[0000] starting ... context=vm. INFO[0073] provisioning ... context=docker. INFO[0074] starting ... context=docker. INFO[0092] done . ```. The docker run `docker run --platform linux/amd64 google/deepvariant:1.5.0` seems to be incomplete. The first installation, it gave this message `Unable to find image 'google/deepvariant:1.5.0' locally. 1.5.0: Pulling from google/deepvariant` and after the pull complete message and the `Status: Downloaded newer image for google/deepvariant:1.5.0`, it was stucked: . ```. 2023-06-04 16:15:06.784293: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. ```. and finally gave `ERRO[1898] error waiting for container: `. I stopped colima and rerun `docker run --platform linux/amd64 google/deepvariant:1.5.0`. it has been stuck like this for over 30 minutes with the same message:. ```. 2023-06-04 16:36:19.453896: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. ERRO[3173] error waiting for container:. ```. Do you have any suggestions?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:1344,interoperability,platform,platform,1344,"@pgrosu Thank you for the reply! I wouldn't have thought to install qemu and colima before. I tried your instructions and added `brew install docker` as well. Successfully started colima. ```. > colima start --arch x86_64 --cpu 4 --memory 8 --disk 20 --cpu-type Broadwell-v4. INFO[0000] starting colima . INFO[0000] runtime: docker . INFO[0000] preparing network ... context=vm. INFO[0000] starting ... context=vm. INFO[0073] provisioning ... context=docker. INFO[0074] starting ... context=docker. INFO[0092] done . ```. The docker run `docker run --platform linux/amd64 google/deepvariant:1.5.0` seems to be incomplete. The first installation, it gave this message `Unable to find image 'google/deepvariant:1.5.0' locally. 1.5.0: Pulling from google/deepvariant` and after the pull complete message and the `Status: Downloaded newer image for google/deepvariant:1.5.0`, it was stucked: . ```. 2023-06-04 16:15:06.784293: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. ```. and finally gave `ERRO[1898] error waiting for container: `. I stopped colima and rerun `docker run --platform linux/amd64 google/deepvariant:1.5.0`. it has been stuck like this for over 30 minutes with the same message:. ```. 2023-06-04 16:36:19.453896: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. ERRO[3173] error waiting for container:. ```. Do you have any suggestions?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:1454,interoperability,messag,message,1454,"@pgrosu Thank you for the reply! I wouldn't have thought to install qemu and colima before. I tried your instructions and added `brew install docker` as well. Successfully started colima. ```. > colima start --arch x86_64 --cpu 4 --memory 8 --disk 20 --cpu-type Broadwell-v4. INFO[0000] starting colima . INFO[0000] runtime: docker . INFO[0000] preparing network ... context=vm. INFO[0000] starting ... context=vm. INFO[0073] provisioning ... context=docker. INFO[0074] starting ... context=docker. INFO[0092] done . ```. The docker run `docker run --platform linux/amd64 google/deepvariant:1.5.0` seems to be incomplete. The first installation, it gave this message `Unable to find image 'google/deepvariant:1.5.0' locally. 1.5.0: Pulling from google/deepvariant` and after the pull complete message and the `Status: Downloaded newer image for google/deepvariant:1.5.0`, it was stucked: . ```. 2023-06-04 16:15:06.784293: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. ```. and finally gave `ERRO[1898] error waiting for container: `. I stopped colima and rerun `docker run --platform linux/amd64 google/deepvariant:1.5.0`. it has been stuck like this for over 30 minutes with the same message:. ```. 2023-06-04 16:36:19.453896: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. ERRO[3173] error waiting for container:. ```. Do you have any suggestions?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:1515,interoperability,platform,platform,1515,"@pgrosu Thank you for the reply! I wouldn't have thought to install qemu and colima before. I tried your instructions and added `brew install docker` as well. Successfully started colima. ```. > colima start --arch x86_64 --cpu 4 --memory 8 --disk 20 --cpu-type Broadwell-v4. INFO[0000] starting colima . INFO[0000] runtime: docker . INFO[0000] preparing network ... context=vm. INFO[0000] starting ... context=vm. INFO[0073] provisioning ... context=docker. INFO[0074] starting ... context=docker. INFO[0092] done . ```. The docker run `docker run --platform linux/amd64 google/deepvariant:1.5.0` seems to be incomplete. The first installation, it gave this message `Unable to find image 'google/deepvariant:1.5.0' locally. 1.5.0: Pulling from google/deepvariant` and after the pull complete message and the `Status: Downloaded newer image for google/deepvariant:1.5.0`, it was stucked: . ```. 2023-06-04 16:15:06.784293: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. ```. and finally gave `ERRO[1898] error waiting for container: `. I stopped colima and rerun `docker run --platform linux/amd64 google/deepvariant:1.5.0`. it has been stuck like this for over 30 minutes with the same message:. ```. 2023-06-04 16:36:19.453896: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. ERRO[3173] error waiting for container:. ```. Do you have any suggestions?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:224,performance,cpu,cpu,224,"@pgrosu Thank you for the reply! I wouldn't have thought to install qemu and colima before. I tried your instructions and added `brew install docker` as well. Successfully started colima. ```. > colima start --arch x86_64 --cpu 4 --memory 8 --disk 20 --cpu-type Broadwell-v4. INFO[0000] starting colima . INFO[0000] runtime: docker . INFO[0000] preparing network ... context=vm. INFO[0000] starting ... context=vm. INFO[0073] provisioning ... context=docker. INFO[0074] starting ... context=docker. INFO[0092] done . ```. The docker run `docker run --platform linux/amd64 google/deepvariant:1.5.0` seems to be incomplete. The first installation, it gave this message `Unable to find image 'google/deepvariant:1.5.0' locally. 1.5.0: Pulling from google/deepvariant` and after the pull complete message and the `Status: Downloaded newer image for google/deepvariant:1.5.0`, it was stucked: . ```. 2023-06-04 16:15:06.784293: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. ```. and finally gave `ERRO[1898] error waiting for container: `. I stopped colima and rerun `docker run --platform linux/amd64 google/deepvariant:1.5.0`. it has been stuck like this for over 30 minutes with the same message:. ```. 2023-06-04 16:36:19.453896: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. ERRO[3173] error waiting for container:. ```. Do you have any suggestions?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:232,performance,memor,memory,232,"@pgrosu Thank you for the reply! I wouldn't have thought to install qemu and colima before. I tried your instructions and added `brew install docker` as well. Successfully started colima. ```. > colima start --arch x86_64 --cpu 4 --memory 8 --disk 20 --cpu-type Broadwell-v4. INFO[0000] starting colima . INFO[0000] runtime: docker . INFO[0000] preparing network ... context=vm. INFO[0000] starting ... context=vm. INFO[0073] provisioning ... context=docker. INFO[0074] starting ... context=docker. INFO[0092] done . ```. The docker run `docker run --platform linux/amd64 google/deepvariant:1.5.0` seems to be incomplete. The first installation, it gave this message `Unable to find image 'google/deepvariant:1.5.0' locally. 1.5.0: Pulling from google/deepvariant` and after the pull complete message and the `Status: Downloaded newer image for google/deepvariant:1.5.0`, it was stucked: . ```. 2023-06-04 16:15:06.784293: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. ```. and finally gave `ERRO[1898] error waiting for container: `. I stopped colima and rerun `docker run --platform linux/amd64 google/deepvariant:1.5.0`. it has been stuck like this for over 30 minutes with the same message:. ```. 2023-06-04 16:36:19.453896: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. ERRO[3173] error waiting for container:. ```. Do you have any suggestions?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:243,performance,disk,disk,243,"@pgrosu Thank you for the reply! I wouldn't have thought to install qemu and colima before. I tried your instructions and added `brew install docker` as well. Successfully started colima. ```. > colima start --arch x86_64 --cpu 4 --memory 8 --disk 20 --cpu-type Broadwell-v4. INFO[0000] starting colima . INFO[0000] runtime: docker . INFO[0000] preparing network ... context=vm. INFO[0000] starting ... context=vm. INFO[0073] provisioning ... context=docker. INFO[0074] starting ... context=docker. INFO[0092] done . ```. The docker run `docker run --platform linux/amd64 google/deepvariant:1.5.0` seems to be incomplete. The first installation, it gave this message `Unable to find image 'google/deepvariant:1.5.0' locally. 1.5.0: Pulling from google/deepvariant` and after the pull complete message and the `Status: Downloaded newer image for google/deepvariant:1.5.0`, it was stucked: . ```. 2023-06-04 16:15:06.784293: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. ```. and finally gave `ERRO[1898] error waiting for container: `. I stopped colima and rerun `docker run --platform linux/amd64 google/deepvariant:1.5.0`. it has been stuck like this for over 30 minutes with the same message:. ```. 2023-06-04 16:36:19.453896: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. ERRO[3173] error waiting for container:. ```. Do you have any suggestions?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:253,performance,cpu,cpu-type,253,"@pgrosu Thank you for the reply! I wouldn't have thought to install qemu and colima before. I tried your instructions and added `brew install docker` as well. Successfully started colima. ```. > colima start --arch x86_64 --cpu 4 --memory 8 --disk 20 --cpu-type Broadwell-v4. INFO[0000] starting colima . INFO[0000] runtime: docker . INFO[0000] preparing network ... context=vm. INFO[0000] starting ... context=vm. INFO[0073] provisioning ... context=docker. INFO[0074] starting ... context=docker. INFO[0092] done . ```. The docker run `docker run --platform linux/amd64 google/deepvariant:1.5.0` seems to be incomplete. The first installation, it gave this message `Unable to find image 'google/deepvariant:1.5.0' locally. 1.5.0: Pulling from google/deepvariant` and after the pull complete message and the `Status: Downloaded newer image for google/deepvariant:1.5.0`, it was stucked: . ```. 2023-06-04 16:15:06.784293: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. ```. and finally gave `ERRO[1898] error waiting for container: `. I stopped colima and rerun `docker run --platform linux/amd64 google/deepvariant:1.5.0`. it has been stuck like this for over 30 minutes with the same message:. ```. 2023-06-04 16:36:19.453896: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. ERRO[3173] error waiting for container:. ```. Do you have any suggestions?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:355,performance,network,network,355,"@pgrosu Thank you for the reply! I wouldn't have thought to install qemu and colima before. I tried your instructions and added `brew install docker` as well. Successfully started colima. ```. > colima start --arch x86_64 --cpu 4 --memory 8 --disk 20 --cpu-type Broadwell-v4. INFO[0000] starting colima . INFO[0000] runtime: docker . INFO[0000] preparing network ... context=vm. INFO[0000] starting ... context=vm. INFO[0073] provisioning ... context=docker. INFO[0074] starting ... context=docker. INFO[0092] done . ```. The docker run `docker run --platform linux/amd64 google/deepvariant:1.5.0` seems to be incomplete. The first installation, it gave this message `Unable to find image 'google/deepvariant:1.5.0' locally. 1.5.0: Pulling from google/deepvariant` and after the pull complete message and the `Status: Downloaded newer image for google/deepvariant:1.5.0`, it was stucked: . ```. 2023-06-04 16:15:06.784293: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. ```. and finally gave `ERRO[1898] error waiting for container: `. I stopped colima and rerun `docker run --platform linux/amd64 google/deepvariant:1.5.0`. it has been stuck like this for over 30 minutes with the same message:. ```. 2023-06-04 16:36:19.453896: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. ERRO[3173] error waiting for container:. ```. Do you have any suggestions?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:1002,performance,optimiz,optimized,1002,"@pgrosu Thank you for the reply! I wouldn't have thought to install qemu and colima before. I tried your instructions and added `brew install docker` as well. Successfully started colima. ```. > colima start --arch x86_64 --cpu 4 --memory 8 --disk 20 --cpu-type Broadwell-v4. INFO[0000] starting colima . INFO[0000] runtime: docker . INFO[0000] preparing network ... context=vm. INFO[0000] starting ... context=vm. INFO[0073] provisioning ... context=docker. INFO[0074] starting ... context=docker. INFO[0092] done . ```. The docker run `docker run --platform linux/amd64 google/deepvariant:1.5.0` seems to be incomplete. The first installation, it gave this message `Unable to find image 'google/deepvariant:1.5.0' locally. 1.5.0: Pulling from google/deepvariant` and after the pull complete message and the `Status: Downloaded newer image for google/deepvariant:1.5.0`, it was stucked: . ```. 2023-06-04 16:15:06.784293: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. ```. and finally gave `ERRO[1898] error waiting for container: `. I stopped colima and rerun `docker run --platform linux/amd64 google/deepvariant:1.5.0`. it has been stuck like this for over 30 minutes with the same message:. ```. 2023-06-04 16:36:19.453896: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. ERRO[3173] error waiting for container:. ```. Do you have any suggestions?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:1036,performance,Network,Network,1036,"@pgrosu Thank you for the reply! I wouldn't have thought to install qemu and colima before. I tried your instructions and added `brew install docker` as well. Successfully started colima. ```. > colima start --arch x86_64 --cpu 4 --memory 8 --disk 20 --cpu-type Broadwell-v4. INFO[0000] starting colima . INFO[0000] runtime: docker . INFO[0000] preparing network ... context=vm. INFO[0000] starting ... context=vm. INFO[0073] provisioning ... context=docker. INFO[0074] starting ... context=docker. INFO[0092] done . ```. The docker run `docker run --platform linux/amd64 google/deepvariant:1.5.0` seems to be incomplete. The first installation, it gave this message `Unable to find image 'google/deepvariant:1.5.0' locally. 1.5.0: Pulling from google/deepvariant` and after the pull complete message and the `Status: Downloaded newer image for google/deepvariant:1.5.0`, it was stucked: . ```. 2023-06-04 16:15:06.784293: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. ```. and finally gave `ERRO[1898] error waiting for container: `. I stopped colima and rerun `docker run --platform linux/amd64 google/deepvariant:1.5.0`. it has been stuck like this for over 30 minutes with the same message:. ```. 2023-06-04 16:36:19.453896: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. ERRO[3173] error waiting for container:. ```. Do you have any suggestions?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:1082,performance,CPU,CPU,1082,"@pgrosu Thank you for the reply! I wouldn't have thought to install qemu and colima before. I tried your instructions and added `brew install docker` as well. Successfully started colima. ```. > colima start --arch x86_64 --cpu 4 --memory 8 --disk 20 --cpu-type Broadwell-v4. INFO[0000] starting colima . INFO[0000] runtime: docker . INFO[0000] preparing network ... context=vm. INFO[0000] starting ... context=vm. INFO[0073] provisioning ... context=docker. INFO[0074] starting ... context=docker. INFO[0092] done . ```. The docker run `docker run --platform linux/amd64 google/deepvariant:1.5.0` seems to be incomplete. The first installation, it gave this message `Unable to find image 'google/deepvariant:1.5.0' locally. 1.5.0: Pulling from google/deepvariant` and after the pull complete message and the `Status: Downloaded newer image for google/deepvariant:1.5.0`, it was stucked: . ```. 2023-06-04 16:15:06.784293: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. ```. and finally gave `ERRO[1898] error waiting for container: `. I stopped colima and rerun `docker run --platform linux/amd64 google/deepvariant:1.5.0`. it has been stuck like this for over 30 minutes with the same message:. ```. 2023-06-04 16:36:19.453896: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. ERRO[3173] error waiting for container:. ```. Do you have any suggestions?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:1102,performance,perform,performance-critical,1102,"@pgrosu Thank you for the reply! I wouldn't have thought to install qemu and colima before. I tried your instructions and added `brew install docker` as well. Successfully started colima. ```. > colima start --arch x86_64 --cpu 4 --memory 8 --disk 20 --cpu-type Broadwell-v4. INFO[0000] starting colima . INFO[0000] runtime: docker . INFO[0000] preparing network ... context=vm. INFO[0000] starting ... context=vm. INFO[0073] provisioning ... context=docker. INFO[0074] starting ... context=docker. INFO[0092] done . ```. The docker run `docker run --platform linux/amd64 google/deepvariant:1.5.0` seems to be incomplete. The first installation, it gave this message `Unable to find image 'google/deepvariant:1.5.0' locally. 1.5.0: Pulling from google/deepvariant` and after the pull complete message and the `Status: Downloaded newer image for google/deepvariant:1.5.0`, it was stucked: . ```. 2023-06-04 16:15:06.784293: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. ```. and finally gave `ERRO[1898] error waiting for container: `. I stopped colima and rerun `docker run --platform linux/amd64 google/deepvariant:1.5.0`. it has been stuck like this for over 30 minutes with the same message:. ```. 2023-06-04 16:36:19.453896: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. ERRO[3173] error waiting for container:. ```. Do you have any suggestions?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:1271,performance,error,error,1271,"@pgrosu Thank you for the reply! I wouldn't have thought to install qemu and colima before. I tried your instructions and added `brew install docker` as well. Successfully started colima. ```. > colima start --arch x86_64 --cpu 4 --memory 8 --disk 20 --cpu-type Broadwell-v4. INFO[0000] starting colima . INFO[0000] runtime: docker . INFO[0000] preparing network ... context=vm. INFO[0000] starting ... context=vm. INFO[0073] provisioning ... context=docker. INFO[0074] starting ... context=docker. INFO[0092] done . ```. The docker run `docker run --platform linux/amd64 google/deepvariant:1.5.0` seems to be incomplete. The first installation, it gave this message `Unable to find image 'google/deepvariant:1.5.0' locally. 1.5.0: Pulling from google/deepvariant` and after the pull complete message and the `Status: Downloaded newer image for google/deepvariant:1.5.0`, it was stucked: . ```. 2023-06-04 16:15:06.784293: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. ```. and finally gave `ERRO[1898] error waiting for container: `. I stopped colima and rerun `docker run --platform linux/amd64 google/deepvariant:1.5.0`. it has been stuck like this for over 30 minutes with the same message:. ```. 2023-06-04 16:36:19.453896: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. ERRO[3173] error waiting for container:. ```. Do you have any suggestions?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:1576,performance,optimiz,optimized,1576,"@pgrosu Thank you for the reply! I wouldn't have thought to install qemu and colima before. I tried your instructions and added `brew install docker` as well. Successfully started colima. ```. > colima start --arch x86_64 --cpu 4 --memory 8 --disk 20 --cpu-type Broadwell-v4. INFO[0000] starting colima . INFO[0000] runtime: docker . INFO[0000] preparing network ... context=vm. INFO[0000] starting ... context=vm. INFO[0073] provisioning ... context=docker. INFO[0074] starting ... context=docker. INFO[0092] done . ```. The docker run `docker run --platform linux/amd64 google/deepvariant:1.5.0` seems to be incomplete. The first installation, it gave this message `Unable to find image 'google/deepvariant:1.5.0' locally. 1.5.0: Pulling from google/deepvariant` and after the pull complete message and the `Status: Downloaded newer image for google/deepvariant:1.5.0`, it was stucked: . ```. 2023-06-04 16:15:06.784293: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. ```. and finally gave `ERRO[1898] error waiting for container: `. I stopped colima and rerun `docker run --platform linux/amd64 google/deepvariant:1.5.0`. it has been stuck like this for over 30 minutes with the same message:. ```. 2023-06-04 16:36:19.453896: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. ERRO[3173] error waiting for container:. ```. Do you have any suggestions?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:1610,performance,Network,Network,1610,"@pgrosu Thank you for the reply! I wouldn't have thought to install qemu and colima before. I tried your instructions and added `brew install docker` as well. Successfully started colima. ```. > colima start --arch x86_64 --cpu 4 --memory 8 --disk 20 --cpu-type Broadwell-v4. INFO[0000] starting colima . INFO[0000] runtime: docker . INFO[0000] preparing network ... context=vm. INFO[0000] starting ... context=vm. INFO[0073] provisioning ... context=docker. INFO[0074] starting ... context=docker. INFO[0092] done . ```. The docker run `docker run --platform linux/amd64 google/deepvariant:1.5.0` seems to be incomplete. The first installation, it gave this message `Unable to find image 'google/deepvariant:1.5.0' locally. 1.5.0: Pulling from google/deepvariant` and after the pull complete message and the `Status: Downloaded newer image for google/deepvariant:1.5.0`, it was stucked: . ```. 2023-06-04 16:15:06.784293: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. ```. and finally gave `ERRO[1898] error waiting for container: `. I stopped colima and rerun `docker run --platform linux/amd64 google/deepvariant:1.5.0`. it has been stuck like this for over 30 minutes with the same message:. ```. 2023-06-04 16:36:19.453896: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. ERRO[3173] error waiting for container:. ```. Do you have any suggestions?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:1656,performance,CPU,CPU,1656,"@pgrosu Thank you for the reply! I wouldn't have thought to install qemu and colima before. I tried your instructions and added `brew install docker` as well. Successfully started colima. ```. > colima start --arch x86_64 --cpu 4 --memory 8 --disk 20 --cpu-type Broadwell-v4. INFO[0000] starting colima . INFO[0000] runtime: docker . INFO[0000] preparing network ... context=vm. INFO[0000] starting ... context=vm. INFO[0073] provisioning ... context=docker. INFO[0074] starting ... context=docker. INFO[0092] done . ```. The docker run `docker run --platform linux/amd64 google/deepvariant:1.5.0` seems to be incomplete. The first installation, it gave this message `Unable to find image 'google/deepvariant:1.5.0' locally. 1.5.0: Pulling from google/deepvariant` and after the pull complete message and the `Status: Downloaded newer image for google/deepvariant:1.5.0`, it was stucked: . ```. 2023-06-04 16:15:06.784293: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. ```. and finally gave `ERRO[1898] error waiting for container: `. I stopped colima and rerun `docker run --platform linux/amd64 google/deepvariant:1.5.0`. it has been stuck like this for over 30 minutes with the same message:. ```. 2023-06-04 16:36:19.453896: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. ERRO[3173] error waiting for container:. ```. Do you have any suggestions?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:1676,performance,perform,performance-critical,1676,"@pgrosu Thank you for the reply! I wouldn't have thought to install qemu and colima before. I tried your instructions and added `brew install docker` as well. Successfully started colima. ```. > colima start --arch x86_64 --cpu 4 --memory 8 --disk 20 --cpu-type Broadwell-v4. INFO[0000] starting colima . INFO[0000] runtime: docker . INFO[0000] preparing network ... context=vm. INFO[0000] starting ... context=vm. INFO[0073] provisioning ... context=docker. INFO[0074] starting ... context=docker. INFO[0092] done . ```. The docker run `docker run --platform linux/amd64 google/deepvariant:1.5.0` seems to be incomplete. The first installation, it gave this message `Unable to find image 'google/deepvariant:1.5.0' locally. 1.5.0: Pulling from google/deepvariant` and after the pull complete message and the `Status: Downloaded newer image for google/deepvariant:1.5.0`, it was stucked: . ```. 2023-06-04 16:15:06.784293: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. ```. and finally gave `ERRO[1898] error waiting for container: `. I stopped colima and rerun `docker run --platform linux/amd64 google/deepvariant:1.5.0`. it has been stuck like this for over 30 minutes with the same message:. ```. 2023-06-04 16:36:19.453896: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. ERRO[3173] error waiting for container:. ```. Do you have any suggestions?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:1822,performance,error,error,1822,"@pgrosu Thank you for the reply! I wouldn't have thought to install qemu and colima before. I tried your instructions and added `brew install docker` as well. Successfully started colima. ```. > colima start --arch x86_64 --cpu 4 --memory 8 --disk 20 --cpu-type Broadwell-v4. INFO[0000] starting colima . INFO[0000] runtime: docker . INFO[0000] preparing network ... context=vm. INFO[0000] starting ... context=vm. INFO[0073] provisioning ... context=docker. INFO[0074] starting ... context=docker. INFO[0092] done . ```. The docker run `docker run --platform linux/amd64 google/deepvariant:1.5.0` seems to be incomplete. The first installation, it gave this message `Unable to find image 'google/deepvariant:1.5.0' locally. 1.5.0: Pulling from google/deepvariant` and after the pull complete message and the `Status: Downloaded newer image for google/deepvariant:1.5.0`, it was stucked: . ```. 2023-06-04 16:15:06.784293: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. ```. and finally gave `ERRO[1898] error waiting for container: `. I stopped colima and rerun `docker run --platform linux/amd64 google/deepvariant:1.5.0`. it has been stuck like this for over 30 minutes with the same message:. ```. 2023-06-04 16:36:19.453896: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. ERRO[3173] error waiting for container:. ```. Do you have any suggestions?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:784,safety,compl,complete,784,"@pgrosu Thank you for the reply! I wouldn't have thought to install qemu and colima before. I tried your instructions and added `brew install docker` as well. Successfully started colima. ```. > colima start --arch x86_64 --cpu 4 --memory 8 --disk 20 --cpu-type Broadwell-v4. INFO[0000] starting colima . INFO[0000] runtime: docker . INFO[0000] preparing network ... context=vm. INFO[0000] starting ... context=vm. INFO[0073] provisioning ... context=docker. INFO[0074] starting ... context=docker. INFO[0092] done . ```. The docker run `docker run --platform linux/amd64 google/deepvariant:1.5.0` seems to be incomplete. The first installation, it gave this message `Unable to find image 'google/deepvariant:1.5.0' locally. 1.5.0: Pulling from google/deepvariant` and after the pull complete message and the `Status: Downloaded newer image for google/deepvariant:1.5.0`, it was stucked: . ```. 2023-06-04 16:15:06.784293: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. ```. and finally gave `ERRO[1898] error waiting for container: `. I stopped colima and rerun `docker run --platform linux/amd64 google/deepvariant:1.5.0`. it has been stuck like this for over 30 minutes with the same message:. ```. 2023-06-04 16:36:19.453896: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. ERRO[3173] error waiting for container:. ```. Do you have any suggestions?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:1271,safety,error,error,1271,"@pgrosu Thank you for the reply! I wouldn't have thought to install qemu and colima before. I tried your instructions and added `brew install docker` as well. Successfully started colima. ```. > colima start --arch x86_64 --cpu 4 --memory 8 --disk 20 --cpu-type Broadwell-v4. INFO[0000] starting colima . INFO[0000] runtime: docker . INFO[0000] preparing network ... context=vm. INFO[0000] starting ... context=vm. INFO[0073] provisioning ... context=docker. INFO[0074] starting ... context=docker. INFO[0092] done . ```. The docker run `docker run --platform linux/amd64 google/deepvariant:1.5.0` seems to be incomplete. The first installation, it gave this message `Unable to find image 'google/deepvariant:1.5.0' locally. 1.5.0: Pulling from google/deepvariant` and after the pull complete message and the `Status: Downloaded newer image for google/deepvariant:1.5.0`, it was stucked: . ```. 2023-06-04 16:15:06.784293: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. ```. and finally gave `ERRO[1898] error waiting for container: `. I stopped colima and rerun `docker run --platform linux/amd64 google/deepvariant:1.5.0`. it has been stuck like this for over 30 minutes with the same message:. ```. 2023-06-04 16:36:19.453896: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. ERRO[3173] error waiting for container:. ```. Do you have any suggestions?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:1822,safety,error,error,1822,"@pgrosu Thank you for the reply! I wouldn't have thought to install qemu and colima before. I tried your instructions and added `brew install docker` as well. Successfully started colima. ```. > colima start --arch x86_64 --cpu 4 --memory 8 --disk 20 --cpu-type Broadwell-v4. INFO[0000] starting colima . INFO[0000] runtime: docker . INFO[0000] preparing network ... context=vm. INFO[0000] starting ... context=vm. INFO[0073] provisioning ... context=docker. INFO[0074] starting ... context=docker. INFO[0092] done . ```. The docker run `docker run --platform linux/amd64 google/deepvariant:1.5.0` seems to be incomplete. The first installation, it gave this message `Unable to find image 'google/deepvariant:1.5.0' locally. 1.5.0: Pulling from google/deepvariant` and after the pull complete message and the `Status: Downloaded newer image for google/deepvariant:1.5.0`, it was stucked: . ```. 2023-06-04 16:15:06.784293: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. ```. and finally gave `ERRO[1898] error waiting for container: `. I stopped colima and rerun `docker run --platform linux/amd64 google/deepvariant:1.5.0`. it has been stuck like this for over 30 minutes with the same message:. ```. 2023-06-04 16:36:19.453896: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. ERRO[3173] error waiting for container:. ```. Do you have any suggestions?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:355,security,network,network,355,"@pgrosu Thank you for the reply! I wouldn't have thought to install qemu and colima before. I tried your instructions and added `brew install docker` as well. Successfully started colima. ```. > colima start --arch x86_64 --cpu 4 --memory 8 --disk 20 --cpu-type Broadwell-v4. INFO[0000] starting colima . INFO[0000] runtime: docker . INFO[0000] preparing network ... context=vm. INFO[0000] starting ... context=vm. INFO[0073] provisioning ... context=docker. INFO[0074] starting ... context=docker. INFO[0092] done . ```. The docker run `docker run --platform linux/amd64 google/deepvariant:1.5.0` seems to be incomplete. The first installation, it gave this message `Unable to find image 'google/deepvariant:1.5.0' locally. 1.5.0: Pulling from google/deepvariant` and after the pull complete message and the `Status: Downloaded newer image for google/deepvariant:1.5.0`, it was stucked: . ```. 2023-06-04 16:15:06.784293: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. ```. and finally gave `ERRO[1898] error waiting for container: `. I stopped colima and rerun `docker run --platform linux/amd64 google/deepvariant:1.5.0`. it has been stuck like this for over 30 minutes with the same message:. ```. 2023-06-04 16:36:19.453896: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. ERRO[3173] error waiting for container:. ```. Do you have any suggestions?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:784,security,compl,complete,784,"@pgrosu Thank you for the reply! I wouldn't have thought to install qemu and colima before. I tried your instructions and added `brew install docker` as well. Successfully started colima. ```. > colima start --arch x86_64 --cpu 4 --memory 8 --disk 20 --cpu-type Broadwell-v4. INFO[0000] starting colima . INFO[0000] runtime: docker . INFO[0000] preparing network ... context=vm. INFO[0000] starting ... context=vm. INFO[0073] provisioning ... context=docker. INFO[0074] starting ... context=docker. INFO[0092] done . ```. The docker run `docker run --platform linux/amd64 google/deepvariant:1.5.0` seems to be incomplete. The first installation, it gave this message `Unable to find image 'google/deepvariant:1.5.0' locally. 1.5.0: Pulling from google/deepvariant` and after the pull complete message and the `Status: Downloaded newer image for google/deepvariant:1.5.0`, it was stucked: . ```. 2023-06-04 16:15:06.784293: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. ```. and finally gave `ERRO[1898] error waiting for container: `. I stopped colima and rerun `docker run --platform linux/amd64 google/deepvariant:1.5.0`. it has been stuck like this for over 30 minutes with the same message:. ```. 2023-06-04 16:36:19.453896: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. ERRO[3173] error waiting for container:. ```. Do you have any suggestions?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:1036,security,Network,Network,1036,"@pgrosu Thank you for the reply! I wouldn't have thought to install qemu and colima before. I tried your instructions and added `brew install docker` as well. Successfully started colima. ```. > colima start --arch x86_64 --cpu 4 --memory 8 --disk 20 --cpu-type Broadwell-v4. INFO[0000] starting colima . INFO[0000] runtime: docker . INFO[0000] preparing network ... context=vm. INFO[0000] starting ... context=vm. INFO[0073] provisioning ... context=docker. INFO[0074] starting ... context=docker. INFO[0092] done . ```. The docker run `docker run --platform linux/amd64 google/deepvariant:1.5.0` seems to be incomplete. The first installation, it gave this message `Unable to find image 'google/deepvariant:1.5.0' locally. 1.5.0: Pulling from google/deepvariant` and after the pull complete message and the `Status: Downloaded newer image for google/deepvariant:1.5.0`, it was stucked: . ```. 2023-06-04 16:15:06.784293: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. ```. and finally gave `ERRO[1898] error waiting for container: `. I stopped colima and rerun `docker run --platform linux/amd64 google/deepvariant:1.5.0`. it has been stuck like this for over 30 minutes with the same message:. ```. 2023-06-04 16:36:19.453896: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. ERRO[3173] error waiting for container:. ```. Do you have any suggestions?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:1610,security,Network,Network,1610,"@pgrosu Thank you for the reply! I wouldn't have thought to install qemu and colima before. I tried your instructions and added `brew install docker` as well. Successfully started colima. ```. > colima start --arch x86_64 --cpu 4 --memory 8 --disk 20 --cpu-type Broadwell-v4. INFO[0000] starting colima . INFO[0000] runtime: docker . INFO[0000] preparing network ... context=vm. INFO[0000] starting ... context=vm. INFO[0073] provisioning ... context=docker. INFO[0074] starting ... context=docker. INFO[0092] done . ```. The docker run `docker run --platform linux/amd64 google/deepvariant:1.5.0` seems to be incomplete. The first installation, it gave this message `Unable to find image 'google/deepvariant:1.5.0' locally. 1.5.0: Pulling from google/deepvariant` and after the pull complete message and the `Status: Downloaded newer image for google/deepvariant:1.5.0`, it was stucked: . ```. 2023-06-04 16:15:06.784293: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. ```. and finally gave `ERRO[1898] error waiting for container: `. I stopped colima and rerun `docker run --platform linux/amd64 google/deepvariant:1.5.0`. it has been stuck like this for over 30 minutes with the same message:. ```. 2023-06-04 16:36:19.453896: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. ERRO[3173] error waiting for container:. ```. Do you have any suggestions?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:367,testability,context,context,367,"@pgrosu Thank you for the reply! I wouldn't have thought to install qemu and colima before. I tried your instructions and added `brew install docker` as well. Successfully started colima. ```. > colima start --arch x86_64 --cpu 4 --memory 8 --disk 20 --cpu-type Broadwell-v4. INFO[0000] starting colima . INFO[0000] runtime: docker . INFO[0000] preparing network ... context=vm. INFO[0000] starting ... context=vm. INFO[0073] provisioning ... context=docker. INFO[0074] starting ... context=docker. INFO[0092] done . ```. The docker run `docker run --platform linux/amd64 google/deepvariant:1.5.0` seems to be incomplete. The first installation, it gave this message `Unable to find image 'google/deepvariant:1.5.0' locally. 1.5.0: Pulling from google/deepvariant` and after the pull complete message and the `Status: Downloaded newer image for google/deepvariant:1.5.0`, it was stucked: . ```. 2023-06-04 16:15:06.784293: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. ```. and finally gave `ERRO[1898] error waiting for container: `. I stopped colima and rerun `docker run --platform linux/amd64 google/deepvariant:1.5.0`. it has been stuck like this for over 30 minutes with the same message:. ```. 2023-06-04 16:36:19.453896: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. ERRO[3173] error waiting for container:. ```. Do you have any suggestions?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:403,testability,context,context,403,"@pgrosu Thank you for the reply! I wouldn't have thought to install qemu and colima before. I tried your instructions and added `brew install docker` as well. Successfully started colima. ```. > colima start --arch x86_64 --cpu 4 --memory 8 --disk 20 --cpu-type Broadwell-v4. INFO[0000] starting colima . INFO[0000] runtime: docker . INFO[0000] preparing network ... context=vm. INFO[0000] starting ... context=vm. INFO[0073] provisioning ... context=docker. INFO[0074] starting ... context=docker. INFO[0092] done . ```. The docker run `docker run --platform linux/amd64 google/deepvariant:1.5.0` seems to be incomplete. The first installation, it gave this message `Unable to find image 'google/deepvariant:1.5.0' locally. 1.5.0: Pulling from google/deepvariant` and after the pull complete message and the `Status: Downloaded newer image for google/deepvariant:1.5.0`, it was stucked: . ```. 2023-06-04 16:15:06.784293: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. ```. and finally gave `ERRO[1898] error waiting for container: `. I stopped colima and rerun `docker run --platform linux/amd64 google/deepvariant:1.5.0`. it has been stuck like this for over 30 minutes with the same message:. ```. 2023-06-04 16:36:19.453896: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. ERRO[3173] error waiting for container:. ```. Do you have any suggestions?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:443,testability,context,context,443,"@pgrosu Thank you for the reply! I wouldn't have thought to install qemu and colima before. I tried your instructions and added `brew install docker` as well. Successfully started colima. ```. > colima start --arch x86_64 --cpu 4 --memory 8 --disk 20 --cpu-type Broadwell-v4. INFO[0000] starting colima . INFO[0000] runtime: docker . INFO[0000] preparing network ... context=vm. INFO[0000] starting ... context=vm. INFO[0073] provisioning ... context=docker. INFO[0074] starting ... context=docker. INFO[0092] done . ```. The docker run `docker run --platform linux/amd64 google/deepvariant:1.5.0` seems to be incomplete. The first installation, it gave this message `Unable to find image 'google/deepvariant:1.5.0' locally. 1.5.0: Pulling from google/deepvariant` and after the pull complete message and the `Status: Downloaded newer image for google/deepvariant:1.5.0`, it was stucked: . ```. 2023-06-04 16:15:06.784293: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. ```. and finally gave `ERRO[1898] error waiting for container: `. I stopped colima and rerun `docker run --platform linux/amd64 google/deepvariant:1.5.0`. it has been stuck like this for over 30 minutes with the same message:. ```. 2023-06-04 16:36:19.453896: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. ERRO[3173] error waiting for container:. ```. Do you have any suggestions?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:483,testability,context,context,483,"@pgrosu Thank you for the reply! I wouldn't have thought to install qemu and colima before. I tried your instructions and added `brew install docker` as well. Successfully started colima. ```. > colima start --arch x86_64 --cpu 4 --memory 8 --disk 20 --cpu-type Broadwell-v4. INFO[0000] starting colima . INFO[0000] runtime: docker . INFO[0000] preparing network ... context=vm. INFO[0000] starting ... context=vm. INFO[0073] provisioning ... context=docker. INFO[0074] starting ... context=docker. INFO[0092] done . ```. The docker run `docker run --platform linux/amd64 google/deepvariant:1.5.0` seems to be incomplete. The first installation, it gave this message `Unable to find image 'google/deepvariant:1.5.0' locally. 1.5.0: Pulling from google/deepvariant` and after the pull complete message and the `Status: Downloaded newer image for google/deepvariant:1.5.0`, it was stucked: . ```. 2023-06-04 16:15:06.784293: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. ```. and finally gave `ERRO[1898] error waiting for container: `. I stopped colima and rerun `docker run --platform linux/amd64 google/deepvariant:1.5.0`. it has been stuck like this for over 30 minutes with the same message:. ```. 2023-06-04 16:36:19.453896: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. ERRO[3173] error waiting for container:. ```. Do you have any suggestions?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:232,usability,memor,memory,232,"@pgrosu Thank you for the reply! I wouldn't have thought to install qemu and colima before. I tried your instructions and added `brew install docker` as well. Successfully started colima. ```. > colima start --arch x86_64 --cpu 4 --memory 8 --disk 20 --cpu-type Broadwell-v4. INFO[0000] starting colima . INFO[0000] runtime: docker . INFO[0000] preparing network ... context=vm. INFO[0000] starting ... context=vm. INFO[0073] provisioning ... context=docker. INFO[0074] starting ... context=docker. INFO[0092] done . ```. The docker run `docker run --platform linux/amd64 google/deepvariant:1.5.0` seems to be incomplete. The first installation, it gave this message `Unable to find image 'google/deepvariant:1.5.0' locally. 1.5.0: Pulling from google/deepvariant` and after the pull complete message and the `Status: Downloaded newer image for google/deepvariant:1.5.0`, it was stucked: . ```. 2023-06-04 16:15:06.784293: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. ```. and finally gave `ERRO[1898] error waiting for container: `. I stopped colima and rerun `docker run --platform linux/amd64 google/deepvariant:1.5.0`. it has been stuck like this for over 30 minutes with the same message:. ```. 2023-06-04 16:36:19.453896: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. ERRO[3173] error waiting for container:. ```. Do you have any suggestions?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:810,usability,Statu,Status,810,"@pgrosu Thank you for the reply! I wouldn't have thought to install qemu and colima before. I tried your instructions and added `brew install docker` as well. Successfully started colima. ```. > colima start --arch x86_64 --cpu 4 --memory 8 --disk 20 --cpu-type Broadwell-v4. INFO[0000] starting colima . INFO[0000] runtime: docker . INFO[0000] preparing network ... context=vm. INFO[0000] starting ... context=vm. INFO[0073] provisioning ... context=docker. INFO[0074] starting ... context=docker. INFO[0092] done . ```. The docker run `docker run --platform linux/amd64 google/deepvariant:1.5.0` seems to be incomplete. The first installation, it gave this message `Unable to find image 'google/deepvariant:1.5.0' locally. 1.5.0: Pulling from google/deepvariant` and after the pull complete message and the `Status: Downloaded newer image for google/deepvariant:1.5.0`, it was stucked: . ```. 2023-06-04 16:15:06.784293: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. ```. and finally gave `ERRO[1898] error waiting for container: `. I stopped colima and rerun `docker run --platform linux/amd64 google/deepvariant:1.5.0`. it has been stuck like this for over 30 minutes with the same message:. ```. 2023-06-04 16:36:19.453896: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. ERRO[3173] error waiting for container:. ```. Do you have any suggestions?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:1102,usability,perform,performance-critical,1102,"@pgrosu Thank you for the reply! I wouldn't have thought to install qemu and colima before. I tried your instructions and added `brew install docker` as well. Successfully started colima. ```. > colima start --arch x86_64 --cpu 4 --memory 8 --disk 20 --cpu-type Broadwell-v4. INFO[0000] starting colima . INFO[0000] runtime: docker . INFO[0000] preparing network ... context=vm. INFO[0000] starting ... context=vm. INFO[0073] provisioning ... context=docker. INFO[0074] starting ... context=docker. INFO[0092] done . ```. The docker run `docker run --platform linux/amd64 google/deepvariant:1.5.0` seems to be incomplete. The first installation, it gave this message `Unable to find image 'google/deepvariant:1.5.0' locally. 1.5.0: Pulling from google/deepvariant` and after the pull complete message and the `Status: Downloaded newer image for google/deepvariant:1.5.0`, it was stucked: . ```. 2023-06-04 16:15:06.784293: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. ```. and finally gave `ERRO[1898] error waiting for container: `. I stopped colima and rerun `docker run --platform linux/amd64 google/deepvariant:1.5.0`. it has been stuck like this for over 30 minutes with the same message:. ```. 2023-06-04 16:36:19.453896: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. ERRO[3173] error waiting for container:. ```. Do you have any suggestions?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:1271,usability,error,error,1271,"@pgrosu Thank you for the reply! I wouldn't have thought to install qemu and colima before. I tried your instructions and added `brew install docker` as well. Successfully started colima. ```. > colima start --arch x86_64 --cpu 4 --memory 8 --disk 20 --cpu-type Broadwell-v4. INFO[0000] starting colima . INFO[0000] runtime: docker . INFO[0000] preparing network ... context=vm. INFO[0000] starting ... context=vm. INFO[0073] provisioning ... context=docker. INFO[0074] starting ... context=docker. INFO[0092] done . ```. The docker run `docker run --platform linux/amd64 google/deepvariant:1.5.0` seems to be incomplete. The first installation, it gave this message `Unable to find image 'google/deepvariant:1.5.0' locally. 1.5.0: Pulling from google/deepvariant` and after the pull complete message and the `Status: Downloaded newer image for google/deepvariant:1.5.0`, it was stucked: . ```. 2023-06-04 16:15:06.784293: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. ```. and finally gave `ERRO[1898] error waiting for container: `. I stopped colima and rerun `docker run --platform linux/amd64 google/deepvariant:1.5.0`. it has been stuck like this for over 30 minutes with the same message:. ```. 2023-06-04 16:36:19.453896: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. ERRO[3173] error waiting for container:. ```. Do you have any suggestions?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:1305,usability,stop,stopped,1305,"@pgrosu Thank you for the reply! I wouldn't have thought to install qemu and colima before. I tried your instructions and added `brew install docker` as well. Successfully started colima. ```. > colima start --arch x86_64 --cpu 4 --memory 8 --disk 20 --cpu-type Broadwell-v4. INFO[0000] starting colima . INFO[0000] runtime: docker . INFO[0000] preparing network ... context=vm. INFO[0000] starting ... context=vm. INFO[0073] provisioning ... context=docker. INFO[0074] starting ... context=docker. INFO[0092] done . ```. The docker run `docker run --platform linux/amd64 google/deepvariant:1.5.0` seems to be incomplete. The first installation, it gave this message `Unable to find image 'google/deepvariant:1.5.0' locally. 1.5.0: Pulling from google/deepvariant` and after the pull complete message and the `Status: Downloaded newer image for google/deepvariant:1.5.0`, it was stucked: . ```. 2023-06-04 16:15:06.784293: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. ```. and finally gave `ERRO[1898] error waiting for container: `. I stopped colima and rerun `docker run --platform linux/amd64 google/deepvariant:1.5.0`. it has been stuck like this for over 30 minutes with the same message:. ```. 2023-06-04 16:36:19.453896: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. ERRO[3173] error waiting for container:. ```. Do you have any suggestions?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:1676,usability,perform,performance-critical,1676,"@pgrosu Thank you for the reply! I wouldn't have thought to install qemu and colima before. I tried your instructions and added `brew install docker` as well. Successfully started colima. ```. > colima start --arch x86_64 --cpu 4 --memory 8 --disk 20 --cpu-type Broadwell-v4. INFO[0000] starting colima . INFO[0000] runtime: docker . INFO[0000] preparing network ... context=vm. INFO[0000] starting ... context=vm. INFO[0073] provisioning ... context=docker. INFO[0074] starting ... context=docker. INFO[0092] done . ```. The docker run `docker run --platform linux/amd64 google/deepvariant:1.5.0` seems to be incomplete. The first installation, it gave this message `Unable to find image 'google/deepvariant:1.5.0' locally. 1.5.0: Pulling from google/deepvariant` and after the pull complete message and the `Status: Downloaded newer image for google/deepvariant:1.5.0`, it was stucked: . ```. 2023-06-04 16:15:06.784293: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. ```. and finally gave `ERRO[1898] error waiting for container: `. I stopped colima and rerun `docker run --platform linux/amd64 google/deepvariant:1.5.0`. it has been stuck like this for over 30 minutes with the same message:. ```. 2023-06-04 16:36:19.453896: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. ERRO[3173] error waiting for container:. ```. Do you have any suggestions?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:1822,usability,error,error,1822,"@pgrosu Thank you for the reply! I wouldn't have thought to install qemu and colima before. I tried your instructions and added `brew install docker` as well. Successfully started colima. ```. > colima start --arch x86_64 --cpu 4 --memory 8 --disk 20 --cpu-type Broadwell-v4. INFO[0000] starting colima . INFO[0000] runtime: docker . INFO[0000] preparing network ... context=vm. INFO[0000] starting ... context=vm. INFO[0073] provisioning ... context=docker. INFO[0074] starting ... context=docker. INFO[0092] done . ```. The docker run `docker run --platform linux/amd64 google/deepvariant:1.5.0` seems to be incomplete. The first installation, it gave this message `Unable to find image 'google/deepvariant:1.5.0' locally. 1.5.0: Pulling from google/deepvariant` and after the pull complete message and the `Status: Downloaded newer image for google/deepvariant:1.5.0`, it was stucked: . ```. 2023-06-04 16:15:06.784293: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. ```. and finally gave `ERRO[1898] error waiting for container: `. I stopped colima and rerun `docker run --platform linux/amd64 google/deepvariant:1.5.0`. it has been stuck like this for over 30 minutes with the same message:. ```. 2023-06-04 16:36:19.453896: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. ERRO[3173] error waiting for container:. ```. Do you have any suggestions?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:184,availability,error,error-message,184,"Actually those messages might not be a warning, ""The above is not a warning and is just a point of information."". https://discuss.tensorflow.org/t/tensorflow-with-proper-compiler-flag-error-message/12393/3. Now I will try with the test quickstart run.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:15,integrability,messag,messages,15,"Actually those messages might not be a warning, ""The above is not a warning and is just a point of information."". https://discuss.tensorflow.org/t/tensorflow-with-proper-compiler-flag-error-message/12393/3. Now I will try with the test quickstart run.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:190,integrability,messag,message,190,"Actually those messages might not be a warning, ""The above is not a warning and is just a point of information."". https://discuss.tensorflow.org/t/tensorflow-with-proper-compiler-flag-error-message/12393/3. Now I will try with the test quickstart run.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:15,interoperability,messag,messages,15,"Actually those messages might not be a warning, ""The above is not a warning and is just a point of information."". https://discuss.tensorflow.org/t/tensorflow-with-proper-compiler-flag-error-message/12393/3. Now I will try with the test quickstart run.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:190,interoperability,messag,message,190,"Actually those messages might not be a warning, ""The above is not a warning and is just a point of information."". https://discuss.tensorflow.org/t/tensorflow-with-proper-compiler-flag-error-message/12393/3. Now I will try with the test quickstart run.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:184,performance,error,error-message,184,"Actually those messages might not be a warning, ""The above is not a warning and is just a point of information."". https://discuss.tensorflow.org/t/tensorflow-with-proper-compiler-flag-error-message/12393/3. Now I will try with the test quickstart run.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:184,safety,error,error-message,184,"Actually those messages might not be a warning, ""The above is not a warning and is just a point of information."". https://discuss.tensorflow.org/t/tensorflow-with-proper-compiler-flag-error-message/12393/3. Now I will try with the test quickstart run.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:231,safety,test,test,231,"Actually those messages might not be a warning, ""The above is not a warning and is just a point of information."". https://discuss.tensorflow.org/t/tensorflow-with-proper-compiler-flag-error-message/12393/3. Now I will try with the test quickstart run.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:231,testability,test,test,231,"Actually those messages might not be a warning, ""The above is not a warning and is just a point of information."". https://discuss.tensorflow.org/t/tensorflow-with-proper-compiler-flag-error-message/12393/3. Now I will try with the test quickstart run.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:184,usability,error,error-message,184,"Actually those messages might not be a warning, ""The above is not a warning and is just a point of information."". https://discuss.tensorflow.org/t/tensorflow-with-proper-compiler-flag-error-message/12393/3. Now I will try with the test quickstart run.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:249,deployability,instal,installed,249,"The reason why it is taking so long is because of the emulation component in `qemu`. Docker Desktop performs the qemu emulation of those instructions, when you run it with `--platform`. The reason I wanted to try `colima` is because it will use the installed `qemu` on the Mac rather the one specifically shipped with Docker. The newest `qemu` has the AVX instructions. . How come you don't want to run it on the Cloud directly, as DeepVariant is suboptimal on a laptop with large datasets. Digging through my notes, I noticed I had to write my own customizations to build DeepVariant from scratch for non-AVX platforms, but that's TensorFlow-specific. It's a lot of work. What specific issues are you seeing under Ubuntu when you're trying to install it?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:567,deployability,build,build,567,"The reason why it is taking so long is because of the emulation component in `qemu`. Docker Desktop performs the qemu emulation of those instructions, when you run it with `--platform`. The reason I wanted to try `colima` is because it will use the installed `qemu` on the Mac rather the one specifically shipped with Docker. The newest `qemu` has the AVX instructions. . How come you don't want to run it on the Cloud directly, as DeepVariant is suboptimal on a laptop with large datasets. Digging through my notes, I noticed I had to write my own customizations to build DeepVariant from scratch for non-AVX platforms, but that's TensorFlow-specific. It's a lot of work. What specific issues are you seeing under Ubuntu when you're trying to install it?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:744,deployability,instal,install,744,"The reason why it is taking so long is because of the emulation component in `qemu`. Docker Desktop performs the qemu emulation of those instructions, when you run it with `--platform`. The reason I wanted to try `colima` is because it will use the installed `qemu` on the Mac rather the one specifically shipped with Docker. The newest `qemu` has the AVX instructions. . How come you don't want to run it on the Cloud directly, as DeepVariant is suboptimal on a laptop with large datasets. Digging through my notes, I noticed I had to write my own customizations to build DeepVariant from scratch for non-AVX platforms, but that's TensorFlow-specific. It's a lot of work. What specific issues are you seeing under Ubuntu when you're trying to install it?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:413,energy efficiency,Cloud,Cloud,413,"The reason why it is taking so long is because of the emulation component in `qemu`. Docker Desktop performs the qemu emulation of those instructions, when you run it with `--platform`. The reason I wanted to try `colima` is because it will use the installed `qemu` on the Mac rather the one specifically shipped with Docker. The newest `qemu` has the AVX instructions. . How come you don't want to run it on the Cloud directly, as DeepVariant is suboptimal on a laptop with large datasets. Digging through my notes, I noticed I had to write my own customizations to build DeepVariant from scratch for non-AVX platforms, but that's TensorFlow-specific. It's a lot of work. What specific issues are you seeing under Ubuntu when you're trying to install it?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:64,integrability,compon,component,64,"The reason why it is taking so long is because of the emulation component in `qemu`. Docker Desktop performs the qemu emulation of those instructions, when you run it with `--platform`. The reason I wanted to try `colima` is because it will use the installed `qemu` on the Mac rather the one specifically shipped with Docker. The newest `qemu` has the AVX instructions. . How come you don't want to run it on the Cloud directly, as DeepVariant is suboptimal on a laptop with large datasets. Digging through my notes, I noticed I had to write my own customizations to build DeepVariant from scratch for non-AVX platforms, but that's TensorFlow-specific. It's a lot of work. What specific issues are you seeing under Ubuntu when you're trying to install it?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:447,integrability,sub,suboptimal,447,"The reason why it is taking so long is because of the emulation component in `qemu`. Docker Desktop performs the qemu emulation of those instructions, when you run it with `--platform`. The reason I wanted to try `colima` is because it will use the installed `qemu` on the Mac rather the one specifically shipped with Docker. The newest `qemu` has the AVX instructions. . How come you don't want to run it on the Cloud directly, as DeepVariant is suboptimal on a laptop with large datasets. Digging through my notes, I noticed I had to write my own customizations to build DeepVariant from scratch for non-AVX platforms, but that's TensorFlow-specific. It's a lot of work. What specific issues are you seeing under Ubuntu when you're trying to install it?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:64,interoperability,compon,component,64,"The reason why it is taking so long is because of the emulation component in `qemu`. Docker Desktop performs the qemu emulation of those instructions, when you run it with `--platform`. The reason I wanted to try `colima` is because it will use the installed `qemu` on the Mac rather the one specifically shipped with Docker. The newest `qemu` has the AVX instructions. . How come you don't want to run it on the Cloud directly, as DeepVariant is suboptimal on a laptop with large datasets. Digging through my notes, I noticed I had to write my own customizations to build DeepVariant from scratch for non-AVX platforms, but that's TensorFlow-specific. It's a lot of work. What specific issues are you seeing under Ubuntu when you're trying to install it?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:175,interoperability,platform,platform,175,"The reason why it is taking so long is because of the emulation component in `qemu`. Docker Desktop performs the qemu emulation of those instructions, when you run it with `--platform`. The reason I wanted to try `colima` is because it will use the installed `qemu` on the Mac rather the one specifically shipped with Docker. The newest `qemu` has the AVX instructions. . How come you don't want to run it on the Cloud directly, as DeepVariant is suboptimal on a laptop with large datasets. Digging through my notes, I noticed I had to write my own customizations to build DeepVariant from scratch for non-AVX platforms, but that's TensorFlow-specific. It's a lot of work. What specific issues are you seeing under Ubuntu when you're trying to install it?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:292,interoperability,specif,specifically,292,"The reason why it is taking so long is because of the emulation component in `qemu`. Docker Desktop performs the qemu emulation of those instructions, when you run it with `--platform`. The reason I wanted to try `colima` is because it will use the installed `qemu` on the Mac rather the one specifically shipped with Docker. The newest `qemu` has the AVX instructions. . How come you don't want to run it on the Cloud directly, as DeepVariant is suboptimal on a laptop with large datasets. Digging through my notes, I noticed I had to write my own customizations to build DeepVariant from scratch for non-AVX platforms, but that's TensorFlow-specific. It's a lot of work. What specific issues are you seeing under Ubuntu when you're trying to install it?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:610,interoperability,platform,platforms,610,"The reason why it is taking so long is because of the emulation component in `qemu`. Docker Desktop performs the qemu emulation of those instructions, when you run it with `--platform`. The reason I wanted to try `colima` is because it will use the installed `qemu` on the Mac rather the one specifically shipped with Docker. The newest `qemu` has the AVX instructions. . How come you don't want to run it on the Cloud directly, as DeepVariant is suboptimal on a laptop with large datasets. Digging through my notes, I noticed I had to write my own customizations to build DeepVariant from scratch for non-AVX platforms, but that's TensorFlow-specific. It's a lot of work. What specific issues are you seeing under Ubuntu when you're trying to install it?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:643,interoperability,specif,specific,643,"The reason why it is taking so long is because of the emulation component in `qemu`. Docker Desktop performs the qemu emulation of those instructions, when you run it with `--platform`. The reason I wanted to try `colima` is because it will use the installed `qemu` on the Mac rather the one specifically shipped with Docker. The newest `qemu` has the AVX instructions. . How come you don't want to run it on the Cloud directly, as DeepVariant is suboptimal on a laptop with large datasets. Digging through my notes, I noticed I had to write my own customizations to build DeepVariant from scratch for non-AVX platforms, but that's TensorFlow-specific. It's a lot of work. What specific issues are you seeing under Ubuntu when you're trying to install it?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:678,interoperability,specif,specific,678,"The reason why it is taking so long is because of the emulation component in `qemu`. Docker Desktop performs the qemu emulation of those instructions, when you run it with `--platform`. The reason I wanted to try `colima` is because it will use the installed `qemu` on the Mac rather the one specifically shipped with Docker. The newest `qemu` has the AVX instructions. . How come you don't want to run it on the Cloud directly, as DeepVariant is suboptimal on a laptop with large datasets. Digging through my notes, I noticed I had to write my own customizations to build DeepVariant from scratch for non-AVX platforms, but that's TensorFlow-specific. It's a lot of work. What specific issues are you seeing under Ubuntu when you're trying to install it?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:64,modifiability,compon,component,64,"The reason why it is taking so long is because of the emulation component in `qemu`. Docker Desktop performs the qemu emulation of those instructions, when you run it with `--platform`. The reason I wanted to try `colima` is because it will use the installed `qemu` on the Mac rather the one specifically shipped with Docker. The newest `qemu` has the AVX instructions. . How come you don't want to run it on the Cloud directly, as DeepVariant is suboptimal on a laptop with large datasets. Digging through my notes, I noticed I had to write my own customizations to build DeepVariant from scratch for non-AVX platforms, but that's TensorFlow-specific. It's a lot of work. What specific issues are you seeing under Ubuntu when you're trying to install it?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:100,performance,perform,performs,100,"The reason why it is taking so long is because of the emulation component in `qemu`. Docker Desktop performs the qemu emulation of those instructions, when you run it with `--platform`. The reason I wanted to try `colima` is because it will use the installed `qemu` on the Mac rather the one specifically shipped with Docker. The newest `qemu` has the AVX instructions. . How come you don't want to run it on the Cloud directly, as DeepVariant is suboptimal on a laptop with large datasets. Digging through my notes, I noticed I had to write my own customizations to build DeepVariant from scratch for non-AVX platforms, but that's TensorFlow-specific. It's a lot of work. What specific issues are you seeing under Ubuntu when you're trying to install it?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:54,testability,emul,emulation,54,"The reason why it is taking so long is because of the emulation component in `qemu`. Docker Desktop performs the qemu emulation of those instructions, when you run it with `--platform`. The reason I wanted to try `colima` is because it will use the installed `qemu` on the Mac rather the one specifically shipped with Docker. The newest `qemu` has the AVX instructions. . How come you don't want to run it on the Cloud directly, as DeepVariant is suboptimal on a laptop with large datasets. Digging through my notes, I noticed I had to write my own customizations to build DeepVariant from scratch for non-AVX platforms, but that's TensorFlow-specific. It's a lot of work. What specific issues are you seeing under Ubuntu when you're trying to install it?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:118,testability,emul,emulation,118,"The reason why it is taking so long is because of the emulation component in `qemu`. Docker Desktop performs the qemu emulation of those instructions, when you run it with `--platform`. The reason I wanted to try `colima` is because it will use the installed `qemu` on the Mac rather the one specifically shipped with Docker. The newest `qemu` has the AVX instructions. . How come you don't want to run it on the Cloud directly, as DeepVariant is suboptimal on a laptop with large datasets. Digging through my notes, I noticed I had to write my own customizations to build DeepVariant from scratch for non-AVX platforms, but that's TensorFlow-specific. It's a lot of work. What specific issues are you seeing under Ubuntu when you're trying to install it?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:100,usability,perform,performs,100,"The reason why it is taking so long is because of the emulation component in `qemu`. Docker Desktop performs the qemu emulation of those instructions, when you run it with `--platform`. The reason I wanted to try `colima` is because it will use the installed `qemu` on the Mac rather the one specifically shipped with Docker. The newest `qemu` has the AVX instructions. . How come you don't want to run it on the Cloud directly, as DeepVariant is suboptimal on a laptop with large datasets. Digging through my notes, I noticed I had to write my own customizations to build DeepVariant from scratch for non-AVX platforms, but that's TensorFlow-specific. It's a lot of work. What specific issues are you seeing under Ubuntu when you're trying to install it?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:549,usability,custom,customizations,549,"The reason why it is taking so long is because of the emulation component in `qemu`. Docker Desktop performs the qemu emulation of those instructions, when you run it with `--platform`. The reason I wanted to try `colima` is because it will use the installed `qemu` on the Mac rather the one specifically shipped with Docker. The newest `qemu` has the AVX instructions. . How come you don't want to run it on the Cloud directly, as DeepVariant is suboptimal on a laptop with large datasets. Digging through my notes, I noticed I had to write my own customizations to build DeepVariant from scratch for non-AVX platforms, but that's TensorFlow-specific. It's a lot of work. What specific issues are you seeing under Ubuntu when you're trying to install it?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:231,deployability,instal,installed,231,"@pgrosu Thank you for your response. When you say ""the cloud"" do you mean to run it on a server/a super computer? I predicted that I will need root access (sudo) which I don't have. 1. Is there a way to do this? Docker is not even installed there. 2. So are you suggesting that we should not use mac (apple silicon)? The ubuntu 20.04 that was installed on my mac (using UTM), should this work? Thank you for your time and guidance!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:343,deployability,instal,installed,343,"@pgrosu Thank you for your response. When you say ""the cloud"" do you mean to run it on a server/a super computer? I predicted that I will need root access (sudo) which I don't have. 1. Is there a way to do this? Docker is not even installed there. 2. So are you suggesting that we should not use mac (apple silicon)? The ubuntu 20.04 that was installed on my mac (using UTM), should this work? Thank you for your time and guidance!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:55,energy efficiency,cloud,cloud,55,"@pgrosu Thank you for your response. When you say ""the cloud"" do you mean to run it on a server/a super computer? I predicted that I will need root access (sudo) which I don't have. 1. Is there a way to do this? Docker is not even installed there. 2. So are you suggesting that we should not use mac (apple silicon)? The ubuntu 20.04 that was installed on my mac (using UTM), should this work? Thank you for your time and guidance!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:116,energy efficiency,predict,predicted,116,"@pgrosu Thank you for your response. When you say ""the cloud"" do you mean to run it on a server/a super computer? I predicted that I will need root access (sudo) which I don't have. 1. Is there a way to do this? Docker is not even installed there. 2. So are you suggesting that we should not use mac (apple silicon)? The ubuntu 20.04 that was installed on my mac (using UTM), should this work? Thank you for your time and guidance!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:413,performance,time,time,413,"@pgrosu Thank you for your response. When you say ""the cloud"" do you mean to run it on a server/a super computer? I predicted that I will need root access (sudo) which I don't have. 1. Is there a way to do this? Docker is not even installed there. 2. So are you suggesting that we should not use mac (apple silicon)? The ubuntu 20.04 that was installed on my mac (using UTM), should this work? Thank you for your time and guidance!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:116,safety,predict,predicted,116,"@pgrosu Thank you for your response. When you say ""the cloud"" do you mean to run it on a server/a super computer? I predicted that I will need root access (sudo) which I don't have. 1. Is there a way to do this? Docker is not even installed there. 2. So are you suggesting that we should not use mac (apple silicon)? The ubuntu 20.04 that was installed on my mac (using UTM), should this work? Thank you for your time and guidance!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:148,security,access,access,148,"@pgrosu Thank you for your response. When you say ""the cloud"" do you mean to run it on a server/a super computer? I predicted that I will need root access (sudo) which I don't have. 1. Is there a way to do this? Docker is not even installed there. 2. So are you suggesting that we should not use mac (apple silicon)? The ubuntu 20.04 that was installed on my mac (using UTM), should this work? Thank you for your time and guidance!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:422,usability,guidanc,guidance,422,"@pgrosu Thank you for your response. When you say ""the cloud"" do you mean to run it on a server/a super computer? I predicted that I will need root access (sudo) which I don't have. 1. Is there a way to do this? Docker is not even installed there. 2. So are you suggesting that we should not use mac (apple silicon)? The ubuntu 20.04 that was installed on my mac (using UTM), should this work? Thank you for your time and guidance!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:247,availability,cluster,cluster,247,"1) Cloud has Docker and sudo access, but you have to pay for it for the time you use it (it's basically VM created instances with storage that is pay-per-usage):. https://cloud.google.com/life-sciences/docs/tutorials/deepvariant. 2) If you have a cluster that you have access to, that would be perfect. You can use singularity, or if you don't have Docker there are ways to run Docker is some limited way without root access. 3) Regarding Ubuntu, tell me what you see when you run the following script under your DeepVariant folder (the one git cloned):. ```. #!/bin/bash. source settings.sh. ./run-prereq.sh. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:247,deployability,cluster,cluster,247,"1) Cloud has Docker and sudo access, but you have to pay for it for the time you use it (it's basically VM created instances with storage that is pay-per-usage):. https://cloud.google.com/life-sciences/docs/tutorials/deepvariant. 2) If you have a cluster that you have access to, that would be perfect. You can use singularity, or if you don't have Docker there are ways to run Docker is some limited way without root access. 3) Regarding Ubuntu, tell me what you see when you run the following script under your DeepVariant folder (the one git cloned):. ```. #!/bin/bash. source settings.sh. ./run-prereq.sh. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:3,energy efficiency,Cloud,Cloud,3,"1) Cloud has Docker and sudo access, but you have to pay for it for the time you use it (it's basically VM created instances with storage that is pay-per-usage):. https://cloud.google.com/life-sciences/docs/tutorials/deepvariant. 2) If you have a cluster that you have access to, that would be perfect. You can use singularity, or if you don't have Docker there are ways to run Docker is some limited way without root access. 3) Regarding Ubuntu, tell me what you see when you run the following script under your DeepVariant folder (the one git cloned):. ```. #!/bin/bash. source settings.sh. ./run-prereq.sh. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:171,energy efficiency,cloud,cloud,171,"1) Cloud has Docker and sudo access, but you have to pay for it for the time you use it (it's basically VM created instances with storage that is pay-per-usage):. https://cloud.google.com/life-sciences/docs/tutorials/deepvariant. 2) If you have a cluster that you have access to, that would be perfect. You can use singularity, or if you don't have Docker there are ways to run Docker is some limited way without root access. 3) Regarding Ubuntu, tell me what you see when you run the following script under your DeepVariant folder (the one git cloned):. ```. #!/bin/bash. source settings.sh. ./run-prereq.sh. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:72,performance,time,time,72,"1) Cloud has Docker and sudo access, but you have to pay for it for the time you use it (it's basically VM created instances with storage that is pay-per-usage):. https://cloud.google.com/life-sciences/docs/tutorials/deepvariant. 2) If you have a cluster that you have access to, that would be perfect. You can use singularity, or if you don't have Docker there are ways to run Docker is some limited way without root access. 3) Regarding Ubuntu, tell me what you see when you run the following script under your DeepVariant folder (the one git cloned):. ```. #!/bin/bash. source settings.sh. ./run-prereq.sh. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:29,security,access,access,29,"1) Cloud has Docker and sudo access, but you have to pay for it for the time you use it (it's basically VM created instances with storage that is pay-per-usage):. https://cloud.google.com/life-sciences/docs/tutorials/deepvariant. 2) If you have a cluster that you have access to, that would be perfect. You can use singularity, or if you don't have Docker there are ways to run Docker is some limited way without root access. 3) Regarding Ubuntu, tell me what you see when you run the following script under your DeepVariant folder (the one git cloned):. ```. #!/bin/bash. source settings.sh. ./run-prereq.sh. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:269,security,access,access,269,"1) Cloud has Docker and sudo access, but you have to pay for it for the time you use it (it's basically VM created instances with storage that is pay-per-usage):. https://cloud.google.com/life-sciences/docs/tutorials/deepvariant. 2) If you have a cluster that you have access to, that would be perfect. You can use singularity, or if you don't have Docker there are ways to run Docker is some limited way without root access. 3) Regarding Ubuntu, tell me what you see when you run the following script under your DeepVariant folder (the one git cloned):. ```. #!/bin/bash. source settings.sh. ./run-prereq.sh. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:418,security,access,access,418,"1) Cloud has Docker and sudo access, but you have to pay for it for the time you use it (it's basically VM created instances with storage that is pay-per-usage):. https://cloud.google.com/life-sciences/docs/tutorials/deepvariant. 2) If you have a cluster that you have access to, that would be perfect. You can use singularity, or if you don't have Docker there are ways to run Docker is some limited way without root access. 3) Regarding Ubuntu, tell me what you see when you run the following script under your DeepVariant folder (the one git cloned):. ```. #!/bin/bash. source settings.sh. ./run-prereq.sh. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:2130,availability,ERROR,ERROR,2130," Current. Dload Upload Total Spent Left Speed. 100 2518k 100 2518k 0 0 5671k 0 --:--:-- --:--:-- --:--:-- 5671k. Collecting pip. Using cached pip-23.1.2-py3-none-any.whl (2.1 MB). Installing collected packages: pip. Attempting uninstall: pip. Found existing installation: pip 23.1.2. Uninstalling pip-23.1.2:. Successfully uninstalled pip-23.1.2. WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH. Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location. Successfully installed pip-23.1.2. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Python 3.8.10. pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8). ========== [Sun 04 Jun 2023 11:11:13 PM UTC] Stage 'Install python3 packages' starting. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:2402,availability,ERROR,ERROR,2402,"pip 23.1.2. Uninstalling pip-23.1.2:. Successfully uninstalled pip-23.1.2. WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH. Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location. Successfully installed pip-23.1.2. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Python 3.8.10. pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8). ========== [Sun 04 Jun 2023 11:11:13 PM UTC] Stage 'Install python3 packages' starting. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible. ========== [Sun 04 Jun 2023 11:11:24 PM UTC] Stage 'Install TensorFlow pip package' starting. Installing standard CPU-only TensorFlow 2.11.0 wheel. ERROR: pip's dependency resolver does not currently take into account all the packages that are insta",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:2674,availability,ERROR,ERROR,2674,"script-location. Successfully installed pip-23.1.2. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Python 3.8.10. pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8). ========== [Sun 04 Jun 2023 11:11:13 PM UTC] Stage 'Install python3 packages' starting. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible. ========== [Sun 04 Jun 2023 11:11:24 PM UTC] Stage 'Install TensorFlow pip package' starting. Installing standard CPU-only TensorFlow 2.11.0 wheel. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible. ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install CUDA' starting. ========== [Sun 04 Jun 202",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:3304,availability,ERROR,ERROR,3304,"nsorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible. ========== [Sun 04 Jun 2023 11:11:24 PM UTC] Stage 'Install TensorFlow pip package' starting. Installing standard CPU-only TensorFlow 2.11.0 wheel. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible. ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install CUDA' starting. ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install TensorRT' starting. ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install other packages' starting. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible. googleapis-common-protos 1.59.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5,",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:3817,availability,ERROR,ERROR,3817,"lowing dependency conflicts. tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible. ========== [Sun 04 Jun 2023 11:11:24 PM UTC] Stage 'Install TensorFlow pip package' starting. Installing standard CPU-only TensorFlow 2.11.0 wheel. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible. ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install CUDA' starting. ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install TensorRT' starting. ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install other packages' starting. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible. googleapis-common-protos 1.59.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible. ========== [Sun 04 Jun 2023 11:11:27 PM UTC] Stage 'run-prereq.sh complete' starting. ```. The last time I tried running `./build-prereq.sh`, I got error on the building of Clif. llvm-11-linker-tools not available. and now the error is:. ```. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/loca",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:4508,availability,error,error,4508,"fe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible. ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install CUDA' starting. ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install TensorRT' starting. ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install other packages' starting. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible. googleapis-common-protos 1.59.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible. ========== [Sun 04 Jun 2023 11:11:27 PM UTC] Stage 'run-prereq.sh complete' starting. ```. The last time I tried running `./build-prereq.sh`, I got error on the building of Clif. llvm-11-linker-tools not available. and now the error is:. ```. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:4564,availability,avail,available,4564,"ible. ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install CUDA' starting. ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install TensorRT' starting. ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install other packages' starting. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible. googleapis-common-protos 1.59.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible. ========== [Sun 04 Jun 2023 11:11:27 PM UTC] Stage 'run-prereq.sh complete' starting. ```. The last time I tried running `./build-prereq.sh`, I got error on the building of Clif. llvm-11-linker-tools not available. and now the error is:. ```. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") . -- Checking for module 'protobuf'. -- Found prot",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:4587,availability,error,error,4587," 04 Jun 2023 11:11:26 PM UTC] Stage 'Install CUDA' starting. ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install TensorRT' starting. ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install other packages' starting. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible. googleapis-common-protos 1.59.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible. ========== [Sun 04 Jun 2023 11:11:27 PM UTC] Stage 'run-prereq.sh complete' starting. ```. The last time I tried running `./build-prereq.sh`, I got error on the building of Clif. llvm-11-linker-tools not available. and now the error is:. ```. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") . -- Checking for module 'protobuf'. -- Found protobuf, version 3.13.0.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:4605,availability,echo,echo,4605,"1:26 PM UTC] Stage 'Install CUDA' starting. ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install TensorRT' starting. ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install other packages' starting. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible. googleapis-common-protos 1.59.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible. ========== [Sun 04 Jun 2023 11:11:27 PM UTC] Stage 'run-prereq.sh complete' starting. ```. The last time I tried running `./build-prereq.sh`, I got error on the building of Clif. llvm-11-linker-tools not available. and now the error is:. ```. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") . -- Checking for module 'protobuf'. -- Found protobuf, version 3.13.0. -- Checking for ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:6036,availability,Error,Error,6036,"mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") . -- Checking for module 'protobuf'. -- Found protobuf, version 3.13.0. -- Checking for module 'libglog'. -- Found libglog, version 0.4.0. -- Looking for pthread.h. -- Looking for pthread.h - found. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed. -- Looking for pthread_create in pthreads. -- Looking for pthread_create in pthreads - not found. -- Looking for pthread_create in pthread. -- Looking for pthread_create in pthread - found. -- Found Threads: TRUE . CMake Error at clif/cmake/modules/CLIFUtils.cmake:37 (find_package):. Could not find a configuration file for package ""LLVM"" that is compatible. with requested version ""11.1.0"". The following configuration files were considered but not accepted:. /usr/lib/llvm-11/lib/cmake/llvm/LLVMConfig.cmake, version: 11.0.0. /usr/lib/llvm-11/cmake/LLVMConfig.cmake, version: 11.0.0. /lib/llvm-11/cmake/LLVMConfig.cmake, version: 11.0.0. Call Stack (most recent call first):. clif/CMakeLists.txt:22 (include). -- Configuring incomplete, errors occurred! See also ""/root/clif/build/CMakeFiles/CMakeOutput.log"". See also ""/root/clif/build/CMakeFiles/CMakeError.log"". real	2m44.183s. user	0m18.337s. sys	0m18.865s. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:6555,availability,error,errors,6555,"mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") . -- Checking for module 'protobuf'. -- Found protobuf, version 3.13.0. -- Checking for module 'libglog'. -- Found libglog, version 0.4.0. -- Looking for pthread.h. -- Looking for pthread.h - found. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed. -- Looking for pthread_create in pthreads. -- Looking for pthread_create in pthreads - not found. -- Looking for pthread_create in pthread. -- Looking for pthread_create in pthread - found. -- Found Threads: TRUE . CMake Error at clif/cmake/modules/CLIFUtils.cmake:37 (find_package):. Could not find a configuration file for package ""LLVM"" that is compatible. with requested version ""11.1.0"". The following configuration files were considered but not accepted:. /usr/lib/llvm-11/lib/cmake/llvm/LLVMConfig.cmake, version: 11.0.0. /usr/lib/llvm-11/cmake/LLVMConfig.cmake, version: 11.0.0. /lib/llvm-11/cmake/LLVMConfig.cmake, version: 11.0.0. Call Stack (most recent call first):. clif/CMakeLists.txt:22 (include). -- Configuring incomplete, errors occurred! See also ""/root/clif/build/CMakeFiles/CMakeOutput.log"". See also ""/root/clif/build/CMakeFiles/CMakeError.log"". real	2m44.183s. user	0m18.337s. sys	0m18.865s. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:727,deployability,Stage,Stage,727,"@pgrosu From the choices above, I want to pursue (2) singularity the most. . 2. Is there an instruction that you can point me to for this? 3. I want to show the messages I got from running Ubuntu 20.04 on Mac's UTM. settings.sh has been modified:. ```. export CUDNN_INSTALL_PATH=""/usr/lib/aarch64-linux-gnu"" #instead of /usr/lib/x86_64-linux-gnu. # orig: export DV_USE_GCP_OPTIMIZED_TF_WHL=""${DV_USE_GCP_OPTIMIZED_TF_WHL:-1}"". export DV_USE_GCP_OPTIMIZED_TF_WHL=""0"" #force using CPU only tensorflow. ```. The message when running:. ```. > sudo su. > source settings.sh. > ./run-prereq.sh. ========== This script is only maintained for Ubuntu 20.04. ========== Load config settings. ========== [Sun 04 Jun 2023 11:11:08 PM UTC] Stage 'Misc setup' starting. ========== [Sun 04 Jun 2023 11:11:09 PM UTC] Stage 'Update package list' starting. ========== [Sun 04 Jun 2023 11:11:10 PM UTC] Stage 'run-prereq.sh: Install development packages' starting. Calling wait_for_dpkg_lock. ========== [Sun 04 Jun 2023 11:11:10 PM UTC] Stage 'Install python3 packaging infrastructure' starting. % Total % Received % Xferd Average Speed Time Time Time Current. Dload Upload Total Spent Left Speed. 100 2518k 100 2518k 0 0 5671k 0 --:--:-- --:--:-- --:--:-- 5671k. Collecting pip. Using cached pip-23.1.2-py3-none-any.whl (2.1 MB). Installing collected packages: pip. Attempting uninstall: pip. Found existing installation: pip 23.1.2. Uninstalling pip-23.1.2:. Successfully uninstalled pip-23.1.2. WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH. Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location. Successfully installed pip-23.1.2. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Python 3.8.10. pip 23.1.2 from /root/.local/lib/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:801,deployability,Stage,Stage,801,"@pgrosu From the choices above, I want to pursue (2) singularity the most. . 2. Is there an instruction that you can point me to for this? 3. I want to show the messages I got from running Ubuntu 20.04 on Mac's UTM. settings.sh has been modified:. ```. export CUDNN_INSTALL_PATH=""/usr/lib/aarch64-linux-gnu"" #instead of /usr/lib/x86_64-linux-gnu. # orig: export DV_USE_GCP_OPTIMIZED_TF_WHL=""${DV_USE_GCP_OPTIMIZED_TF_WHL:-1}"". export DV_USE_GCP_OPTIMIZED_TF_WHL=""0"" #force using CPU only tensorflow. ```. The message when running:. ```. > sudo su. > source settings.sh. > ./run-prereq.sh. ========== This script is only maintained for Ubuntu 20.04. ========== Load config settings. ========== [Sun 04 Jun 2023 11:11:08 PM UTC] Stage 'Misc setup' starting. ========== [Sun 04 Jun 2023 11:11:09 PM UTC] Stage 'Update package list' starting. ========== [Sun 04 Jun 2023 11:11:10 PM UTC] Stage 'run-prereq.sh: Install development packages' starting. Calling wait_for_dpkg_lock. ========== [Sun 04 Jun 2023 11:11:10 PM UTC] Stage 'Install python3 packaging infrastructure' starting. % Total % Received % Xferd Average Speed Time Time Time Current. Dload Upload Total Spent Left Speed. 100 2518k 100 2518k 0 0 5671k 0 --:--:-- --:--:-- --:--:-- 5671k. Collecting pip. Using cached pip-23.1.2-py3-none-any.whl (2.1 MB). Installing collected packages: pip. Attempting uninstall: pip. Found existing installation: pip 23.1.2. Uninstalling pip-23.1.2:. Successfully uninstalled pip-23.1.2. WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH. Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location. Successfully installed pip-23.1.2. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Python 3.8.10. pip 23.1.2 from /root/.local/lib/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:808,deployability,Updat,Update,808,"@pgrosu From the choices above, I want to pursue (2) singularity the most. . 2. Is there an instruction that you can point me to for this? 3. I want to show the messages I got from running Ubuntu 20.04 on Mac's UTM. settings.sh has been modified:. ```. export CUDNN_INSTALL_PATH=""/usr/lib/aarch64-linux-gnu"" #instead of /usr/lib/x86_64-linux-gnu. # orig: export DV_USE_GCP_OPTIMIZED_TF_WHL=""${DV_USE_GCP_OPTIMIZED_TF_WHL:-1}"". export DV_USE_GCP_OPTIMIZED_TF_WHL=""0"" #force using CPU only tensorflow. ```. The message when running:. ```. > sudo su. > source settings.sh. > ./run-prereq.sh. ========== This script is only maintained for Ubuntu 20.04. ========== Load config settings. ========== [Sun 04 Jun 2023 11:11:08 PM UTC] Stage 'Misc setup' starting. ========== [Sun 04 Jun 2023 11:11:09 PM UTC] Stage 'Update package list' starting. ========== [Sun 04 Jun 2023 11:11:10 PM UTC] Stage 'run-prereq.sh: Install development packages' starting. Calling wait_for_dpkg_lock. ========== [Sun 04 Jun 2023 11:11:10 PM UTC] Stage 'Install python3 packaging infrastructure' starting. % Total % Received % Xferd Average Speed Time Time Time Current. Dload Upload Total Spent Left Speed. 100 2518k 100 2518k 0 0 5671k 0 --:--:-- --:--:-- --:--:-- 5671k. Collecting pip. Using cached pip-23.1.2-py3-none-any.whl (2.1 MB). Installing collected packages: pip. Attempting uninstall: pip. Found existing installation: pip 23.1.2. Uninstalling pip-23.1.2:. Successfully uninstalled pip-23.1.2. WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH. Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location. Successfully installed pip-23.1.2. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Python 3.8.10. pip 23.1.2 from /root/.local/lib/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:884,deployability,Stage,Stage,884,"@pgrosu From the choices above, I want to pursue (2) singularity the most. . 2. Is there an instruction that you can point me to for this? 3. I want to show the messages I got from running Ubuntu 20.04 on Mac's UTM. settings.sh has been modified:. ```. export CUDNN_INSTALL_PATH=""/usr/lib/aarch64-linux-gnu"" #instead of /usr/lib/x86_64-linux-gnu. # orig: export DV_USE_GCP_OPTIMIZED_TF_WHL=""${DV_USE_GCP_OPTIMIZED_TF_WHL:-1}"". export DV_USE_GCP_OPTIMIZED_TF_WHL=""0"" #force using CPU only tensorflow. ```. The message when running:. ```. > sudo su. > source settings.sh. > ./run-prereq.sh. ========== This script is only maintained for Ubuntu 20.04. ========== Load config settings. ========== [Sun 04 Jun 2023 11:11:08 PM UTC] Stage 'Misc setup' starting. ========== [Sun 04 Jun 2023 11:11:09 PM UTC] Stage 'Update package list' starting. ========== [Sun 04 Jun 2023 11:11:10 PM UTC] Stage 'run-prereq.sh: Install development packages' starting. Calling wait_for_dpkg_lock. ========== [Sun 04 Jun 2023 11:11:10 PM UTC] Stage 'Install python3 packaging infrastructure' starting. % Total % Received % Xferd Average Speed Time Time Time Current. Dload Upload Total Spent Left Speed. 100 2518k 100 2518k 0 0 5671k 0 --:--:-- --:--:-- --:--:-- 5671k. Collecting pip. Using cached pip-23.1.2-py3-none-any.whl (2.1 MB). Installing collected packages: pip. Attempting uninstall: pip. Found existing installation: pip 23.1.2. Uninstalling pip-23.1.2:. Successfully uninstalled pip-23.1.2. WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH. Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location. Successfully installed pip-23.1.2. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Python 3.8.10. pip 23.1.2 from /root/.local/lib/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:906,deployability,Instal,Install,906,"@pgrosu From the choices above, I want to pursue (2) singularity the most. . 2. Is there an instruction that you can point me to for this? 3. I want to show the messages I got from running Ubuntu 20.04 on Mac's UTM. settings.sh has been modified:. ```. export CUDNN_INSTALL_PATH=""/usr/lib/aarch64-linux-gnu"" #instead of /usr/lib/x86_64-linux-gnu. # orig: export DV_USE_GCP_OPTIMIZED_TF_WHL=""${DV_USE_GCP_OPTIMIZED_TF_WHL:-1}"". export DV_USE_GCP_OPTIMIZED_TF_WHL=""0"" #force using CPU only tensorflow. ```. The message when running:. ```. > sudo su. > source settings.sh. > ./run-prereq.sh. ========== This script is only maintained for Ubuntu 20.04. ========== Load config settings. ========== [Sun 04 Jun 2023 11:11:08 PM UTC] Stage 'Misc setup' starting. ========== [Sun 04 Jun 2023 11:11:09 PM UTC] Stage 'Update package list' starting. ========== [Sun 04 Jun 2023 11:11:10 PM UTC] Stage 'run-prereq.sh: Install development packages' starting. Calling wait_for_dpkg_lock. ========== [Sun 04 Jun 2023 11:11:10 PM UTC] Stage 'Install python3 packaging infrastructure' starting. % Total % Received % Xferd Average Speed Time Time Time Current. Dload Upload Total Spent Left Speed. 100 2518k 100 2518k 0 0 5671k 0 --:--:-- --:--:-- --:--:-- 5671k. Collecting pip. Using cached pip-23.1.2-py3-none-any.whl (2.1 MB). Installing collected packages: pip. Attempting uninstall: pip. Found existing installation: pip 23.1.2. Uninstalling pip-23.1.2:. Successfully uninstalled pip-23.1.2. WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH. Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location. Successfully installed pip-23.1.2. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Python 3.8.10. pip 23.1.2 from /root/.local/lib/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:1019,deployability,Stage,Stage,1019,"es above, I want to pursue (2) singularity the most. . 2. Is there an instruction that you can point me to for this? 3. I want to show the messages I got from running Ubuntu 20.04 on Mac's UTM. settings.sh has been modified:. ```. export CUDNN_INSTALL_PATH=""/usr/lib/aarch64-linux-gnu"" #instead of /usr/lib/x86_64-linux-gnu. # orig: export DV_USE_GCP_OPTIMIZED_TF_WHL=""${DV_USE_GCP_OPTIMIZED_TF_WHL:-1}"". export DV_USE_GCP_OPTIMIZED_TF_WHL=""0"" #force using CPU only tensorflow. ```. The message when running:. ```. > sudo su. > source settings.sh. > ./run-prereq.sh. ========== This script is only maintained for Ubuntu 20.04. ========== Load config settings. ========== [Sun 04 Jun 2023 11:11:08 PM UTC] Stage 'Misc setup' starting. ========== [Sun 04 Jun 2023 11:11:09 PM UTC] Stage 'Update package list' starting. ========== [Sun 04 Jun 2023 11:11:10 PM UTC] Stage 'run-prereq.sh: Install development packages' starting. Calling wait_for_dpkg_lock. ========== [Sun 04 Jun 2023 11:11:10 PM UTC] Stage 'Install python3 packaging infrastructure' starting. % Total % Received % Xferd Average Speed Time Time Time Current. Dload Upload Total Spent Left Speed. 100 2518k 100 2518k 0 0 5671k 0 --:--:-- --:--:-- --:--:-- 5671k. Collecting pip. Using cached pip-23.1.2-py3-none-any.whl (2.1 MB). Installing collected packages: pip. Attempting uninstall: pip. Found existing installation: pip 23.1.2. Uninstalling pip-23.1.2:. Successfully uninstalled pip-23.1.2. WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH. Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location. Successfully installed pip-23.1.2. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Python 3.8.10. pip 23.1.2 from /root/.local/lib/python3.8/site-package",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:1026,deployability,Instal,Install,1026,", I want to pursue (2) singularity the most. . 2. Is there an instruction that you can point me to for this? 3. I want to show the messages I got from running Ubuntu 20.04 on Mac's UTM. settings.sh has been modified:. ```. export CUDNN_INSTALL_PATH=""/usr/lib/aarch64-linux-gnu"" #instead of /usr/lib/x86_64-linux-gnu. # orig: export DV_USE_GCP_OPTIMIZED_TF_WHL=""${DV_USE_GCP_OPTIMIZED_TF_WHL:-1}"". export DV_USE_GCP_OPTIMIZED_TF_WHL=""0"" #force using CPU only tensorflow. ```. The message when running:. ```. > sudo su. > source settings.sh. > ./run-prereq.sh. ========== This script is only maintained for Ubuntu 20.04. ========== Load config settings. ========== [Sun 04 Jun 2023 11:11:08 PM UTC] Stage 'Misc setup' starting. ========== [Sun 04 Jun 2023 11:11:09 PM UTC] Stage 'Update package list' starting. ========== [Sun 04 Jun 2023 11:11:10 PM UTC] Stage 'run-prereq.sh: Install development packages' starting. Calling wait_for_dpkg_lock. ========== [Sun 04 Jun 2023 11:11:10 PM UTC] Stage 'Install python3 packaging infrastructure' starting. % Total % Received % Xferd Average Speed Time Time Time Current. Dload Upload Total Spent Left Speed. 100 2518k 100 2518k 0 0 5671k 0 --:--:-- --:--:-- --:--:-- 5671k. Collecting pip. Using cached pip-23.1.2-py3-none-any.whl (2.1 MB). Installing collected packages: pip. Attempting uninstall: pip. Found existing installation: pip 23.1.2. Uninstalling pip-23.1.2:. Successfully uninstalled pip-23.1.2. WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH. Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location. Successfully installed pip-23.1.2. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Python 3.8.10. pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:1052,deployability,infrastructur,infrastructure,1052,"arity the most. . 2. Is there an instruction that you can point me to for this? 3. I want to show the messages I got from running Ubuntu 20.04 on Mac's UTM. settings.sh has been modified:. ```. export CUDNN_INSTALL_PATH=""/usr/lib/aarch64-linux-gnu"" #instead of /usr/lib/x86_64-linux-gnu. # orig: export DV_USE_GCP_OPTIMIZED_TF_WHL=""${DV_USE_GCP_OPTIMIZED_TF_WHL:-1}"". export DV_USE_GCP_OPTIMIZED_TF_WHL=""0"" #force using CPU only tensorflow. ```. The message when running:. ```. > sudo su. > source settings.sh. > ./run-prereq.sh. ========== This script is only maintained for Ubuntu 20.04. ========== Load config settings. ========== [Sun 04 Jun 2023 11:11:08 PM UTC] Stage 'Misc setup' starting. ========== [Sun 04 Jun 2023 11:11:09 PM UTC] Stage 'Update package list' starting. ========== [Sun 04 Jun 2023 11:11:10 PM UTC] Stage 'run-prereq.sh: Install development packages' starting. Calling wait_for_dpkg_lock. ========== [Sun 04 Jun 2023 11:11:10 PM UTC] Stage 'Install python3 packaging infrastructure' starting. % Total % Received % Xferd Average Speed Time Time Time Current. Dload Upload Total Spent Left Speed. 100 2518k 100 2518k 0 0 5671k 0 --:--:-- --:--:-- --:--:-- 5671k. Collecting pip. Using cached pip-23.1.2-py3-none-any.whl (2.1 MB). Installing collected packages: pip. Attempting uninstall: pip. Found existing installation: pip 23.1.2. Uninstalling pip-23.1.2:. Successfully uninstalled pip-23.1.2. WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH. Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location. Successfully installed pip-23.1.2. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Python 3.8.10. pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8). ========== [Sun 0",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:1313,deployability,Instal,Installing,1313,"f /usr/lib/x86_64-linux-gnu. # orig: export DV_USE_GCP_OPTIMIZED_TF_WHL=""${DV_USE_GCP_OPTIMIZED_TF_WHL:-1}"". export DV_USE_GCP_OPTIMIZED_TF_WHL=""0"" #force using CPU only tensorflow. ```. The message when running:. ```. > sudo su. > source settings.sh. > ./run-prereq.sh. ========== This script is only maintained for Ubuntu 20.04. ========== Load config settings. ========== [Sun 04 Jun 2023 11:11:08 PM UTC] Stage 'Misc setup' starting. ========== [Sun 04 Jun 2023 11:11:09 PM UTC] Stage 'Update package list' starting. ========== [Sun 04 Jun 2023 11:11:10 PM UTC] Stage 'run-prereq.sh: Install development packages' starting. Calling wait_for_dpkg_lock. ========== [Sun 04 Jun 2023 11:11:10 PM UTC] Stage 'Install python3 packaging infrastructure' starting. % Total % Received % Xferd Average Speed Time Time Time Current. Dload Upload Total Spent Left Speed. 100 2518k 100 2518k 0 0 5671k 0 --:--:-- --:--:-- --:--:-- 5671k. Collecting pip. Using cached pip-23.1.2-py3-none-any.whl (2.1 MB). Installing collected packages: pip. Attempting uninstall: pip. Found existing installation: pip 23.1.2. Uninstalling pip-23.1.2:. Successfully uninstalled pip-23.1.2. WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH. Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location. Successfully installed pip-23.1.2. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Python 3.8.10. pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8). ========== [Sun 04 Jun 2023 11:11:13 PM UTC] Stage 'Install python3 packages' starting. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorflow-cp",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:1391,deployability,instal,installation,1391,"SE_GCP_OPTIMIZED_TF_WHL:-1}"". export DV_USE_GCP_OPTIMIZED_TF_WHL=""0"" #force using CPU only tensorflow. ```. The message when running:. ```. > sudo su. > source settings.sh. > ./run-prereq.sh. ========== This script is only maintained for Ubuntu 20.04. ========== Load config settings. ========== [Sun 04 Jun 2023 11:11:08 PM UTC] Stage 'Misc setup' starting. ========== [Sun 04 Jun 2023 11:11:09 PM UTC] Stage 'Update package list' starting. ========== [Sun 04 Jun 2023 11:11:10 PM UTC] Stage 'run-prereq.sh: Install development packages' starting. Calling wait_for_dpkg_lock. ========== [Sun 04 Jun 2023 11:11:10 PM UTC] Stage 'Install python3 packaging infrastructure' starting. % Total % Received % Xferd Average Speed Time Time Time Current. Dload Upload Total Spent Left Speed. 100 2518k 100 2518k 0 0 5671k 0 --:--:-- --:--:-- --:--:-- 5671k. Collecting pip. Using cached pip-23.1.2-py3-none-any.whl (2.1 MB). Installing collected packages: pip. Attempting uninstall: pip. Found existing installation: pip 23.1.2. Uninstalling pip-23.1.2:. Successfully uninstalled pip-23.1.2. WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH. Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location. Successfully installed pip-23.1.2. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Python 3.8.10. pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8). ========== [Sun 04 Jun 2023 11:11:13 PM UTC] Stage 'Install python3 packages' starting. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompati",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:1526,deployability,instal,installed,1526," ```. > sudo su. > source settings.sh. > ./run-prereq.sh. ========== This script is only maintained for Ubuntu 20.04. ========== Load config settings. ========== [Sun 04 Jun 2023 11:11:08 PM UTC] Stage 'Misc setup' starting. ========== [Sun 04 Jun 2023 11:11:09 PM UTC] Stage 'Update package list' starting. ========== [Sun 04 Jun 2023 11:11:10 PM UTC] Stage 'run-prereq.sh: Install development packages' starting. Calling wait_for_dpkg_lock. ========== [Sun 04 Jun 2023 11:11:10 PM UTC] Stage 'Install python3 packaging infrastructure' starting. % Total % Received % Xferd Average Speed Time Time Time Current. Dload Upload Total Spent Left Speed. 100 2518k 100 2518k 0 0 5671k 0 --:--:-- --:--:-- --:--:-- 5671k. Collecting pip. Using cached pip-23.1.2-py3-none-any.whl (2.1 MB). Installing collected packages: pip. Attempting uninstall: pip. Found existing installation: pip 23.1.2. Uninstalling pip-23.1.2:. Successfully uninstalled pip-23.1.2. WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH. Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location. Successfully installed pip-23.1.2. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Python 3.8.10. pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8). ========== [Sun 04 Jun 2023 11:11:13 PM UTC] Stage 'Install python3 packages' starting. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:1707,deployability,instal,installed,1707,":11:08 PM UTC] Stage 'Misc setup' starting. ========== [Sun 04 Jun 2023 11:11:09 PM UTC] Stage 'Update package list' starting. ========== [Sun 04 Jun 2023 11:11:10 PM UTC] Stage 'run-prereq.sh: Install development packages' starting. Calling wait_for_dpkg_lock. ========== [Sun 04 Jun 2023 11:11:10 PM UTC] Stage 'Install python3 packaging infrastructure' starting. % Total % Received % Xferd Average Speed Time Time Time Current. Dload Upload Total Spent Left Speed. 100 2518k 100 2518k 0 0 5671k 0 --:--:-- --:--:-- --:--:-- 5671k. Collecting pip. Using cached pip-23.1.2-py3-none-any.whl (2.1 MB). Installing collected packages: pip. Attempting uninstall: pip. Found existing installation: pip 23.1.2. Uninstalling pip-23.1.2:. Successfully uninstalled pip-23.1.2. WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH. Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location. Successfully installed pip-23.1.2. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Python 3.8.10. pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8). ========== [Sun 04 Jun 2023 11:11:13 PM UTC] Stage 'Install python3 packages' starting. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. ERROR: pip's dependency resolver does ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:1852,deployability,manag,manager,1852,"4 Jun 2023 11:11:10 PM UTC] Stage 'run-prereq.sh: Install development packages' starting. Calling wait_for_dpkg_lock. ========== [Sun 04 Jun 2023 11:11:10 PM UTC] Stage 'Install python3 packaging infrastructure' starting. % Total % Received % Xferd Average Speed Time Time Time Current. Dload Upload Total Spent Left Speed. 100 2518k 100 2518k 0 0 5671k 0 --:--:-- --:--:-- --:--:-- 5671k. Collecting pip. Using cached pip-23.1.2-py3-none-any.whl (2.1 MB). Installing collected packages: pip. Attempting uninstall: pip. Found existing installation: pip 23.1.2. Uninstalling pip-23.1.2:. Successfully uninstalled pip-23.1.2. WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH. Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location. Successfully installed pip-23.1.2. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Python 3.8.10. pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8). ========== [Sun 04 Jun 2023 11:11:13 PM UTC] Stage 'Install python3 packages' starting. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorb",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:2087,deployability,Stage,Stage,2087,"ceived % Xferd Average Speed Time Time Time Current. Dload Upload Total Spent Left Speed. 100 2518k 100 2518k 0 0 5671k 0 --:--:-- --:--:-- --:--:-- 5671k. Collecting pip. Using cached pip-23.1.2-py3-none-any.whl (2.1 MB). Installing collected packages: pip. Attempting uninstall: pip. Found existing installation: pip 23.1.2. Uninstalling pip-23.1.2:. Successfully uninstalled pip-23.1.2. WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH. Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location. Successfully installed pip-23.1.2. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Python 3.8.10. pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8). ========== [Sun 04 Jun 2023 11:11:13 PM UTC] Stage 'Install python3 packages' starting. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. tensorflow-cpu-aws 2.11.0 requires protobuf<",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:2094,deployability,Instal,Install,2094," Xferd Average Speed Time Time Time Current. Dload Upload Total Spent Left Speed. 100 2518k 100 2518k 0 0 5671k 0 --:--:-- --:--:-- --:--:-- 5671k. Collecting pip. Using cached pip-23.1.2-py3-none-any.whl (2.1 MB). Installing collected packages: pip. Attempting uninstall: pip. Found existing installation: pip 23.1.2. Uninstalling pip-23.1.2:. Successfully uninstalled pip-23.1.2. WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH. Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location. Successfully installed pip-23.1.2. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Python 3.8.10. pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8). ========== [Sun 04 Jun 2023 11:11:13 PM UTC] Stage 'Install python3 packages' starting. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:2143,deployability,depend,dependency,2143," Upload Total Spent Left Speed. 100 2518k 100 2518k 0 0 5671k 0 --:--:-- --:--:-- --:--:-- 5671k. Collecting pip. Using cached pip-23.1.2-py3-none-any.whl (2.1 MB). Installing collected packages: pip. Attempting uninstall: pip. Found existing installation: pip 23.1.2. Uninstalling pip-23.1.2:. Successfully uninstalled pip-23.1.2. WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH. Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location. Successfully installed pip-23.1.2. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Python 3.8.10. pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8). ========== [Sun 04 Jun 2023 11:11:13 PM UTC] Stage 'Install python3 packages' starting. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incomp",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:2226,deployability,instal,installed,2226,"-:--:-- 5671k. Collecting pip. Using cached pip-23.1.2-py3-none-any.whl (2.1 MB). Installing collected packages: pip. Attempting uninstall: pip. Found existing installation: pip 23.1.2. Uninstalling pip-23.1.2:. Successfully uninstalled pip-23.1.2. WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH. Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location. Successfully installed pip-23.1.2. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Python 3.8.10. pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8). ========== [Sun 04 Jun 2023 11:11:13 PM UTC] Stage 'Install python3 packages' starting. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible. ========== [Sun 04 Jun 2023 11:11:24 PM UTC] Stage 'Install TensorFlow pip ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:2283,deployability,depend,dependency,2283,"3-none-any.whl (2.1 MB). Installing collected packages: pip. Attempting uninstall: pip. Found existing installation: pip 23.1.2. Uninstalling pip-23.1.2:. Successfully uninstalled pip-23.1.2. WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH. Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location. Successfully installed pip-23.1.2. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Python 3.8.10. pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8). ========== [Sun 04 Jun 2023 11:11:13 PM UTC] Stage 'Install python3 packages' starting. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible. ========== [Sun 04 Jun 2023 11:11:24 PM UTC] Stage 'Install TensorFlow pip package' starting. Installing standard CPU-only TensorFlo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:2415,deployability,depend,dependency,2415,"nstalling pip-23.1.2:. Successfully uninstalled pip-23.1.2. WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH. Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location. Successfully installed pip-23.1.2. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Python 3.8.10. pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8). ========== [Sun 04 Jun 2023 11:11:13 PM UTC] Stage 'Install python3 packages' starting. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible. ========== [Sun 04 Jun 2023 11:11:24 PM UTC] Stage 'Install TensorFlow pip package' starting. Installing standard CPU-only TensorFlow 2.11.0 wheel. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This beha",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:2498,deployability,instal,installed,2498,"p, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH. Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location. Successfully installed pip-23.1.2. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Python 3.8.10. pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8). ========== [Sun 04 Jun 2023 11:11:13 PM UTC] Stage 'Install python3 packages' starting. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible. ========== [Sun 04 Jun 2023 11:11:24 PM UTC] Stage 'Install TensorFlow pip package' starting. Installing standard CPU-only TensorFlow 2.11.0 wheel. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. werkzeug 2.3.4 requires ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:2555,deployability,depend,dependency,2555,"ich is not on PATH. Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location. Successfully installed pip-23.1.2. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Python 3.8.10. pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8). ========== [Sun 04 Jun 2023 11:11:13 PM UTC] Stage 'Install python3 packages' starting. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible. ========== [Sun 04 Jun 2023 11:11:24 PM UTC] Stage 'Install TensorFlow pip package' starting. Installing standard CPU-only TensorFlow 2.11.0 wheel. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:2687,deployability,depend,dependency,2687,". Successfully installed pip-23.1.2. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Python 3.8.10. pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8). ========== [Sun 04 Jun 2023 11:11:13 PM UTC] Stage 'Install python3 packages' starting. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible. ========== [Sun 04 Jun 2023 11:11:24 PM UTC] Stage 'Install TensorFlow pip package' starting. Installing standard CPU-only TensorFlow 2.11.0 wheel. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible. ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install CUDA' starting. ========== [Sun 04 Jun 2023 11:11:26 PM U",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:2770,deployability,instal,installed,2770,"sult in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Python 3.8.10. pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8). ========== [Sun 04 Jun 2023 11:11:13 PM UTC] Stage 'Install python3 packages' starting. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible. ========== [Sun 04 Jun 2023 11:11:24 PM UTC] Stage 'Install TensorFlow pip package' starting. Installing standard CPU-only TensorFlow 2.11.0 wheel. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible. ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install CUDA' starting. ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install TensorRT' starting. ========== [Sun 04 Jun 2023 11:11:26 PM UTC]",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:2827,deployability,depend,dependency,2827," the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Python 3.8.10. pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8). ========== [Sun 04 Jun 2023 11:11:13 PM UTC] Stage 'Install python3 packages' starting. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible. ========== [Sun 04 Jun 2023 11:11:24 PM UTC] Stage 'Install TensorFlow pip package' starting. Installing standard CPU-only TensorFlow 2.11.0 wheel. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible. ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install CUDA' starting. ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install TensorRT' starting. ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install other packages' starting. ERROR: pip's de",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:3201,deployability,Stage,Stage,3201,"the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible. ========== [Sun 04 Jun 2023 11:11:24 PM UTC] Stage 'Install TensorFlow pip package' starting. Installing standard CPU-only TensorFlow 2.11.0 wheel. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible. ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install CUDA' starting. ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install TensorRT' starting. ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install other packages' starting. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible. googleapis-common-protos 1.5",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:3208,deployability,Instal,Install,3208,"ages that are installed. This behaviour is the source of the following dependency conflicts. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible. ========== [Sun 04 Jun 2023 11:11:24 PM UTC] Stage 'Install TensorFlow pip package' starting. Installing standard CPU-only TensorFlow 2.11.0 wheel. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible. ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install CUDA' starting. ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install TensorRT' starting. ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install other packages' starting. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible. googleapis-common-protos 1.59.0 requ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:3250,deployability,Instal,Installing,3250,"the source of the following dependency conflicts. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible. ========== [Sun 04 Jun 2023 11:11:24 PM UTC] Stage 'Install TensorFlow pip package' starting. Installing standard CPU-only TensorFlow 2.11.0 wheel. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible. ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install CUDA' starting. ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install TensorRT' starting. ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install other packages' starting. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible. googleapis-common-protos 1.59.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:3317,deployability,depend,dependency,3317,"s 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible. ========== [Sun 04 Jun 2023 11:11:24 PM UTC] Stage 'Install TensorFlow pip package' starting. Installing standard CPU-only TensorFlow 2.11.0 wheel. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible. ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install CUDA' starting. ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install TensorRT' starting. ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install other packages' starting. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible. googleapis-common-protos 1.59.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:3400,deployability,instal,installed,3400,"OR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible. ========== [Sun 04 Jun 2023 11:11:24 PM UTC] Stage 'Install TensorFlow pip package' starting. Installing standard CPU-only TensorFlow 2.11.0 wheel. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible. ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install CUDA' starting. ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install TensorRT' starting. ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install other packages' starting. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible. googleapis-common-protos 1.59.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible. ========== [Sun 04 Jun 2023 11:11:27 PM UTC] ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:3457,deployability,depend,dependency,3457,"o account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible. ========== [Sun 04 Jun 2023 11:11:24 PM UTC] Stage 'Install TensorFlow pip package' starting. Installing standard CPU-only TensorFlow 2.11.0 wheel. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible. ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install CUDA' starting. ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install TensorRT' starting. ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install other packages' starting. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible. googleapis-common-protos 1.59.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible. ========== [Sun 04 Jun 2023 11:11:27 PM UTC] Stage 'run-prereq.sh complete' starting. ```. The last ti",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:3620,deployability,Stage,Stage,3620,", but you have numpy 1.19.2 which is incompatible. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible. ========== [Sun 04 Jun 2023 11:11:24 PM UTC] Stage 'Install TensorFlow pip package' starting. Installing standard CPU-only TensorFlow 2.11.0 wheel. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible. ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install CUDA' starting. ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install TensorRT' starting. ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install other packages' starting. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible. googleapis-common-protos 1.59.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible. ========== [Sun 04 Jun 2023 11:11:27 PM UTC] Stage 'run-prereq.sh complete' starting. ```. The last time I tried running `./build-prereq.sh`, I got error on the building of Clif. llvm-11-linker-tools not available. and now the error is:. ```. + echo -n 'Using Pyt",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:3627,deployability,Instal,Install,3627,"u have numpy 1.19.2 which is incompatible. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible. ========== [Sun 04 Jun 2023 11:11:24 PM UTC] Stage 'Install TensorFlow pip package' starting. Installing standard CPU-only TensorFlow 2.11.0 wheel. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible. ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install CUDA' starting. ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install TensorRT' starting. ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install other packages' starting. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible. googleapis-common-protos 1.59.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible. ========== [Sun 04 Jun 2023 11:11:27 PM UTC] Stage 'run-prereq.sh complete' starting. ```. The last time I tried running `./build-prereq.sh`, I got error on the building of Clif. llvm-11-linker-tools not available. and now the error is:. ```. + echo -n 'Using Python inte",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:3696,deployability,Stage,Stage,3696,"esolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible. ========== [Sun 04 Jun 2023 11:11:24 PM UTC] Stage 'Install TensorFlow pip package' starting. Installing standard CPU-only TensorFlow 2.11.0 wheel. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible. ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install CUDA' starting. ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install TensorRT' starting. ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install other packages' starting. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible. googleapis-common-protos 1.59.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible. ========== [Sun 04 Jun 2023 11:11:27 PM UTC] Stage 'run-prereq.sh complete' starting. ```. The last time I tried running `./build-prereq.sh`, I got error on the building of Clif. llvm-11-linker-tools not available. and now the error is:. ```. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/loc",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:3703,deployability,Instal,Install,3703,"does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible. ========== [Sun 04 Jun 2023 11:11:24 PM UTC] Stage 'Install TensorFlow pip package' starting. Installing standard CPU-only TensorFlow 2.11.0 wheel. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible. ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install CUDA' starting. ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install TensorRT' starting. ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install other packages' starting. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible. googleapis-common-protos 1.59.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible. ========== [Sun 04 Jun 2023 11:11:27 PM UTC] Stage 'run-prereq.sh complete' starting. ```. The last time I tried running `./build-prereq.sh`, I got error on the building of Clif. llvm-11-linker-tools not available. and now the error is:. ```. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:3776,deployability,Stage,Stage,3776,". This behaviour is the source of the following dependency conflicts. tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible. ========== [Sun 04 Jun 2023 11:11:24 PM UTC] Stage 'Install TensorFlow pip package' starting. Installing standard CPU-only TensorFlow 2.11.0 wheel. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible. ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install CUDA' starting. ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install TensorRT' starting. ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install other packages' starting. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible. googleapis-common-protos 1.59.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible. ========== [Sun 04 Jun 2023 11:11:27 PM UTC] Stage 'run-prereq.sh complete' starting. ```. The last time I tried running `./build-prereq.sh`, I got error on the building of Clif. llvm-11-linker-tools not available. and now the error is:. ```. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/bui",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:3783,deployability,Instal,Install,3783,"ehaviour is the source of the following dependency conflicts. tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible. ========== [Sun 04 Jun 2023 11:11:24 PM UTC] Stage 'Install TensorFlow pip package' starting. Installing standard CPU-only TensorFlow 2.11.0 wheel. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible. ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install CUDA' starting. ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install TensorRT' starting. ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install other packages' starting. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible. googleapis-common-protos 1.59.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible. ========== [Sun 04 Jun 2023 11:11:27 PM UTC] Stage 'run-prereq.sh complete' starting. ```. The last time I tried running `./build-prereq.sh`, I got error on the building of Clif. llvm-11-linker-tools not available. and now the error is:. ```. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cm",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:3830,deployability,depend,dependency,3830,"cy conflicts. tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible. ========== [Sun 04 Jun 2023 11:11:24 PM UTC] Stage 'Install TensorFlow pip package' starting. Installing standard CPU-only TensorFlow 2.11.0 wheel. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible. ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install CUDA' starting. ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install TensorRT' starting. ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install other packages' starting. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible. googleapis-common-protos 1.59.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible. ========== [Sun 04 Jun 2023 11:11:27 PM UTC] Stage 'run-prereq.sh complete' starting. ```. The last time I tried running `./build-prereq.sh`, I got error on the building of Clif. llvm-11-linker-tools not available. and now the error is:. ```. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:3913,deployability,instal,installed,3913," 4.23.2 which is incompatible. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible. ========== [Sun 04 Jun 2023 11:11:24 PM UTC] Stage 'Install TensorFlow pip package' starting. Installing standard CPU-only TensorFlow 2.11.0 wheel. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible. ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install CUDA' starting. ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install TensorRT' starting. ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install other packages' starting. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible. googleapis-common-protos 1.59.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible. ========== [Sun 04 Jun 2023 11:11:27 PM UTC] Stage 'run-prereq.sh complete' starting. ```. The last time I tried running `./build-prereq.sh`, I got error on the building of Clif. llvm-11-linker-tools not available. and now the error is:. ```. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler ident",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:3970,deployability,depend,dependency,3970,"requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible. ========== [Sun 04 Jun 2023 11:11:24 PM UTC] Stage 'Install TensorFlow pip package' starting. Installing standard CPU-only TensorFlow 2.11.0 wheel. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible. ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install CUDA' starting. ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install TensorRT' starting. ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install other packages' starting. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible. googleapis-common-protos 1.59.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible. ========== [Sun 04 Jun 2023 11:11:27 PM UTC] Stage 'run-prereq.sh complete' starting. ```. The last time I tried running `./build-prereq.sh`, I got error on the building of Clif. llvm-11-linker-tools not available. and now the error is:. ```. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:3999,deployability,api,api-core,3999,"u have numpy 1.19.2 which is incompatible. tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible. ========== [Sun 04 Jun 2023 11:11:24 PM UTC] Stage 'Install TensorFlow pip package' starting. Installing standard CPU-only TensorFlow 2.11.0 wheel. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible. ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install CUDA' starting. ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install TensorRT' starting. ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install other packages' starting. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible. googleapis-common-protos 1.59.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible. ========== [Sun 04 Jun 2023 11:11:27 PM UTC] Stage 'run-prereq.sh complete' starting. ```. The last time I tried running `./build-prereq.sh`, I got error on the building of Clif. llvm-11-linker-tools not available. and now the error is:. ```. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for wo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:4405,deployability,Stage,Stage,4405,"d. This behaviour is the source of the following dependency conflicts. werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible. ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install CUDA' starting. ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install TensorRT' starting. ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install other packages' starting. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible. googleapis-common-protos 1.59.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible. ========== [Sun 04 Jun 2023 11:11:27 PM UTC] Stage 'run-prereq.sh complete' starting. ```. The last time I tried running `./build-prereq.sh`, I got error on the building of Clif. llvm-11-linker-tools not available. and now the error is:. ```. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile featur",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:4484,deployability,build,build-prereq,4484,"3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible. ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install CUDA' starting. ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install TensorRT' starting. ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install other packages' starting. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible. googleapis-common-protos 1.59.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible. ========== [Sun 04 Jun 2023 11:11:27 PM UTC] Stage 'run-prereq.sh complete' starting. ```. The last time I tried running `./build-prereq.sh`, I got error on the building of Clif. llvm-11-linker-tools not available. and now the error is:. ```. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-con",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:4521,deployability,build,building,4521," you have markupsafe 2.0.1 which is incompatible. ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install CUDA' starting. ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install TensorRT' starting. ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install other packages' starting. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible. googleapis-common-protos 1.59.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible. ========== [Sun 04 Jun 2023 11:11:27 PM UTC] Stage 'run-prereq.sh complete' starting. ```. The last time I tried running `./build-prereq.sh`, I got error on the building of Clif. llvm-11-linker-tools not available. and now the error is:. ```. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") . -- C",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:4753,deployability,build,build,4753,"23 11:11:26 PM UTC] Stage 'Install other packages' starting. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible. googleapis-common-protos 1.59.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible. ========== [Sun 04 Jun 2023 11:11:27 PM UTC] Stage 'run-prereq.sh complete' starting. ```. The last time I tried running `./build-prereq.sh`, I got error on the building of Clif. llvm-11-linker-tools not available. and now the error is:. ```. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") . -- Checking for module 'protobuf'. -- Found protobuf, version 3.13.0. -- Checking for module 'libglog'. -- Found libglog, version 0.4.0. -- Looking for pthread.h. -- Looking for pthread.h - found. -- Performing Test CMAKE_HAVE_LIBC_PTH",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:4776,deployability,build,build,4776,"ge 'Install other packages' starting. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible. googleapis-common-protos 1.59.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible. ========== [Sun 04 Jun 2023 11:11:27 PM UTC] Stage 'run-prereq.sh complete' starting. ```. The last time I tried running `./build-prereq.sh`, I got error on the building of Clif. llvm-11-linker-tools not available. and now the error is:. ```. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") . -- Checking for module 'protobuf'. -- Found protobuf, version 3.13.0. -- Checking for module 'libglog'. -- Found libglog, version 0.4.0. -- Looking for pthread.h. -- Looking for pthread.h - found. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD. -- Performing Tes",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:5501,deployability,version,version,5501,"ot error on the building of Clif. llvm-11-linker-tools not available. and now the error is:. ```. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") . -- Checking for module 'protobuf'. -- Found protobuf, version 3.13.0. -- Checking for module 'libglog'. -- Found libglog, version 0.4.0. -- Looking for pthread.h. -- Looking for pthread.h - found. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed. -- Looking for pthread_create in pthreads. -- Looking for pthread_create in pthreads - not found. -- Looking for pthread_create in pthread. -- Looking for pthread_create in pthread - found. -- Found Threads: TRUE . CMake Error at clif/cmake/modules/CLIFUtils.cmake:37 (find_package):. Could not find a configuration file for package ""LLVM"" that is compatible. with requested version ""11.1.0"". The following configuration files were considered but not accepted:. /usr/lib/llvm-11/lib/cmake/llvm/LLVMConfig.cmake, version: 11.0.0. /usr/lib/llvm-11/cmake/LLVMConfig.cmake, version: 11.0.0. /lib/llvm-11/cmake/LLVMConfig.cmake, version: 11.0.0. Call Stack (most recent call first):. clif/CMakeL",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:5537,deployability,modul,module,5537,"lvm-11-linker-tools not available. and now the error is:. ```. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") . -- Checking for module 'protobuf'. -- Found protobuf, version 3.13.0. -- Checking for module 'libglog'. -- Found libglog, version 0.4.0. -- Looking for pthread.h. -- Looking for pthread.h - found. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed. -- Looking for pthread_create in pthreads. -- Looking for pthread_create in pthreads - not found. -- Looking for pthread_create in pthread. -- Looking for pthread_create in pthread - found. -- Found Threads: TRUE . CMake Error at clif/cmake/modules/CLIFUtils.cmake:37 (find_package):. Could not find a configuration file for package ""LLVM"" that is compatible. with requested version ""11.1.0"". The following configuration files were considered but not accepted:. /usr/lib/llvm-11/lib/cmake/llvm/LLVMConfig.cmake, version: 11.0.0. /usr/lib/llvm-11/cmake/LLVMConfig.cmake, version: 11.0.0. /lib/llvm-11/cmake/LLVMConfig.cmake, version: 11.0.0. Call Stack (most recent call first):. clif/CMakeLists.txt:22 (include). -- Configuri",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:5575,deployability,version,version,5575,"now the error is:. ```. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") . -- Checking for module 'protobuf'. -- Found protobuf, version 3.13.0. -- Checking for module 'libglog'. -- Found libglog, version 0.4.0. -- Looking for pthread.h. -- Looking for pthread.h - found. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed. -- Looking for pthread_create in pthreads. -- Looking for pthread_create in pthreads - not found. -- Looking for pthread_create in pthread. -- Looking for pthread_create in pthread - found. -- Found Threads: TRUE . CMake Error at clif/cmake/modules/CLIFUtils.cmake:37 (find_package):. Could not find a configuration file for package ""LLVM"" that is compatible. with requested version ""11.1.0"". The following configuration files were considered but not accepted:. /usr/lib/llvm-11/lib/cmake/llvm/LLVMConfig.cmake, version: 11.0.0. /usr/lib/llvm-11/cmake/LLVMConfig.cmake, version: 11.0.0. /lib/llvm-11/cmake/LLVMConfig.cmake, version: 11.0.0. Call Stack (most recent call first):. clif/CMakeLists.txt:22 (include). -- Configuring incomplete, errors occurred! See als",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:5607,deployability,modul,module,5607,"-n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") . -- Checking for module 'protobuf'. -- Found protobuf, version 3.13.0. -- Checking for module 'libglog'. -- Found libglog, version 0.4.0. -- Looking for pthread.h. -- Looking for pthread.h - found. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed. -- Looking for pthread_create in pthreads. -- Looking for pthread_create in pthreads - not found. -- Looking for pthread_create in pthread. -- Looking for pthread_create in pthread - found. -- Found Threads: TRUE . CMake Error at clif/cmake/modules/CLIFUtils.cmake:37 (find_package):. Could not find a configuration file for package ""LLVM"" that is compatible. with requested version ""11.1.0"". The following configuration files were considered but not accepted:. /usr/lib/llvm-11/lib/cmake/llvm/LLVMConfig.cmake, version: 11.0.0. /usr/lib/llvm-11/cmake/LLVMConfig.cmake, version: 11.0.0. /lib/llvm-11/cmake/LLVMConfig.cmake, version: 11.0.0. Call Stack (most recent call first):. clif/CMakeLists.txt:22 (include). -- Configuring incomplete, errors occurred! See also ""/root/clif/build/CMakeFiles/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:5643,deployability,version,version,5643,"cal/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") . -- Checking for module 'protobuf'. -- Found protobuf, version 3.13.0. -- Checking for module 'libglog'. -- Found libglog, version 0.4.0. -- Looking for pthread.h. -- Looking for pthread.h - found. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed. -- Looking for pthread_create in pthreads. -- Looking for pthread_create in pthreads - not found. -- Looking for pthread_create in pthread. -- Looking for pthread_create in pthread - found. -- Found Threads: TRUE . CMake Error at clif/cmake/modules/CLIFUtils.cmake:37 (find_package):. Could not find a configuration file for package ""LLVM"" that is compatible. with requested version ""11.1.0"". The following configuration files were considered but not accepted:. /usr/lib/llvm-11/lib/cmake/llvm/LLVMConfig.cmake, version: 11.0.0. /usr/lib/llvm-11/cmake/LLVMConfig.cmake, version: 11.0.0. /lib/llvm-11/cmake/LLVMConfig.cmake, version: 11.0.0. Call Stack (most recent call first):. clif/CMakeLists.txt:22 (include). -- Configuring incomplete, errors occurred! See also ""/root/clif/build/CMakeFiles/CMakeOutput.log"". See also ""/root/cli",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:5807,deployability,Fail,Failed,5807,"mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") . -- Checking for module 'protobuf'. -- Found protobuf, version 3.13.0. -- Checking for module 'libglog'. -- Found libglog, version 0.4.0. -- Looking for pthread.h. -- Looking for pthread.h - found. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed. -- Looking for pthread_create in pthreads. -- Looking for pthread_create in pthreads - not found. -- Looking for pthread_create in pthread. -- Looking for pthread_create in pthread - found. -- Found Threads: TRUE . CMake Error at clif/cmake/modules/CLIFUtils.cmake:37 (find_package):. Could not find a configuration file for package ""LLVM"" that is compatible. with requested version ""11.1.0"". The following configuration files were considered but not accepted:. /usr/lib/llvm-11/lib/cmake/llvm/LLVMConfig.cmake, version: 11.0.0. /usr/lib/llvm-11/cmake/LLVMConfig.cmake, version: 11.0.0. /lib/llvm-11/cmake/LLVMConfig.cmake, version: 11.0.0. Call Stack (most recent call first):. clif/CMakeLists.txt:22 (include). -- Configuring incomplete, errors occurred! See also ""/root/clif/build/CMakeFiles/CMakeOutput.log"". See also ""/root/clif/build/CMakeFiles/CMakeError.log"". real	2m44.183s. user	0m18.337s. sys	0m18.865s. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:6056,deployability,modul,modules,6056,"mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") . -- Checking for module 'protobuf'. -- Found protobuf, version 3.13.0. -- Checking for module 'libglog'. -- Found libglog, version 0.4.0. -- Looking for pthread.h. -- Looking for pthread.h - found. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed. -- Looking for pthread_create in pthreads. -- Looking for pthread_create in pthreads - not found. -- Looking for pthread_create in pthread. -- Looking for pthread_create in pthread - found. -- Found Threads: TRUE . CMake Error at clif/cmake/modules/CLIFUtils.cmake:37 (find_package):. Could not find a configuration file for package ""LLVM"" that is compatible. with requested version ""11.1.0"". The following configuration files were considered but not accepted:. /usr/lib/llvm-11/lib/cmake/llvm/LLVMConfig.cmake, version: 11.0.0. /usr/lib/llvm-11/cmake/LLVMConfig.cmake, version: 11.0.0. /lib/llvm-11/cmake/LLVMConfig.cmake, version: 11.0.0. Call Stack (most recent call first):. clif/CMakeLists.txt:22 (include). -- Configuring incomplete, errors occurred! See also ""/root/clif/build/CMakeFiles/CMakeOutput.log"". See also ""/root/clif/build/CMakeFiles/CMakeError.log"". real	2m44.183s. user	0m18.337s. sys	0m18.865s. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:6117,deployability,configurat,configuration,6117,"mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") . -- Checking for module 'protobuf'. -- Found protobuf, version 3.13.0. -- Checking for module 'libglog'. -- Found libglog, version 0.4.0. -- Looking for pthread.h. -- Looking for pthread.h - found. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed. -- Looking for pthread_create in pthreads. -- Looking for pthread_create in pthreads - not found. -- Looking for pthread_create in pthread. -- Looking for pthread_create in pthread - found. -- Found Threads: TRUE . CMake Error at clif/cmake/modules/CLIFUtils.cmake:37 (find_package):. Could not find a configuration file for package ""LLVM"" that is compatible. with requested version ""11.1.0"". The following configuration files were considered but not accepted:. /usr/lib/llvm-11/lib/cmake/llvm/LLVMConfig.cmake, version: 11.0.0. /usr/lib/llvm-11/cmake/LLVMConfig.cmake, version: 11.0.0. /lib/llvm-11/cmake/LLVMConfig.cmake, version: 11.0.0. Call Stack (most recent call first):. clif/CMakeLists.txt:22 (include). -- Configuring incomplete, errors occurred! See also ""/root/clif/build/CMakeFiles/CMakeOutput.log"". See also ""/root/clif/build/CMakeFiles/CMakeError.log"". real	2m44.183s. user	0m18.337s. sys	0m18.865s. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:6190,deployability,version,version,6190,"mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") . -- Checking for module 'protobuf'. -- Found protobuf, version 3.13.0. -- Checking for module 'libglog'. -- Found libglog, version 0.4.0. -- Looking for pthread.h. -- Looking for pthread.h - found. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed. -- Looking for pthread_create in pthreads. -- Looking for pthread_create in pthreads - not found. -- Looking for pthread_create in pthread. -- Looking for pthread_create in pthread - found. -- Found Threads: TRUE . CMake Error at clif/cmake/modules/CLIFUtils.cmake:37 (find_package):. Could not find a configuration file for package ""LLVM"" that is compatible. with requested version ""11.1.0"". The following configuration files were considered but not accepted:. /usr/lib/llvm-11/lib/cmake/llvm/LLVMConfig.cmake, version: 11.0.0. /usr/lib/llvm-11/cmake/LLVMConfig.cmake, version: 11.0.0. /lib/llvm-11/cmake/LLVMConfig.cmake, version: 11.0.0. Call Stack (most recent call first):. clif/CMakeLists.txt:22 (include). -- Configuring incomplete, errors occurred! See also ""/root/clif/build/CMakeFiles/CMakeOutput.log"". See also ""/root/clif/build/CMakeFiles/CMakeError.log"". real	2m44.183s. user	0m18.337s. sys	0m18.865s. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:6222,deployability,configurat,configuration,6222,"mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") . -- Checking for module 'protobuf'. -- Found protobuf, version 3.13.0. -- Checking for module 'libglog'. -- Found libglog, version 0.4.0. -- Looking for pthread.h. -- Looking for pthread.h - found. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed. -- Looking for pthread_create in pthreads. -- Looking for pthread_create in pthreads - not found. -- Looking for pthread_create in pthread. -- Looking for pthread_create in pthread - found. -- Found Threads: TRUE . CMake Error at clif/cmake/modules/CLIFUtils.cmake:37 (find_package):. Could not find a configuration file for package ""LLVM"" that is compatible. with requested version ""11.1.0"". The following configuration files were considered but not accepted:. /usr/lib/llvm-11/lib/cmake/llvm/LLVMConfig.cmake, version: 11.0.0. /usr/lib/llvm-11/cmake/LLVMConfig.cmake, version: 11.0.0. /lib/llvm-11/cmake/LLVMConfig.cmake, version: 11.0.0. Call Stack (most recent call first):. clif/CMakeLists.txt:22 (include). -- Configuring incomplete, errors occurred! See also ""/root/clif/build/CMakeFiles/CMakeOutput.log"". See also ""/root/clif/build/CMakeFiles/CMakeError.log"". real	2m44.183s. user	0m18.337s. sys	0m18.865s. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:6327,deployability,version,version,6327,"mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") . -- Checking for module 'protobuf'. -- Found protobuf, version 3.13.0. -- Checking for module 'libglog'. -- Found libglog, version 0.4.0. -- Looking for pthread.h. -- Looking for pthread.h - found. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed. -- Looking for pthread_create in pthreads. -- Looking for pthread_create in pthreads - not found. -- Looking for pthread_create in pthread. -- Looking for pthread_create in pthread - found. -- Found Threads: TRUE . CMake Error at clif/cmake/modules/CLIFUtils.cmake:37 (find_package):. Could not find a configuration file for package ""LLVM"" that is compatible. with requested version ""11.1.0"". The following configuration files were considered but not accepted:. /usr/lib/llvm-11/lib/cmake/llvm/LLVMConfig.cmake, version: 11.0.0. /usr/lib/llvm-11/cmake/LLVMConfig.cmake, version: 11.0.0. /lib/llvm-11/cmake/LLVMConfig.cmake, version: 11.0.0. Call Stack (most recent call first):. clif/CMakeLists.txt:22 (include). -- Configuring incomplete, errors occurred! See also ""/root/clif/build/CMakeFiles/CMakeOutput.log"". See also ""/root/clif/build/CMakeFiles/CMakeError.log"". real	2m44.183s. user	0m18.337s. sys	0m18.865s. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:6385,deployability,version,version,6385,"mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") . -- Checking for module 'protobuf'. -- Found protobuf, version 3.13.0. -- Checking for module 'libglog'. -- Found libglog, version 0.4.0. -- Looking for pthread.h. -- Looking for pthread.h - found. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed. -- Looking for pthread_create in pthreads. -- Looking for pthread_create in pthreads - not found. -- Looking for pthread_create in pthread. -- Looking for pthread_create in pthread - found. -- Found Threads: TRUE . CMake Error at clif/cmake/modules/CLIFUtils.cmake:37 (find_package):. Could not find a configuration file for package ""LLVM"" that is compatible. with requested version ""11.1.0"". The following configuration files were considered but not accepted:. /usr/lib/llvm-11/lib/cmake/llvm/LLVMConfig.cmake, version: 11.0.0. /usr/lib/llvm-11/cmake/LLVMConfig.cmake, version: 11.0.0. /lib/llvm-11/cmake/LLVMConfig.cmake, version: 11.0.0. Call Stack (most recent call first):. clif/CMakeLists.txt:22 (include). -- Configuring incomplete, errors occurred! See also ""/root/clif/build/CMakeFiles/CMakeOutput.log"". See also ""/root/clif/build/CMakeFiles/CMakeError.log"". real	2m44.183s. user	0m18.337s. sys	0m18.865s. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:6439,deployability,version,version,6439,"mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") . -- Checking for module 'protobuf'. -- Found protobuf, version 3.13.0. -- Checking for module 'libglog'. -- Found libglog, version 0.4.0. -- Looking for pthread.h. -- Looking for pthread.h - found. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed. -- Looking for pthread_create in pthreads. -- Looking for pthread_create in pthreads - not found. -- Looking for pthread_create in pthread. -- Looking for pthread_create in pthread - found. -- Found Threads: TRUE . CMake Error at clif/cmake/modules/CLIFUtils.cmake:37 (find_package):. Could not find a configuration file for package ""LLVM"" that is compatible. with requested version ""11.1.0"". The following configuration files were considered but not accepted:. /usr/lib/llvm-11/lib/cmake/llvm/LLVMConfig.cmake, version: 11.0.0. /usr/lib/llvm-11/cmake/LLVMConfig.cmake, version: 11.0.0. /lib/llvm-11/cmake/LLVMConfig.cmake, version: 11.0.0. Call Stack (most recent call first):. clif/CMakeLists.txt:22 (include). -- Configuring incomplete, errors occurred! See also ""/root/clif/build/CMakeFiles/CMakeOutput.log"". See also ""/root/clif/build/CMakeFiles/CMakeError.log"". real	2m44.183s. user	0m18.337s. sys	0m18.865s. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:6461,deployability,Stack,Stack,6461,"mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") . -- Checking for module 'protobuf'. -- Found protobuf, version 3.13.0. -- Checking for module 'libglog'. -- Found libglog, version 0.4.0. -- Looking for pthread.h. -- Looking for pthread.h - found. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed. -- Looking for pthread_create in pthreads. -- Looking for pthread_create in pthreads - not found. -- Looking for pthread_create in pthread. -- Looking for pthread_create in pthread - found. -- Found Threads: TRUE . CMake Error at clif/cmake/modules/CLIFUtils.cmake:37 (find_package):. Could not find a configuration file for package ""LLVM"" that is compatible. with requested version ""11.1.0"". The following configuration files were considered but not accepted:. /usr/lib/llvm-11/lib/cmake/llvm/LLVMConfig.cmake, version: 11.0.0. /usr/lib/llvm-11/cmake/LLVMConfig.cmake, version: 11.0.0. /lib/llvm-11/cmake/LLVMConfig.cmake, version: 11.0.0. Call Stack (most recent call first):. clif/CMakeLists.txt:22 (include). -- Configuring incomplete, errors occurred! See also ""/root/clif/build/CMakeFiles/CMakeOutput.log"". See also ""/root/clif/build/CMakeFiles/CMakeError.log"". real	2m44.183s. user	0m18.337s. sys	0m18.865s. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:6593,deployability,build,build,6593,"mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") . -- Checking for module 'protobuf'. -- Found protobuf, version 3.13.0. -- Checking for module 'libglog'. -- Found libglog, version 0.4.0. -- Looking for pthread.h. -- Looking for pthread.h - found. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed. -- Looking for pthread_create in pthreads. -- Looking for pthread_create in pthreads - not found. -- Looking for pthread_create in pthread. -- Looking for pthread_create in pthread - found. -- Found Threads: TRUE . CMake Error at clif/cmake/modules/CLIFUtils.cmake:37 (find_package):. Could not find a configuration file for package ""LLVM"" that is compatible. with requested version ""11.1.0"". The following configuration files were considered but not accepted:. /usr/lib/llvm-11/lib/cmake/llvm/LLVMConfig.cmake, version: 11.0.0. /usr/lib/llvm-11/cmake/LLVMConfig.cmake, version: 11.0.0. /lib/llvm-11/cmake/LLVMConfig.cmake, version: 11.0.0. Call Stack (most recent call first):. clif/CMakeLists.txt:22 (include). -- Configuring incomplete, errors occurred! See also ""/root/clif/build/CMakeFiles/CMakeOutput.log"". See also ""/root/clif/build/CMakeFiles/CMakeError.log"". real	2m44.183s. user	0m18.337s. sys	0m18.865s. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:6622,deployability,log,log,6622,"mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") . -- Checking for module 'protobuf'. -- Found protobuf, version 3.13.0. -- Checking for module 'libglog'. -- Found libglog, version 0.4.0. -- Looking for pthread.h. -- Looking for pthread.h - found. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed. -- Looking for pthread_create in pthreads. -- Looking for pthread_create in pthreads - not found. -- Looking for pthread_create in pthread. -- Looking for pthread_create in pthread - found. -- Found Threads: TRUE . CMake Error at clif/cmake/modules/CLIFUtils.cmake:37 (find_package):. Could not find a configuration file for package ""LLVM"" that is compatible. with requested version ""11.1.0"". The following configuration files were considered but not accepted:. /usr/lib/llvm-11/lib/cmake/llvm/LLVMConfig.cmake, version: 11.0.0. /usr/lib/llvm-11/cmake/LLVMConfig.cmake, version: 11.0.0. /lib/llvm-11/cmake/LLVMConfig.cmake, version: 11.0.0. Call Stack (most recent call first):. clif/CMakeLists.txt:22 (include). -- Configuring incomplete, errors occurred! See also ""/root/clif/build/CMakeFiles/CMakeOutput.log"". See also ""/root/clif/build/CMakeFiles/CMakeError.log"". real	2m44.183s. user	0m18.337s. sys	0m18.865s. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:6649,deployability,build,build,6649,"mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") . -- Checking for module 'protobuf'. -- Found protobuf, version 3.13.0. -- Checking for module 'libglog'. -- Found libglog, version 0.4.0. -- Looking for pthread.h. -- Looking for pthread.h - found. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed. -- Looking for pthread_create in pthreads. -- Looking for pthread_create in pthreads - not found. -- Looking for pthread_create in pthread. -- Looking for pthread_create in pthread - found. -- Found Threads: TRUE . CMake Error at clif/cmake/modules/CLIFUtils.cmake:37 (find_package):. Could not find a configuration file for package ""LLVM"" that is compatible. with requested version ""11.1.0"". The following configuration files were considered but not accepted:. /usr/lib/llvm-11/lib/cmake/llvm/LLVMConfig.cmake, version: 11.0.0. /usr/lib/llvm-11/cmake/LLVMConfig.cmake, version: 11.0.0. /lib/llvm-11/cmake/LLVMConfig.cmake, version: 11.0.0. Call Stack (most recent call first):. clif/CMakeLists.txt:22 (include). -- Configuring incomplete, errors occurred! See also ""/root/clif/build/CMakeFiles/CMakeOutput.log"". See also ""/root/clif/build/CMakeFiles/CMakeError.log"". real	2m44.183s. user	0m18.337s. sys	0m18.865s. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:6677,deployability,log,log,6677,"mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") . -- Checking for module 'protobuf'. -- Found protobuf, version 3.13.0. -- Checking for module 'libglog'. -- Found libglog, version 0.4.0. -- Looking for pthread.h. -- Looking for pthread.h - found. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed. -- Looking for pthread_create in pthreads. -- Looking for pthread_create in pthreads - not found. -- Looking for pthread_create in pthread. -- Looking for pthread_create in pthread - found. -- Found Threads: TRUE . CMake Error at clif/cmake/modules/CLIFUtils.cmake:37 (find_package):. Could not find a configuration file for package ""LLVM"" that is compatible. with requested version ""11.1.0"". The following configuration files were considered but not accepted:. /usr/lib/llvm-11/lib/cmake/llvm/LLVMConfig.cmake, version: 11.0.0. /usr/lib/llvm-11/cmake/LLVMConfig.cmake, version: 11.0.0. /lib/llvm-11/cmake/LLVMConfig.cmake, version: 11.0.0. Call Stack (most recent call first):. clif/CMakeLists.txt:22 (include). -- Configuring incomplete, errors occurred! See also ""/root/clif/build/CMakeFiles/CMakeOutput.log"". See also ""/root/clif/build/CMakeFiles/CMakeError.log"". real	2m44.183s. user	0m18.337s. sys	0m18.865s. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:479,energy efficiency,CPU,CPU,479,"@pgrosu From the choices above, I want to pursue (2) singularity the most. . 2. Is there an instruction that you can point me to for this? 3. I want to show the messages I got from running Ubuntu 20.04 on Mac's UTM. settings.sh has been modified:. ```. export CUDNN_INSTALL_PATH=""/usr/lib/aarch64-linux-gnu"" #instead of /usr/lib/x86_64-linux-gnu. # orig: export DV_USE_GCP_OPTIMIZED_TF_WHL=""${DV_USE_GCP_OPTIMIZED_TF_WHL:-1}"". export DV_USE_GCP_OPTIMIZED_TF_WHL=""0"" #force using CPU only tensorflow. ```. The message when running:. ```. > sudo su. > source settings.sh. > ./run-prereq.sh. ========== This script is only maintained for Ubuntu 20.04. ========== Load config settings. ========== [Sun 04 Jun 2023 11:11:08 PM UTC] Stage 'Misc setup' starting. ========== [Sun 04 Jun 2023 11:11:09 PM UTC] Stage 'Update package list' starting. ========== [Sun 04 Jun 2023 11:11:10 PM UTC] Stage 'run-prereq.sh: Install development packages' starting. Calling wait_for_dpkg_lock. ========== [Sun 04 Jun 2023 11:11:10 PM UTC] Stage 'Install python3 packaging infrastructure' starting. % Total % Received % Xferd Average Speed Time Time Time Current. Dload Upload Total Spent Left Speed. 100 2518k 100 2518k 0 0 5671k 0 --:--:-- --:--:-- --:--:-- 5671k. Collecting pip. Using cached pip-23.1.2-py3-none-any.whl (2.1 MB). Installing collected packages: pip. Attempting uninstall: pip. Found existing installation: pip 23.1.2. Uninstalling pip-23.1.2:. Successfully uninstalled pip-23.1.2. WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH. Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location. Successfully installed pip-23.1.2. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Python 3.8.10. pip 23.1.2 from /root/.local/lib/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:660,energy efficiency,Load,Load,660,"@pgrosu From the choices above, I want to pursue (2) singularity the most. . 2. Is there an instruction that you can point me to for this? 3. I want to show the messages I got from running Ubuntu 20.04 on Mac's UTM. settings.sh has been modified:. ```. export CUDNN_INSTALL_PATH=""/usr/lib/aarch64-linux-gnu"" #instead of /usr/lib/x86_64-linux-gnu. # orig: export DV_USE_GCP_OPTIMIZED_TF_WHL=""${DV_USE_GCP_OPTIMIZED_TF_WHL:-1}"". export DV_USE_GCP_OPTIMIZED_TF_WHL=""0"" #force using CPU only tensorflow. ```. The message when running:. ```. > sudo su. > source settings.sh. > ./run-prereq.sh. ========== This script is only maintained for Ubuntu 20.04. ========== Load config settings. ========== [Sun 04 Jun 2023 11:11:08 PM UTC] Stage 'Misc setup' starting. ========== [Sun 04 Jun 2023 11:11:09 PM UTC] Stage 'Update package list' starting. ========== [Sun 04 Jun 2023 11:11:10 PM UTC] Stage 'run-prereq.sh: Install development packages' starting. Calling wait_for_dpkg_lock. ========== [Sun 04 Jun 2023 11:11:10 PM UTC] Stage 'Install python3 packaging infrastructure' starting. % Total % Received % Xferd Average Speed Time Time Time Current. Dload Upload Total Spent Left Speed. 100 2518k 100 2518k 0 0 5671k 0 --:--:-- --:--:-- --:--:-- 5671k. Collecting pip. Using cached pip-23.1.2-py3-none-any.whl (2.1 MB). Installing collected packages: pip. Attempting uninstall: pip. Found existing installation: pip 23.1.2. Uninstalling pip-23.1.2:. Successfully uninstalled pip-23.1.2. WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH. Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location. Successfully installed pip-23.1.2. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Python 3.8.10. pip 23.1.2 from /root/.local/lib/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:1134,energy efficiency,Current,Current,1134," 3. I want to show the messages I got from running Ubuntu 20.04 on Mac's UTM. settings.sh has been modified:. ```. export CUDNN_INSTALL_PATH=""/usr/lib/aarch64-linux-gnu"" #instead of /usr/lib/x86_64-linux-gnu. # orig: export DV_USE_GCP_OPTIMIZED_TF_WHL=""${DV_USE_GCP_OPTIMIZED_TF_WHL:-1}"". export DV_USE_GCP_OPTIMIZED_TF_WHL=""0"" #force using CPU only tensorflow. ```. The message when running:. ```. > sudo su. > source settings.sh. > ./run-prereq.sh. ========== This script is only maintained for Ubuntu 20.04. ========== Load config settings. ========== [Sun 04 Jun 2023 11:11:08 PM UTC] Stage 'Misc setup' starting. ========== [Sun 04 Jun 2023 11:11:09 PM UTC] Stage 'Update package list' starting. ========== [Sun 04 Jun 2023 11:11:10 PM UTC] Stage 'run-prereq.sh: Install development packages' starting. Calling wait_for_dpkg_lock. ========== [Sun 04 Jun 2023 11:11:10 PM UTC] Stage 'Install python3 packaging infrastructure' starting. % Total % Received % Xferd Average Speed Time Time Time Current. Dload Upload Total Spent Left Speed. 100 2518k 100 2518k 0 0 5671k 0 --:--:-- --:--:-- --:--:-- 5671k. Collecting pip. Using cached pip-23.1.2-py3-none-any.whl (2.1 MB). Installing collected packages: pip. Attempting uninstall: pip. Found existing installation: pip 23.1.2. Uninstalling pip-23.1.2:. Successfully uninstalled pip-23.1.2. WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH. Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location. Successfully installed pip-23.1.2. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Python 3.8.10. pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8). ========== [Sun 04 Jun 2023 11:11:13 PM UTC] Stage 'Install python3 packages' starting. ERROR: p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:1852,energy efficiency,manag,manager,1852,"4 Jun 2023 11:11:10 PM UTC] Stage 'run-prereq.sh: Install development packages' starting. Calling wait_for_dpkg_lock. ========== [Sun 04 Jun 2023 11:11:10 PM UTC] Stage 'Install python3 packaging infrastructure' starting. % Total % Received % Xferd Average Speed Time Time Time Current. Dload Upload Total Spent Left Speed. 100 2518k 100 2518k 0 0 5671k 0 --:--:-- --:--:-- --:--:-- 5671k. Collecting pip. Using cached pip-23.1.2-py3-none-any.whl (2.1 MB). Installing collected packages: pip. Attempting uninstall: pip. Found existing installation: pip 23.1.2. Uninstalling pip-23.1.2:. Successfully uninstalled pip-23.1.2. WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH. Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location. Successfully installed pip-23.1.2. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Python 3.8.10. pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8). ========== [Sun 04 Jun 2023 11:11:13 PM UTC] Stage 'Install python3 packages' starting. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorb",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:2172,energy efficiency,current,currently,2172,"d. 100 2518k 100 2518k 0 0 5671k 0 --:--:-- --:--:-- --:--:-- 5671k. Collecting pip. Using cached pip-23.1.2-py3-none-any.whl (2.1 MB). Installing collected packages: pip. Attempting uninstall: pip. Found existing installation: pip 23.1.2. Uninstalling pip-23.1.2:. Successfully uninstalled pip-23.1.2. WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH. Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location. Successfully installed pip-23.1.2. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Python 3.8.10. pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8). ========== [Sun 04 Jun 2023 11:11:13 PM UTC] Stage 'Install python3 packages' starting. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible. ========== [Sun 04 Ju",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:2316,energy efficiency,cpu,cpu-aws,2316,"ing collected packages: pip. Attempting uninstall: pip. Found existing installation: pip 23.1.2. Uninstalling pip-23.1.2:. Successfully uninstalled pip-23.1.2. WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH. Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location. Successfully installed pip-23.1.2. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Python 3.8.10. pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8). ========== [Sun 04 Jun 2023 11:11:13 PM UTC] Stage 'Install python3 packages' starting. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible. ========== [Sun 04 Jun 2023 11:11:24 PM UTC] Stage 'Install TensorFlow pip package' starting. Installing standard CPU-only TensorFlow 2.11.0 wheel. ERROR: pip's dep",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:2444,energy efficiency,current,currently,2444,"sfully uninstalled pip-23.1.2. WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH. Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location. Successfully installed pip-23.1.2. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Python 3.8.10. pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8). ========== [Sun 04 Jun 2023 11:11:13 PM UTC] Stage 'Install python3 packages' starting. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible. ========== [Sun 04 Jun 2023 11:11:24 PM UTC] Stage 'Install TensorFlow pip package' starting. Installing standard CPU-only TensorFlow 2.11.0 wheel. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the fo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:2588,energy efficiency,cpu,cpu-aws,2588,"ing this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location. Successfully installed pip-23.1.2. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Python 3.8.10. pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8). ========== [Sun 04 Jun 2023 11:11:13 PM UTC] Stage 'Install python3 packages' starting. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible. ========== [Sun 04 Jun 2023 11:11:24 PM UTC] Stage 'Install TensorFlow pip package' starting. Installing standard CPU-only TensorFlow 2.11.0 wheel. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible. ========== [Sun 0",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:2716,energy efficiency,current,currently,2716,"23.1.2. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Python 3.8.10. pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8). ========== [Sun 04 Jun 2023 11:11:13 PM UTC] Stage 'Install python3 packages' starting. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible. ========== [Sun 04 Jun 2023 11:11:24 PM UTC] Stage 'Install TensorFlow pip package' starting. Installing standard CPU-only TensorFlow 2.11.0 wheel. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible. ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install CUDA' starting. ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install TensorRT' ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:2960,energy efficiency,cpu,cpu-aws,2960,"0. pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8). ========== [Sun 04 Jun 2023 11:11:13 PM UTC] Stage 'Install python3 packages' starting. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible. ========== [Sun 04 Jun 2023 11:11:24 PM UTC] Stage 'Install TensorFlow pip package' starting. Installing standard CPU-only TensorFlow 2.11.0 wheel. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible. ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install CUDA' starting. ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install TensorRT' starting. ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install other packages' starting. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the foll",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:3057,energy efficiency,cpu,cpu-aws,3057,"Jun 2023 11:11:13 PM UTC] Stage 'Install python3 packages' starting. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible. ========== [Sun 04 Jun 2023 11:11:24 PM UTC] Stage 'Install TensorFlow pip package' starting. Installing standard CPU-only TensorFlow 2.11.0 wheel. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible. ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install CUDA' starting. ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install TensorRT' starting. ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install other packages' starting. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:3270,energy efficiency,CPU,CPU-only,3270,"ollowing dependency conflicts. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible. ========== [Sun 04 Jun 2023 11:11:24 PM UTC] Stage 'Install TensorFlow pip package' starting. Installing standard CPU-only TensorFlow 2.11.0 wheel. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible. ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install CUDA' starting. ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install TensorRT' starting. ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install other packages' starting. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible. googleapis-common-protos 1.59.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:3346,energy efficiency,current,currently,3346,", but you have numpy 1.19.2 which is incompatible. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible. ========== [Sun 04 Jun 2023 11:11:24 PM UTC] Stage 'Install TensorFlow pip package' starting. Installing standard CPU-only TensorFlow 2.11.0 wheel. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible. ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install CUDA' starting. ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install TensorRT' starting. ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install other packages' starting. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible. googleapis-common-protos 1.59.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incom",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:3859,energy efficiency,current,currently,3859,"1.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible. ========== [Sun 04 Jun 2023 11:11:24 PM UTC] Stage 'Install TensorFlow pip package' starting. Installing standard CPU-only TensorFlow 2.11.0 wheel. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible. ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install CUDA' starting. ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install TensorRT' starting. ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install other packages' starting. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible. googleapis-common-protos 1.59.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible. ========== [Sun 04 Jun 2023 11:11:27 PM UTC] Stage 'run-prereq.sh complete' starting. ```. The last time I tried running `./build-prereq.sh`, I got error on the building of Clif. llvm-11-linker-tools not available. and now the error is:. ```. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:4003,energy efficiency,core,core,4003,"have numpy 1.19.2 which is incompatible. tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible. ========== [Sun 04 Jun 2023 11:11:24 PM UTC] Stage 'Install TensorFlow pip package' starting. Installing standard CPU-only TensorFlow 2.11.0 wheel. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible. ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install CUDA' starting. ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install TensorRT' starting. ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install other packages' starting. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible. googleapis-common-protos 1.59.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible. ========== [Sun 04 Jun 2023 11:11:27 PM UTC] Stage 'run-prereq.sh complete' starting. ```. The last time I tried running `./build-prereq.sh`, I got error on the building of Clif. llvm-11-linker-tools not available. and now the error is:. ```. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for work",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:161,integrability,messag,messages,161,"@pgrosu From the choices above, I want to pursue (2) singularity the most. . 2. Is there an instruction that you can point me to for this? 3. I want to show the messages I got from running Ubuntu 20.04 on Mac's UTM. settings.sh has been modified:. ```. export CUDNN_INSTALL_PATH=""/usr/lib/aarch64-linux-gnu"" #instead of /usr/lib/x86_64-linux-gnu. # orig: export DV_USE_GCP_OPTIMIZED_TF_WHL=""${DV_USE_GCP_OPTIMIZED_TF_WHL:-1}"". export DV_USE_GCP_OPTIMIZED_TF_WHL=""0"" #force using CPU only tensorflow. ```. The message when running:. ```. > sudo su. > source settings.sh. > ./run-prereq.sh. ========== This script is only maintained for Ubuntu 20.04. ========== Load config settings. ========== [Sun 04 Jun 2023 11:11:08 PM UTC] Stage 'Misc setup' starting. ========== [Sun 04 Jun 2023 11:11:09 PM UTC] Stage 'Update package list' starting. ========== [Sun 04 Jun 2023 11:11:10 PM UTC] Stage 'run-prereq.sh: Install development packages' starting. Calling wait_for_dpkg_lock. ========== [Sun 04 Jun 2023 11:11:10 PM UTC] Stage 'Install python3 packaging infrastructure' starting. % Total % Received % Xferd Average Speed Time Time Time Current. Dload Upload Total Spent Left Speed. 100 2518k 100 2518k 0 0 5671k 0 --:--:-- --:--:-- --:--:-- 5671k. Collecting pip. Using cached pip-23.1.2-py3-none-any.whl (2.1 MB). Installing collected packages: pip. Attempting uninstall: pip. Found existing installation: pip 23.1.2. Uninstalling pip-23.1.2:. Successfully uninstalled pip-23.1.2. WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH. Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location. Successfully installed pip-23.1.2. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Python 3.8.10. pip 23.1.2 from /root/.local/lib/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:509,integrability,messag,message,509,"@pgrosu From the choices above, I want to pursue (2) singularity the most. . 2. Is there an instruction that you can point me to for this? 3. I want to show the messages I got from running Ubuntu 20.04 on Mac's UTM. settings.sh has been modified:. ```. export CUDNN_INSTALL_PATH=""/usr/lib/aarch64-linux-gnu"" #instead of /usr/lib/x86_64-linux-gnu. # orig: export DV_USE_GCP_OPTIMIZED_TF_WHL=""${DV_USE_GCP_OPTIMIZED_TF_WHL:-1}"". export DV_USE_GCP_OPTIMIZED_TF_WHL=""0"" #force using CPU only tensorflow. ```. The message when running:. ```. > sudo su. > source settings.sh. > ./run-prereq.sh. ========== This script is only maintained for Ubuntu 20.04. ========== Load config settings. ========== [Sun 04 Jun 2023 11:11:08 PM UTC] Stage 'Misc setup' starting. ========== [Sun 04 Jun 2023 11:11:09 PM UTC] Stage 'Update package list' starting. ========== [Sun 04 Jun 2023 11:11:10 PM UTC] Stage 'run-prereq.sh: Install development packages' starting. Calling wait_for_dpkg_lock. ========== [Sun 04 Jun 2023 11:11:10 PM UTC] Stage 'Install python3 packaging infrastructure' starting. % Total % Received % Xferd Average Speed Time Time Time Current. Dload Upload Total Spent Left Speed. 100 2518k 100 2518k 0 0 5671k 0 --:--:-- --:--:-- --:--:-- 5671k. Collecting pip. Using cached pip-23.1.2-py3-none-any.whl (2.1 MB). Installing collected packages: pip. Attempting uninstall: pip. Found existing installation: pip 23.1.2. Uninstalling pip-23.1.2:. Successfully uninstalled pip-23.1.2. WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH. Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location. Successfully installed pip-23.1.2. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Python 3.8.10. pip 23.1.2 from /root/.local/lib/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:2143,integrability,depend,dependency,2143," Upload Total Spent Left Speed. 100 2518k 100 2518k 0 0 5671k 0 --:--:-- --:--:-- --:--:-- 5671k. Collecting pip. Using cached pip-23.1.2-py3-none-any.whl (2.1 MB). Installing collected packages: pip. Attempting uninstall: pip. Found existing installation: pip 23.1.2. Uninstalling pip-23.1.2:. Successfully uninstalled pip-23.1.2. WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH. Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location. Successfully installed pip-23.1.2. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Python 3.8.10. pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8). ========== [Sun 04 Jun 2023 11:11:13 PM UTC] Stage 'Install python3 packages' starting. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incomp",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:2283,integrability,depend,dependency,2283,"3-none-any.whl (2.1 MB). Installing collected packages: pip. Attempting uninstall: pip. Found existing installation: pip 23.1.2. Uninstalling pip-23.1.2:. Successfully uninstalled pip-23.1.2. WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH. Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location. Successfully installed pip-23.1.2. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Python 3.8.10. pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8). ========== [Sun 04 Jun 2023 11:11:13 PM UTC] Stage 'Install python3 packages' starting. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible. ========== [Sun 04 Jun 2023 11:11:24 PM UTC] Stage 'Install TensorFlow pip package' starting. Installing standard CPU-only TensorFlo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:2415,integrability,depend,dependency,2415,"nstalling pip-23.1.2:. Successfully uninstalled pip-23.1.2. WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH. Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location. Successfully installed pip-23.1.2. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Python 3.8.10. pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8). ========== [Sun 04 Jun 2023 11:11:13 PM UTC] Stage 'Install python3 packages' starting. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible. ========== [Sun 04 Jun 2023 11:11:24 PM UTC] Stage 'Install TensorFlow pip package' starting. Installing standard CPU-only TensorFlow 2.11.0 wheel. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This beha",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:2555,integrability,depend,dependency,2555,"ich is not on PATH. Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location. Successfully installed pip-23.1.2. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Python 3.8.10. pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8). ========== [Sun 04 Jun 2023 11:11:13 PM UTC] Stage 'Install python3 packages' starting. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible. ========== [Sun 04 Jun 2023 11:11:24 PM UTC] Stage 'Install TensorFlow pip package' starting. Installing standard CPU-only TensorFlow 2.11.0 wheel. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:2687,integrability,depend,dependency,2687,". Successfully installed pip-23.1.2. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Python 3.8.10. pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8). ========== [Sun 04 Jun 2023 11:11:13 PM UTC] Stage 'Install python3 packages' starting. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible. ========== [Sun 04 Jun 2023 11:11:24 PM UTC] Stage 'Install TensorFlow pip package' starting. Installing standard CPU-only TensorFlow 2.11.0 wheel. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible. ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install CUDA' starting. ========== [Sun 04 Jun 2023 11:11:26 PM U",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:2827,integrability,depend,dependency,2827," the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Python 3.8.10. pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8). ========== [Sun 04 Jun 2023 11:11:13 PM UTC] Stage 'Install python3 packages' starting. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible. ========== [Sun 04 Jun 2023 11:11:24 PM UTC] Stage 'Install TensorFlow pip package' starting. Installing standard CPU-only TensorFlow 2.11.0 wheel. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible. ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install CUDA' starting. ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install TensorRT' starting. ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install other packages' starting. ERROR: pip's de",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:3317,integrability,depend,dependency,3317,"s 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible. ========== [Sun 04 Jun 2023 11:11:24 PM UTC] Stage 'Install TensorFlow pip package' starting. Installing standard CPU-only TensorFlow 2.11.0 wheel. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible. ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install CUDA' starting. ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install TensorRT' starting. ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install other packages' starting. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible. googleapis-common-protos 1.59.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:3457,integrability,depend,dependency,3457,"o account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible. ========== [Sun 04 Jun 2023 11:11:24 PM UTC] Stage 'Install TensorFlow pip package' starting. Installing standard CPU-only TensorFlow 2.11.0 wheel. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible. ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install CUDA' starting. ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install TensorRT' starting. ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install other packages' starting. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible. googleapis-common-protos 1.59.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible. ========== [Sun 04 Jun 2023 11:11:27 PM UTC] Stage 'run-prereq.sh complete' starting. ```. The last ti",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:3830,integrability,depend,dependency,3830,"cy conflicts. tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible. ========== [Sun 04 Jun 2023 11:11:24 PM UTC] Stage 'Install TensorFlow pip package' starting. Installing standard CPU-only TensorFlow 2.11.0 wheel. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible. ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install CUDA' starting. ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install TensorRT' starting. ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install other packages' starting. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible. googleapis-common-protos 1.59.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible. ========== [Sun 04 Jun 2023 11:11:27 PM UTC] Stage 'run-prereq.sh complete' starting. ```. The last time I tried running `./build-prereq.sh`, I got error on the building of Clif. llvm-11-linker-tools not available. and now the error is:. ```. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:3970,integrability,depend,dependency,3970,"requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible. ========== [Sun 04 Jun 2023 11:11:24 PM UTC] Stage 'Install TensorFlow pip package' starting. Installing standard CPU-only TensorFlow 2.11.0 wheel. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible. ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install CUDA' starting. ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install TensorRT' starting. ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install other packages' starting. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible. googleapis-common-protos 1.59.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible. ========== [Sun 04 Jun 2023 11:11:27 PM UTC] Stage 'run-prereq.sh complete' starting. ```. The last time I tried running `./build-prereq.sh`, I got error on the building of Clif. llvm-11-linker-tools not available. and now the error is:. ```. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:3999,integrability,api,api-core,3999,"u have numpy 1.19.2 which is incompatible. tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible. ========== [Sun 04 Jun 2023 11:11:24 PM UTC] Stage 'Install TensorFlow pip package' starting. Installing standard CPU-only TensorFlow 2.11.0 wheel. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible. ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install CUDA' starting. ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install TensorRT' starting. ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install other packages' starting. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible. googleapis-common-protos 1.59.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible. ========== [Sun 04 Jun 2023 11:11:27 PM UTC] Stage 'run-prereq.sh complete' starting. ```. The last time I tried running `./build-prereq.sh`, I got error on the building of Clif. llvm-11-linker-tools not available. and now the error is:. ```. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for wo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:5501,integrability,version,version,5501,"ot error on the building of Clif. llvm-11-linker-tools not available. and now the error is:. ```. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") . -- Checking for module 'protobuf'. -- Found protobuf, version 3.13.0. -- Checking for module 'libglog'. -- Found libglog, version 0.4.0. -- Looking for pthread.h. -- Looking for pthread.h - found. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed. -- Looking for pthread_create in pthreads. -- Looking for pthread_create in pthreads - not found. -- Looking for pthread_create in pthread. -- Looking for pthread_create in pthread - found. -- Found Threads: TRUE . CMake Error at clif/cmake/modules/CLIFUtils.cmake:37 (find_package):. Could not find a configuration file for package ""LLVM"" that is compatible. with requested version ""11.1.0"". The following configuration files were considered but not accepted:. /usr/lib/llvm-11/lib/cmake/llvm/LLVMConfig.cmake, version: 11.0.0. /usr/lib/llvm-11/cmake/LLVMConfig.cmake, version: 11.0.0. /lib/llvm-11/cmake/LLVMConfig.cmake, version: 11.0.0. Call Stack (most recent call first):. clif/CMakeL",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:5575,integrability,version,version,5575,"now the error is:. ```. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") . -- Checking for module 'protobuf'. -- Found protobuf, version 3.13.0. -- Checking for module 'libglog'. -- Found libglog, version 0.4.0. -- Looking for pthread.h. -- Looking for pthread.h - found. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed. -- Looking for pthread_create in pthreads. -- Looking for pthread_create in pthreads - not found. -- Looking for pthread_create in pthread. -- Looking for pthread_create in pthread - found. -- Found Threads: TRUE . CMake Error at clif/cmake/modules/CLIFUtils.cmake:37 (find_package):. Could not find a configuration file for package ""LLVM"" that is compatible. with requested version ""11.1.0"". The following configuration files were considered but not accepted:. /usr/lib/llvm-11/lib/cmake/llvm/LLVMConfig.cmake, version: 11.0.0. /usr/lib/llvm-11/cmake/LLVMConfig.cmake, version: 11.0.0. /lib/llvm-11/cmake/LLVMConfig.cmake, version: 11.0.0. Call Stack (most recent call first):. clif/CMakeLists.txt:22 (include). -- Configuring incomplete, errors occurred! See als",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:5643,integrability,version,version,5643,"cal/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") . -- Checking for module 'protobuf'. -- Found protobuf, version 3.13.0. -- Checking for module 'libglog'. -- Found libglog, version 0.4.0. -- Looking for pthread.h. -- Looking for pthread.h - found. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed. -- Looking for pthread_create in pthreads. -- Looking for pthread_create in pthreads - not found. -- Looking for pthread_create in pthread. -- Looking for pthread_create in pthread - found. -- Found Threads: TRUE . CMake Error at clif/cmake/modules/CLIFUtils.cmake:37 (find_package):. Could not find a configuration file for package ""LLVM"" that is compatible. with requested version ""11.1.0"". The following configuration files were considered but not accepted:. /usr/lib/llvm-11/lib/cmake/llvm/LLVMConfig.cmake, version: 11.0.0. /usr/lib/llvm-11/cmake/LLVMConfig.cmake, version: 11.0.0. /lib/llvm-11/cmake/LLVMConfig.cmake, version: 11.0.0. Call Stack (most recent call first):. clif/CMakeLists.txt:22 (include). -- Configuring incomplete, errors occurred! See also ""/root/clif/build/CMakeFiles/CMakeOutput.log"". See also ""/root/cli",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:6117,integrability,configur,configuration,6117,"mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") . -- Checking for module 'protobuf'. -- Found protobuf, version 3.13.0. -- Checking for module 'libglog'. -- Found libglog, version 0.4.0. -- Looking for pthread.h. -- Looking for pthread.h - found. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed. -- Looking for pthread_create in pthreads. -- Looking for pthread_create in pthreads - not found. -- Looking for pthread_create in pthread. -- Looking for pthread_create in pthread - found. -- Found Threads: TRUE . CMake Error at clif/cmake/modules/CLIFUtils.cmake:37 (find_package):. Could not find a configuration file for package ""LLVM"" that is compatible. with requested version ""11.1.0"". The following configuration files were considered but not accepted:. /usr/lib/llvm-11/lib/cmake/llvm/LLVMConfig.cmake, version: 11.0.0. /usr/lib/llvm-11/cmake/LLVMConfig.cmake, version: 11.0.0. /lib/llvm-11/cmake/LLVMConfig.cmake, version: 11.0.0. Call Stack (most recent call first):. clif/CMakeLists.txt:22 (include). -- Configuring incomplete, errors occurred! See also ""/root/clif/build/CMakeFiles/CMakeOutput.log"". See also ""/root/clif/build/CMakeFiles/CMakeError.log"". real	2m44.183s. user	0m18.337s. sys	0m18.865s. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:6190,integrability,version,version,6190,"mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") . -- Checking for module 'protobuf'. -- Found protobuf, version 3.13.0. -- Checking for module 'libglog'. -- Found libglog, version 0.4.0. -- Looking for pthread.h. -- Looking for pthread.h - found. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed. -- Looking for pthread_create in pthreads. -- Looking for pthread_create in pthreads - not found. -- Looking for pthread_create in pthread. -- Looking for pthread_create in pthread - found. -- Found Threads: TRUE . CMake Error at clif/cmake/modules/CLIFUtils.cmake:37 (find_package):. Could not find a configuration file for package ""LLVM"" that is compatible. with requested version ""11.1.0"". The following configuration files were considered but not accepted:. /usr/lib/llvm-11/lib/cmake/llvm/LLVMConfig.cmake, version: 11.0.0. /usr/lib/llvm-11/cmake/LLVMConfig.cmake, version: 11.0.0. /lib/llvm-11/cmake/LLVMConfig.cmake, version: 11.0.0. Call Stack (most recent call first):. clif/CMakeLists.txt:22 (include). -- Configuring incomplete, errors occurred! See also ""/root/clif/build/CMakeFiles/CMakeOutput.log"". See also ""/root/clif/build/CMakeFiles/CMakeError.log"". real	2m44.183s. user	0m18.337s. sys	0m18.865s. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:6222,integrability,configur,configuration,6222,"mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") . -- Checking for module 'protobuf'. -- Found protobuf, version 3.13.0. -- Checking for module 'libglog'. -- Found libglog, version 0.4.0. -- Looking for pthread.h. -- Looking for pthread.h - found. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed. -- Looking for pthread_create in pthreads. -- Looking for pthread_create in pthreads - not found. -- Looking for pthread_create in pthread. -- Looking for pthread_create in pthread - found. -- Found Threads: TRUE . CMake Error at clif/cmake/modules/CLIFUtils.cmake:37 (find_package):. Could not find a configuration file for package ""LLVM"" that is compatible. with requested version ""11.1.0"". The following configuration files were considered but not accepted:. /usr/lib/llvm-11/lib/cmake/llvm/LLVMConfig.cmake, version: 11.0.0. /usr/lib/llvm-11/cmake/LLVMConfig.cmake, version: 11.0.0. /lib/llvm-11/cmake/LLVMConfig.cmake, version: 11.0.0. Call Stack (most recent call first):. clif/CMakeLists.txt:22 (include). -- Configuring incomplete, errors occurred! See also ""/root/clif/build/CMakeFiles/CMakeOutput.log"". See also ""/root/clif/build/CMakeFiles/CMakeError.log"". real	2m44.183s. user	0m18.337s. sys	0m18.865s. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:6327,integrability,version,version,6327,"mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") . -- Checking for module 'protobuf'. -- Found protobuf, version 3.13.0. -- Checking for module 'libglog'. -- Found libglog, version 0.4.0. -- Looking for pthread.h. -- Looking for pthread.h - found. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed. -- Looking for pthread_create in pthreads. -- Looking for pthread_create in pthreads - not found. -- Looking for pthread_create in pthread. -- Looking for pthread_create in pthread - found. -- Found Threads: TRUE . CMake Error at clif/cmake/modules/CLIFUtils.cmake:37 (find_package):. Could not find a configuration file for package ""LLVM"" that is compatible. with requested version ""11.1.0"". The following configuration files were considered but not accepted:. /usr/lib/llvm-11/lib/cmake/llvm/LLVMConfig.cmake, version: 11.0.0. /usr/lib/llvm-11/cmake/LLVMConfig.cmake, version: 11.0.0. /lib/llvm-11/cmake/LLVMConfig.cmake, version: 11.0.0. Call Stack (most recent call first):. clif/CMakeLists.txt:22 (include). -- Configuring incomplete, errors occurred! See also ""/root/clif/build/CMakeFiles/CMakeOutput.log"". See also ""/root/clif/build/CMakeFiles/CMakeError.log"". real	2m44.183s. user	0m18.337s. sys	0m18.865s. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:6385,integrability,version,version,6385,"mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") . -- Checking for module 'protobuf'. -- Found protobuf, version 3.13.0. -- Checking for module 'libglog'. -- Found libglog, version 0.4.0. -- Looking for pthread.h. -- Looking for pthread.h - found. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed. -- Looking for pthread_create in pthreads. -- Looking for pthread_create in pthreads - not found. -- Looking for pthread_create in pthread. -- Looking for pthread_create in pthread - found. -- Found Threads: TRUE . CMake Error at clif/cmake/modules/CLIFUtils.cmake:37 (find_package):. Could not find a configuration file for package ""LLVM"" that is compatible. with requested version ""11.1.0"". The following configuration files were considered but not accepted:. /usr/lib/llvm-11/lib/cmake/llvm/LLVMConfig.cmake, version: 11.0.0. /usr/lib/llvm-11/cmake/LLVMConfig.cmake, version: 11.0.0. /lib/llvm-11/cmake/LLVMConfig.cmake, version: 11.0.0. Call Stack (most recent call first):. clif/CMakeLists.txt:22 (include). -- Configuring incomplete, errors occurred! See also ""/root/clif/build/CMakeFiles/CMakeOutput.log"". See also ""/root/clif/build/CMakeFiles/CMakeError.log"". real	2m44.183s. user	0m18.337s. sys	0m18.865s. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:6439,integrability,version,version,6439,"mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") . -- Checking for module 'protobuf'. -- Found protobuf, version 3.13.0. -- Checking for module 'libglog'. -- Found libglog, version 0.4.0. -- Looking for pthread.h. -- Looking for pthread.h - found. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed. -- Looking for pthread_create in pthreads. -- Looking for pthread_create in pthreads - not found. -- Looking for pthread_create in pthread. -- Looking for pthread_create in pthread - found. -- Found Threads: TRUE . CMake Error at clif/cmake/modules/CLIFUtils.cmake:37 (find_package):. Could not find a configuration file for package ""LLVM"" that is compatible. with requested version ""11.1.0"". The following configuration files were considered but not accepted:. /usr/lib/llvm-11/lib/cmake/llvm/LLVMConfig.cmake, version: 11.0.0. /usr/lib/llvm-11/cmake/LLVMConfig.cmake, version: 11.0.0. /lib/llvm-11/cmake/LLVMConfig.cmake, version: 11.0.0. Call Stack (most recent call first):. clif/CMakeLists.txt:22 (include). -- Configuring incomplete, errors occurred! See also ""/root/clif/build/CMakeFiles/CMakeOutput.log"". See also ""/root/clif/build/CMakeFiles/CMakeError.log"". real	2m44.183s. user	0m18.337s. sys	0m18.865s. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:6531,integrability,Configur,Configuring,6531,"mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") . -- Checking for module 'protobuf'. -- Found protobuf, version 3.13.0. -- Checking for module 'libglog'. -- Found libglog, version 0.4.0. -- Looking for pthread.h. -- Looking for pthread.h - found. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed. -- Looking for pthread_create in pthreads. -- Looking for pthread_create in pthreads - not found. -- Looking for pthread_create in pthread. -- Looking for pthread_create in pthread - found. -- Found Threads: TRUE . CMake Error at clif/cmake/modules/CLIFUtils.cmake:37 (find_package):. Could not find a configuration file for package ""LLVM"" that is compatible. with requested version ""11.1.0"". The following configuration files were considered but not accepted:. /usr/lib/llvm-11/lib/cmake/llvm/LLVMConfig.cmake, version: 11.0.0. /usr/lib/llvm-11/cmake/LLVMConfig.cmake, version: 11.0.0. /lib/llvm-11/cmake/LLVMConfig.cmake, version: 11.0.0. Call Stack (most recent call first):. clif/CMakeLists.txt:22 (include). -- Configuring incomplete, errors occurred! See also ""/root/clif/build/CMakeFiles/CMakeOutput.log"". See also ""/root/clif/build/CMakeFiles/CMakeError.log"". real	2m44.183s. user	0m18.337s. sys	0m18.865s. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:161,interoperability,messag,messages,161,"@pgrosu From the choices above, I want to pursue (2) singularity the most. . 2. Is there an instruction that you can point me to for this? 3. I want to show the messages I got from running Ubuntu 20.04 on Mac's UTM. settings.sh has been modified:. ```. export CUDNN_INSTALL_PATH=""/usr/lib/aarch64-linux-gnu"" #instead of /usr/lib/x86_64-linux-gnu. # orig: export DV_USE_GCP_OPTIMIZED_TF_WHL=""${DV_USE_GCP_OPTIMIZED_TF_WHL:-1}"". export DV_USE_GCP_OPTIMIZED_TF_WHL=""0"" #force using CPU only tensorflow. ```. The message when running:. ```. > sudo su. > source settings.sh. > ./run-prereq.sh. ========== This script is only maintained for Ubuntu 20.04. ========== Load config settings. ========== [Sun 04 Jun 2023 11:11:08 PM UTC] Stage 'Misc setup' starting. ========== [Sun 04 Jun 2023 11:11:09 PM UTC] Stage 'Update package list' starting. ========== [Sun 04 Jun 2023 11:11:10 PM UTC] Stage 'run-prereq.sh: Install development packages' starting. Calling wait_for_dpkg_lock. ========== [Sun 04 Jun 2023 11:11:10 PM UTC] Stage 'Install python3 packaging infrastructure' starting. % Total % Received % Xferd Average Speed Time Time Time Current. Dload Upload Total Spent Left Speed. 100 2518k 100 2518k 0 0 5671k 0 --:--:-- --:--:-- --:--:-- 5671k. Collecting pip. Using cached pip-23.1.2-py3-none-any.whl (2.1 MB). Installing collected packages: pip. Attempting uninstall: pip. Found existing installation: pip 23.1.2. Uninstalling pip-23.1.2:. Successfully uninstalled pip-23.1.2. WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH. Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location. Successfully installed pip-23.1.2. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Python 3.8.10. pip 23.1.2 from /root/.local/lib/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:509,interoperability,messag,message,509,"@pgrosu From the choices above, I want to pursue (2) singularity the most. . 2. Is there an instruction that you can point me to for this? 3. I want to show the messages I got from running Ubuntu 20.04 on Mac's UTM. settings.sh has been modified:. ```. export CUDNN_INSTALL_PATH=""/usr/lib/aarch64-linux-gnu"" #instead of /usr/lib/x86_64-linux-gnu. # orig: export DV_USE_GCP_OPTIMIZED_TF_WHL=""${DV_USE_GCP_OPTIMIZED_TF_WHL:-1}"". export DV_USE_GCP_OPTIMIZED_TF_WHL=""0"" #force using CPU only tensorflow. ```. The message when running:. ```. > sudo su. > source settings.sh. > ./run-prereq.sh. ========== This script is only maintained for Ubuntu 20.04. ========== Load config settings. ========== [Sun 04 Jun 2023 11:11:08 PM UTC] Stage 'Misc setup' starting. ========== [Sun 04 Jun 2023 11:11:09 PM UTC] Stage 'Update package list' starting. ========== [Sun 04 Jun 2023 11:11:10 PM UTC] Stage 'run-prereq.sh: Install development packages' starting. Calling wait_for_dpkg_lock. ========== [Sun 04 Jun 2023 11:11:10 PM UTC] Stage 'Install python3 packaging infrastructure' starting. % Total % Received % Xferd Average Speed Time Time Time Current. Dload Upload Total Spent Left Speed. 100 2518k 100 2518k 0 0 5671k 0 --:--:-- --:--:-- --:--:-- 5671k. Collecting pip. Using cached pip-23.1.2-py3-none-any.whl (2.1 MB). Installing collected packages: pip. Attempting uninstall: pip. Found existing installation: pip 23.1.2. Uninstalling pip-23.1.2:. Successfully uninstalled pip-23.1.2. WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH. Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location. Successfully installed pip-23.1.2. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Python 3.8.10. pip 23.1.2 from /root/.local/lib/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:1806,interoperability,conflict,conflicting,1806,"te package list' starting. ========== [Sun 04 Jun 2023 11:11:10 PM UTC] Stage 'run-prereq.sh: Install development packages' starting. Calling wait_for_dpkg_lock. ========== [Sun 04 Jun 2023 11:11:10 PM UTC] Stage 'Install python3 packaging infrastructure' starting. % Total % Received % Xferd Average Speed Time Time Time Current. Dload Upload Total Spent Left Speed. 100 2518k 100 2518k 0 0 5671k 0 --:--:-- --:--:-- --:--:-- 5671k. Collecting pip. Using cached pip-23.1.2-py3-none-any.whl (2.1 MB). Installing collected packages: pip. Attempting uninstall: pip. Found existing installation: pip 23.1.2. Uninstalling pip-23.1.2:. Successfully uninstalled pip-23.1.2. WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH. Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location. Successfully installed pip-23.1.2. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Python 3.8.10. pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8). ========== [Sun 04 Jun 2023 11:11:13 PM UTC] Stage 'Install python3 packages' starting. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:2294,interoperability,conflict,conflicts,2294,"whl (2.1 MB). Installing collected packages: pip. Attempting uninstall: pip. Found existing installation: pip 23.1.2. Uninstalling pip-23.1.2:. Successfully uninstalled pip-23.1.2. WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH. Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location. Successfully installed pip-23.1.2. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Python 3.8.10. pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8). ========== [Sun 04 Jun 2023 11:11:13 PM UTC] Stage 'Install python3 packages' starting. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible. ========== [Sun 04 Jun 2023 11:11:24 PM UTC] Stage 'Install TensorFlow pip package' starting. Installing standard CPU-only TensorFlow 2.11.0 wh",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:2388,interoperability,incompatib,incompatible,2388,"tallation: pip 23.1.2. Uninstalling pip-23.1.2:. Successfully uninstalled pip-23.1.2. WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH. Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location. Successfully installed pip-23.1.2. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Python 3.8.10. pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8). ========== [Sun 04 Jun 2023 11:11:13 PM UTC] Stage 'Install python3 packages' starting. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible. ========== [Sun 04 Jun 2023 11:11:24 PM UTC] Stage 'Install TensorFlow pip package' starting. Installing standard CPU-only TensorFlow 2.11.0 wheel. ERROR: pip's dependency resolver does not currently take into account all the packages tha",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:2566,interoperability,conflict,conflicts,2566,"on PATH. Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location. Successfully installed pip-23.1.2. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Python 3.8.10. pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8). ========== [Sun 04 Jun 2023 11:11:13 PM UTC] Stage 'Install python3 packages' starting. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible. ========== [Sun 04 Jun 2023 11:11:24 PM UTC] Stage 'Install TensorFlow pip package' starting. Installing standard CPU-only TensorFlow 2.11.0 wheel. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatib",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:2660,interoperability,incompatib,incompatible,2660," --no-warn-script-location. Successfully installed pip-23.1.2. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Python 3.8.10. pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8). ========== [Sun 04 Jun 2023 11:11:13 PM UTC] Stage 'Install python3 packages' starting. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible. ========== [Sun 04 Jun 2023 11:11:24 PM UTC] Stage 'Install TensorFlow pip package' starting. Installing standard CPU-only TensorFlow 2.11.0 wheel. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible. ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install CUDA' starting. ========== [Sun",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:2838,interoperability,conflict,conflicts,2838," package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Python 3.8.10. pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8). ========== [Sun 04 Jun 2023 11:11:13 PM UTC] Stage 'Install python3 packages' starting. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible. ========== [Sun 04 Jun 2023 11:11:24 PM UTC] Stage 'Install TensorFlow pip package' starting. Installing standard CPU-only TensorFlow 2.11.0 wheel. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible. ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install CUDA' starting. ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install TensorRT' starting. ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install other packages' starting. ERROR: pip's dependency re",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:2935,interoperability,incompatib,incompatible,2935,"ings/venv. Python 3.8.10. pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8). ========== [Sun 04 Jun 2023 11:11:13 PM UTC] Stage 'Install python3 packages' starting. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible. ========== [Sun 04 Jun 2023 11:11:24 PM UTC] Stage 'Install TensorFlow pip package' starting. Installing standard CPU-only TensorFlow 2.11.0 wheel. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible. ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install CUDA' starting. ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install TensorRT' starting. ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install other packages' starting. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:3032,interoperability,incompatib,incompatible,3032,"8). ========== [Sun 04 Jun 2023 11:11:13 PM UTC] Stage 'Install python3 packages' starting. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible. ========== [Sun 04 Jun 2023 11:11:24 PM UTC] Stage 'Install TensorFlow pip package' starting. Installing standard CPU-only TensorFlow 2.11.0 wheel. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible. ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install CUDA' starting. ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install TensorRT' starting. ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install other packages' starting. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. google-api-core 2.11.0 requires protobuf!=3.20",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:3142,interoperability,incompatib,incompatible,3142,"dency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible. ========== [Sun 04 Jun 2023 11:11:24 PM UTC] Stage 'Install TensorFlow pip package' starting. Installing standard CPU-only TensorFlow 2.11.0 wheel. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible. ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install CUDA' starting. ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install TensorRT' starting. ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install other packages' starting. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:3261,interoperability,standard,standard,3261," of the following dependency conflicts. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible. ========== [Sun 04 Jun 2023 11:11:24 PM UTC] Stage 'Install TensorFlow pip package' starting. Installing standard CPU-only TensorFlow 2.11.0 wheel. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible. ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install CUDA' starting. ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install TensorRT' starting. ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install other packages' starting. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible. googleapis-common-protos 1.59.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
